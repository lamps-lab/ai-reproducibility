{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b7c7ddb635bfb18bb5b4f2619c7a29bdeeeaccb4",
                "externalIds": {
                    "ArXiv": "2310.00300",
                    "CorpusId": 263334090
                },
                "corpusId": 263334090,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b7c7ddb635bfb18bb5b4f2619c7a29bdeeeaccb4",
                "title": "An Easy Rejection Sampling Baseline via Gradient Refined Proposals",
                "abstract": "Rejection sampling is a common tool for low dimensional problems ($d \\leq 2$), often touted as an\"easy\"way to obtain valid samples from a distribution $f(\\cdot)$ of interest. In practice it is non-trivial to apply, often requiring considerable mathematical effort to devise a good proposal distribution $g(\\cdot)$ and select a supremum $C$. More advanced samplers require additional mathematical derivations, limitations on $f(\\cdot)$, or even cross-validation, making them difficult to apply. We devise a new approximate baseline approach to rejection sampling that works with less information, requiring only a differentiable $f(\\cdot)$ be specified, making it easier to use. We propose a new approach to rejection sampling by refining a parameterized proposal distribution with a loss derived from the acceptance threshold. In this manner we obtain comparable or better acceptance rates on current benchmarks by up to $7.3\\times$, while requiring no extra assumptions or any derivations to use: only a differentiable $f(\\cdot)$ is required. While approximate, the results are correct with high probability, and in all tests pass a distributional check. This makes our approach easy to use, reproduce, and efficacious.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34885007",
                        "name": "Edward Raff"
                    },
                    {
                        "authorId": "153883159",
                        "name": "Mark McLean"
                    },
                    {
                        "authorId": "2052617339",
                        "name": "James Holt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Optimization method AdaBelief [32] with exponential learning rate decay",
                "Mini batches Batches of size Nbatch are used, loss is averaged over batch Optimization method AdaBelief [32] with exponential learning rate decay",
                "This model has been observed to well match responses of biological neurons when the attached part is taken from a QIF [32].",
                "Alternative optimization method AdaBelief [32], but with variable learning rate."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9084f24e5e377b310b48c88a8ace294431219926",
                "externalIds": {
                    "ArXiv": "2309.14523",
                    "CorpusId": 262825401
                },
                "corpusId": 262825401,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9084f24e5e377b310b48c88a8ace294431219926",
                "title": "Smooth Exact Gradient Descent Learning in Spiking Neural Networks",
                "abstract": "Artificial neural networks are highly successfully trained with backpropagation. For spiking neural networks, however, a similar gradient descent scheme seems prohibitive due to the sudden, disruptive (dis-)appearance of spikes. Here, we demonstrate exact gradient descent learning based on spiking dynamics that change only continuously. These are generated by neuron models whose spikes vanish and appear at the end of a trial, where they do not influence other neurons anymore. This also enables gradient-based spike addition and removal. We apply our learning scheme to induce and continuously move spikes to desired times, in single neurons and recurrent networks. Further, it achieves competitive performance in a benchmark task using deep, initially silent networks. Our results show how non-disruptive learning is possible despite discrete spikes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2081089738",
                        "name": "Christian Klos"
                    },
                    {
                        "authorId": "2128183",
                        "name": "Raoul-Martin Memmesheimer"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d11fa95a5992d452b44764f521bfe6559de404fe",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-14267",
                    "ArXiv": "2309.14267",
                    "DOI": "10.48550/arXiv.2309.14267",
                    "CorpusId": 262826035
                },
                "corpusId": 262826035,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d11fa95a5992d452b44764f521bfe6559de404fe",
                "title": "Identity-preserving Editing of Multiple Facial Attributes by Learning Global Edit Directions and Local Adjustments",
                "abstract": "Semantic facial attribute editing using pre-trained Generative Adversarial Networks (GANs) has attracted a great deal of attention and effort from researchers in recent years. Due to the high quality of face images generated by StyleGANs, much work has focused on the StyleGANs' latent space and the proposed methods for facial image editing. Although these methods have achieved satisfying results for manipulating user-intended attributes, they have not fulfilled the goal of preserving the identity, which is an important challenge. We present ID-Style, a new architecture capable of addressing the problem of identity loss during attribute manipulation. The key components of ID-Style include Learnable Global Direction (LGD), which finds a shared and semi-sparse direction for each attribute, and an Instance-Aware Intensity Predictor (IAIP) network, which finetunes the global direction according to the input instance. Furthermore, we introduce two losses during training to enforce the LGD to find semi-sparse semantic directions, which along with the IAIP, preserve the identity of the input instance. Despite reducing the size of the network by roughly 95% as compared to similar state-of-the-art works, it outperforms baselines by 10% and 7% in Identity preserving metric (FRS) and average accuracy of manipulation (mACC), respectively.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2166047162",
                        "name": "Najmeh Mohammadbagheri"
                    },
                    {
                        "authorId": "2246883917",
                        "name": "Fardin Ayar"
                    },
                    {
                        "authorId": "1780566",
                        "name": "A. Nickabadi"
                    },
                    {
                        "authorId": "2246883824",
                        "name": "Reza Safabakhsh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ed45e1f12f8a16ebddb25044bf29573329db00b8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-12095",
                    "ArXiv": "2309.12095",
                    "DOI": "10.48550/arXiv.2309.12095",
                    "CorpusId": 262084000
                },
                "corpusId": 262084000,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ed45e1f12f8a16ebddb25044bf29573329db00b8",
                "title": "Bayesian sparsification for deep neural networks with Bayesian model reduction",
                "abstract": "Deep learning's immense capabilities are often constrained by the complexity of its models, leading to an increasing demand for effective sparsification techniques. Bayesian sparsification for deep learning emerges as a crucial approach, facilitating the design of models that are both computationally efficient and competitive in terms of performance across various deep learning applications. The state-of-the-art -- in Bayesian sparsification of deep neural networks -- combines structural shrinkage priors on model weights with an approximate inference scheme based on black-box stochastic variational inference. However, model inversion of the full generative model is exceptionally computationally demanding, especially when compared to standard deep learning of point estimates. In this context, we advocate for the use of Bayesian model reduction (BMR) as a more efficient alternative for pruning of model weights. As a generalization of the Savage-Dickey ratio, BMR allows a post-hoc elimination of redundant model weights based on the posterior estimates under a straightforward (non-hierarchical) generative model. Our comparative study highlights the computational efficiency and the pruning rate of the BMR method relative to the established stochastic variational inference (SVI) scheme, when applied to the full hierarchical generative model. We illustrate the potential of BMR to prune model parameters across various deep learning architectures, from classical networks like LeNet to modern frameworks such as Vision Transformers and MLP-Mixers.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243337300",
                        "name": "Dimitrije Markovi'c"
                    },
                    {
                        "authorId": "2256690562",
                        "name": "K. Friston"
                    },
                    {
                        "authorId": "1775941",
                        "name": "S. Kiebel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Amongst them, some notable algorithms are Nesterov\u2019s accelerated gradient-descent (NAG) [21], heavy-ball method (HBM) [22], and Adabelief [24].",
                "vorably for machine learning problems [24].",
                "Amongst\nthem, some notable algorithms are Nesterov\u2019s accelerated gradient-descent (NAG) [21], heavy-ball method (HBM) [22], and Adabelief [24].",
                "In particular, the recent Adabelief method has been demonstrated to compare favorably for machine learning problems [24].",
                "Built upon the prototypical gradient-descent (GD) algorithm [20], several accelerated and adaptive gradient algorithms have been proposed for solving (1) [21]\u2013[24]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "18c7219e142cfa2ab3d44f925ccef8aefc592793",
                "externalIds": {
                    "ArXiv": "2309.09957",
                    "CorpusId": 262046199
                },
                "corpusId": 262046199,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/18c7219e142cfa2ab3d44f925ccef8aefc592793",
                "title": "Quantum Circuit Optimization through Iteratively Pre-Conditioned Gradient Descent",
                "abstract": "For typical quantum subroutines in the gate-based model of quantum computing, explicit decompositions of circuits in terms of single-qubit and two-qubit entangling gates may exist. However, they often lead to large-depth circuits that are challenging for noisy intermediate-scale quantum (NISQ) hardware. Additionally, exact decompositions might only exist for some modular quantum circuits. Therefore, it is essential to find gate combinations that approximate these circuits to high fidelity with potentially low depth, for example, using gradient-based optimization. Traditional optimizers often run into problems of slow convergence requiring many iterations, and perform poorly in the presence of noise. Here we present iteratively preconditioned gradient descent (IPG) for optimizing quantum circuits and demonstrate performance speedups for state preparation and implementation of quantum algorithmic subroutines. IPG is a noise-resilient, higher-order algorithm that has shown promising gains in convergence speed for classical optimizations, converging locally at a linear rate for convex problems and superlinearly when the solution is unique. Specifically, we show an improvement in fidelity by a factor of $10^4$ for preparing a 4-qubit W state and a maximally entangled 5-qubit GHZ state compared to other commonly used classical optimizers tuning the same ansatz. We also show gains for optimizing a unitary for a quantum Fourier transform using IPG, and report results of running such optimized circuits on IonQ's quantum processing unit (QPU). Such faster convergence with promise for noise-resilience could provide advantages for quantum algorithms on NISQ hardware, especially since the cost of running each iteration on a quantum computer is substantially higher than the classical optimizer step.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2242865116",
                        "name": "Dhruv Srinivasan"
                    },
                    {
                        "authorId": "1768752581",
                        "name": "Kushal Chakrabarti"
                    },
                    {
                        "authorId": "2243186805",
                        "name": "Nikhil Chopra"
                    },
                    {
                        "authorId": "2243008473",
                        "name": "Avik Dutt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Optimization techniques such as Adam [67], Adagrad [68], Adadelta [69], and RMSprop [70] have been applied to improve the performance of neural network-based summarization models [71, 72, 73]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b4e650318956a5162df67e43ec2a33d30b98d424",
                "externalIds": {
                    "DOI": "10.1145/3624013",
                    "CorpusId": 261706338
                },
                "corpusId": 261706338,
                "publicationVenue": {
                    "id": "0f6a3a08-5e16-47c6-a0d5-fa6fdc7c16fc",
                    "name": "ACM Transactions on Asian and Low-Resource Language Information Processing",
                    "type": "conference",
                    "alternate_names": [
                        "ALRLIP",
                        "ACM Trans Asian Low-resource Lang Inf Process"
                    ],
                    "issn": "2375-4699",
                    "url": "https://tallip.acm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b4e650318956a5162df67e43ec2a33d30b98d424",
                "title": "Hindi Text Summarization using Sequence to Sequence Neural Network",
                "abstract": "Text summarizing reduces a large block of text data to a precise, short, and intelligible text that conveys the whole meaning of the actual text in a few words while maintaining the original context. Due to a lack of relevant summaries, it is hard to understand the main idea of the document. Text summarization using the abstractive technique is well-studied in English, although it is still in its infancy in Indian regional languages. In this study, we investigate the effectiveness of using a sequence-to-sequence (Seq2Seq) neural network based on attention and its optimization for text summarization for the Hindi language (HiATS), explicitly comparing the Adam and RMSprop optimizers. Our method allows the model to take the Hindi language dataset and, as output, provides a concise summary that accurately reflects the gist of the original text. The performance of the models will be evaluated using Rouge-1 and Rouge-2 metrics.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2065653332",
                        "name": "N. Kumari"
                    },
                    {
                        "authorId": "1823010",
                        "name": "Pardeep Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The training was performed for 1M steps using AdaBelief optimizer [29] taking 2 weeks on two NVIDIA RTX A6000 GPUs."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6fff864f702d8fcb6d3bc3b8f5257b6f7b3a9a62",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-05295",
                    "ArXiv": "2309.05295",
                    "DOI": "10.48550/arXiv.2309.05295",
                    "CorpusId": 261682402
                },
                "corpusId": 261682402,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6fff864f702d8fcb6d3bc3b8f5257b6f7b3a9a62",
                "title": "Discrete Denoising Diffusion Approach to Integer Factorization",
                "abstract": "Integer factorization is a famous computational problem unknown whether being solvable in the polynomial time. With the rise of deep neural networks, it is interesting whether they can facilitate faster factorization. We present an approach to factorization utilizing deep neural networks and discrete denoising diffusion that works by iteratively correcting errors in a partially-correct solution. To this end, we develop a new seq2seq neural network architecture, employ relaxed categorical distribution and adapt the reverse diffusion process to cope better with inaccuracies in the denoising step. The approach is able to find factors for integers of up to 56 bits long. Our analysis indicates that investment in training leads to an exponential decrease of sampling steps required at inference to achieve a given success rate, thus counteracting an exponential run-time increase depending on the bit-length.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2295280",
                        "name": "K\u0101rlis Freivalds"
                    },
                    {
                        "authorId": "150989416",
                        "name": "Emils Ozolins"
                    },
                    {
                        "authorId": "2591875",
                        "name": "Guntis Barzdins"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare AdaPlus with six state-of-the-art optimzers including SGDM [1], Adam [2], Nadam [5], RAdam [7], AdamW [3], and AdaBelief [6].",
                "1, we further integrate the stepsize adjusting mechanism proposed in [6] and finally propose a new optimizer named AdaPlus.",
                "As that reported in [6], using each optimizer, we train the model for 100 epochs, generating 64,000 fake images from noise.",
                "Apart from Nadam [5], AdamW [3], and AdaBelief [6], other variants of Adam also have been proposed (e.",
                "We mainly consider the \u201clarge gradient, small curvature\u201d case in which AdaBelief [6], with precise stepsize adjustment, performs differently from other adaptive methods (e.",
                "We perform extensive comparisons with six state-of-the-art optimizers: SGDM [1], Adam [2], Nadam [5], AdamW [3], RAdam [7], and AdaBelief [6].",
                "The experimental evaluations follow that reported in [6].",
                "We note that SGDM, Adam, RAdam, and AdaBelief use the same hyper-parameter tunning strategy as reported [6] which we do not report in detail due to space limit."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "e8fbed54eb98928033147e9cfe23f83580227225",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-01966",
                    "ArXiv": "2309.01966",
                    "DOI": "10.48550/arXiv.2309.01966",
                    "CorpusId": 261529954
                },
                "corpusId": 261529954,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e8fbed54eb98928033147e9cfe23f83580227225",
                "title": "AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on AdamW Basis",
                "abstract": "This paper proposes an efficient optimizer called AdaPlus which integrates Nesterov momentum and precise stepsize adjustment on AdamW basis. AdaPlus combines the advantages of AdamW, Nadam, and AdaBelief and, in particular, does not introduce any extra hyper-parameters. We perform extensive experimental evaluations on three machine learning tasks to validate the effectiveness of AdaPlus. The experiment results validate that AdaPlus (i) is the best adaptive method which performs most comparable with (even slightly better than) SGD with momentum on image classification tasks and (ii) outperforms other state-of-the-art optimizers on language modeling tasks and illustrates the highest stability when training GANs. The experiment code of AdaPlus is available at: https://github.com/guanleics/AdaPlus.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237801334",
                        "name": "Lei Guan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0495b631081a9032c611cb35097a21b2b0367d06",
                "externalIds": {
                    "DOI": "10.1016/j.bspc.2023.105251",
                    "CorpusId": 260212396,
                    "PubMed": "37587924"
                },
                "corpusId": 260212396,
                "publicationVenue": {
                    "id": "1bac31b4-014a-4981-ae41-af2a40acc162",
                    "name": "Biomedical Signal Processing and Control",
                    "type": "journal",
                    "alternate_names": [
                        "Biomed Signal Process Control"
                    ],
                    "issn": "1746-8094",
                    "url": "https://www.journals.elsevier.com/biomedical-signal-processing-and-control",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/17468094"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0495b631081a9032c611cb35097a21b2b0367d06",
                "title": "A Novel Application of Spectrograms with Machine Learning Can Detect Patient Ventilator Dyssynchrony.",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2090915085",
                        "name": "I. Obeso"
                    },
                    {
                        "authorId": "120364617",
                        "name": "B. Yoon"
                    },
                    {
                        "authorId": "145879340",
                        "name": "D. Ledbetter"
                    },
                    {
                        "authorId": "145905861",
                        "name": "M. Aczon"
                    },
                    {
                        "authorId": "2795834",
                        "name": "Eugene Laksana"
                    },
                    {
                        "authorId": "2143584283",
                        "name": "Alice Zhou"
                    },
                    {
                        "authorId": "11601610",
                        "name": "R. A. Eckberg"
                    },
                    {
                        "authorId": "119443632",
                        "name": "Keith Mertan"
                    },
                    {
                        "authorId": "2909617",
                        "name": "R. Khemani"
                    },
                    {
                        "authorId": "144616817",
                        "name": "R. Wetzel"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "576ce7f354efc6ab2363a9f9e2df3665ef51caf8",
                "externalIds": {
                    "DOI": "10.1016/j.eswa.2023.121659",
                    "CorpusId": 262226242
                },
                "corpusId": 262226242,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/576ce7f354efc6ab2363a9f9e2df3665ef51caf8",
                "title": "Parameter training method for convolutional neural networks based on improved Hausdorff-like derivative",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2221249878",
                        "name": "Kai Jia"
                    },
                    {
                        "authorId": "2153265574",
                        "name": "Zhe Gao"
                    },
                    {
                        "authorId": "2240091513",
                        "name": "Shasha Xiao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[43] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan.",
                "We train all our FFN and RNN networks with crossentropy loss and AdaBelief optimizer [43].",
                "Experimental set-up We pre-train on the train set, with the AdaBelief optimizer [43], with a learning rate of 3."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d5225dc8208b2b4b59c3c96c90de45f601806047",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-12075",
                    "ArXiv": "2308.12075",
                    "DOI": "10.48550/arXiv.2308.12075",
                    "CorpusId": 261076365
                },
                "corpusId": 261076365,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d5225dc8208b2b4b59c3c96c90de45f601806047",
                "title": "Stabilizing RNN Gradients through Pre-training",
                "abstract": "Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponential explosion that emerges from counting gradient paths in a rectangular grid in depth and time. We propose a new approach to mitigate this issue, that consists on giving a weight of a half to the time and depth contributions to the gradient, instead of the classical weight of one. Our empirical results confirm that pre-training both feed-forward and recurrent networks to fulfill the LSC often results in improved final performance across models. This study contributes to the field by providing a means to stabilize networks of any complexity. Our approach can be implemented as an additional step before pre-training on large augmented datasets, and as an alternative to finding stable initializations analytically.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2151859590",
                        "name": "Luca Herranz-Celotti"
                    },
                    {
                        "authorId": "1680808",
                        "name": "J. Rouat"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief [24] was used as the optimization algorithm."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "977f2f760d0eb99d73182fd8334c1986989c5b07",
                "externalIds": {
                    "DOI": "10.21437/interspeech.2023-286",
                    "CorpusId": 260905773
                },
                "corpusId": 260905773,
                "publicationVenue": {
                    "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
                    "name": "Interspeech",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Int Speech Commun Assoc",
                        "INTERSPEECH",
                        "Conference of the International Speech Communication Association"
                    ],
                    "issn": "2308-457X",
                    "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
                    "alternate_urls": [
                        "http://www.isca-speech.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/977f2f760d0eb99d73182fd8334c1986989c5b07",
                "title": "Speech Synthesis from Articulatory Movements Recorded by Real-time MRI",
                "abstract": "Previous speech synthesis models from articulatory movements recorded using real-time MRI (rtMRI) only predicted vocal tract shape parameters and required additional pitch information to generate a speech waveform. This study proposes a two-stage deep learning model composed of CNN-BiLSTM that predicts a mel-spectrogram from a rtMRI video and a HiFi-GAN vocoder that synthesizes a speech waveform. We evaluated our model on two databases: the ATR 503 sentences rtMRI database and the USC-TIMIT database. The experimental results on the ATR 503 sentences rtMRI database show that the PESQ score and the RMSE of F 0 are 1.64 and 26.7 Hz. This demonstrates that all acoustic parameters, including fundamental frequency, can be estimated from the rtMRI videos. In the experiment on the USC-TIMIT database, we obtained a good PESQ score and RMSE for F 0 . However, the synthesized speech is unclear, indicating that the quality of the datasets affects the intelligibility of the synthesized speech.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2229490672",
                        "name": "Yuto Otani"
                    },
                    {
                        "authorId": "2059595087",
                        "name": "Shun Sawada"
                    },
                    {
                        "authorId": "20966634",
                        "name": "Hidefumi Ohmura"
                    },
                    {
                        "authorId": "50368245",
                        "name": "Kouichi Katsurada"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[25] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan.",
                "Thus, AdaBelief adapts the update pace based on the alignment between these two information sources.",
                "But when there is a large gap between the estimator and noisy gradient, AdaBelief slows down the updates.",
                "AdaBelief [25]: at = \u03c4at\u22121 + (1\u2212 \u03c4)(\u2207gf (ut, yt; \u03b6t) \u00b7 v\u2032 t \u2212 vt)(2), At = diag ( \u221a at + \u03c1) ; bt = \u03c4bt\u22121 + (1\u2212 \u03c4)(\u2207yf (ut, yt; \u03b6t)\u2212 wt)(2), Bt = diag( \u221a bt + \u03c1).",
                "Note that this is the first work that introduces AdaBelief into the compositional minimax optimization problem without using a large batch size.",
                "AdaBelief [25]:\nat = \u03c4at\u22121 + (1\u2212 \u03c4)(\u2207gf (ut, yt; \u03b6t) \u00b7 v\u2032t \u2212 vt)2, At = diag ( \u221a at + \u03c1) ; bt = \u03c4bt\u22121 + (1\u2212 \u03c4)(\u2207yf (ut, yt; \u03b6t)\u2212 wt)2, Bt = diag( \u221a bt + \u03c1).",
                "Adaptive learning rates have been widely used in stochastic optimization problems, with many successful methods proposed such as Adam [24], AdaBelief [25], AMSGrad [26], and AdaBound [27].",
                "In case 2, we consider using AdaBelief.",
                "It is worth noting that we can generate the two matrices At and Bt by a class of adaptive learning rates generators such as Adam [24], AdaBelief, [25], AMSGrad [26], AdaBound [27].",
                "\u25a1\nF ADA-NSTORM with the Different Adam-Type Generator\nAdaptive learning rates have been widely used in stochastic optimization problems, with many successful methods proposed such as Adam [24], AdaBelief [25], AMSGrad [26], and AdaBound [27].",
                "AdaBelief incorporates both the noisy gradients and estimator values when updating x."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b09f1cae59ba7ce979c37cd25a728a25b3b677aa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-09604",
                    "ArXiv": "2308.09604",
                    "DOI": "10.48550/arXiv.2308.09604",
                    "CorpusId": 261030894
                },
                "corpusId": 261030894,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b09f1cae59ba7ce979c37cd25a728a25b3b677aa",
                "title": "Breaking the Complexity Barrier in Compositional Minimax Optimization",
                "abstract": "Compositional minimax optimization is a pivotal yet under-explored challenge across machine learning, including distributionally robust training and policy evaluation for reinforcement learning. Current techniques exhibit suboptimal complexity or rely heavily on large batch sizes. This paper proposes Nested STOchastic Recursive Momentum (NSTORM), attaining the optimal sample complexity of $O(\\kappa^3/\\epsilon^3)$ for finding an $\\epsilon$-accurate solution. However, NSTORM requires low learning rates, potentially limiting applicability. Thus we introduce ADAptive NSTORM (ADA-NSTORM) with adaptive learning rates, proving it achieves the same sample complexity while experiments demonstrate greater effectiveness. Our methods match lower bounds for minimax optimization without large batch requirements, validated through extensive experiments. This work significantly advances compositional minimax optimization, a crucial capability for distributional robustness and policy evaluation",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2232655446",
                        "name": "Jin Liu"
                    },
                    {
                        "authorId": "2115621207",
                        "name": "Xiaokang Pan"
                    },
                    {
                        "authorId": "2213554635",
                        "name": "Junwen Duan"
                    },
                    {
                        "authorId": "2232748446",
                        "name": "Hongdong Li"
                    },
                    {
                        "authorId": "2110859833",
                        "name": "Youqi Li"
                    },
                    {
                        "authorId": "2070846345",
                        "name": "Zhe Qu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f8d0838de6f2e85a1fe9f5ddc269d60e5cd4d50b",
                "externalIds": {
                    "DOI": "10.1117/12.2673821",
                    "CorpusId": 260855872
                },
                "corpusId": 260855872,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f8d0838de6f2e85a1fe9f5ddc269d60e5cd4d50b",
                "title": "Improved segmentation of damages on high-resolution coating images using CNN-based ensemble learning",
                "abstract": "The automation of inspection processes in aircraft engines comprises challenging computer vision tasks. In particular, the inspection of coating damages in confined spaces with hand-held endoscopes is based on image data acquired under dynamic operating conditions (illumination, position and orientation of the sensor, etc.). In this study, 2D RGB video data is processed to quantify damages in large coating areas. Therefore, the video frames are pre-processed by feature tracking and stitching algorithms to generate high-resolution overview images. For the subsequent analysis of the whole coating area and to overcome the challenges posed by the diverse image data, Convolutional Neural Networks (CNNs) are applied. In a preliminary study, it was found that the image analysis is advantageous when executed on different scales. Here, one CNN is applied on small image patches without down-scaling, while a second CNN is applied on larger down-scaled image patches. This multi-scale approach raises the challenge to combine the predictions of both networks. Therefore, this study presents a novel method to increase the segmentation accuracy by interpreting the network results to derive a final segmentation mask. This ensemble method consists of a CNN, which is applied on the predictions of the given patches from the overview images. The evaluation of this method comprises different pre-processing techniques regarding the logit outputs of the preceding networks as well as additional information such as RGB image data. Further, different network structures are evaluated, which include own structures specifically designed for this task. Finally, these approaches are compared against state-of-the-art network structures.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2088509664",
                        "name": "Kolja Hedrich"
                    },
                    {
                        "authorId": "113191831",
                        "name": "L. Hinz"
                    },
                    {
                        "authorId": "2847349",
                        "name": "E. Reithmeier"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is a commonly used assumption in theoretical analysis of stochastic nonconvex optimization problems [25,36]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d0921250d45876f0a0f01ac30e799c820022193d",
                "externalIds": {
                    "DOI": "10.3390/math11153403",
                    "CorpusId": 260648127
                },
                "corpusId": 260648127,
                "publicationVenue": {
                    "id": "6175efe8-6f8e-4cbe-8cee-d154f4e78627",
                    "name": "Mathematics",
                    "issn": "2227-7390",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-283014",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-283014",
                        "https://www.mdpi.com/journal/mathematics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d0921250d45876f0a0f01ac30e799c820022193d",
                "title": "A Derivative-Incorporated Adaptive Gradient Method for Federated Learning",
                "abstract": "As a new machine learning technique, federated learning has received more attention in recent years, which enables decentralized model training across data silos or edge intelligent devices in the Internet of Things without exchanging local raw data. All kinds of algorithms are proposed to solve the challenges in federated learning. However, most of these methods are based on stochastic gradient descent, which undergoes slow convergence and unstable performance during the training stage. In this paper, we propose a differential adaptive federated optimization method, which incorporates an adaptive learning rate and the gradient difference into the iteration rule of the global model. We further adopt the first-order moment estimation to compute the approximate value of the differential term so as to avoid amplifying the random noise from the input data sample. The theoretical convergence guarantee is established for our proposed method in a stochastic non-convex setting under full client participation and partial client participation cases. Experiments for the image classification task are performed on two standard datasets by training a neural network model, and experiment results on different baselines demonstrate the effectiveness of our proposed method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2191085433",
                        "name": "Huimin Gao"
                    },
                    {
                        "authorId": "1701108",
                        "name": "Qingtao Wu"
                    },
                    {
                        "authorId": "2230182858",
                        "name": "Hongyan Cao"
                    },
                    {
                        "authorId": "51178617",
                        "name": "Xuhui Zhao"
                    },
                    {
                        "authorId": "144646645",
                        "name": "Junlong Zhu"
                    },
                    {
                        "authorId": "1680884",
                        "name": "Mingchuan Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[52] J. Zhang et al., \u201cAdabelief optimizer: Adapting stepsizes by the belief in observed gradients,\u201d 2020, arXiv:2010.07468.",
                "Based on the losses calculated in each phase, the FLP model is updated with Adam [51], while AT model is updated with Adabelief [52]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "625d5162adbc5db195fdb4e1d9f625c309a7e6b1",
                "externalIds": {
                    "DBLP": "journals/tai/ChanC23",
                    "DOI": "10.1109/TAI.2022.3173582",
                    "CorpusId": 248713211
                },
                "corpusId": 248713211,
                "publicationVenue": {
                    "id": "3c27e831-750f-45bc-9914-2148a5259eba",
                    "name": "IEEE Transactions on Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Artif Intell"
                    ],
                    "issn": "2691-4581",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688"
                },
                "url": "https://www.semanticscholar.org/paper/625d5162adbc5db195fdb4e1d9f625c309a7e6b1",
                "title": "Lightweight Convolutional-iConformer for Sound Event Detection",
                "abstract": "The development of a sound event detection (SED) system is no trivial task where one has to consider both audio tagging and temporal localization concurrently. Often model ensembling is adopted to increase the overall detection accuracy. However, this can result in a large system that may face deployment issues in a resource-constrained environment. Subsequently, strongly labeled data was found to improve the audio classification performance in sound-related domains; this may indicate that such data may be required for SED model development. However, such data will inevitably contain a certain level of noise. In order to reduce the number of parameters, we proposed a lightweight system that utilized an improved depthwise separable convolution and an improved conformer layer. This lightweight system is then trained using an extension of the binary cross-entropy loss which considers the reverse binary cross-entropy to combat the noise that may be present in the training data. Based on the proposed framework, our lightweight system can obtain an event-based F1-score of 52%, and the ensemble of four systems through posterior averaging can further improve the event-based F1-score to 53.5%. Such results indicate a minimum margin of 16% against the Detection and Classification of Acoustic Scenes and events (DCASE) 2020 challenge task 4 baseline system. By comparing against the nonensembled system of the first-place submission in DCASE 2020 challenge task 4, our nonensembled system can achieve a higher event-based F1-score of 6% with 75% fewer parameters. In terms of the performance of the ensembled system, our approach remains competitive against the ensembled system of the first-place submission in DCASE 2020 challenge task 4 and has a winning margin of 2.9% with 88% fewer parameters. Comparison with other state-of-the-art also indicates that our system performance is better despite using a lightweight system.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144585713",
                        "name": "T. K. Chan"
                    },
                    {
                        "authorId": "46761071",
                        "name": "C. Chin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9450d640bda6bc427fa14533be06169f5c77ddcc",
                "externalIds": {
                    "DOI": "10.1029/2023SW003485",
                    "CorpusId": 260718958
                },
                "corpusId": 260718958,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9450d640bda6bc427fa14533be06169f5c77ddcc",
                "title": "Neural Networks for Operational SYM\u2010H Forecasting Using Attention and SWICS Plasma Features",
                "abstract": "In this work, we present an Artificial Neural Network for operational forecasting of the SYM\u2010H geomagnetic index up to 2 hr ahead using the Interplanetary Magnetic Field, the solar wind plasma features and previous SYM\u2010H values. Former works that forecast the SYM\u2010H index use data measured by ACE, in particular from the MAG and SWEPAM instruments. However, the plasma data present a high amount of missing samples. This issue has been addressed in the literature, often using linear interpolation, which leads to a non\u2010accurate data reconstruction, specially when the features are missing during the most intense periods of a geomagnetic storm. To overcome that issue, we use ACE's Solar Wind Ion Composition Spectrometer (SWICS) data to fill the missing plasma features. To validate this technique, we compare the results of our forecasting model trained using plasma features in two ways: only using SWEPAM and performing linear interpolation and using SWICS to fill the missing values in SWEPAM. Then, both models are evaluated in an operational scenario, when only SWEPAM data are available and interpolation can only be performed if the missing values are surrounded by valid measurements. In both cases, our model outperforms the current literature forecasting the SYM\u2010H one and 2 hr ahead, yielding the best results when the training has been done using the data completed using SWICS measurements.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1944377110",
                        "name": "Armando Collado\u2010Villaverde"
                    },
                    {
                        "authorId": "2203932769",
                        "name": "Pablo Mu\u00f1oz"
                    },
                    {
                        "authorId": "2055130105",
                        "name": "C. Cid"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Even though adaptive techniques generalize [48] less effectively than SGD for many models, like convolutional neural networks (CNNs), they are typically employed as the default method because of their stability in challenging situations, such as the SqueezeNet model."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f3afa2a4d3a440020e450cfe1ee67f92c85c68cd",
                "externalIds": {
                    "PubMedCentral": "10417387",
                    "DOI": "10.3390/diagnostics13152531",
                    "CorpusId": 260397120,
                    "PubMed": "37568894"
                },
                "corpusId": 260397120,
                "publicationVenue": {
                    "id": "1944b6e1-2c1d-4f42-88e3-9f8a52f57e47",
                    "name": "Diagnostics",
                    "issn": "2075-4418",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217965",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/diagnostics",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217965"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f3afa2a4d3a440020e450cfe1ee67f92c85c68cd",
                "title": "Assist-Dermo: A Lightweight Separable Vision Transformer Model for Multiclass Skin Lesion Classification",
                "abstract": "A dermatologist-like automatic classification system is developed in this paper to recognize nine different classes of pigmented skin lesions (PSLs), using a separable vision transformer (SVT) technique to assist clinical experts in early skin cancer detection. In the past, researchers have developed a few systems to recognize nine classes of PSLs. However, they often require enormous computations to achieve high performance, which is burdensome to deploy on resource-constrained devices. In this paper, a new approach to designing SVT architecture is developed based on SqueezeNet and depthwise separable CNN models. The primary goal is to find a deep learning architecture with few parameters that has comparable accuracy to state-of-the-art (SOTA) architectures. This paper modifies the SqueezeNet design for improved runtime performance by utilizing depthwise separable convolutions rather than simple conventional units. To develop this Assist-Dermo system, a data augmentation technique is applied to control the PSL imbalance problem. Next, a pre-processing step is integrated to select the most dominant region and then enhance the lesion patterns in a perceptual-oriented color space. Afterwards, the Assist-Dermo system is designed to improve efficacy and performance with several layers and multiple filter sizes but fewer filters and parameters. For the training and evaluation of Assist-Dermo models, a set of PSL images is collected from different online data sources such as Ph2, ISBI-2017, HAM10000, and ISIC to recognize nine classes of PSLs. On the chosen dataset, it achieves an accuracy (ACC) of 95.6%, a sensitivity (SE) of 96.7%, a specificity (SP) of 95%, and an area under the curve (AUC) of 0.95. The experimental results show that the suggested Assist-Dermo technique outperformed SOTA algorithms when recognizing nine classes of PSLs. The Assist-Dermo system performed better than other competitive systems and can support dermatologists in the diagnosis of a wide variety of PSLs through dermoscopy. The Assist-Dermo model code is freely available on GitHub for the scientific community.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1871576588",
                        "name": "Q. Abbas"
                    },
                    {
                        "authorId": "2226571108",
                        "name": "Yassine Daadaa"
                    },
                    {
                        "authorId": "2151513092",
                        "name": "Umer Rashid"
                    },
                    {
                        "authorId": "144479632",
                        "name": "Mostafa E. A. Ibrahim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "aa46cf12837b42353592d0b8090dd478d2624ad3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-14021",
                    "ArXiv": "2307.14021",
                    "DOI": "10.48550/arXiv.2307.14021",
                    "CorpusId": 260164908
                },
                "corpusId": 260164908,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/aa46cf12837b42353592d0b8090dd478d2624ad3",
                "title": "Retinotopy Inspired Brain Encoding Model and the All-for-One Training Recipe",
                "abstract": "Brain encoding models aim to predict brain voxel-wise responses to stimuli images, replicating brain signals captured by neuroimaging techniques. There is a large volume of publicly available data, but training a comprehensive brain encoding model is challenging. The main difficulties stem from a) diversity within individual brain, with functional heterogeneous brain regions; b) diversity of brains from different subjects, due to genetic and developmental differences; c) diversity of imaging modalities and processing pipelines. We use this diversity to our advantage by introducing the All-for-One training recipe, which divides the challenging one-big-model problem into multiple small models, with the small models aggregating the knowledge while preserving the distinction between the different functional regions. Agnostic of the training recipe, we use biological knowledge of the brain, specifically retinotopy, to introduce inductive bias to learn a 3D brain-to-image mapping that ensures a) each neuron knows which image regions and semantic levels to gather information, and b) no neurons are left behind in the model. We pre-trained a brain encoding model using over one million data points from five public datasets spanning three imaging modalities. To the best of our knowledge, this is the most comprehensive brain encoding model to the date. We demonstrate the effectiveness of the pre-trained model as a drop-in replacement for commonly used vision backbone models. Furthermore, we demonstrate the application of the model to brain decoding. Code and the model checkpoint will be made available.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150129143",
                        "name": "Huzheng Yang"
                    },
                    {
                        "authorId": "2182163977",
                        "name": "Jianbo Shi"
                    },
                    {
                        "authorId": "2148511424",
                        "name": "James C. Gee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Compared with Adam, the newly developed Amsgrad [12] and Adabelief [13] use the exponential moving average calculation method when calculating the second-order moment."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "19c7012f2cd61bb6e126c37b4acd34a8127df7f2",
                "externalIds": {
                    "DOI": "10.23919/CCC58697.2023.10241241",
                    "CorpusId": 262076906
                },
                "corpusId": 262076906,
                "publicationVenue": {
                    "id": "23f8fe4c-6537-4027-a334-6a5863115984",
                    "name": "Cybersecurity and Cyberforensics Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Chin Control Conf",
                        "Computational Complexity Conference",
                        "CCC",
                        "Comput Complex Conf",
                        "Cybersecur Cyberforensics Conf",
                        "Conference on Computational Complexity",
                        "Computing Colombian Conference",
                        "Conf Comput Complex",
                        "Comput Colomb Conf",
                        "Chinese Control Conference"
                    ],
                    "url": "http://computationalcomplexity.org/"
                },
                "url": "https://www.semanticscholar.org/paper/19c7012f2cd61bb6e126c37b4acd34a8127df7f2",
                "title": "Training Neural Networks with Momental Bound of Learning Rate",
                "abstract": "Adaptive algorithms are widely used in deep learning because of their fast convergence. Among them, Adam is the most widely used algorithm. However, studies have shown that Adam's generalization ability is weak. AdaX is a variant of Adam, which modifies the second moment of Adam and has good generalization ability. We propose a new adaptive and momental bound algorithm, called AdaXod, which characterizes of exponentially averaging the learning rate and is particularly useful for training deep neural networks. By setting an adaptively limited learning rate in the AdaX algorithm, the resultant AdaXod can effectively eliminate the problem of excessive learning rate in the later stage of neural network model training and thus stabilize training. Simulation experiments verify that AdaXod eliminates large learning rates during neural network training and outperforms other optimizers, especially on the complex network structures such as DenseNet.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217932069",
                        "name": "Yuanxuan Liu"
                    },
                    {
                        "authorId": "2243599095",
                        "name": "Dequan Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "8ea5cdebdce6c6b6970d1b7a5102fa3e45b2f83a",
                "externalIds": {
                    "ArXiv": "2307.11108",
                    "DBLP": "journals/corr/abs-2307-11108",
                    "DOI": "10.48550/arXiv.2307.11108",
                    "CorpusId": 260091616
                },
                "corpusId": 260091616,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8ea5cdebdce6c6b6970d1b7a5102fa3e45b2f83a",
                "title": "Flatness-Aware Minimization for Domain Generalization",
                "abstract": "Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets. Additionally, we confirm that FAD is capable of discovering flatter optima in comparison to other zeroth-order and first-order flatness-aware optimization methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51258901",
                        "name": "Xingxuan Zhang"
                    },
                    {
                        "authorId": "150287491",
                        "name": "Renzhe Xu"
                    },
                    {
                        "authorId": "2187083103",
                        "name": "Han Yu"
                    },
                    {
                        "authorId": "2153514690",
                        "name": "Yancheng Dong"
                    },
                    {
                        "authorId": "2220457183",
                        "name": "Pengfei Tian"
                    },
                    {
                        "authorId": "2224467806",
                        "name": "Peng Cu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The internal potential is optimize via the training loop below where the Adabelief optimizer is utilized for its combination of adaptive learning and performance [10]:"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b2b2c8a9cd3a356d6310447910d372daecefca1b",
                "externalIds": {
                    "ArXiv": "2307.09311",
                    "DBLP": "journals/corr/abs-2307-09311",
                    "DOI": "10.48550/arXiv.2307.09311",
                    "CorpusId": 259950982
                },
                "corpusId": 259950982,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b2b2c8a9cd3a356d6310447910d372daecefca1b",
                "title": "Automatic Differentiation for Inverse Problems with Applications in Quantum Transport",
                "abstract": "A neural solver and differentiable simulation of the quantum transmitting boundary model is presented for the inverse quantum transport problem. The neural solver is used to engineer continuous transmission properties and the differentiable simulation is used to engineer current-voltage characteristics.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2052629523",
                        "name": "I. Williams"
                    },
                    {
                        "authorId": "3337342",
                        "name": "E. Polizzi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "086a432f844e96167dbd93edd2ad683295461181",
                "externalIds": {
                    "ArXiv": "2307.09638",
                    "DBLP": "journals/corr/abs-2307-09638",
                    "DOI": "10.48550/arXiv.2307.09638",
                    "CorpusId": 259982690
                },
                "corpusId": 259982690,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/086a432f844e96167dbd93edd2ad683295461181",
                "title": "Promoting Exploration in Memory-Augmented Adam using Critical Momenta",
                "abstract": "Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51437465",
                        "name": "Pranshu Malviya"
                    },
                    {
                        "authorId": "24039720",
                        "name": "Gon\u00e7alo Mordido"
                    },
                    {
                        "authorId": "14398916",
                        "name": "A. Baratin"
                    },
                    {
                        "authorId": "101340781",
                        "name": "Reza Babanezhad Harikandeh"
                    },
                    {
                        "authorId": "2223971135",
                        "name": "Jerry Huang"
                    },
                    {
                        "authorId": "1388317459",
                        "name": "S. Lacoste-Julien"
                    },
                    {
                        "authorId": "1996134",
                        "name": "Razvan Pascanu"
                    },
                    {
                        "authorId": "123607932",
                        "name": "Sarath Chandar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Lots of optimizers have been proposed with the goal of speeding up convergence [25, 113, 118]; yet, Schmidt et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0b0d22adc201913c7ff186504db129cc51d9971c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-06440",
                    "ArXiv": "2307.06440",
                    "DOI": "10.48550/arXiv.2307.06440",
                    "CorpusId": 259847436
                },
                "corpusId": 259847436,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0b0d22adc201913c7ff186504db129cc51d9971c",
                "title": "No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models",
                "abstract": "The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "66914903",
                        "name": "Jean Kaddour"
                    },
                    {
                        "authorId": "12389060",
                        "name": "Oscar Key"
                    },
                    {
                        "authorId": "40284207",
                        "name": "Piotr Nawrot"
                    },
                    {
                        "authorId": "3051815",
                        "name": "Pasquale Minervini"
                    },
                    {
                        "authorId": "1940272",
                        "name": "Matt J. Kusner"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[54] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James S.",
                "During the VQE stage of the training, the adabelief optimizer [54] is used to update the quantum circuit parameters, while Nesterov\u2019s accelerated gradient descent scheme [55] is performed for the Schmidts coefficient."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "bab88cf3d429f081fc698e01c91b0672deda1a92",
                "externalIds": {
                    "ArXiv": "2307.02633",
                    "DBLP": "journals/corr/abs-2307-02633",
                    "DOI": "10.48550/arXiv.2307.02633",
                    "CorpusId": 259360498
                },
                "corpusId": 259360498,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bab88cf3d429f081fc698e01c91b0672deda1a92",
                "title": "Hybrid Ground-State Quantum Algorithms based on Neural Schr\u00f6dinger Forging",
                "abstract": "Entanglement forging based variational algorithms leverage the bi-partition of quantum systems for addressing ground state problems. The primary limitation of these approaches lies in the exponential summation required over the numerous potential basis states, or bitstrings, when performing the Schmidt decomposition of the whole system. To overcome this challenge, we propose a new method for entanglement forging employing generative neural networks to identify the most pertinent bitstrings, eliminating the need for the exponential sum. Through empirical demonstrations on systems of increasing complexity, we show that the proposed algorithm achieves comparable or superior performance compared to the existing standard implementation of entanglement forging. Moreover, by controlling the amount of required resources, this scheme can be applied to larger, as well as non permutation invariant systems, where the latter constraint is associated with the Heisenberg forging procedure. We substantiate our findings through numerical simulations conducted on spins models exhibiting one-dimensional ring, two-dimensional triangular lattice topologies, and nuclear shell model configurations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2221227557",
                        "name": "Paulin de Schoulepnikoff"
                    },
                    {
                        "authorId": "1398322899",
                        "name": "O. Kiss"
                    },
                    {
                        "authorId": "3425469",
                        "name": "S. Vallecorsa"
                    },
                    {
                        "authorId": "50666189",
                        "name": "Giuseppe Carleo"
                    },
                    {
                        "authorId": "41132173",
                        "name": "M. Grossi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Robust training of large language models (LLMs) often relies on adaptive gradient-based optimization methods (Li et al., 2022; Kingma and Ba, 2015; Zhuang et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2c430566b9a12c66ed306a2d1e19d27a29d720aa",
                "externalIds": {
                    "ArXiv": "2307.02047",
                    "DBLP": "conf/acl/LuoRZJ0023",
                    "ACL": "2023.acl-long.243",
                    "DOI": "10.48550/arXiv.2307.02047",
                    "CorpusId": 259342823
                },
                "corpusId": 259342823,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/2c430566b9a12c66ed306a2d1e19d27a29d720aa",
                "title": "CAME: Confidence-guided Adaptive Memory Efficient Optimization",
                "abstract": "Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218050858",
                        "name": "Yang Luo"
                    },
                    {
                        "authorId": "153457264",
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "authorId": "2109654065",
                        "name": "Zangwei Zheng"
                    },
                    {
                        "authorId": "2222322049",
                        "name": "Zhuo Jiang"
                    },
                    {
                        "authorId": "145820291",
                        "name": "Xin Jiang"
                    },
                    {
                        "authorId": "2054451943",
                        "name": "Yang You"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Considering the curvature of the loss function, AdaBelief (Zhuang et al., 2020) and AdaMomentum (Wang et al.",
                "Image Classification Consistent with general optimizer researches (Zhuang et al., 2020), we conduct experiments on two image classification tasks, CIFAR-10 and CIFAR-100 (Krizhevsky et al.",
                ", 2018), given \u03b8i, E[\u03b4i|\u03b8i] = 0; On the other hand, suppose the optimizer runs for a long time, the bias of EMA is small (Zhuang et al., 2020), thus E(It) approaches E(gt) as step increases.",
                "Consistent with general optimizer researches (Zhuang et al., 2020), we conduct experiments on two image classification tasks, CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009) in CV field, and the results are presented in Table 1.",
                "Suppose the optimizer runs for a long time, the bias of EMA is small (Zhuang et al., 2020), thus E(It) approaches E(gt) as step increases.",
                "Considering the curvature of the loss function, AdaBelief (Zhuang et al., 2020) and AdaMomentum (Wang et al., 2021) are proposed.",
                "\u2026typical optimizers, including classic SGD (Robbins & Monro, 1951) and Adam (Kingma & Ba, 2014), our base, SGDM (Sutskever et al., 2013)1 and RAdam (Liu et al., 2019), the current state-of-the-art AdaBelief (Zhuang et al., 2020), and the optimizer combined of many modules, Ranger (Wright, 2019).",
                "The third attempt is modifying the process of optimizers with adaptive learning rate to achieve better local optimum, which is the most popular field in recent researches (Zhuang et al., 2020; Li et al., 2020a).",
                "\u2026for the follow reasons: on the one hand, gt = \u2207f(\u03b8t) + \u03b4t in which E[\u03b4t] = 0, so according to (Chen et al., 2018), given \u03b8i, E[\u03b4i|\u03b8i] = 0; On the other hand, suppose the optimizer runs for a long time, the bias of EMA is small (Zhuang et al., 2020), thus E(It) approaches E(gt) as step increases.",
                ", 2019), the current state-of-the-art AdaBelief (Zhuang et al., 2020), and the optimizer combined of many modules, Ranger (Wright, 2019).",
                "\u2026we then get\n\u2212 E [ t\u2211 i=1 \u03b1i\u3008\u2207f(\u03b8i), gi/ \u221a v\u0302i\u3009 ]\n\u22642H2E  t\u2211 i=2 d\u2211 j=1 \u2223\u2223\u2223(\u03b1i/(\u221av\u0302i)j \u2212 \u03b1i\u22121/(\u221av\u0302i\u22121)j)\u2223\u2223\u2223 + 2H2E  d\u2211 j=1 (\u03b11/ \u221a v\u03021)j  \u2212 E\n[ t\u2211 i=1 \u03b1i\u3008\u2207f(\u03b8i),\u2207f(\u03b8i)/ \u221a v\u0302i\u3009 ] (32)\nThen, consider the term with \u00b5. Suppose the optimizer runs for a long time, the bias of EMA is small (Zhuang et\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "969d9252d2e3d0c605bc1e83279edd198c27b80b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-00631",
                    "ArXiv": "2307.00631",
                    "DOI": "10.48550/arXiv.2307.00631",
                    "CorpusId": 259316718
                },
                "corpusId": 259316718,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/969d9252d2e3d0c605bc1e83279edd198c27b80b",
                "title": "Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non-adaptive Momentum Optimizers",
                "abstract": "Optimizer is an essential component for the success of deep learning, which guides the neural network to update the parameters according to the loss on the training set. SGD and Adam are two classical and effective optimizers on which researchers have proposed many variants, such as SGDM and RAdam. In this paper, we innovatively combine the backward-looking and forward-looking aspects of the optimizer algorithm and propose a novel \\textsc{Admeta} (\\textbf{A} \\textbf{D}ouble exponential \\textbf{M}oving averag\\textbf{E} \\textbf{T}o \\textbf{A}daptive and non-adaptive momentum) optimizer framework. For backward-looking part, we propose a DEMA variant scheme, which is motivated by a metric in the stock market, to replace the common exponential moving average scheme. While in the forward-looking part, we present a dynamic lookahead strategy which asymptotically approaches a set value, maintaining its speed at early stage and high convergence performance at final stage. Based on this idea, we provide two optimizer implementations, \\textsc{AdmetaR} and \\textsc{AdmetaS}, the former based on RAdam and the latter based on SGDM. Through extensive experiments on diverse tasks, we find that the proposed \\textsc{Admeta} optimizer outperforms our base optimizers and shows advantages over recently proposed competitive optimizers. We also provide theoretical proof of these two algorithms, which verifies the convergence of our proposed \\textsc{Admeta}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2194763439",
                        "name": "Yineng Chen"
                    },
                    {
                        "authorId": "30658665",
                        "name": "Z. Li"
                    },
                    {
                        "authorId": "2107901992",
                        "name": "Lefei Zhang"
                    },
                    {
                        "authorId": "145728792",
                        "name": "Bo Du"
                    },
                    {
                        "authorId": "2146232510",
                        "name": "Hai Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The novel variant [34], which adapts step sizes according to the belief in current gradients (AdaBelief), has a better convergence, generalization, and training stability in both convex and non-convex cases by modifying Adam without additional parameters."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cc7350eec6602a7e4746a46cd79d5fc8665d6bbd",
                "externalIds": {
                    "DBLP": "journals/sensors/GaoWZZZ23",
                    "PubMedCentral": "10347066",
                    "DOI": "10.3390/s23136034",
                    "CorpusId": 259686606,
                    "PubMed": "37447882"
                },
                "corpusId": 259686606,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cc7350eec6602a7e4746a46cd79d5fc8665d6bbd",
                "title": "FedADT: An Adaptive Method Based on Derivative Term for Federated Learning",
                "abstract": "Federated learning is served as a novel distributed training framework that enables multiple clients of the internet of things to collaboratively train a global model while the data remains local. However, the implement of federated learning faces many problems in practice, such as the large number of training for convergence due to the size of model and the lack of adaptivity by the stochastic gradient-based update at the client side. Meanwhile, it is sensitive to noise during the optimization process that can affect the performance of the final model. For these reasons, we propose Federated Adaptive learning based on Derivative Term, called FedADT in this paper, which incorporates adaptive step size and difference of gradient in the update of local model. To further reduce the influence of noise on the derivative term that is estimated by difference of gradient, we use moving average decay on the derivative term. Moreover, we analyze the convergence performance of the proposed algorithm for non-convex objective function, i.e., the convergence rate of 1/nT can be achieved by choosing appropriate hyper-parameters, where n is the number of clients and T is the number of iterations, respectively. Finally, various experiments for the image classification task are conducted by training widely used convolutional neural network on MNIST and Fashion MNIST datasets to verify the effectiveness of FedADT. In addition, the receiver operating characteristic curve is used to display the result of the proposed algorithm by predicting the categories of clothing on the Fashion MNIST dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2191085433",
                        "name": "Huimin Gao"
                    },
                    {
                        "authorId": "1701108",
                        "name": "Qingtao Wu"
                    },
                    {
                        "authorId": "51178617",
                        "name": "Xuhui Zhao"
                    },
                    {
                        "authorId": "144646645",
                        "name": "Junlong Zhu"
                    },
                    {
                        "authorId": "1680884",
                        "name": "Mingchuan Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use the default training hyperparameters of SGD, Adam, and AdamW in these settings (He et al., 2016; Zhuang et al., 2020; Chen et al., 2021), and set MSBPG \u2019s learning rate (initial stepsize) as 0.",
                "\u2026experimental settings for training neural networks, including reducing the stepsize to 0.1 times its original value near the end of training (Zhuang et al., 2020; Chen et al., 2021; Luo et al., 2019)\nand adopting a cosine annealing schedule for the stepsizes (Loshchilov and Hutter, 2016,\u2026",
                "Here we follow the commonly used experimental setting for training LSTMs (Zhuang et al., 2020; Chen et al., 2021), which reduces the stepsize to 0.",
                "Here we follow the commonly used experimental setting for training LSTMs (Zhuang et al., 2020; Chen et al., 2021), which reduces the stepsize to 0.1 times its original value two times (at 75th epoch and 150th epoch) during the training process.",
                "For training hyperparameters, we use the default settings for SGD, Adam, and AdamW in training 1-, 2-, 3-layer LSTMs (Zhuang et al., 2020; Chen et al., 2021).",
                "1 times its original value near the end of training (Zhuang et al., 2020; Chen et al., 2021; Luo et al., 2019)",
                "We use the default training hyperparameters of SGD, Adam, and AdamW in these settings (He et al., 2016; Zhuang et al., 2020; Chen et al., 2021), and set MSBPG \u2019s learning rate (initial stepsize) as 0.1, momentum coefficient \u03b2 as 0.9, weight decay coefficient \u03b3 as 1\u00d7 10\u22123."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "50f046d2eb1f0b237d9fe2ac473e1e8dedbd306e",
                "externalIds": {
                    "ArXiv": "2306.14522",
                    "DBLP": "journals/corr/abs-2306-14522",
                    "DOI": "10.48550/arXiv.2306.14522",
                    "CorpusId": 259252447
                },
                "corpusId": 259252447,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/50f046d2eb1f0b237d9fe2ac473e1e8dedbd306e",
                "title": "Nonconvex Stochastic Bregman Proximal Gradient Method with Application to Deep Learning",
                "abstract": "The widely used stochastic gradient methods for minimizing nonconvex composite objective functions require the Lipschitz smoothness of the differentiable part. But the requirement does not hold true for problem classes including quadratic inverse problems and training neural networks. To address this issue, we investigate a family of stochastic Bregman proximal gradient (SBPG) methods, which only require smooth adaptivity of the differentiable part. SBPG replaces the upper quadratic approximation used in SGD with the Bregman proximity measure, resulting in a better approximation model that captures the non-Lipschitz gradients of the nonconvex objective. We formulate the vanilla SBPG and establish its convergence properties under nonconvex setting without finite-sum structure. Experimental results on quadratic inverse problems testify the robustness of SBPG. Moreover, we propose a momentum-based version of SBPG (MSBPG) and prove it has improved convergence properties. We apply MSBPG to the training of deep neural networks with a polynomial kernel function, which ensures the smooth adaptivity of the loss function. Experimental results on representative benchmarks demonstrate the effectiveness and robustness of MSBPG in training neural networks. Since the additional computation cost of MSBPG compared with SGD is negligible in large-scale optimization, MSBPG can potentially be employed as an universal open-source optimizer in the future.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "11058112",
                        "name": "Kuan-Fu Ding"
                    },
                    {
                        "authorId": "2143426098",
                        "name": "Jingyang Li"
                    },
                    {
                        "authorId": "144163407",
                        "name": "K. Toh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For our optimizer we used AdaBelief (Zhuang et al., 2020), which is a version of Adam (Kingma and Ba, 2015) that instead of the accumulating squared gradients, accumulates the squared difference between the gradient and the momentum.",
                "In initial experiments, we found AdaBelief to increase stability."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "06cdec9e605ffc674138d60e4d7814dd4450b92e",
                "externalIds": {
                    "ArXiv": "2306.13421",
                    "DBLP": "journals/corr/abs-2306-13421",
                    "DOI": "10.48550/arXiv.2306.13421",
                    "CorpusId": 259243694
                },
                "corpusId": 259243694,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/06cdec9e605ffc674138d60e4d7814dd4450b92e",
                "title": "Long-range Language Modeling with Self-retrieval",
                "abstract": "Retrieval-augmented language models (LMs) have received much attention recently. However, typically the retriever is not trained jointly as a native component of the LM, but added to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another. In this work, we propose the Retrieval-Pretrained Transformer (RPT), an architecture and training procedure for jointly training a retrieval-augmented LM from scratch for the task of modeling long texts. Given a recently generated text chunk in a long document, the LM computes query representations, which are then used to retrieve earlier chunks in the document, located potentially tens of thousands of tokens before. Information from retrieved chunks is fused into the LM representations to predict the next target chunk. We train the retriever component with a semantic objective, where the goal is to retrieve chunks that increase the probability of the next chunk, according to a reference LM. We evaluate RPT on four long-range language modeling tasks, spanning books, code, and mathematical writing, and demonstrate that RPT improves retrieval quality and subsequently perplexity across the board compared to strong baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2001128224",
                        "name": "Ohad Rubin"
                    },
                    {
                        "authorId": "1750652",
                        "name": "Jonathan Berant"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2dffb04c5dfc23f12ab8e8d1d80244cc2ec86251",
                "externalIds": {
                    "PubMedCentral": "10287677",
                    "DOI": "10.1038/s41598-023-37389-2",
                    "CorpusId": 259233939,
                    "PubMed": "37349526"
                },
                "corpusId": 259233939,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2dffb04c5dfc23f12ab8e8d1d80244cc2ec86251",
                "title": "Deep-learning approach to detect childhood glaucoma based on periocular photograph",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "6883336",
                        "name": "Y. Kitaguchi"
                    },
                    {
                        "authorId": "2220453411",
                        "name": "Rina Hayakawa"
                    },
                    {
                        "authorId": "20507315",
                        "name": "Rumi Kawashima"
                    },
                    {
                        "authorId": "47368510",
                        "name": "K. Matsushita"
                    },
                    {
                        "authorId": "117157826",
                        "name": "Hisashi Tanaka"
                    },
                    {
                        "authorId": "1738501775",
                        "name": "R. Kawasaki"
                    },
                    {
                        "authorId": "2211506391",
                        "name": "Takahiro Fujino"
                    },
                    {
                        "authorId": "32650727",
                        "name": "S. Usui"
                    },
                    {
                        "authorId": "4923785",
                        "name": "Hiroshi Shimojyo"
                    },
                    {
                        "authorId": "52226515",
                        "name": "T. Okazaki"
                    },
                    {
                        "authorId": "32545393",
                        "name": "K. Nishida"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3e9d54f8217acafd65552489e90f3d5ac884dabf",
                "externalIds": {
                    "DOI": "10.1109/ICCNS58795.2023.10193585",
                    "CorpusId": 260386204
                },
                "corpusId": 260386204,
                "publicationVenue": {
                    "id": "3c4447e5-bf74-426d-ac93-650df74ea250",
                    "name": "International Conference on Communication and Network Security",
                    "type": "conference",
                    "alternate_names": [
                        "ICCNS",
                        "Int Conf Commun Netw Secur"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3e9d54f8217acafd65552489e90f3d5ac884dabf",
                "title": "A Comparison Study of Deep Learning Algorithms for Metasurface Harvester Designs",
                "abstract": "The paper compares three deep learning artificial intelligence algorithms used for metasurface design. In-house design code for designing metasurface structures was developed with Python, the NumPy library. To facilitate the study, the three algorithms used are AdaBelief, Adam, and Yogi. According to the numerical comparison study, Adam has a better performance in terms of model generalization with a large dataset (in our case 7000 samples), while Adabelief and Yogi show a better performance in terms of a low dataset (in our case 4,000 samples), and Yogi has a better performance with a lower dataset correlation between the predicted performance of the energy harvester obtained from three algorithms. Yogi and Adablief performance could be improved by manipulating the hyper-parameters.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226514404",
                        "name": "Haitham Al Ajmi"
                    },
                    {
                        "authorId": "1399715792",
                        "name": "M. Bait-Suwailam"
                    },
                    {
                        "authorId": "2120347",
                        "name": "L. Khriji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To optimize the parameters of the model, we use Adam [5] and AdaBelief [22] as optimizers for the Twitter and Weibo datasets."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7981c5820295b39c2f7a7610997b4ae8d52d0df4",
                "externalIds": {
                    "DBLP": "conf/ijcnn/YinSYZZS23",
                    "DOI": "10.1109/IJCNN54540.2023.10191341",
                    "CorpusId": 260387221
                },
                "corpusId": 260387221,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/7981c5820295b39c2f7a7610997b4ae8d52d0df4",
                "title": "Research on fake news detection method based on multi-level semantic enhancement",
                "abstract": "Most of the existing fake news detection methods focus on the feature information at the news performance level, and pay insufficient attention to the semantic information of the news content, which cannot fully obtain the semantic information in the news. Therefore, this paper proposes a multi-level semantic enhanced fake news detection framework (MLSED), which imitates the way modern people read news. Aiming at the two main aspects of semantic information including entity objects and event topics, the core semantic information in news content is gradually obtained by mutual enhancement to detect fake news. Extensive experiments on two real datasets show that MLSED can fully capture the semantic information of news for fake news detection, and outperforms state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2087076158",
                        "name": "Xinyan Yin"
                    },
                    {
                        "authorId": "2152305686",
                        "name": "Tao Sun"
                    },
                    {
                        "authorId": "2226689867",
                        "name": "Chunyan Yang"
                    },
                    {
                        "authorId": "2188413693",
                        "name": "Zihao Zhang"
                    },
                    {
                        "authorId": "48505793",
                        "name": "Xiang Zhang"
                    },
                    {
                        "authorId": "2181226057",
                        "name": "Mengli Su"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3bc15624c2d4447eb29420f045817adf79047720",
                "externalIds": {
                    "DOI": "10.1109/icassp49357.2023.10096624",
                    "CorpusId": 258538854
                },
                "corpusId": 258538854,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/3bc15624c2d4447eb29420f045817adf79047720",
                "title": "Improving the Stochastic Gradient Descent\u2019s Test Accuracy by Manipulating the \u2113\u221e Norm of its Gradient Approximation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2067971838",
                        "name": "Paul Rodr\u00edguez"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f45564b711f1cce8d836d5945082a559a6cda38e",
                "externalIds": {
                    "DOI": "10.1109/icassp49357.2023.10096892",
                    "CorpusId": 258545796
                },
                "corpusId": 258545796,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/f45564b711f1cce8d836d5945082a559a6cda38e",
                "title": "Measuring the Transferability of \u2113\u221e Attacks by the \u21132 Norm",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150293336",
                        "name": "Sizhe Chen"
                    },
                    {
                        "authorId": "2035647651",
                        "name": "Qinghua Tao"
                    },
                    {
                        "authorId": "17064832",
                        "name": "Zhixing Ye"
                    },
                    {
                        "authorId": "144335593",
                        "name": "X. Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bd3d6463ed1f8397f343e5271ce40a5f5a6f0f70",
                "externalIds": {
                    "PubMedCentral": "10290554",
                    "DOI": "10.1093/bioinformatics/btad378",
                    "CorpusId": 259126733,
                    "PubMed": "37294804"
                },
                "corpusId": 259126733,
                "publicationVenue": {
                    "id": "15d4205f-903b-403c-9a2e-906f02ce04d8",
                    "name": "Bioinformatics",
                    "type": "journal",
                    "alternate_names": [
                        "Int Conf Bioinform",
                        "International Conference on Bioinformatics",
                        "BIOINFORMATICS"
                    ],
                    "issn": "1367-4803",
                    "alternate_issns": [
                        "1367-4811"
                    ],
                    "url": "http://bioinformatics.oxfordjournals.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bd3d6463ed1f8397f343e5271ce40a5f5a6f0f70",
                "title": "Finding motifs using DNA images derived from sparse representations",
                "abstract": "Abstract Motivation Motifs play a crucial role in computational biology, as they provide valuable information about the binding specificity of proteins. However, conventional motif discovery methods typically rely on simple combinatoric or probabilistic approaches, which can be biased by heuristics such as substring-masking for multiple motif discovery. In recent years, deep neural networks have become increasingly popular for motif discovery, as they are capable of capturing complex patterns in data. Nonetheless, inferring motifs from neural networks remains a challenging problem, both from a modeling and computational standpoint, despite the success of these networks in supervised learning tasks. Results We present a principled representation learning approach based on a hierarchical sparse representation for motif discovery. Our method effectively discovers gapped, long, or overlapping motifs that we show to commonly exist in next-generation sequencing datasets, in addition to the short and enriched primary binding sites. Our model is fully interpretable, fast, and capable of capturing motifs in a large number of DNA strings. A key concept emerged from our approach\u2014enumerating at the image level\u2014effectively overcomes the k-mers paradigm, enabling modest computational resources for capturing the long and varied but conserved patterns, in addition to capturing the primary binding sites. Availability and implementation Our method is available as a Julia package under the MIT license at https://github.com/kchu25/MOTIFs.jl, and the results on experimental data can be found at https://zenodo.org/record/7783033.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2190181276",
                        "name": "Shane Chu"
                    },
                    {
                        "authorId": "2549187",
                        "name": "G. Stormo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "88897232892c588921559d42edaf5faa2cadae40",
                "externalIds": {
                    "DBLP": "conf/cvpr/YongSZ23",
                    "DOI": "10.1109/CVPR52729.2023.00760",
                    "CorpusId": 260068403
                },
                "corpusId": 260068403,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/88897232892c588921559d42edaf5faa2cadae40",
                "title": "A General Regret Bound of Preconditioned Gradient Method for DNN Training",
                "abstract": "While adaptive learning rate methods, such as Adam, have achieved remarkable improvement in optimizing Deep Neural Networks (DNNs), they consider only the diagonal elements of the full preconditioned matrix. Though the full-matrix preconditioned gradient methods theoretically have a lower regret bound, they are impractical for use to train DNNs because of the high complexity. In this paper, we present a general regret bound with a constrained full-matrix preconditioned gradient, and show that the updating formula of the preconditioner can be derived by solving a cone-constrained optimization problem. With the block-diagonal and Kronecker-factorized constraints, a specific guide function can be obtained. By minimizing the upper bound of the guide function, we develop a new DNN optimizer, termed AdaBK. A series of techniques, including statistics updating, dampening, efficient matrix inverse root computation, and gradient amplitude preservation, are developed to make AdaBK effective and efficient to implement. The proposed AdaBK can be readily embedded into many existing DNN optimizers, e.g., SGDM and Adam W, and the corresponding SGDM _BK and Adam W _BK algorithms demonstrate significant improvements over existing DNN optimizers on benchmark vision tasks, including image classification, object detection and segmentation. The code is publicly available at https://github.com/Yonghongwei/AdaBK.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "7906116",
                        "name": "Hongwei Yong"
                    },
                    {
                        "authorId": "48186551",
                        "name": "Ying Sun"
                    },
                    {
                        "authorId": "2152837131",
                        "name": "Lei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "+ apply sharpening technique [2] for noise modeling + data augmentation to prevent overfitting + create Pseudo MA images to balance sample distribution + utilize coarse-to-fine (CTF) generator [3] to keep fidelity + negative learning to maximize dissimilarity with noncorresponding patches [4] + use adabelief optimizer [5] to make training more stable"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "96f632009e2d213a0eec06b56208aa4b2bf87584",
                "externalIds": {
                    "DOI": "10.1109/CAI54212.2023.00152",
                    "CorpusId": 260388377
                },
                "corpusId": 260388377,
                "publicationVenue": {
                    "id": "890c63fa-e5f7-4a31-b830-c416160ec6f7",
                    "name": "Conference on Algebraic Informatics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Algebraic Informatics",
                        "CAI"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=343"
                },
                "url": "https://www.semanticscholar.org/paper/96f632009e2d213a0eec06b56208aa4b2bf87584",
                "title": "Improved Contrastive Unpaired Translation for Metal Artifacts Reduction in Nasopharyngeal CT Images",
                "abstract": "Metal artifacts (MA) reduction is crucial for clinical application yet often lacks paired training data. Learning MA reduction from unpaired data and enforcing fidelity seems a trade-off. The study proposed an improved contrastive unpaired translation solution to address the issues and demonstrate its efficacy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216383608",
                        "name": "Yu-Hsing Hsieh"
                    },
                    {
                        "authorId": "2125430066",
                        "name": "Jia-Da Li"
                    },
                    {
                        "authorId": "2109318534",
                        "name": "Yao Lee"
                    },
                    {
                        "authorId": "2109414233",
                        "name": "Chu-Song Chen"
                    },
                    {
                        "authorId": "2218493770",
                        "name": "LiFu Wu"
                    },
                    {
                        "authorId": "6150227",
                        "name": "S. H. Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Currently, first-order gradient methods, such as SGD with momentum [2] and adaptive methods [3], [26], [27] are the most widely used deep learning optimization methods.",
                "[27] proposed another adaptive gradient method called AdaBelief, which adapts the stepsize according to the \u201cbelief\u201d in the current gradient direction."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2b33b2ee42f0fdd4448665a069b5ad7e7f562b2a",
                "externalIds": {
                    "ArXiv": "2305.18240",
                    "DBLP": "journals/corr/abs-2305-18240",
                    "DOI": "10.48550/arXiv.2305.18240",
                    "CorpusId": 258960587
                },
                "corpusId": 258960587,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2b33b2ee42f0fdd4448665a069b5ad7e7f562b2a",
                "title": "XGrad: Boosting Gradient-Based Optimizers With Weight Prediction",
                "abstract": "In this paper, we propose a general deep learning training framework XGrad which introduces weight prediction into the popular gradient-based optimizers to boost their convergence and generalization when training the deep neural network (DNN) models. In particular, ahead of each mini-batch training, the future weights are predicted according to the update rule of the used optimizer and are then applied to both the forward pass and backward propagation. In this way, during the whole training period, the optimizer always utilizes the gradients w.r.t. the future weights to update the DNN parameters, making the gradient-based optimizer achieve better convergence and generalization compared to the original optimizer without weight prediction. XGrad is rather straightforward to implement yet pretty effective in boosting the convergence of gradient-based optimizers and the accuracy of DNN models. Empirical results concerning the most three popular gradient-based optimizers including SGD with momentum, Adam, and AdamW demonstrate the effectiveness of our proposal. The experimental results validate that XGrad can attain higher model accuracy than the original optimizers when training the DNN models. The code of XGrad will be available at: https://github.com/guanleics/XGrad.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2059059973",
                        "name": "Lei Guan"
                    },
                    {
                        "authorId": "2108481496",
                        "name": "Dongsheng Li"
                    },
                    {
                        "authorId": "2218505964",
                        "name": "Jian Meng"
                    },
                    {
                        "authorId": "2218437825",
                        "name": "Yanqi Shi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c420fc0beacc9b3e92bcfe4d1de0165ecd9ca143",
                "externalIds": {
                    "ArXiv": "2305.15997",
                    "DBLP": "journals/corr/abs-2305-15997",
                    "DOI": "10.48550/arXiv.2305.15997",
                    "CorpusId": 258887990
                },
                "corpusId": 258887990,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c420fc0beacc9b3e92bcfe4d1de0165ecd9ca143",
                "title": "SING: A Plug-and-Play DNN Learning Technique",
                "abstract": "We propose SING (StabIlized and Normalized Gradient), a plug-and-play technique that improves the stability and generalization of the Adam(W) optimizer. SING is straightforward to implement and has minimal computational overhead, requiring only a layer-wise standardization of the gradients fed to Adam(W) without introducing additional hyper-parameters. We support the effectiveness and practicality of the proposed approach by showing improved results on a wide range of architectures, problems (such as image classification, depth estimation, and natural language processing), and in combination with other optimizers. We provide a theoretical analysis of the convergence of the method, and we show that by virtue of the standardization, SING can escape local minima narrower than a threshold that is inversely proportional to the network's depth.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2163450787",
                        "name": "Adrien Courtois"
                    },
                    {
                        "authorId": "7665349",
                        "name": "Damien Scieur"
                    },
                    {
                        "authorId": "27053481",
                        "name": "J. Morel"
                    },
                    {
                        "authorId": "153005108",
                        "name": "P. Arias"
                    },
                    {
                        "authorId": "104538018",
                        "name": "Thomas Eboli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(1)\nA common simplification of Adam that is more amenable to analysis (Balles & Hennig, 2018; Bernstein et al., 2018; Zhuang et al., 2020; Kunstner et al., 2023) is SignGD, which dates back to RProp (Braun & Riedmiller, 1992) that motivated RMSProp (Hinton et al., 2012) and Adam.",
                "Many follow-up works proposed variants of Adam (Dozat, 2016; Shazeer & Stern, 2018; Reddi et al., 2019; Loshchilov & Hutter, 2017; Zhuang et al., 2020; You et al., 2019).",
                "(1) A common simplification of Adam that is more amenable to analysis (Balles & Hennig, 2018; Bernstein et al., 2018; Zhuang et al., 2020; Kunstner et al., 2023) is SignGD, which dates back to RProp (Braun & Riedmiller, 1992) that motivated RMSProp (Hinton et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "33b68ea3ce551e33d634660f20ce43fbff0b5fca",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-14342",
                    "ArXiv": "2305.14342",
                    "DOI": "10.48550/arXiv.2305.14342",
                    "CorpusId": 258841030
                },
                "corpusId": 258841030,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/33b68ea3ce551e33d634660f20ce43fbff0b5fca",
                "title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training",
                "abstract": "Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT-2 models of sizes ranging from 125M to 770M, Sophia achieves a 2x speed-up compared with Adam in the number of steps, total compute, and wall-clock time. Theoretically, we show that Sophia adapts to the curvature in different components of the parameters, which can be highly heterogeneous for language modeling tasks. Our run-time bound does not depend on the condition number of the loss.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118903632",
                        "name": "Hong Liu"
                    },
                    {
                        "authorId": "46947755",
                        "name": "Zhiyuan Li"
                    },
                    {
                        "authorId": "145385471",
                        "name": "David Leo Wright Hall"
                    },
                    {
                        "authorId": "145419642",
                        "name": "Percy Liang"
                    },
                    {
                        "authorId": "2114186424",
                        "name": "Tengyu Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We experimented with SGD, Adam [68], AdaBelief [69], and AdamW [70] optimizers, and found the results of AdamW [70] better than others.",
                "We experimented with SGD, Adam [47], AdaBelief [48], and AdamW [49] optimizers, and found the results of AdamW [48] better than others."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "70d59e24719a3e29f94916476d005dc59167a1a4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-08396",
                    "ArXiv": "2305.08396",
                    "DOI": "10.48550/arXiv.2305.08396",
                    "CorpusId": 258686203
                },
                "corpusId": 258686203,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/70d59e24719a3e29f94916476d005dc59167a1a4",
                "title": "MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation",
                "abstract": "In this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision transformer (CNN-Transformer) for medical image segmentation. The proposed Hybrid Decoder, based on MaxViT-block, is designed to harness the power of both the convolution and self-attention mechanisms at each decoding stage with a nominal memory and computational burden. The inclusion of multi-axis self-attention, within each decoder stage, significantly enhances the discriminating capacity between the object and background regions, thereby helping in improving the segmentation efficiency. In the Hybrid Decoder block, the fusion process commences by integrating the upsampled lower-level decoder features, obtained through transpose convolution, with the skip-connection features derived from the hybrid encoder. Subsequently, the fused features undergo refinement through the utilization of a multi-axis attention mechanism. The proposed decoder block is repeated multiple times to progressively segment the nuclei regions. Experimental results on MoNuSeg18 and MoNuSAC20 dataset demonstrates the effectiveness of the proposed technique. Our MaxViT-UNet outperformed the previous CNN-based (UNet) and Transformer-based (Swin-UNet) techniques by a considerable margin on both of the standard datasets. The following github (https://github.com/PRLAB21/MaxViT-UNet) contains the implementation and trained weights.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149888758",
                        "name": "Abdul Rehman Khan"
                    },
                    {
                        "authorId": "2108486220",
                        "name": "Asifullah Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We can further apply AdaBelief method [24] to improve the transferability of protected samples by gradually reducing the learning rate, which we leave for future work."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c1d24ab606d6487e6daeed5c9e85dc650804754f",
                "externalIds": {
                    "ArXiv": "2305.05736",
                    "DBLP": "conf/wisec/WangGWCY23",
                    "DOI": "10.1145/3558482.3590189",
                    "CorpusId": 258587820
                },
                "corpusId": 258587820,
                "publicationVenue": {
                    "id": "d124b0ab-188e-40bf-8189-10f73a25bc83",
                    "name": "Wireless Network Security",
                    "type": "conference",
                    "alternate_names": [
                        "Wirel Netw Secur",
                        "WISEC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3090"
                },
                "url": "https://www.semanticscholar.org/paper/c1d24ab606d6487e6daeed5c9e85dc650804754f",
                "title": "VSMask: Defending Against Voice Synthesis Attack via Real-Time Predictive Perturbation",
                "abstract": "Deep learning based voice synthesis technology generates artificial human-like speeches, which has been used in deepfakes or identity theft attacks. Existing defense mechanisms inject subtle adversarial perturbations into the raw speech audios to mislead the voice synthesis models. However, optimizing the adversarial perturbation not only consumes substantial computation time, but it also requires the availability of entire speech. Therefore, they are not suitable for protecting live speech streams, such as voice messages or online meetings. In this paper, we propose VSMask, a real-time protection mechanism against voice synthesis attacks. Different from offline protection schemes, VSMask leverages a predictive neural network to forecast the most effective perturbation for the upcoming streaming speech. VSMask introduces a universal perturbation tailored for arbitrary speech input to shield a real-time speech in its entirety. To minimize the audio distortion within the protected speech, we implement a weight-based perturbation constraint to reduce the perceptibility of the added perturbation. We comprehensively evaluate VSMask protection performance under different scenarios. The experimental results indicate that VSMask can effectively defend against 3 popular voice synthesis models. None of the synthetic voice could deceive the speaker verification models or human ears with VSMask protection. In a physical world experiment, we demonstrate that VSMask successfully safeguards the real-time speech by injecting the perturbation over the air.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146016665",
                        "name": "Yuanda Wang"
                    },
                    {
                        "authorId": "7014630",
                        "name": "Hanqing Guo"
                    },
                    {
                        "authorId": "2152584697",
                        "name": "Guangjing Wang"
                    },
                    {
                        "authorId": "2152689250",
                        "name": "Bocheng Chen"
                    },
                    {
                        "authorId": "2480351",
                        "name": "Qiben Yan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is noted that the Amsgrad algorithm corrects the convergence error, while Adabelief adjusts the step size based on the \u201cbelief\u201d of the current gradient, where the \u201cbelief\u201d is the deviation between the observed and predicted values of the gradient [27].",
                "Compared with Adam, the newly developed Amsgrad [26] and Adabelief [27] use the exponential moving average calculation method when calculating the second moment.",
                "Adabelief updates the\n17696 Y.\u00a0Liu, D.\u00a0Li\n1 3\nstep size according to the \u201cbelief\u201d in the current gradient direction."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d2730d165446b915b723c5b290090a98827c8f2f",
                "externalIds": {
                    "DBLP": "journals/tjs/LiuL23",
                    "DOI": "10.1007/s11227-023-05338-5",
                    "CorpusId": 258635664
                },
                "corpusId": 258635664,
                "publicationVenue": {
                    "id": "26ed29a9-64ce-4d6c-9024-8b022fd2fe22",
                    "name": "Journal of Supercomputing",
                    "type": "journal",
                    "alternate_names": [
                        "The Journal of Supercomputing",
                        "J Supercomput"
                    ],
                    "issn": "0920-8542",
                    "url": "http://www.springer.com/computer/programming/journal/11227",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11227",
                        "https://www.springer.com/computer/swe/journal/11227?changeHeader"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d2730d165446b915b723c5b290090a98827c8f2f",
                "title": "AdaXod: a new adaptive and momental bound algorithm for training deep neural networks",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217932069",
                        "name": "Yuanxuan Liu"
                    },
                    {
                        "authorId": "49620653",
                        "name": "Dequan Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Motivated by Adam, a number of efficient Adam-family methods are developed, such as AdaBelief (Zhuang et al., 2020), AMSGrad (Reddi et al.",
                "In this section, we establish the convergence properties of ADAM, AMSGrad, Yogi and AdaBelief for solving UNP based on our proposed framework when the objective function f takes the following finite-sum formulation,\nf (x) := 1 N\nN\n\u2211 i=1 fi(x).",
                "As demonstrated in Section 4, this condition can be satisfied by numerous popular Adam-family methods, including Adam, AdaBelief, AMSGrad, NAdam, and Yogi.",
                "Motivated by Adam, a number of efficient Adam-family methods are developed, such as AdaBelief (Zhuang et al., 2020), AMSGrad (Reddi et al., 2018), NAdam (Dozat), Yogi (Zaheer et al., 2018), etc.",
                "\u2022 AdaBelief-C: vk+1 = (1\u2212 \u03c42\u03b7k)vk + \u03c42\u03b7k|g\u0302k \u2212mk+1|;\n\u2022 AMSGrad-C: vk+1 = max{vk, |g\u0302k|};\n\u2022 Yogi-C: vk+1 = vk \u2212 \u03c42\u03b7ksign(vk \u2212 |g\u0302k",
                "Moreover, we demonstrate that our proposed framework can be employed to analyze the convergence properties for a class of Adam-family methods with diminishing stepsize, including Adam, AdaBelief, AMSGrad, NAdam, and Yogi.",
                "Assumption 3.2(3) enforces regularity conditions on the set-valued mapping U , which are satisfied in a wide range of adaptive stochastic gradient methods such as Adam, AdaBelief, AMSGrad, NAdam, Yogi, as discussed later in Section 4.",
                "AdaBelief: For any k \u2265 0, it holds that\n\u2016vk+1\u2016 \u2264 (1\u2212 \u03c42\u03b7k) \u2016vk\u2016+ \u03c42\u03b7k \u2225\u2225(gk \u2212mk+1)2\u2225\u2225 \u2264 max { \u2016v0\u2016 , sup\nk\u22650\n\u2225\u2225(gk \u2212mk+1)2\u2225\u2225 } .",
                "Table 3 summarizes the updating rules for Adam, AdaBelief, AMSGrad, NAdam and Yogi, their corresponding set-valued mappings U in the framework (AFM), and the settings for the parameters \u03b1 and \u03ba.",
                "\u2022 Convergence properties for Adam-family methods We show that Adam, AdaBelief, AMSGrad, NAdam and Yogi, when equipped with diminishing stepsizes, follow our proposed framework (AFM)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d7b114bff7cd490531ab19059711d02b0943b931",
                "externalIds": {
                    "ArXiv": "2305.03938",
                    "DBLP": "journals/corr/abs-2305-03938",
                    "DOI": "10.48550/arXiv.2305.03938",
                    "CorpusId": 258557177
                },
                "corpusId": 258557177,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d7b114bff7cd490531ab19059711d02b0943b931",
                "title": "Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees",
                "abstract": "In this paper, we present a comprehensive study on the convergence properties of Adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. We introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. Our proposed framework encompasses various popular Adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. Furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. Through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. Extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2041943093",
                        "name": "Nachuan Xiao"
                    },
                    {
                        "authorId": "1720734790",
                        "name": "Xiaoyin Hu"
                    },
                    {
                        "authorId": "89121677",
                        "name": "Xin Liu"
                    },
                    {
                        "authorId": "144163407",
                        "name": "K. Toh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2013 Number of batch \u2013 Number of epoch \u2013 Loss ratio \u2013 Learning rate, epsilon, weight decay (Adabelief)\nThe performance of machine learning depends on the optimization of the hyperparameters.",
                "The CrossEntropyLoss function was adopted to obtain Lossnode and Lossedge herein, and the variables were optimized by Adabelief [27].",
                "The variables were optimized by Adabelief."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "97fe9b7723c5a2380951f8d254524ba28c30cb4c",
                "externalIds": {
                    "DOI": "10.1140/epja/s10050-023-01016-5",
                    "CorpusId": 258620472
                },
                "corpusId": 258620472,
                "publicationVenue": {
                    "id": "d55c3048-0f69-4644-a8d6-9407ea7da10c",
                    "name": "European Physical Journal A",
                    "type": "journal",
                    "alternate_names": [
                        "Eur Phys J",
                        "European Physical Journal",
                        "Eur Phys J A"
                    ],
                    "issn": "1434-6001",
                    "url": "http://www.springer.com/10050",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10050",
                        "http://www.epj.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/97fe9b7723c5a2380951f8d254524ba28c30cb4c",
                "title": "Development of machine learning analyses with graph neural network for the WASA-FRS experiment",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152188850",
                        "name": "H. Ekawa"
                    },
                    {
                        "authorId": "2134826741",
                        "name": "W. Dou"
                    },
                    {
                        "authorId": "46193715",
                        "name": "Y. Gao"
                    },
                    {
                        "authorId": "2157410718",
                        "name": "Y. He"
                    },
                    {
                        "authorId": "1388278517",
                        "name": "A. Kasagi"
                    },
                    {
                        "authorId": "2140614536",
                        "name": "E. Liu"
                    },
                    {
                        "authorId": "2134039243",
                        "name": "A. Muneem"
                    },
                    {
                        "authorId": "92931884",
                        "name": "M. Nakagawa"
                    },
                    {
                        "authorId": "104234567",
                        "name": "C. Rappold"
                    },
                    {
                        "authorId": "1730377",
                        "name": "N. Saito"
                    },
                    {
                        "authorId": "2148360538",
                        "name": "T. Saito"
                    },
                    {
                        "authorId": "81406335",
                        "name": "M. Taki"
                    },
                    {
                        "authorId": "2228422167",
                        "name": "Y. K. Tanaka"
                    },
                    {
                        "authorId": "2016994840",
                        "name": "H. Wang"
                    },
                    {
                        "authorId": "84671812",
                        "name": "J. Yoshida"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Optimizer: AdaBelief [43] with learning rate 5 \u00b7 10\u22124, betas (0.9, 0.999), eps 10\u221216, using weight decoupling without rectifying, to have both fast convergence and generalization.",
                "Optimizer: AdaBelief [43] with learning rate 5 \u00b7 10\u22124, betas (0."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "fdbccca0beb30bc3e580f48eb5bb84c343c18d5b",
                "externalIds": {
                    "PubMedCentral": "10215335",
                    "DOI": "10.3390/bioengineering10050555",
                    "CorpusId": 258530415,
                    "PubMed": "37237625"
                },
                "corpusId": 258530415,
                "publicationVenue": {
                    "id": "103075b0-1b66-4b69-9c47-f54875634fba",
                    "name": "Bioengineering",
                    "type": "journal",
                    "issn": "2306-5354",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-354376",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-354376",
                        "https://www.mdpi.com/journal/bioengineering"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fdbccca0beb30bc3e580f48eb5bb84c343c18d5b",
                "title": "Efficient Lung Ultrasound Classification",
                "abstract": "A machine learning method for classifying lung ultrasound is proposed here to provide a point of care tool for supporting a safe, fast, and accurate diagnosis that can also be useful during a pandemic such as SARS-CoV-2. Given the advantages (e.g., safety, speed, portability, cost-effectiveness) provided by the ultrasound technology over other examinations (e.g., X-ray, computer tomography, magnetic resonance imaging), our method was validated on the largest public lung ultrasound dataset. Focusing on both accuracy and efficiency, our solution is based on an efficient adaptive ensembling of two EfficientNet-b0 models reaching 100% of accuracy, which, to our knowledge, outperforms the previous state-of-the-art models by at least 5%. The complexity is restrained by adopting specific design choices: ensembling with an adaptive combination layer, ensembling performed on the deep features, and minimal ensemble using two weak models only. In this way, the number of parameters has the same order of magnitude of a single EfficientNet-b0 and the computational cost (FLOPs) is reduced at least by 20%, doubled by parallelization. Moreover, a visual analysis of the saliency maps on sample images of all the classes of the dataset reveals where an inaccurate weak model focuses its attention versus an accurate one.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "49074846",
                        "name": "A. Bruno"
                    },
                    {
                        "authorId": "2214149905",
                        "name": "Giacomo Ignesti"
                    },
                    {
                        "authorId": "1974713",
                        "name": "O. Salvetti"
                    },
                    {
                        "authorId": "2539268",
                        "name": "D. Moroni"
                    },
                    {
                        "authorId": "46767485",
                        "name": "M. Martinelli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The first model, like ours, is trained on the MNIST dataset [13], while the other two models are trained on the CIFAR-10 dataset [30].",
                "The modified algorithm, HN_Adam, is tested by using it to train a deep convolutional neural network using two different datasets CIFAR-10 [30] and MNIST [13].",
                "7 Architecture of the deep CNN model using the CIFAR-10 dataset\nminimum loss function during training process, the accuracy of the testing on test dataset are calculated and listed in Table 5.",
                "The results of the compared algorithms are taken the same as in [30, 66].",
                "We use HN_Adam, AdaBelief [30], Adam [8], SGD [33], Yogi [38], RAdam [40] and MSVAG [39] as learning algorithms during the training process of the ResNet18 deep network model.",
                "%) all accuracy for 150 epochs\nAlexNet- ResNet20 (2020)\n[37]\nMNIST\nCIFAR-10\nEVGO For MNIST(Val = 98.06%- Test = 98.12",
                "The HN Adam algorithm is compared to the basic Adam algorithm and the SGD algorithm, as well as five other SGD adaptive algorithms: AdaBeilf [30], Adam, RMSprop, AMSGrad, and Adagrad.",
                "ResNet18, PreActResNet18\n(2019) [35]\nCIFAR-10 AMSGrad and\nAdamX\n\u2013\n\u2013\nCNN1, CNN2 (2019) [61] MNIST HuperAdam Accuracy = 98.63%\n99.78% After 1000 steps\n(ResNet20, ResNet32)\n(2020) [62]\nCIFAR-10 SGD\nAdam\nAdamW\nAdaHessian\nAccuracy = (92.08\u201393.14",
                "This results in a large number of iterations and increases the risk of becoming trapped in local\nTable 1 Performances results for some selected optimization algorithms\nDNN Models Datasets Algorithm Performance\nCNN (2015) [8] MNIST Adam,\nAdaMax\nLoss = 0.26\nWRN-22, WRN-28 (2018)\n[13]\nCIFAR-10-\nCIFAR-100\nND-Adam Loss = (3.70\u201319.30) (3.70\u201318.42)\nDeep4Net ResNet (2019)\n[42]\nCIFAR-10 (AdamW)-\n(SGDW)\nAccuracy = (73.68",
                "(B) Experimental setup\nThe CIFAR-10 dataset is used to train a deep CNN model with a total of 955,512 parameters.",
                "VGG11, ResNet18,\nDenseNet121 (2020) [60]\nCIFAR-10\nCIFAR-100\nEAdam\nAdam\nRAdam\nAdabelief\nAccuracy = (91.45%\u201394.99%- 95.61",
                "BPNN (2023) [50] MNIST\nCIFAR-10\nACGB-Adam For MNIST (Loss(MSE) = 0.253- Accuracy = 95.9",
                "The CIFAR-10 dataset [30] is used to train the CNN model.",
                "The model is trained using the optimization algorithms HN_Adam, AdaBelief, Adam, AMSGrad, SGD, RMSProp, and AdaGrad individually.",
                "The results of our proposed HN_Adam algorithm are obtained considering the parameter settings for Mini-batch size, learning rate (g), b1, b2, and e to be the same as in [30]."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c03965da9f5c382097d01f589ac346fb712525df",
                "externalIds": {
                    "DBLP": "journals/nca/ReyadSA23",
                    "DOI": "10.1007/s00521-023-08568-z",
                    "CorpusId": 258349118
                },
                "corpusId": 258349118,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c03965da9f5c382097d01f589ac346fb712525df",
                "title": "A modified Adam algorithm for deep neural network optimization",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9198388",
                        "name": "M. Reyad"
                    },
                    {
                        "authorId": "1733046",
                        "name": "A. Sarhan"
                    },
                    {
                        "authorId": "1948978",
                        "name": "M. Arafa"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Adabelief [47], another popular optimizer uses the square of the difference between the current gradient and the current exponential moving average to"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f39d7f9ede7530808e3d229ed55e4cb6b7de300e",
                "externalIds": {
                    "DBLP": "conf/ijcnn/MishraK23",
                    "ArXiv": "2304.10457",
                    "DOI": "10.1109/IJCNN54540.2023.10191702",
                    "CorpusId": 258236204
                },
                "corpusId": 258236204,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/f39d7f9ede7530808e3d229ed55e4cb6b7de300e",
                "title": "Angle based dynamic learning rate for gradient descent",
                "abstract": "In our work, we propose a novel yet simple approach to obtain an adaptive learning rate for gradient-based descent methods on classification tasks. Instead of the traditional approach of selecting adaptive learning rates via the decayed expectation of gradient-based terms, we use the angle between the current gradient and the new gradient: this new gradient is computed from the direction orthogonal to the current gradient, which further helps us in determining a better adaptive learning rate based on angle history, thereby, leading to relatively better accuracy compared to the existing state-of-the-art optimizers. On a wide variety of benchmark datasets with prominent image classification architectures such as ResNet, DenseNet, Efficient-Net, and VGG, we find that our method leads to the highest accuracy in most of the datasets. Moreover, we prove that our method is convergent.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214809297",
                        "name": "Neel Mishra"
                    },
                    {
                        "authorId": "152934231",
                        "name": "Priyesh Kumar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The loss function is cross entropy and the AdaBelief [21] optimizer was used."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4ef3d5c0cbbe33689c1f8252cf24d58c29ea9a86",
                "externalIds": {
                    "DOI": "10.1101/2023.04.13.536542",
                    "CorpusId": 258182259
                },
                "corpusId": 258182259,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4ef3d5c0cbbe33689c1f8252cf24d58c29ea9a86",
                "title": "Ensembles for improved detection of invasive breast cancer in histological images",
                "abstract": "Accurate detection of invasive breast cancer (IC) can provide decision support to pathologists as well as improve downstream computational analyses, where detection of IC is a first step. Tissue containing IC is characterized by the presence of specific morphological features, which can be learned by convolutional neural networks (CNN). Here, we compare the use of a single CNN model versus an ensemble of several base models with the same CNN architecture, and we evaluate prediction performance as well as variability across ensemble based model predictions. Two in-house datasets comprising 587 WSI are used to train an ensemble of ten InceptionV3 models whose consensus is used to determine the presence of IC. A novel visualization strategy was developed to communicate ensemble agreement spatially. Performance was evaluated in an internal test set with 118 WSIs, and in an additional external dataset (TCGA breast cancer) with 157 WSI. We observed that the ensemble-based strategy outperformed the single CNN-model alternative with respect to accuracy on tile level in 89% of all WSIs in the test set. The overall accuracy was 0.92 (DICE coefficient, 0.90) for the ensemble model, and 0.85 (DICE coefficient, 0.83) for the single CNN alternative in the internal test set. For TCGA the ensemble outperformed the single CNN in 96.8% of the WSI, with an accuracy of 0.87 (DICE coefficient 0.89), the single model provides an accuracy of 0.75 (DICE coefficient 0.78) The results suggest that an ensemble-based modeling strategy for breast cancer invasive cancer detection consistently outperforms the conventional single model alternative. Furthermore, visualization of the ensemble agreement and confusion areas provide direct visual interpretation of the results. High performing cancer detection can provide decision support in the routine pathology setting as well as facilitate downstream computational analyses.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40519546",
                        "name": "L. Solorzano"
                    },
                    {
                        "authorId": "144661118",
                        "name": "S. Robertson"
                    },
                    {
                        "authorId": "2055412",
                        "name": "J. Hartman"
                    },
                    {
                        "authorId": "3154395",
                        "name": "M. Rantalainen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Reinforcement Learning: We use the Adabelief optimizer [77] with \u03b2=(0."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "74541907bc5c08a551f802d13f23a1e33e0dd00c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-04797",
                    "ArXiv": "2304.04797",
                    "DOI": "10.1016/j.neucom.2023.126737",
                    "CorpusId": 258060188
                },
                "corpusId": 258060188,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/74541907bc5c08a551f802d13f23a1e33e0dd00c",
                "title": "RAPID: Enabling Fast Online Policy Learning in Dynamic Public Cloud Environments",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "122192397",
                        "name": "Drew Penney"
                    },
                    {
                        "authorId": "2156071993",
                        "name": "Bin Li"
                    },
                    {
                        "authorId": "11223062",
                        "name": "Lizhong Chen"
                    },
                    {
                        "authorId": "2199424",
                        "name": "J. Sydir"
                    },
                    {
                        "authorId": "1742262860",
                        "name": "Anna Drewek-Ossowicka"
                    },
                    {
                        "authorId": "1698320",
                        "name": "R. Illikkal"
                    },
                    {
                        "authorId": "113282542",
                        "name": "Charlie Tai"
                    },
                    {
                        "authorId": "144025680",
                        "name": "R. Iyer"
                    },
                    {
                        "authorId": "2552212",
                        "name": "Andrew J. Herdrich"
                    }
                ]
            }
        },
        {
            "contexts": [
                "999, = 1e\u2212 14, and \u03b1 = 1e\u2212 3, following their original method AdaBelief [6].",
                "To ensure the optimizer make proper decision in all the three cases, [6] completely modified the second-order momentum to st = \u03b22st\u22121 + (1\u2212 \u03b22)(gt \u2212mt)(2) and proposed a new algorithm called AdaBelief.",
                "In regard to the essential reason of Adam\u2019s poor performance, [6] analyzed three different cases about curvature of the loss function in which Adam always takes improper stepsize.",
                "Proof: [6] proved that the regret of AdaBelief is with the following upper bound:",
                "To promote the generalization ability of Adam, [6] proposed an effective method named AdaBelief, which resets the second-order momentum as a new form st = \u03b22st\u22121 + (1\u2212 \u03b22)(gt \u2212mt)(2).",
                "Note that the second-order momentum in [6] is defined as follows:",
                "Note that the regularization is ignored for brevity as in other articles, such as [4], [6], [11].",
                "Moreover, the first-order momentum mt in [6] is: mt = \u03b21mt\u22121 + (1\u2212 \u03b21)gt."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c886f2bcb0f1d4e84b49786aad6e796762de95ce",
                "externalIds": {
                    "DBLP": "journals/tetci/ZhouHCWHL23",
                    "DOI": "10.1109/TETCI.2022.3171797",
                    "CorpusId": 249032644
                },
                "corpusId": 249032644,
                "publicationVenue": {
                    "id": "544cddb9-1149-469a-8377-d8c34f08d8b1",
                    "name": "IEEE Transactions on Emerging Topics in Computational Intelligence",
                    "alternate_names": [
                        "IEEE Trans Emerg Top Comput Intell"
                    ],
                    "issn": "2471-285X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7433297",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7433297"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c886f2bcb0f1d4e84b49786aad6e796762de95ce",
                "title": "Towards Faster Training Algorithms Exploiting Bandit Sampling From Convex to Strongly Convex Conditions",
                "abstract": "The training process for deep learning and pattern recognition normally involves the use of convex and strongly convex optimization algorithms such as AdaBelief and SAdam to handle lots of \u201cuninformative\u201d samples that should be ignored, thus incurring extra calculations. To solve this open problem, we propose to design bandit sampling method to make these algorithms focus on \u201cinformative\u201d samples during training process. Our contribution is twofold: first, we propose a convex optimization algorithm with bandit sampling, termed AdaBeliefBS, and prove that it converges faster than its original version; second, we prove that bandit sampling works well for strongly convex algorithms, and propose a generalized SAdam, called SAdamBS, that converges faster than SAdam. Finally, we conduct a series of experiments on various benchmark datasets to verify the fast convergence rate of our proposed algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145498691",
                        "name": "Yangfan Zhou"
                    },
                    {
                        "authorId": "5380819",
                        "name": "Kaizhu Huang"
                    },
                    {
                        "authorId": "2116394152",
                        "name": "Cheng Cheng"
                    },
                    {
                        "authorId": "2108084717",
                        "name": "Xuguang Wang"
                    },
                    {
                        "authorId": "145125161",
                        "name": "A. Hussain"
                    },
                    {
                        "authorId": "89121677",
                        "name": "Xin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d5bc71a84f5c596abb8bc433e9a0be3dc15a0a3e",
                "externalIds": {
                    "DBLP": "journals/cg/BaldiAD23",
                    "DOI": "10.1016/j.cag.2023.04.004",
                    "CorpusId": 258227844
                },
                "corpusId": 258227844,
                "publicationVenue": {
                    "id": "ca858314-5beb-4241-a7e4-f7748b3f2081",
                    "name": "Computers & graphics",
                    "type": "journal",
                    "alternate_names": [
                        "Computer Graphics",
                        "Comput graph",
                        "Computers & Graphics",
                        "Comput  Graph",
                        "Comput Graph",
                        "Computer graphics",
                        "Comput  graph"
                    ],
                    "issn": "0097-8493",
                    "alternate_issns": [
                        "0097-8930",
                        "1558-4569"
                    ],
                    "url": "https://www.sciencedirect.com/journal/computers-and-graphics",
                    "alternate_urls": [
                        "http://portal.acm.org/siggraph/newsletter",
                        "https://dl.acm.org/newsletter/siggraph",
                        "http://www.sciencedirect.com/science/journal/00978493"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d5bc71a84f5c596abb8bc433e9a0be3dc15a0a3e",
                "title": "Differentiable point process texture basis functions for inverse procedural modeling of cellular stochastic structures",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214804447",
                        "name": "Guillaume Baldi"
                    },
                    {
                        "authorId": "3308830",
                        "name": "R\u00e9mi All\u00e8gre"
                    },
                    {
                        "authorId": "1753136",
                        "name": "J. Dischler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The ERIL agent was trained with the ADABELIEF optimizer [31]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "341308f408e1403cdaecd57c38d04a2b649645c4",
                "externalIds": {
                    "DBLP": "journals/alr/BarthelemyKIII23",
                    "DOI": "10.1007/s10015-023-00868-w",
                    "CorpusId": 257865101
                },
                "corpusId": 257865101,
                "publicationVenue": {
                    "id": "d89bb9ba-512c-4c81-bdb3-1d3274e215e0",
                    "name": "Artificial Life and Robotics",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Life Robot"
                    ],
                    "issn": "1433-5298",
                    "url": "https://link.springer.com/journal/10015"
                },
                "url": "https://www.semanticscholar.org/paper/341308f408e1403cdaecd57c38d04a2b649645c4",
                "title": "Learning to mimic programmers gaze behavior for program comprehension improvement",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2061412388",
                        "name": "J. Barthelemy"
                    },
                    {
                        "authorId": "35200320",
                        "name": "Takatomi Kubo"
                    },
                    {
                        "authorId": "2072440756",
                        "name": "Takeshi D. Itoh"
                    },
                    {
                        "authorId": "150173029",
                        "name": "Kiyoka Ikeda"
                    },
                    {
                        "authorId": "49230982",
                        "name": "K. Ikeda"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(12)\nThen, the optimizer methods including AdamaX [35], AdaBelief [36], AdamP [37], Admod [38], and Novograd [39] are used to find the minimum values of the above optimization problems.",
                "To minimize the inverse variational problems, several optimization methods including AdamaX [35], AdaBelief [36], AdamP [37], Admod [38], and Novograd [39] are used to find theminimumvalues of the above optimization problems.",
                "Then, the optimizer methods including AdamaX [35], AdaBelief [36], AdamP [37], Admod [38], and Novograd [39] are used to find the minimum values of the above optimization problems."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "529afe075cca1e615b5410f8958bb6c58afce429",
                "externalIds": {
                    "DBLP": "journals/sivp/LiuHQH23",
                    "DOI": "10.1007/s11760-023-02544-9",
                    "CorpusId": 257652381
                },
                "corpusId": 257652381,
                "publicationVenue": {
                    "id": "11437c2b-f0a0-4db5-ac17-56dd7a223080",
                    "name": "Signal, Image and Video Processing",
                    "type": "journal",
                    "alternate_names": [
                        "Signal Image Video Process"
                    ],
                    "issn": "1863-1703",
                    "url": "https://link.springer.com/journal/11760"
                },
                "url": "https://www.semanticscholar.org/paper/529afe075cca1e615b5410f8958bb6c58afce429",
                "title": "Learning mean curvature-based regularization to solve the inverse variational problems from noisy data",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144325990",
                        "name": "Hongchen Liu"
                    },
                    {
                        "authorId": "2191250804",
                        "name": "Chunping Hou"
                    },
                    {
                        "authorId": "2064925064",
                        "name": "Hongbo Qu"
                    },
                    {
                        "authorId": "3292845",
                        "name": "Yonghong Hou"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6ac6518277be1561151f88366a24956d9f4fa56a",
                "externalIds": {
                    "ArXiv": "2303.09283",
                    "DBLP": "journals/corr/abs-2303-09283",
                    "DOI": "10.48550/arXiv.2303.09283",
                    "CorpusId": 257557581
                },
                "corpusId": 257557581,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6ac6518277be1561151f88366a24956d9f4fa56a",
                "title": "Exploring Resiliency to Natural Image Corruptions in Deep Learning using Design Diversity",
                "abstract": "In this paper, we investigate the relationship between diversity metrics, accuracy, and resiliency to natural image corruptions of Deep Learning (DL) image classifier ensembles. We investigate the potential of an attribution-based diversity metric to improve the known accuracy-diversity trade-off of the typical prediction-based diversity. Our motivation is based on analytical studies of design diversity that have shown that a reduction of common failure modes is possible if diversity of design choices is achieved. Using ResNet50 as a comparison baseline, we evaluate the resiliency of multiple individual DL model architectures against dataset distribution shifts corresponding to natural image corruptions. We compare ensembles created with diverse model architectures trained either independently or through a Neural Architecture Search technique and evaluate the correlation of prediction-based and attribution-based diversity to the final ensemble accuracy. We evaluate a set of diversity enforcement heuristics based on negative correlation learning to assess the final ensemble resilience to natural image corruptions and inspect the resulting prediction, activation, and attribution diversity. Our key observations are: 1) model architecture is more important for resiliency than model size or model accuracy, 2) attribution-based diversity is less negatively correlated to the ensemble accuracy than prediction-based diversity, 3) a balanced loss function of individual and ensemble accuracy creates more resilient ensembles for image natural corruptions, 4) architecture diversity produces more diversity in all explored diversity metrics: predictions, attributions, and activations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210794577",
                        "name": "Rafael Rosales"
                    },
                    {
                        "authorId": "2166351986",
                        "name": "Pablo Mu\u00f1oz"
                    },
                    {
                        "authorId": "2454462",
                        "name": "M. Paulitsch"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b7699ac7e3fc0a9a3fc16fd32ec00fdbe32e0184",
                "externalIds": {
                    "ArXiv": "2303.05916",
                    "DBLP": "journals/corr/abs-2303-05916",
                    "DOI": "10.48550/arXiv.2303.05916",
                    "CorpusId": 257482360
                },
                "corpusId": 257482360,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b7699ac7e3fc0a9a3fc16fd32ec00fdbe32e0184",
                "title": "GECCO: Geometrically-Conditioned Point Diffusion Models",
                "abstract": "Diffusion models generating images conditionally on text, such as Dall-E 2 and Stable Diffusion, have recently made a splash far beyond the computer vision community. Here, we tackle the related problem of generating point clouds, both unconditionally, and conditionally with images. For the latter, we introduce a novel geometrically-motivated conditioning scheme based on projecting sparse image features into the point cloud and attaching them to each individual point, at every step in the denoising process. This approach improves geometric consistency and yields greater fidelity than current methods relying on unstructured, global latent codes. Additionally, we show how to apply recent continuous-time diffusion schemes. Our method performs on par or above the state of art on conditional and unconditional experiments on synthetic data, while being faster, lighter, and delivering tractable likelihoods. We show it can also scale to diverse indoors scenes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "4827500",
                        "name": "M. Tyszkiewicz"
                    },
                    {
                        "authorId": "153918727",
                        "name": "P. Fua"
                    },
                    {
                        "authorId": "1995333",
                        "name": "Eduard Trulls"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(7)\nMeanwhile, we can also use many other forms of adaptive matrix At, e.g., we can also generate the adaptive matrix At, as AdaBelief algorithm [Zhuang et al., 2020], defined as\nat = \u03c4at\u22121 + (1\u2212 \u03c4)(wt \u2212\u2207xf(xt, yt; \u03bet))2, At = diag( \u221a at + \u03c1), (8)\nwhere \u03c4 \u2208 (0, 1).",
                "(10)\nMeanwhile, we can also use many other forms of adaptive matrix Bt, e.g., we can also generate the adaptive matrix At, as AdaBelief algorithm [Zhuang et al., 2020], defined as\nbt = \u03c4bt\u22121 + (1\u2212 \u03c4)(vt \u2212\u2207yf(xt, yt; \u03bet))2, Bt = diag( \u221a bt + \u03c1).",
                ", we can also generate the adaptive matrix At, as AdaBelief algorithm [Zhuang et al., 2020], defined as bt = \u03c4bt\u22121 + (1\u2212 \u03c4)(vt \u2212\u2207yf(xt, yt; \u03bet))(2), Bt = diag( \u221a",
                ", we can also generate the adaptive matrix At, as AdaBelief algorithm [Zhuang et al., 2020], defined as at = \u03c4at\u22121 + (1\u2212 \u03c4)(wt \u2212\u2207xf(xt, yt; \u03bet))(2), At = diag( \u221a at + \u03c1), (8) where \u03c4 \u2208 (0, 1)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3939256c250bd1afcd972a18a4b5f342007f5afb",
                "externalIds": {
                    "ArXiv": "2303.03984",
                    "DBLP": "journals/corr/abs-2303-03984",
                    "DOI": "10.48550/arXiv.2303.03984",
                    "CorpusId": 257378554
                },
                "corpusId": 257378554,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3939256c250bd1afcd972a18a4b5f342007f5afb",
                "title": "Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax Optimization",
                "abstract": "In the paper, we study a class of nonconvex nonconcave minimax optimization problems (i.e., $\\min_x\\max_y f(x,y)$), where $f(x,y)$ is possible nonconvex in $x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition in $y$. Moreover, we propose a class of enhanced momentum-based gradient descent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic Nonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use various adaptive learning rates in updating the variables $x$ and $y$ without relying on any global and coordinate-wise adaptive learning rates. Theoretically, we present an effective convergence analysis framework for our methods. Specifically, we prove that our MSGDA and AdaMSGDA methods have the best known sample (gradient) complexity of $O(\\epsilon^{-3})$ only requiring one sample at each loop in finding an $\\epsilon$-stationary solution (i.e., $\\mathbb{E}\\|\\nabla F(x)\\|\\leq \\epsilon$, where $F(x)=\\max_y f(x,y)$). This manuscript commemorates the mathematician Boris Polyak (1935-2023).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3057688",
                        "name": "Feihu Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4dc236dcaf8f8ea2754a66c1429ea087b3f4d743",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-03540",
                    "ArXiv": "2303.03540",
                    "DOI": "10.1109/ICSE-NIER58687.2023.00027",
                    "CorpusId": 257378300
                },
                "corpusId": 257378300,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4dc236dcaf8f8ea2754a66c1429ea087b3f4d743",
                "title": "Judging Adam: Studying the Performance of Optimization Methods on ML4SE Tasks",
                "abstract": "Solving a problem with a deep learning model requires researchers to optimize the loss function with a certain optimization method. The research community has developed more than a hundred different optimizers, yet there is scarce data on optimizer performance in various tasks. In particular, none of the benchmarks test the performance of optimizers on source code-related problems. However, existing benchmark data indicates that certain optimizers may be more efficient for particular domains. In this work, we test the performance of various optimizers on deep learning models for source code and find that the choice of an optimizer can have a significant impact on the model quality, with up to two-fold score differences between some of the relatively well-performing optimizers. We also find that RAdam optimizer (and its modification with the Lookahead envelope) is the best optimizer that almost always performs well on the tasks we consider. Our findings show a need for a more extensive study of the optimizers in code-related tasks, and indicate that the ML4SE community should consider using RAdam instead of Adam as the default optimizer for code-related deep learning tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "88727557",
                        "name": "D. Pasechnyuk"
                    },
                    {
                        "authorId": "2210858132",
                        "name": "Anton Prazdnichnykh"
                    },
                    {
                        "authorId": "103293241",
                        "name": "Mikhail Evtikhiev"
                    },
                    {
                        "authorId": "2851011",
                        "name": "T. Bryksin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We used the same hyperparameter values for the AdaBelief optimizer depending on datasets as described in (Zhuang et al. 2020).",
                "We used the SGD optimizer for training the alternatives, and the AdaBelief optimizer (Zhuang et al. 2020) for fine-tuning the student model."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "eb36afb336dff204652ef5dc39c9ce32353e48cb",
                "externalIds": {
                    "DBLP": "conf/aaai/LeeM23",
                    "ArXiv": "2303.01913",
                    "DOI": "10.48550/arXiv.2303.01913",
                    "CorpusId": 257353724
                },
                "corpusId": 257353724,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/eb36afb336dff204652ef5dc39c9ce32353e48cb",
                "title": "Bespoke: A Block-Level Neural Network Optimization Framework for Low-Cost Deployment",
                "abstract": "As deep learning models become popular, there is a lot of need for deploying them to diverse device environments. Because it is costly to develop and optimize a neural network for every single environment, there is a line of research to search neural networks for multiple target environments efficiently. However, existing works for such a situation still suffer from requiring many GPUs and expensive costs. Motivated by this, we propose a novel neural network optimization framework named Bespoke for low-cost deployment. Our framework searches for a lightweight model by replacing parts of an original model with randomly selected alternatives, each of which comes from a pretrained neural network or the original model. In the practical sense, Bespoke has two significant merits. One is that it requires near zero cost for designing the search space of neural networks. The other merit is that it exploits the sub-networks of public pretrained neural networks, so the total cost is minimal compared to the existing works. We conduct experiments exploring Bespoke's the merits, and the results show that it finds efficient models for multiple targets with meager cost.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2041345402",
                        "name": "Jong-Ryul Lee"
                    },
                    {
                        "authorId": "2115058",
                        "name": "Yong-Hyuk Moon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the outer loop, the optimization algorithm is AdaBelief (Zhuang et al., 2020), sweeping the learning rate over 1e-4, 1e-5, 1e-6."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "34c2f6fa7096af0b8f128c6725be77ac32e0c90f",
                "externalIds": {
                    "ArXiv": "2303.00800",
                    "DBLP": "journals/corr/abs-2303-00800",
                    "DOI": "10.48550/arXiv.2303.00800",
                    "CorpusId": 257280379
                },
                "corpusId": 257280379,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/34c2f6fa7096af0b8f128c6725be77ac32e0c90f",
                "title": "Continuous-Time Functional Diffusion Processes",
                "abstract": "We introduce Functional Diffusion Processes (FDPs), which generalize score-based diffusion models to infinite-dimensional function spaces. FDPs require a new mathematical framework to describe the forward and backward dynamics, and several extensions to derive practical training objectives. These include infinite-dimensional versions of Girsanov theorem, in order to be able to compute an ELBO, and of the sampling theorem, in order to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. We use FDPs to build a new breed of generative models in function spaces, which do not require specialized network architectures, and that can work with any kind of continuous data. Our results on real data show that FDPs achieve high-quality image generation, using a simple MLP architecture with orders of magnitude fewer parameters than existing diffusion models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47584398",
                        "name": "Giulio Franzese"
                    },
                    {
                        "authorId": "47628623",
                        "name": "Simone Rossi"
                    },
                    {
                        "authorId": "1878342442",
                        "name": "Dario Rossi"
                    },
                    {
                        "authorId": "34066178",
                        "name": "Markus Heinonen"
                    },
                    {
                        "authorId": "3138895",
                        "name": "M. Filippone"
                    },
                    {
                        "authorId": "143822502",
                        "name": "P. Michiardi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Take a gradient step using the Adabeleif optimizer (Zhuang et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "eb8fc355d129898d8f95bf9dc5160cb2f288d9cb",
                "externalIds": {
                    "ArXiv": "2302.12235",
                    "DBLP": "journals/corr/abs-2302-12235",
                    "DOI": "10.48550/arXiv.2302.12235",
                    "CorpusId": 257102725
                },
                "corpusId": 257102725,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/eb8fc355d129898d8f95bf9dc5160cb2f288d9cb",
                "title": "Q-Flow: Generative Modeling for Differential Equations of Open Quantum Dynamics with Normalizing Flows",
                "abstract": "Studying the dynamics of open quantum systems can enable breakthroughs both in fundamental physics and applications to quantum engineering and quantum computation. Since the density matrix $\\rho$, which is the fundamental description for the dynamics of such systems, is high-dimensional, customized deep generative neural networks have been instrumental in modeling $\\rho$. However, the complex-valued nature and normalization constraints of $\\rho$, as well as its complicated dynamics, prohibit a seamless connection between open quantum systems and the recent advances in deep generative modeling. Here we lift that limitation by utilizing a reformulation of open quantum system dynamics to a partial differential equation (PDE) for a corresponding probability distribution $Q$, the Husimi Q function. Thus, we model the Q function seamlessly with off-the-shelf deep generative models such as normalizing flows. Additionally, we develop novel methods for learning normalizing flow evolution governed by high-dimensional PDEs based on the Euler method and the application of the time-dependent variational principle. We name the resulting approach $Q$-$Flow$ and demonstrate the scalability and efficiency of Q-Flow on open quantum system simulations, including the dissipative harmonic oscillator and the dissipative bosonic model. Q-Flow is superior to conventional PDE solvers and state-of-the-art physics-informed neural network solvers, especially in high-dimensional systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2097881334",
                        "name": "Owen Dugan"
                    },
                    {
                        "authorId": "48985656",
                        "name": "Peter Y. Lu"
                    },
                    {
                        "authorId": "26916003",
                        "name": "R. Dangovski"
                    },
                    {
                        "authorId": "49017316",
                        "name": "Di Luo"
                    },
                    {
                        "authorId": "1398486683",
                        "name": "M. Soljavci'c"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We utilize the AdaBelief optimizer since it performs preconditioning based on local curvature information.",
                "To minimize the training loss (7) with the scaling factor mt = 1d , the AdaBelief (Zhuang et al., 2020) optimizer is used for 100 000 iterations using a mini-batch size of 128 along with an initial learning rate of 10\u22123."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e0686a59d6459ef5378f51418155dee15afb3b81",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-10502",
                    "ArXiv": "2302.10502",
                    "DOI": "10.48550/arXiv.2302.10502",
                    "CorpusId": 257050695
                },
                "corpusId": 257050695,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e0686a59d6459ef5378f51418155dee15afb3b81",
                "title": "Learning Gradually Non-convex Image Priors Using Score Matching",
                "abstract": "In this paper, we propose a unified framework of denoising score-based models in the context of graduated non-convex energy minimization. We show that for sufficiently large noise variance, the associated negative log density -- the energy -- becomes convex. Consequently, denoising score-based models essentially follow a graduated non-convexity heuristic. We apply this framework to learning generalized Fields of Experts image priors that approximate the joint density of noisy images and their associated variances. These priors can be easily incorporated into existing optimization algorithms for solving inverse problems and naturally implement a fast and robust graduated non-convexity mechanism.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50289209",
                        "name": "Erich Kobler"
                    },
                    {
                        "authorId": "1730097",
                        "name": "T. Pock"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also employ four popular handcrafted optimizers: RAdam (Liu et al., 2020), NAdam (Dozat, 2016), AdaBelief (Zhuang et al., 2020), AMSGrad (Reddi et al., 2018) and two optimizers discovered by AutoML: PowerSign (Bello et al., 2017) and AddSign (Bello et al., 2017) to train ViT-S/16 and ViT-B/16 on\u2026",
                ", 2020), NAdam (Dozat, 2016), AdaBelief (Zhuang et al., 2020), AMSGrad (Reddi et al.",
                "Note that the search also discovers other existing or novel algorithms shown in Appendix D, e.g., some with better regularization and some resembling AdaBelief (Zhuang et al., 2020) and AdaGrad (Duchi et al., 2011).",
                "There are a large number of handcrafted optimizers, mostly adaptive ones, introduced in recent years (Anil et al., 2020; Balles and Hennig, 2018; Bernstein et al., 2018; Dozat, 2016; Liu et al., 2020; Zhuang et al., 2020).",
                "It dynamically calculates the dot product between the weight and gradient, before computing the weight decay. def train(w, g, m, v, lr): m = interp(m, g, 0.16) g2 = square(g) v = interpolate(v, g2, 0.001) v753 = dot(g, w) sqrt_v = sqrt(v) update = m / sqrt_v wd = v753 * w update = sin(update) update = update + wd lr = lr * 0.0216 update = update * lr v = sin(v) return update, m, v\nProgram 6: Algorithm that tracks the second moment without EMA decay, which is the same as AdaGrad. def train(w, g, m, v, lr): m = interp(m, g, 0.1) g2 = square(g) g2 = v + g2 v = interp(v, g2, 0.0015) sqrt_v = sqrt(v) update = m / sqrt_v v70 = get_pi() v = min(v, v70) update = sinh(update) lr = lr * 0.0606 update = update * lr return update, m, v\nProgram 7: Algorithm uses the difference between gradient and momentum to track the second moment, resembling AdaBelief. def train(w, g, m, v, lr): m = interp(m, g, 0.1) g = g - m g2 = square(g) v = interp(v, g2, 0.001) sqrt_v = sqrt(v) update = m / sqrt_v wd = w * 0.0238 update = update + wd lr = lr * 0.03721 update = update * lr return update, m, v",
                "\u2026optimizers (Anil et al., 2020; Bernstein et al., 2018; Dozat, 2016; Duchi et al., 2011; Gupta et al., 2018; Kingma and Ba, 2014; Liu et al., 2020; Ma and Yarats, 2019; Reddi et al., 2018; Riedmiller and Braun, 1993; Shazeer and Stern, 2018; Zhuang et al., 2020), which we discuss in Section 3.2.",
                "We also employ four popular handcrafted optimizers: RAdam (Liu et al., 2020), NAdam (Dozat, 2016), AdaBelief (Zhuang et al., 2020), AMSGrad (Reddi et al., 2018) and two optimizers discovered by AutoML: PowerSign (Bello et al., 2017) and AddSign (Bello et al., 2017) to train ViT-S/16 and ViT-B/16 on ImageNet (with RandAug and Mixup).",
                ", some with better regularization and some resembling AdaBelief (Zhuang et al., 2020) and AdaGrad (Duchi et al.",
                "Other related works include numerous handcrafted optimizers (Anil et al., 2020; Bernstein et al., 2018; Dozat, 2016; Duchi et al., 2011; Gupta et al., 2018; Kingma and Ba, 2014; Liu et al., 2020; Ma and Yarats, 2019; Reddi et al., 2018; Riedmiller and Braun, 1993; Shazeer and Stern, 2018; Zhuang et al., 2020), which we discuss in Section 3."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "a7f59b2162ae0ea2520753b1b9b17277490a9458",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-06675",
                    "ArXiv": "2302.06675",
                    "DOI": "10.48550/arXiv.2302.06675",
                    "CorpusId": 256846990
                },
                "corpusId": 256846990,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a7f59b2162ae0ea2520753b1b9b17277490a9458",
                "title": "Symbolic Discovery of Optimization Algorithms",
                "abstract": "We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\\textbf{Lion}$ ($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and 91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143737082",
                        "name": "Xiangning Chen"
                    },
                    {
                        "authorId": "145246869",
                        "name": "Chen Liang"
                    },
                    {
                        "authorId": "2110408964",
                        "name": "Da Huang"
                    },
                    {
                        "authorId": "2892780",
                        "name": "Esteban Real"
                    },
                    {
                        "authorId": "4054249",
                        "name": "Kaiyuan Wang"
                    },
                    {
                        "authorId": "2187205985",
                        "name": "Yao Liu"
                    },
                    {
                        "authorId": "143950636",
                        "name": "Hieu Pham"
                    },
                    {
                        "authorId": "9929684",
                        "name": "Xuanyi Dong"
                    },
                    {
                        "authorId": "1821711",
                        "name": "Thang Luong"
                    },
                    {
                        "authorId": "1793529",
                        "name": "Cho-Jui Hsieh"
                    },
                    {
                        "authorId": "2141538599",
                        "name": "Yifeng Lu"
                    },
                    {
                        "authorId": "1397917613",
                        "name": "Quoc V. Le"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7d21999f1dd29d62105230b8aa5cbab881429639",
                "externalIds": {
                    "ArXiv": "2302.06103",
                    "DBLP": "journals/corr/abs-2302-06103",
                    "DOI": "10.48550/arXiv.2302.06103",
                    "CorpusId": 256826803
                },
                "corpusId": 256826803,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7d21999f1dd29d62105230b8aa5cbab881429639",
                "title": "FedDA: Faster Framework of Local Adaptive Gradient Methods via Restarted Dual Averaging",
                "abstract": "Federated learning (FL) is an emerging learning paradigm to tackle massively distributed data. In Federated Learning, a set of clients jointly perform a machine learning task under the coordination of a server. The FedAvg algorithm is one of the most widely used methods to solve Federated Learning problems. In FedAvg, the learning rate is a constant rather than changing adaptively. The adaptive gradient methods show superior performance over the constant learning rate schedule; however, there is still no general framework to incorporate adaptive gradient methods into the federated setting. In this paper, we propose \\textbf{FedDA}, a novel framework for local adaptive gradient methods. The framework adopts a restarted dual averaging technique and is flexible with various gradient estimation methods and adaptive learning rate formulations. In particular, we analyze \\textbf{FedDA-MVR}, an instantiation of our framework, and show that it achieves gradient complexity $\\tilde{O}(\\epsilon^{-1.5})$ and communication complexity $\\tilde{O}(\\epsilon^{-1})$ for finding a stationary point $\\epsilon$. This matches the best known rate for first-order FL algorithms and \\textbf{FedDA-MVR} is the first adaptive FL algorithm that achieves this rate. We also perform extensive numerical experiments to verify the efficacy of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108933511",
                        "name": "Junyi Li"
                    },
                    {
                        "authorId": "3057688",
                        "name": "Feihu Huang"
                    },
                    {
                        "authorId": "145114933",
                        "name": "Heng Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3e2f92d2d38952fc21b7c3b74035e4058b4a2b99",
                "externalIds": {
                    "DBLP": "conf/icml/LooHLR23",
                    "ArXiv": "2302.06755",
                    "DOI": "10.48550/arXiv.2302.06755",
                    "CorpusId": 256846981
                },
                "corpusId": 256846981,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3e2f92d2d38952fc21b7c3b74035e4058b4a2b99",
                "title": "Dataset Distillation with Convexified Implicit Gradients",
                "abstract": "We propose a new dataset distillation algorithm using reparameterization and convexification of implicit gradients (RCIG), that substantially improves the state-of-the-art. To this end, we first formulate dataset distillation as a bi-level optimization problem. Then, we show how implicit gradients can be effectively used to compute meta-gradient updates. We further equip the algorithm with a convexified approximation that corresponds to learning on top of a frozen finite-width neural tangent kernel. Finally, we improve bias in implicit gradients by parameterizing the neural network to enable analytical computation of final-layer parameters given the body parameters. RCIG establishes the new state-of-the-art on a diverse series of dataset distillation tasks. Notably, with one image per class, on resized ImageNet, RCIG sees on average a 108% improvement over the previous state-of-the-art distillation algorithm. Similarly, we observed a 66% gain over SOTA on Tiny-ImageNet and 37% on CIFAR-100.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2028788047",
                        "name": "Noel Loo"
                    },
                    {
                        "authorId": "8252176",
                        "name": "Ramin M. Hasani"
                    },
                    {
                        "authorId": "39083616",
                        "name": "Mathias Lechner"
                    },
                    {
                        "authorId": "2064605577",
                        "name": "Daniela Rus"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Motivated by the existing work [20]\u2013[22], we would like to reduce the range of the adaptive stepsizes of Adam to make the new optimizer get closer to SGD with momentum.",
                "Our analysis follows a strategy similar to that used to analyse AdaBelief in [22].",
                "In this task, the ten optimizers were evaluated by following a similar experimental setup as in [22].",
                "The second open source is the original implementation of AdaBelief [22].",
                "The AdaBelief method of [22] extends Adam by tracking the EMA of the squared prediction error (mt\u2212gt)(2) instead of g(2)t when",
                "To demonstrate the effectiveness of the proposed method, eight adaptive optimizers from the literature were tested and compared, namely Yogi [17], RAdam [20], MSVAG [18], Fromage [19], Adam [11], AdaBound [21], AdamW [15], and AdaBelief [22]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "2fa601e84ddae375e500b255f48f50e3bd41bfff",
                "externalIds": {
                    "ArXiv": "2302.01029",
                    "DBLP": "journals/corr/abs-2302-01029",
                    "DOI": "10.48550/arXiv.2302.01029",
                    "CorpusId": 256503676
                },
                "corpusId": 256503676,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2fa601e84ddae375e500b255f48f50e3bd41bfff",
                "title": "On Suppressing Range of Adaptive Stepsizes of Adam to Improve Generalisation Performance",
                "abstract": "A number of recent adaptive optimizers improve the generalisation performance of Adam by essentially reducing the variance of adaptive stepsizes to get closer to SGD with momentum. Following the above motivation, we suppress the range of the adaptive stepsizes of Adam by exploiting the layerwise gradient statistics. In particular, at each iteration, we propose to perform three consecutive operations on the second momentum v_t before using it to update a DNN model: (1): down-scaling, (2): epsilon-embedding, and (3): down-translating. The resulting algorithm is referred to as SET-Adam, where SET is a brief notation of the three operations. The down-scaling operation on v_t is performed layerwise by making use of the angles between the layerwise subvectors of v_t and the corresponding all-one subvectors. Extensive experimental results show that SET-Adam outperforms eight adaptive optimizers when training transformers and LSTMs for NLP, and VGG and ResNet for image classification over CIAF10 and CIFAR100 while matching the best performance of the eight adaptive methods when training WGAN-GP models for image generation tasks. Furthermore, SET-Adam produces higher validation accuracies than Adam and AdaBelief for training ResNet18 over ImageNet.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143914586",
                        "name": "Guoqiang Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[24] proposed another adaptive gradient method called AdaBelief, which adapts the stepsize according to the \u201cbelief\u201d in the current gradient direction.",
                "Generally, the commonly used first-order gradient methods can be categorized into two groups: the accelerated stochastic gradient descent (SGD) family [15,16,18] and adaptive gradient methods [7,23, 24].",
                "unified learning rate for all parameters, adaptive gradient methods compute a specific learning rate for each individual parameter [24]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6e3e211d6be8fa66a5f8cfc60a67f0ef3f7ab9ab",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-00195",
                    "ArXiv": "2302.00195",
                    "DOI": "10.48550/arXiv.2302.00195",
                    "CorpusId": 256459693
                },
                "corpusId": 256459693,
                "publicationVenue": {
                    "id": "1e517cb2-1ca1-45b8-964a-456366bdcdc1",
                    "name": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "PAKDD",
                        "Pacific-asia Conf Knowl Discov Data Min"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6e3e211d6be8fa66a5f8cfc60a67f0ef3f7ab9ab",
                "title": "Weight Prediction Boosts the Convergence of AdamW",
                "abstract": "In this paper, we introduce weight prediction into the AdamW optimizer to boost its convergence when training the deep neural network (DNN) models. In particular, ahead of each mini-batch training, we predict the future weights according to the update rule of AdamW and then apply the predicted future weights to do both forward pass and backward propagation. In this way, the AdamW optimizer always utilizes the gradients w.r.t. the future weights instead of current weights to update the DNN parameters, making the AdamW optimizer achieve better convergence. Our proposal is simple and straightforward to implement but effective in boosting the convergence of DNN training. We performed extensive experimental evaluations on image classification and language modeling tasks to verify the effectiveness of our proposal. The experimental results validate that our proposal can boost the convergence of AdamW and achieve better accuracy than AdamW when training the DNN models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2059059973",
                        "name": "Lei Guan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "65cd013e67cb677c5095ddb74d6f9081cf81af9a",
                "externalIds": {
                    "DBLP": "journals/tccn/GhasemiP23",
                    "DOI": "10.1109/TCCN.2022.3222792",
                    "CorpusId": 253614857
                },
                "corpusId": 253614857,
                "publicationVenue": {
                    "id": "65e58b80-9699-4da6-bd60-929b57b8533d",
                    "name": "IEEE Transactions on Cognitive Communications and Networking",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Cogn Commun Netw"
                    ],
                    "issn": "2332-7731",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6687307",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6687307"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/65cd013e67cb677c5095ddb74d6f9081cf81af9a",
                "title": "DeepAir: Enabling Data-Driven Dynamic Spectrum Sharing via Scalable Forecasting",
                "abstract": "The rapid uptake of wireless technologies over the past decade has resulted in an increasing pressure on the limited radio spectrum resources. To improve the efficiency of current allocation policies, regulators in many jurisdictions are considering dynamic spectrum sharing. The success, however, of an optimized system hinges on the ability to sense, characterize, and forecast spectrum usage behaviour. Since traditional methods prove unable to scale to a wide range of channels, we propose DeepAir, a robust and scalable model that is capable of learning and predicting complex temporal and spectral dependencies in multivariate spectrum data. Specifically, we design a Sequence-to-Sequence model that employs an encoder-decoder architecture with two Deep Temporal Convolutional Networks. Using a test set consisting of approximately 900 channels in the Land Mobile Radio bands, we obtain a median RMSE and median MAE of 6.51 and 5.15, respectively. We then apply transfer learning to demonstrate the effectiveness of this model in forecasting patterns from any sensor, regardless of the band, sensitivity, and geographical location. Furthermore, the model exhibits no performance degradation up to three years after training for both short and long forecast horizons. Finally, we use DeepAir to quantify spectrum availability to enhance existing spectrum sharing capabilities.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144418414",
                        "name": "A. Ghasemi"
                    },
                    {
                        "authorId": "2365246",
                        "name": "Janak J. Parekh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c31b9aa3625fdf0b401a3d7d1c0ea437685e57f5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-11546",
                    "ArXiv": "2301.11546",
                    "DOI": "10.48550/arXiv.2301.11546",
                    "CorpusId": 256358539
                },
                "corpusId": 256358539,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c31b9aa3625fdf0b401a3d7d1c0ea437685e57f5",
                "title": "Adapting Step-size: A Unified Perspective to Analyze and Improve Gradient-based Methods for Adversarial Attacks",
                "abstract": "Learning adversarial examples can be formulated as an optimization problem of maximizing the loss function with some box-constraints. However, for solving this induced optimization problem, the state-of-the-art gradient-based methods such as FGSM, I-FGSM and MI-FGSM look different from their original methods especially in updating the direction, which makes it difficult to understand them and then leaves some theoretical issues to be addressed in viewpoint of optimization. In this paper, from the perspective of adapting step-size, we provide a unified theoretical interpretation of these gradient-based adversarial learning methods. We show that each of these algorithms is in fact a specific reformulation of their original gradient methods but using the step-size rules with only current gradient information. Motivated by such analysis, we present a broad class of adaptive gradient-based algorithms based on the regular gradient methods, in which the step-size strategy utilizing information of the accumulated gradients is integrated. Such adaptive step-size strategies directly normalize the scale of the gradients rather than use some empirical operations. The important benefit is that convergence for the iterative algorithms is guaranteed and then the whole optimization process can be stabilized. The experiments demonstrate that our AdaI-FGM consistently outperforms I-FGSM and AdaMI-FGM remains competitive with MI-FGSM for black-box attacks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "39085395",
                        "name": "Wei Tao"
                    },
                    {
                        "authorId": "2056689994",
                        "name": "Lei Bao"
                    },
                    {
                        "authorId": "2203287756",
                        "name": "Long Sheng"
                    },
                    {
                        "authorId": "2236258",
                        "name": "Gao-wei Wu"
                    },
                    {
                        "authorId": "144750297",
                        "name": "Qing Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2)W\u0304SN = W\n\ud835\udf0e(W)\n1 3\nWe use AdaBelief Optimizer [8] as the new optimizer for the whole network structure.",
                "And a new optimizer - AdaBelief Optimizer is adopted to optimize the whole network, in which BCEWithLogitsLoss is added in the loss function to enhance the generalization ability.",
                "Thirdly, we use AdaBelief Optimizer [8] on the whole network structure, in which BCEWithLogitsLoss is added to enhance generalization.",
                "1 3 We use AdaBelief Optimizer [8] as the new optimizer for the whole network structure."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bdb6133e74705da1de48efa8570c6ba2e67561a7",
                "externalIds": {
                    "DBLP": "journals/mms/ChenWXWSJ23",
                    "DOI": "10.1007/s00530-023-01047-4",
                    "CorpusId": 256289737
                },
                "corpusId": 256289737,
                "publicationVenue": {
                    "id": "d1997ea9-9d41-4458-9280-94feb013bd15",
                    "name": "Multimedia Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Syst"
                    ],
                    "issn": "0942-4962",
                    "url": "http://www.springer.com/computer/information+systems+and+applications/journal/530?changeHeader",
                    "alternate_urls": [
                        "https://link.springer.com/journal/530",
                        "http://www.springer.com/computer/information+systems+and+applications/journal/530?changeHeader="
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bdb6133e74705da1de48efa8570c6ba2e67561a7",
                "title": "Style transfer network for complex multi-stroke text",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3145060",
                        "name": "Fangmei Chen"
                    },
                    {
                        "authorId": "2145199365",
                        "name": "Yuying Wang"
                    },
                    {
                        "authorId": "2202883136",
                        "name": "Sheng Xu"
                    },
                    {
                        "authorId": "2595425",
                        "name": "Fasheng Wang"
                    },
                    {
                        "authorId": "2075375099",
                        "name": "Fuming Sun"
                    },
                    {
                        "authorId": "2112826794",
                        "name": "X. Jia"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that we could not replicate the mAP from (Zhuang et al., 2020); we suspect the reason is their use of the MMDetection (Chen et al., 2019) framework, which does various extra image augmentation transforms.",
                "PASCAL VOC on Faster-RCNN We train PASCAL VOC on Faster-RCNN (Ren et al., 2015) with pretrained ResNet-50 backbone, following (Zhuang et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "16023b9fd617be0c23405d7703380212693af2a8",
                "externalIds": {
                    "ArXiv": "2301.10133",
                    "DBLP": "journals/corr/abs-2301-10133",
                    "DOI": "10.48550/arXiv.2301.10133",
                    "CorpusId": 256194597
                },
                "corpusId": 256194597,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/16023b9fd617be0c23405d7703380212693af2a8",
                "title": "Read the Signs: Towards Invariance to Gradient Descent's Hyperparameter Initialization",
                "abstract": "We propose ActiveLR, an optimization meta algorithm that localizes the learning rate, $\\alpha$, and adapts them at each epoch according to whether the gradient at each epoch changes sign or not. This sign-conscious algorithm is aware of whether from the previous step to the current one the update of each parameter has been too large or too small and adjusts the $\\alpha$ accordingly. We implement the Active version (ours) of widely used and recently published gradient descent optimizers, namely SGD with momentum, AdamW, RAdam, and AdaBelief. Our experiments on ImageNet, CIFAR-10, WikiText-103, WikiText-2, and PASCAL VOC using different model architectures, such as ResNet and Transformers, show an increase in generalizability and training set fit, and decrease in training time for the Active variants of the tested optimizers. The results also show robustness of the Active variant of these optimizers to different values of the initial learning rate. Furthermore, the detrimental effects of using large mini-batch sizes are mitigated. ActiveLR, thus, alleviates the need for hyper-parameter search for two of the most commonly tuned hyper-parameters that require heavy time and computational costs to pick. We encourage AI researchers and practitioners to use the Active variant of their optimizer of choice for faster training, better generalizability, and reducing carbon footprint of training deep neural networks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2202412815",
                        "name": "Davood Wadi"
                    },
                    {
                        "authorId": "1822481",
                        "name": "M. Fredette"
                    },
                    {
                        "authorId": "145268827",
                        "name": "S. S\u00e9n\u00e9cal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Instead of the default cumulative process of averaging the historical gradients in optimization algorithms, such as AdaBelief, AMSGrad, and AdamW, we try to sample an unbiased set of historical gradients about global data in each update round.",
                "optimizers based on historical gradient information, such as AMSGrad [5], AdamW [6], AdaBelief [7], etc.",
                "(3)\n4) For the AdaBelief optimization method.",
                "What\u2019s worse, when using optimizers based on historical gradient information, such as AMSGrad [5], AdamW [6], AdaBelief [7], etc., their historical gradients also tend to fail to yield valid guidance information, which means we cannot make use of start-to-art optimization methods to achieve better model performance in these scenarios.",
                "These existing start-to-art optimization methods include AdamW [6], AMSGrad [5], and AdaBelief [7]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fb2ab76610eb5b83de6f9620e0bce64c95c0040c",
                "externalIds": {
                    "DBLP": "journals/iotj/YouLJCY23",
                    "DOI": "10.1109/JIOT.2022.3203233",
                    "CorpusId": 251986871
                },
                "corpusId": 251986871,
                "publicationVenue": {
                    "id": "228761ec-c40a-479b-8309-9dcbe9851bcd",
                    "name": "IEEE Internet of Things Journal",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Internet Thing J"
                    ],
                    "issn": "2327-4662",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER288-ELE",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/servlet/opac?punumber=6488907",
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6488907",
                        "http://ieee-iotj.org/#"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fb2ab76610eb5b83de6f9620e0bce64c95c0040c",
                "title": "Reschedule Gradients: Temporal Non-IID Resilient Federated Learning",
                "abstract": "Federated learning is a popular framework designed to perform the distributed machine learning while protecting client privacy. However, the heterogeneous data distribution in real-world environments makes it difficult to converge when performing model training. In this article, we propose the federated gradient scheduling (FedGS), an improved historical gradient sampling utilization method for optimizers that utilize historical gradients in the federated learning to alleviate the instability problem of historical gradient information due to non-IID. FedGS consists of two main steps to improve federated learning performance. First, clustering uses clients\u2019 label distributions, which relabel clients and their submitting gradients. Second, sampling gradient clusters to generate an IID gradient set, which feeds to optimizers to derive valid momentum information. Besides, we introduce differential privacy to collaborate with FedGS to enhance clients\u2019 privacy protection strength. Compared to previous non-IID federated learning solutions, our method can achieve greater resistance to temporal non-IID. Moreover, experiments show that FedGS can achieve faster convergence and performance gain of up to 10% over existing state-to-art methods in some scenarios. FedGS can combine with existing methods easily to achieve better performance. We further verify that our method has robust performance gains in different non-IID scenarios, demonstrating the adaptability of FedGS for different scenarios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2183634857",
                        "name": "Xianyao You"
                    },
                    {
                        "authorId": "2154487593",
                        "name": "Ximeng Liu"
                    },
                    {
                        "authorId": "2089088719",
                        "name": "Nan Jiang"
                    },
                    {
                        "authorId": "47468661",
                        "name": "Jianping Cai"
                    },
                    {
                        "authorId": "3436314",
                        "name": "Zuobin Ying"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To solve this problem, techniques, such as rectified linear unit (ReLU) [17] activation, batch normalization (BN) [18], layer normalization (LN) [19], and MBGD algorithms with dynamical learning rate [20]\u2013[22] have been proposed.",
                "We have also observed from the above studies that MBGDbased optimization of TSK fuzzy systems is very sensitive to the choice of optimizers, such as Adam [8], [20], AdaBound [9], [21], and AdaBelief [10], [22]."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2f886ba92f96f0380ff0e9db90ad6f8e8152257b",
                "externalIds": {
                    "DBLP": "journals/tfs/CuiXPW23",
                    "DOI": "10.1109/TFUZZ.2022.3185464",
                    "CorpusId": 249975529
                },
                "corpusId": 249975529,
                "publicationVenue": {
                    "id": "c6b969a5-d295-4c65-bbe8-a2dc52990db8",
                    "name": "IEEE transactions on fuzzy systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE trans fuzzy syst",
                        "IEEE Trans Fuzzy Syst",
                        "IEEE Transactions on Fuzzy Systems"
                    ],
                    "issn": "1063-6706",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=91"
                },
                "url": "https://www.semanticscholar.org/paper/2f886ba92f96f0380ff0e9db90ad6f8e8152257b",
                "title": "Layer Normalization for TSK Fuzzy System Optimization in Regression Problems",
                "abstract": "Recently, mini-batch gradient descent (MBGD)-based optimization has become popular in Takagi\u2013Sugeno\u2013Kang (TSK) fuzzy system optimization. However, it suffers from some challenges, including the curse of dimensionality and the sensitivity to the choice of the optimizer. The former has been alleviated by our previously proposed high-dimensional TSK (HTSK) algorithm. In this article, we point out that the latter is caused by the gradient vanishing problem on the rule consequent parameters, which in turn is caused by the small magnitude of the normalized rule firing levels, especially when the number of rules is large. Thus, the rule consequents are easily trapped into a bad local minimum with poor generalization performance. We propose to use first layer normalization (LN) to amplify the small firing levels, and then rectified linear unit (ReLU) to discard rules far away from the current training sample. We evaluated our proposed HTSK-LN and HTSK-LN-ReLU on twelve regression datasets with various sizes and dimensionalities. Experiments demonstrated that they can significantly improve the generalization performance, regardless of the training set size, feature dimensionality, choice of the optimizer, and rulebase size.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145634433",
                        "name": "Yuqi Cui"
                    },
                    {
                        "authorId": "2129511382",
                        "name": "Yifan Xu"
                    },
                    {
                        "authorId": "2047944391",
                        "name": "Ruimin Peng"
                    },
                    {
                        "authorId": "144855927",
                        "name": "Dongrui Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Updates of parameters can be scaled individually by dividing by the exponential moving average of the variance of subsequent gradients, as it is done in the AdaBelief optimizer [20].",
                "We combine backtracking Nesterov accelerated gradient descent (NAGD) with dynamic scaling of individual directions (preconditioning) known from AdaBelief [20] to obtain an optimization scheme that converges significantly faster and reaches orders of magnitude lower residues than the conventional stochastic reconfiguration approach.",
                "We combine backtracking Nesterov accelerated gradient descent (NAGD) with dynamic scaling of individual directions (preconditioning) known from AdaBelief [19] to obtain an optimization scheme that converges significantly faster and reaches orders of magnitude lower residues than the conventional stochastic reconfiguration approach.",
                "Updates of parameters can be scaled individually by dividing by the exponential moving average of the variance of subsequent gradients, as it is done in the AdaBelief optimizer [19].",
                "[20] J. Zhuang, T. Tang, Y. Ding, S. Tatikonda, N. Dvornek, X. Papademetris, and J. S. Duncan, \u201cAdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients,\u201d Dec. 2020."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a0193555f94e7d0b1620b0e0cb3b7b6a50ded02e",
                "externalIds": {
                    "ArXiv": "2212.13453",
                    "DOI": "10.1103/PhysRevB.107.205102",
                    "CorpusId": 255185898
                },
                "corpusId": 255185898,
                "publicationVenue": {
                    "id": "52113867-f77b-4f26-a1cf-8e577dd325ea",
                    "name": "Physical review B",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev B",
                        "Phys Rev B",
                        "Physical Review B"
                    ],
                    "issn": "2469-9950",
                    "alternate_issns": [
                        "1098-0121",
                        "0556-2805"
                    ],
                    "url": "https://journals.aps.org/prb",
                    "alternate_urls": [
                        "https://journals.aps.org/prb/",
                        "http://journals.aps.org/prb/",
                        "http://prola.aps.org/",
                        "https://www.tib.eu/de/openurl/search?amp;DlicenseModel=nl&issn=1098-0121,0163-1829",
                        "http://prb.aps.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a0193555f94e7d0b1620b0e0cb3b7b6a50ded02e",
                "title": "Quantum transport in open spin chains using neural-network quantum states",
                "abstract": "In this work we study the treatment of asymmetric open quantum systems with neural networks based on the restricted Boltzmann machine. In particular, we are interested in the non-equilibrium steady state current in the boundary-driven (anisotropic) Heisenberg spin chain. We address previously published difficulties in treating asymmetric dissipative systems with neural-network quantum states and Monte-Carlo sampling and present an optimization method and a sampling technique that can be used to obtain high-fidelity steady state approximations of such systems. We point out some inherent symmetries of the Lindblad operator under consideration and exploit them during sampling. We show that local observables are not always a good indicator of the quality of the approximation and finally present results for the spin current that are in agreement with known results of simple open Heisenberg chains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2198486432",
                        "name": "Johannes Mellak"
                    },
                    {
                        "authorId": "47590666",
                        "name": "E. Arrigoni"
                    },
                    {
                        "authorId": "1730097",
                        "name": "T. Pock"
                    },
                    {
                        "authorId": "152587070",
                        "name": "W. von der Linden"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We train the neural network source term by using the AdaBelief [36] optimizer with a learning rate of 10\u22123 and 3, 000 epochs.",
                "The discrete corrective forcing term is again trained by using the AdaBelief [36] optimizer with a learning rate of 10\u22123, 100 batches, and 3000 epochs."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4ede7a857b1d994af377a38f06d5cb93a48fb7b0",
                "externalIds": {
                    "ArXiv": "2212.09967",
                    "DBLP": "journals/corr/abs-2212-09967",
                    "DOI": "10.48550/arXiv.2212.09967",
                    "CorpusId": 254877716
                },
                "corpusId": 254877716,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4ede7a857b1d994af377a38f06d5cb93a48fb7b0",
                "title": "Learning Subgrid-scale Models with Neural Ordinary Differential Equations",
                "abstract": "We propose a new approach to learning the subgrid-scale model when simulating partial differential equations (PDEs) solved by the method of lines and their representation in chaotic ordinary differential equations, based on neural ordinary differential equations (NODEs). Solving systems with fine temporal and spatial grid scales is an ongoing computational challenge, and closure models are generally difficult to tune. Machine learning approaches have increased the accuracy and efficiency of computational fluid dynamics solvers. In this approach neural networks are used to learn the coarse- to fine-grid map, which can be viewed as subgrid-scale parameterization. We propose a strategy that uses the NODE and partial knowledge to learn the source dynamics at a continuous level. Our method inherits the advantages of NODEs and can be used to parameterize subgrid scales, approximate coupling operators, and improve the efficiency of low-order solvers. Numerical results with the two-scale Lorenz 96 ODE, the convection-diffusion PDE, and the viscous Burgers' PDE are used to illustrate this approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111157969",
                        "name": "Shinhoo Kang"
                    },
                    {
                        "authorId": "35084316",
                        "name": "E. Constantinescu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, the parameters are updated using (13).\ns\u0302t = st 1 \u2212 \u03c6t1 and r\u0302t = rt 1 \u2212 \u03c6t2\n(12)\n\u03c7t+1 = \u03c7t \u2212 \u03b7\u221a r\u0302t + .s\u0302t (13)\n1 3\nA few optimization methods have been introduced recently, such as AdaBelief [26], MADGRAD [27], diffGrad [28], Gradient Centralization [29], and RADAM [30].",
                "AdaBelief tries to solve the stability problem raised in GAN (Generative Adversarial Network) using Adam by achieving more generalization, stability, and a fast convergence rate.",
                "A few optimization methods have been introduced recently, such as AdaBelief [26], MADGRAD [27], diffGrad [28], Gradient Centralization [29], and RADAM [30]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a4ab832d2b5a132b6d388aeaa9d2359615ad2223",
                "externalIds": {
                    "DBLP": "journals/apin/BhaktaNSGCP23",
                    "DOI": "10.1007/s10489-022-04382-7",
                    "CorpusId": 254845031
                },
                "corpusId": 254845031,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a4ab832d2b5a132b6d388aeaa9d2359615ad2223",
                "title": "DiffMoment: an adaptive optimization technique for convolutional neural network",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2142276339",
                        "name": "Shubhankar Bhakta"
                    },
                    {
                        "authorId": "144575541",
                        "name": "U. Nandi"
                    },
                    {
                        "authorId": "2124283",
                        "name": "Tapas Si"
                    },
                    {
                        "authorId": "67207146",
                        "name": "S. K. Ghosal"
                    },
                    {
                        "authorId": "2566794",
                        "name": "Chiranjit Changdar"
                    },
                    {
                        "authorId": "2053894892",
                        "name": "R. K. Pal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "From Table I, the proposed NadamSSM algorithm has the highest test set and training set accuracy on the VGG11 model, but AdaBelief and AdamSSM perform better on ResNet34.",
                "Specifically, all the parameter values of AdaBelief and AdamSSM are set as per the implementation in their papers [22], [28].",
                "We implement our NadamSSM algorithm in discretetime and compare its performance with AdaBelief [28], AdamSSM [22], and Nadam [5] algorithms for solving the following machine learning tasks: image classification on CIFAR-10 dataset [29], with ResNet34 [30] and VGG11 [31] models, and language modeling on Penn TreeBank (PTB) dataset [32], with 3-layer long short-term memory (LSTM) [33] model.",
                "We use the experimental setup as in the recent AdaBelief paper [28].",
                "For Nadam and NadamSSM, \u03bb1 = 0.67, \u03bb2 = 0.0067 are such that \u03b21 = (1 \u2212 \u03b4\u03bb1) and \u03b22 = (1 \u2212 \u03b4\u03bb2) are same as AdaBelief. \u03bb3 is chosen from {c \u00d7 10\u22123/\u03b4 : c = 1, 2, 3, 4, 5}.",
                "We note that when Nadam is better than AdaBelief/AdamSSM, NadamSSM significantly improves on Nadam.",
                "Similarly, on ResNet34, when Nadam is poorer than AdaBelief/AdamSSM, so is NadamSSM but with better performance than Nadam.",
                "The parameter \u03f5 and the learning rate \u03b7 are same as AdaBelief and AdamSSM.",
                "However, the experimental results in [28] have not compared AdaBelief with Nadam.",
                "1 at epoch 150; the mini-batch size is 128 [28].",
                "AdaBelief has been shown to be more efficient than the popular optimizers [28] on benchmark machine learning tasks.",
                "1 at epochs 100 and 145; the mini-batch size is 20 [28].",
                "Following [28], the l2-regularization hyperparameter is set to 5\u00d710\u22124 for image classification and 1."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9f6fb7a4d1f3209e7ee02444cd48be886ddf6148",
                "externalIds": {
                    "DOI": "10.1109/ICC56513.2022.10093397",
                    "CorpusId": 258136734
                },
                "corpusId": 258136734,
                "publicationVenue": {
                    "id": "ef3e0f36-d48c-476c-b829-4e1eab47c7a4",
                    "name": "International Conference on Intelligent Cloud Computing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Internet Thing Data Cloud Comput",
                        "International Conference on Internet of Things, Data, and Cloud Computing",
                        "Indian Control Conf",
                        "ICC",
                        "International Conference on Circuits, systems, electronics, control & signal processing",
                        "International Conference on Circuits",
                        "Int Conf Circuit",
                        "Indian Control Conference",
                        "Int Conf Circuit syst electron control  signal process",
                        "Int Conf Intell Cloud Comput"
                    ],
                    "url": "http://icc-conference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/9f6fb7a4d1f3209e7ee02444cd48be886ddf6148",
                "title": "A State-Space Perspective on the Expedited Gradient Methods: Nadam, RAdam, and Rescaled Gradient Flow",
                "abstract": "Fast gradient-descent algorithms are the default practice in training complex machine learning models. This paper presents the convergence guarantee of two existing adaptive gradient algorithms, Nadam and RAdam, for the first time, and the rescaled gradient flow in solving non-convex optimization. The analyses of all three algorithms are unified by a common underlying proof sketch, relying upon Barbalat's lemma. The utility of another tool from classical control, the transfer function, hitherto used to propose a new variant of the famous Adam optimizer, is extended in this paper for developing an improved variant of the Nadam algorithm. Our experimental results validate the efficiency of this proposed algorithm for solving benchmark machine learning problems in a shorter time and with enhanced accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1768752581",
                        "name": "Kushal Chakrabarti"
                    },
                    {
                        "authorId": "145229889",
                        "name": "N. Chopra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Consistent with the practical implementation of AdaBound [15] and other adaptive gradient algorithms [14], [21], our analysis considers the bias correction steps.",
                "MAdam has been demonstrated to be more efficient than the AdaBelief optimizer [16], which in turn is superior to the other existing optimizers [21], on several machine learning tasks.",
                "We define a positive valued function h : [0,\u221e) \u2192 R>0, which signifies the initial bias correction term, as is used in the practice while implementing AdaBound or any other adaptive gradient algorithms [15], [21]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "cfda8377748243f421c3be7c4ce52f0fe912bc90",
                "externalIds": {
                    "DBLP": "conf/cdc/ChakrabartiC22",
                    "DOI": "10.1109/CDC51059.2022.9992512",
                    "CorpusId": 255599014
                },
                "corpusId": 255599014,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/cfda8377748243f421c3be7c4ce52f0fe912bc90",
                "title": "Analysis and Synthesis of Adaptive Gradient Algorithms in Machine Learning: The Case of AdaBound and MAdamSSM",
                "abstract": "Adaptive gradient algorithms have become the prevalent tool in training complex neural networks; recent examples include AdaBound and MAdam. For a better understanding of such existing optimization algorithms and design ideas for new algorithms, well-known tools from classical control appear to be promising. This area of research is built upon modeling optimization algorithms as closed-loop dynamical systems. Consequently, this paper exploits a control-theoretic methodology in analyzing AdaBound, a recent adaptive gradient algorithm, and proposing a novel optimizer for machine learning. Specifically, inspired by the recently developed state-space perspective in the G-AdaGrad and the Adam algorithm, we present a simple convergence analysis of the AdaBound algorithm for non-convex optimization problems. Next, we propose a new variant of the MAdam algorithm upon applying the concept of transfer functions. Our experimental results demonstrate the efficiency of the proposed algorithm in training CNN models for image classification problems. The findings in this paper suggest further exploration of the existing tools from control theory in complex machine learning problems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1768752581",
                        "name": "Kushal Chakrabarti"
                    },
                    {
                        "authorId": "145229889",
                        "name": "N. Chopra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use the improved baseline and the SWIN-UNETR transformer versions with repeat_interleave and channel_conv, trained with AdamW and AdaBelief optimizers.",
                "Optimizer Following the previous winning solution [13], we conduct experiments with the AdaBelief [14] other than just the AdamW [15] optimizer."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f883f14e46152ce8a2391abb1b666578a3459826",
                "externalIds": {
                    "ArXiv": "2212.02456",
                    "DBLP": "journals/corr/abs-2212-02456",
                    "DOI": "10.48550/arXiv.2212.02456",
                    "CorpusId": 254246945
                },
                "corpusId": 254246945,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f883f14e46152ce8a2391abb1b666578a3459826",
                "title": "Solving the Weather4cast Challenge via Visual Transformers for 3D Images",
                "abstract": "Accurately forecasting the weather is an important task, as many real-world processes and decisions depend on future meteorological conditions. The NeurIPS 2022 challenge entitled Weather4cast poses the problem of predicting rainfall events for the next eight hours given the preceding hour of satellite observations as a context. Motivated by the recent success of transformer-based architectures in computer vision, we implement and propose two methodologies based on this architecture to tackle this challenge. We \ufb01nd that ensembling different transformers with some baseline models achieves the best performance we could measure on the unseen test data. Our approach has been ranked 3 rd in the competition.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2106191576",
                        "name": "Yury Belousov"
                    },
                    {
                        "authorId": "2193556000",
                        "name": "Sergey Polezhaev"
                    },
                    {
                        "authorId": "1573571821",
                        "name": "Brian Pulfer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We selected a second optimizer: AdaBelief [18], a recent variant of Adam.",
                "Additionally, AdaBelief [18] is shown to be a noticeably better optimizer choice than Adam [19] for the training of low-resolution models."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6463e9d479685305e8860c37bc091c1729de14a8",
                "externalIds": {
                    "ArXiv": "2212.01420",
                    "CorpusId": 254246911
                },
                "corpusId": 254246911,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6463e9d479685305e8860c37bc091c1729de14a8",
                "title": "Obtaining transferable chemical insight from solving machine-learning classification problems: Thermodynamical properties prediction, atomic composition as good as Coulomb matrix",
                "abstract": "Machine learning (ML) can be used to construct surrogate models for the fast prediction of a property of interest. ML can thus be applied to chemical projects, where the usual experimentation or calculation techniques can take hours or days for just one sample. In this manner, the most promising candidate samples could be extracted from an extensive database and subjected to further in-depth analysis. Despite their broad applicability, it can be challenging to apply ML methods to a given chemical problem since a multitude of design decisions must be made, such as the molecular descriptor to use or the optimizer to train the model. Here we present a methodology for the meaningful exploration of a given molecular problem through classification experiments. This conceptually simple methodology results in transferable insight on the selected problem and can be used as a platform from which prediction difficulty is estimated, molecular representations are tested and refined, and more precise or ambitious projects can be undertaken. Physicochemical insight can also be obtained. This methodology is illustrated through the use of multiple molecular descriptors for the prediction of enthalpy, Gibbs' free energy, zero-point vibrational energy, and constant-volume calorific capacity of the molecules from the public database QM9 [Ramakrishnan2014] with 133,885 organic molecules. A noteworthy result is that for the classification problem we propose, the low-resolution descriptor `atomic composition' [Tchagang2019] can reach a classification rate almost on par with the high-resolution `sorted Coulomb matrix' [Rupp2012,Montavon2012,Hansen2013] ($>90\\%$), provided that an appropriate optimizer is used during training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2193555854",
                        "name": "Leon Alday-Toledo"
                    },
                    {
                        "authorId": "1402500659",
                        "name": "R. Bernal-Jaquez"
                    },
                    {
                        "authorId": "1404434350",
                        "name": "Sa\u00fal Zapotecas-Mart\u00ednez"
                    },
                    {
                        "authorId": "1397738525",
                        "name": "Jose L. Mendoza-Cortes"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5ecd7d96b3d6db0ad31a692245b21a89db356bbb",
                "externalIds": {
                    "DBLP": "journals/nn/KimWBHKJL23",
                    "DOI": "10.1016/j.neunet.2022.12.001",
                    "CorpusId": 254532279,
                    "PubMed": "36565690"
                },
                "corpusId": 254532279,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5ecd7d96b3d6db0ad31a692245b21a89db356bbb",
                "title": "Variable three-term conjugate gradient method for training artificial neural networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8363295",
                        "name": "Hansu Kim"
                    },
                    {
                        "authorId": "2141048872",
                        "name": "Chuxuan Wang"
                    },
                    {
                        "authorId": "2195274759",
                        "name": "Hyoseok Byun"
                    },
                    {
                        "authorId": "10335919",
                        "name": "Weifei Hu"
                    },
                    {
                        "authorId": "11869426",
                        "name": "Sanghyuk Kim"
                    },
                    {
                        "authorId": "2173432688",
                        "name": "Qing Jiao"
                    },
                    {
                        "authorId": "152240959",
                        "name": "T. Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Training is performed for 166K steps with AdaBelief optimizer [Zhuang et al., 2020] having learning rate 3e-3."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9386b49cc4ece5231997ed50b7372c38bff00062",
                "externalIds": {
                    "ArXiv": "2212.00121",
                    "DBLP": "journals/corr/abs-2212-00121",
                    "DOI": "10.48550/arXiv.2212.00121",
                    "CorpusId": 254125499
                },
                "corpusId": 254125499,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9386b49cc4ece5231997ed50b7372c38bff00062",
                "title": "Denoising Diffusion for Sampling SAT Solutions",
                "abstract": "Generating diverse solutions to the Boolean Satisfiability Problem (SAT) is a hard computational problem with practical applications for testing and functional verification of software and hardware designs. We explore the way to generate such solutions using Denoising Diffusion coupled with a Graph Neural Network to implement the denoising function. We find that the obtained accuracy is similar to the currently best purely neural method and the produced SAT solutions are highly diverse, even if the system is trained with non-random solutions from a standard solver.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2295280",
                        "name": "K\u0101rlis Freivalds"
                    },
                    {
                        "authorId": "3180278",
                        "name": "Sergejs Kozlovics"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Almost all modern adaptive learning rate optimizers and their variants (Duchi, Hazan, and Singer 2011; Zeiler 2012; Kingma and Ba 2015; Reddi, Kale, and Kumar 2018; Zhuang et al. 2020; Zou et al. 2019; Chen et al. 2022a,b, 2021; Zou et al. 2018), such as AdaGrad, AdaDelta, RMSProp, Adam, AMSGrad, etc.",
                "\u2026adaptive learning rate optimizers and their variants (Duchi, Hazan, and Singer 2011; Zeiler 2012; Kingma and Ba 2015; Reddi, Kale, and Kumar 2018; Zhuang et al. 2020; Zou et al. 2019; Chen et al. 2022a,b, 2021; Zou et al. 2018), such as AdaGrad, AdaDelta, RMSProp, Adam, AMSGrad, etc., can\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8522f9a7bc7c958b5f66373b47ff68e1833089d6",
                "externalIds": {
                    "DBLP": "conf/aaai/YangPWYSCXJG23",
                    "ArXiv": "2211.15055",
                    "DOI": "10.48550/arXiv.2211.15055",
                    "CorpusId": 254043876
                },
                "corpusId": 254043876,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8522f9a7bc7c958b5f66373b47ff68e1833089d6",
                "title": "AdaTask: A Task-aware Adaptive Learning Rate Approach to Multi-task Learning",
                "abstract": "Multi-task learning (MTL) models have demonstrated impressive results in computer vision, natural language processing, and recommender systems. Even though many approaches have been proposed, how well these approaches balance different tasks on each parameter still remains unclear. In this paper, we propose to measure the task dominance degree of a parameter by the total updates of each task on this parameter. Specifically, we compute the total updates by the exponentially decaying Average of the squared Updates (AU) on a parameter from the corresponding task. Based on this novel metric, we observe that many parameters in existing MTL methods, especially those in the higher shared layers, are still dominated by one or several tasks. The dominance of AU is mainly due to the dominance of accumulative gradients from one or several tasks. Motivated by this, we propose a Task-wise Adaptive learning rate approach, AdaTask in short, to separate the accumulative gradients and hence the learning rate of each task for each parameter in adaptive learning rate approaches (e.g., AdaGrad, RMSProp, and Adam). Comprehensive experiments on computer vision and recommender system MTL datasets demonstrate that AdaTask significantly improves the performance of dominated tasks, resulting SOTA average task-wise performance. Analysis on both synthetic and real-world datasets shows AdaTask balance parameters in every shared layer well.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151497321",
                        "name": "Enneng Yang"
                    },
                    {
                        "authorId": "12692416",
                        "name": "Junwei Pan"
                    },
                    {
                        "authorId": "2561964",
                        "name": "Ximei Wang"
                    },
                    {
                        "authorId": "2167708764",
                        "name": "Haibin Yu"
                    },
                    {
                        "authorId": "2172820082",
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "2192666804",
                        "name": "Xihua Chen"
                    },
                    {
                        "authorId": "2110394629",
                        "name": "Lei Xiao"
                    },
                    {
                        "authorId": "2109627059",
                        "name": "Jie Jiang"
                    },
                    {
                        "authorId": "38896551",
                        "name": "G. Guo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To obtain the convergence rate as adam family methods and the generalization ability as SGD family methods, [10] presents the AdaBelief algorithmwhichmodified fromAdam."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "82121b42fe9b30a0ef3d8d70144dc61a2ba01076",
                "externalIds": {
                    "DBLP": "conf/aiss/Qiu0WZL22",
                    "DOI": "10.1145/3573834.3574539",
                    "CorpusId": 255943060
                },
                "corpusId": 255943060,
                "publicationVenue": {
                    "id": "f7bde3b1-c0c0-45ef-ba58-e39fa8bbcdc1",
                    "name": "International Conference on Advanced Information Science and System",
                    "type": "conference",
                    "alternate_names": [
                        "AISS",
                        "Int Conf Adv Inf Sci Syst"
                    ],
                    "url": "https://dl.acm.org/conference/aiss"
                },
                "url": "https://www.semanticscholar.org/paper/82121b42fe9b30a0ef3d8d70144dc61a2ba01076",
                "title": "Layer-wise based Adabelief Optimization Algorithm for Deep Learning",
                "abstract": "For the optimization problem of deep learning, it is important to formulate a optimization method that can improve the convergence rate without sacrificing generalization ability. This paper proposes a layer-wise based Adabelief optimization algorithm to solve the deep learning optimization problems more efficiently. In the proposed algorithm, each layer of the deep neural network is set different learning rate appropriately in order to achieve a faster convergence rate. We also give the theorems that can guarantee the convergence property of Layer-wised AdaBelief method. Finally, we evaluate the effectiveness and efficiency of the proposed algorithm on experimental examples. Experimental results show that the converges speed of the layer-wised AdaBelief algorithm is the fastest compared with the mainstream algorithms. Besides, the new algorithm also maintaining an excellent convergence result in all numerical examples.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2201307425",
                        "name": "Zhiyong Qiu"
                    },
                    {
                        "authorId": "2149505731",
                        "name": "Zhenhua Guo"
                    },
                    {
                        "authorId": "2152723084",
                        "name": "Li Wang"
                    },
                    {
                        "authorId": "47827612",
                        "name": "Yaqian Zhao"
                    },
                    {
                        "authorId": "151482578",
                        "name": "Rengang Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Not only does this show good results [24,25,34,42] in practical applications but also the learning rate is required to decay in the theoretical convergence analysis, such as \u03b7t = 1/t [43], \u03b7t = 1/ \u221a t [42]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "36c27d7458506f1b6a26e7089ad9f801c9d10e66",
                "externalIds": {
                    "DOI": "10.3390/app122312023",
                    "CorpusId": 254010458
                },
                "corpusId": 254010458,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/36c27d7458506f1b6a26e7089ad9f801c9d10e66",
                "title": "A Scaling Transition Method from SGDM to SGD with 2ExpLR Strategy",
                "abstract": "In deep learning, the vanilla stochastic gradient descent (SGD) and SGD with heavy-ball momentum (SGDM) methods have a wide range of applications due to their simplicity and great generalization. This paper uses an exponential scaling method to realize a smooth and stable transition from SGDM to SGD, which combines the advantages of the fast training speed of SGDM and the accurate convergence of SGD (named TSGD). We also provide some theoretical results on the convergence of this algorithm. At the same time, we take advantage of the learning rate warmup strategy\u2019s stability and the learning rate decay strategy\u2019s high accuracy. A warmup\u2013decay learning rate strategy with double exponential functions is proposed (named 2ExpLR). The experimental results on different datasets for the proposed algorithms indicate that the accuracy is improved significantly and that the training is faster and more stable.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111861049",
                        "name": "Kun Zeng"
                    },
                    {
                        "authorId": "2116148461",
                        "name": "Jinlan Liu"
                    },
                    {
                        "authorId": "2115506330",
                        "name": "Zhixia Jiang"
                    },
                    {
                        "authorId": "7471931",
                        "name": "Dongpo Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3d317965691b2e6046c55cbf0f06d296b9f9e0de",
                "externalIds": {
                    "ArXiv": "2211.10824",
                    "PubMedCentral": "10373495",
                    "DOI": "10.1021/acs.jctc.3c00038",
                    "CorpusId": 253735266,
                    "PubMed": "37071815"
                },
                "corpusId": 253735266,
                "publicationVenue": {
                    "id": "05f4640f-1261-4186-8153-0cb50b1169b3",
                    "name": "Journal of Chemical Theory and Computation",
                    "type": "journal",
                    "alternate_names": [
                        "J Chem Theory Comput"
                    ],
                    "issn": "1549-9618",
                    "url": "http://pubs.acs.org/jctc",
                    "alternate_urls": [
                        "https://pubs.acs.org/journal/jctcce",
                        "http://pubs.acs.org/journals/jctcce/index.html",
                        "http://pubs.acs.org/journal/jctcce"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3d317965691b2e6046c55cbf0f06d296b9f9e0de",
                "title": "Hybrid Auxiliary Field Quantum Monte Carlo for Molecular Systems",
                "abstract": "We propose a quantum Monte Carlo approach to solve the many-body Schr\u00f6dinger equation for the electronic ground state. The method combines optimization from variational Monte Carlo and propagation from auxiliary field quantum Monte Carlo in a way that significantly alleviates the sign problem. In application to molecular systems, we obtain highly accurate results for configurations dominated by either dynamic or static electronic correlation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7378026",
                        "name": "Yixiao Chen"
                    },
                    {
                        "authorId": "2125538501",
                        "name": "Linfeng Zhang"
                    },
                    {
                        "authorId": "2255989739",
                        "name": "W. E"
                    },
                    {
                        "authorId": "2990747",
                        "name": "R. Car"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a710eacd1445681a430e50115e5b4af6b33d2ee3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-07059",
                    "ArXiv": "2211.07059",
                    "DOI": "10.48550/arXiv.2211.07059",
                    "CorpusId": 253510445
                },
                "corpusId": 253510445,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a710eacd1445681a430e50115e5b4af6b33d2ee3",
                "title": "Dealing with missing data using attention and latent space regularization",
                "abstract": "Most practical data science problems encounter missing data. A wide variety of solutions exist, each with strengths and weaknesses that depend upon the missingness-generating process. Here we develop a theoretical framework for training and inference using only observed variables enabling modeling of incomplete datasets without imputation. Using an information and measure-theoretic argument we construct models with latent space representations that regularize against the potential bias introduced by missing data. The theoretical properties of this approach are demonstrated empirically using a synthetic dataset. The performance of this approach is tested on 11 benchmarking datasets with missingness and 18 datasets corrupted across three missingness patterns with comparison against a state-of-the-art model and industry-standard imputation. We show that our proposed method overcomes the weaknesses of imputation methods and outperforms the current state-of-the-art.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1398770162",
                        "name": "J. Penny-Dimri"
                    },
                    {
                        "authorId": "1734286",
                        "name": "C. Bergmeir"
                    },
                    {
                        "authorId": "2190799569",
                        "name": "Julian Smith"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6893cba8d1e30dc23ead3a6379280c82c2b0bb87",
                "externalIds": {
                    "ArXiv": "2211.04422",
                    "DBLP": "journals/corr/abs-2211-04422",
                    "DOI": "10.48550/arXiv.2211.04422",
                    "CorpusId": 253397864
                },
                "corpusId": 253397864,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6893cba8d1e30dc23ead3a6379280c82c2b0bb87",
                "title": "Black Box Lie Group Preconditioners for SGD",
                "abstract": "A matrix free and a low rank approximation preconditioner are proposed to accelerate the convergence of stochastic gradient descent (SGD) by exploiting curvature information sampled from Hessian-vector products or finite differences of parameters and gradients similar to the BFGS algorithm. Both preconditioners are fitted with an online updating manner minimizing a criterion that is free of line search and robust to stochastic gradient noise, and further constrained to be on certain connected Lie groups to preserve their corresponding symmetry or invariance, e.g., orientation of coordinates by the connected general linear group with positive determinants. The Lie group's equivariance property facilitates preconditioner fitting, and its invariance property saves any need of damping, which is common in second-order optimizers, but difficult to tune. The learning rate for parameter updating and step size for preconditioner fitting are naturally normalized, and their default values work well in most situations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1926284928",
                        "name": "Xi-Lin Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model parameters were optimized using the NadaBelief optimizer (a combination of the Adabelief [87] and Nadam [17]), quantization-aware training was applied to improve the accuracy of the resulting INT8 model."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ea5d250ed2f5c620a8414dbd91535bddfd57025a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-05910",
                    "ArXiv": "2211.05910",
                    "DOI": "10.48550/arXiv.2211.05910",
                    "CorpusId": 253499289
                },
                "corpusId": 253499289,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ea5d250ed2f5c620a8414dbd91535bddfd57025a",
                "title": "Efficient and Accurate Quantized Image Super-Resolution on Mobile NPUs, Mobile AI & AIM 2022 challenge: Report",
                "abstract": "Image super-resolution is a common task on mobile and IoT devices, where one often needs to upscale and enhance low-resolution images and video frames. While numerous solutions have been proposed for this problem in the past, they are usually not compatible with low-power mobile NPUs having many computational and memory constraints. In this Mobile AI challenge, we address this problem and propose the participants to design an efficient quantized image super-resolution solution that can demonstrate a real-time performance on mobile NPUs. The participants were provided with the DIV2K dataset and trained INT8 models to do a high-quality 3X image upscaling. The runtime of all models was evaluated on the Synaptics VS680 Smart Home board with a dedicated edge NPU capable of accelerating quantized neural networks. All proposed solutions are fully compatible with the above NPU, demonstrating an up to 60 FPS rate when reconstructing Full HD resolution images. A detailed description of all models developed in the challenge is provided in this paper.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153319973",
                        "name": "Andrey D. Ignatov"
                    },
                    {
                        "authorId": "1732855",
                        "name": "R. Timofte"
                    },
                    {
                        "authorId": "50410597",
                        "name": "Maurizio Denna"
                    },
                    {
                        "authorId": "2119279897",
                        "name": "Abdelbadie Younes"
                    },
                    {
                        "authorId": "9482782",
                        "name": "G. Gankhuyag"
                    },
                    {
                        "authorId": "3815682",
                        "name": "Jingang Huh"
                    },
                    {
                        "authorId": "2190748258",
                        "name": "Myeong Kyun Kim"
                    },
                    {
                        "authorId": "2010973378",
                        "name": "Kihwan Yoon"
                    },
                    {
                        "authorId": "122873240",
                        "name": "H. Moon"
                    },
                    {
                        "authorId": "2190727978",
                        "name": "Seungho Lee"
                    },
                    {
                        "authorId": "1809767",
                        "name": "Yoonsik Choe"
                    },
                    {
                        "authorId": "1683954",
                        "name": "Jinwoo Jeong"
                    },
                    {
                        "authorId": "2001983",
                        "name": "Sungjei Kim"
                    },
                    {
                        "authorId": "1742300925",
                        "name": "M. Smyl"
                    },
                    {
                        "authorId": "1973073",
                        "name": "Tomasz Latkowski"
                    },
                    {
                        "authorId": "2042799660",
                        "name": "Pawe\u0142 Kubik"
                    },
                    {
                        "authorId": "2190692152",
                        "name": "Micha\u0142 Sokolski"
                    },
                    {
                        "authorId": "2115555102",
                        "name": "Yu Ma"
                    },
                    {
                        "authorId": "2004765823",
                        "name": "Jiahao Chao"
                    },
                    {
                        "authorId": "2109471726",
                        "name": "Zhou Zhou"
                    },
                    {
                        "authorId": "2183080603",
                        "name": "Hong-Xin Gao"
                    },
                    {
                        "authorId": "2005165736",
                        "name": "Zhen Yang"
                    },
                    {
                        "authorId": "2075414786",
                        "name": "Zhenbing Zeng"
                    },
                    {
                        "authorId": "2089175828",
                        "name": "Zhengyang Zhuge"
                    },
                    {
                        "authorId": "48161878",
                        "name": "LI"
                    },
                    {
                        "authorId": "2116275997",
                        "name": "Dan Zhu"
                    },
                    {
                        "authorId": "2115372604",
                        "name": "Mengdi Sun"
                    },
                    {
                        "authorId": "2142231644",
                        "name": "Ran Duan"
                    },
                    {
                        "authorId": "2145972384",
                        "name": "Yan Gao"
                    },
                    {
                        "authorId": "1471414455",
                        "name": "Lingshun Kong"
                    },
                    {
                        "authorId": "1476719517",
                        "name": "Long Sun"
                    },
                    {
                        "authorId": null,
                        "name": "Xiang Li"
                    },
                    {
                        "authorId": "2153649312",
                        "name": "X. Zhang"
                    },
                    {
                        "authorId": "1519062623",
                        "name": "Jiawei Zhang"
                    },
                    {
                        "authorId": "2118460875",
                        "name": "Yaqi Wu"
                    },
                    {
                        "authorId": "9416881",
                        "name": "Jin-shan Pan"
                    },
                    {
                        "authorId": "1564624035",
                        "name": "G. Yu"
                    },
                    {
                        "authorId": "2155115477",
                        "name": "Jin Zhang"
                    },
                    {
                        "authorId": "1884418505",
                        "name": "Feng Zhang"
                    },
                    {
                        "authorId": "2125041018",
                        "name": "Zhe Ma"
                    },
                    {
                        "authorId": "2108981136",
                        "name": "Hongbin Wang"
                    },
                    {
                        "authorId": "2110687038",
                        "name": "Hojin Cho"
                    },
                    {
                        "authorId": "2190478437",
                        "name": "Steve Kim"
                    },
                    {
                        "authorId": "2108840329",
                        "name": "Hua Li"
                    },
                    {
                        "authorId": "2004581377",
                        "name": "Yan Ma"
                    },
                    {
                        "authorId": "2114939732",
                        "name": "Ziwei Luo"
                    },
                    {
                        "authorId": "2110860410",
                        "name": "Youwei Li"
                    },
                    {
                        "authorId": "2116663081",
                        "name": "Lei Yu"
                    },
                    {
                        "authorId": "2162783717",
                        "name": "Zhihong Wen"
                    },
                    {
                        "authorId": "2143598284",
                        "name": "Qi Wu"
                    },
                    {
                        "authorId": "1934546",
                        "name": "Haoqiang Fan"
                    },
                    {
                        "authorId": "2108589268",
                        "name": "Shuaicheng Liu"
                    },
                    {
                        "authorId": "2107957726",
                        "name": "Lize Zhang"
                    },
                    {
                        "authorId": "27010311",
                        "name": "Zhikai Zong"
                    },
                    {
                        "authorId": "2112277106",
                        "name": "J. Kwon"
                    },
                    {
                        "authorId": "2178782744",
                        "name": "Junxi Zhang"
                    },
                    {
                        "authorId": "50651666",
                        "name": "Mengyuan Li"
                    },
                    {
                        "authorId": "2059146221",
                        "name": "N. Fu"
                    },
                    {
                        "authorId": "2044349862",
                        "name": "Guanchen Ding"
                    },
                    {
                        "authorId": "2115313367",
                        "name": "Han Zhu"
                    },
                    {
                        "authorId": "2117102773",
                        "name": "Zhen Chen"
                    },
                    {
                        "authorId": "2108550387",
                        "name": "Gen Li"
                    },
                    {
                        "authorId": null,
                        "name": "Yuanfan Zhang"
                    },
                    {
                        "authorId": "2110833025",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2109550584",
                        "name": "Dafeng Zhang"
                    },
                    {
                        "authorId": "82461476",
                        "name": "Neo Yang"
                    },
                    {
                        "authorId": "2219300757",
                        "name": "Fitz Liu"
                    },
                    {
                        "authorId": "2109827502",
                        "name": "Jerry Zhao"
                    },
                    {
                        "authorId": "2614944",
                        "name": "Mustafa Ayazoglu"
                    },
                    {
                        "authorId": "2167316936",
                        "name": "Bahri Batuhan Bilecen"
                    },
                    {
                        "authorId": "2057923373",
                        "name": "Shota Hirose"
                    },
                    {
                        "authorId": "2184190115",
                        "name": "Kasidis Arunruangsirilert"
                    },
                    {
                        "authorId": "2065214456",
                        "name": "Luo Ao"
                    },
                    {
                        "authorId": "28285629",
                        "name": "H. Leung"
                    },
                    {
                        "authorId": "2124195935",
                        "name": "Andrew Wei"
                    },
                    {
                        "authorId": "2146651773",
                        "name": "Jie Liu"
                    },
                    {
                        "authorId": "2146553963",
                        "name": "Qiang Liu"
                    },
                    {
                        "authorId": "2145102943",
                        "name": "Dahai Yu"
                    },
                    {
                        "authorId": "2112329810",
                        "name": "Ao Li"
                    },
                    {
                        "authorId": "143919201",
                        "name": "Lei Luo"
                    },
                    {
                        "authorId": "2115584357",
                        "name": "Ce Zhu"
                    },
                    {
                        "authorId": "15828122",
                        "name": "Seongmin Hong"
                    },
                    {
                        "authorId": "46755494",
                        "name": "Dongwon Park"
                    },
                    {
                        "authorId": "2116660205",
                        "name": "Joonhee Lee"
                    },
                    {
                        "authorId": "2190300843",
                        "name": "Byeong Hyun Lee"
                    },
                    {
                        "authorId": "2145524894",
                        "name": "Seunggyu Lee"
                    },
                    {
                        "authorId": "73176962",
                        "name": "Sengsub Chun"
                    },
                    {
                        "authorId": "92566531",
                        "name": "Ruiyuan He"
                    },
                    {
                        "authorId": "51436760",
                        "name": "Xuhao Jiang"
                    },
                    {
                        "authorId": "2170518157",
                        "name": "Haihang Ruan"
                    },
                    {
                        "authorId": "2107989630",
                        "name": "Xinjian Zhang"
                    },
                    {
                        "authorId": "2153466074",
                        "name": "Jing Liu"
                    },
                    {
                        "authorId": "2152650400",
                        "name": "Garas Gendy"
                    },
                    {
                        "authorId": "3252623",
                        "name": "Nabil Sabor"
                    },
                    {
                        "authorId": "2115103043",
                        "name": "J. Hou"
                    },
                    {
                        "authorId": "2067670136",
                        "name": "Guanghui He"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, we can generate adaptive matrices as in AdaBelief [Zhuang et al., 2020] algorithm,\ndefined as\nat = \u033atat\u22121 + (1 \u2212 \u033at) ( w\u0304t \u2212 w\u0304t0 )2 , At = diag( \u221a at + \u03c1), (8) bt = \u033atbt\u22121 + (1\u2212 \u033at)||v\u0304t \u2212 v\u0304t0 ||, Bt = (bt + \u03c1)Ip, (9)\nwhere t0 = t \u2212 q. Note that we can directly choose \u03b1t and \u03b2t\u2026",
                "For example, we can generate adaptive matrices as in AdaBelief [Zhuang et al., 2020] algorithm,\ndefined as\nat = \u033atat\u22121 + (1 \u2212 \u033at) ( w\u0304t \u2212 w\u0304t0 )2 , At = diag( \u221a at + \u03c1), (8) bt = \u033atbt\u22121 + (1\u2212 \u033at)||v\u0304t \u2212 v\u0304t0 ||, Bt = (bt + \u03c1)Ip, (9)\nwhere t0 = t \u2212 q. Note that we can directly choose \u03b1t and \u03b2t instead of \u033at to reduce the number of tuning parameters in our algorithm.",
                "To improve the generalization performance of Adam, recently some adaptive gradient methods such as AdamW [Loshchilov and Hutter, 2018] and AdaBelief [Zhuang et al., 2020] have been proposed.",
                "For example, we can generate adaptive matrices as in AdaBelief [Zhuang et al., 2020] algorithm,"
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f79da7185cd231412c1dd4c4f060b2b4d5f2e879",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-01122",
                    "ArXiv": "2211.01122",
                    "DOI": "10.48550/arXiv.2211.01122",
                    "CorpusId": 253255250
                },
                "corpusId": 253255250,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f79da7185cd231412c1dd4c4f060b2b4d5f2e879",
                "title": "Fast Adaptive Federated Bilevel Optimization",
                "abstract": "Bilevel optimization is a popular hierarchical model in machine learning, and has been widely applied to many machine learning tasks such as meta learning, hyperparameter learning and policy optimization. Although many bilevel optimization algorithms recently have been developed, few adaptive algorithm focuses on the bilevel optimization under the distributed setting. It is well known that the adaptive gradient methods show superior performances on both distributed and non-distributed optimization. In the paper, thus, we propose a novel adaptive federated bilevel optimization algorithm (i.e.,AdaFBiO) to solve the distributed bilevel optimization problems, where the objective function of Upper-Level (UL) problem is possibly nonconvex, and that of Lower-Level (LL) problem is strongly convex. Specifically, our AdaFBiO algorithm builds on the momentum-based variance reduced technique and local-SGD to obtain the best known sample and communication complexities simultaneously. In particular, our AdaFBiO algorithm uses the unified adaptive matrices to flexibly incorporate various adaptive learning rates to update variables in both UL and LL problems. Moreover, we provide a convergence analysis framework for our AdaFBiO algorithm, and prove it needs the sample complexity of $\\tilde{O}(\\epsilon^{-3})$ with communication complexity of $\\tilde{O}(\\epsilon^{-2})$ to obtain an $\\epsilon$-stationary point. Experimental results on federated hyper-representation learning and federated data hyper-cleaning tasks verify efficiency of our algorithm.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3057688",
                        "name": "Feihu Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "00535412d19639794028a204e8dcaa4819bb5096",
                "externalIds": {
                    "DBLP": "journals/nn/XiePW23",
                    "DOI": "10.1016/j.neunet.2022.11.018",
                    "CorpusId": 253653518,
                    "PubMed": "36450188"
                },
                "corpusId": 253653518,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/00535412d19639794028a204e8dcaa4819bb5096",
                "title": "A fractional gradient descent algorithm robust to the initial weights of multilayer perceptron",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "14770349",
                        "name": "Xue-mei Xie"
                    },
                    {
                        "authorId": "7805040",
                        "name": "Yi-fei Pu"
                    },
                    {
                        "authorId": "40859449",
                        "name": "Jian Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The training is done using the Adabelief optimizer [43],"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7bb155211e385f9545abcfa8e8a090ca439e9abe",
                "externalIds": {
                    "DOI": "10.1109/TENCON55691.2022.9978124",
                    "CorpusId": 254930715
                },
                "corpusId": 254930715,
                "publicationVenue": {
                    "id": "1ec5a55c-414e-4f79-b7dc-48e1dba3618e",
                    "name": "IEEE Region 10 Conference",
                    "type": "conference",
                    "alternate_names": [
                        "TENCON",
                        "Digit Process Appl",
                        "Digital Processing Applications",
                        "IEEE Reg 10 Conf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7bb155211e385f9545abcfa8e8a090ca439e9abe",
                "title": "Lightweight Networks for COVID-19 Detection from Chest X-Ray Images inside a Low-Tier Android Device",
                "abstract": "The efforts to inoculate majority of the population have been slower than expected and this is especially true for lower income countries. This problem has caused a lot of worries and further accentuates the importance of timely and effective mass testing considering the emergence of newer variants. The RT-PCR is still the gold standard diagnostic test for COVID-19 detection, but its limitations has led researchers and scientists to explore supplementary screening methods. One effective tool to consider is Chest X-Ray (CXR) imaging and combining it with deep learning has piqued attention from the artificial intelligence (AI) community. To further contribute to this research area, this work focuses on creating, evaluating, and comparing lightweight and mobile-phone-suitable COVID-detecting models. These transfer learning models together with their corresponding dynamic-range quantized versions are first tested according to their classification performance. Afterwards, the models are pushed in a low-tier phone to measure their resource consumption and inference timings. Results show that the utilization of EfficientNetB0 and MobileNetV3 (Small & Large) architectures for transfer learning without any quantization can produce at least 91 % overall average accuracy for 3-class classification scheme. For systems requiring more efficient models, using the quantized versions of the transfer learning models particularly with EfficientNetB0 and MobileNetV3Large as foundation can render at most 0.79 % accuracy loss but still show more than 95% f1-scores for the COVID-19 class.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149077011",
                        "name": "Dave Jammin A. Bacad"
                    },
                    {
                        "authorId": "9072054",
                        "name": "P. Abu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Various adaptive learning rates have emerged, including AdaGrad [11], RMSprop, AdaDelta [12], Adam [13], Nadam [14] and AdaBelief [15]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e53e668b1cc8e76be31a0e2b32db60b2aa7ef74c",
                "externalIds": {
                    "DOI": "10.1109/ICICML57342.2022.10009665",
                    "CorpusId": 255776516
                },
                "corpusId": 255776516,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e53e668b1cc8e76be31a0e2b32db60b2aa7ef74c",
                "title": "An optimization Strategy for Deep Neural Networks Training",
                "abstract": "Learning rate is one of the essential hyperparameters influencing the training process and the accuracy of deep neural networks. However, until now, it is challenging to determine an optimal learning rate. A large learning rate can accelerate the training process but may bring instability in the training process and miss the global optimum. In contrast, a small learning rate would be in a stable training process, but the training speed would be slow, and the training process is easy to fall into local optimum. In this paper, first, the impact of the learning rate is analyzed. It is found that a learning rate schedule should consist of two stages to take into account the speed and accuracy of the training process simultaneously. Based on this consideration, this paper proposes an improvement strategy of learning rate schedules: a two-stage integration strategy of a large fixed learning rate and a rapid decay learning rate. Second, the proposed strategy is applied to optimize a series of widely used learning rate settings. Extensive experiments on CIFAR-10 and CIFAR-100 datasets with VGG19, ResNets, ResNext, DenseNets, SeNet, and some other models demonstrate that the proposed strategy optimizes the learning rate settings, the performance of trained models is enhanced.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116518387",
                        "name": "Tingting Wu"
                    },
                    {
                        "authorId": "2134589428",
                        "name": "Peng Zeng"
                    },
                    {
                        "authorId": "50828784",
                        "name": "Chunhe Song"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "60d9595e19e48cfa72c53f2fadb13bdeb107315c",
                "externalIds": {
                    "DOI": "10.1109/ICCE-Asia57006.2022.9954726",
                    "CorpusId": 254101065
                },
                "corpusId": 254101065,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/60d9595e19e48cfa72c53f2fadb13bdeb107315c",
                "title": "Efficient Training of EfficientNetV2-S Using AdaBelief Optimizer",
                "abstract": "The EfficientNetV2 architectures have classification accuracies exceeding 80% on the Imagenet-1K/21K datasets. Here, the networks used to be trained using stochastic gradient descent with momentum ($SGD+M$) as the optimizer. Such a method is known to be effective but takes too much time, making it impractical to train the networks from scratch especially when computational power is limited. To address this, we provide a guide on how to use the AdaBelief Optimizer for training EfficientNetV2-S. Results show that, even without complex training configurations, using AdaBelief can lead the EfficientNetV2-S network to achieve a top-1 accuracy as high as 80% on the Imagenet-1K validation set within 80 epochs of training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2034264105",
                        "name": "Hyung-Joon Jeon"
                    },
                    {
                        "authorId": "2070842178",
                        "name": "Jaewook Jeon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief [42] has been proposed to obtain a good generalization by adopting the step size according to the \u2018-belief-\u2019 in the current gradient direction."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "aae2a1666616044032cc20f0aad08a0d079f1567",
                "externalIds": {
                    "DBLP": "journals/apin/VermaM23",
                    "DOI": "10.1007/s10489-022-04205-9",
                    "CorpusId": 253177033
                },
                "corpusId": 253177033,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/aae2a1666616044032cc20f0aad08a0d079f1567",
                "title": "WSAGrad: a novel adaptive gradient based method",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2895811",
                        "name": "Krutika Verma"
                    },
                    {
                        "authorId": "1907137",
                        "name": "Abyayananda Maiti"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We optimize (6) with AdaBelief [37] (\u03b21 = 0.9, \u03b22 = 0.999).",
                "We optimize (6) with AdaBelief [37] (\u03b21 = 0."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "107885b4b5eb88e3973810ba10bb1da0f49ee290",
                "externalIds": {
                    "ArXiv": "2210.13834",
                    "DBLP": "journals/corr/abs-2210-13834",
                    "DOI": "10.48550/arXiv.2210.13834",
                    "CorpusId": 253107781,
                    "PubMed": "37656651"
                },
                "corpusId": 253107781,
                "publicationVenue": {
                    "id": "e0cda45d-3074-4ac0-80b8-e5250df00b89",
                    "name": "IEEE Transactions on Medical Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Med Imaging"
                    ],
                    "issn": "0278-0062",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=42",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/107885b4b5eb88e3973810ba10bb1da0f49ee290",
                "title": "Stable deep MRI reconstruction using Generative Priors",
                "abstract": "Data-driven approaches recently achieved remarkable success in magnetic resonance imaging (MRI) reconstruction, but integration into clinical routine remains challenging due to a lack of generalizability and interpretability. In this paper, we address these challenges in a unified framework based on generative image priors. We propose a novel deep neural network based regularizer which is trained in a generative setting on reference magnitude images only. After training, the regularizer encodes higher-level domain statistics which we demonstrate by synthesizing images without data. Embedding the trained model in a classical variational approach yields high-quality reconstructions irrespective of the sub-sampling pattern. In addition, the model shows stable behavior when confronted with out-of-distribution data in the form of contrast variation. Furthermore, a probabilistic interpretation provides a distribution of reconstructions and hence allows uncertainty quantification. To reconstruct parallel MRI, we propose a fast algorithm to jointly estimate the image and the sensitivity maps. The results demonstrate competitive performance, on par with state-of-the-art end-to-end deep learning methods, while preserving the flexibility with respect to sub-sampling patterns and allowing for uncertainty quantification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1897006386",
                        "name": "Martin Zach"
                    },
                    {
                        "authorId": "3597472",
                        "name": "F. Knoll"
                    },
                    {
                        "authorId": "1730097",
                        "name": "T. Pock"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "8805bba460886ce60240f47a143946c80346dbc9",
                "externalIds": {
                    "ArXiv": "2210.12067",
                    "DBLP": "journals/corr/abs-2210-12067",
                    "DOI": "10.48550/arXiv.2210.12067",
                    "CorpusId": 253080373
                },
                "corpusId": 253080373,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8805bba460886ce60240f47a143946c80346dbc9",
                "title": "Efficient Dataset Distillation Using Random Feature Approximation",
                "abstract": "Dataset distillation compresses large datasets into smaller synthetic coresets which retain performance with the aim of reducing the storage and computational burden of processing the entire dataset. Today's best-performing algorithm, \\textit{Kernel Inducing Points} (KIP), which makes use of the correspondence between infinite-width neural networks and kernel-ridge regression, is prohibitively slow due to the exact computation of the neural tangent kernel matrix, scaling $O(|S|^2)$, with $|S|$ being the coreset size. To improve this, we propose a novel algorithm that uses a random feature approximation (RFA) of the Neural Network Gaussian Process (NNGP) kernel, which reduces the kernel matrix computation to $O(|S|)$. Our algorithm provides at least a 100-fold speedup over KIP and can run on a single GPU. Our new method, termed an RFA Distillation (RFAD), performs competitively with KIP and other dataset condensation algorithms in accuracy over a range of large-scale datasets, both in kernel regression and finite-width network training. We demonstrate the effectiveness of our approach on tasks involving model interpretability and privacy preservation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2028788047",
                        "name": "Noel Loo"
                    },
                    {
                        "authorId": "8252176",
                        "name": "Ramin M. Hasani"
                    },
                    {
                        "authorId": "2056330",
                        "name": "Alexander Amini"
                    },
                    {
                        "authorId": "2064605577",
                        "name": "Daniela Rus"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Instead of using the Adam optimizer, we've gone with its AdaBelief alternative here [17]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "56e7082049cfb3b02291ea4fc326054fa601ce22",
                "externalIds": {
                    "DOI": "10.1109/ICSESS54813.2022.9930271",
                    "CorpusId": 253252752
                },
                "corpusId": 253252752,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/56e7082049cfb3b02291ea4fc326054fa601ce22",
                "title": "Ethnic Costume Grayscale Image Coloring Method with Improved Pix2Pix",
                "abstract": "Grayscale image coloring is challenging for high-resolution images of ethnic costume class, and current coloring methods are mainly applicable to low-resolution images, which pay less enough attention to local regions. While ethnic costume images are characterized by rich semantic information, diverse colors and high resolution, the original methods are difficult to show good results on ethnic costume dataset and prone to problems such as inaccurate coloring in local areas. In this paper, we propose a method of colorization that combining global features and attention mechanism with Pix2Pix, by injecting grayscale map into different layers of the generator as global features, and accelerating the speed of the convergence by the attention mechanism. The experimental results show that the proposed method has better coloring effect on ethnic costume images and could generate higher quality images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2189473639",
                        "name": "Xin Tang"
                    },
                    {
                        "authorId": "143683856",
                        "name": "Bin Wen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e1c25e1027ee6271126ef7e1c409e9700990bc26",
                "externalIds": {
                    "ArXiv": "2210.11275",
                    "CorpusId": 253383648
                },
                "corpusId": 253383648,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e1c25e1027ee6271126ef7e1c409e9700990bc26",
                "title": "Causal Structural Hypothesis Testing and Data Generation Models",
                "abstract": "A vast amount of expert and domain knowledge is captured by causal structural priors, yet there has been little research on testing such priors for generalization and data synthesis purposes. We propose a novel model architecture, Causal Structural Hypothesis Testing, that can use nonparametric, structural causal knowledge and approximate a causal model\u2019s functional relationships using deep neural networks. We use these architectures for comparing structural priors, akin to hypothesis testing, using a deliberate (non-random) split of training and testing data. Extensive simulations demonstrate the effectiveness of out-of-distribution generalization error as a proxy for causal structural prior hypothesis testing and offers a statistical baseline for interpreting results. We show that the variational version of the architecture, Causal Structural Variational Hypothesis Testing can improve performance in low SNR regimes. Due to the simplicity and low parameter count of the models, practitioners can test and compare structural prior hypotheses on small dataset and use the priors with the best generalization capacity to synthesize much larger, causally-informed datasets. Finally, we validate our methods on a synthetic pendulum dataset, and show a use-case on a real-world trauma surgery ground-level falls dataset. Our code is available on GitHub. 2",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1807735",
                        "name": "Jeffrey Q. Jiang"
                    },
                    {
                        "authorId": "2164382932",
                        "name": "Omead Brandon Pooladzandi"
                    },
                    {
                        "authorId": "2055252526",
                        "name": "Sunay Bhat"
                    },
                    {
                        "authorId": "144223987",
                        "name": "G. Pottie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Adabelief was designed for quick convergence of results in the sense of adaptive methods, favorable generalization similar to that of SGD which leads to greater accuracy, as well as training stability.",
                "The current model was established premised on k-fold cross-validation sampling and optimized with the Adabelief optimizer concept.",
                "Since adaptive methods usually converge faster than stochastic gradient descent (SGD), a variant of the Adam optimizer, the Adabelief [26], was used to optimize the final model."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "72e4cd6df7669772e2d4f4132410dc0b076f4bcd",
                "externalIds": {
                    "DOI": "10.1109/HealthCom54947.2022.9982781",
                    "CorpusId": 254930473
                },
                "corpusId": 254930473,
                "publicationVenue": {
                    "id": "0846f41f-c23f-4c3e-9237-673180f1c44d",
                    "name": "International Conference on e-Health Networking, Applications and Services",
                    "type": "conference",
                    "alternate_names": [
                        "HealthCom",
                        "International Conference on E-health Networking, Application & Services",
                        "Int Conf E-health Netw Appl  Serv",
                        "Healthcom",
                        "Int Conf e-health Netw Appl Serv"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/72e4cd6df7669772e2d4f4132410dc0b076f4bcd",
                "title": "Towards Artificial Intelligence-enabled Medical Pre-operative Airway Assessment",
                "abstract": "For surgeries which require general anesthesia, airway management is imperative. Difficult airway, which inhibits proper intubation, can be fatal. As such, pre-operative airway assessments are conducted by clinicians to determine the ease of intubation as well as to identify patients with difficult airway. To improve the process, artificial intelligence (AI) methods can be employed to predict such difficult airway situations so that suitable preparations can be made beforehand. However, due to the need for explainability of AI models required by healthcare regulations, typical black box models which work best with most data-driven AI methods cannot be used. Therefore, in the current work, a machine learning model has been established to predict the specific medical facial landmarks that are currently used by clinicians. These include the eyes, mentum, thyroid notch, suprasternal notch, forehead, tragus and radix. The model is based on convolutional neural network and a practical facial landmark detector concept. Furthermore, k-fold cross-validation sampling and the Adabelief optimizer have been utilized. The model prediction results display accurate prediction of the features, with the testing loss exhibiting good stability and maintaining well below 0.01 throughout. Attributed to that, the current model can lead to meaningful diagnosis of difficult airway during airway assessments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39328246",
                        "name": "Qinjie Lin"
                    },
                    {
                        "authorId": "3079944",
                        "name": "Chin-Boon Chng"
                    },
                    {
                        "authorId": "90078912",
                        "name": "J. Too"
                    },
                    {
                        "authorId": "2144143539",
                        "name": "Jinshuo Zhang"
                    },
                    {
                        "authorId": "49957662",
                        "name": "Haobing Liu"
                    },
                    {
                        "authorId": "46214187",
                        "name": "T. Foong"
                    },
                    {
                        "authorId": "2197600972",
                        "name": "Will Loh"
                    },
                    {
                        "authorId": "2148657",
                        "name": "C. Chui"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "21d7bd089a60ba9b59e60cb5f268fc9954baa95a",
                "externalIds": {
                    "DBLP": "conf/icip/TaiFLL22",
                    "DOI": "10.1109/ICIP46576.2022.9897974",
                    "CorpusId": 253338267
                },
                "corpusId": 253338267,
                "publicationVenue": {
                    "id": "b6369c33-5d70-463c-8e82-95a54efa3cc8",
                    "name": "International Conference on Information Photonics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Image Process",
                        "ICIP",
                        "Int Conf Inf Photonics",
                        "International Conference on Image Processing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/21d7bd089a60ba9b59e60cb5f268fc9954baa95a",
                "title": "Higher-Order Recurrent Network with Space-Time Attention for Video Early Action Recognition",
                "abstract": "Endowing visual agents with predictive capability is a key step towards video intelligence at scale. Early action recognition aims to predict the action labels before fully observing the complete video frames. Unlike action recognition, the model is asked to forecast the future or the effects by only observing the initial few frames. The strong reasoning ability over the temporal dimension is the key to success. To this end, in this paper, we propose a novel recurrent network with decomposed space-time attention and higher-order design to capture the temporal dependency associated with the specific actions. Our method achieves state-of-the-art performance on Something-Something and EPIC-Kitchens datasets under the early action recognition setting, showing evidence of predictive capability that we attribute to our higher-order recurrent design with space-time attention.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27084141",
                        "name": "Tsung-Ming Tai"
                    },
                    {
                        "authorId": "3144258",
                        "name": "G. Fiameni"
                    },
                    {
                        "authorId": "2115293259",
                        "name": "Cheng-Kuang Lee"
                    },
                    {
                        "authorId": "1717522",
                        "name": "O. Lanz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the training of GenericADMM-Net, the input size, convolution filter size, number of filters, number of stages, number of epochs, and optimizer were 256 \u00d7 256, 5 \u00d7 5, 128, 10, 300, and AdaBelief [23], respectively."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9c72abaaa1fddf64708a6a8bb25fc75570ead634",
                "externalIds": {
                    "DBLP": "conf/icip/YamatoI22",
                    "DOI": "10.1109/ICIP46576.2022.9897337",
                    "CorpusId": 253346768
                },
                "corpusId": 253346768,
                "publicationVenue": {
                    "id": "b6369c33-5d70-463c-8e82-95a54efa3cc8",
                    "name": "International Conference on Information Photonics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Image Process",
                        "ICIP",
                        "Int Conf Inf Photonics",
                        "International Conference on Image Processing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9c72abaaa1fddf64708a6a8bb25fc75570ead634",
                "title": "Super-Resolution Magnetic Resonance Imaging using Segmented Signals in Phase-Scrambling Fourier Transform Imaging and Deep Learning",
                "abstract": "We propose an image acquisition and reconstruction method based on segment-wise signal sampling and skipping. Unrolling model-based deep learning reconstruction is used to improve the quality of reconstructed images and reduce the reconstruction time. Simulation experiments show that the skipped signals were reconstructed in the iterative reconstruction based on a convolutional neural network and that the evaluation scores of reconstructed images were improved for unsegmented band-limited signals. The proposed method is applied to an experimentally obtained phase-scrambling Fourier transform signal to demonstrate its effectiveness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2069623090",
                        "name": "Kazuki Yamato"
                    },
                    {
                        "authorId": "2107254610",
                        "name": "S. Ito"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", Adam [16], diffGrad [5], Radam [20] and AdaBelief [32]) for the results comparison by applying the proposed concept with these optimizers.",
                "Typical scenarios depicting the importance of adaptive parameter update in optimization [32].",
                "on the variance threshold; and AdaBelief [32] considers the EMA of square of difference between the gradient and first order moment (i.",
                "The Adam optimizer suffers near the minimum due to high moment leading to overshooting of minimum and oscillation near minimum [5], [20], [32].",
                "However, in order to show the generalization of the gradient norm correction approach, we also integrate it with the recent state-of-the-art optimizers, including diffGrad [5], Radam [20] and AdaBelief [32] optimizers and propose diffGradNorm, RadamNorm and AdaBeliefNorm optimizers, respectively.",
                ", O( \u221a T )) [16], [5], [20], [32], which is computed in the worst possible case.",
                "We use the proposed AdaNorm with Adam [16], diffGrad [5], Radam [20] and AdaBelief [32] optimizers and Algorithm 2: AdamNorm Optimizer"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b09982567afef0961b9509d71cec9ba82ed562f9",
                "externalIds": {
                    "DBLP": "conf/wacv/DubeySC23",
                    "ArXiv": "2210.06364",
                    "DOI": "10.1109/WACV56688.2023.00525",
                    "CorpusId": 252846704
                },
                "corpusId": 252846704,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/b09982567afef0961b9509d71cec9ba82ed562f9",
                "title": "AdaNorm: Adaptive Gradient Norm Correction based Optimizer for CNNs",
                "abstract": "The stochastic gradient descent (SGD) optimizers are generally used to train the convolutional neural networks (CNNs). In recent years, several adaptive momentum based SGD optimizers have been introduced, such as Adam, diffGrad, Radam and AdaBelief. However, the existing SGD optimizers do not exploit the gradient norm of past iterations and lead to poor convergence and performance. In this paper, we propose a novel AdaNorm based SGD optimizers by correcting the norm of gradient in each iteration based on the adaptive training history of gradient norm. By doing so, the proposed optimizers are able to maintain high and representive gradient throughout the training and solves the low and atypical gradient problems. The proposed concept is generic and can be used with any existing SGD optimizer. We show the efficacy of the proposed AdaNorm with four state-of-the-art optimizers, including Adam, diffGrad, Radam and AdaBelief. We depict the performance improvement due to the proposed optimizers using three CNN models, including VGG16, ResNet18 and ResNet50, on three benchmark object recognition datasets, including CIFAR10, CIFAR100 and TinyImageNet.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "34992579",
                        "name": "S. Dubey"
                    },
                    {
                        "authorId": "2108384213",
                        "name": "S. Singh"
                    },
                    {
                        "authorId": "1759420",
                        "name": "B. Chaudhuri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "But Transformer architectures suffer from unstable training due to the large gradient variance [31, 61] in the Adam optimizer [26], so have to resort to the warmup trick, which adopts a small learning rate at initialization to stabilize the training process.",
                "It is in line with prior observations that the initial gradient variance should be small to enable a large learning rate [31, 61]."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "409228e539f5c34025a38a7de4abe2a089a7bb15",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-05956",
                    "ArXiv": "2210.05956",
                    "DOI": "10.48550/arXiv.2210.05956",
                    "CorpusId": 252846428
                },
                "corpusId": 252846428,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/409228e539f5c34025a38a7de4abe2a089a7bb15",
                "title": "Towards Theoretically Inspired Neural Initialization Optimization",
                "abstract": "Automated machine learning has been widely explored to reduce human efforts in designing neural architectures and looking for proper hyperparameters. In the domain of neural initialization, however, similar automated techniques have rarely been studied. Most existing initialization methods are handcrafted and highly dependent on specific architectures. In this paper, we propose a differentiable quantity, named GradCosine, with theoretical insights to evaluate the initial state of a neural network. Specifically, GradCosine is the cosine similarity of sample-wise gradients with respect to the initialized parameters. By analyzing the sample-wise optimization landscape, we show that both the training and test performance of a network can be improved by maximizing GradCosine under gradient norm constraint. Based on this observation, we further propose the neural initialization optimization (NIO) algorithm. Generalized from the sample-wise analysis into the real batch setting, NIO is able to automatically look for a better initialization with negligible cost compared with the training time. With NIO, we improve the classification performance of a variety of neural architectures on CIFAR-10, CIFAR-100, and ImageNet. Moreover, we find that our method can even help to train large vision Transformer architecture without warmup.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46285967",
                        "name": "Yibo Yang"
                    },
                    {
                        "authorId": null,
                        "name": "Hong Wang"
                    },
                    {
                        "authorId": "2024825871",
                        "name": "Haobo Yuan"
                    },
                    {
                        "authorId": "33383055",
                        "name": "Zhouchen Lin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The model is trained on a batch size of 128 and 100 epochs, and to optimize the parameters of the model, we use Adam [16] and AdaBelief [17] as optimizers for the Twitter and Weibo datasets."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0bf17743e9516e7abeb9083b852ef7120e43038e",
                "externalIds": {
                    "DBLP": "conf/smc/SuSQWYZ22",
                    "DOI": "10.1109/SMC53654.2022.9945560",
                    "CorpusId": 253630028
                },
                "corpusId": 253630028,
                "publicationVenue": {
                    "id": "e84bb5a1-8f79-42cc-8eb1-3a52f7c73d63",
                    "name": "IEEE International Conference on Systems, Man and Cybernetics",
                    "type": "conference",
                    "alternate_names": [
                        "Smoky Mountains Computational Sciences and Engineering Conference",
                        "Smoky Mt Comput Sci Eng Conf",
                        "IEEE Int Conf Syst Man Cybern",
                        "SMC",
                        "Syst Man Cybern",
                        "Systems, Man and Cybernetics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0bf17743e9516e7abeb9083b852ef7120e43038e",
                "title": "Research on multi-level image-text fusion method for rumor detection",
                "abstract": "In recent years, public opinion events such as \u201cfake news\u201d and \u201cnews reversal\u201d have occurred frequently, and spreading rumors through images has become a new form of rumor circulating in the digital age. Most of the existing methods only consider the text content, ignoring the role of the information in the additional images; for the fusion between multiple modalities, their adequate information cannot be fully utilized, and the graphic and text information have not fully interacted. Therefore, we propose a multi-level image-text fusion method (MLFRD), which can effectively obtain local and global information about events, improve the connection between text and images, and improve the performance of rumor detection. MLFRD consists of three parts, a multimodal feature extractor to extract textual and visual features from posts, the extracted features are sent to a multilevel feature fusion network for efficient fusion, and finally to a rumor detector for rumor discrimination. We conduct extensive experiments on two real datasets, and MLFRD can better fuse features between multiple modalities for rumor detection and outperform state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2181226057",
                        "name": "Mengli Su"
                    },
                    {
                        "authorId": "2152305686",
                        "name": "Tao Sun"
                    },
                    {
                        "authorId": "2181209998",
                        "name": "Zhibang Quan"
                    },
                    {
                        "authorId": "2181683227",
                        "name": "Jishu Wei"
                    },
                    {
                        "authorId": "2087076158",
                        "name": "Xinyan Yin"
                    },
                    {
                        "authorId": "2191243411",
                        "name": "Shenjie Zhong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ADABELIEF [20] replaces the secondorder estimate with their own defined \u201cbelief\u201d.",
                "We find that PWPROP consistently outperforms existing SOTA solvers including ADAM, AMSGRAD, ADAMW, ADAMP, NOSADAM, RADAM and ADABELIEF on various perturbations, e.g., L2-norm perturbation level \u03be = 0.0157.",
                "We slightly tune the hyperparameters in PWPROP according to the suggestions in [20].",
                "We also compare our PWPROP algorithm with many state-ofthe-art solvers, such as SGDM, ADAM, AMSGRAD, ADAMP [21], NOSADAM, RADAM and ADABELIEF."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "291f0279a12aab82e182225639016771921439d2",
                "externalIds": {
                    "DBLP": "conf/ictai/WangXZSLLS22",
                    "DOI": "10.1109/ICTAI56018.2022.00081",
                    "CorpusId": 258220448
                },
                "corpusId": 258220448,
                "publicationVenue": {
                    "id": "ba1e9488-a629-4abe-a0c2-ec2c79c91616",
                    "name": "IEEE International Conference on Tools with Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Tool Artif Intell",
                        "ICTAI",
                        "IEEE Int Conf Tool Artif Intell",
                        "International Conference on Tools with Artificial Intelligence"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1488"
                },
                "url": "https://www.semanticscholar.org/paper/291f0279a12aab82e182225639016771921439d2",
                "title": "PWPROP: A Progressive Weighted Adaptive Method for Training Deep Neural Networks",
                "abstract": "In recent years, adaptive optimization methods for deep learning have attracted considerable attention. AMSGRAD indicates that the adaptive methods may be hard to converge to optimal solutions of some convex problems due to the divergence of its adaptive learning rate as in ADAM. However, we find that AMSGRAD may generalize worse than ADAM for some deep learning tasks. We first show that AMSGRAD may not find a flat minimum. So how can we design an optimization method to find a flat minimum with low training loss? Few works focus on this important problem. We propose a novel progressive weighted adaptive optimization algorithm, called PWPROP, with fewer hyperparameters than its counterparts such as ADAM. By intuitively constructing a \u201csharp-flat minima\u201d model, we show that how different second-order estimates affect the ability to escape a sharp minimum. Moreover, we also prove that PWPROP can address the non-convergence issue of ADAM and has a sublinear convergence rate for non-convex problems. Extensive experimental results show that PWPROP is effective and suitable for various deep learning architectures such as Transformer, and achieves state-of-the-art results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47859450",
                        "name": "D. Wang"
                    },
                    {
                        "authorId": "2148847430",
                        "name": "Tao Xu"
                    },
                    {
                        "authorId": "2175300063",
                        "name": "Huatian Zhang"
                    },
                    {
                        "authorId": "3062185",
                        "name": "Fanhua Shang"
                    },
                    {
                        "authorId": "2115669537",
                        "name": "Hongying Liu"
                    },
                    {
                        "authorId": "35588611",
                        "name": "Yuanyuan Liu"
                    },
                    {
                        "authorId": "3493398",
                        "name": "Shengmei Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[56], Adafactor [57], diffGrad [58], AdaMod [59], and AdamP [60].",
                "[56], Adafactor [57], diffGrad [58], AdaMod [59], and AdamP [60] are used for solving it, and the parameter update equation is, \u0398n+1 = \u0398n \u2212 \u03b1n\u2207\u0398L (\u0398n) ."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "917af645497f278e5f4d331c5c1c7cca23cd9426",
                "externalIds": {
                    "DBLP": "journals/ton/LiYLW22",
                    "DOI": "10.1109/TNET.2022.3158987",
                    "CorpusId": 248028813
                },
                "corpusId": 248028813,
                "publicationVenue": {
                    "id": "b1aea3ab-edf0-430b-a9c2-cce5469f6b23",
                    "name": "IEEE/ACM Transactions on Networking",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE ACM Trans Netw",
                        "IEEE ACM Transactions on Networking",
                        "IEEE/ACM Trans Netw"
                    ],
                    "issn": "1063-6692",
                    "url": "http://portal.acm.org/ton/",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=90",
                        "https://ieeexplore.ieee.org/servlet/opac?punumber=90"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/917af645497f278e5f4d331c5c1c7cca23cd9426",
                "title": "Learning Quantum Drift-Diffusion Phenomenon by Physics-Constraint Machine Learning",
                "abstract": "Recently, deep learning (DL) is widely used to detect physical phenomena and has obtained encouraging results. Several works have shown that it can learn quantum phenomenon. Subsequently, quantum machine learning (QML) has been paid more attention by academia and industry. Quantum drift-diffusion (QDD) is a commonplace physical phenomenon, which is a macroscopic description of electrons and holes in a semiconductor. They are commonly used to attain an understanding of the property of semiconductor devices in physics and engineering. We are motivated by the relaxation-time limit from the quantum-Navier-Stokes-Poisson system (QNSP) to the QDD equation and the existence of finite energy weak solutions to the QDD equation has been proved. Therefore, in this work, the quantum drift-diffusion learning neural network (QDDLNN) is proposed to investigate the quantum drift phenomena from limited observations. Furthermore, a piece of numerical evidence is found that the NNs can describe quantum transport phenomena by simulating the quantum confinement transport equation-quantum Navier-Stokes equation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109639708",
                        "name": "Chun Li"
                    },
                    {
                        "authorId": "1758436",
                        "name": "Yunyun Yang"
                    },
                    {
                        "authorId": "2108951501",
                        "name": "Hui Liang"
                    },
                    {
                        "authorId": "1758916",
                        "name": "Boying Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief [9] is a version of Adam that adaptively caculates the variance value using the expected value of the gradient.",
                "\u201a We surveyed and analyzed the performance of several adaptive optimizers in the training of Deformable DETR [9] where AdaBelief was the optimal choice and achieved the highest results."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "bff2dcb61e685a0ba2646d84ba9db0b2d2dff73d",
                "externalIds": {
                    "DBLP": "conf/mapr/DinhBBLVN22",
                    "DOI": "10.1109/MAPR56351.2022.9924933",
                    "CorpusId": 253122853
                },
                "corpusId": 253122853,
                "publicationVenue": {
                    "id": "1e3b2c00-e4cd-4b5a-b5f1-23b9e4d77569",
                    "name": "International Conference on Multimedia Analysis and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "MAPR",
                        "International Conference Multimedia Analysis and Pattern Recognition",
                        "Int Conf Multimedia Anal Pattern Recognit"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bff2dcb61e685a0ba2646d84ba9db0b2d2dff73d",
                "title": "Performance Evaluation of Optimizers for Deformable-DETR in Natural Disaster Damage Assessment",
                "abstract": "Global natural disasters are becoming more frequent and severe as a result of climate change. Recent advances in computer vision, particularly deep learning-based techniques and unmanned aerial vehicle (UAV) remote sensing, can aid disaster response teams in assessing the damage. Prior methods appear to be ineffective or were designed with inductive biases, making them difficult to conduct during the disaster damage assessment. In this paper, we investigate deep-learning-based methods capable of rapidly assessing building damage that follows natural disasters. Furthermore, we examine Deformable DETR, which is an improvement upon DETR, an object detection method based on the Transformer architecture, in terms of efficiency and convergence time, while inheriting DETR\u2019s simple implementation and adaptable architecture, making it suitable for the task of damage detection. We also experimented and analyzed the performance of several optimizers to improve the performance of Deformable DETR.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2201683080",
                        "name": "Minh Dinh"
                    },
                    {
                        "authorId": "2188869647",
                        "name": "Vu L. Bui"
                    },
                    {
                        "authorId": "2146735404",
                        "name": "Doanh C. Bui"
                    },
                    {
                        "authorId": "2188869768",
                        "name": "Duong Phi Long"
                    },
                    {
                        "authorId": "2198372",
                        "name": "Nguyen D. Vo"
                    },
                    {
                        "authorId": "1399684830",
                        "name": "Khang Nguyen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b2c034d81ac1a85e41d64c098d9a3d268562724d",
                "externalIds": {
                    "DBLP": "journals/sigpro/ZhangBZ23",
                    "ArXiv": "2209.14671",
                    "DOI": "10.1016/j.sigpro.2023.109088",
                    "CorpusId": 252595655
                },
                "corpusId": 252595655,
                "publicationVenue": {
                    "id": "20298d29-1b7a-4631-8e1d-e0a5d34bcf58",
                    "name": "Signal Processing",
                    "type": "journal",
                    "alternate_names": [
                        "Signal Process"
                    ],
                    "issn": "0165-1684",
                    "url": "https://www.journals.elsevier.com/signal-processing",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/01651684"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b2c034d81ac1a85e41d64c098d9a3d268562724d",
                "title": "ELFPIE: an error-laxity Fourier ptychographic iterative engine",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35514632",
                        "name": "Shuhe Zhang"
                    },
                    {
                        "authorId": "145489810",
                        "name": "T. Berendschot"
                    },
                    {
                        "authorId": "2145788756",
                        "name": "Jinhua Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The samples were then fed into the CNN module which used the AdaBelief optimizer (Zhuang et al., 2020) with a learning rate of 1e-3 and the epsilon of 1e-7 to minimize the cross-entropy loss function."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a42ffd658610906402e6f47f8bd8243f274c49f8",
                "externalIds": {
                    "PubMedCentral": "9535340",
                    "DOI": "10.3389/fnagi.2022.945024",
                    "CorpusId": 252408226,
                    "PubMed": "36212045"
                },
                "corpusId": 252408226,
                "publicationVenue": {
                    "id": "31b2ea45-decb-4c4d-8e3e-683974781c91",
                    "name": "Frontiers in Aging Neuroscience",
                    "type": "journal",
                    "alternate_names": [
                        "Front Aging Neurosci"
                    ],
                    "issn": "1663-4365",
                    "url": "http://www.frontiersin.org/aging_neuroscience",
                    "alternate_urls": [
                        "https://www.frontiersin.org/journals/aging-neuroscience",
                        "http://www.frontiersin.org/agingneuroscience/",
                        "http://journal.frontiersin.org/journal/aging-neuroscience"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a42ffd658610906402e6f47f8bd8243f274c49f8",
                "title": "Electroencephalogram signals emotion recognition based on convolutional neural network-recurrent neural network framework with channel-temporal attention mechanism for older adults",
                "abstract": "Reminiscence and conversation between older adults and younger volunteers using past photographs are very effective in improving the emotional state of older adults and alleviating depression. However, we need to evaluate the emotional state of the older adult while conversing on the past photographs. While electroencephalogram (EEG) has a significantly stronger association with emotion than other physiological signals, the challenge is to eliminate muscle artifacts in the EEG during speech as well as to reduce the number of dry electrodes to improve user comfort while maintaining high emotion recognition accuracy. Therefore, we proposed the CTA-CNN-Bi-LSTM emotion recognition framework. EEG signals of eight channels (P3, P4, F3, F4, F7, F8, T7, and T8) were first implemented in the MEMD-CCA method on three brain regions separately (Frontal, Temporal, Parietal) to remove the muscle artifacts then were fed into the Channel-Temporal attention module to get the weights of channels and temporal points most relevant to the positive, negative and neutral emotions to recode the EEG data. A Convolutional Neural Networks (CNNs) module then extracted the spatial information in the new EEG data to obtain the spatial feature maps which were then sequentially inputted into a Bi-LSTM module to learn the bi-directional temporal information for emotion recognition. Finally, we designed four group experiments to demonstrate that the proposed CTA-CNN-Bi-LSTM framework outperforms the previous works. And the highest average recognition accuracy of the positive, negative, and neutral emotions achieved 98.75%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112654690",
                        "name": "Lei Jiang"
                    },
                    {
                        "authorId": "145325410",
                        "name": "Panote Siriaraya"
                    },
                    {
                        "authorId": "67142357",
                        "name": "Dongeun Choi"
                    },
                    {
                        "authorId": "150106435",
                        "name": "Fangmeng Zeng"
                    },
                    {
                        "authorId": "2070988125",
                        "name": "Noriaki Kuwahara"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, we mention here Adabelief [18] as an alternative to Adam which is believed to be more stable to noisy gradients, and have better generalization properties than Adam.",
                "Optimizers used We chose to test SGD, Clipped SGD, Adam and Adabelief.",
                "That is, in dimension 100, any optimizer that uses forward gradients are almost always at least 10 times slower than the best optimizer that uses true gradient (which almost always is Adabelief).",
                "Adabelief is obtained by replacing the second moment of Adam with the moving average of empirical variance, i. e.\nv\u0303t+1 = \u03b22v\u0303t + (1 \u2212 \u03b22)(g(\u03b8t) \u2212 mt)2\nThe rational for this update is that mt can be interpreted as a prevision for the gradient, and vt as our confidence in the current gradient sample with respect to what the prevision was.",
                "Adabelief appears to exhibit better robustness to gradient noise than Adam, which is of course highly desirable with forward gradients."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3993ff60d1dccf1d360c37e2b988716ee29bba7e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-06302",
                    "ArXiv": "2209.06302",
                    "DOI": "10.48550/arXiv.2209.06302",
                    "CorpusId": 252222514
                },
                "corpusId": 252222514,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3993ff60d1dccf1d360c37e2b988716ee29bba7e",
                "title": "Optimization without Backpropagation",
                "abstract": "Forward gradients have been recently introduced to bypass backpropagation in autodi \ufb00 erentiation, while retaining unbiased estimators of true gradients. We derive an optimality condition to obtain best approximating forward gradients, which leads us to mathematical insights that suggest optimization in high dimension is challenging with forward gradients. Our extensive experiments on test functions support this claim.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2184778438",
                        "name": "Gabriel Belouze"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The AdaBelief optimizer was outperformed by the Adam optimizer for all the training scenarios.",
                "We compared the commonly used Adam optimizer [19], Stochastic Gradient Descent (SGD), and the recently proposed AdaBelief [20]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6c9f9649dfe5e062642b19843251195a7fec82c5",
                "externalIds": {
                    "DBLP": "conf/icdl/SejnovaS22",
                    "DOI": "10.1109/ICDL53763.2022.9962185",
                    "CorpusId": 254098732
                },
                "corpusId": 254098732,
                "publicationVenue": {
                    "id": "43cdd274-e9fc-43e9-acf3-f020552ac76a",
                    "name": "International Conference on Development and Learning",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Dielectr Liq",
                        "Int Conf Dev Learn",
                        "International Conference on Dielectric Liquids",
                        "ICDL"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6c9f9649dfe5e062642b19843251195a7fec82c5",
                "title": "Feedback-Driven Incremental Imitation Learning Using Sequential VAE",
                "abstract": "Variational Autoencoders (VAEs) have attracted a lot of attention from the machine learning community in recent years. The usage of VAEs in learning by demonstration and robotics is still very restricted due to the need for effective learning from only a few examples and due to the difficult evaluation of the reconstruction quality. In this paper, we utilize the current models of conditional variational autoencoders for the purpose of teaching a robot simple actions from demonstration in an incremental fashion. We in detail evaluate various training approaches and define parameters that are important for enabling high-quality samples and reconstructions. The quality of the generated samples in different stages of learning is evaluated both quantitatively and qualitatively on the humanoid robot Pepper. We show that the robot can reach a reasonable quality of generated actions already after 20 observed samples.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "67076580",
                        "name": "G. Sejnova"
                    },
                    {
                        "authorId": "144529147",
                        "name": "K. \u0160t\u011bp\u00e1nov\u00e1"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ec45c5c3c27eb8dc6046b591e0a853dac50295fc",
                "externalIds": {
                    "DBLP": "journals/frai/BrunoMDRMFTM22",
                    "PubMedCentral": "9499023",
                    "DOI": "10.3389/frai.2022.868926",
                    "CorpusId": 252114318,
                    "PubMed": "36160929"
                },
                "corpusId": 252114318,
                "publicationVenue": {
                    "id": "6a8c0041-d0b7-4e32-b52c-33adef005c7e",
                    "name": "Frontiers in Artificial Intelligence",
                    "alternate_names": [
                        "Front Artif Intell"
                    ],
                    "issn": "2624-8212",
                    "url": "https://www.frontiersin.org/journals/artificial-intelligence#"
                },
                "url": "https://www.semanticscholar.org/paper/ec45c5c3c27eb8dc6046b591e0a853dac50295fc",
                "title": "Improving plant disease classification by adaptive minimal ensembling",
                "abstract": "A novel method for improving plant disease classification, a challenging and time-consuming process, is proposed. First, using as baseline EfficientNet, a recent and advanced family of architectures having an excellent accuracy/complexity trade-off, we have introduced, devised, and applied refined techniques based on transfer learning, regularization, stratification, weighted metrics, and advanced optimizers in order to achieve improved performance. Then, we go further by introducing adaptive minimal ensembling, which is a unique input to the knowledge base of the proposed solution. This represents a leap forward since it allows improving the accuracy with limited complexity using only two EfficientNet-b0 weak models, performing ensembling on feature vectors by a trainable layer instead of classic aggregation on outputs. To the best of our knowledge, such an approach to ensembling has never been used before in literature. Our method was tested on PlantVillage, a public reference dataset used for benchmarking models' performances for crop disease diagnostic, considering both its original and augmented versions. We noticeably improved the state of the art by achieving 100% accuracy in both the original and augmented datasets. Results were obtained using PyTorch to train, test, and validate the models; reproducibility is granted by providing exhaustive details, including hyperparameters used in the experimentation. A Web interface is also made publicly available to test the proposed methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49074846",
                        "name": "A. Bruno"
                    },
                    {
                        "authorId": "2073986452",
                        "name": "D. Moroni"
                    },
                    {
                        "authorId": "89025435",
                        "name": "R. Dainelli"
                    },
                    {
                        "authorId": "153674270",
                        "name": "L. Rocchi"
                    },
                    {
                        "authorId": "40219341",
                        "name": "Silvia Morelli"
                    },
                    {
                        "authorId": "1658823530",
                        "name": "E. Ferrari"
                    },
                    {
                        "authorId": "144799365",
                        "name": "P. Toscano"
                    },
                    {
                        "authorId": "46767485",
                        "name": "M. Martinelli"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f7657cd9dd67f7df19f04d1421eb062106955c0a",
                "externalIds": {
                    "PubMedCentral": "9538560",
                    "DOI": "10.1002/mp.15969",
                    "CorpusId": 252070151,
                    "PubMed": "36057788"
                },
                "corpusId": 252070151,
                "publicationVenue": {
                    "id": "4e90b61b-3ae1-4b4c-9348-d67a3825e703",
                    "name": "Medical Physics (Lancaster)",
                    "type": "journal",
                    "alternate_names": [
                        "Med Phys (lancaster",
                        "Medical Physics",
                        "Med Phys"
                    ],
                    "issn": "0094-2405",
                    "url": "http://scitation.aip.org/content/aapm/journal/medphys",
                    "alternate_urls": [
                        "https://www.medphys.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f7657cd9dd67f7df19f04d1421eb062106955c0a",
                "title": "A novel adaptive cubic quasi\u2010Newton optimizer for deep learning based medical image analysis tasks, validated on detection of COVID\u201019 and segmentation for COVID\u201019 lung infection, liver tumor, and optic disc/cup",
                "abstract": "Abstract Background Most of existing deep learning research in medical image analysis is focused on networks with stronger performance. These networks have achieved success, while their architectures are complex and even contain massive parameters ranging from thousands to millions in numbers. The nature of high dimension and nonconvex makes it easy to train a suboptimal model through the popular stochastic first\u2010order optimizers, which only use gradient information. Purpose Our purpose is to design an adaptive cubic quasi\u2010Newton optimizer, which could help to escape from suboptimal solution and improve the performance of deep neural networks on four medical image analysis tasks including: detection of COVID\u201019, COVID\u201019 lung infection segmentation, liver tumor segmentation, optic disc/cup segmentation. Methods In this work, we introduce a novel adaptive cubic quasi\u2010Newton optimizer with high\u2010order moment (termed ACQN\u2010H) for medical image analysis. The optimizer dynamically captures the curvature of the loss function by diagonally approximated Hessian and the norm of difference between previous two estimates, which helps to escape from saddle points more efficiently. In addition, to reduce the variance introduced by the stochastic nature of the problem, ACQN\u2010H hires high\u2010order moment through exponential moving average on iteratively calculated approximated Hessian matrix. Extensive experiments are performed to access the performance of ACQN\u2010H. These include detection of COVID\u201019 using COVID\u2010Net on dataset COVID\u2010chestxray, which contains 16 565 training samples and 1841 test samples; COVID\u201019 lung infection segmentation using Inf\u2010Net on COVID\u2010CT, which contains 45, 5, and 5 computer tomography (CT) images for training, validation, and testing, respectively; liver tumor segmentation using ResUNet on LiTS2017, which consists of 50 622 abdominal scan images for training and 26 608 images for testing; optic disc/cup segmentation using MRNet on RIGA, which has 655 color fundus images for training and 95 for testing. The results are compared with commonly used stochastic first\u2010order optimizers such as Adam, SGD, and AdaBound, and recently proposed stochastic quasi\u2010Newton optimizer Apollo. In task detection of COVID\u201019, we use classification accuracy as the evaluation metric. For the other three medical image segmentation tasks, seven commonly used evaluation metrics are utilized, that is, Dice, structure measure, enhanced\u2010alignment measure (EM), mean absolute error (MAE), intersection over union (IoU), true positive rate (TPR), and true negative rate. Results Experiments on four tasks show that ACQN\u2010H achieves improvements over other stochastic optimizers: (1) comparing with AdaBound, ACQN\u2010H achieves 0.49%, 0.11%, and 0.70% higher accuracy on the COVID\u2010chestxray dataset using network COVID\u2010Net with VGG16, ResNet50 and DenseNet121 as backbones, respectively; (2) ACQN\u2010H has the best scores in terms of evaluation metrics Dice, TPR, EM, and MAE on COVID\u2010CT dataset using network Inf\u2010Net. Particularly, ACQN\u2010H achieves 1.0% better Dice as compared to Apollo; (3) ACQN\u2010H achieves the best results on LiTS2017 dataset using network ResUNet, and outperforms Adam in terms of Dice by 2.3%; (4) ACQN\u2010H improves the performance of network MRNet on RIGA dataset, and achieves 0.5% and 1.0% better scores on cup segmentation for Dice and IoU, respectively, compared with SGD. We also present fivefold validation results of four tasks. It can be found that the results on detection of COVID\u201019, liver tumor segmentation and optic disc/cup segmentation can achieve high performance with low variance. For COVID\u201019 lung infection segmentation, the variance on test set is much larger than on validation set, which may due to small size of dataset. Conclusions The proposed optimizer ACQN\u2010H has been validated on four medical image analysis tasks including: detection of COVID\u201019 using COVID\u2010Net on COVID\u2010chestxray, COVID\u201019 lung infection segmentation using Inf\u2010Net on COVID\u2010CT, liver tumor segmentation using ResUNet on LiTS2017, optic disc/cup segmentation using MRNet on RIGA. Experiments show that ACQN\u2010H can achieve some performance improvement. Moreover, the work is expected to boost the performance of existing deep learning networks in medical image analysis.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156649341",
                        "name": "Yan Liu"
                    },
                    {
                        "authorId": "14278028",
                        "name": "Maojun Zhang"
                    },
                    {
                        "authorId": "2069512564",
                        "name": "Zhiwei Zhong"
                    },
                    {
                        "authorId": "2441459",
                        "name": "Xiangrong Zeng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief optimizer is used to achieve good generalization, fast convergence, and good stability simultaneously [25]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "42b554136cf10f5eef7503c0fa633e451688b867",
                "externalIds": {
                    "DOI": "10.1109/JLT.2022.3186895",
                    "CorpusId": 251847970
                },
                "corpusId": 251847970,
                "publicationVenue": {
                    "id": "a1f606c0-a59f-4771-bdb9-aac3a2ca72eb",
                    "name": "Journal of Lightwave Technology",
                    "type": "journal",
                    "alternate_names": [
                        "J Light Technol"
                    ],
                    "issn": "0733-8724",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=50",
                    "alternate_urls": [
                        "http://jlt.osa.org/journal/JLT/about.cfm",
                        "http://www.opticsinfobase.org/jlt/journal/JLT/about.cfm",
                        "http://jlt.osa.org/issue.cfm"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/42b554136cf10f5eef7503c0fa633e451688b867",
                "title": "Hybrid of Free Space Optics Communication and Sensor System Using IWDM Technique",
                "abstract": "In this work, we designed a novel hybrid of free-space optics (FSO) communication and intensity and wavelength division multiplexing (IWDM)-fiber Bragg grating (FBG) sensor system over a single shared FSO transmission channel for both optical and sensor signal transmission. In the proposed system, we use one broadband light source and one FSO link for both optical and FBG sensor signal transmission, which significantly reduces the cost of system installation and complexity. Besides, the FSO transmission channel is used to solve the problem of geographical constraints when the environment is difficult to install the fiber. In addition, IWDM is one of the best multiplexing technique to increase the sensor number by more than two times the conventional wavelength division multiplexing (WDM) method. Besides, to solve the overlap spectra problem of IWDM based FBG sensor system, we proposed the stacked gated recurrent unit (SGRU) algorithm with the neural network (NN) profile method. The results prove that the proposed SGRU with the NN profile method improves the accuracy of wavelength detection techniques with the Gaussian profile method. Moreover, the FSO transmission channel achieves a clear eye diagram even the system is a hybrid of optical communication and FBG sensors system.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "93038913",
                        "name": "S. T. Hayle"
                    },
                    {
                        "authorId": "31329239",
                        "name": "Y. C. Manie"
                    },
                    {
                        "authorId": "2093201484",
                        "name": "C. Yao"
                    },
                    {
                        "authorId": "2995822",
                        "name": "Tsung-Yuan Yeh"
                    },
                    {
                        "authorId": "2182908692",
                        "name": "Cheng-Hung Yu"
                    },
                    {
                        "authorId": "21618414",
                        "name": "P. Peng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We select the AdaBelief Optimizer [60] for proposed framework."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e158f0298ff3215b76fd4d41b5cfebcea4f5c221",
                "externalIds": {
                    "DBLP": "journals/ijdsa/BorisovBKK23",
                    "DOI": "10.1007/s41060-022-00350-z",
                    "CorpusId": 251774824
                },
                "corpusId": 251774824,
                "publicationVenue": {
                    "id": "c3875580-2cce-4a95-a094-fff60bb381b9",
                    "name": "International Journal of Data Science and Analysis",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Data Sci Anal",
                        "International Journal of Data Science and Analytics"
                    ],
                    "issn": "2575-1883",
                    "alternate_issns": [
                        "2364-4168"
                    ],
                    "url": "http://www.sciencepublishinggroup.com/journal/index?journalid=367",
                    "alternate_urls": [
                        "https://link.springer.com/journal/41060"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e158f0298ff3215b76fd4d41b5cfebcea4f5c221",
                "title": "DeepTLF: robust deep neural networks for heterogeneous tabular data",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152648871",
                        "name": "V. Borisov"
                    },
                    {
                        "authorId": "2102011",
                        "name": "Klaus Broelemann"
                    },
                    {
                        "authorId": "1884159",
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "authorId": "1686448",
                        "name": "G. Kasneci"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 10\u22122, 10\u22123, and 10\u22124) is robust for training deep neural networks [10, 16, 23, 28, 3, 26, 2], we focus on using a small constant learning rate \u03b1.",
                "The various adaptive methods include Adaptive Gradient (AdaGrad) [4], Root Mean Square Propagation (RMSProp) [21], Adaptive Moment Estimation (Adam) [10], Adaptive Mean Square Gradient (AMSGrad) [16], Yogi [23], Adam with decoupled weight decay (AdamW) [12], and AdaBelief (named for adapting stepsizes by the belief in observed gradients) [26].",
                "Meanwhile, practical results for adaptive methods were presented in [10, 16, 23, 28, 3, 26, 2].",
                "Theoretical analyses of adaptive methods for nonconvex optimization were presented in [23, 28, 3, 25, 26, 2, 9] (see [5, 1, 18, 11] for convergence analyses of SGD for nonconvex optimization)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "137ffab2db25084e8fb31e4f537711356791b509",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-09814",
                    "ArXiv": "2208.09814",
                    "DOI": "10.48550/arXiv.2208.09814",
                    "CorpusId": 251719388
                },
                "corpusId": 251719388,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/137ffab2db25084e8fb31e4f537711356791b509",
                "title": "Critical Bach Size Minimizes Stochastic First-Order Oracle Complexity of Deep Learning Optimizer using Hyperparameters Close to One",
                "abstract": "Practical results have shown that deep learning optimizers using small constant learning rates, hyperparameters close to one, and large batch sizes can find the model parameters of deep neural networks that minimize the loss functions. We first show theoretical evidence that the momentum method (Momentum) and adaptive moment estimation (Adam) perform well in the sense that the upper bound of the theoretical performance measure is small with a small constant learning rate, hyperparameters close to one, and a large batch size. Next, we show that there exists a batch size called the critical batch size minimizing the stochastic first-order oracle (SFO) complexity, which is the stochastic gradient computation cost, and that SFO complexity increases once the batch size exceeds the critical batch size. Finally, we provide numerical results that support our theoretical results. That is, the numerical results indicate that Adam using a small constant learning rate, hyperparameters close to one, and the critical batch size minimizing SFO complexity has faster convergence than Momentum and stochastic gradient descent (SGD).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2018304",
                        "name": "H. Iiduka"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e0e2649af2187b6be07d1cecc86a514a2581d6ff",
                "externalIds": {
                    "ArXiv": "2208.09900",
                    "DBLP": "journals/corr/abs-2208-09900",
                    "DOI": "10.48550/arXiv.2208.09900",
                    "CorpusId": 251719171
                },
                "corpusId": 251719171,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e0e2649af2187b6be07d1cecc86a514a2581d6ff",
                "title": "Provable Adaptivity in Adam",
                "abstract": "Adaptive Moment Estimation (Adam) optimizer is widely used in deep learning tasks because of its fast convergence properties. However, the convergence of Adam is still not well understood. In particular, the existing analysis of Adam cannot clearly demonstrate the advantage of Adam over SGD. We attribute this theoretical embarrassment to $L$-smooth condition (i.e., assuming the gradient is globally Lipschitz continuous with constant $L$) adopted by literature, which has been pointed out to often fail in practical neural networks. To tackle this embarrassment, we analyze the convergence of Adam under a relaxed condition called $(L_0,L_1)$ smoothness condition, which allows the gradient Lipschitz constant to change with the local gradient norm. $(L_0,L_1)$ is strictly weaker than $L$-smooth condition and it has been empirically verified to hold for practical deep neural networks. Under the $(L_0,L_1)$ smoothness condition, we establish the convergence for Adam with practical hyperparameters. Specifically, we argue that Adam can adapt to the local smoothness condition, justifying the \\emph{adaptivity} of Adam. In contrast, SGD can be arbitrarily slow under this condition. Our result might shed light on the benefit of adaptive gradient methods over non-adaptive ones.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390816671",
                        "name": "Bohan Wang"
                    },
                    {
                        "authorId": "2164118584",
                        "name": "Yushun Zhang"
                    },
                    {
                        "authorId": "2973831",
                        "name": "Huishuai Zhang"
                    },
                    {
                        "authorId": "47580728",
                        "name": "Qi Meng"
                    },
                    {
                        "authorId": "11958025",
                        "name": "Zhirui Ma"
                    },
                    {
                        "authorId": "2110264835",
                        "name": "Tie-Yan Liu"
                    },
                    {
                        "authorId": "2154939268",
                        "name": "Wei Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief [27] is used as the optimizer, which combines the strengths of both the Adam and the SGD optimizer\u2014adaptivity and generalization."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "43a79fb30777154837b39504b9c48675410c101a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-08672",
                    "ArXiv": "2208.08672",
                    "DOI": "10.1109/JIOT.2023.3265980",
                    "CorpusId": 251643608
                },
                "corpusId": 251643608,
                "publicationVenue": {
                    "id": "228761ec-c40a-479b-8309-9dcbe9851bcd",
                    "name": "IEEE Internet of Things Journal",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Internet Thing J"
                    ],
                    "issn": "2327-4662",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER288-ELE",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/servlet/opac?punumber=6488907",
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6488907",
                        "http://ieee-iotj.org/#"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/43a79fb30777154837b39504b9c48675410c101a",
                "title": "RRWaveNet: A Compact End-to-End Multiscale Residual CNN for Robust PPG Respiratory Rate Estimation",
                "abstract": "Respiratory rate (RR) is an important biomarker as RR changes can reflect severe medical events, such as heart disease, lung disease, and sleep disorders. Unfortunately, standard manual RR counting is prone to human error and cannot be performed continuously. This study proposes a method for continuously estimating RR, RRWaveNet. The method is a compact end-to-end deep learning model which does not require feature engineering and can use low-cost raw photoplethysmography (PPG) as input signal. RRWaveNet was tested subject-independently and compared to baseline in four data sets (BIDMC, CapnoBase, WESAD, and SensAI) and using three window sizes (16, 32, and 64 s). RRWaveNet outperformed current state-of-the-art methods with mean absolute errors at an optimal window size of 1.66 \u00b1 1.01, 1.59 \u00b1 1.08, 1.92 \u00b1 0.96, and 1.23 \u00b1 0.61 breaths per minute for each data set. In remote monitoring settings, such as in the WESAD and SensAI data sets, we apply transfer learning to improve the performance using two other ICU data sets as pretraining data sets, reducing the MAE by up to 21%. This shows that this model allows accurate and practical estimation of RR on affordable and wearable devices. Our study also shows feasibility of remote RR monitoring in the context of telemedicine and at home.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "24327857",
                        "name": "Pongpanut Osathitporn"
                    },
                    {
                        "authorId": "2090198513",
                        "name": "Guntitat Sawadwuthikul"
                    },
                    {
                        "authorId": "2144932476",
                        "name": "Punnawish Thuwajit"
                    },
                    {
                        "authorId": "2051541994",
                        "name": "Kawisara Ueafuea"
                    },
                    {
                        "authorId": "2181882652",
                        "name": "Thee Mateepithaktham"
                    },
                    {
                        "authorId": "9217391",
                        "name": "Narin Kunaseth"
                    },
                    {
                        "authorId": "1380391479",
                        "name": "Tanut Choksatchawathi"
                    },
                    {
                        "authorId": "3144189",
                        "name": "P. Punyabukkana"
                    },
                    {
                        "authorId": "3046569",
                        "name": "E. Mignot"
                    },
                    {
                        "authorId": "2236848",
                        "name": "Theerawit Wilaiprasitporn"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026maximum value was used as a parameter, in order to accelerate the learning, and the minimum value (in this case, the value suggested in the article [Zhuang et al., 2020], from the AdaBelief ) optimizer was added to the callback ReduceLROnPlateau training parameter, with the intention of varying\u2026",
                "As per the paper of its creators, [Zhuang et al., 2020], this optimizer has the great advantage of having the fast convergence achieved by adaptive methods such as Adam, and also the high generalization capacity as does SGD.",
                "After finding the best learning rate range, the maximum value was used as a parameter, in order to accelerate the learning, and the minimum value (in this case, the value suggested in the article [Zhuang et al., 2020], from the AdaBelief ) optimizer was added to the callback ReduceLROnPlateau training parameter, with the intention of varying the learning rate throughout the training and overcoming the stagnation that could occur when using the same rate over many epochs.",
                "As per the paper of its creators, [Zhuang et al., 2020], this optimizer has the great advantage of having the fast convergence achieved by adaptive methods such as Adam, and also the high generalization capacity as does SGD. Firstly, the Triangular Cyclical Learning Rate optimizer was used as the\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4c49033ad7aaa0fee7b70a1bc61b2b4aa284ce03",
                "externalIds": {
                    "ArXiv": "2208.09349",
                    "DBLP": "journals/corr/abs-2208-09349",
                    "DOI": "10.48550/arXiv.2208.09349",
                    "CorpusId": 251740945
                },
                "corpusId": 251740945,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4c49033ad7aaa0fee7b70a1bc61b2b4aa284ce03",
                "title": "DCNNV-19: A Deep Convolutional Neural Network for COVID-19 Detection in Chest Computed Tomographies",
                "abstract": "This technical report proposes the use of a deep convolutional neural network as a preliminary diagnostic method in the analysis of chest computed tomography images from patients with symptoms of Severe Acute Respiratory Syndrome (SARS) and suspected COVID-19 disease, especially on occasions when the delay of the RT-PCR result and the absence of urgent care could result in serious temporary, long-term, or permanent health damage. The model was trained on 83,391 images, validated on 15,297, and tested on 22,185 figures, achieving an F1-Score of 98%, 97.59% in Cohen's Kappa, 98.4% in Accuracy, and 5.09% in Loss. Attesting a highly accurate automated classification and providing results in less time than the current gold-standard exam, Real-Time reverse-transcriptase Polymerase Chain Reaction (RT-PCR).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182450415",
                        "name": "Victor Felipe Reis-Silva"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Indeed, all our implementations are also based on the code provided by Adablief [2]2.",
                "All results except Adan and Padam in the table are reported by AdaBelief [2].",
                "The reported results in [2] slightly differ from the those in [41] because of their different settings for LSTM and training hyper-parameters.",
                "[2] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan.",
                "Adam-type [35] % % `\u221e \u2264 c\u221e O ( c(2)\u221ed \u22124) \u03a9( \u22124) RMSProp [23, 40] % % `\u221e \u2264 c\u221e O (\u221a c\u221ed \u22124 ) \u03a9 ( \u22124 ) Lipschitz AdamW [3] \" \u2014 \u2014 \u2014 \u2014 Adabelief [2] % % `2 \u2264 c2 O ( c(6)2 \u22124) \u03a9( \u22124) Gradient Padam [41] % % `\u221e \u2264 c\u221e O (\u221a c\u221ed \u22124 ) \u03a9 ( \u22124 ) LAMB [4] % O ( \u22124 ) `2 \u2264 c2 O ( c(2)2d \u22124) \u03a9( \u22124) Adan (ours) \" % `\u221e \u2264 c\u221e O ( c2.",
                "AMSGrad [24], Adabound [26], and Adabelief [2].",
                "This complexity is lower than O ( c(6)2 \u22124) of Adabelief [2] and O ( c(2)2d \u22124) of LAMB, especially on over-parameterized networks.",
                "LSTM Adan AdaBelief [2] SGD [20] AdaBound [26] Adam [1] AdamW [3] Padam [41] RAdam [27] Yogi [62] 1 layer 83.",
                "Adan SGD [20] Nadam [44] AdaBound [26] Adam [1] Radam [27] Padam [41] LAMB [4] AdamW [3] AdaBlief [2] 70."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "568eb10d17f1643228303670fe0f1d6608bd6f4d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-06677",
                    "ArXiv": "2208.06677",
                    "DOI": "10.48550/arXiv.2208.06677",
                    "CorpusId": 251564316
                },
                "corpusId": 251564316,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/568eb10d17f1643228303670fe0f1d6608bd6f4d",
                "title": "Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models",
                "abstract": "In deep learning, different kinds of deep networks typically need different optimizers, which have to be chosen after multiple trials, making the training process inefficient. To relieve this issue and consistently improve the model training speed across deep networks, we propose the ADAptive Nesterov momentum algorithm, Adan for short. Adan first reformulates the vanilla Nesterov acceleration to develop a new Nesterov momentum estimation (NME) method, which avoids the extra overhead of computing gradient at the extrapolation point. Then Adan adopts NME to estimate the gradient's first- and second-order moments in adaptive gradient algorithms for convergence acceleration. Besides, we prove that Adan finds an $\\epsilon$-approximate first-order stationary point within $O(\\epsilon^{-3.5})$ stochastic gradient complexity on the non-convex stochastic problems (e.g., deep learning problems), matching the best-known lower bound. Extensive experimental results show that Adan consistently surpasses the corresponding SoTA optimizers on vision, language, and RL tasks and sets new SoTAs for many popular networks and frameworks, e.g., ResNet, ConvNext, ViT, Swin, MAE, DETR, GPT-2, Transformer-XL, and BERT. More surprisingly, Adan can use half of the training cost (epochs) of SoTA optimizers to achieve higher or comparable performance on ViT, GPT-2, MAE, e.t.c., and also shows great tolerance to a large range of minibatch size, e.g., from 1k to 32k. Code is released at https://github.com/sail-sg/Adan, and has been used in multiple popular deep learning frameworks or projects.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2543387",
                        "name": "Xingyu Xie"
                    },
                    {
                        "authorId": "2153245275",
                        "name": "Pan Zhou"
                    },
                    {
                        "authorId": "3092681",
                        "name": "Huan Li"
                    },
                    {
                        "authorId": "33383055",
                        "name": "Zhouchen Lin"
                    },
                    {
                        "authorId": "143653681",
                        "name": "Shuicheng Yan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Numerical experiments\nIn this section, we compare the performance of the proposed SGEM and AEGD with several other methods, including SGDM, AdaBelief [38], AdaBound [21], RAdam [19], Yogi [36], and Adam [13], when applied to training deep neural networks.",
                "Numerical experiments In this section, we compare the performance of the proposed SGEM and AEGD with several other methods, including SGDM, AdaBelief [38], AdaBound [21], RAdam [19], Yogi [36], and Adam [13], when applied to training deep neural networks."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ecb356505e7bdc4e82347df33696668b604f7a91",
                "externalIds": {
                    "ArXiv": "2208.02208",
                    "DBLP": "journals/corr/abs-2208-02208",
                    "DOI": "10.48550/arXiv.2208.02208",
                    "CorpusId": 251280082
                },
                "corpusId": 251280082,
                "publicationVenue": {
                    "id": "3920fb0a-e610-4b5d-8760-f43cfa321466",
                    "name": "Numerical Algorithms",
                    "type": "journal",
                    "alternate_names": [
                        "Numer Algorithm"
                    ],
                    "issn": "1017-1398",
                    "url": "https://www.springer.com/computer/theoretical+computer+science/journal/11075?changeHeader",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11075"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ecb356505e7bdc4e82347df33696668b604f7a91",
                "title": "SGEM: stochastic gradient with energy and momentum",
                "abstract": "In this paper, we propose SGEM, Stochastic Gradient with Energy and Momentum, to solve a large class of general non-convex stochastic optimization problems, based on the AEGD method that originated in the work [AEGD: Adaptive Gradient Descent with Energy. arXiv: 2010.05109]. SGEM incorporates both energy and momentum at the same time so as to inherit their dual advantages. We show that SGEM features an unconditional energy stability property, and derive energy-dependent convergence rates in the general nonconvex stochastic setting, as well as a regret bound in the online convex setting. A lower threshold for the energy variable is also provided. Our experimental results show that SGEM converges faster than AEGD and generalizes better or at least as well as SGDM in training some deep neural networks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109527698",
                        "name": "Hailiang Liu"
                    },
                    {
                        "authorId": "2117127998",
                        "name": "Xuping Tian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c0c3b6b2350e3af15af38a1b9d9d52d3ff674cae",
                "externalIds": {
                    "DBLP": "journals/istr/WangLCWR22",
                    "DOI": "10.1016/j.jisa.2022.103227",
                    "CorpusId": 249419674
                },
                "corpusId": 249419674,
                "publicationVenue": {
                    "id": "5291374f-6a33-40a6-a399-3fdd1ee0fd51",
                    "name": "Journal of Information Security and Applications",
                    "type": "journal",
                    "alternate_names": [
                        "J Inf Secur Appl"
                    ],
                    "issn": "2214-2126",
                    "url": "https://www.journals.elsevier.com/journal-of-information-security-and-applications"
                },
                "url": "https://www.semanticscholar.org/paper/c0c3b6b2350e3af15af38a1b9d9d52d3ff674cae",
                "title": "AB-FGSM: AdaBelief optimizer and FGSM-based approach to generate adversarial examples",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108734934",
                        "name": "Yixiang Wang"
                    },
                    {
                        "authorId": "2130356639",
                        "name": "Jiqiang Liu"
                    },
                    {
                        "authorId": "2072562098",
                        "name": "Xiaolin Chang"
                    },
                    {
                        "authorId": "2144538151",
                        "name": "Jianhua Wang"
                    },
                    {
                        "authorId": "2113498218",
                        "name": "Ricardo J. Rodr\u00edguez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, the MEA term is similar to the \u201cbelief\" in [15] , which can adjust the step size to avoid oscillation around a local minimum.",
                "Moreover, the MEA term plays a similar role of \u201cbelief\" in AdaBelief [15], which can adjust the step size in terms of the current gradient direction to avoid the oscillation around the local optimum."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0520617454c397f2e357838b42b03b608140faf1",
                "externalIds": {
                    "DOI": "10.23919/CCC55666.2022.9902453",
                    "CorpusId": 252850580
                },
                "corpusId": 252850580,
                "publicationVenue": {
                    "id": "23f8fe4c-6537-4027-a334-6a5863115984",
                    "name": "Cybersecurity and Cyberforensics Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Chin Control Conf",
                        "Computational Complexity Conference",
                        "CCC",
                        "Comput Complex Conf",
                        "Cybersecur Cyberforensics Conf",
                        "Conference on Computational Complexity",
                        "Computing Colombian Conference",
                        "Conf Comput Complex",
                        "Comput Colomb Conf",
                        "Chinese Control Conference"
                    ],
                    "url": "http://computationalcomplexity.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0520617454c397f2e357838b42b03b608140faf1",
                "title": "A Memory Enhancement Adjustment Method Based on Stochastic Gradients",
                "abstract": "Stochastic gradient descent methods and its variants have been widely used to learn the parameters of a neural network by solving an associated non-convex minimization problem. We propose a new momentum method and adaptive method (MEA/AdaMEA) based on memory enhancement adjustment, which is different from the traditional momentum methods and adaptive methods. At the theoretical level, we prove the convergence of the MEA method for solving a non-convex minimization problem, and then analyze the generalization error of the MEA method from the perspective of consistent stability, showing that the MEA method can improve the stability of the learned model and enhance the generalization performance. At the experimental level, we compare the empirical results of the MEA/AdaMEA method and common optimizers for deep learning on the datasets CIFAR10 and CIFAR100 under the network architectures of the convolutional neural networks ResNet18 and ResNet34. Experiments demonstrate the effectiveness of our proposed MEA/AdaMEA.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2187728506",
                        "name": "Gan Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Table 1: The results comparison of Adam [13], Radam [18] and Adabelief [27] optimizers with the gradient centralization [26] and the proposed moment centralization on CIFAR10 dataset using VGG16 [22] and ResNet18 [9] CNN models.",
                "Similarly, we also incorporate the moment centralization concept with Radam [18] and Adabelief [27] optimizers.",
                "In this paper, we use it with state-of-the-art optimizers, including Adam [13], Radam [18] and Adabelief [27].",
                "Recently, Adabelief optimizer [27] utilizes the residual of gradient and first order moment to compute the second order moment which improves the training at saddle regions and local minimum.",
                "Table 2: The results comparison of Adam [13], Radam [18] and Adabelief [27] optimizers with the gradient centralization [26] and the proposed moment centralization on CIFAR100 dataset using VGG16 [22] and ResNet18 [9] CNN models.",
                "Table 3: The results comparison of Adam [13], Radam [18] and Adabelief [27] optimizers with the gradient centralization [26] and the proposed moment centralization on TinyImageNet dataset using VGG16 [22] and ResNet18 [9] CNN models."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1a61cba68301ebe9903e46df600547a71a16b7c7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-09066",
                    "ArXiv": "2207.09066",
                    "DOI": "10.48550/arXiv.2207.09066",
                    "CorpusId": 250644516
                },
                "corpusId": 250644516,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1a61cba68301ebe9903e46df600547a71a16b7c7",
                "title": "Moment Centralization based Gradient Descent Optimizers for Convolutional Neural Networks",
                "abstract": "Convolutional neural networks (CNNs) have shown very appealing performance for many computer vision applications. The training of CNNs is generally performed using stochastic gradient descent (SGD) based optimization techniques. The adaptive momentum-based SGD optimizers are the recent trends. However, the existing optimizers are not able to maintain a zero mean in the first-order moment and struggle with optimization. In this paper, we propose a moment centralization-based SGD optimizer for CNNs. Specifically, we impose the zero mean constraints on the first-order moment explicitly. The proposed moment centralization is generic in nature and can be integrated with any of the existing adaptive momentum-based optimizers. The proposed idea is tested with three state-of-the-art optimization techniques, including Adam, Radam, and Adabelief on benchmark CIFAR10, CIFAR100, and TinyImageNet datasets for image classification. The performance of the existing optimizers is generally improved when integrated with the proposed moment centralization. Further, The results of the proposed moment centralization are also better than the existing gradient centralization. The analytical analysis using the toy example shows that the proposed method leads to a shorter and smoother optimization trajectory. The source code is made publicly available at \\url{https://github.com/sumanthsadhu/MC-optimizer}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176865371",
                        "name": "Sumanth Sadu"
                    },
                    {
                        "authorId": "34992579",
                        "name": "S. Dubey"
                    },
                    {
                        "authorId": "50160637",
                        "name": "S. Sreeja"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We used AdaBelief optimizer [28], said to be stable, be fast like Adam and generalize well like SGD."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "eb82ce0a2f59a291897e7b67465c758a4dd7dd65",
                "externalIds": {
                    "DBLP": "conf/ijcnn/BaikSLK22",
                    "DOI": "10.1109/IJCNN55064.2022.9891998",
                    "CorpusId": 252625299
                },
                "corpusId": 252625299,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/eb82ce0a2f59a291897e7b67465c758a4dd7dd65",
                "title": "Spatial-Channel Transformer for Scene Recognition",
                "abstract": "Despite the great success of attention mechanisms on object recognition, scene recognition remains a challenging problem. The reason is that discriminative regions are not evident in a scene image. For example, a tree in an image can be a cue to recognize a scene, but the tree cannot be the only cue for recognizing the scene. That means several scene categories (e.g. mountain, marsh, and river) can contain a tree. Thus sometimes, overall regions, rather than specific regions, need to be considered for scene recognition. To solve the problem, we propose Spatial-Channel Transformer (SC-Transformer). The SC-Transformer is a simple yet effective module that uses a new attention mechanism by incorporating the importance between the spatial and the channel domain for a given scene image. If the given scene image should be considered only within some specific regions, SC-Transformer turns off the channel attention, and vice versa. Furthermore, the attention mechanism used in our proposed method is advanced from previous approaches. Previous spatial and channel attention mechanisms were designed in a sequential or parallel manner. These mechanisms eventually combine spatial and channel attention together, so spatial and channel attention may often interfere with each other. In contrast to the previous works, we present a new mechanism that simultaneously considers spatial and channel attentions. We validate our approach on a large-scale scene recognition dataset and outperform the previous state-of-the-art spatial-channel attention mechanism. Experimental results demonstrate the efficacy of our attention mechanism for scene recognition.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "69034279",
                        "name": "Seunghyun Baik"
                    },
                    {
                        "authorId": "79755154",
                        "name": "Hongje Seong"
                    },
                    {
                        "authorId": "2939462",
                        "name": "Youngjo Lee"
                    },
                    {
                        "authorId": "1842453",
                        "name": "Euntai Kim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d99a00f4c1e88d30076bf2fd4b9335a98f8beab5",
                "externalIds": {
                    "DBLP": "conf/iscc/WangCRW22",
                    "DOI": "10.1109/ISCC55528.2022.9912903",
                    "CorpusId": 252999907
                },
                "corpusId": 252999907,
                "publicationVenue": {
                    "id": "159aed30-148b-4b50-99a4-372b0af958d9",
                    "name": "International Symposium on Computers and Communications",
                    "type": "conference",
                    "alternate_names": [
                        "ISCC",
                        "Int Symp Comput Commun"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000156/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/d99a00f4c1e88d30076bf2fd4b9335a98f8beab5",
                "title": "Assessing Anonymous and Selfish Free-rider Attacks in Federated Learning",
                "abstract": "Federated Learning (FL) is a distributed learning framework and gains interest due to protecting the privacy of participants. Thus, if some participants are free-riders who are attackers without contributing any computation resources and privacy data, the model faces privacy leakage and inferior performance. In this paper, we explore and define two free-rider attack scenarios, anonymous and selfish free-rider attacks. Then we propose two methods, namely novel and advanced methods, to construct these two attacks. Extensive experiment results reveal the effectiveness in terms of the less deviation with conventional FL using the novel method, and high false positive rate to puzzle defense model using the advanced method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144538151",
                        "name": "Jianhua Wang"
                    },
                    {
                        "authorId": "2072562098",
                        "name": "Xiaolin Chang"
                    },
                    {
                        "authorId": "2113498218",
                        "name": "Ricardo J. Rodr\u00edguez"
                    },
                    {
                        "authorId": "2108734934",
                        "name": "Yixiang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "All rights reserved.\ngradients, updates, parameters, etc. (e.g. SGD with momentum, Adam, AdaBelief, Lookahead) (Qian 1999; Kingma and Ba 2017; Zhuang et al. 2020; Zhang et al. 2019).",
                "While we do not propose an alternative to AdaBelief, leaving that to future\nwork, we attempt to move toward quantifying the behavior of optimizers at the gradient level in order empirically justify and explain novel methods like AdaBelief.",
                "SGD with momentum, Adam, AdaBelief, Lookahead) (Qian 1999; Kingma and Ba 2017; Zhuang et al. 2020; Zhang et al. 2019).",
                "Consider the following example: one might expect if estimated gradients are in the same direction then they likely approximate the true gradient well; indeed, this is the intuition behind AdaBelief (Zhuang et al. 2020), which demonstrates strong performance across multiple tasks."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e627ae62ecc215d44f3001ad812fb74766ff37cd",
                "externalIds": {
                    "DBLP": "conf/aaai/ZhangQLP22",
                    "DOI": "10.1609/aaai.v36i11.21691",
                    "CorpusId": 250274238
                },
                "corpusId": 250274238,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e627ae62ecc215d44f3001ad812fb74766ff37cd",
                "title": "Understanding Stochastic Optimization Behavior at the Layer Update Level (Student Abstract)",
                "abstract": "Popular first-order stochastic optimization methods for deep neural networks (DNNs) are usually either accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum) or adaptive step-size methods (e.g. Adam/AdaMax, AdaBelief). In many contexts, including image classification with DNNs, adaptive methods tend to generalize poorly compared to SGD, i.e. get stuck in non-robust local minima; however, SGD typically converges slower. We analyze possible reasons for this behavior by modeling gradient updates as vectors of random variables and comparing them to probabilistic bounds to identify \"meaningful\" updates. Through experiments, we observe that only layers close to the output have \"definitely non-random\" update behavior. In the future, the tools developed here may be useful in rigorously quantifying and analyzing intuitions about why some optimizers and particular DNN architectures perform better than others.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108020035",
                        "name": "Jack G. Zhang"
                    },
                    {
                        "authorId": "1562120766",
                        "name": "Guanhui Qiao"
                    },
                    {
                        "authorId": "2174774843",
                        "name": "Alexandru Lopotenco"
                    },
                    {
                        "authorId": "2065218572",
                        "name": "Ian Pan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We used AdaBelief optimizer (Zhuang et al. 2020) with learning rate 0.001 and default smoothing parameter of \u03b21 = 0.9 and \u03b22 = 0.999.",
                "We used AdaBelief optimizer (Zhuang et al. 2020) with learning rate 0."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "56a76ae99639c1238059f6d9a19e12880740d01b",
                "externalIds": {
                    "DBLP": "conf/aaai/LeeKJR22",
                    "DOI": "10.1609/aaai.v36i7.20697",
                    "CorpusId": 250296712
                },
                "corpusId": 250296712,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/56a76ae99639c1238059f6d9a19e12880740d01b",
                "title": "Differentially Private Normalizing Flows for Synthetic Tabular Data Generation",
                "abstract": "Normalizing flows have shown to be a promising approach to deep generative modeling due to their ability to exactly evaluate density --- other alternatives either implicitly model the density or use approximate surrogate density. In this work, we present a differentially private normalizing flow model for heterogeneous tabular data. Normalizing flows are in general not amenable to differentially private training because they require complex neural networks with larger depth (compared to other generative models) and use specialized architectures for which per-example gradient computation is difficult (or unknown). To reduce the parameter complexity, the proposed model introduces a conditional spline flow which simulates transformations at different stages depending on additional input and is shared among sub-flows. For privacy, we introduce two fine-grained gradient clipping strategies that provide a better signal-to-noise ratio and derive fast gradient clipping methods for layers with custom parameterization. Our empirical evaluations show that the proposed model preserves statistical properties of original dataset better than other baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9174454",
                        "name": "Jaewoo Lee"
                    },
                    {
                        "authorId": null,
                        "name": "Minjung Kim"
                    },
                    {
                        "authorId": "112953122",
                        "name": "Yonghyun Jeong"
                    },
                    {
                        "authorId": "1808057682",
                        "name": "Youngmin Ro"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", Adaptive Gradient (AdaGrad) [6], Root Mean Square Propagation (RMSProp) [26], Adaptive Moment Estimation (Adam) [13], Yogi [30], Adaptive Mean Square Gradient (AMSGrad) [21], Adam with decoupled weight decay (AdamW) [15], and AdaBelief (named for adapting stepsizes by the belief in observed gradients) [33].",
                "1) is advantageous for training deep neural networks [13, 21, 30, 36, 5, 33, 4].",
                "Motivated by the results in [21, 33], we will analyze Adam with (1.",
                "Convergence analyses of adaptive methods using diminishing learning rates that do not depend on L were presented in [36, 5, 33, 12], while convergence analyses of adaptive methods using constant learning rates that do not depend on L were presented in [32, 4, 12].",
                "3 Our results and contribution Numerical evaluations presented in [13, 21, 30, 36, 5, 33, 4] showed that Adam and its variants perform well when they use a small constant learning rate \u03b1 and hyperparameters \u03b21 and \u03b22 with values close to 1.",
                "Motivated by the results in [21, 33], we decided to study Adam under Condition (1.",
                "While the numerical evaluations presented in [13, 21, 30, 36, 5, 33, 4] have shown that adaptive methods using \u03b21 and \u03b22 close to 1 are advantageous for training deep neural",
                "Therefore, the results we present in this paper are theoretical confirmation of the numerical evaluations [13, 21, 30, 36, 5, 33, 4] showing that Adam and its variants using \u03b21 and \u03b22 close to 1 perform well.",
                "Convergence analyses of adaptive methods for nonconvex optimization were presented in [30, 36, 5, 32, 33, 4, 12]."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4bf9abb2d93d83bc8358024f96f769ea4bab7580",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-13290",
                    "ArXiv": "2206.13290",
                    "DOI": "10.48550/arXiv.2206.13290",
                    "CorpusId": 250073125
                },
                "corpusId": 250073125,
                "publicationVenue": {
                    "id": "3920fb0a-e610-4b5d-8760-f43cfa321466",
                    "name": "Numerical Algorithms",
                    "type": "journal",
                    "alternate_names": [
                        "Numer Algorithm"
                    ],
                    "issn": "1017-1398",
                    "url": "https://www.springer.com/computer/theoretical+computer+science/journal/11075?changeHeader",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11075"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4bf9abb2d93d83bc8358024f96f769ea4bab7580",
                "title": "Theoretical analysis of Adam using hyperparameters close to one without Lipschitz smoothness",
                "abstract": "Convergence and convergence rate analyses of adaptive methods, such as Adaptive Moment Estimation (Adam) and its variants, have been widely studied for nonconvex optimization. The analyses are based on assumptions that the expected or empirical average loss function is Lipschitz smooth (i.e., its gradient is Lipschitz continuous) and the learning rates depend on the Lipschitz constant of the Lipschitz continuous gradient. Meanwhile, numerical evaluations of Adam and its variants have clarified that using small constant learning rates without depending on the Lipschitz constant and hyperparameters ($\\beta_1$ and $\\beta_2$) close to one is advantageous for training deep neural networks. Since computing the Lipschitz constant is NP-hard, the Lipschitz smoothness condition would be unrealistic. This paper provides theoretical analyses of Adam without assuming the Lipschitz smoothness condition in order to bridge the gap between theory and practice. The main contribution is to show theoretical evidence that Adam using small learning rates and hyperparameters close to one performs well, whereas the previous theoretical results were all for hyperparameters close to zero. Our analysis also leads to the finding that Adam performs well with large batch sizes. Moreover, we show that Adam performs well when it uses diminishing learning rates and hyperparameters close to one.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2018304",
                        "name": "H. Iiduka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[60] J. Zhuang et al., \u201cAdaBelief optimizer: Adapting stepsizes by the belief in observed gradients,\u201d in Proc.",
                "AdaBelief improves the Adam drawback of reducing step size when the gradient is significant and variance is small.",
                "For the DNN training, we leveraged a small hyperparameter search with two similar optimizer algorithms: AdamW [59] and AdaBelief [60].",
                "Small hyperparameter search for BPN and VN identified that the most suitable combination of the network architecture is: Attention-U-Net with Dice loss function, AdaBelief as optimizer, a learning rate of 0.0001 without scheduling, and a batch\nsize of eight."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6a3525b11c5d9017384d691bb3ae9007ef344888",
                "externalIds": {
                    "DBLP": "journals/titb/HillenLSNS22",
                    "DOI": "10.1109/JBHI.2022.3186530",
                    "CorpusId": 250090720,
                    "PubMed": "35759601"
                },
                "corpusId": 250090720,
                "publicationVenue": {
                    "id": "eac74c9c-a5c0-417d-8088-8164a6a8bfb3",
                    "name": "IEEE journal of biomedical and health informatics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Journal of Biomedical and Health Informatics",
                        "IEEE j biomed health informatics",
                        "IEEE J Biomed Health Informatics"
                    ],
                    "issn": "2168-2194",
                    "url": "https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=6221020",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221020"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6a3525b11c5d9017384d691bb3ae9007ef344888",
                "title": "Towards Exercise Radiomics: Deep Neural Network-Based Automatic Analysis of Thermal Images Captured During Exercise",
                "abstract": "Infrared thermography is increasingly applied in sports science due to promising observations regarding changes in skin\u2019s surface radiation temperature (<inline-formula><tex-math notation=\"LaTeX\">$T_{sr}$</tex-math></inline-formula>) before, during, and after exercise. The common manual thermogram analysis limits an objective and reproducible measurement of <inline-formula><tex-math notation=\"LaTeX\">$T_{sr}$</tex-math></inline-formula>. Previous analysis approaches depend on expert knowledge and have not been applied during movement. We aimed to develop a deep neural network (DNN) capable of automatically and objectively segmenting body parts, recognizing blood vessel-associated <inline-formula><tex-math notation=\"LaTeX\">$T_{sr}$</tex-math></inline-formula> distributions, and continuously measuring <inline-formula><tex-math notation=\"LaTeX\">$T_{sr}$</tex-math></inline-formula> during exercise. We conducted 38 cardiopulmonary exercise tests on a treadmill. We developed two DNNs: body part network and vessel network, to perform semantic segmentation of 1 107 855 thermal images. Both DNNs were trained with 263 training and 75 validation images. Additionally, we compare the results of a common manual thermogram analysis with these of the DNNs. Performance analysis identified a mean IoU of 0.8 for body part network and 0.6 for vessel network. There is a high agreement between manual and automatic analysis (r = 0.999; p <inline-formula><tex-math notation=\"LaTeX\">$< $</tex-math></inline-formula>0.001; T-test: p = 0.116), with a mean difference of 0.01 <inline-formula><tex-math notation=\"LaTeX\">$^\\circ$</tex-math></inline-formula>C (0.08). Non-parametric Bland Altman\u2019s analysis showed that the 95% agreement ranges between - 0.086 <inline-formula><tex-math notation=\"LaTeX\">$^\\circ$</tex-math></inline-formula>C and 0.228<inline-formula><tex-math notation=\"LaTeX\">$^\\circ$</tex-math></inline-formula>C. The developed DNNs enable automatic, objective, and continuous measurement of <inline-formula><tex-math notation=\"LaTeX\">$T_{sr}$</tex-math></inline-formula> and recognition of blood vessel-associated <inline-formula><tex-math notation=\"LaTeX\">$T_{sr}$</tex-math></inline-formula> distributions in resting and moving legs. Hence, the DNNs surpass previous algorithms by eliminating manual region of interest selection and form the currently needed foundation to extensively investigate <inline-formula><tex-math notation=\"LaTeX\">$T_{sr}$</tex-math></inline-formula> distributions related to non-invasive diagnostics of (patho-)physiological traits in means of exercise radiomics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150935131",
                        "name": "B. Hillen"
                    },
                    {
                        "authorId": "2053968643",
                        "name": "Daniel Andr\u00e9s L\u00f3pez"
                    },
                    {
                        "authorId": "1912123",
                        "name": "E. Sch\u00f6mer"
                    },
                    {
                        "authorId": "36356358",
                        "name": "M. N\u00e4gele"
                    },
                    {
                        "authorId": "39151713",
                        "name": "P. Simon"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "95c8d75dcd653f832d09b02908b45a00ebd48685",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-10869",
                    "ArXiv": "2206.10869",
                    "DOI": "10.48550/arXiv.2206.10869",
                    "CorpusId": 249926907
                },
                "corpusId": 249926907,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/95c8d75dcd653f832d09b02908b45a00ebd48685",
                "title": "NVIDIA-UNIBZ Submission for EPIC-KITCHENS-100 Action Anticipation Challenge 2022",
                "abstract": "In this report, we describe the technical details of our submission for the EPIC-Kitchen-100 action anticipation challenge. Our modelings, the higher-order recurrent space-time transformer and the message-passing neural network with edge learning, are both recurrent-based architectures which observe only 2.5 seconds inference context to form the action anticipation prediction. By averaging the prediction scores from a set of models compiled with our proposed training pipeline, we achieved strong performance on the test set, which is 19.61% overall mean top-5 recall, recorded as second place on the public leaderboard.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27084141",
                        "name": "Tsung-Ming Tai"
                    },
                    {
                        "authorId": "1717522",
                        "name": "O. Lanz"
                    },
                    {
                        "authorId": "3144258",
                        "name": "G. Fiameni"
                    },
                    {
                        "authorId": "2172012230",
                        "name": "Yi-Kwan Wong"
                    },
                    {
                        "authorId": "2172155201",
                        "name": "Sze-Sen Poon"
                    },
                    {
                        "authorId": "2115293259",
                        "name": "Cheng-Kuang Lee"
                    },
                    {
                        "authorId": "2067676799",
                        "name": "K. Cheung"
                    },
                    {
                        "authorId": "144308998",
                        "name": "S. See"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Both phases are performed using theAdaBelief [28] optimizer which guarantees both fast convergence and generalization."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "929a689213be4915ba0012a8438f2ce5c3780a0f",
                "externalIds": {
                    "ArXiv": "2206.07394",
                    "DBLP": "journals/corr/abs-2206-07394",
                    "DOI": "10.1111/exsy.13424",
                    "CorpusId": 249674566
                },
                "corpusId": 249674566,
                "publicationVenue": {
                    "id": "7e79de3e-93d0-413c-9f0d-e0288f94b4d5",
                    "name": "Expert systems",
                    "type": "journal",
                    "alternate_names": [
                        "Expert Syst",
                        "Expert Systems",
                        "Expert syst"
                    ],
                    "issn": "0266-4720",
                    "url": "https://onlinelibrary.wiley.com/journal/14680394"
                },
                "url": "https://www.semanticscholar.org/paper/929a689213be4915ba0012a8438f2ce5c3780a0f",
                "title": "Efficient Adaptive Ensembling for Image Classification",
                "abstract": "In recent times, with the exception of sporadic cases, the trend in computer vision is to achieve minor improvements compared to considerable increases in complexity. To reverse this trend, we propose a novel method to boost image classification performances without increasing complexity. To this end, we revisited ensembling, a powerful approach, often not used properly due to its more complex nature and the training time, so as to make it feasible through a specific design choice. First, we trained two EfficientNet\u2010b0 end\u2010to\u2010end models (known to be the architecture with the best overall accuracy/complexity trade\u2010off for image classification) on disjoint subsets of data (i.e., bagging). Then, we made an efficient adaptive ensemble by performing fine\u2010tuning of a trainable combination layer. In this way, we were able to outperform the state\u2010of\u2010the\u2010art by an average of 0.5% on the accuracy, with restrained complexity both in terms of the number of parameters (by 5\u201360 times), and the FLoating point Operations Per Second FLOPS by 10\u2013100 times on several major benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49074846",
                        "name": "A. Bruno"
                    },
                    {
                        "authorId": "2073986452",
                        "name": "D. Moroni"
                    },
                    {
                        "authorId": "46767485",
                        "name": "M. Martinelli"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b6be7bfdf99b5ab42d77e537a1658a4e85fe0f54",
                "externalIds": {
                    "DBLP": "conf/aicas/KimLPS22",
                    "DOI": "10.1109/AICAS54282.2022.9869953",
                    "CorpusId": 252113021
                },
                "corpusId": 252113021,
                "publicationVenue": {
                    "id": "c00492c7-e775-4f0a-a843-9f5bbb219dbe",
                    "name": "International Conference on Artificial Intelligence Circuits and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "AICAS",
                        "Int Conf Artif Intell Circuit Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b6be7bfdf99b5ab42d77e537a1658a4e85fe0f54",
                "title": "Efficient Deep Learning Algorithm for Alzheimer's Disease Diagnosis using Retinal Images",
                "abstract": "Alzheimer's Disease (AD) is crucial to detect in the early stage and recent studies suggest diagnosing AD using Machine Learning. We propose an efficient AD diagnosis Deep Learning algorithm using retinal fundus images. The proposed method employs MobileNetV3 as a backbone, and an Attention mechanism is applied. However, we modified the backbone structure into U-Net-like architecture to perform better. Furthermore, we modified the conventional Attention mechanism to the Weighted Attention mechanism. The masking-adding process has been applied in the training method of the model. Therefore, our model has 8.4% better classification performance than the current state-of-the-art model by achieving 0.929 AUC in the validation set.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2184222212",
                        "name": "Do Young Kim"
                    },
                    {
                        "authorId": "2184211224",
                        "name": "Young Jun Lim"
                    },
                    {
                        "authorId": "2152818461",
                        "name": "J. Park"
                    },
                    {
                        "authorId": "1751468",
                        "name": "M. Sunwoo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief\nBased on the classic optimizer Adam [25], AdaBelief [26] adjust the training stride according to the Belief in the gradient direction.",
                "Keywords: smart grid; short-term load forecasting; feature engineering; variational modal decomposition; deep learning; Informer; AdaBelief",
                "The model is optimized using AdaBelief, which improves the accuracy and operational efficiency of the model operation.",
                "AdaBelief Based on the classic optimizer Adam [25], AdaBelief [26] adjust the training stride according to the Belief in the gradient direction.",
                "Experiment IV: AdaBelief Optimization Experiment\nFinally, this paper presents a comparative experiment on the optimization performance of Adabelief.",
                "AdaBelief replaces vt in Adam with st. vt and st are EMA of gt2 and (gt \u2212Mt)2, respectively.",
                "As can be seen from Table 2, although AdaBelief achieved the best prediction accuracy, it did not have the best convergence rate.",
                "(4) Optimizing the proposed model using AdaBelief can significantly improve the prediction accuracy, but will reduce the convergence speed.",
                "Algorithm 2 Adam 1:Intialize \u03b80, M0 \u2190 0, v0 \u2190 0, t\u2190 0 2:While \u03b8 is not converged: t\u2190 t + 1, gt \u2190 \u2207\u03b8 ft(\u03b8t\u22121), Mt \u2190 \u03b21Mt\u22121 + (1\u2212 \u03b21)gt, vt \u2190 \u03b22st\u22121 + (1\u2212 \u03b22)gt2\n3:Update \u03b8t \u2190 \u220f F ,\u221avt ( \u03b8t\u22121 \u2212 \u03b1 Mt\u221avt+\u03b5 )\nAlgorithm 3 AdaBelief 1:Intialize \u03b80, M0 \u2190 0, s0 \u2190 0, t\u2190 0 2:While \u03b8 is not converged: t\u2190 t + 1, gt \u2190 \u2207\u03b8 ft(\u03b8t\u22121), Mt \u2190 \u03b21Mt\u22121 + (1\u2212 \u03b21)gt, st \u2190 \u03b22st\u22121 + (1\u2212 \u03b22)(gt \u2212Mt)2\n3:Update \u03b8t \u2190 \u220f F ,\u221ast ( \u03b8t\u22121 \u2212 \u03b1 Mt\u221ast+\u03b5 )\nWhere gt represents the t-th step, Mt represents the exponential moving average (EMA) of gt, and \u03b1 is learning rate."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4dce51b0ce925df0912961e246498e46b93f66f0",
                "externalIds": {
                    "DOI": "10.3390/en15124198",
                    "CorpusId": 249592399
                },
                "corpusId": 249592399,
                "publicationVenue": {
                    "id": "1cd505d9-195d-4f99-b91c-169e872644d4",
                    "name": "Energies",
                    "type": "journal",
                    "issn": "1996-1073",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155563",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155563",
                        "https://www.mdpi.com/journal/energies",
                        "http://www.mdpi.com/journal/energies"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4dce51b0ce925df0912961e246498e46b93f66f0",
                "title": "Self-Attention-Based Short-Term Load Forecasting Considering Demand-Side Management",
                "abstract": "Accurate and rapid forecasting of short-term loads facilitates demand-side management by electricity retailers. The complexity of customer demand makes traditional forecasting methods incapable of meeting the accuracy requirements, so a self-attention based short-term load forecasting (STLF) considering demand-side management is proposed. In the data preprocessing stage, non-parametric kernel density estimation is used to construct customer electricity consumption feature curves, and then historical load data are used to delineate the feasible domain range for outlier detection. In the feature selection stage, the feature data are selected using variational modal decomposition and a maximum information coefficient to enhance the model prediction accuracy. In the model prediction stage, the decomposed intrinsic mode function components are independently predicted and reconstructed using an Informer based on improved self-attention. Additionally, the novel AdaBlief optimizer is used to optimize the model parameters. Cross-sectional and longitudinal experiments are conducted on a regional-level load dataset set in Spain. The experimental results prove that the proposed method is superior to other methods in STLF.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155387136",
                        "name": "Fan Yu"
                    },
                    {
                        "authorId": "2152503694",
                        "name": "Lei Wang"
                    },
                    {
                        "authorId": "1843443",
                        "name": "Qiaoyong Jiang"
                    },
                    {
                        "authorId": "2072780997",
                        "name": "Qunmin Yan"
                    },
                    {
                        "authorId": "2070399743",
                        "name": "Shi Qiao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another novel algorithm Adabelief intuitively adjusts the step size based on the \u201cbelief\u201d in the current gradient direction [18], wherein the \u201cbelief\u201d is determined by the difference between the observed gradient and the prediction."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9c0bf75075cf7d45d379d6dfa6caa71b7ff40d1f",
                "externalIds": {
                    "DBLP": "journals/apin/GuiLF23",
                    "DOI": "10.1007/s10489-022-03629-7",
                    "CorpusId": 249448976
                },
                "corpusId": 249448976,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9c0bf75075cf7d45d379d6dfa6caa71b7ff40d1f",
                "title": "A fast adaptive algorithm for training deep neural networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2168356218",
                        "name": "Yangting Gui"
                    },
                    {
                        "authorId": "49620653",
                        "name": "Dequan Li"
                    },
                    {
                        "authorId": "2031531510",
                        "name": "Runyue Fang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The convergence guarantee of the AdaBelief algorithm has been provided by the authors in discretetime [9].",
                "Following [9], these hyperparameters are selected as described below.",
                "However, in these experiments, few other existing methods such as Yogi, MSAVG, AdamW, and Fromage have achieved faster convergence of the training cost than AdaBelief [9].",
                "The AdaBelief algorithm is similar to Adam, except that \u2225\u2207if(x(t))\u2225(2) in (3) is replaced by \u2225\u2207if(x(t))\u2212 \u03bci(t)\u2225(2) so that 1 \u221a \u03bdi(t) represents the belief in observed gradient [9].",
                "1 at epoch 100 and 145; and a mini-batch size of 20 is used [9].",
                "In this section, we present experimental results on benchmark machine learning problems, comparing the convergence rate and test-set accuracy of the proposed AdamSSM algorithm with several other adaptive gradient methods: AdaBelief [9], AdaBound [10], Adam [8], AdamW [11], Fromage [12], MSVAG [13], RAdam [14], SGD [32], and Yogi [15].",
                "The notable ones among them include AdaBelief [9], AdaBound [10], AdamW [11], AMSGrad [6], Fromage [12], MSVAG [13], RAdam [14], and Yogi [15].",
                "Following [9], the l2-regularization hyperparameter is set to 5 \u00d7 10\u22124 for image classification and 1.",
                "To conduct the experiments, we adapt the experimental setup used in the recent AdaBelief paper [9] and the AdaBound paper [10].",
                "1 at epoch 150; and a mini-batch size of 128 is used [9, 10]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7ac91046fbdfeaa63455456ada3ea49df52b7997",
                "externalIds": {
                    "ArXiv": "2206.02034",
                    "DBLP": "journals/corr/abs-2206-02034",
                    "DOI": "10.48550/arXiv.2206.02034",
                    "CorpusId": 249395511
                },
                "corpusId": 249395511,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7ac91046fbdfeaa63455456ada3ea49df52b7997",
                "title": "A Control Theoretic Framework for Adaptive Gradient Optimizers in Machine Learning",
                "abstract": "Adaptive gradient methods have become popular in optimizing deep neural networks; recent examples include AdaGrad and Adam. Although Adam usually converges faster, variations of Adam, for instance, the AdaBelief algorithm, have been proposed to enhance Adam's poor generalization ability compared to the classical stochastic gradient method. This paper develops a generic framework for adaptive gradient methods that solve non-convex optimization problems. We first model the adaptive gradient methods in a state-space framework, which allows us to present simpler convergence proofs of adaptive optimizers such as AdaGrad, Adam, and AdaBelief. We then utilize the transfer function paradigm from classical control theory to propose a new variant of Adam, coined AdamSSM. We add an appropriate pole-zero pair in the transfer function from squared gradients to the second moment estimate. We prove the convergence of the proposed AdamSSM algorithm. Applications on benchmark machine learning tasks of image classification using CNN architectures and language modeling using LSTM architecture demonstrate that the AdamSSM algorithm improves the gap between generalization accuracy and faster convergence than the recent adaptive gradient methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1768752581",
                        "name": "Kushal Chakrabarti"
                    },
                    {
                        "authorId": "145229889",
                        "name": "N. Chopra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief [47] in combination with the lookahead optimizer [48] is adopted."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "145792247bff45a60b66e227ec9bc8ba680672a8",
                "externalIds": {
                    "DBLP": "conf/icpr/TaiFLSL22",
                    "ArXiv": "2206.01009",
                    "DOI": "10.1109/ICPR56361.2022.9956467",
                    "CorpusId": 249282653
                },
                "corpusId": 249282653,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/145792247bff45a60b66e227ec9bc8ba680672a8",
                "title": "Unified Recurrence Modeling for Video Action Anticipation",
                "abstract": "Forecasting future events based on evidence of current conditions is an innate skill of human beings, and key for predicting the outcome of any decision making. In artificial vision for example, we would like to predict the next human action before it happens, without observing the future video frames associated to it. Computer vision models for action anticipation are expected to collect the subtle evidence in the preamble of the target actions. In prior studies recurrence modeling often leads to better performance, the strong temporal inference is assumed to be a key element for reasonable prediction. To this end, we propose a unified recurrence modeling for video action anticipation via message passing framework. The information flow in space-time can be described by the interaction between vertices and edges, and the changes of vertices for each incoming frame reflects the underlying dynamics. Our model leverages self-attention as the building blocks for each of the message passing functions. In addition, we introduce different edge learning strategies that can be end-to-end optimized to gain better flexibility for the connectivity between vertices. Our experimental results demonstrate that our proposed method outperforms previous works on the large-scale EPIC-Kitchen dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27084141",
                        "name": "Tsung-Ming Tai"
                    },
                    {
                        "authorId": "3144258",
                        "name": "G. Fiameni"
                    },
                    {
                        "authorId": "2115293259",
                        "name": "Cheng-Kuang Lee"
                    },
                    {
                        "authorId": "144308998",
                        "name": "S. See"
                    },
                    {
                        "authorId": "1717522",
                        "name": "O. Lanz"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bc1e3da488f9f3bb8483e85c4eb061d4f6418bd4",
                "externalIds": {
                    "DOI": "10.1109/ICVR55215.2022.9847937",
                    "CorpusId": 251762758
                },
                "corpusId": 251762758,
                "publicationVenue": {
                    "id": "1908b4f6-3f86-4f57-a6e1-169166fa8142",
                    "name": "International Conference on Virtual Reality",
                    "type": "conference",
                    "alternate_names": [
                        "ICVR",
                        "Int Conf Virtual Real"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bc1e3da488f9f3bb8483e85c4eb061d4f6418bd4",
                "title": "The Improved Adaptive Algorithm of Deep Learning with Barzilai-Borwein Step Size",
                "abstract": "To solve the problem that it is difficult to determine the learning rate when training a neural network model, this paper proposes an improved adaptive algorithm based on the Barzilai-Borwein (BB) step size. In this paper, the new algorithm accelerates the model's training through the second-order momentum and adapts the learning rate according to the BB step size. We also set an adequate range for the learning rate to ensure the stability of adaptive adjustment and reduce the error of step size. Compared with different algorithms in a series of popular models, the new algorithm significantly avoids the tediousness of manually adjusting the learning rate and helps to improve the convergence speed. The results show that the new algorithm is feasible and effective.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108156482",
                        "name": "Zhijun Wang"
                    },
                    {
                        "authorId": "46225375",
                        "name": "He-Bei Gao"
                    },
                    {
                        "authorId": "2166827225",
                        "name": "Bin Zhang"
                    },
                    {
                        "authorId": "2182446604",
                        "name": "Zhou-Xiang Xu"
                    },
                    {
                        "authorId": "2108113063",
                        "name": "Xiao-Qin Zhang"
                    },
                    {
                        "authorId": "2164989697",
                        "name": "Hong Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "cfb10793da4aa4730e58a47f8aa555cd97f89f9d",
                "externalIds": {
                    "ArXiv": "2205.10725",
                    "DOI": "10.1103/PhysRevLett.130.258402",
                    "CorpusId": 258179632,
                    "PubMed": "37418715"
                },
                "corpusId": 258179632,
                "publicationVenue": {
                    "id": "16c9f9d4-bee1-435d-8c85-22a3deba109d",
                    "name": "Physical Review Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Rev Lett"
                    ],
                    "issn": "0031-9007",
                    "url": "https://journals.aps.org/prl/",
                    "alternate_urls": [
                        "http://journals.aps.org/prl/",
                        "http://prl.aps.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cfb10793da4aa4730e58a47f8aa555cd97f89f9d",
                "title": "Schr\u00f6dinger Dynamics and Berry Phase of Undulatory Locomotion.",
                "abstract": "Spectral mode representations play an essential role in various areas of physics, from quantum mechanics to fluid turbulence, but they are not yet extensively used to characterize and describe the behavioral dynamics of living systems. Here, we show that mode-based linear models inferred from experimental live-imaging data can provide an accurate low-dimensional description of undulatory locomotion in worms, centipedes, robots, and snakes. By incorporating physical symmetries and known biological constraints into the dynamical model, we find that the shape dynamics are generically governed by Schr\u00f6dinger equations in mode space. The eigenstates of the effective biophysical Hamiltonians and their adiabatic variations enable the efficient classification and differentiation of locomotion behaviors in natural, simulated, and robotic organisms using Grassmann distances and Berry phases. While our analysis focuses on a widely studied class of biophysical locomotion phenomena, the underlying approach generalizes to other physical or living systems that permit a mode representation subject to geometric shape constraints.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2166049296",
                        "name": "Alexander E. Cohen"
                    },
                    {
                        "authorId": "146085118",
                        "name": "Alasdair D. Hastewell"
                    },
                    {
                        "authorId": "143820834",
                        "name": "Sreeparna Pradhan"
                    },
                    {
                        "authorId": "14269940",
                        "name": "S. Flavell"
                    },
                    {
                        "authorId": "1798934",
                        "name": "J. Dunkel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief [22] is used as the optimizer and the max training epoch is set at 100, where an early stopping criterion is used if the model does not improve within 25 epochs."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "95709c5919e74ad36b40213658bfb8669fc5fea8",
                "externalIds": {
                    "DOI": "10.21437/l3das.2022-3",
                    "CorpusId": 249811654
                },
                "corpusId": 249811654,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/95709c5919e74ad36b40213658bfb8669fc5fea8",
                "title": "Cross-Stitch Network with Adaptive Loss Weightage for Sound Event Localization and Detection",
                "abstract": "A Sound Event Localization and Detection system is capable of identifying the type and source of an acoustic event in a 3-dimensional space. Typically, such a system is trained using a Multi-Task Learning (MTL) framework, where the loss propagated is a linear combination of individual task losses. However, it has been found that the hard-parameter sharing strategy for an MTL framework can degrade the system performance. In addition, deriving the optimal loss combination can be time-consuming empirically and may not be the best way. This work proposes a cross-stitch network with a novel attention module that improves the feature representations. Further, we propose the use of a loss balancing algorithm to weigh the loss contribution adaptively, thereby eliminating the need to tune the loss weightage empirically. The proposed system is then evaluated on L3DAS22 challenge dataset as a part of our challenge participation and achieves a significant performance improvement of over 20% compared to the state-of-the-art SELDnet. We also note that our system ranked 3 rd in the L3DAS22 challenge Task 2 without any data augmentation or external dataset to increase the training samples.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144585713",
                        "name": "T. K. Chan"
                    },
                    {
                        "authorId": "2052289396",
                        "name": "R. Das"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[37] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan.",
                "Other variations include (a) Nadam[34] that uses Nesterov momentum, (b) AdamW[35] that decouples the weight decay from the optimization step, (c) AdaBound [36] that maintains a dynamic upper and lower bound on the step size, (d) AdaBelief[37] uses a decaying average of estimated variance in the gradient in place of the running average of the squared gradients, (e) QHAdam[38] that replaces both momentum estimators in Adam with quasi-hyperbolic terms, etc.",
                "Other variations include (a) Nadam[34] that uses Nesterov momentum, (b) AdamW[35] that decouples the weight decay from the optimization step, (c) AdaBound [36] that maintains a dynamic upper and lower bound on the step size, (d) AdaBelief[37] uses a decaying average of estimated variance in the gradient in place of the running average of the squared gradients, (e) QHAdam[38] that replaces both momentum estimators in Adam with quasi-hyperbolic terms, etc. LAMB[9] used a layerwise adaptive version of Adam to pretrain large language models efficiently."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "00a836a546fae1e7afaa6c51c4d4829f2060575c",
                "externalIds": {
                    "ArXiv": "2205.10287",
                    "DBLP": "journals/corr/abs-2205-10287",
                    "DOI": "10.48550/arXiv.2205.10287",
                    "CorpusId": 248965287
                },
                "corpusId": 248965287,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/00a836a546fae1e7afaa6c51c4d4829f2060575c",
                "title": "On the SDEs and Scaling Rules for Adaptive Gradient Algorithms",
                "abstract": "Approximating Stochastic Gradient Descent (SGD) as a Stochastic Differential Equation (SDE) has allowed researchers to enjoy the benefits of studying a continuous optimization trajectory while carefully preserving the stochasticity of SGD. Analogous study of adaptive gradient methods, such as RMSprop and Adam, has been challenging because there were no rigorously proven SDE approximations for these methods. This paper derives the SDE approximations for RMSprop and Adam, giving theoretical guarantees of their correctness as well as experimental validation of their applicability to common large-scaling vision and language settings. A key practical result is the derivation of a $\\textit{square root scaling rule}$ to adjust the optimization hyperparameters of RMSprop and Adam when changing batch size, and its empirical validation in deep learning settings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49288855",
                        "name": "Sadhika Malladi"
                    },
                    {
                        "authorId": "41049476",
                        "name": "Kaifeng Lyu"
                    },
                    {
                        "authorId": "6191376",
                        "name": "A. Panigrahi"
                    },
                    {
                        "authorId": "145563459",
                        "name": "Sanjeev Arora"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief optimizer [31] is adopted to train the 1D-DRSETL model in this paper."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b568854355f3ff28a2330f729c531a4a1dafdc33",
                "externalIds": {
                    "DOI": "10.1088/1361-6501/ac6f46",
                    "CorpusId": 248761096
                },
                "corpusId": 248761096,
                "publicationVenue": {
                    "id": "80f85f47-9f79-4683-b9e0-9df43c2cc065",
                    "name": "Measurement science and technology",
                    "type": "journal",
                    "alternate_names": [
                        "Meas sci technol",
                        "Meas Sci Technol",
                        "Measurement Science and Technology"
                    ],
                    "issn": "0957-0233",
                    "url": "http://www.iop.org/EJ/journal/-page=scope/0957-0233/1",
                    "alternate_urls": [
                        "http://iopscience.org/mst",
                        "https://iopscience.iop.org/0957-0233"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b568854355f3ff28a2330f729c531a4a1dafdc33",
                "title": "1D-DRSETL: a novel unsupervised transfer learning method for cross-condition fault diagnosis of rolling bearing",
                "abstract": "Transfer learning can meet the challenge of cross-condition fault diagnosis. However, the diagnostic effectiveness of transfer learning in actual applications is unsatisfactory, mainly due to the great unbalance in labeling between testing and training samples. A one-dimensional dual residual squeeze-and-excitation transfer learning network (1D-DRSETL) is proposed for an unsupervised accurate intelligent diagnosis under cross-condition in this paper for unlabeled small sample. First, a special block is designed to obtain transferable features by adaptively focusing on fault-sensitive information. Second, the joint maximum mean discrepancy is utilized to deal with the feature matching problem under cross-conditions. Then, speed up model training with AdaBelief optimizer. Finally, cross-conditions transfer diagnosis experiments are designed to demonstrate the superiority of the method based on a self-made dataset and the publicly available rolling bearings dataset. The experimental results show that the proposed method can achieve higher fault diagnosis accuracy and better robustness under cross-conditions than the contrasting methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9290958",
                        "name": "Jinyu Tong"
                    },
                    {
                        "authorId": "2165295236",
                        "name": "Cang Liu"
                    },
                    {
                        "authorId": "3049990",
                        "name": "Jinde Zheng"
                    },
                    {
                        "authorId": "145654016",
                        "name": "Haiyang Pan"
                    },
                    {
                        "authorId": "48632119",
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "authorId": "9324466",
                        "name": "Jiahan Bao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients. arXiv doi:10.",
                "The chosen optimizer is AdaBelief [28] with a learning rate of 1e-3, and the loss function is Categorical Cross Entropy."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "699e2d99c1506ac63988b00e34e08e59b6640735",
                "externalIds": {
                    "ArXiv": "2205.08383",
                    "DBLP": "journals/corr/abs-2205-08383",
                    "DOI": "10.13140/RG.2.2.14341.01769",
                    "CorpusId": 248834463
                },
                "corpusId": 248834463,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/699e2d99c1506ac63988b00e34e08e59b6640735",
                "title": "Bias and Fairness on Multimodal Emotion Detection Algorithms",
                "abstract": "Numerous studies have shown that machine learning algorithms can latch onto protected attributes such as race and gender and generate predictions that systematically discriminate against one or more groups. To date the majority of bias and fairness research has been on unimodal models. In this work, we explore the biases that exist in emotion recognition systems in relationship to the modalities utilized, and study how multimodal approaches affect system bias and fairness. We consider audio, text, and video modalities, as well as all possible multimodal combinations of those, and find that text alone has the least bias, and accounts for the majority of the models' performances, raising doubts about the worthiness of multimodal emotion recognition systems when bias and fairness are desired alongside model performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2133452483",
                        "name": "Matheus Schmitz"
                    },
                    {
                        "authorId": "2165470778",
                        "name": "Rehan Ahmed"
                    },
                    {
                        "authorId": "2109673155",
                        "name": "Jim Cao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In a recent study [20], the Vanilla SGD has been replaced with many modern adaptive learning rate methods such as AdaGrad [31], Adam [32], and AdaBelief [33] to achieve even faster convergence speed and better performance."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1db5090a7f4898ea5484a9c09d0d7b57e1ab6e84",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-02666",
                    "ArXiv": "2205.02666",
                    "DOI": "10.1109/QSW59989.2023.00019",
                    "CorpusId": 248525067
                },
                "corpusId": 248525067,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1db5090a7f4898ea5484a9c09d0d7b57e1ab6e84",
                "title": "LAWS: Look Around and Warm-Start Natural Gradient Descent for Quantum Neural Networks",
                "abstract": "Variational quantum algorithms (VQAs) have recently received much attention due to their promising performance in Noisy Intermediate-Scale Quantum computers (NISQ). However, VQAs run on parameterized quantum circuits (PQC) with randomly initialized parameters are characterized by barren plateaus (BP) where the gradient vanishes exponentially in the number of qubits. In this paper, we proposed a Look Around Warm-Start (LAWS) quantum natural gradient (QNG) algorithm to mitigate the widespread existing BP issues. LAWS is a combinatorial optimization strategy taking advantage of model parameter initialization and fast convergence of QNG. LAWS repeatedly reinitializes parameter search space for the next iteration parameter update. The reinitialized parameter search space is carefully chosen by sampling the gradient close to the current optimal. Moreover, we present a unified framework (WS-SGD) for integrating parameter initialization techniques into the optimizer. We provide the convergence proof of the proposed framework for both convex and non-convex objective functions based on Polyak-Lojasiewicz (PL) condition. Our experiment results show that the proposed algorithm could mitigate the BP and have better generalization ability in quantum classification problems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51117228",
                        "name": "Zeyi Tao"
                    },
                    {
                        "authorId": "2110451712",
                        "name": "Jindi Wu"
                    },
                    {
                        "authorId": "2055702077",
                        "name": "Qi Xia"
                    },
                    {
                        "authorId": "2108647483",
                        "name": "Qun Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7f3a105018ea44d35b1cc72c8c7251e38bcf5576",
                "externalIds": {
                    "PubMedCentral": "10284915",
                    "ArXiv": "2204.11669",
                    "DBLP": "journals/corr/abs-2204-11669",
                    "DOI": "10.1038/s41746-023-00859-y",
                    "CorpusId": 248377253,
                    "PubMed": "37344684"
                },
                "corpusId": 248377253,
                "publicationVenue": {
                    "id": "ef485645-f75f-4344-8b9d-3c260e69503b",
                    "name": "npj Digital Medicine",
                    "alternate_names": [
                        "npj Digit Med"
                    ],
                    "issn": "2398-6352",
                    "url": "http://www.nature.com/npjdigitalmed/"
                },
                "url": "https://www.semanticscholar.org/paper/7f3a105018ea44d35b1cc72c8c7251e38bcf5576",
                "title": "Deep-learning-enabled brain hemodynamic mapping using resting-state fMRI",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "116019368",
                        "name": "Xirui Hou"
                    },
                    {
                        "authorId": "1742263224",
                        "name": "Pengfei Guo"
                    },
                    {
                        "authorId": "11792070",
                        "name": "Puyang Wang"
                    },
                    {
                        "authorId": "2100444",
                        "name": "Peiying Liu"
                    },
                    {
                        "authorId": "31535020",
                        "name": "D. Lin"
                    },
                    {
                        "authorId": "2072839844",
                        "name": "Hongli Fan"
                    },
                    {
                        "authorId": "2153433799",
                        "name": "Yang Li"
                    },
                    {
                        "authorId": "48901974",
                        "name": "Zhiliang Wei"
                    },
                    {
                        "authorId": "3401356",
                        "name": "Zixuan Lin"
                    },
                    {
                        "authorId": "48219885",
                        "name": "Dengrong Jiang"
                    },
                    {
                        "authorId": "2163480586",
                        "name": "Jin Jin"
                    },
                    {
                        "authorId": "2146669852",
                        "name": "Catherine Kelly"
                    },
                    {
                        "authorId": "145960981",
                        "name": "J. Pillai"
                    },
                    {
                        "authorId": "2163501051",
                        "name": "Judy Huang"
                    },
                    {
                        "authorId": "48718506",
                        "name": "M. Pinho"
                    },
                    {
                        "authorId": "2250074860",
                        "name": "Binu P. Thomas"
                    },
                    {
                        "authorId": "4281951",
                        "name": "B. Welch"
                    },
                    {
                        "authorId": "3260451",
                        "name": "Denise C. Park"
                    },
                    {
                        "authorId": "2158449",
                        "name": "V. Patel"
                    },
                    {
                        "authorId": "2872683",
                        "name": "A. Hillis"
                    },
                    {
                        "authorId": "2115605430",
                        "name": "Hanzhang Lu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "this paper, we use an adaptive moment based method named AdaBelief [9] for a gradient based optimization as detailed in section III-C.",
                "In [9], AdaBelief was shown to combine the fast convergence of Adam based strategies with the good generalization of stochastic gradient decent strategies.",
                "The method uses the recently proposed adaptive moment estimation algorithm AdaBelief [9] with 2022 IEEE 61st Conference on Decision and Control (CDC) December 6-9, 2022.",
                "In this work, we propose to use AdaBelief, a variant of Adam [9].",
                "999 and \u03b5 = 10\u221281 which are the typical parameters used in [9] for AdaBelief and Adam based strategies in practice [15]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "38ae2a8f0697f101b1967c6e655fe676190c2f49",
                "externalIds": {
                    "ArXiv": "2204.11380",
                    "DBLP": "journals/corr/abs-2204-11380",
                    "DOI": "10.1109/CDC51059.2022.9992558",
                    "CorpusId": 248377009
                },
                "corpusId": 248377009,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/38ae2a8f0697f101b1967c6e655fe676190c2f49",
                "title": "An Online Stochastic Optimization Approach for Insulin Intensification in Type 2 Diabetes with Attention to Pseudo-Hypoglycemia*",
                "abstract": "In this paper, we present a model free approach to calculate long-acting insulin doses for Type 2 Diabetic (T2D) subjects in order to bring their blood glucose (BG) concentration to be within a safe range. The proposed strategy tunes the parameters of a proposed control law by using a zerothorder online stochastic optimization approach for a defined cost function. The strategy uses gradient estimates obtained by a Recursive Least Square (RLS) scheme in an adaptive moment estimation based approach named AdaBelief. Additionally, we show how the proposed strategy with a feedback rating measurement can accommodate for a phenomena known as relative hypoglycemia or pseudo-hypoglycemia (PHG) in which subjects experience hypoglycemia symptoms depending on how quick their BG concentration is lowered. The performance of the insulin calculation strategy is demonstrated and compared with current insulin calculation strategies using simulations with three different models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1853923901",
                        "name": "Mohamad Al Ahdab"
                    },
                    {
                        "authorId": "39379168",
                        "name": "T. Knudsen"
                    },
                    {
                        "authorId": "1716087",
                        "name": "J. Stoustrup"
                    },
                    {
                        "authorId": "144640816",
                        "name": "J. Leth"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is a composition of Positive-Negative momentum [15] with \u03b22 = 1, AdaBelief [18], decoupled weight decay [7], LookAhead [17] with a merge time of 5 and \u03b1 = 0."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3b054ae84a1204a17a51e3224d7352939a66eca4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-11045",
                    "ArXiv": "2204.11045",
                    "DOI": "10.1109/CVPRW56347.2022.00536",
                    "CorpusId": 248377101
                },
                "corpusId": 248377101,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3b054ae84a1204a17a51e3224d7352939a66eca4",
                "title": "Investigating Neural Architectures by Synthetic Dataset Design",
                "abstract": "Recent years have seen the emergence of many new neural network structures (architectures and layers). To solve a given task, a network requires a certain set of abilities reflected in its structure. The required abilities depend on each task. There is so far no systematic study of the real capacities of the proposed neural structures. The question of what each structure can and cannot achieve is only partially answered by its performance on common benchmarks. Indeed, natural data contain complex unknown statistical cues. It is therefore impossible to know what cues a given neural structure is taking advantage of in such data. In this work, we sketch a methodology to measure the effect of each structure on a network\u2019s ability, by designing ad hoc synthetic datasets. Each dataset is tailored to assess a given ability and is reduced to its simplest form: each input contains exactly the amount of information needed to solve the task. We illustrate our methodology by building three datasets to evaluate each of the three following network properties: a) the ability to link local cues to distant inferences, b) the translation covariance and c) the ability to group pixels with the same characteristics and share information among them. Using a first simplified depth estimation dataset, we pinpoint a serious nonlocal deficit of the U-Net. We then evaluate how to resolve this limitation by embedding its structure with nonlocal layers, which allow computing complex features with long-range dependencies. Using a second dataset, we compare different positional encoding methods and use the results to further improve the U-Net on the depth estimation task. The third introduced dataset serves to demonstrate the need for self-attention-like mechanisms for resolving more realistic depth estimation tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2163450787",
                        "name": "Adrien Courtois"
                    },
                    {
                        "authorId": "27053481",
                        "name": "J. Morel"
                    },
                    {
                        "authorId": "153005108",
                        "name": "P. Arias"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "aea0bf57a450f249399ee61897deddba0b5107fc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-01536",
                    "ArXiv": "2204.01536",
                    "DOI": "10.1109/SSPD54131.2022.9896184",
                    "CorpusId": 247939328
                },
                "corpusId": 247939328,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/aea0bf57a450f249399ee61897deddba0b5107fc",
                "title": "Deep Learning for Spectral Filling in Radio Frequency Applications",
                "abstract": "Due to the Internet of Things (IoT) proliferation, Radio Frequency (RF) channels are increasingly congested with new kinds of devices, which carry unique and diverse communication needs. This poses complex challenges in modern digital communications, and calls for the development of technological innovations that (i) optimize capacity (bitrate) in limited bandwidth environments, (ii) integrate cooperatively with alreadydeployed RF protocols, and (iii) are adaptive to the ever-changing demands in modern digital communications. In this paper we present methods for applying deep neural networks for spectral filling. Given an RF channel transmitting digital messages with a pre-established modulation scheme, we automatically learn novel modulation schemes for sending extra information, in the form of additional messages, \u201caround\u201d the fixed-modulation signals (i.e., without interfering with them). In so doing, we effectively increase channel capacity without increasing bandwidth. We further demonstrate the ability to generate signals that closely resemble the original modulations, such that the presence of extra messages is undetectable to third-party listeners. We present three computational experiments demonstrating the efficacy of our methods, and conclude by discussing the implications of our results for modern RF applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32077057",
                        "name": "Matthew Setzler"
                    },
                    {
                        "authorId": "2000890245",
                        "name": "Elizabeth Coda"
                    },
                    {
                        "authorId": "4155554",
                        "name": "J. Rounds"
                    },
                    {
                        "authorId": "2102289777",
                        "name": "M. Vann"
                    },
                    {
                        "authorId": "2112890304",
                        "name": "Michael Girard"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Network AdaBelief [57] AdaBound [33] AdaGrad [9] Adam [25] AdamP [17] SLS [48] SAM [12] SGD [14] SGDP [17] RMSGD",
                "From a different lens, this work also sheds light on the behaviour of commonly practiced hyper-parameter tuning techniques like learning rate scheduling through decay methods [12, 14, 17, 32, 33, 57] or functional methods [16, 32, 41, 42]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "df9b2850011fcd6f29f075f846a563460f35d0a1",
                "externalIds": {
                    "DBLP": "conf/cvpr/HosseiniTP22",
                    "ArXiv": "2203.16723",
                    "DOI": "10.1109/CVPR52688.2022.01005",
                    "CorpusId": 247839135
                },
                "corpusId": 247839135,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/df9b2850011fcd6f29f075f846a563460f35d0a1",
                "title": "Exploiting Explainable Metrics for Augmented SGD",
                "abstract": "Explaining the generalization characteristics of deep learning is an emerging topic in advanced machine learning. There are several unanswered questions about how learning under stochastic optimization really works and why certain strategies are better than others. In this paper, we address the following question: can we probe intermediate layers of a deep neural network to identify and quantify the learning quality of each layer? With this question in mind, we propose new explainability metrics that measure the redundant information in a network's layers using a low-rank factorization framework and quantify a complexity measure that is highly correlated with the generalization performance of a given optimizer, network, and dataset. We subsequently exploit these metrics to augment the Stochastic Gradient Descent (SGD) optimizer by adaptively adjusting the learning rate in each layer to improve in generalization performance. Our augmented SGD - dubbed RMSGD - introduces minimal computational overhead compared to SOTA methods and outperforms them by exhibiting strong generalization characteristics across application, architecture, and dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143854143",
                        "name": "M. S. Hosseini"
                    },
                    {
                        "authorId": "2123320135",
                        "name": "Mathieu Tuli"
                    },
                    {
                        "authorId": "1705037",
                        "name": "K. Plataniotis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4b3bfdeea6b12f1f4ac0f4ee958828f58750ed93",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-14495",
                    "ArXiv": "2203.14495",
                    "DOI": "10.48550/arXiv.2203.14495",
                    "CorpusId": 247762174
                },
                "corpusId": 247762174,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4b3bfdeea6b12f1f4ac0f4ee958828f58750ed93",
                "title": "Conjugate Gradient Method for Generative Adversarial Networks",
                "abstract": "One of the training strategies of generative models is to minimize the Jensen--Shannon divergence between the model distribution and the data distribution. Since data distribution is unknown, generative adversarial networks (GANs) formulate this problem as a game between two models, a generator and a discriminator. The training can be formulated in the context of game theory and the local Nash equilibrium (LNE). It does not seem feasible to derive guarantees of stability or optimality for the existing methods. This optimization problem is far more challenging than the single objective setting. Here, we use the conjugate gradient method to reliably and efficiently solve the LNE problem in GANs. We give a proof and convergence analysis under mild assumptions showing that the proposed method converges to a LNE with three different learning rate update rules, including a constant learning rate. Finally, we demonstrate that the proposed method outperforms stochastic gradient descent (SGD) and momentum SGD in terms of best Frechet inception distance (FID) score and outperforms Adam on average. The code is available at \\url{https://github.com/Hiroki11x/ConjugateGradient_GAN}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065887868",
                        "name": "Hiroki Naganuma"
                    },
                    {
                        "authorId": "2018304",
                        "name": "H. Iiduka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Different U-Net architectures are investigated and we also assess the performance of a new adaptive optimizer, namely AdaBelief [17], versus other standard optimizers.",
                "In [17] it is claimed that AdaBelief achieves three goals: generalization, fast convergence, and training stability.",
                "We assessed the performance of the new AdaBelief optimizer [17] versus Adam [18] and the Stochastic Descent Gradient (SGD) [19]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9eba08a87aca403b576c2a6e4bfbc04da607f719",
                "externalIds": {
                    "DBLP": "conf/isbi/LahlouhCBSP22",
                    "DOI": "10.1109/ISBI52829.2022.9761708",
                    "CorpusId": 246973315
                },
                "corpusId": 246973315,
                "publicationVenue": {
                    "id": "a38e0d3d-6929-4868-b4e4-af8bbacf711e",
                    "name": "IEEE International Symposium on Biomedical Imaging",
                    "type": "conference",
                    "alternate_names": [
                        "ISBI",
                        "International Symposium on Biomedical Imaging",
                        "Int Symp Biomed Imaging",
                        "IEEE Int Symp Biomed Imaging"
                    ],
                    "issn": "1945-7928",
                    "alternate_issns": [
                        "1945-8452"
                    ],
                    "url": "http://www.biomedicalimaging.org/",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9eba08a87aca403b576c2a6e4bfbc04da607f719",
                "title": "Aortic Arch Anatomy Characterization from MRA: A CNN-Based Segmentation Approach",
                "abstract": "Neurovascular pathologies are often treated with the help of imaging to guide catheters inside arteries. However, positioning a microcatheter into the aortic arch and threading it through blood vessels for embolization, mechanical thrombectomy or stenting is a challenging task. Indeed, adverse aortic arch anatomies are frequently encountered, especially when the aortic arch is dilated, or the supra-aortic branches are elongated and tortuous. In this article, we propose a pipeline using convolutional neural networks for the segmentation of the aortic arch from magnetic resonance images for further anatomy classification purpose. This pipeline is composed of two successive modules, dedicated to the localization and the accurate segmentation of the aortic arch and the origin of supra-aortic branches, respectively. These segmentations are then used to generate 3D models from which the anatomy and the type of the aortic arches can be characterized. A quantitative evaluation of this approach, carried out on various U-Net architectures and different optimizers, leads to satisfactory segmentation results, then allowing a reliable characterization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114991782",
                        "name": "M. Lahlouh"
                    },
                    {
                        "authorId": "2720889",
                        "name": "Y. Chenoune"
                    },
                    {
                        "authorId": "2069378662",
                        "name": "R. Blanc"
                    },
                    {
                        "authorId": "2365409",
                        "name": "J. Szewczyk"
                    },
                    {
                        "authorId": "1790064",
                        "name": "Nicolas Passat"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In other words, we have improved the regret bound of [20].",
                "Our analysis follows a strategy similar to that used to analyse AdaBelief in [20].",
                "We have added the dimensionality d to the last quantity of (6) in the appendix of [20].",
                "which are obtained from the appendices of [20].",
                "In [20], the authors motivate the EMA of (mt \u2212 gt)(2) without explaining the inclusion of \u01eb.",
                "As will be shown later, we do not replace \u03b21t by \u03b21 when dealing with the quantity 1 2\u03b7t(1\u2212\u03b21t) [\u2016v 1/4 t (\u03b8t\u22121 \u2212 \u03b8 )\u2016(2)2 \u2212 \u2016v 1/4 t (\u03b8t \u2212 \u03b8 )\u2016(2)2] as is done in the derivation of (3) in the appendix of [20].",
                "Due to the great success of Adam in training DNNs, various extensions of Adam have been proposed, including AdamW [15], NAdam [16], Yogi [17], MSVAG [18], Fromage [19], and AdaBelief [20].",
                "Note that the upper bound we obtain is essentially tighter than that in [20] due to two minor corrections.",
                "In particular, the first term in (15) is of order O(1/T ) while the corresponding one in [20] is essentially of order 1/( \u221a T ).",
                "Note that (16) corresponds to (2) in the appendix of [20] for AdaBelief."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d1a029608df9d652418c776bf3dc6aa836216e7e",
                "externalIds": {
                    "ArXiv": "2203.13273",
                    "CorpusId": 256194616
                },
                "corpusId": 256194616,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d1a029608df9d652418c776bf3dc6aa836216e7e",
                "title": "A DNN Optimizer that Improves over AdaBelief by Suppression of the Adaptive Stepsize Range",
                "abstract": "We make contributions towards improving adaptive-optimizer performance. Our improvements are based on suppression of the range of adaptive stepsizes in the AdaBelief optimizer. Firstly, we show that the particular placement of the parameter epsilon within the update expressions of AdaBelief reduces the range of the adaptive stepsizes, making AdaBelief closer to SGD with momentum. Secondly, we extend AdaBelief by further suppressing the range of the adaptive stepsizes. To achieve the above goal, we perform mutual layerwise vector projections between the gradient g_t and its first momentum m_t before using them to estimate the second momentum. The new optimization method is referred to as Aida. Thirdly, extensive experimental results show that Aida outperforms nine optimizers when training transformers and LSTMs for NLP, and VGG and ResNet for image classification over CIAF10 and CIFAR100 while matching the best performance of the nine methods when training WGAN-GP models for image generation tasks. Furthermore, Aida produces higher validation accuracies than AdaBelief for training ResNet18 over ImageNet. Code is availableat this URL",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46172209",
                        "name": "Guoqiang Zhang"
                    },
                    {
                        "authorId": "48341306",
                        "name": "K. Niwa"
                    },
                    {
                        "authorId": "144015910",
                        "name": "W. Kleijn"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d37997597611ce026a8f50943710d76e4f7c6665",
                "externalIds": {
                    "DBLP": "journals/titb/YaoSHYSHC22",
                    "DOI": "10.1109/JBHI.2022.3162118",
                    "CorpusId": 247678164,
                    "PubMed": "35324451"
                },
                "corpusId": 247678164,
                "publicationVenue": {
                    "id": "eac74c9c-a5c0-417d-8088-8164a6a8bfb3",
                    "name": "IEEE journal of biomedical and health informatics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Journal of Biomedical and Health Informatics",
                        "IEEE j biomed health informatics",
                        "IEEE J Biomed Health Informatics"
                    ],
                    "issn": "2168-2194",
                    "url": "https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=6221020",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221020"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d37997597611ce026a8f50943710d76e4f7c6665",
                "title": "A Novel 3D Unsupervised Domain Adaptation Framework for Cross-Modality Medical Image Segmentation",
                "abstract": "We consider the problem of volumetric (3D) unsupervised domain adaptation (UDA) in cross-modality medical image segmentation, aiming to perform segmentation on the unannotated target domain (e.g. MRI) with the help of labeled source domain (e.g. CT). Previous UDA methods in medical image analysis usually suffer from two challenges: 1) they focus on processing and analyzing data at 2D level only, thus missing semantic information from the depth level; 2) one-to-one mapping is adopted during the style-transfer process, leading to insufficient alignment in the target domain. Different from the existing methods, in our work, we conduct a first of its kind investigation on multi-style image translation for complete image alignment to alleviate the domain shift problem, and also introduce 3D segmentation in domain adaptation tasks to maintain semantic consistency at the depth level. In particular, we develop an unsupervised domain adaptation framework incorporating a novel quartet self-attention module to efficiently enhance relationships between widely separated features in spatial regions on a higher dimension, leading to a substantial improvement in segmentation accuracy in the unlabeled target domain. In two challenging cross-modality tasks, specifically brain structures and multi-organ abdominal segmentation, our model is shown to outperform current state-of-the-art methods by a significant margin, demonstrating its potential as a benchmark resource for the biomedical and health informatics research community.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390975870",
                        "name": "Kai Yao"
                    },
                    {
                        "authorId": "2108747698",
                        "name": "Zixian Su"
                    },
                    {
                        "authorId": "5380819",
                        "name": "Kaizhu Huang"
                    },
                    {
                        "authorId": "50030828",
                        "name": "Xi Yang"
                    },
                    {
                        "authorId": "2143845451",
                        "name": "Jie Sun"
                    },
                    {
                        "authorId": "145125161",
                        "name": "A. Hussain"
                    },
                    {
                        "authorId": "1706223",
                        "name": "Frans Coenen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "58ab7d67f2eeb4d753cf443da3bccaeaa7897fde",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-08065",
                    "ArXiv": "2203.08065",
                    "DOI": "10.48550/arXiv.2203.08065",
                    "CorpusId": 247450846
                },
                "corpusId": 247450846,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/58ab7d67f2eeb4d753cf443da3bccaeaa7897fde",
                "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
                "abstract": "The recently proposed Sharpness-Aware Minimization (SAM) improves generalization by minimizing a \\textit{perturbed loss} defined as the maximum loss within a neighborhood in the parameter space. However, we show that both sharp and flat minima can have a low perturbed loss, implying that SAM does not always prefer flat minima. Instead, we define a \\textit{surrogate gap}, a measure equivalent to the dominant eigenvalue of Hessian at a local minimum when the radius of the neighborhood (to derive the perturbed loss) is small. The surrogate gap is easy to compute and feasible for direct minimization during training. Based on the above observations, we propose Surrogate \\textbf{G}ap Guided \\textbf{S}harpness-\\textbf{A}ware \\textbf{M}inimization (GSAM), a novel improvement over SAM with negligible computation overhead. Conceptually, GSAM consists of two steps: 1) a gradient descent like SAM to minimize the perturbed loss, and 2) an \\textit{ascent} step in the \\textit{orthogonal} direction (after gradient decomposition) to minimize the surrogate gap and yet not affect the perturbed loss. GSAM seeks a region with both small loss (by step 1) and low sharpness (by step 2), giving rise to a model with high generalization capabilities. Theoretically, we show the convergence of GSAM and provably better generalization than SAM. Empirically, GSAM consistently improves generalization (e.g., +3.2\\% over SAM and +5.4\\% over AdamW on ImageNet top-1 accuracy for ViT-B/32). Code is released at \\url{ https://sites.google.com/view/gsam-iclr22/home}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46251934",
                        "name": "Juntang Zhuang"
                    },
                    {
                        "authorId": "40206014",
                        "name": "Boqing Gong"
                    },
                    {
                        "authorId": "36001694",
                        "name": "Liangzhe Yuan"
                    },
                    {
                        "authorId": "2115350367",
                        "name": "Yin Cui"
                    },
                    {
                        "authorId": "2595180",
                        "name": "Hartwig Adam"
                    },
                    {
                        "authorId": "5507046",
                        "name": "N. Dvornek"
                    },
                    {
                        "authorId": "1688323",
                        "name": "S. Tatikonda"
                    },
                    {
                        "authorId": "2158859164",
                        "name": "James S. Duncan"
                    },
                    {
                        "authorId": "2115431213",
                        "name": "Ting Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7456058418256eb859c5957988bedc82685120f6",
                "externalIds": {
                    "ArXiv": "2203.01603",
                    "DBLP": "journals/corr/abs-2203-01603",
                    "DOI": "10.48550/arXiv.2203.01603",
                    "CorpusId": 247222737
                },
                "corpusId": 247222737,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7456058418256eb859c5957988bedc82685120f6",
                "title": "AdaFamily: A family of Adam-like adaptive gradient methods",
                "abstract": "We propose AdaFamily, a novel method for training deep neural networks. It is a family of adaptive gradient methods and can be interpreted as sort of a blend of the optimization algorithms Adam, AdaBelief and AdaMomentum. We perform experiments on standard datasets for image classification, demonstrating that our proposed method outperforms these algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2343941",
                        "name": "Hannes Fassold"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d116b93d405399b949518453cbff46e51f8e6d60",
                "externalIds": {
                    "PubMedCentral": "8954820",
                    "DOI": "10.3390/ijms23063354",
                    "CorpusId": 247602495,
                    "PubMed": "35328775"
                },
                "corpusId": 247602495,
                "publicationVenue": {
                    "id": "8506a01a-40b8-4e6f-bbb8-ce2492139c15",
                    "name": "International Journal of Molecular Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Mol Sci"
                    ],
                    "issn": "1422-0067",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-157693",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/ijms",
                        "http://www.mdpi.com/journal/ijms/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-157693"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d116b93d405399b949518453cbff46e51f8e6d60",
                "title": "Simulating the Feasibility of Using Liquid Micro-Jets for Determining Electron\u2013Liquid Scattering Cross-Sections",
                "abstract": "The extraction of electron\u2013liquid phase cross-sections (surface and bulk) is proposed through the measurement of (differential) energy loss spectra for electrons scattered from a liquid micro-jet. The signature physical elements of the scattering processes on the energy loss spectra are highlighted using a Monte Carlo simulation technique, originally developed for simulating electron transport in liquids. Machine learning techniques are applied to the simulated electron energy loss spectra, to invert the data and extract the cross-sections. The extraction of the elastic cross-section for neon was determined within 9% accuracy over the energy range 1\u2013100 eV. The extension toward the simultaneous determination of elastic and ionisation cross-sections resulted in a decrease in accuracy, now to within 18% accuracy for elastic scattering and 1% for ionisation. Additional methods are explored to enhance the accuracy of the simultaneous extraction of liquid phase cross-sections.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159602495",
                        "name": "Dale L. Muccignat"
                    },
                    {
                        "authorId": "2555343",
                        "name": "P. Stokes"
                    },
                    {
                        "authorId": "47689671",
                        "name": "D. Cocks"
                    },
                    {
                        "authorId": "3988904",
                        "name": "J. Gascooke"
                    },
                    {
                        "authorId": "145378899",
                        "name": "Darryl B Jones"
                    },
                    {
                        "authorId": "2819285",
                        "name": "M. Brunger"
                    },
                    {
                        "authorId": "2107867566",
                        "name": "R. White"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also adopted the use of AdaBelief, a new optimizer which has shown to converge as quickly as adaptive optimizers (such as Adam [40]) and to generalize better than Stochastic Gradient Descent (SGD) [41] in complex architectures such as GANs [42]; see Figure 3.",
                "Keywords: cycle GANs; semantic segmentation; patch extraction; saliency; classification; regression",
                "The mean squared error (MSE) loss function was used for training, and Adam [42,49] was used as the optimizer, with a circular scheduler having the lower and upper boundaries of 0.0001 and 0.00015, respectively; the step size used was equal to twice the size of the dataset.",
                "Other works that employ cycle-GANs for highly specialised tasks have shown the benefit of differing learning rates for the two sub-networks [38,39].",
                "A cycle-consistent GAN consists of two complementary GANs and aims to learn domain translation, with the key idea being that each generator learns to synthesise data from the corresponding domain.",
                "The mean squared error (MSE) loss function was used for training, and Adam [42,49] was used as the optimizer, with a circular scheduler having the lower and upper boundaries of 0.",
                "Stochastic gradient descent was used as the optimizer since it had been demonstrated to generalize better than Adam [40] in related image classification problems [42]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "129316cb6a75f9f4bccbd26ecae5645cbd294506",
                "externalIds": {
                    "DBLP": "journals/information/ZachariouASMS22",
                    "DOI": "10.3390/info13020096",
                    "CorpusId": 247035559
                },
                "corpusId": 247035559,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/129316cb6a75f9f4bccbd26ecae5645cbd294506",
                "title": "Tuberculosis Bacteria Detection and Counting in Fluorescence Microscopy Images Using a Multi-Stage Deep Learning Pipeline",
                "abstract": "The manual observation of sputum smears by fluorescence microscopy for the diagnosis and treatment monitoring of patients with tuberculosis (TB) is a laborious and subjective task. In this work, we introduce an automatic pipeline which employs a novel deep learning-based approach to rapidly detect Mycobacterium tuberculosis (Mtb) organisms in sputum samples and thus quantify the burden of the disease. Fluorescence microscopy images are used as input in a series of networks, which ultimately produces a final count of present bacteria more quickly and consistently than manual analysis by healthcare workers. The pipeline consists of four stages: annotation by cycle-consistent generative adversarial networks (GANs), extraction of salient image patches, classification of the extracted patches, and finally, regression to yield the final bacteria count. We empirically evaluate the individual stages of the pipeline as well as perform a unified evaluation on previously unseen data that were given ground-truth labels by an experienced microscopist. We show that with no human intervention, the pipeline can provide the bacterial count for a sample of images with an error of less than 5%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2007525274",
                        "name": "Marios Zachariou"
                    },
                    {
                        "authorId": "46837178",
                        "name": "Ognjen Arandjelov\u00edc"
                    },
                    {
                        "authorId": "49582394",
                        "name": "W. Sabiiti"
                    },
                    {
                        "authorId": "6472536",
                        "name": "B. Mtafya"
                    },
                    {
                        "authorId": "145106575",
                        "name": "D. Sloan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We used the AdaBelief optimizer [33] with a learning rate of 10\u22124 to optimize the transformer\u2019s weights.",
                "[33] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan, \u201cAdabelief optimizer: Adapting stepsizes by the belief in observed gradients,\u201d Conference on Neural Information Processing Systems (2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4321a76a5a4ac4b06763a80773fa00df4fdc2376",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-08708",
                    "ArXiv": "2202.08708",
                    "CorpusId": 246904664
                },
                "corpusId": 246904664,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4321a76a5a4ac4b06763a80773fa00df4fdc2376",
                "title": "Learning stochastic dynamics and predicting emergent behavior using transformers",
                "abstract": "We show that a neural network originally designed for language processing can learn the dynamical rules of a stochastic system by observation of a single dynamical trajectory of the system, and can accurately predict its emergent behavior under conditions not observed during training. We consider a lattice model of active matter undergoing continuous-time Monte Carlo dynamics, simulated at a density at which its steady state comprises small, dispersed clusters. We train a neural network called a transformer on a single trajectory of the model. The transformer, which we show has the capacity to represent dynamical rules that are numerous and nonlocal, learns that the dynamics of this model consists of a small number of processes. Forward-propagated trajectories of the trained transformer, at densities not encountered during training, exhibit motility-induced phase separation and so predict the existence of a nonequilibrium phase transition. Transformers have the flexibility to learn dynamical rules from observation without explicit enumeration of rates or coarse-graining of configuration space, and so the procedure used here can be applied to a wide range of physical systems, including those with large and complex dynamical generators.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "102258022",
                        "name": "Corneel Casert"
                    },
                    {
                        "authorId": "47465696",
                        "name": "Isaac Tamblyn"
                    },
                    {
                        "authorId": "2574566",
                        "name": "S. Whitelam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The models were trained using the AdaBelief [Zhuang et al., 2020] optimization algorithm with a learning rate of 1\u00d7 10\u22123 and a batch size of 256, and \u03b1 = 10.0.",
                "The model was trained for around 4 hours on a NVIDIA 3090 gpu with a learning rate of 1\u00d7 10\u22124\nTrue density Learned density Learned rank\nwith the AdaBelief [Zhuang et al., 2020] optimization algorithm and a batch size of 2048 and \u03b1 = 5.0.",
                "\u2026Jki(g(x)) 2 ) + \u03b3||f(g(x))\u2212 x||2, k \u223c Uniform(1, . . . , 10)\n(79)\nObjective1iNF = \u2211 x\u2208D \u2212 log pz(g(x)) + 1 2 log |J(g(x))TJ(g(x))|+ \u03b3||f(g(x))\u2212 x||2 (80)\nFor both models we set \u03b3 = 10, used a batch size of 64, learning rate of 1\u00d7 10\u22124 and the AdaBelief [Zhuang et al., 2020] optimization algorithm.",
                "The objectives we optimized were: Objective1iPF = \u2211 x\u2208D \u2212 log pz(g(x)) + dim(z) 2 log (\u2211 i Jki(g(x)) 2 ) + \u03b3||f(g(x))\u2212 x||2, k \u223c Uniform(1, . . . , 10)\n(79)\nObjective1iNF = \u2211 x\u2208D \u2212 log pz(g(x)) + 1 2 log |J(g(x))TJ(g(x))|+ \u03b3||f(g(x))\u2212 x||2 (80)\nFor both models we set \u03b3 = 10, used a batch size of 64, learning rate of 1\u00d7 10\u22124 and the AdaBelief [Zhuang et al., 2020] optimization algorithm."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e9e0ee3a91f3a0af92827276b0721ccb3dd0c400",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-07037",
                    "ArXiv": "2202.07037",
                    "CorpusId": 246863832
                },
                "corpusId": 246863832,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e9e0ee3a91f3a0af92827276b0721ccb3dd0c400",
                "title": "Principal Manifold Flows",
                "abstract": "Normalizing flows map an independent set of latent variables to their samples using a bijective transformation. Despite the exact correspondence between samples and latent variables, their high level relationship is not well understood. In this paper we characterize the geometric structure of flows using principal manifolds and understand the relationship between latent variables and samples using contours. We introduce a novel class of normalizing flows, called principal manifold flows (PF), whose contours are its principal manifolds, and a variant for injective flows (iPF) that is more efficient to train than regular injective flows. PFs can be constructed using any flow architecture, are trained with a regularized maximum likelihood objective and can perform density estimation on all of their principal manifolds. In our experiments we show that PFs and iPFs are able to learn the principal manifolds over a variety of datasets. Additionally, we show that PFs can perform density estimation on data that lie on a manifold with variable dimensionality, which is not possible with existing normalizing flows.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056450967",
                        "name": "Edmond Cunningham"
                    },
                    {
                        "authorId": "36119737",
                        "name": "Adam D. Cobb"
                    },
                    {
                        "authorId": "37747652",
                        "name": "Susmit Jha"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thus, in future research, we would consider employing a large dataset and carryout more exhaustive tests to optimize the performance of the deep learning networks and test other algorithms such as AdaBelief [52] optimizer which converges fast and has high accuracy on image classification and language modeling."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "35f7c9ae3417c54ea616c5d97131669b480f9330",
                "externalIds": {
                    "DOI": "10.3390/app12041957",
                    "CorpusId": 246846201
                },
                "corpusId": 246846201,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/35f7c9ae3417c54ea616c5d97131669b480f9330",
                "title": "Application of Deep Learning to Construct Breast Cancer Diagnosis Model",
                "abstract": "(1) Background: According to Taiwan\u2019s ministry of health statistics, the rate of breast cancer in women is increasing annually. Each year, more than 10,000 women suffer from breast cancer, and over 2000 die of the disease. The mortality rate is annually increasing, but if breast cancer tumors are detected earlier, and appropriate treatment is provided immediately, the survival rate of patients will increase enormously. (2) Methods: This research aimed to develop a stepwise breast cancer model architecture to improve diagnostic accuracy and reduce the misdiagnosis rate of breast cancer. In the first stage, a breast cancer risk factor dataset was utilized. After pre-processing, Artificial Neural Network (ANN) and the support vector machine (SVM) were applied to the dataset to classify breast cancer tumors and compare their performances. The ANN achieved 76.6% classification accuracy, and the SVM using radial functions achieved the best classification accuracy of 91.6%. Therefore, SVM was utilized in the determination of results concerning the relevant breast cancer risk factors. In the second stage, we trained AlexNet, ResNet101, and InceptionV3 networks using transfer learning. The networks were studied using Adaptive Moment Estimation (ADAM) and Stochastic Gradient Descent with Momentum (SGDM) based optimization algorithm to diagnose benign and malignant tumors, and the results were evaluated; (3) Results: According to the results, AlexNet obtained 81.16%, ResNet101 85.51%, and InceptionV3 achieved a remarkable accuracy of 91.3%. The results of the three models were utilized in establishing a voting combination, and the soft-voting method was applied to average the prediction result for which a test accuracy of 94.20% was obtained; (4) Conclusions: Despite the small number of images in this study, the accuracy is higher compared to other literature. The proposed method has demonstrated the need for an additional productive tool in clinical settings when radiologists are evaluating mammography images of patients.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40300174",
                        "name": "Rong-Ho Lin"
                    },
                    {
                        "authorId": "2154510983",
                        "name": "Benjamin Kofi Kujabi"
                    },
                    {
                        "authorId": "30743682",
                        "name": "Chun-Ling Chuang"
                    },
                    {
                        "authorId": "2155611818",
                        "name": "Ching-Shun Lin"
                    },
                    {
                        "authorId": "2154586626",
                        "name": "Chun-Jen Chiu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Subsequently, other variants of Adam are proposed in [47, 48, 49, 50, 51, 52, 53].",
                "[53] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "98850975e574e08695a9f32b4c8747dc7f8bcc17",
                "externalIds": {
                    "ArXiv": "2202.06009",
                    "DBLP": "journals/corr/abs-2202-06009",
                    "CorpusId": 246823969
                },
                "corpusId": 246823969,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/98850975e574e08695a9f32b4c8747dc7f8bcc17",
                "title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam",
                "abstract": "1-bit gradient compression and local steps are two representative techniques that enable drastic communication reduction in distributed SGD. Their benefits, however, remain an open question on Adam-based large model pre-training (e.g. BERT and GPT). In this paper, we demonstrate the non-linearity in Adam causes slow convergence even when 1-bit compression or local steps are individually applied. To alleviate this limitation, we propose 0/1 Adam that linearizes each Adam step via approximating its optimizer states using their stale estimates and linear correlation. 0/1 Adam performs an Adam-like step to preserve the adaptivity, while its linearity allows utilizing 1-bit compression and local steps simultaneously for wall-clock time speed up. We provide convergence guarantee for 0/1 Adam on smooth non-convex objectives. On various large-scale benchmarks such as BERT-Base, BERT-Large, GPT-2 pre-training and ImageNet, we demonstrate on up to 128 GPUs that 0/1 Adam is able to reduce up to 87% of data volume, 54% of communication rounds, and achieve up to 2$\\times$ higher training throughput and end-to-end training time reduction compared to the state-of-the-art baseline 1-bit Adam; while enjoying the same statistical convergence speed and end task model accuracy on GLUE dataset and ImageNet validation set.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1454006122",
                        "name": "Yucheng Lu"
                    },
                    {
                        "authorId": "2609325",
                        "name": "Conglong Li"
                    },
                    {
                        "authorId": "2112111675",
                        "name": "Minjia Zhang"
                    },
                    {
                        "authorId": "1801197",
                        "name": "Christopher De Sa"
                    },
                    {
                        "authorId": "2145020341",
                        "name": "Yuxiong He"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1, a batch size of 32, with the AdaBelief optimizer [Zhuang et al., 2020], a learning rate of 1 \u00b7 10\u22123, gradient norm clipped to 1, weight decay rate of 0.",
                "All networks have been trained with crossentropy, label smoothing of 0.1, a batch size of 32, with the AdaBelief optimizer [Zhuang et al., 2020], a learning rate of 1 \u00b7 10\u22123, gradient norm clipped to 1, weight decay rate of 0.1, with Stochastic Weight Averaging [Izmailov et al., 2018] and Decoupled\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "76a219a60c19255d295173bd67672d41e1efe618",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-00282",
                    "ArXiv": "2202.00282",
                    "CorpusId": 246442303
                },
                "corpusId": 246442303,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/76a219a60c19255d295173bd67672d41e1efe618",
                "title": "Surrogate Gradients Design",
                "abstract": "Surrogate gradient (SG) training provides the possibility to quickly transfer all the gains made in deep learning to neuromorphic computing and neuromorphic processors, with the consequent reduction in energy consumption. Evidence supports that training can be robust to the choice of SG shape, after an extensive search of hyper-parameters. However, random or grid search of hyper-parameters becomes exponentially unfeasible as we consider more hyper-parameters. Moreover, every point in the search can itself be highly time and energy consuming for large networks and large datasets. In this article we show how complex tasks and networks are more sensitive to SG choice. Secondly, we show how low dampening, high sharpness and low tail fatness are preferred. Thirdly, we observe that Glorot Uniform initialization is generally preferred by most SG choices, with variability in the results. We finally provide a theoretical solution to reduce the need of extensive gridsearch, to find SG shape and initializations that result in improved accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151859590",
                        "name": "Luca Herranz-Celotti"
                    },
                    {
                        "authorId": "1680808",
                        "name": "J. Rouat"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The proposed model was implemented in Pytorch, where we used the AdaBelief optimization algorithm to train the network [79]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f9f26818972d46ea261b184c93d567b33b436a81",
                "externalIds": {
                    "PubMedCentral": "8876295",
                    "DOI": "10.3390/jpm12020310",
                    "CorpusId": 247044555,
                    "PubMed": "35207797"
                },
                "corpusId": 247044555,
                "publicationVenue": {
                    "id": "721ed878-9136-408b-b2a3-dce27b9fa39e",
                    "name": "Journal of Personalized Medicine",
                    "type": "journal",
                    "alternate_names": [
                        "J Pers Med"
                    ],
                    "issn": "2075-4426",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-225205",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jpm",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-225205"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f9f26818972d46ea261b184c93d567b33b436a81",
                "title": "COVID-19 Detection in CT/X-ray Imagery Using Vision Transformers",
                "abstract": "The steady spread of the 2019 Coronavirus disease has brought about human and economic losses, imposing a new lifestyle across the world. On this point, medical imaging tests such as computed tomography (CT) and X-ray have demonstrated a sound screening potential. Deep learning methodologies have evidenced superior image analysis capabilities with respect to prior handcrafted counterparts. In this paper, we propose a novel deep learning framework for Coronavirus detection using CT and X-ray images. In particular, a Vision Transformer architecture is adopted as a backbone in the proposed network, in which a Siamese encoder is utilized. The latter is composed of two branches: one for processing the original image and another for processing an augmented view of the original image. The input images are divided into patches and fed through the encoder. The proposed framework is evaluated on public CT and X-ray datasets. The proposed system confirms its superiority over state-of-the-art methods on CT and X-ray data in terms of accuracy, precision, recall, specificity, and F1 score. Furthermore, the proposed system also exhibits good robustness when a small portion of training data is allocated.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "134348127",
                        "name": "M. M. Al Rahhal"
                    },
                    {
                        "authorId": "1795469",
                        "name": "Y. Bazi"
                    },
                    {
                        "authorId": "40470677",
                        "name": "R. M. Jomaa"
                    },
                    {
                        "authorId": "3013948",
                        "name": "Ahmad AlShibli"
                    },
                    {
                        "authorId": "1755194",
                        "name": "N. Alajlan"
                    },
                    {
                        "authorId": "2393332",
                        "name": "M. L. Mekhalfi"
                    },
                    {
                        "authorId": "1774633",
                        "name": "F. Melgani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This implies that TTUR based on AdaBelief is a powerful way to train GANs.",
                "Zhuang, J., Tang, T., Ding, Y., Tatikonda, S., Dvornek, N., Papademetris, X., and Duncan, J. S. AdaBelief optimizer: Adapting stepsizes by the belief in observed gradients.",
                "1) such as Adam (Kingma & Ba, 2015), AdaBelief (Zhuang et al., 2020) and RMSProp (Tieleman & Hinton, 2012).",
                "Figure 2 indicates that the measured critical batch sizes minimizing the SFO complexities of Adam, AdaBelief, and RMSProp are 2 5 = 32 , 2 5 = 32 , and 2 3 = 8 , respectively.",
                "By referring to the results in (Chen et al., 2019; Iiduka, 2022a; Zhuang et al., 2020), we can check that Hn and H D n in Table 4 satisfy (A1) and (A2).",
                "It was shown numerically that TTUR based on AdaBelief (short for adapting step sizes by the belief in observed gradients) (Zhuang et al., 2020) with constant learning rates \u03b1 G and \u03b1 D has good scores in terms of the Fr\u00b4echet inception distance (FID) (Heusel et al., 2017), which is a performance measure of optimizers for training GANs.",
                "It was shown numerically that TTUR based on AdaBelief (short for adapting step sizes by the belief in observed gradients) (Zhuang et al., 2020) with constant learning rates \u03b1 and \u03b1 has good scores in terms of the Fr\u00e9chet inception distance (FID) (Heusel et al.",
                "We de\ufb01ne the inner product of x , y \u2208 R d by \u27e8 x , y \u27e9 := x \u22a4 y and the norm of p which are equivalent to the following variational inequalities (see Appendix A.11): for all \u03b8 \u2208 R \u0398 and all w \u2208 R W , (1) Let us examine TTURs based on adaptive methods (see Algorithm 1 and Table 4 in Appendix A.1) such as Adam (Kingma & Ba, 2015), AdaBelief (Zhuang et al., 2020), and RMSProp (Tieleman & Hinton, 2012).",
                "Proposition 3.4(i) and (ii) indicate that the estimated critical batch sizes of Adam and AdaBelief strongly depend on the values of \u03b2 1 and \u03b2 2 .",
                "Figure 4 indicates that the measured critical batch sizes for Adam, AdaBelief, and RMSProp are 2 , 2 2 = 4 , and 2 6 = 64 , respectively.",
                "99 for AdaBelief (see Table 3).",
                "9 for AdaBelief and 126 .",
                "In particular, the numerical results in (Heusel et al., 2017; Zhuang et al., 2020) show that TTUR performs well with constant learning rates.",
                ", 2019) v n = \u03b7 v n\u22121 + (1\u2212 \u03b7)pn v n = \u03b7v n\u22121 + (1\u2212 \u03b7)pn (\u03b3 = \u03b3 = 0) v\u0302 n = (max{v\u0302 n\u22121,i, v n,i})i=1 v\u0302 n = (max{v\u0302 n\u22121,i, v n,i})i=1 Hn = diag( \u221a v\u0302G n,i) H D n = diag( \u221a v\u0302D n,i) AdaBelief p\u0303n = \u2207LG,Sn(\u03b8n)\u2212mn p\u0303n = \u2207LD,Rn(wn)\u2212mn (Zhuang et al., 2020) s\u0303n = p\u0303 G n p\u0303n s\u0303n = p\u0303n p\u0303n (sn,i \u2264 sn+1,i) sn = \u03b2 2 v n\u22121 + (1\u2212 \u03b2 2 )s\u0303n sn = \u03b2 2 v n\u22121 + (1\u2212 \u03b2 2 )s\u0303n (sn,i \u2264 sn+1,i) \u015dn = sGn 1\u2212\u03b2G 2 \u015dn = sDn 1\u2212\u03b2D 2 (\u03b3 = \u03b3 = \u03b2 1 = \u03b2 D 1 ) H G n = diag( \u221a \u015dn,i) H D n = diag( \u221a \u015dn,i)",
                "Accordingly, the estimated critical batch sizes of Adam and AdaBelief were found to be the same as the measured ones.",
                "25 for AdaBelief, and 20 ."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "0639922d9b590d062ae183018c596b213cb3b04b",
                "externalIds": {
                    "DBLP": "conf/icml/SatoI23",
                    "ArXiv": "2201.11989",
                    "CorpusId": 258437330
                },
                "corpusId": 258437330,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0639922d9b590d062ae183018c596b213cb3b04b",
                "title": "Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule",
                "abstract": "Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide numerical results indicating that the number of steps needed to achieve a low FID score decreases as the batch size increases and that the SFO complexity increases once the batch size exceeds the measured critical batch size. Moreover, we show that measured critical batch sizes are close to the sizes estimated from our theoretical results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1659997847",
                        "name": "Naoki Sato"
                    },
                    {
                        "authorId": "2018304",
                        "name": "H. Iiduka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Hence, Problem 2 with X = Rd can be expressed as the problem [17], [18] of finding a local minimizer of f over Rd , that is,",
                "The convergence rate analyses for SGD [10], mini-batch SGD (MSGD) [11], AMSGrad [17], and AdaBelief [18] are summarized in",
                "Variants of Adam and AMSGrad have been presented that adapt the step sizes; they include belief in observed gradients (AdaBelief) [18] and AMSGrad with weighted gradient and dynamic bound (AMSGWDC) [19].",
                "The constant learning rate rule is used for SGD [10], SPIDER [20], and ALROAs [21], while the diminishing learning rate rule is used for SGD [10], MSGD [11], AMSGrad [17], AdaBelief [18], and ALROAs [21].",
                "AdaBelief was presented in [18]; it is defined as",
                "This implies that the convergence rate of the existing ALROAs, such as AMSGrad and AdaBelief, is O(1/\u221ak), which is an improvement on the previous results [17], [18] in Table I.",
                "Table II lists examples of Hk and shows that Algorithm 1 with X = Rd includes the existing ALROAs, such as Nesterov momentum [23], [24], AMSGrad [16], [17], and AMSGWDC [19],(2) AdaBelief [18], and modified Adam (MAdam) [21], for unconstrained nonconvex finite-sum optimization [see also (7) and (9) for the definitions of AMSGrad and AdaBelief].",
                "The previous studies [10], [11], [17], [18], [21] showed that SGD, AMSGrad, and the variants of Adam and AMSGrad can perform nonconvex optimization in deep neural networks (see Table I)."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "46f318d3c8af48b45d6132332f4b1ec73ac2ea23",
                "externalIds": {
                    "DOI": "10.1109/TNNLS.2022.3142726",
                    "CorpusId": 246388086,
                    "PubMed": "35089865"
                },
                "corpusId": 246388086,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/46f318d3c8af48b45d6132332f4b1ec73ac2ea23",
                "title": "\u03f5-Approximation of Adaptive Leaning Rate Optimization Algorithms for Constrained Nonconvex Stochastic Optimization",
                "abstract": "This brief considers constrained nonconvex stochastic finite-sum and online optimization in deep neural networks. Adaptive-learning-rate optimization algorithms (ALROAs), such as Adam, AMSGrad, and their variants, have widely been used for these optimizations because they are powerful and useful in theory and practice. Here, it is shown that the ALROAs are <inline-formula> <tex-math notation=\"LaTeX\">$\\epsilon $ </tex-math></inline-formula>-approximations for these optimizations. We provide the learning rates, mini-batch sizes, number of iterations, and stochastic gradient complexity with which to achieve <inline-formula> <tex-math notation=\"LaTeX\">$\\epsilon $ </tex-math></inline-formula>-approximations of the algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2018304",
                        "name": "H. Iiduka"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "00a96a7de8b3857245299975e7bdcc734399be6e",
                "externalIds": {
                    "DBLP": "journals/nca/YuSL22",
                    "DOI": "10.1007/s00521-021-06765-2",
                    "CorpusId": 246375632
                },
                "corpusId": 246375632,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/00a96a7de8b3857245299975e7bdcc734399be6e",
                "title": "A fractional-order momentum optimization approach of deep neural networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50077551",
                        "name": "Zhongliang Yu"
                    },
                    {
                        "authorId": "2148441625",
                        "name": "G. Sun"
                    },
                    {
                        "authorId": "2054671505",
                        "name": "Jianfeng Lv"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ten use gradients to update the model parameters [15], [21]\u2013[25]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "302aeea77b62a79229d06d51057b06693f371f6c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-09193",
                    "ArXiv": "2201.09193",
                    "DOI": "10.1109/TMM.2022.3158066",
                    "CorpusId": 246240619
                },
                "corpusId": 246240619,
                "publicationVenue": {
                    "id": "10e76a35-58d6-443c-9683-fc16f2dd0a92",
                    "name": "IEEE transactions on multimedia",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Transactions on Multimedia",
                        "IEEE Trans Multimedia",
                        "IEEE trans multimedia"
                    ],
                    "issn": "1520-9210",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046"
                },
                "url": "https://www.semanticscholar.org/paper/302aeea77b62a79229d06d51057b06693f371f6c",
                "title": "Learning to Minimize the Remainder in Supervised Learning",
                "abstract": "The learning process of deep learning methods usually updates the model\u2019s parameters in multiple iterations. Each iteration can be viewed as the first-order approximation of Taylor\u2019s series expansion. The remainder, which consists of higher-order terms, is usually ignored in the learning process for simplicity. This learning scheme empowers various multimedia-based applications, such as image retrieval, recommendation system, and video search. Generally, multimedia data (e.g. images) are semantics-rich and high-dimensional, hence the remainders of approximations are possibly non-zero. In this work, we consider that the remainder is informative and study how it affects the learning process. To this end, we propose a new learning approach, namely gradient adjustment learning (GAL), to leverage the knowledge learned from the past training iterations to adjust vanilla gradients, such that the remainders are minimized and the approximations are improved. The proposed GAL is model- and optimizer-agnostic, and is easy to adapt to the standard learning framework. It is evaluated on three tasks, i.e. image classification, object detection, and regression, with state-of-the-art models and optimizers. The experiments show that the proposed GAL consistently enhances the evaluated models, whereas the ablation studies validate various aspects of the proposed GAL. The code is available at https://github.com/luoyan407/gradient_adjustment.git.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112601696",
                        "name": "Yan Luo"
                    },
                    {
                        "authorId": "3026404",
                        "name": "Yongkang Wong"
                    },
                    {
                        "authorId": "145977143",
                        "name": "Mohan S. Kankanhalli"
                    },
                    {
                        "authorId": null,
                        "name": "Qi Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(7) and st = (gt \u2212 mt)(2) for AdaBelief [8]).",
                "Note the similarity between this regret bound and the one derived by [21] and by [8] using AMSGrad.",
                "Furthermore, although many SGD-based optimization algorithms are available, in this study, we focus on comparing AdaTerm against the two related works (t-momentumbased Adam - t-Adam [18] - and At-momentum-based Adam At-Adam [19] -)3 and include only the results from Adam [4], AdaBelief [8], and RAdam [23] as references for non-robust optimization.",
                "Among these, RAdam [7] and AdaBelief [8] have illustrated the state-of-the-art (SOTA) learning performance, to the best of our knowledge."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "2566a378ca8a004cc63d5c19d87a49302e3e2fa9",
                "externalIds": {
                    "DBLP": "journals/ijon/IlboudoKM23",
                    "ArXiv": "2201.06714",
                    "DOI": "10.1016/j.neucom.2023.126692",
                    "CorpusId": 261076206
                },
                "corpusId": 261076206,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2566a378ca8a004cc63d5c19d87a49302e3e2fa9",
                "title": "AdaTerm: Adaptive T-distribution estimated robust moments for Noise-Robust stochastic gradient optimization",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1515577860",
                        "name": "Wendyam Eric Lionel Ilboudo"
                    },
                    {
                        "authorId": "5216888",
                        "name": "Taisuke Kobayashi"
                    },
                    {
                        "authorId": "3248224",
                        "name": "Takamitsu Matsubara"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The fast optimizer [4] and sparse training [5] have reduced the training algorithm complexity."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "723511b2ec784bf892018c713ad568de17ebd912",
                "externalIds": {
                    "DBLP": "conf/aspdac/YangLSYYL22",
                    "DOI": "10.1109/ASP-DAC52403.2022.9712505",
                    "CorpusId": 247050215
                },
                "corpusId": 247050215,
                "publicationVenue": {
                    "id": "9b12daea-e3b6-4d55-92a5-6803ca0d3e3d",
                    "name": "Asia and South Pacific Design Automation Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Asia s pac Des Autom Conf",
                        "ASP-DAC"
                    ],
                    "url": "http://www.aspdac.com/"
                },
                "url": "https://www.semanticscholar.org/paper/723511b2ec784bf892018c713ad568de17ebd912",
                "title": "Toward Low-Bit Neural Network Training Accelerator by Dynamic Group Accumulation",
                "abstract": "Low-bit quantization is a big challenge for neural network training. Conventional training hardware adopts FP32 to accumulate the partial-sum result, which seriously degrades energy efficiency. In this paper, a technology called dynamic group accumulation (DGA) is proposed to reduce the accumulation error. First, we model the proposed group accumulation method and give the optimal DGA algorithm. Second, we design a training architecture and implement a hardware-efficient DGA unit. Third, we make a comprehensive analysis of the DGA algorithm and training architecture. The proposed method is evaluated on CIFAR and ImageNet datasets, and results show that DGA can reduce accumulation bit-width by 6 bits while achieving the same precision as the static group method. With the FP12 DGA, the CNN algorithm only loses 0.11% accuracy in ImageNet training, and our architecture saves 32% of power consumption compared to the FP32 baseline.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108585951",
                        "name": "Yixiong Yang"
                    },
                    {
                        "authorId": "48757644",
                        "name": "Ruoyang Liu"
                    },
                    {
                        "authorId": "1819427054",
                        "name": "Wenyu Sun"
                    },
                    {
                        "authorId": "3405772",
                        "name": "Jinshan Yue"
                    },
                    {
                        "authorId": "39150998",
                        "name": "Huazhong Yang"
                    },
                    {
                        "authorId": "2442306",
                        "name": "Yongpan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Once the architectures are fixed, we adjust the following hyperparameters: (i) Optimizer algorithm: Adam [40] or AdaBelief [83]; (ii) Skip-Connection: An additional connection in the Critic network inspired by ResNet [31] architectures; (iii) Activation function at the Generator output: Linear or tanh; (iv) TTUR [32]; (v) Dropout [67].",
                "Once the architectures are fixed, we adjust the following hyperparameters: (i) Optimizer algorithm: Adam [5] or AdaBelief [33]; (ii) Skip-Connection: An additional connection in the Critic network inspired by the ResNet architecture [40] architectures; (iii) Activation function at the Generator output: Linear or tanh; (iv) TTUR [41]; (v) Dropout [30]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3e58cbbb2d4c0fc4b5f41dfb757f9b15f05e6441",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-06147",
                    "ArXiv": "2201.06147",
                    "DOI": "10.1007/s10489-022-03557-6",
                    "CorpusId": 246016242
                },
                "corpusId": 246016242,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3e58cbbb2d4c0fc4b5f41dfb757f9b15f05e6441",
                "title": "Data augmentation through multivariate scenario forecasting in Data Centers using Generative Adversarial Networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2107203171",
                        "name": "J. P'erez"
                    },
                    {
                        "authorId": "3135281",
                        "name": "Patricia Arroba"
                    },
                    {
                        "authorId": "1794140",
                        "name": "Jose M. Moya"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides, the newly proposed AdaBelief optimization [21] technique has Fig."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6c3df62f5ed8a3f8254e0510145357ec49d172b7",
                "externalIds": {
                    "DBLP": "journals/mta/ChatterjeeCI22",
                    "PubMedCentral": "8734137",
                    "DOI": "10.1007/s11042-021-11811-1",
                    "CorpusId": 245812179,
                    "PubMed": "35018130"
                },
                "corpusId": 245812179,
                "publicationVenue": {
                    "id": "477368e9-7a8e-475a-8c93-6d623797fd06",
                    "name": "Multimedia tools and applications",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Tools and Applications",
                        "Multimedia Tool Appl",
                        "Multimedia tool appl"
                    ],
                    "issn": "1380-7501",
                    "url": "https://www.springer.com/computer/information+systems+and+applications/journal/11042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6c3df62f5ed8a3f8254e0510145357ec49d172b7",
                "title": "Deep learning techniques for observing the impact of the global warming from satellite images of water-bodies",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055301468",
                        "name": "Rajdeep Chatterjee"
                    },
                    {
                        "authorId": "1992797948",
                        "name": "Ankita Chatterjee"
                    },
                    {
                        "authorId": "2150050989",
                        "name": "SK Hafizul Islam"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bd6a7b9b53bf78add1b78eb2e0a0e8d2eb3793c1",
                "externalIds": {
                    "DOI": "10.1117/12.2626430",
                    "CorpusId": 246557684
                },
                "corpusId": 246557684,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bd6a7b9b53bf78add1b78eb2e0a0e8d2eb3793c1",
                "title": "A comparative study of recently deep learning optimizers",
                "abstract": "Deep learning has achieved great success in computer vision, natural language processing, recommendation systems and other fields. However, the models of deep neural network (DNN) are very complex, which often contain millions of parameters and tens or even hundreds of layers. Optimizing weights of DNNs is easy to fall into local optima, and hard to achieve better performance. Thus, how to choose an effective optimizer which is able to obtain network with higher precision and stronger generalization ability is of great significance. In this article, we make a review of some popular historical and state-of-the-art optimizers, and conclude them into three main streams: first order optimizers that accelerate convergence speed of stochastic gradient descent or/and adaptively adjust learning rates; second order optimizers that can make use of second-order information of loss landscape which helps escape from local optima; proxy optimizers that are able to deal with non-differentiable loss functions through combining with the proxy algorithm. We also summarize the first and second order moment used in different optimizers. Moreover, we provide an insightful comparison on some optimizers through image classification. The results show that first order optimizers like AdaMod and Ranger not only have low computational cost, but also show great convergence speed. Meanwhile, the optimizers that can introduce curvature information such as Adabelief and Apollo, have a better generalization especially when optimizing complex network.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1679704",
                        "name": "Y. Liu"
                    },
                    {
                        "authorId": "14278028",
                        "name": "Maojun Zhang"
                    },
                    {
                        "authorId": "2069512564",
                        "name": "Zhiwei Zhong"
                    },
                    {
                        "authorId": "2441459",
                        "name": "Xiangrong Zeng"
                    },
                    {
                        "authorId": "145223341",
                        "name": "Xin Long"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[34] to enhance the learning of wider representations with fewer parameters.",
                "In this study, TMwas based on the work by [34], with the attention mechanism described in [30]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "6700754da6c4cb573e0102434ebf0d8e172ee980",
                "externalIds": {
                    "DBLP": "journals/cin/AlkhammashAA21",
                    "PubMedCentral": "8684576",
                    "DOI": "10.1155/2021/6089677",
                    "CorpusId": 245333909,
                    "PubMed": "34934420"
                },
                "corpusId": 245333909,
                "publicationVenue": {
                    "id": "f32b7322-b69c-4e63-801d-8f50784ef778",
                    "name": "Computational Intelligence and Neuroscience",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Intell Neurosci"
                    ],
                    "issn": "1687-5265",
                    "url": "https://www.hindawi.com/journals/cin/"
                },
                "url": "https://www.semanticscholar.org/paper/6700754da6c4cb573e0102434ebf0d8e172ee980",
                "title": "Novel Prediction Model for COVID-19 in Saudi Arabia Based on an LSTM Algorithm",
                "abstract": "The rapid emergence of the novel SARS-CoV-2 poses a challenge and has attracted worldwide attention. Artificial intelligence (AI) can be used to combat this pandemic and control the spread of the virus. In particular, deep learning-based time-series techniques are used to predict worldwide COVID-19 cases for short-term and medium-term dependencies using adaptive learning. This study aimed to predict daily COVID-19 cases and investigate the critical factors that increase the transmission rate of this outbreak by examining different influential factors. Furthermore, the study analyzed the effectiveness of COVID-19 prevention measures. A fully connected deep neural network, long short-term memory (LSTM), and transformer model were used as the AI models for the prediction of new COVID-19 cases. Initially, data preprocessing and feature extraction were performed using COVID-19 datasets from Saudi Arabia. The performance metrics for all models were computed, and the results were subjected to comparative analysis to detect the most reliable model. Additionally, statistical hypothesis analysis and correlation analysis were performed on the COVID-19 datasets by including features such as daily mobility, total cases, people fully vaccinated per hundred, weekly hospital admissions per million, intensive care unit patients, and new deaths per million. The results show that the LSTM algorithm had the highest accuracy of all the algorithms and an error of less than 2%. The findings of this study contribute to our understanding of COVID-19 containment. This study also provides insights into the prevention of future outbreaks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3066841",
                        "name": "Eman H. Alkhammash"
                    },
                    {
                        "authorId": "7399020",
                        "name": "H. Algethami"
                    },
                    {
                        "authorId": "51337869",
                        "name": "R. Alshahrani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mainly, substituting the Adam optimizer [12] by AdaBelief [27], using two different models conditioned on a rain rate threshold, or using an ensemble of models (being the latest the second major improvement)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "359d0c71751352b55537fa994037d826ac793ba3",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/HerruzoGLCRH0K21",
                    "DOI": "10.1109/BigData52589.2021.9672063",
                    "CorpusId": 245934574
                },
                "corpusId": 245934574,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/359d0c71751352b55537fa994037d826ac793ba3",
                "title": "High-resolution multi-channel weather forecasting \u2013 First insights on transfer learning from the Weather4cast Competitions 2021",
                "abstract": "Weather forecasting is both a high impact application as well as a complex Big Data modelling challenge. Recent advances in machine learning have already demonstrated the power of non-physical modelling approaches for the prediction of rainfall. The Weather4cast competitions now provide a unique multi-channel benchmark for the prediction of up to 8 hours of weather with high temporal and spatial resolutions (15 min, 4 km) for a diverse set of large regions across Earth. This diversity, for the first time, also permits a meaningful spatial transfer learning challenge in weather forecasting.Weather4cast introduces multi-channel weather \u2018movies\u2019 that encode temperature, rainfall, cloud properties, and turbulence as derived from the meteorological satellites by the EUMETSAT NWC SAF. Inspired by the Traffic4cast competitions at the NeurIPS conferences in 2019 and 2020, weather forecasting is thus presented as a video frame prediction task. As then, the U-Net based models developed for photographic image analysis intriguingly did well on these artificial videos. In contrast, however, the winning submission did not employ a U-Net but a recurrent convolutional network with residual units.Weather4cast introduces the first spatial transfer learning challenge in weather forecasting: only one-hour short snippets from spatial regions never seen before were provided as input to models. We can thus now present first insights from submissions to this spatial transfer learning challenge. Notably, models with better core prediction performance also generalized better. Moreover, the two top-ranked models \u2013 one RCN based, one U-Net based \u2013 were further ahead of the remaining top-ranked submissions for spatial transfer learning (+6%) than in the core prediction challenge (+1%).While submissions tested varying strategies for input data selection and training, there remains a wide range of additional complementary approaches to be explored in future analyses. The competition and its leaderboards remain available and open for new submissions on the weather4cast.ai website.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3433525",
                        "name": "Pedro Herruzo"
                    },
                    {
                        "authorId": "1790005",
                        "name": "A. Gruca"
                    },
                    {
                        "authorId": "2149929462",
                        "name": "Lloren\u00e7 Lliso"
                    },
                    {
                        "authorId": "49222247",
                        "name": "X. Calbet"
                    },
                    {
                        "authorId": "93039823",
                        "name": "P. R\u00edpodas"
                    },
                    {
                        "authorId": "3308557",
                        "name": "S. Hochreiter"
                    },
                    {
                        "authorId": "2058236577",
                        "name": "Michael Kopp"
                    },
                    {
                        "authorId": "29846943",
                        "name": "David P. Kreil"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We consider the following algorithm (Algorithm 1), which is a unified algorithm for most deep learning optimizers,1 including Momentum [18], AMSGrad [19], AMSBound [13], Adam [11], and AdaBelief [30], which are listed in Table 1.",
                "using \u03b1k = 1/ \u221a k has an O(logK/ \u221a K) convergence rate [30].",
                "Hk = diag(\u1e7dk,i) Adam pk = \u2207LBk(\u03b8k) \u2207LBk(\u03b8k) [11] vk = \u03b7vk\u22121 + (1\u2212 \u03b7)pk (vk,i \u2264 vk+1,i) v\u0304k = vk 1\u2212\u03b6k Hk = diag( \u221a v\u0304k,i) AdaBelief p\u0303k = \u2207LBk(\u03b8k)\u2212mk [30] s\u0303k = p\u0303k p\u0303k (sk,i \u2264 sk+1,i) sk = \u03b7vk\u22121 + (1\u2212 \u03b7)s\u0303k \u015dk = sk 1\u2212\u03b6k Hk = diag( \u221a \u015dk,i)",
                "The convergence of adaptive methods has been studied for nonconvex optimization [4, 7, 10, 30], and the convergence of stochastic gradient descent (SGD) methods has been studied for nonconvex optimization [3,8, 12,21].",
                "A method that unifies adaptive methods such as AMSGrad and AdaBelief has been shown to have a convergence rate of O(1/ \u221a K) when \u03b1k = 1/ \u221a k [10], which improves on the results of [4, 30].",
                "We consider the following algorithm (Algorithm 1), which is a unified algorithm for most deep learning optimizers,(1) including Momentum [18], AMSGrad [19], AMSBound [13], Adam [11], and AdaBelief [30], which are listed in Table 1.",
                "AdaBelief optimizer: Adapting stepsizes by the belief in observed gradients.",
                "AdaBelief (which adapts the step size in accordance with the belief in the observed gradients)\nKey words and phrases. adaptive method, batch size, nonconvex optimization, stochastic firstorder oracle complexity."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9f9d9fdd6be6ec9c9dba76c8262261e880b12b60",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-07163",
                    "ArXiv": "2112.07163",
                    "CorpusId": 245131213
                },
                "corpusId": 245131213,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9f9d9fdd6be6ec9c9dba76c8262261e880b12b60",
                "title": "Minimization of Stochastic First-order Oracle Complexity of Adaptive Methods for Nonconvex Optimization",
                "abstract": "Numerical evaluations have definitively shown that, for deep learning optimizers such as stochastic gradient descent, momentum, and adaptive methods, the number of steps needed to train a deep neural network halves for each doubling of the batch size and that there is a region of diminishing returns beyond the critical batch size. In this paper, we determine the actual critical batch size by using the global minimizer of the stochastic first-order oracle (SFO) complexity of the optimizer. To prove the existence of the actual critical batch size, we set the lower and upper bounds of the SFO complexity and prove that there exist critical batch sizes in the sense of minimizing the lower and upper bounds. This proof implies that, if the SFO complexity fits the lower and upper bounds, then the existence of these critical batch sizes demonstrates the existence of the actual critical batch size. We also discuss the conditions needed for the SFO complexity to fit the lower and upper bounds and provide numerical results that support our theoretical results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2018304",
                        "name": "H. Iiduka"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c8a8cbbaec3da905cac3769ae16adc4ffa2afc89",
                "externalIds": {
                    "ArXiv": "2112.02761",
                    "DBLP": "journals/corr/abs-2112-02761",
                    "CorpusId": 244909145
                },
                "corpusId": 244909145,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c8a8cbbaec3da905cac3769ae16adc4ffa2afc89",
                "title": "BCD Nets: Scalable Variational Approaches for Bayesian Causal Discovery",
                "abstract": "A structural equation model (SEM) is an effective framework to reason over causal relationships represented via a directed acyclic graph (DAG). Recent advances have enabled effective maximum-likelihood point estimation of DAGs from observational data. However, a point estimate may not accurately capture the uncertainty in inferring the underlying graph in practical scenarios, wherein the true DAG is non-identifiable and/or the observed dataset is limited. We propose Bayesian Causal Discovery Nets (BCD Nets), a variational inference framework for estimating a distribution over DAGs characterizing a linear-Gaussian SEM. Developing a full Bayesian posterior over DAGs is challenging due to the the discrete and combinatorial nature of graphs. We analyse key design choices for scalable VI over DAGs, such as 1) the parametrization of DAGs via an expressive variational family, 2) a continuous relaxation that enables low-variance stochastic optimization, and 3) suitable priors over the latent variables. We provide a series of experiments on real and synthetic data showing that BCD Nets outperform maximum-likelihood methods on standard causal discovery metrics such as structural Hamming distance in low data regimes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "24769718",
                        "name": "Chris Cundy"
                    },
                    {
                        "authorId": "1954250",
                        "name": "Aditya Grover"
                    },
                    {
                        "authorId": "2490652",
                        "name": "Stefano Ermon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The initialization of weights for each layer was done with HE normal initialization [27] and the training of the architecture was done with AdaBelief optimizer [28] with standard parameters."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0c7161da5b9f4f33ed9253c1dd0e52d9965ed02f",
                "externalIds": {
                    "DOI": "10.26434/chemrxiv-2021-18x0d",
                    "CorpusId": 244927299
                },
                "corpusId": 244927299,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0c7161da5b9f4f33ed9253c1dd0e52d9965ed02f",
                "title": "HyFactor: Hydrogen-count labelled graph-based defactorization Autoencoder",
                "abstract": "Graph-based architectures are becoming increasingly popular as a tool for structure generation. Here, we introduce a novel open-source architecture HyFactor which is inspired by previously reported DEFactor architecture and based on the hydrogen labeled graphs. Since the original DEFactor code was not available, its new implementation (ReFactor) was prepared in this work for the benchmarking purpose. HyFactor demonstrates its high performance on the ZINC 250K MOSES and ChEMBL data set and in molecular generation tasks, it is considerably more effective than ReFactor. The code of HyFactor and all models obtained in this study are publicly available from our GitHub repository: https://github.com/Laboratoire-de-Chemoinformatique/hyfactor",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "113443419",
                        "name": "T. Akhmetshin"
                    },
                    {
                        "authorId": "29410339",
                        "name": "A. Lin"
                    },
                    {
                        "authorId": "2143284706",
                        "name": "D. Mazitov"
                    },
                    {
                        "authorId": "2143294953",
                        "name": "Evgenii Ziaikin"
                    },
                    {
                        "authorId": "8443273",
                        "name": "T. Madzhidov"
                    },
                    {
                        "authorId": "1685042",
                        "name": "A. Varnek"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, if importing the AdaBelief (Zhuang et al., 2020) code fails, the module will not be registered and therefore not be available in the graphical user interface."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e41ebdbd1a55bbda63082a2beed1039ab0fa46dd",
                "externalIds": {
                    "ArXiv": "2112.01796",
                    "DBLP": "journals/corr/abs-2112-01796",
                    "CorpusId": 244896499
                },
                "corpusId": 244896499,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e41ebdbd1a55bbda63082a2beed1039ab0fa46dd",
                "title": "The UniNAS framework: combining modules in arbitrarily complex configurations with argument trees",
                "abstract": "Designing code to be simplistic yet to offer choice is a tightrope walk. Additional modules such as optimizers and data sets make a framework useful to a broader audience, but the added complexity quickly becomes a problem. Framework parameters may apply only to some modules but not others, be mutually exclusive or depend on each other, often in unclear ways. Even so, many frameworks are limited to a few specific use cases. This paper presents the underlying concept of UniNAS, a framework designed to incorporate a variety of Neural Architecture Search approaches. Since they differ in the number of optimizers and networks, hyper-parameter optimization, network designs, candidate operations, and more, a traditional approach can not solve the task. Instead, every module defines its own hyper-parameters and a local tree structure of module requirements. A configuration file specifies which modules are used, their used parameters, and which other modules they use in turn This concept of argument trees enables combining and reusing modules in complex configurations while avoiding many problems mentioned above. Argument trees can also be configured from a graphical user interface so that designing and changing experiments becomes possible without writing a single line of code. UniNAS is publicly available at https://github.com/cogsys-tuebingen/uninas",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "52220956",
                        "name": "K. A. Laube"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also experiment with using more recent optimizers [66] to construct the attacks (results are provided in the supplementals)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "124ec9d97a7ea076f3ae8e53389734a0ab918bb6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-15121",
                    "ArXiv": "2111.15121",
                    "DOI": "10.1109/CVPR52688.2022.01306",
                    "CorpusId": 244729495
                },
                "corpusId": 244729495,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/124ec9d97a7ea076f3ae8e53389734a0ab918bb6",
                "title": "Pyramid Adversarial Training Improves ViT Performance",
                "abstract": "Aggressive data augmentation is a key component of the strong generalization capabilities of Vision Transformer (ViT). One such data augmentation technique is adversarial training (AT); however, many prior works [28,45] have shown that this often results in poor clean accuracy. In this work, we present pyramid adversarial training (PyramidAT), a simple and effective technique to improve ViT's overall performance. We pair it with a \u201cmatched\u201d Dropout and stochastic depth regularization, which adopts the same Dropout and stochastic depth configuration for the clean and adversarial samples. Similar to the improvements on CNNs by AdvProp [61] (not directly applicable to ViT), our pyramid adversarial training breaks the trade-off between in-distribution accuracy and out-of-distribution robustness for ViT and related architectures. It leads to 1.82% absolute improvement on ImageNet clean accuracy for the ViT-B model when trained only on ImageNet-1K data, while simultaneously boosting performance on 7 ImageNet ro-bustness metrics, by absolute numbers ranging from 1.76% to 15.68%. We set a new state-of-the-art for ImageNet-C (41.42 mCE), ImageNet-R (53.92%), and ImageNet-Sketch (41.04%) without extra data, using only the ViT-B/16 backbone and our pyramid adversarial training. Our code is publicly available at pyramidat.github.io.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144382278",
                        "name": "Charles Herrmann"
                    },
                    {
                        "authorId": "2121366367",
                        "name": "Kyle Sargent"
                    },
                    {
                        "authorId": "39978626",
                        "name": "Lu Jiang"
                    },
                    {
                        "authorId": "2984143",
                        "name": "R. Zabih"
                    },
                    {
                        "authorId": "2914394",
                        "name": "Huiwen Chang"
                    },
                    {
                        "authorId": "2107890439",
                        "name": "Ce Liu"
                    },
                    {
                        "authorId": "1707347",
                        "name": "Dilip Krishnan"
                    },
                    {
                        "authorId": "3232265",
                        "name": "Deqing Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "And the latter introduces the idea of various optimizers (e.g., momentum and NAG [45], Adam [28], AdaBelief [68]) into the basic iterative attack method [29] to improve the stability of the gradient and enhance the transferability of the generated adversarial examples.",
                "Besides, Yang et al. [66] absorb the AdaBelief optimizer into the update of the gradient and propose ABI-FGM to further boost the success rates of adversarial examples for black-box attacks.",
                ", momentum and NAG [45], Adam [28], AdaBelief [68]) into the basic iterative attack method [29] to improve the stability of the gradient and enhance the transferability of the generated adversarial examples."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4adee130b8da7677f151b4a6ba42a1a6312ac3df",
                "externalIds": {
                    "ArXiv": "2111.13844",
                    "DBLP": "journals/corr/abs-2111-13844",
                    "DOI": "10.1007/978-3-031-20065-6_1",
                    "CorpusId": 244714851
                },
                "corpusId": 244714851,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/4adee130b8da7677f151b4a6ba42a1a6312ac3df",
                "title": "Adaptive Image Transformations for Transfer-based Adversarial Attack",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2112341070",
                        "name": "Zheng Yuan"
                    },
                    {
                        "authorId": "40539618",
                        "name": "J. Zhang"
                    },
                    {
                        "authorId": "145455919",
                        "name": "S. Shan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Each model is trained for 10 epochs using the AdaBelief optimizer (Zhuang et al. 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c3d769578637c4eebda2ace167b51a0a3ff7698d",
                "externalIds": {
                    "ArXiv": "2111.07671",
                    "DBLP": "conf/ki/DulnyHK22",
                    "DOI": "10.1007/978-3-031-15791-2_8",
                    "CorpusId": 244117873
                },
                "corpusId": 244117873,
                "publicationVenue": {
                    "id": "bff947de-0729-467e-b9cd-57b193e706a7",
                    "name": "Deutsche Jahrestagung f\u00fcr K\u00fcnstliche Intelligenz",
                    "type": "conference",
                    "alternate_names": [
                        "KI",
                        "Dtsch Jahrestag K\u00fcnstl Intell"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c3d769578637c4eebda2ace167b51a0a3ff7698d",
                "title": "NeuralPDE: Modelling Dynamical Systems from Data",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2078578340",
                        "name": "Andrzej Dulny"
                    },
                    {
                        "authorId": "1792623",
                        "name": "A. Hotho"
                    },
                    {
                        "authorId": "2054845760",
                        "name": "Anna Krause"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In our baseline decoder comparison we compare Adam [16] and Adabelief [33].",
                "Preliminary experiments showed consistent, favourable results for Adabelief in all setups."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bfb220f40ea2f6fd091cb7994f22612d4b6ba10e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-08462",
                    "ArXiv": "2111.08462",
                    "CorpusId": 244130142
                },
                "corpusId": 244130142,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bfb220f40ea2f6fd091cb7994f22612d4b6ba10e",
                "title": "Towards Lightweight Controllable Audio Synthesis with Conditional Implicit Neural Representations",
                "abstract": "The high temporal resolution of audio and our perceptual sensitivity to small irregularities in waveforms make synthesizing at high sampling rates a complex and computationally intensive task, prohibiting real-time, controllable synthesis within many approaches. In this work we aim to shed light on the potential of Conditional Implicit Neural Representations (CINRs) as lightweight backbones in generative frameworks for audio synthesis. Our experiments show that small Periodic Conditional INRs (PCINRs) learn faster and generally produce quantitatively better audio reconstructions than Transposed Convolutional Neural Networks with equal parameter counts. However, their performance is very sensitive to activation scaling hyperparameters. When learning to represent more uniform sets, PCINRs tend to introduce artificial high-frequency components in reconstructions. We validate this noise can be minimized by applying standard weight regularization during training or decreasing the compositional depth of PCINRs, and suggest directions for future research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2140578249",
                        "name": "Jan Zuiderveld"
                    },
                    {
                        "authorId": "8300792",
                        "name": "M. Federici"
                    },
                    {
                        "authorId": "2231179",
                        "name": "E. Bekkers"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Abbreviations GAN: Generative adversarial network; HR-MPF: High-resolution network with multi-scale progressive fusion; PDM: Progressive decoding module; HRNet: High-resolution network; CT: Computed tomography; CAD: Computer-aided diagnosis; Acc: Accuracy; DSC: Dice similarity coefficient; MIoU: Mean intersection over union; Prec: Precision; SE: Sensitivity; SP: Specificity; TP: True positive; TN: True negative; FP: False positive; FN: False negative; AUC : Area under curve.",
                "999)) is used as optimization algorithm of segmentation network, which is of fast convergence and high accuracy, and performs high stability when training a GAN [37].",
                "Y. Sun, C. Zhou, Y. Fu, X. Xue, Parasitic GAN for semi-supervised brain tumor segmentation.",
                "In some other GAN-based segmentation methods, there is not only a segmentation network, but also a generator.",
                "For example, Conditional Generative Adversarial Network (CGAN) was adopted by Qin et\u00a0al.",
                "In our method, the loss function adopts that in WGAN, which is defined as follows:\n(5)Lcls = \u2212 n \u2211\nk=1\npk log p\u0302k ,\n(6)Ltotal = LS + 3Lcls = Lseg + 1Ladv + 2Lb + 3Lcls.\n(7)LD = \u2212E[D(y)] + E[D(y\u0302)],\nwhere D(y) means the output of the discriminator for input ground truth.",
                "Inspired by GAN, the proposed framework takes the segmentation network as generator and adopts a discriminator at the end for adversarial training.",
                "WGAN [35] improves the traditional GAN loss function and solves the problem of unstable training and collapse mode.",
                "H. Shi, J. Lu, Q. Zhou, A novel data augmentation method using style-based GAN for robust pulmonary nodule segmentation.",
                "The discriminator in Spine-GAN [20] outputted 0 or 1 representing whether the input was ground truth or prediction result.",
                "In order to generate more diverse CT images for training, Generative Adversarial Network (GAN) [14] is widely used as a data augmentation method.",
                "GAN can not only be used to synthesize images and augment datasets, but also effectively improve the quality of segmentation results.",
                "Therefore, the design of a reasonable loss function of discriminator is also crucial to the training process of GAN.",
                "[17] introduced a style-based GAN to synthesize the pulmonary nodules with different styles, and the experiments proved that using augmented samples can obtain more accurate and robust segmentation results.",
                "Y. Onishi, A. Teramoto, M. Tsujimoto, T. Tsukamoto, K. Saito, H. Toyama, K. Imaizumi, H. Fujita, Investigation of pulmonary nodule classification using multi-scale residual network enhanced with 3DGAN-synthesized volumes.",
                "For instance, in Parasitic GAN [21], the segmentation network generated pixel-wise segmentation predictions, the generator synthesized supplementary label maps based on the input random noise, so that the discriminator could learn the more accurate boundaries of ground truth.",
                "AdaBelief (learning rate=2.5\u00d7 10\u22124 , eps=10\u22126 , Betas=(0.5, 0.999)) is used as optimization algorithm of segmentation network, which is of fast convergence and high accuracy, and performs high stability when training a GAN [37].",
                "3) Loss function of discriminator Although the training of GAN is the process of confrontation between generator and discriminator, the segmentation module outputting segmentation results could be regarded as a generator.",
                "Z. Han, B. Wei, A. Mercado, S. Leung, S. Li, Spine-GAN: Semantic segmentation of multiple spinal structures."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "01ae5fd476cbd7a81ab0b79a0bee3a73e827f9b3",
                "externalIds": {
                    "DBLP": "journals/ejivp/ZhuZYWY21",
                    "DOI": "10.1186/s13640-021-00574-2",
                    "CorpusId": 244078125
                },
                "corpusId": 244078125,
                "publicationVenue": {
                    "id": "430b5d98-2f44-470f-94e9-f73baf19a7e1",
                    "name": "EURASIP Journal on Image and Video Processing",
                    "type": "journal",
                    "alternate_names": [
                        "Eurasip Journal on Image and Video Processing",
                        "Eurasip J Image Video Process",
                        "EURASIP J Image Video Process"
                    ],
                    "issn": "1687-5176",
                    "url": "http://jivp.eurasipjournals.com/",
                    "alternate_urls": [
                        "https://jivp-eurasipjournals.springeropen.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/01ae5fd476cbd7a81ab0b79a0bee3a73e827f9b3",
                "title": "HR-MPF: high-resolution representation network with multi-scale progressive fusion for pulmonary nodule segmentation and classification",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2153461816",
                        "name": "Ling Zhu"
                    },
                    {
                        "authorId": "35008646",
                        "name": "Hongqing Zhu"
                    },
                    {
                        "authorId": "2051623953",
                        "name": "Suyi Yang"
                    },
                    {
                        "authorId": "2108815641",
                        "name": "Pengyu Wang"
                    },
                    {
                        "authorId": "2152848159",
                        "name": "Yang Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While the original worked used the widely adopted Adam optimizer, in this work AdaBelief [10], a modification of the algorithm used in Adam [11], was adopted.",
                "The results obtained in this study support the claim by the authors of AdaBelief that results obtained with it generalize better to datasets outside the training data.",
                "According to its authors, AdaBelief improves the generalization ability of the solutions it finds compared to those found by Adam.",
                "The runs with variables other than temperature were performed with AdaBelief using these hyperparameters, as the time remaining in the contest did not allow for parameter tuning for each variable separately.",
                "After some experimentation with the parameters, AdaBelief with the default hyperparameters was adopted, with the \u03f5 parameter set to 10\u221214 (the default of the TensorFlow implementation), as non-default parameters either degraded the results or failed to provide a significant benefit.",
                "3) The AdaBelief optimizer seems to somewhat outperform the popular Adam optimizer at this task, giving better results with the validation dataset.",
                "Between the retraining and the new optimizer, the addition of new training data seems to be the more important factor, as the S5 model trained with Adam was only slightly worse than the other models trained with AdaBelief.",
                "The different versions of the shallow model for temperature are as follows: S1 was trained from random initialization using the default settings of the AdaBelief optimizer, S2 was trained by initializing the weights with those of S1 and resetting the learning rate to 10\u22123 in the beginning of training, S3 was trained like S2 but setting the \u03f5 parameter to 10\u22127 (another value suggested by the authors), S4 was trained\nlike S1 but enabling weight decay in AdaBelief, and S5 was trained like S1 but using the Adam optimizer instead.",
                "First, it can be seen that simply retraining the model with the new data and the AdaBelief optimizer improved the results considerably (up to 4.8% for cma) over the models that produces the best results in Stage 1, except for crr intensity where the improvement was more marginal (0.2%).",
                "The hyperparameters of AdaBelief and comparisons to Adam were examined using the temperature variable."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "dfe5a80e949f140e5a02ed521f4fac6bcf87cae9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-06240",
                    "ArXiv": "2111.06240",
                    "DOI": "10.1109/BigData52589.2021.9671869",
                    "CorpusId": 243985873
                },
                "corpusId": 243985873,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dfe5a80e949f140e5a02ed521f4fac6bcf87cae9",
                "title": "Improvements to short-term weather prediction with recurrent-convolutional networks",
                "abstract": "The Weather4cast 2021 competition gave the participants a task of predicting the time evolution of two-dimensional fields of satellite-based meteorological data. This paper describes the author\u2019s efforts, after initial success in the first stage of the competition, to improve the model further in the second stage. The improvements consisted of a shallower model variant that is competitive against the deeper version, adoption of the AdaBelief optimizer, improved handling of one of the predicted variables where the training set was found not to represent the validation set well, and ensembling multiple models to improve the results further. The largest quantitative improvements to the competition metrics can be attributed to the increased amount of training data available in the second stage of the competition, followed by the effects of model ensembling. Qualitative results show that the model can predict the time evolution of the fields, including the motion of the fields over time, starting with sharp predictions for the immediate future and blurring of the outputs in later frames to account for the increased uncertainty.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50451945",
                        "name": "J. Leinonen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The Adabelief optimizer [37] is employed with a mini batch size of 32 using 10(5) iterations, b1 1\u20444 0:9 and b2 1\u20444 0:999, where the initial learning rate 4 10 4 is halved every 25 000 iterations."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1aaf3006138691b0e6ce436f44dae5f760155cfd",
                "externalIds": {
                    "DBLP": "journals/pami/KoblerEKP22",
                    "DOI": "10.1109/TPAMI.2021.3124086",
                    "CorpusId": 241111814,
                    "PubMed": "34727026"
                },
                "corpusId": 241111814,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1aaf3006138691b0e6ce436f44dae5f760155cfd",
                "title": "Total Deep Variation: A Stable Regularization Method for Inverse Problems",
                "abstract": "Various problems in computer vision and medical imaging can be cast as inverse problems. A frequent method for solving inverse problems is the variational approach, which amounts to minimizing an energy composed of a data fidelity term and a regularizer. Classically, handcrafted regularizers are used, which are commonly outperformed by state-of-the-art deep learning approaches. In this work, we combine the variational formulation of inverse problems with deep learning by introducing the data-driven general-purpose total deep variation regularizer. In its core, a convolutional neural network extracts local features on multiple scales and in successive blocks. This combination allows for a rigorous mathematical analysis including an optimal control formulation of the training problem in a mean-field setting and a stability analysis with respect to the initial values and the parameters of the regularizer. In addition, we experimentally verify the robustness against adversarial attacks and numerically derive upper bounds for the generalization error. Finally, we achieve state-of-the-art results for several imaging tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50289209",
                        "name": "Erich Kobler"
                    },
                    {
                        "authorId": "2908265",
                        "name": "Alexander Effland"
                    },
                    {
                        "authorId": "1751053",
                        "name": "K. Kunisch"
                    },
                    {
                        "authorId": "1730097",
                        "name": "T. Pock"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3a00bab02bd541ffdf40c40ac8640d855008e898",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-13041",
                    "PubMedCentral": "9041419",
                    "ArXiv": "2110.13041",
                    "DOI": "10.3389/fdata.2022.787421",
                    "CorpusId": 239768689,
                    "PubMed": "35496379"
                },
                "corpusId": 239768689,
                "publicationVenue": {
                    "id": "165fa1b5-e07f-4b6e-9203-04493f6a7c5c",
                    "name": "Frontiers in Big Data",
                    "alternate_names": [
                        "Front Big Data"
                    ],
                    "issn": "2624-909X",
                    "url": "https://www.frontiersin.org/journals/big-data"
                },
                "url": "https://www.semanticscholar.org/paper/3a00bab02bd541ffdf40c40ac8640d855008e898",
                "title": "Applications and Techniques for Fast Machine Learning in Science",
                "abstract": "In this community review report, we discuss applications and techniques for fast machine learning (ML) in science\u2014the concept of integrating powerful ML methods into the real-time experimental data processing loop to accelerate scientific discovery. The material for the report builds on two workshops held by the Fast ML for Science community and covers three main areas: applications for fast ML across a number of scientific domains; techniques for training and implementing performant and resource-efficient ML algorithms; and computing architectures, platforms, and technologies for deploying these algorithms. We also present overlapping challenges across the multiple scientific domains where common solutions can be found. This community report is intended to give plenty of examples and inspiration for scientific discovery through integrated and accelerated ML solutions. This is followed by a high-level overview and organization of technical advances, including an abundance of pointers to source material, which can enable these breakthroughs.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "103553716",
                        "name": "A. Deiana"
                    },
                    {
                        "authorId": "1389912827",
                        "name": "Nhan Tran"
                    },
                    {
                        "authorId": "31074012",
                        "name": "J. Agar"
                    },
                    {
                        "authorId": "1791399",
                        "name": "Michaela Blott"
                    },
                    {
                        "authorId": "20609731",
                        "name": "G. D. Guglielmo"
                    },
                    {
                        "authorId": "1389378372",
                        "name": "Javier Mauricio Duarte"
                    },
                    {
                        "authorId": "144326960",
                        "name": "P. Harris"
                    },
                    {
                        "authorId": "1694228",
                        "name": "S. Hauck"
                    },
                    {
                        "authorId": "47842372",
                        "name": "Miaoyuan Liu"
                    },
                    {
                        "authorId": "48339925",
                        "name": "M. Neubauer"
                    },
                    {
                        "authorId": "41016473",
                        "name": "J. Ngadiuba"
                    },
                    {
                        "authorId": "7727946",
                        "name": "S. Memik"
                    },
                    {
                        "authorId": "1858059",
                        "name": "M. Pierini"
                    },
                    {
                        "authorId": "1389781932",
                        "name": "T. Aarrestad"
                    },
                    {
                        "authorId": "51479292",
                        "name": "S. B\u00e4hr"
                    },
                    {
                        "authorId": "2103420010",
                        "name": "J\u00fcrgen Becker"
                    },
                    {
                        "authorId": "1659023551",
                        "name": "A. Berthold"
                    },
                    {
                        "authorId": "101290570",
                        "name": "R. Bonventre"
                    },
                    {
                        "authorId": "2134508103",
                        "name": "Tomas E. Muller Bravo"
                    },
                    {
                        "authorId": "15019078",
                        "name": "M. Diefenthaler"
                    },
                    {
                        "authorId": "143879884",
                        "name": "Zhen Dong"
                    },
                    {
                        "authorId": "2130422213",
                        "name": "Nick Fritzsche"
                    },
                    {
                        "authorId": "10419477",
                        "name": "A. Gholami"
                    },
                    {
                        "authorId": "47202566",
                        "name": "E. Govorkova"
                    },
                    {
                        "authorId": "5470608",
                        "name": "K. Hazelwood"
                    },
                    {
                        "authorId": "150057758",
                        "name": "C. Herwig"
                    },
                    {
                        "authorId": "2160823883",
                        "name": "B. Khan"
                    },
                    {
                        "authorId": "2109586102",
                        "name": "Sehoon Kim"
                    },
                    {
                        "authorId": "94932026",
                        "name": "T. Klijnsma"
                    },
                    {
                        "authorId": "2144470693",
                        "name": "Yaling Liu"
                    },
                    {
                        "authorId": "152314290",
                        "name": "K. Lo"
                    },
                    {
                        "authorId": "2116108017",
                        "name": "Tri Nguyen"
                    },
                    {
                        "authorId": "3160140",
                        "name": "G. Pezzullo"
                    },
                    {
                        "authorId": "148122307",
                        "name": "Seyedramin Rasoulinezhad"
                    },
                    {
                        "authorId": "2149997015",
                        "name": "R. Rivera"
                    },
                    {
                        "authorId": "3211968",
                        "name": "K. Scholberg"
                    },
                    {
                        "authorId": "2134444308",
                        "name": "Justin Selig"
                    },
                    {
                        "authorId": "39968063",
                        "name": "Sougata Sen"
                    },
                    {
                        "authorId": "144455360",
                        "name": "D. Strukov"
                    },
                    {
                        "authorId": "2110621591",
                        "name": "William Tang"
                    },
                    {
                        "authorId": "51050561",
                        "name": "S. Thais"
                    },
                    {
                        "authorId": "95185112",
                        "name": "K. Unger"
                    },
                    {
                        "authorId": "34706692",
                        "name": "R. Vilalta"
                    },
                    {
                        "authorId": "103367160",
                        "name": "B. Krosigk"
                    },
                    {
                        "authorId": "1780852",
                        "name": "T. Warburton"
                    },
                    {
                        "authorId": "1825744630",
                        "name": "M. A. Flechas"
                    },
                    {
                        "authorId": "117671580",
                        "name": "Anthony Aportela"
                    },
                    {
                        "authorId": "1387705404",
                        "name": "T. Calvet"
                    },
                    {
                        "authorId": "152407598",
                        "name": "L. Cristella"
                    },
                    {
                        "authorId": "2135002560",
                        "name": "Daniel Diaz"
                    },
                    {
                        "authorId": "30556043",
                        "name": "C. Doglioni"
                    },
                    {
                        "authorId": "2075031487",
                        "name": "M. Galati"
                    },
                    {
                        "authorId": "104208482",
                        "name": "E. Khoda"
                    },
                    {
                        "authorId": "145334779",
                        "name": "F. Fahim"
                    },
                    {
                        "authorId": "30794420",
                        "name": "Davide Giri"
                    },
                    {
                        "authorId": "50140234",
                        "name": "B. Hawks"
                    },
                    {
                        "authorId": "143980870",
                        "name": "Duc Hoang"
                    },
                    {
                        "authorId": "3039388",
                        "name": "B. Holzman"
                    },
                    {
                        "authorId": "2072776572",
                        "name": "Shih-Chieh Hsu"
                    },
                    {
                        "authorId": "91226838",
                        "name": "S. Jindariani"
                    },
                    {
                        "authorId": "145278823",
                        "name": "I. Johnson"
                    },
                    {
                        "authorId": "2029887059",
                        "name": "R. Kansal"
                    },
                    {
                        "authorId": "1749986",
                        "name": "R. Kastner"
                    },
                    {
                        "authorId": "145234605",
                        "name": "E. Katsavounidis"
                    },
                    {
                        "authorId": "1750878974",
                        "name": "J. Krupa"
                    },
                    {
                        "authorId": "2112519768",
                        "name": "Pan Li"
                    },
                    {
                        "authorId": "2144881368",
                        "name": "S. Madireddy"
                    },
                    {
                        "authorId": "2135040335",
                        "name": "Ethan Marx"
                    },
                    {
                        "authorId": "47848277",
                        "name": "Patric McCormack"
                    },
                    {
                        "authorId": "2068077835",
                        "name": "Andres Meza"
                    },
                    {
                        "authorId": "1390569426",
                        "name": "J. Mitrevski"
                    },
                    {
                        "authorId": "104382931",
                        "name": "M. A. Mohammed"
                    },
                    {
                        "authorId": "2083730746",
                        "name": "F. Mokhtar"
                    },
                    {
                        "authorId": "152205553",
                        "name": "Eric A. Moreno"
                    },
                    {
                        "authorId": "102468326",
                        "name": "S. Nagu"
                    },
                    {
                        "authorId": "145771572",
                        "name": "R. Narayan"
                    },
                    {
                        "authorId": "2135387217",
                        "name": "N. Palladino"
                    },
                    {
                        "authorId": "144514893",
                        "name": "Zhiqiang Que"
                    },
                    {
                        "authorId": "2149239930",
                        "name": "Sang Eon Park"
                    },
                    {
                        "authorId": "144826759",
                        "name": "S. Ramamoorthy"
                    },
                    {
                        "authorId": "145881831",
                        "name": "D. Rankin"
                    },
                    {
                        "authorId": "2135388110",
                        "name": "Simon Rothman"
                    },
                    {
                        "authorId": "2115605125",
                        "name": "Ashish Sharma"
                    },
                    {
                        "authorId": "49143070",
                        "name": "S. Summers"
                    },
                    {
                        "authorId": "51433013",
                        "name": "Pietro Vischia"
                    },
                    {
                        "authorId": "52630992",
                        "name": "J. Vlimant"
                    },
                    {
                        "authorId": "2047582841",
                        "name": "Olivia Weng"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "517a73973fd3a971168dcce18f563dfde1b810ab",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-09714",
                    "ArXiv": "2110.09714",
                    "DOI": "10.1145/3460120.3485383",
                    "CorpusId": 239024295
                },
                "corpusId": 239024295,
                "publicationVenue": {
                    "id": "73f7fe95-b68b-468f-b7ba-3013ca879e50",
                    "name": "Conference on Computer and Communications Security",
                    "type": "conference",
                    "alternate_names": [
                        "Int Workshop Cogn Cell Syst",
                        "CCS",
                        "Comput Commun Secur",
                        "CcS",
                        "International Symposium on Community-centric Systems",
                        "International Workshop on Cognitive Cellular Systems",
                        "Conf Comput Commun Secur",
                        "Comb Comput Sci",
                        "Int Symp Community-centric Syst",
                        "Combinatorics and Computer Science",
                        "Circuits, Signals, and Systems",
                        "Computer and Communications Security",
                        "Circuit Signal Syst"
                    ],
                    "url": "https://dl.acm.org/conference/ccs"
                },
                "url": "https://www.semanticscholar.org/paper/517a73973fd3a971168dcce18f563dfde1b810ab",
                "title": "Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information",
                "abstract": "Adversarial attacks against commercial black-box speech platforms, including cloud speech APIs and voice control devices, have received little attention until recent years. Constructing such attacks is difficult mainly due to the unique characteristics of time-domain speech signals and the much more complex architecture of acoustic systems. The current \"black-box\" attacks all heavily rely on the knowledge of prediction/confidence scores or other probability information to craft effective adversarial examples (AEs), which can be intuitively defended by service providers without returning these messages. In this paper, we take one more step forward and propose two novel adversarial attacks in more practical and rigorous scenarios. For commercial cloud speech APIs, we propose Occam, a decision-only black-box adversarial attack, where only final decisions are available to the adversary. In Occam, we formulate the decision-only AE generation as a discontinuous large-scale global optimization problem, and solve it by adaptively decomposing this complicated problem into a set of sub-problems and cooperatively optimizing each one. Our Occam is a one-size-fits-all approach, which achieves 100% success rates of attacks (SRoA) with an average SNR of 14.23dB, on a wide range of popular speech and speaker recognition APIs, including Google, Alibaba, Microsoft, Tencent, iFlytek, and Jingdong, outperforming the state-of-the-art black-box attacks. For commercial voice control devices, we propose NI-Occam, the first non-interactive physical adversarial attack, where the adversary does not need to query the oracle and has no access to its internal information and training data. We, for the first time, combine adversarial attacks with model inversion attacks, and thus generate the physically-effective audio AEs with high transferability without any interaction with target devices. Our experimental results show that NI-Occam can successfully fool Apple Siri, Microsoft Cortana, Google Assistant, iFlytek and Amazon Echo with an average SRoA of 52% and SNR of 9.65dB, shedding light on non-interactive physical attacks against voice control devices.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "119580732",
                        "name": "Baolin Zheng"
                    },
                    {
                        "authorId": "12699836",
                        "name": "Peipei Jiang"
                    },
                    {
                        "authorId": null,
                        "name": "Qian Wang"
                    },
                    {
                        "authorId": "2118912606",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": "2088079836",
                        "name": "Chao Shen"
                    },
                    {
                        "authorId": "2116638601",
                        "name": "Cong Wang"
                    },
                    {
                        "authorId": "1476800389",
                        "name": "Yunjie Ge"
                    },
                    {
                        "authorId": "2133359322",
                        "name": "Qingyang Teng"
                    },
                    {
                        "authorId": "2145523607",
                        "name": "Shenyi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We train our simulator using AdaBelief [53] optimizer for 200 epochs, using initial learning rate 4e-4 with warmup strategy at the beginning, gradually decayed to zero by cosine annealing [27]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fd541056bdda291086e6309bd64febb360d3d7d0",
                "externalIds": {
                    "DBLP": "conf/mm/GuLZT21",
                    "DOI": "10.1145/3474085.3475229",
                    "CorpusId": 239011515
                },
                "corpusId": 239011515,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fd541056bdda291086e6309bd64febb360d3d7d0",
                "title": "How to Learn a Domain-Adaptive Event Simulator?",
                "abstract": "The low-latency streams captured by event cameras have shown impressive potential in addressing vision tasks such as video reconstruction and optical flow estimation. However, these tasks often require massive training event streams, which are expensive to collect and largely bypassed by recently proposed event camera simulators. To align the statistics of synthetic events with that of target event cameras, existing simulators often need to be heuristically tuned with elaborative manual efforts and thus become incompetent to automatically adapt to various domains. To address this issue, this work proposes one of the first learning-based, domain-adaptive event simulator. Given a specific domain, the proposed simulator learns pixel-wise distributions of event contrast thresholds that, after stochastic sampling and paralleled rendering, can generate event representations well aligned with those from the data from realistic event cameras. To achieve such domain-specific alignment, we design a novel divide-and-conquer discrimination scheme that adaptively evaluates the synthetic-to-real consistency of event representations according to the local statistics of images and events. Trained with the data synthesized by the proposed simulator, the performances of state-of-the-art event-based video reconstruction and optical flow estimation approaches are boosted up to 22.9% and 2.8%, respectively. In addition, we show significantly improved domain adaptation capability over existing event simulators and tuning strategies, consistently on three real event datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50987252",
                        "name": "Daxin Gu"
                    },
                    {
                        "authorId": "2118372693",
                        "name": "Jia Li"
                    },
                    {
                        "authorId": "48380141",
                        "name": "Yu Zhang"
                    },
                    {
                        "authorId": "40161651",
                        "name": "Yonghong Tian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "8f326c9a8a73bc93592c2eef1bae55fd47c38f72",
                "externalIds": {
                    "DOI": "10.33774/chemrxiv-2021-vr43g",
                    "CorpusId": 239014225
                },
                "corpusId": 239014225,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8f326c9a8a73bc93592c2eef1bae55fd47c38f72",
                "title": "Self-Supervised Learning for Molecular Property Prediction",
                "abstract": "Predicting molecular properties remains a challenging task with numerous potential applications, notably in drug discovery. Recently, the development of deep learning, combined with rising amounts of data, has provided powerful tools to build predictive models. Since molecules can be encoded as graphs, Graph Neural Networks (GNNs) have emerged as a popular choice of architecture to tackle this task. Training GNNs to predict molecular properties however faces the challenge of collecting annotated data which is a costly and time consuming process. On the other hand, it is easy to access large databases of molecules without annotations. In this setting, self-supervised learning can efficiently leverage large amounts of non-annotated data to compensate for the lack of annotated ones. In this work, we introduce a self-supervised framework for GNNs tailored specifically for molecular property prediction. Our framework uses multiple pretext tasks focusing on different scales of molecules (atoms, fragments and entire molecules). We evaluate our method on a representative set of GNN architectures and datasets and also consider the impact of the choice of input features. Our results show that our framework can successfully improve performance compared to training from scratch, especially in low data regimes. The improvement varies depending on the dataset, model architecture and, importantly, on the choice of input feature representation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "83946298",
                        "name": "Laurent Dillard"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(3.2) is a suitable optimization function that can be solved by existing gradient-descent optimization algorithms such as Adam (Kingma and Ba 2015) and AdaBelief (Zhuang et al. 2020).",
                "2) is a suitable optimization function that can be solved by existing gradient-descent optimization algorithms such as Adam (Kingma and Ba 2015) and AdaBelief (Zhuang et al. 2020).",
                "The AdaBelief optimizer (Zhuang et al. 2020) is implemented to boost the performance of the models in the training phase."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "21e6dd66d5cb82c5cd19d86bdacaaa9bce336a07",
                "externalIds": {
                    "ArXiv": "2110.07305",
                    "DBLP": "journals/isci/WangLCRW22",
                    "DOI": "10.1016/j.ins.2022.07.157",
                    "CorpusId": 238857225
                },
                "corpusId": 238857225,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/21e6dd66d5cb82c5cd19d86bdacaaa9bce336a07",
                "title": "DI-AA: An Interpretable White-box Attack for Fooling Deep Neural Networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2136913597",
                        "name": "Yixiang Wang"
                    },
                    {
                        "authorId": "2130356639",
                        "name": "Jiqiang Liu"
                    },
                    {
                        "authorId": "2072562098",
                        "name": "Xiaolin Chang"
                    },
                    {
                        "authorId": "2144538151",
                        "name": "Jianhua Wang"
                    },
                    {
                        "authorId": "2146700557",
                        "name": "Ricardo J. Rodr'iguez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this section, we summarize the AdaBelief [18] method in Algo.",
                "uncentered Most optimizers such as Adam and AdaDelta uses uncentered second momentum in the denominator; RMSProp-center [11], SDProp [22] and AdaBelief [18] use square root of centered",
                "The adaptive family uses element-wise learning rate, and the representatives include AdaGrad [9], AdaDelta [10], RMSProp [11], Adam [12] and its variants such as AdamW [13], AMSGrad [14] AdaBound [15], AdaShift [16], RAdam [17] and AdaBelief [18].",
                "AdaBelief (Sync-Center) AdaBelief optimizer [18] is summarized in Algo.",
                "2 imply that async-optimizers achieves a convergence rate of O(1/ \u221a T ) for the stochastic non-convex problem, which matches the oracle complexity and outperforms the O(logT/ \u221a T ) rate of sync-optimizers (Adam [14], RMSProp[25], AdaBelief [18]).",
                "is reported in PyTorch Documentation, \u2020 is reported in [30], \u2217 is reported in [17], \u2021 is reported in [18] SGD Adam AdamW RAdam AdaShift AdaBelief ACProp 69.",
                "AdaBelief [18] is shown to achieve good generalization like the SGD family, fast convergence like the adaptive family, and training stability in complex settings such as GANs.",
                "In this section, we show that ACProp converges at a rate of O(1/ \u221a T ) in the stochastic nonconvex case, which matches the oracle [23] for first-order optimizers and outperforms the O(logT/ \u221a T ) rate for sync-optimizers (Adam, RMSProp and AdaBelief) [26, 25, 18]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "2230bbefb20de8ca700d4ee684f3a0b32c88da58",
                "externalIds": {
                    "ArXiv": "2110.05454",
                    "DBLP": "journals/corr/abs-2110-05454",
                    "CorpusId": 260498313
                },
                "corpusId": 260498313,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2230bbefb20de8ca700d4ee684f3a0b32c88da58",
                "title": "Momentum Centering and Asynchronous Update for Adaptive Gradient Methods",
                "abstract": "We propose ACProp (Asynchronous-centering-Prop), an adaptive optimizer which combines centering of second momentum and asynchronous update (e.g. for $t$-th update, denominator uses information up to step $t-1$, while numerator uses gradient at $t$-th step). ACProp has both strong theoretical properties and empirical performance. With the example by Reddi et al. (2018), we show that asynchronous optimizers (e.g. AdaShift, ACProp) have weaker convergence condition than synchronous optimizers (e.g. Adam, RMSProp, AdaBelief); within asynchronous optimizers, we show that centering of second momentum further weakens the convergence condition. We demonstrate that ACProp has a convergence rate of $O(\\frac{1}{\\sqrt{T}})$ for the stochastic non-convex case, which matches the oracle rate and outperforms the $O(\\frac{logT}{\\sqrt{T}})$ rate of RMSProp and Adam. We validate ACProp in extensive empirical studies: ACProp outperforms both SGD and other adaptive optimizers in image classification with CNN, and outperforms well-tuned adaptive optimizers in the training of various GAN models, reinforcement learning and transformers. To sum up, ACProp has good theoretical properties including weak convergence condition and optimal convergence rate, and strong empirical performance including good generalization like SGD and training stability like Adam. We provide the implementation at https://github.com/juntang-zhuang/ACProp-Optimizer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46251934",
                        "name": "Juntang Zhuang"
                    },
                    {
                        "authorId": "144506371",
                        "name": "Yifan Ding"
                    },
                    {
                        "authorId": "2069747336",
                        "name": "Tommy M. Tang"
                    },
                    {
                        "authorId": "5507046",
                        "name": "N. Dvornek"
                    },
                    {
                        "authorId": "1688323",
                        "name": "S. Tatikonda"
                    },
                    {
                        "authorId": "2158859164",
                        "name": "James S. Duncan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The algorithm claimed to have two properties, fast convergence as in adaptive gradient methods, and good generalization as in the SGD family.[1]",
                "As shown in Figure 1, we can observe that Adam[4] and AdaBelief[1] optimizer begin to show the sign of overfitting at epoch 3, their train losses are flattened and test losses start to fluctuate.",
                "Recently two new adam optimizers, AdaBelief[1] and Padam[5] are introduced among the community.",
                "Recently, AdaBelief[1] and Padam[5] are introduced among the community."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "efccb62274930340d11ff627d09673a4c95bc66f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-01598",
                    "ArXiv": "2110.01598",
                    "CorpusId": 238259292
                },
                "corpusId": 238259292,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/efccb62274930340d11ff627d09673a4c95bc66f",
                "title": "Effectiveness of Optimization Algorithms in Deep Image Classification",
                "abstract": "Adam is applied widely to train neural networks. Different kinds of Adam methods with different features pop out. Recently two new adam optimizers, AdaBelief and Padam are introduced among the community. We analyze these two adam optimizers and compare them with other conventional optimizers (Adam, SGD + Momentum) in the scenario of image classification. We evaluate the performance of these optimization algorithms on AlexNet and simplified versions of VGGNet, ResNet using the EMNIST dataset. (Benchmark algorithm is available at \\hyperref[https://github.com/chuiyunjun/projectCSC413]{https://github.com/chuiyunjun/projectCSC413}).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2112446711",
                        "name": "Zhaoyang Zhu"
                    },
                    {
                        "authorId": "2156232108",
                        "name": "Haozhe Sun"
                    },
                    {
                        "authorId": "2145179001",
                        "name": "Chi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We trained MIDeepBR using the adabelief [Zhuang et al. 2020] optimizer, with learning rate of 0.",
                "We trained MIDeepBR using the adabelief [Zhuang et al. 2020] optimizer, with learning rate of 0.00001 for 100 epochs."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "eafbc65f9568941cd711bd333386de78c553c3e3",
                "externalIds": {
                    "DBLP": "conf/sbbd/MartinsCMMM21",
                    "DOI": "10.5753/sbbd.2021.17868",
                    "CorpusId": 245172773
                },
                "corpusId": 245172773,
                "publicationVenue": {
                    "id": "ae70daa0-c404-4e18-ac73-f8a37c17fdd9",
                    "name": "Brazilian Symposium on Databases",
                    "type": "conference",
                    "alternate_names": [
                        "SBBD",
                        "Braz Symp Database"
                    ],
                    "url": "http://www.sbc.org.br/"
                },
                "url": "https://www.semanticscholar.org/paper/eafbc65f9568941cd711bd333386de78c553c3e3",
                "title": "Detection of Misinformation about COVID-19 in Brazilian Portuguese WhatsApp Messages Using Deep Learning",
                "abstract": "During the COVID-19 pandemic, the misinformation problem arose once again through social networks, like a harmful health advice and false solutions epidemic. In Brazil, as well as in many developing countries, one of the primary sources of misinformation is the messaging application WhatsApp. Thus, the automatic misinformation detection (MID) about COVID-19 in Brazilian Portuguese WhatsApp messages becomes a crucial challenge. Still, due to WhatsApp's private messaging nature, there are still few methods of misinformation detection developed specifically for the WhatsApp platform. In this paper, we propose a new approach, called MIDeepBR, based on BiLSTM neural networks, pooling operations and attention mechanism, which is able to automatically detect misinformation in Brazilian Portuguese WhatsApp messages. Experimental results evidence the suitability of the proposed approach to automatic misinformation detection. Our best results achieved an F1 score of 0.834, while in previous works, the best results achieved an F1 score of 0.778. Thus, MIDeepBR outperforms the previous works.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "133627373",
                        "name": "Ant\u00f4nio Diogo Forte Martins"
                    },
                    {
                        "authorId": "2106437321",
                        "name": "Lucas Cabral"
                    },
                    {
                        "authorId": "2106437591",
                        "name": "Pedro Jorge Chaves Mour\u00e3o"
                    },
                    {
                        "authorId": "145183879",
                        "name": "Jos\u00e9 Maria S. Monteiro"
                    },
                    {
                        "authorId": "40342539",
                        "name": "Javam C. Machado"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The baseline optimizers were SGDM, Adam, AdaBelief [66], Lookahead [65], AdaHessian [63] and RNA [52].",
                "In practice, the choices of optimizers can vary for different applications [63] and their practical performances are still unsatisfactory in terms of convergence rate or generalization ability [66].",
                "The experimental setting was the same as that in AdaBelief [66]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "259b3c379577560fc625a4d66dce089e77f879b3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-01543",
                    "ArXiv": "2110.01543",
                    "CorpusId": 238259097
                },
                "corpusId": 238259097,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/259b3c379577560fc625a4d66dce089e77f879b3",
                "title": "Stochastic Anderson Mixing for Nonconvex Stochastic Optimization",
                "abstract": "Anderson mixing (AM) is an acceleration method for fixed-point iterations. Despite its success and wide usage in scientific computing, the convergence theory of AM remains unclear, and its applications to machine learning problems are not well explored. In this paper, by introducing damped projection and adaptive regularization to classical AM, we propose a Stochastic Anderson Mixing (SAM) scheme to solve nonconvex stochastic optimization problems. Under mild assumptions, we establish the convergence theory of SAM, including the almost sure convergence to stationary points and the worst-case iteration complexity. Moreover, the complexity bound can be improved when randomly choosing an iterate as the output. To further accelerate the convergence, we incorporate a variance reduction technique into the proposed SAM. We also propose a preconditioned mixing strategy for SAM which can empirically achieve faster convergence or better generalization ability. Finally, we apply the SAM method to train various neural networks including the vanilla CNN, ResNets, WideResNet, ResNeXt, DenseNet and RNN. Experimental results on image classification and language model demonstrate the advantages of our method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113261072",
                        "name": "Fu Wei"
                    },
                    {
                        "authorId": "3183763",
                        "name": "Chenglong Bao"
                    },
                    {
                        "authorId": "2152801239",
                        "name": "Yang Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u00b7 In our experiment, using the AdaBelief[15] optimiFrom VIS To OVIS: A Technical Report To Promote The Development Of The",
                "[15] Juntang Zhuang, Tommy Tang, Sekhar Tatikonda, Nicha Dvornek, et al. AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients.",
                "For optimizer, we did not use AdamW[14] or Adam[20] but chose AdaBelief[15], in which weight_decay is set to 1e-4, weight_decouple and rectify are both set to True.",
                "[15] Juntang Zhuang, Tommy Tang, Sekhar Tatikonda, Nicha Dvornek, et al.",
                "Because the feature map of the instances is obtained\nby copying feature map in mask head FPN[7], the training difficulty increases after removing DCN and DCNv2[13] is more adaptable to network adjustments than DCN.\n\u00b7 The mask head can be simplified and some modules such as 3D convolution can be removed from VisTR,\nbut it needs to follow the processing pipeline of DETR's FPN. \u00b7 In our experiment, using the AdaBelief[15] optimi-\n2\nzer can speed up training, and the learning rate setting should not greater than 1e-4."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "aca17e41075721caa0569e7d82eed6f90214b81f",
                "externalIds": {
                    "DBLP": "conf/iccvw/LiLXL21",
                    "DOI": "10.1109/ICCVW54120.2021.00432",
                    "CorpusId": 244461142
                },
                "corpusId": 244461142,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/aca17e41075721caa0569e7d82eed6f90214b81f",
                "title": "From VIS To OVIS: A Technical Report To Promote The Development Of The Field",
                "abstract": "Occluded Video instance segmentation(OVIS) is a new vision task that has emerged in this years and is processed by video deep learning algorithms. It uses continuous video frames as input, generally ranging from a few frames to hundreds of frames. Before OVIS, there has a task called VIS. To tackle the task of OVIS and VIS, we design a new alghorithm called SimVTR, which based on DETR and VisTR. During the experiment, although we acquire the 27.66 mAP on OVIS test, 25.18m AP on OVIS val, and 31.9 mAP on VIS test, we have found a surprising phenomena that the evaluation mechanism is not sensitive to our mothod SimVTR. When we only use one frame to inference, the model can acquire the similar mAP as dozens frames. SimpleVTR trade off and optimizes the computing resources and effects of end-to-end video instance segmentation algorithm. We used one RTX1080Ti (11G) to experiment, and the batch size can change from 1 to 16 frames. We were surprised to find that only one frame can also get a very high score in inference. The VIS and OVIS cocoapi have some unreasonable place in ytvoseval.py. In this technical report, we prudently point out the phenomena that the evaluation mechanism could have some bug. If this is true, we need check our model to promote the process of the video instance segmentation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116112934",
                        "name": "Wenbo Li"
                    },
                    {
                        "authorId": "2142886986",
                        "name": "Xuesheng Li"
                    },
                    {
                        "authorId": "47514901",
                        "name": "Qiwei Xu"
                    },
                    {
                        "authorId": "2127179506",
                        "name": "Chen Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1afb3dfd5805c10de11db33ce98e53a7ee95ae6a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-14925",
                    "ArXiv": "2109.14925",
                    "CorpusId": 238226695
                },
                "corpusId": 238226695,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1afb3dfd5805c10de11db33ce98e53a7ee95ae6a",
                "title": "Genealogical Population-Based Training for Hyperparameter Optimization",
                "abstract": "HyperParameter Optimization (HPO) aims at finding the best HyperParameters (HPs) of learning models, such as neural networks, in the fastest and most efficient way possible. Most recent HPO algorithms try to optimize HPs regardless of the model that obtained them, assuming that for different models, same HPs will produce very similar results. We break free from this paradigm and propose a new take on preexisting methods that we called Genealogical Population Based Training (GPBT). GPBT, via the shared histories of\"genealogically\"-related models, exploit the coupling of HPs and models in an efficient way. We experimentally demonstrate that our method cuts down by 2 to 3 times the computational cost required, generally allows a 1% accuracy improvement on computer vision tasks, and reduces the variance of the results by an order of magnitude, compared to the current algorithms. Our method is search-algorithm agnostic so that the inner search routine can be any search algorithm like TPE, GP, CMA or random search.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2130973415",
                        "name": "Antoine Scardigli"
                    },
                    {
                        "authorId": "49207108",
                        "name": "P. Fournier"
                    },
                    {
                        "authorId": "2130968999",
                        "name": "Matteo Vilucchio"
                    },
                    {
                        "authorId": "1729623",
                        "name": "D. Naccache"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 We validate the superiority of the proposed injection concept with the recent state-of-the-art optimizers, including Adam [27], diffGrad [28], Radam [29] and AdaBelief [30] using a wide range of CNN models for image classification over four benchmark datasets.",
                "However, the consideration of parameter history is important as the gradient behavior and required stepsize are different for different regions of loss optimization landscape [43], [30].",
                "Basically, we use the proposed AdaInject concept with four existing state-of-the-art optimizers, including Adam [27], diffGrad [28], Radam [29] and AdaBelief [30], and propose the corresponding AdamInject (i.",
                "Several SGD based optimization techniques have been proposed in the recent past [24], [25], [26], [27], [28], [29], [30], and etc.",
                "AdaBelief [30] uses the belief in gradients to compute the second order moment.",
                ", Adam [27], diffGrad [28], Radam [29] and AdaBelief [30]), without and with the proposed injection approach.",
                "But, we show experimently that this problem can be reduced by considering AdaBelief concept [30] with the proposed injection idea (i.",
                "A typical scenario in the optimization depicting the importance of adaptive parameter update in optimization [43], [30]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f2ac47e2e2060a9f5ecae2acff71cb1e3f911d8b",
                "externalIds": {
                    "ArXiv": "2109.12504",
                    "DOI": "10.1109/tai.2022.3208223",
                    "CorpusId": 252367399
                },
                "corpusId": 252367399,
                "publicationVenue": {
                    "id": "3c27e831-750f-45bc-9914-2148a5259eba",
                    "name": "IEEE Transactions on Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Artif Intell"
                    ],
                    "issn": "2691-4581",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688"
                },
                "url": "https://www.semanticscholar.org/paper/f2ac47e2e2060a9f5ecae2acff71cb1e3f911d8b",
                "title": "AdaInject: Injection Based Adaptive Gradient Descent Optimizers for Convolutional Neural Networks",
                "abstract": "\u2014The convolutional neural networks (CNNs) are gen- erally trained using stochastic gradient descent (SGD) based optimization techniques. The existing SGD optimizers generally suffer with the overshooting of the minimum and oscillation near minimum. In this paper, we propose a new approach, hereafter referred as AdaInject, for the gradient descent optimizers by injecting the second order moment into the \ufb01rst order moment. Speci\ufb01cally, the short-term change in parameter is used as a weight to inject the second order moment in the update rule. The AdaInject optimizer controls the parameter update, avoids the overshooting of the minimum and reduces the oscillation near minimum. The proposed approach is generic in nature and can be integrated with any existing SGD optimizer. The effectiveness of the AdaInject optimizer is explained intuitively as well as through some toy examples. We also show the convergence property of the proposed injection based optimizer. Further, we depict the ef\ufb01cacy of the AdaInject approach through extensive experi- ments in conjunction with the state-of-the-art optimizers, namely AdamInject, diffGradInject, RadamInject, and AdaBeliefInject on four benchmark datasets. Different CNN models are used in the experiments. A highest improvement in the top-1 classi\ufb01cation error rate of 16 . 54% is observed using diffGradInject optimizer with ResNeXt29 model over the CIFAR10 dataset. Overall, we observe very promising performance improvement of existing optimizers with the proposed AdaInject approach. The code is available at: https://github.com/shivram1987/AdaInject. on the gradient behavior. However, the existing gradient descent optimization techniques either overshoot the \u201csteep and narrow\u201d valley (i.e., minimum) or oscillate near it, due to large step size caused by the exponential moving average of gradients used for parameter updates. The AdaInject optimization technique we introduce in this paper tackled this problem by incorporating the immediate parameter change weighted second order moment injection for the parameter updates. Using the proposed optimization technique, a signi\ufb01cant improvement is observed in the performance of image classi\ufb01cation using different CNN models. Moreover, the proposed AdaInject approach can be used with any existing adaptive moment based optimization technique. Hence, it can provide the alternative optimizers with better step size control to train different deep learning models for diverse applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34992579",
                        "name": "S. Dubey"
                    },
                    {
                        "authorId": "153037548",
                        "name": "S. H. Shabbeer Basha"
                    },
                    {
                        "authorId": "2108384213",
                        "name": "S. Singh"
                    },
                    {
                        "authorId": "1759420",
                        "name": "B. Chaudhuri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The Adablief optimizer has been shown to have as fast a convergence as Adam, while having both a better generalization and a better training stability compared to Adam [36].",
                "The Adablief optimizer is used which has been shown to have a fast training, good generalization and training stability [36] with a learning rate set to 10\u22125 and weight decay of 0.",
                "(iv) Using the Adablief optimizer compared to the Adam optimizer improves the performance of the network.",
                "We also show the performance for networks with no dilated convolutions, Orthogonal weight initialization [42] instead of Glorot weight initialization, and Adam optimizer [43] instead of Adablief as well as a smaller network with 32 filters instead of 64."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d9c623b6f2921f5b6a631f56d65c72b6b81d153f",
                "externalIds": {
                    "DOI": "10.36227/techrxiv.16624477.v1",
                    "CorpusId": 239301849
                },
                "corpusId": 239301849,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d9c623b6f2921f5b6a631f56d65c72b6b81d153f",
                "title": "Speaker-Independent Speech Enhancement with Brain Signals",
                "abstract": "Single-channel speech enhancement algorithms have seen great improvements over the past few years. Despite these improvements, they still lack the efficiency of the auditory system in extracting attended auditory information in the presence of competing speakers. Recently, it has been shown that the attended auditory information can be decoded from the brain activity of the listener. In this paper, we propose two novel deep learning methods referred to as the Brain Enhanced Speech Denoiser (BESD) and the U-shaped Brain Enhanced Speech Denoiser (U-BESD) respectively, that take advantage of this fact to denoise a multi-talker speech mixture. We use a Feature-wise Linear Modulation (FiLM) between the brain activity and the sound mixture, to better extract the features of the attended speaker to perform speech enhancement. We show, using electroencephalography (EEG) signals recorded from the listener, that U-BESD outperforms a current autoencoder approach in enhancing a speech mixture as well as a speech separation approach that uses brain activity. Moreover, we show that both BESD and U-BESD successfully extract the attended speaker without any prior information about this speaker. This makes both algorithms great candidates for realistic applications where no prior information about the attended speaker is available, such as hearing aids, cellphones, or noise cancelling headphones. All procedures were performed in accordance with the Declaration of Helsinki and were approved by the Ethics Committees of the School of Psychology at Trinity College Dublin, and the Health Sciences Faculty at Trinity College Dublin.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2057763537",
                        "name": "Maryam Hosseini"
                    },
                    {
                        "authorId": "39500693",
                        "name": "L. Celotti"
                    },
                    {
                        "authorId": "2230740",
                        "name": "\u00c9. Plourde"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief computing stepsize by the \u2018belief\u2019 in the current gradient direction[6] and achieves a better performance than AdaBound.",
                "Many variants are proposed to solve the problem, such as AdaBound[4], Radam[5], AdaBelief[6]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bfb013aec77f6bb7ce2a508f29094b6ddb9b454b",
                "externalIds": {
                    "DOI": "10.1088/1742-6596/2010/1/012027",
                    "CorpusId": 237500311
                },
                "corpusId": 237500311,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bfb013aec77f6bb7ce2a508f29094b6ddb9b454b",
                "title": "AdaDiff: Adaptive Gradient Descent with the Differential of Gradient",
                "abstract": "Optimization methods are crucial to train deep neural networks. Adaptive optimization methods, especially Adam, are wildly used because they aren\u2019t sensitive to the selection of learning rate and converge fast. Recent work point out Adam has a performance gap with SGD and even not converge because of the unstable and extreme learning rates. Many variants of Adam are proposed to solve the problem, such as AMSGrad, AdaBound and AdaBelief. In this paper, we propose a new variant of Adam, called AdaDiff. AdaDiff computing gradient descent step size by the exponential moving average(EMA) of gradient and differential of gradients aiming to make the training of networks more stable. We compare our method with other optimizers on various tasks. The results show AdaDiff outperforms Adam and minimizes the performance gap with SGD.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148909394",
                        "name": "Dian Huang"
                    },
                    {
                        "authorId": "50559722",
                        "name": "Dawei Yin"
                    },
                    {
                        "authorId": "2126225749",
                        "name": "Lijun Zhang"
                    },
                    {
                        "authorId": "1800117",
                        "name": "Dezhong Peng"
                    },
                    {
                        "authorId": "2127426897",
                        "name": "Qilin Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief Optimizer (Zhuang et al., 2020) was adopted to train our models, the base learning rate was set to 1e-4, beta=(0.9, 0.999), epsilon=1e-8, weight_decouple=True, weight_decay=1e-2 for non-bias weights.",
                "AdaBelief Optimizer (Zhuang et al., 2020) was adopted to train our models, the base learning rate was set to 1e4, beta=(0."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fed4c11c40a559031bae2e7f092ebfc6ad8256c5",
                "externalIds": {
                    "DOI": "10.1101/2021.08.24.457581",
                    "CorpusId": 237356338
                },
                "corpusId": 237356338,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/fed4c11c40a559031bae2e7f092ebfc6ad8256c5",
                "title": "Effective Ensemble of Deep Neural Networks Predicts Neural Responses to Naturalistic Videos",
                "abstract": "This report provides a review of our submissions to the Algonauts Challenge 2021. In this challenge, neural responses in the visual cortex were recorded using functional neuroimaging when participants were watching naturalistic videos. The goal of the challenge is to develop voxel-wise encoding models which predict such neural signals based on the input videos. Here we built an ensemble of models that extract representations based on the input videos from 4 perspectives: image streams, motion, edges, and audio. We showed that adding new modules into the ensemble consistently improved our prediction performance. Our methods achieved state-of-the-art performance on both the mini track and the full track tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150129143",
                        "name": "Huzheng Yang"
                    },
                    {
                        "authorId": "2437353",
                        "name": "Shanghang Zhang"
                    },
                    {
                        "authorId": "2013680445",
                        "name": "Yifan Wu"
                    },
                    {
                        "authorId": "2144428889",
                        "name": "Yuanning Li"
                    },
                    {
                        "authorId": "80914691",
                        "name": "Shi Gu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This paper considers the following problem [4, 32].",
                "In: Advances in Neural Information Processing Systems (2019) [32] Zhuang, J., Tang, T., Ding, Y., Tatikonda, S., Dvornek, N., Papademetris, X., Duncan, J.S.: AdaBelief optimizer: Adapting stepsizes by the belief in observed gradients.",
                "In [12], a method was presented to unify useful adaptive methods such as AMSGrad and AdaBelief, and it was shown that the method with \u03b1k = 1/ \u221a k has an O(1/ \u221a K) convergence rate, which improves on the results in [4, 32].",
                "Convergence analyses of adaptive methods for nonconvex optimization were studied in [6, 4, 32, 12].",
                "The useful optimizers, such as N-Momentum, AMSGrad, AMSBound, and AdaBelief (Table 3), all satisfy the following conditions:\nAssumption 2.2.",
                "In this paper, we consider the following algorithm (Algorithm 1), which is a unified algorithm for useful optimizers, for example, N-Momentum [19, 27], AMSGrad [21, 4], AMSBound [15], and AdaBelief [32], listed in Table 3 in Appendix.",
                "AdaBelief (named for adapting stepsizes by the belief in observed gradients) using \u03b1k = 1/ \u221a k has O(logK/ \u221a K) convergence [32]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a4b6f886f3f5742c2b9ec9caa9c0557ee1d98c2f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-11713",
                    "ArXiv": "2108.11713",
                    "CorpusId": 237303805
                },
                "corpusId": 237303805,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a4b6f886f3f5742c2b9ec9caa9c0557ee1d98c2f",
                "title": "The Number of Steps Needed for Nonconvex Optimization of a Deep Learning Optimizer is a Rational Function of Batch Size",
                "abstract": "Recently, convergence as well as convergence rate analyses of deep learning optimizers for nonconvex optimization have been widely studied. Meanwhile, numerical evaluations for the optimizers have precisely clarified the relationship between batch size and the number of steps needed for training deep neural networks. The main contribution of this paper is to show theoretically that the number of steps needed for nonconvex optimization of each of the optimizers can be expressed as a rational function of batch size. Having these rational functions leads to two particularly important facts, which were validated numerically in previous studies. The first fact is that there exists an optimal batch size such that the number of steps needed for nonconvex optimization is minimized. This implies that using larger batch sizes than the optimal batch size does not decrease the number of steps needed for nonconvex optimization. The second fact is that the optimal batch size depends on the optimizer. In particular, it is shown theoretically that momentum and Adam-type optimizers can exploit larger optimal batches and further reduce the minimum number of steps needed for nonconvex optimization than can the stochastic gradient descent optimizer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2018304",
                        "name": "H. Iiduka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the optimization, we used AdaBelief (Zhuang et al., 2020) with Adaptive Gradient Clipping (AGC) and a Cosine Annealing Schedule (Loshchilov and Hutter, 2017)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d154cafb9be570c6b5f81142fa0591a39f156184",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-08688",
                    "ArXiv": "2108.08688",
                    "CorpusId": 237213301
                },
                "corpusId": 237213301,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d154cafb9be570c6b5f81142fa0591a39f156184",
                "title": "Contrastive Language-Image Pre-training for the Italian Language",
                "abstract": "CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal model that jointly learns representations of images and texts. The model is trained on a massive amount of English data and shows impressive performance on zero-shot classification tasks. Training the same model on a different language is not trivial, since data in other languages might be not enough and the model needs high-quality translations of the texts to guarantee a good performance. In this paper, we present the first CLIP model for the Italian Language (CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show that CLIP-Italian outperforms the multilingual CLIP model on the tasks of image retrieval and zero-shot classification.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49224924",
                        "name": "Federico Bianchi"
                    },
                    {
                        "authorId": "1481857041",
                        "name": "Giuseppe Attanasio"
                    },
                    {
                        "authorId": "2123694335",
                        "name": "Raphael Pisoni"
                    },
                    {
                        "authorId": "1392653707",
                        "name": "Silvia Terragni"
                    },
                    {
                        "authorId": "1897770594",
                        "name": "Gabriele Sarti"
                    },
                    {
                        "authorId": "2069062028",
                        "name": "S. Lakshmi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Afterward, we use \u2018Adam\u2019 and \u2018Adabelief\u2019 adaptive gradient decent optimizers, which are originally used in fitting neural networks in data science, to find the minimum of loss function.",
                "The \u2018Adabelief\u2019 [9] is another optimizer that we use to estimate \u03c3.",
                "In this paper, we transform the approximation of implied volatility from finding the root of the B-S equation to an optimization model and use two adaptive gradient descent methods (\u2018Adam\u2019 and \u2018Adabelief\u2019) to approximate the value of implied volatility.",
                "One is the ratio of not convergent samples, defined as\nNC = 1\nN N\u2211 j=1 l(cj) with l(cj) =\n{ 0, \u2223\u2223c\u0302j,n \u2212 c\u0302j,n\u22121\u2223\u2223   10\u22124 1, otherwise\nThe other is\nMAE = 1\nN\u0304 N\u2211 j=1 \u2223\u2223c\u0302j \u2212 cj\u2223\u2223 (1\u2212 l(cj)) with N\u0304 = N(1\u2212NC) We present the numerical results of Newton-Raphson iteration, Adam, Adabelief methods in Table 1 with different beginning points \u03c30 = 0.1, 0.25, 0.4, 0.55, 0.7, 0.85, 1."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "45a4d55d4357753887f013f0196a9722e487b3ec",
                "externalIds": {
                    "ArXiv": "2108.07035",
                    "CorpusId": 237091856
                },
                "corpusId": 237091856,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/45a4d55d4357753887f013f0196a9722e487b3ec",
                "title": "Adaptive Gradient Descent Methods for Computing Implied Volatility",
                "abstract": "In this paper, a new numerical method based on adaptive gradient descent optimizers is provided for computing the implied volatility from the Black-Scholes (B-S) option pricing model. It is shown that the new method is more accurate than the close form approximation. Compared with the Newton-Raphson method, the new method obtains a reliable rate of convergence and tends to be less sensitive to the beginning point.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47006268",
                        "name": "Yixia Lu"
                    },
                    {
                        "authorId": "2108927075",
                        "name": "Yihong Wang"
                    },
                    {
                        "authorId": "102673408",
                        "name": "Tinggan Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is trained for 2000 epochs with the AdaBelief optimizer [26] with learning rate 1."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "dbe26576e232dbb148f8b8b91a646a848da63df6",
                "externalIds": {
                    "ArXiv": "2108.06261",
                    "CorpusId": 237048176
                },
                "corpusId": 237048176,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dbe26576e232dbb148f8b8b91a646a848da63df6",
                "title": "Emulating ultrafast dissipative quantum dynamics with deep neural networks",
                "abstract": "The simulation of driven dissipative quantum dynamics is often prohibitively computation-intensive, especially when it is calculated for various shapes of the driving field. We engineer a new feature space for representing the field and demonstrate that a deep neural network can be trained to emulate these dynamics by mapping this representation directly to the target observables. We demonstrate that with this approach, the system response can be retrieved many orders of magnitude faster. We verify the validity of our approach using the example of finite transverse Ising model irradiated with few-cycle magnetic pulses interacting with a Markovian environment. We show that our approach is sufficiently generalizable and robust to reproduce responses to pulses outside the training set.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "102617464",
                        "name": "N. Klimkin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "47c8d75ce048f8037d2e66e0e36000541866ee46",
                "externalIds": {
                    "ArXiv": "2108.04893",
                    "DBLP": "journals/corr/abs-2108-04893",
                    "CorpusId": 236975960
                },
                "corpusId": 236975960,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/47c8d75ce048f8037d2e66e0e36000541866ee46",
                "title": "How Self-Supervised Learning Can be Used for Fine-Grained Head Pose Estimation?",
                "abstract": "The cost of head pose labeling is the main challenge of improving the fine-grained Head Pose Estimation (HPE). Although Self-Supervised Learning (SSL) can be a solution to the lack of huge amounts of labeled data, its efficacy for fine-grained HPE is not yet fully explored. This study aims to assess the usage of SSL in fine-grained HPE based on two scenarios : (1) using SSL for weights pre-training procedure , and (2) leveraging auxiliary SSL losses besides HPE. We design a Hybrid Multi-Task Learning (HMTL) architecture based on the ResNet50 backbone in which both strategies are applied. Our experimental results reveal that the combination of both scenarios is the best for HPE. Together, the average error rate is reduced up to 23.1% for AFLW2000 and 14.2% for BIWI benchmark compared to the baseline. Moreover, it is found that some SSL methods are more suitable for transfer learning, while others may be effective when they are considered as auxiliary tasks incorporated into supervised learning. Finally, it is shown that by using the proposed HMTL architecture, the average error is reduced with different types of initial weights: random, ImageNet and SSL pre-trained weights.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2091909928",
                        "name": "Mahdi Pourmirzaei"
                    },
                    {
                        "authorId": "1801348",
                        "name": "G. Montazer"
                    },
                    {
                        "authorId": "2091913488",
                        "name": "Farzaneh Esmaili"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f0877bd35fec38a6d0180880033d1baa6c97e44c",
                "externalIds": {
                    "DBLP": "journals/nca/OzcanCDS23",
                    "PubMedCentral": "8349600",
                    "DOI": "10.1007/s00521-021-06401-z",
                    "CorpusId": 236966289,
                    "PubMed": "34393380"
                },
                "corpusId": 236966289,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f0877bd35fec38a6d0180880033d1baa6c97e44c",
                "title": "A hybrid DNN\u2013LSTM model for detecting phishing URLs",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2064890443",
                        "name": "Alper Ozcan"
                    },
                    {
                        "authorId": "3213443",
                        "name": "C. Catal"
                    },
                    {
                        "authorId": "34895840",
                        "name": "Emrah Donmez"
                    },
                    {
                        "authorId": "2073337939",
                        "name": "Behcet Senturk"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4392dee3dd6c10c8cbcce4d75718863234f1220d",
                "externalIds": {
                    "MAG": "3172271694",
                    "DBLP": "journals/aei/LvZLL21",
                    "DOI": "10.1016/J.AEI.2021.101318",
                    "CorpusId": 236240084
                },
                "corpusId": 236240084,
                "publicationVenue": {
                    "id": "ec497fa8-833a-4d68-873a-539c20989c22",
                    "name": "Advanced Engineering Informatics",
                    "type": "journal",
                    "alternate_names": [
                        "Adv Eng Informatics"
                    ],
                    "issn": "1474-0346",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622240/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/14740346"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4392dee3dd6c10c8cbcce4d75718863234f1220d",
                "title": "A predictive maintenance system for multi-granularity faults based on AdaBelief-BP neural network and fuzzy decision making",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7775158",
                        "name": "Y. Lv"
                    },
                    {
                        "authorId": "1713794517",
                        "name": "Qianwen Zhou"
                    },
                    {
                        "authorId": "121704392",
                        "name": "Yifan Li"
                    },
                    {
                        "authorId": "2000279662",
                        "name": "Weidong Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(5)\nThese two formulas, similarly to AdaDelta and RMSprop, were used to compute changes in the parameters, thus obtaining the following formula of changes:\n\u03b8t+1 = \u03b8t \u2212 \u03b7\u221a\nv\u0302t + e m\u0302t.",
                "The Adam [40] optimizer computes the learning rate (LR) as a function of data by storing the exponential mean reduction in previous gradients sums (vt) such as AdaDelta and RMSprop [41], which keeps the exponential mean reduction in mt gradients similarly to an acceleration technique.\nmt = \u03b21mt\u22121 + (1\u2212 \u03b21)gt, (2)\nvt = \u03b22vt\u22121 + (1\u2212 \u03b22)g2t , (3)\nwhere mt and vt are the first moment estimates as means and second moment estimates as the decentralized variance of gradients, respectively.",
                "To solve the problem, they used corrected estimates of the first and second estimates [41].",
                "Adam Optimization Algorithm The Adam [40] optimizer computes the learning rate (LR) as a function of data by storing the exponential mean reduction in previous gradients sums (vt) such as AdaDelta and RMSprop [41], which keeps the exponential mean reduction in mt gradients similarly to an acceleration technique."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "80d92d993137e72e693988887c09d411ce4ec730",
                "externalIds": {
                    "MAG": "3190322693",
                    "DOI": "10.3390/en14154649",
                    "CorpusId": 238766754
                },
                "corpusId": 238766754,
                "publicationVenue": {
                    "id": "1cd505d9-195d-4f99-b91c-169e872644d4",
                    "name": "Energies",
                    "type": "journal",
                    "issn": "1996-1073",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155563",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155563",
                        "https://www.mdpi.com/journal/energies",
                        "http://www.mdpi.com/journal/energies"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/80d92d993137e72e693988887c09d411ce4ec730",
                "title": "Efficient Design of Energy Disaggregation Model with BERT-NILM Trained by AdaX Optimization Method for Smart Grid",
                "abstract": "One of the basic conditions for the successful implementation of energy demand-side management (EDM) in smart grids is the monitoring of different loads with an electrical load monitoring system. Energy and sustainability concerns present a multitude of issues that can be addressed using approaches of data mining and machine learning. However, resolving such problems due to the lack of publicly available datasets is cumbersome. In this study, we first designed an efficient energy disaggregation (ED) model and evaluated it on the basis of publicly available benchmark data from the Residential Energy Disaggregation Dataset (REDD), and then we aimed to advance ED research in smart grids using the Turkey Electrical Appliances Dataset (TEAD) containing household electricity usage data. In addition, the TEAD was evaluated using the proposed ED model tested with benchmark REDD data. The Internet of things (IoT) architecture with sensors and Node-Red software installations were established to collect data in the research. In the context of smart metering, a nonintrusive load monitoring (NILM) model was designed to classify household appliances according to TEAD data. A highly accurate supervised ED is introduced, which was designed to raise awareness to customers and generate feedback by demand without the need for smart sensors. It is also cost-effective, maintainable, and easy to install, it does not require much space, and it can be trained to monitor multiple devices. We propose an efficient BERT-NILM tuned by new adaptive gradient descent with exponential long-term memory (Adax), using a deep learning (DL) architecture based on bidirectional encoder representations from transformers (BERT). In this paper, an improved training function was designed specifically for tuning of NILM neural networks. We adapted the Adax optimization technique to the ED field and learned the sequence-to-sequence patterns. With the updated training function, BERT-NILM outperformed state-of-the-art adaptive moment estimation (Adam) optimization across various metrics on REDD datasets; lastly, we evaluated the TEAD dataset using BERT-NILM training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1792413",
                        "name": "I. Cavdar"
                    },
                    {
                        "authorId": "2132626444",
                        "name": "Vahit Feryad"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "899995e30820d657b2e2feb791025cd6b2027e22",
                "externalIds": {
                    "ArXiv": "2107.10879",
                    "DBLP": "journals/corr/abs-2107-10879",
                    "DOI": "10.1038/s42005-022-00987-z",
                    "CorpusId": 236318229
                },
                "corpusId": 236318229,
                "publicationVenue": {
                    "id": "4c30a099-ac6f-4ed0-ac0e-0ca618941906",
                    "name": "Communications Physics",
                    "type": "journal",
                    "alternate_names": [
                        "Commun Phys",
                        "Communications in Physics"
                    ],
                    "issn": "2399-3650",
                    "alternate_issns": [
                        "0868-3166"
                    ],
                    "url": "http://www.nature.com/commsphys/"
                },
                "url": "https://www.semanticscholar.org/paper/899995e30820d657b2e2feb791025cd6b2027e22",
                "title": "Discovering sparse interpretable dynamics from partial observations",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2069300320",
                        "name": "Peter Y. Lu"
                    },
                    {
                        "authorId": "134129737",
                        "name": "J. Ari\u00f1o"
                    },
                    {
                        "authorId": "1973666",
                        "name": "M. Solja\u010di\u0107"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "8a3d37e9217073f9aa29db4ce3103339205f94b3",
                "externalIds": {
                    "MAG": "3184553890",
                    "DOI": "10.5772/INTECHOPEN.98836",
                    "CorpusId": 237688526
                },
                "corpusId": 237688526,
                "publicationVenue": {
                    "id": "96018464-22dc-4b5c-a172-c2f4a30ce131",
                    "name": "Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell"
                    ],
                    "issn": "0004-3702",
                    "alternate_issns": [
                        "2633-1403",
                        "2710-1673",
                        "2710-1681"
                    ],
                    "url": "http://www.elsevier.com/locate/artint",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00043702",
                        "https://www.journals.elsevier.com/artificial-intelligence"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8a3d37e9217073f9aa29db4ce3103339205f94b3",
                "title": "Tourist Sentiment Mining Based on Deep Learning",
                "abstract": "Mining the sentiment of the user on the internet via the context plays a significant role in uncovering the human emotion and in determining the exactness of the underlying emotion in the context. An increasingly enormous number of user-generated content (UGC) in social media and online travel platforms lead to development of data-driven sentiment analysis (SA), and most extant SA in the domain of tourism is conducted using document-based SA (DBSA). However, DBSA cannot be used to examine what specific aspects need to be improved or disclose the unknown dimensions that affect the overall sentiment like aspect-based SA (ABSA). ABSA requires accurate identification of the aspects and sentiment orientation in the UGC. In this book chapter, we illustrate the contribution of data mining based on deep learning in sentiment and emotion detection.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108630571",
                        "name": "Weijun Li"
                    },
                    {
                        "authorId": "2109157321",
                        "name": "Qun Yang"
                    },
                    {
                        "authorId": "121837236",
                        "name": "Wencai Du"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b514119b06533589785b76e26100e4eb9d3c9fac",
                "externalIds": {
                    "ArXiv": "2107.08377",
                    "DBLP": "journals/corr/abs-2107-08377",
                    "CorpusId": 236088170
                },
                "corpusId": 236088170,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b514119b06533589785b76e26100e4eb9d3c9fac",
                "title": "A New Adaptive Gradient Method with Gradient Decomposition",
                "abstract": "Adaptive gradient methods, especially Adam-type methods (such as Adam, AMSGrad, and AdaBound), have been proposed to speed up the training process with an element-wise scaling term on learning rates. However, they often generalize poorly compared with stochastic gradient descent (SGD) and its accelerated schemes such as SGD with momentum (SGDM). In this paper, we propose a new adaptive method called DecGD, which simultaneously achieves good generalization like SGDM and obtain rapid convergence like Adam-type methods. In particular, DecGD decomposes the current gradient into the product of two terms including a surrogate gradient and a loss based vector. Our method adjusts the learning rates adaptively according to the current loss based vector instead of the squared gradients used in Adam-type methods. The intuition for adaptive learning rates of DecGD is that a good optimizer, in general cases, needs to decrease the learning rates as the loss decreases, which is similar to the learning rates decay scheduling technique. Therefore, DecGD gets a rapid convergence in the early phases of training and controls the effective learning rates according to the loss based vectors which help lead to a better generalization. Convergence analysis is discussed in both convex and non-convex situations. Finally, empirical results on widely-used tasks and models demonstrate that DecGD shows better generalization performance than SGDM and rapid convergence like Adam-type methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1740580186",
                        "name": "Zhou Shao"
                    },
                    {
                        "authorId": "2109626810",
                        "name": "Tong Lin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f7de0dc20d63600f1be17a3565777564e91bd071",
                "externalIds": {
                    "ArXiv": "2107.07468",
                    "DBLP": "journals/corr/abs-2107-07468",
                    "DOI": "10.3389/fmats.2021.761229",
                    "CorpusId": 235899135
                },
                "corpusId": 235899135,
                "publicationVenue": {
                    "id": "ac9b8e39-9ee1-4486-a18d-7bc21122acfa",
                    "name": "Frontiers in Materials",
                    "type": "journal",
                    "alternate_names": [
                        "Front Mater"
                    ],
                    "issn": "2296-8016",
                    "url": "https://www.frontiersin.org/journals/materials",
                    "alternate_urls": [
                        "http://journal.frontiersin.org/journal/materials",
                        "http://www.frontiersin.org/Materials"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f7de0dc20d63600f1be17a3565777564e91bd071",
                "title": "A Modular U-Net for Automated Segmentation of X-Ray Tomography Images in Composite Materials",
                "abstract": "X-Ray Computed Tomography (XCT) techniques have evolved to a point that high-resolution data can be acquired so fast that classic segmentation methods are prohibitively cumbersome, demanding automated data pipelines capable of dealing with non-trivial 3D images. Meanwhile, deep learning has demonstrated success in many image processing tasks, including materials science applications, showing a promising alternative for a human-free segmentation pipeline. However, the rapidly increasing number of available architectures can be a serious drag to the wide adoption of this type of models by the end user. In this paper a modular interpretation of U-Net (Modular U-Net) is proposed with a parametrized architecture that can be easily tuned to optimize it. As an example, the model is trained to segment 3D tomography images of a three-phased glass fiber-reinforced Polyamide 66. We compare 2D and 3D versions of our model, finding that the former is slightly better than the latter. We observe that human-comparable results can be achievied even with only 13 annotated slices and using a shallow U-Net yields better results than a deeper one. As a consequence, neural networks show indeed a promising venue to automate XCT data processing pipelines needing no human, adhoc intervention.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2119547002",
                        "name": "Jo\u00e3o P C Bertoldo"
                    },
                    {
                        "authorId": "3204485",
                        "name": "Etienne Decenci\u00e8re"
                    },
                    {
                        "authorId": "2182627",
                        "name": "D. Ryckelynck"
                    },
                    {
                        "authorId": "34719700",
                        "name": "H. Proudhon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Training is done in 100-epochs config-\n1https://github.com/jxbz/fromage#voulez-vous-du-fromage 2https://github.com/juntang-zhuang/Adabelief-Optimizer#\nhyper-parameters-in-pytorch\nuration.",
                "\u2026Mutschler and Zell 2020; Mahsereci and Hennig 2015), the gradient change speed (Dubey et al. 2020), a \u201cbelief\u201d in the current gradient direction (Zhuang et al. 2020), the linearization of the loss (Rolinek and Martius 2018), the percomponent unweighted mean of all historical gradients (Daley\u2026",
                "2020), a \u201cbelief\u201d in the current gradient direction (Zhuang et al. 2020), the linearization of the loss (Rolinek and Martius 2018), the percomponent unweighted mean of all historical gradients (Daley and Amato 2020), handling noise by preconditioning based on a covariance matrix (Ida, Fujiwara, and Iwamura 2017), learning the update-step size (Wu, Ward, and Bottou 2020), looking ahead at the sequence of fast weights generated by another optimizer (Zhang et al.",
                "For Adabelief (Zhuang et al. 2020) we follow the hyperparameters reported in the official implementation2 .",
                "For Adabelief (Zhuang et al. 2020) we follow the hyperparameters reported in the official implementation(2) ."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "0594fba89480e82864a4db4d11049e45427eb265",
                "externalIds": {
                    "ArXiv": "2107.03331",
                    "DBLP": "conf/aaai/DavtyanSCMBF22",
                    "DOI": "10.1609/aaai.v36i6.20599",
                    "CorpusId": 245220555
                },
                "corpusId": 245220555,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0594fba89480e82864a4db4d11049e45427eb265",
                "title": "KOALA: A Kalman Optimization Algorithm with Loss Adaptivity",
                "abstract": "Optimization is often cast as a deterministic problem, where the solution is found through some iterative procedure such as gradient descent. However, when training neural networks the loss function changes over (iteration) time due to the randomized selection of a subset of the samples. This randomization turns the optimization problem into a stochastic one. We propose to consider the loss as a noisy observation with respect to some reference optimum. This interpretation of the loss allows us to adopt Kalman filtering as an optimizer, as its recursive formulation is designed to estimate unknown parameters from noisy measurements. Moreover, we show that the Kalman Filter dynamical model for the evolution of the unknown parameters can be used to capture the gradient dynamics of advanced methods such as Momentum and Adam. We call this stochastic optimization method KOALA, which is short for Kalman Optimization Algorithm with Loss Adaptivity. KOALA is an easy to implement, scalable, and efficient method to train neural networks. We provide convergence analysis and show experimentally that it yields parameter estimates that are on par with or better than existing state of the art optimization algorithms across several neural network architectures and machine learning tasks, such as computer vision and language modeling. The project page with the code and the supplementary materials is available at https://araachie.github.io/koala/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "87459556",
                        "name": "A. Davtyan"
                    },
                    {
                        "authorId": "2117714519",
                        "name": "Sepehr Sameni"
                    },
                    {
                        "authorId": "28319879",
                        "name": "L. Cerkezi"
                    },
                    {
                        "authorId": "41016678",
                        "name": "Givi Meishvili"
                    },
                    {
                        "authorId": "48657002",
                        "name": "Adam Bielski"
                    },
                    {
                        "authorId": "145646305",
                        "name": "P. Favaro"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "18a04e02cc3cd0647821cf2a619411b864bd37a9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-01525",
                    "ArXiv": "2107.01525",
                    "CorpusId": 235732304
                },
                "corpusId": 235732304,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/18a04e02cc3cd0647821cf2a619411b864bd37a9",
                "title": "AdaL: Adaptive Gradient Transformation Contributes to Convergences and Generalizations",
                "abstract": "Adaptive optimization methods have been widely used in deep learning. They scale the learning rates adaptively according to the past gradient, which has been shown to be effective to accelerate the convergence. However, they suffer from poor generalization performance compared with SGD. Recent studies point that smoothing exponential gradient noise leads to generalization degeneration phenomenon. Inspired by this, we propose AdaL, with a transformation on the original gradient. AdaL accelerates the convergence by amplifying the gradient in the early stage, as well as dampens the oscillation and stabilizes the optimization by shrinking the gradient later. Such modification alleviates the smoothness of gradient noise, which produces better generalization performance. We have theoretically proved the convergence of AdaL and demonstrated its effectiveness on several benchmarks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143636313",
                        "name": "Hongwei Zhang"
                    },
                    {
                        "authorId": "152741692",
                        "name": "Weidong Zou"
                    },
                    {
                        "authorId": "2146229992",
                        "name": "Hongbo Zhao"
                    },
                    {
                        "authorId": "1401451771",
                        "name": "Qi Ming"
                    },
                    {
                        "authorId": "1920212541",
                        "name": "Tijin Yan"
                    },
                    {
                        "authorId": "2152311405",
                        "name": "Yuanqing Xia"
                    },
                    {
                        "authorId": "1490530284",
                        "name": "Weipeng Cao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Meanwhile, we also generate the matrix At as in the AdaBelief (Zhuang et al., 2020), defined as:\nv\u03030 = 0, v\u0303t = %v\u0303t\u22121 + (1\u2212",
                "Meanwhile, we also generate the matrix At as in the AdaBelief (Zhuang et al., 2020), defined as:",
                "To improve the generalization performance of Adam, recently several adaptive gradient methods such as AdamW (Loshchilov and Hutter, 2017) and AdaBelief (Zhuang et al., 2020) were developed."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3b5633ea4b82802924b2ad3a953fa70eaed250f5",
                "externalIds": {
                    "DBLP": "conf/aistats/HuangWH23",
                    "ArXiv": "2106.16101",
                    "CorpusId": 235683178
                },
                "corpusId": 235683178,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3b5633ea4b82802924b2ad3a953fa70eaed250f5",
                "title": "AdaGDA: Faster Adaptive Gradient Descent Ascent Methods for Minimax Optimization",
                "abstract": "In the paper, we propose a class of faster adaptive Gradient Descent Ascent (GDA) methods for solving the nonconvex-strongly-concave minimax problems by using the unified adaptive matrices, which include almost all existing coordinate-wise and global adaptive learning rates. In particular, we provide an effective convergence analysis framework for our adaptive GDA methods. Specifically, we propose a fast Adaptive Gradient Descent Ascent (AdaGDA) method based on the basic momentum technique, which reaches a lower gradient complexity of $\\tilde{O}(\\kappa^4\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point without large batches, which improves the existing results of the adaptive GDA methods by a factor of $O(\\sqrt{\\kappa})$. Moreover, we propose an accelerated version of AdaGDA (VR-AdaGDA) method based on the momentum-based variance reduced technique, which achieves a lower gradient complexity of $\\tilde{O}(\\kappa^{4.5}\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point without large batches, which improves the existing results of the adaptive GDA methods by a factor of $O(\\epsilon^{-1})$. Moreover, we prove that our VR-AdaGDA method can reach the best known gradient complexity of $\\tilde{O}(\\kappa^{3}\\epsilon^{-3})$ with the mini-batch size $O(\\kappa^3)$. The experiments on policy evaluation and fair classifier learning tasks are conducted to verify the efficiency of our new algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3057688",
                        "name": "Feihu Huang"
                    },
                    {
                        "authorId": "145114933",
                        "name": "Heng Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Supervised learning was earlystopped with 50-epoch patience using the AdaBelief optimizer [38] with the learning rate 1e-4, and 20 epochs for the warmup, and a batch size of 1024."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "340e2925099f5a05037eda6b251895a3b0560564",
                "externalIds": {
                    "DBLP": "conf/iccv/MatsumoriS0FSI21",
                    "ArXiv": "2106.15550",
                    "DOI": "10.1109/ICCV48922.2021.00191",
                    "CorpusId": 235669729
                },
                "corpusId": 235669729,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/340e2925099f5a05037eda6b251895a3b0560564",
                "title": "Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue",
                "abstract": "Building an interactive artificial intelligence that can ask questions about the real world is one of the biggest challenges for vision and language problems. In particular, goal-oriented visual dialogue, where the aim of the agent is to seek information by asking questions during a turn-taking dialogue, has been gaining scholarly attention recently. While several existing models based on the GuessWhat?! dataset [10] have been proposed, the Questioner typically asks simple category-based questions or absolute spatial questions. This might be problematic for complex scenes where the objects share attributes, or in cases where descriptive questions are required to distinguish objects. In this paper, we propose a novel Questioner architecture, called Unified Questioner Transformer (UniQer), for descriptive question generation with referring expressions. In addition, we build a goal-oriented visual dialogue task called CLEVR Ask. It synthesizes complex scenes that require the Questioner to generate descriptive questions. We train our model with two variants of CLEVR Ask datasets. The results of the quantitative and qualitative evaluations show that UniQer outperforms the baseline.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "52187772",
                        "name": "Shoya Matsumori"
                    },
                    {
                        "authorId": "2007248577",
                        "name": "Kosuke Shingyouchi"
                    },
                    {
                        "authorId": "71782118",
                        "name": "Yukikoko Abe"
                    },
                    {
                        "authorId": "28919794",
                        "name": "Yosuke Fukuchi"
                    },
                    {
                        "authorId": "2332462",
                        "name": "K. Sugiura"
                    },
                    {
                        "authorId": "1752970",
                        "name": "M. Imai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare AdaMomentum with seven state-of-the-art optimizers: SGDM Sutskever et al. (2013), Adam Kingma and Ba (2015), AdamW Loshchilov and Hutter (2017a), Yogi Reddi et al. (2018a), AdaBound Luo et al. (2019), RAdam Liu et al. (2019) and AdaBelief Zhuang et al. (2020).",
                "AdaBelief (Zhuang et al., 2020) adapts stepsizes by the belief in the observed gradients.",
                "For all the optimizers, we fix the weight decay parameter value as 1.2e\u22124 following Zhuang et al. (2020).",
                "In testing phase, AdaMomentum can exhibit performance as good as SGDM and far exceeds other baseline adaptive gradient methods, including the recently proposed AdaBelief Zhuang et al. (2020) optimizer.",
                "This contradicts the result reported in Zhuang et al. (2020), where they claim AdaBelief can be better than SGDM.",
                "Training 200 epochs with ResNet-34 on CIFAR-10, our experiments show that AdaMomentum and SGDM can reach over 96% accuracy, while in Zhuang et al. (2020) the accuracy of SGDM is only around 94%.",
                "This largely stems from the fact that Zhuang et al. (2020) did not take an appropriate stepsize annealing strategy or tune the hyperparameters well."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ddcfe20786231ab868cb19457bde8700c34adb00",
                "externalIds": {
                    "ArXiv": "2106.11514",
                    "CorpusId": 246680143
                },
                "corpusId": 246680143,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ddcfe20786231ab868cb19457bde8700c34adb00",
                "title": "Rethinking Adam: A Twofold Exponential Moving Average Approach",
                "abstract": "Adaptive gradient methods, e.g. \\textsc{Adam}, have achieved tremendous success in machine learning. Scaling the learning rate element-wisely by a certain form of second moment estimate of gradients, such methods are able to attain rapid training of modern deep neural networks. Nevertheless, they are observed to suffer from compromised generalization ability compared with stochastic gradient descent (\\textsc{SGD}) and tend to be trapped in local minima at an early stage during training. Intriguingly, we discover that substituting the gradient in the second raw moment estimate term with its momentumized version in \\textsc{Adam} can resolve the issue. The intuition is that gradient with momentum contains more accurate directional information and therefore its second moment estimation is a more favorable option for learning rate scaling than that of the raw gradient. Thereby we propose \\textsc{AdaMomentum} as a new optimizer reaching the goal of training fast while generalizing much better. We further develop a theory to back up the improvement in generalization and provide convergence guarantees under both convex and nonconvex settings. Extensive experiments on a wide range of tasks and models demonstrate that \\textsc{AdaMomentum} exhibits state-of-the-art performance and superior training stability consistently.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116758319",
                        "name": "Yizhou Wang"
                    },
                    {
                        "authorId": "2110042894",
                        "name": "Y. Kang"
                    },
                    {
                        "authorId": "12282768",
                        "name": "Can Qin"
                    },
                    {
                        "authorId": "2197900626",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": null,
                        "name": "Yi Xu"
                    },
                    {
                        "authorId": "2129519081",
                        "name": "Yulun Zhang"
                    },
                    {
                        "authorId": "46956675",
                        "name": "Y. Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "At the same time, some adaptive gradient methods [24], [25], [28] have been presented to improve the generalization performance of Adam algorithm.",
                "[25] J. Zhuang, T. Tang, Y. Ding, S. C. Tatikonda, N. Dvornek, X. Papademetris, and J. Duncan, \u201cAdabelief optimizer: Adapting stepsizes by the belief in observed gradients,\u201d Advances in neural information processing systems, vol. 33, pp. 18 795\u201318 806, 2020.",
                "In fact, we can also defined some other adaptive matrices At and Bt, as Adabelief [25] :\nat = %at\u22121 + (1\u2212 %)(\u2207xf(xt, yt; \u03bet)\u2212 wt)2, a0 = 0, At = diag (\u221a at + \u03c1 ) , t \u2265 1 (5) bt = %bt\u22121 + (1\u2212 %)\u2016\u2207yg(xt, yt; \u03b6t)\u2212 vt\u2016, b0 > 0, Bt = (bt + \u03c1)Ip, t \u2265 1, (6)\nwhere % \u2208 (0, 1) and \u03c1 > 0.",
                "Recently, thus many adaptive gradient methods [22], [23], [24], [25] have been developed and studied.",
                "In fact, we can also defined some other adaptive matrices At and Bt, as Adabelief [25] : at = %at\u22121 + (1\u2212 %)(\u2207xf(xt, yt; \u03bet)\u2212 wt)(2), a0 = 0, At = diag (\u221a at + \u03c1 ) , t \u2265 1 (5) bt = %bt\u22121 + (1\u2212 %)\u2016\u2207yg(xt, yt; \u03b6t)\u2212 vt\u2016, b0 > 0, Bt = (bt + \u03c1)Ip, t \u2265 1, (6) where % \u2208 (0, 1) and \u03c1 > 0."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b89ecbec9133c48ef4cbca23c422bc40086427a0",
                "externalIds": {
                    "ArXiv": "2106.11396",
                    "DBLP": "journals/corr/abs-2106-11396",
                    "CorpusId": 235592978
                },
                "corpusId": 235592978,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b89ecbec9133c48ef4cbca23c422bc40086427a0",
                "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods",
                "abstract": "Bilevel optimization recently has attracted increased interest in machine learning due to its many applications such as hyper-parameter optimization and meta learning. Although many bilevel methods recently have been proposed, these methods do not consider using adaptive learning rates. It is well known that adaptive learning rates can accelerate optimization algorithms. To fill this gap, in the paper, we propose a novel fast adaptive bilevel framework to solve stochastic bilevel optimization problems that the outer problem is possibly nonconvex and the inner problem is strongly convex. Our framework uses unified adaptive matrices including many types of adaptive learning rates, and can flexibly use the momentum and variance reduced techniques. In particular, we provide a useful convergence analysis framework for the bilevel optimization. Specifically, we propose a fast single-loop adaptive bilevel optimization (BiAdam) algorithm, which achieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary solution. Meanwhile, we propose an accelerated version of BiAdam algorithm (VR-BiAdam), which reaches the best known sample complexity of $\\tilde{O}(\\epsilon^{-3})$. To the best of our knowledge, we first study the adaptive bilevel optimization methods with adaptive learning rates. Experimental results on data hyper-cleaning and hyper-representation learning tasks demonstrate the efficiency of our algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3057688",
                        "name": "Feihu Huang"
                    },
                    {
                        "authorId": "2108933511",
                        "name": "Junyi Li"
                    },
                    {
                        "authorId": "145114933",
                        "name": "Heng Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For either problem, this network is trained for 200 epochs with the AdaBelief [61] optimizer with batch size 256; the learning rate is initially set to 10\u22123 and discounted at the 150th epoch by a factor of 10.",
                "For either problem,\nthis network is trained for 200 epochs with the AdaBelief [61] optimizer with batch size 256; the learning rate is initially set to 10\u22123 and discounted at the 150th epoch by a factor of 10."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5626e4474e53a39a6adfe24e17bb5038f7299daf",
                "externalIds": {
                    "ArXiv": "2106.08638",
                    "CorpusId": 263794401
                },
                "corpusId": 263794401,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5626e4474e53a39a6adfe24e17bb5038f7299daf",
                "title": "Deep neural networks for high harmonic spectroscopy in solids",
                "abstract": "Neural networks are a prominent tool for identifying and modeling complex patterns, which are otherwise hard to detect and analyze. While machine learning and neural networks have been finding applications across many areas of science and technology, their use in decoding ultrafast dynamics of quantum systems driven by strong laser fields has been limited so far. Here we use deep neural networks to analyze simulated noisy spectra of highly nonlinear optical response of a 2-dimensional gapped graphene crystal to intense few-cycle laser pulses. We show that a computationally simple 1-dimensional system provides a useful\"nursery school\"for our neural network, allowing it to be easily retrained to treat more complex systems, recovering the band structure and spectral phases of the incident few-cycle pulse with high accuracy, in spite of significant amplitude noise and phase jitter. Our results both offer a new tool for attosecond spectroscopy of quantum dynamics in solids and also open a route to developing all-solid-state devices for complete characterization of few-cycle pulses, including their nonlinear chirp and the carrier envelope phase.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "102617464",
                        "name": "N. Klimkin"
                    },
                    {
                        "authorId": "1388345850",
                        "name": "'Alvaro Jim'enez-Gal'an"
                    },
                    {
                        "authorId": "2252994417",
                        "name": "Rui E. F. Silva"
                    },
                    {
                        "authorId": "2256719260",
                        "name": "Misha Ivanov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief [29] adopts stepsize according to the \u2018belief\u2019 in the current gradient direction, defined as mt = \u03b11mt\u22121 + (1\u2212 \u03b11)\u2207f(xt; \u03bet), vt = \u03b12vt\u22121 + (1\u2212 \u03b12)(\u2207f(xt; \u03bet)\u2212mt)(2) + \u03b5",
                "Assumption 5 is widely used in the adaptive algorithms [26, 4, 29].",
                "For example, Zaheer et al.[26] and Zhuang et al.[29] used the following iteration form to update the variable x: xt+1 = xt \u2212 \u03b7t mt\u221avt+\u03b5 for all t \u2265 0 and \u03b5 > 0, which is equivalent to xt+1 = xt \u2212 \u03b7tH \u22121 t mt with Ht = diag( \u221a vt + \u03b5).",
                "In Adam, Amsgrad and AdaBelief algorithms, we set the learning rate as 0.001.",
                "In the experiments, we compare our SUPER-ADAM algorithm against several state-of-the-art adaptive gradient algorithms, including: (1) Adam [14], (2) Amsgrad [19], (3) AdaGrad-Norm [15], (4) Adam [17], (5) STORM [7] and (6) AdaBelief [29].",
                "AdaBelief [29] \u00d5( \u22124) O( \u221a log(T ) T 1/4 ) specific 2, 3, 4 Adam [17] O( \u22123.",
                "In AdaBelief algorithm, we set the learing rate 0.1.",
                "This new adaptive matrix Ht is similar to the adaptive learning rate in [29].",
                "In the experiments, we compare our SUPER-ADAM algorithm against several state-of-the-art adaptive gradient algorithms, including: (1) Adam [14], (2) Amsgrad [19], (3) AdaGrad-Norm [15], (4) Adam+ [17], (5) STORM [7] and (6) AdaBelief [29].",
                "[26] and Zhuang et al.[29] used the following iteration form to update the variable x: xt+1 = xt \u2212 \u03b7t mt vt+\u03b5 for all t \u2265 0 and \u03b5 > 0, which is equivalent to xt+1 = xt \u2212 \u03b7tH \u22121 t mt with Ht = diag( \u221a vt + \u03b5).",
                ", STORM algorithm [7] and Adam-type algorithms [14, 19, 29].",
                "More recently, a variant of Adam algorithm (i.e., AdaBelief) [29] has been presented to reach a good generalization as SGD by adopting the stepsize according to the \u2018belief\u2019 in the current gradient direction.",
                ", AdaBelief) [29] has been presented to reach a good generalization as SGD by adopting the stepsize according to the \u2018belief\u2019 in the current gradient direction.",
                "AdaBelief [29] adopts stepsize according to the \u2018belief\u2019 in the current gradient direction, defined as\nmt = \u03b11mt\u22121 + (1\u2212 \u03b11)\u2207f(xt; \u03bet), vt = \u03b12vt\u22121 + (1\u2212 \u03b12)(\u2207f(xt; \u03bet)\u2212mt)2 + \u03b5\nm\u0302t = mt/(\u03b11) t, v\u0302t = vt/(\u03b12) t, xt+1 = xt \u2212 \u03b7t m\u0302t\u221a v\u0302t + \u03b5 , \u2200 t \u2265 1 (8)\nwhere \u03b11 > 0, \u03b12 > 0, and \u03b7t = \u03b7\u221at with \u03b7 > 0, and \u03b5 > 0."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "94cf73922dc4d5697fa2bb3319f9c87d595076c9",
                "externalIds": {
                    "ArXiv": "2106.08208",
                    "DBLP": "conf/nips/HuangLH21",
                    "CorpusId": 235436027
                },
                "corpusId": 235436027,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/94cf73922dc4d5697fa2bb3319f9c87d595076c9",
                "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients",
                "abstract": "Adaptive gradient methods have shown excellent performances for solving many machine learning problems. Although multiple adaptive gradient methods were recently studied, they mainly focus on either empirical or theoretical aspects and also only work for speci\ufb01c problems by using some speci\ufb01c adaptive learning rates. Thus, it is desired to design a universal framework for practical algorithms of adaptive gradients with theoretical guarantee to solve general problems. To \ufb01ll this gap, we propose a faster and universal framework of adaptive gradients (i.e., SUPER-ADAM) by introducing a universal adaptive matrix that includes most existing adaptive gradient forms. Moreover, our framework can \ufb02exibly integrate the momentum and variance reduced techniques. In particular, our novel framework provides the convergence analysis support for adaptive gradient methods under the nonconvex setting. In theoretical analysis, we prove that our SUPER-ADAM algorithm can achieve the best known gradient (i.e., stochastic \ufb01rst-order oracle (SFO)) complexity of \u02dc O ( (cid:15) \u2212 3 ) for \ufb01nding an (cid:15) -stationary point of nonconvex optimization, which matches the lower bound for stochastic smooth nonconvex optimization. In numerical experiments, we employ various deep learning tasks to validate that our algorithm consistently outperforms the existing adaptive algorithms. Code is available at https://github.com/LIJUNYI95/SuperAdam",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3057688",
                        "name": "Feihu Huang"
                    },
                    {
                        "authorId": "2108933511",
                        "name": "Junyi Li"
                    },
                    {
                        "authorId": "145114933",
                        "name": "Heng Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c7e9ae4ddf2ff12280afc95b6edbe6357a918278",
                "externalIds": {
                    "ArXiv": "2106.07162",
                    "DBLP": "journals/corr/abs-2106-07162",
                    "DOI": "10.1109/IJCNN55064.2022.9892733",
                    "CorpusId": 235422359
                },
                "corpusId": 235422359,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/c7e9ae4ddf2ff12280afc95b6edbe6357a918278",
                "title": "Goal-Aware Neural SAT Solver",
                "abstract": "Modern neural networks obtain information about the problem and calculate the output solely from the input values. We argue that it is not always optimal, and the network's performance can be significantly improved by augmenting it with a query mechanism that allows the network at run time to make several solution trials and get feedback on the loss value on each trial. To demonstrate the capabilities of the query mechanism, we formulate an unsupervised (not depending on labels) loss function for Boolean Satisfiability Problem (SAT) and theoretically show that it allows the network to extract rich information about the problem. We then propose a neural SAT solver with a query mechanism called QuerySAT and show that it outperforms the neural baseline on a wide range of SAT tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150989416",
                        "name": "Emils Ozolins"
                    },
                    {
                        "authorId": "2295280",
                        "name": "K\u0101rlis Freivalds"
                    },
                    {
                        "authorId": "2111872099",
                        "name": "Andis Draguns"
                    },
                    {
                        "authorId": "2111867597",
                        "name": "Eliza Gaile"
                    },
                    {
                        "authorId": "2111867332",
                        "name": "Ronalds Zakovskis"
                    },
                    {
                        "authorId": "3180278",
                        "name": "Sergejs Kozlovics"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3f6a93fe090142ae06584d116b6013a3298ff585",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-05449",
                    "ArXiv": "2106.05449",
                    "CorpusId": 235390857
                },
                "corpusId": 235390857,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3f6a93fe090142ae06584d116b6013a3298ff585",
                "title": "Investigating Alternatives to the Root Mean Square for Adaptive Gradient Methods",
                "abstract": "Adam is an adaptive gradient method that has experienced widespread adoption due to its fast and reliable training performance. Recent approaches have not offered significant improvement over Adam, often because they do not innovate upon one of its core features: normalization by the root mean square (RMS) of recent gradients. However, as noted by Kingma and Ba (2015), any number of $L^p$ normalizations are possible, with the RMS corresponding to the specific case of $p=2$. In our work, we theoretically and empirically characterize the influence of different $L^p$ norms on adaptive gradient methods for the first time. We show mathematically how the choice of $p$ influences the size of the steps taken, while leaving other desirable properties unaffected. We evaluate Adam with various $L^p$ norms on a suite of deep learning benchmarks, and find that $p>2$ consistently leads to improved learning speed and final performance. The choices of $p=3$ or $p=6$ also match or outperform state-of-the-art methods in all of our experiments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35217291",
                        "name": "Brett Daley"
                    },
                    {
                        "authorId": "34903901",
                        "name": "Chris Amato"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d72e2373c24b74f63a2c0d2a3f2fc494c82ba1a5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-04636",
                    "ArXiv": "2106.04636",
                    "CorpusId": 235376750
                },
                "corpusId": 235376750,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d72e2373c24b74f63a2c0d2a3f2fc494c82ba1a5",
                "title": "Automatically Differentiable Random Coefficient Logistic Demand Estimation",
                "abstract": "We show how the random coefficient logistic demand (BLP) model can be phrased as an automatically differentiable moment function, including the incorporation of numerical safeguards proposed in the literature. This allows gradient-based frequentist and quasi-Bayesian estimation using the Continuously Updating Estimator (CUE). Drawing from the machine learning literature, we outline hitherto under-utilized best practices in both frequentist and Bayesian estimation techniques. Our Monte Carlo experiments compare the performance of CUE, 2S-GMM, and LTE estimation. Preliminary findings indicate that the CUE estimated using LTE and frequentist optimization has a lower bias but higher MAE compared to the traditional 2-Stage GMM (2S-GMM) approach. We also find that using credible intervals from MCMC sampling for the non-linear parameters together with frequentist analytical standard errors for the concentrated out linear parameters provides empirical coverage closest to the nominal level. The accompanying admest Python package provides a platform for replication and extensibility.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "118187689",
                        "name": "Andrew Chia"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our model is optimized with adaBelief (Zhuang et al., 2020) with a learning rate 1e \u2212 5."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bf07a4087e53738a0ed5aa263e4a5ff730ae74e1",
                "externalIds": {
                    "ArXiv": "2106.05221",
                    "ACL": "2021.acl-long.513",
                    "DBLP": "conf/acl/JiangCLHZ20",
                    "DOI": "10.18653/v1/2021.acl-long.513",
                    "CorpusId": 235376991
                },
                "corpusId": 235376991,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/bf07a4087e53738a0ed5aa263e4a5ff730ae74e1",
                "title": "Multi-hop Graph Convolutional Network with High-order Chebyshev Approximation for Text Reasoning",
                "abstract": "Graph convolutional network (GCN) has become popular in various natural language processing (NLP) tasks with its superiority in long-term and non-consecutive word interactions. However, existing single-hop graph reasoning in GCN may miss some important non-consecutive dependencies. In this study, we define the spectral graph convolutional network with the high-order dynamic Chebyshev approximation (HDGCN), which augments the multi-hop graph reasoning by fusing messages aggregated from direct and long-term dependencies into one convolutional layer. To alleviate the over-smoothing in high-order Chebyshev approximation, a multi-vote-based cross-attention (MVCAttn) with linear computation complexity is also proposed. The empirical results on four transductive and inductive NLP tasks and the ablation study verify the efficacy of the proposed model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "92695867",
                        "name": "Shuoran Jiang"
                    },
                    {
                        "authorId": "144159781",
                        "name": "Qingcai Chen"
                    },
                    {
                        "authorId": "2146075472",
                        "name": "Xin Liu"
                    },
                    {
                        "authorId": "33968873",
                        "name": "Baotian Hu"
                    },
                    {
                        "authorId": "2107937930",
                        "name": "Lisai Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e2fffae0ca979a54d9de97f27778c930d822f550",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-02305",
                    "ArXiv": "2106.02305",
                    "CorpusId": 235352926
                },
                "corpusId": 235352926,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e2fffae0ca979a54d9de97f27778c930d822f550",
                "title": "Local Adaptivity in Federated Learning: Convergence and Consistency",
                "abstract": "The federated learning (FL) framework trains a machine learning model using decentralized data stored at edge client devices by periodically aggregating locally trained models. Popular optimization algorithms of FL use vanilla (stochastic) gradient descent for both local updates at clients and global updates at the aggregating server. Recently, adaptive optimization methods such as AdaGrad have been studied for server updates. However, the effect of using adaptive optimization methods for local updates at clients is not yet understood. We show in both theory and practice that while local adaptive methods can accelerate convergence, they can cause a non-vanishing solution bias, where the final converged solution may be different from the stationary point of the global objective function. We propose correction techniques to overcome this inconsistency and complement the local adaptive methods for FL. Extensive experiments on realistic federated training tasks show that the proposed algorithms can achieve faster convergence and higher test accuracy than the baselines without local adaptivity.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "30880777",
                        "name": "Jianyu Wang"
                    },
                    {
                        "authorId": "2149237996",
                        "name": "Zheng Xu"
                    },
                    {
                        "authorId": "40449749",
                        "name": "Zachary Garrett"
                    },
                    {
                        "authorId": "143676545",
                        "name": "Zachary B. Charles"
                    },
                    {
                        "authorId": "2052793501",
                        "name": "Luyang Liu"
                    },
                    {
                        "authorId": "1490536974",
                        "name": "Gauri Joshi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The ranges for LR, EPS, and WD were selected based on recommendation from (Zhuang et al., 2020).",
                "We tried two optimizers: AdamW (Loshchilov and Hutter, 2017) and AdaBelief (Zhuang et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e5fd86a21c1784e15fabd16ed036b9ceae2b0881",
                "externalIds": {
                    "ACL": "2021.smm4h-1.9",
                    "MAG": "3172688128",
                    "DOI": "10.18653/V1/2021.SMM4H-1.9",
                    "CorpusId": 235097583
                },
                "corpusId": 235097583,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e5fd86a21c1784e15fabd16ed036b9ceae2b0881",
                "title": "BERT Goes Brrr: A Venture Towards the Lesser Error in Classifying Medical Self-Reporters on Twitter",
                "abstract": "This paper describes our team\u2019s submission for the Social Media Mining for Health (SMM4H) 2021 shared task. We participated in three subtasks: Classifying adverse drug effect, COVID-19 self-report, and COVID-19 symptoms. Our system is based on BERT model pre-trained on the domain-specific text. In addition, we perform data cleaning and augmentation, as well as hyperparameter optimization and model ensemble to further boost the BERT performance. We achieved the first rank in both classifying adverse drug effects and COVID-19 self-report tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8129718",
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "authorId": "66436856",
                        "name": "Made Nindyatama Nityasya"
                    },
                    {
                        "authorId": "49918371",
                        "name": "Haryo Akbarianto Wibowo"
                    },
                    {
                        "authorId": "2368148",
                        "name": "Radityo Eko Prasojo"
                    },
                    {
                        "authorId": "36045311",
                        "name": "Tirana Noor Fatyanosa"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The recently proposed AdaBelief [17] is another variation of Adam with a theoretical convergence guarantee."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7ae0ab0913b28d40faef7ef322a2aac5bab1e8c7",
                "externalIds": {
                    "ArXiv": "2106.00092",
                    "DBLP": "journals/corr/abs-2106-00092",
                    "DOI": "10.1109/CDC45484.2021.9682994",
                    "CorpusId": 235266058
                },
                "corpusId": 235266058,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/7ae0ab0913b28d40faef7ef322a2aac5bab1e8c7",
                "title": "Generalized AdaGrad (G-AdaGrad) and Adam: A State-Space Perspective",
                "abstract": "Accelerated gradient-based methods are being extensively used for solving non-convex machine learning problems, especially when the data points are abundant or the available data is distributed across several agents. Two of the prominent accelerated gradient algorithms are AdaGrad and Adam. AdaGrad is the simplest accelerated gradient method, which is particularly effective for sparse data. Adam has been shown to perform favorably in deep learning problems compared to other methods. In this paper, we propose a new fast optimizer, Generalized AdaGrad (G-AdaGrad), for accelerating the solution of potentially non-convex machine learning problems. Specifically, we adopt a state-space perspective for analyzing the convergence of gradient acceleration algorithms, namely G-AdaGrad and Adam, in machine learning. Our proposed state-space models are governed by ordinary differential equations. We present simple convergence proofs of these two algorithms in the deterministic settings with minimal assumptions. Our analysis also provides intuition behind improving upon AdaGrad\u2019s convergence rate. We provide empirical results on MNIST dataset to reinforce our claims on the convergence and performance of G-AdaGrad and Adam.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1768752581",
                        "name": "Kushal Chakrabarti"
                    },
                    {
                        "authorId": "145229889",
                        "name": "N. Chopra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026is global Lipscthiz continuous and bounded [Kingma and Ba, 2015, Xu et al., 2018, Brosse et al., 2018, Duchi et al., 2011, Tieleman and Hinton, 2012, Reddi et al., 2018, Chen et al., 2019, Liu et al., 2020, Luo et al., 2019, Zhuang et al., 2020] although it is not true for neural network problems."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6207ba3cf18a86516d27d5af198c94a72e5be49d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-13937",
                    "ArXiv": "2105.13937",
                    "CorpusId": 235248142
                },
                "corpusId": 235248142,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6207ba3cf18a86516d27d5af198c94a72e5be49d",
                "title": "Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks",
                "abstract": "We present a new class of Langevin based algorithms, which overcomes many of the known shortcomings of popular adaptive optimizers that are currently used for the fine tuning of deep learning models. Its underpinning theory relies on recent advances of Euler's polygonal approximations for stochastic differential equations (SDEs) with monotone coefficients. As a result, it inherits the stability properties of tamed algorithms, while it addresses other known issues, e.g. vanishing gradients in neural networks. In particular, we provide a nonasymptotic analysis and full theoretical guarantees for the convergence properties of an algorithm of this novel class, which we named TH$\\varepsilon$O POULA (or, simply, TheoPouLa). Finally, several experiments are presented with different types of deep learning models, which show the superior performance of TheoPouLa over many popular adaptive optimization algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2069001448",
                        "name": "Dong-Young Lim"
                    },
                    {
                        "authorId": "3433047",
                        "name": "S. Sabanis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "methods, and with the aim of conducting a fair comparison, the best configuration of each of the optimized systems has been taken, following previous works to report the best result in the literature [47], [51], [46].",
                "Particularly, SGD, RMSprop, Adam, AdamW, diffGrad, AdaBelief AngularGradcos and AngularGradtan have been considered.",
                "On the other hand, instead of using the exponential moving average (EMA) of g(2) t , AdaBelief [47] uses the EMA of (gt\u2212mt)(2) as st and the update direction for AdaBelief is mt/ \u221a st.",
                "For instance, in VGG16, the AngularGrad is quite close to SGDM and AdaBelief, whilst the rest of the optimization methods (Adam, AdamW, DiffGrad and RMSprop) are at least two points behind the best OA.",
                "Consequently, configurations of AdaBelief and MSV AG have been provided by Zhuang et al. in [47], SGDM, AdaBound Yogi, and AdamW have been successfully reported by Chen et al. in [51], and RAdam has been studied by Liu et al. in [46].",
                "\u2022 The fourth experiment extends the classification problem, testing the proposed optimizer over the popular classification benchmark of ImageNet [48], drawing a comparison between the proposed AngularGrad and the results obtained by others optimizers such as AdaBelief and MSV AG [47], SGDM, AdaBound, Yogi, Adam, and AdamW [51], and RAdam [46].",
                "In order to visualize the effects using different optimizers on the network architecture, the ResNet50 model is trained using SGDM, RMSProp, Adam, AdamW, diffGrad, AdaBelief, and AngularGradcos, and AngularGradtan over the Mini-Imagenet data set.",
                "These graphics have been obtained from https://github.com/ jettify/pytorch-optimizer with seed = 2\n\u2022 The fourth experiment extends the classification problem, testing the proposed optimizer over the popular classification benchmark of ImageNet [48], drawing a comparison between the proposed AngularGrad and the results obtained by others optimizers such as AdaBelief and MSV AG [47], SGDM, AdaBound, Yogi, Adam, and AdamW [51], and RAdam [46].",
                "However, the loss landscapes of diffGrad, AdaBelief, AngularGradcos and AngularGradtan are more or less uniform in shape.",
                "in [47], SGDM, AdaBound Yogi, and AdamW have been successfully reported by Chen et al.",
                "9 (SGD) (RMSprop) (Adam) (AdamW)\n(diffGrad) (AdaBelief) (AngularGradcos) (AngularGradtan)\nFig.",
                "%) and AdaBelief (which achieves the 70.08% exploiting curvature information) are able to surpass the 70% accuracy, where Adam and MSV AG optimizers provide the lowest accuracies (63.23%-66.54% and 65.99%, respectively).",
                "However, a large number of them (e.g. SGD, SGDW, AdaBelief, Lookahead, etc.) cannot reach the global optima in a stipulated number of iterations as per the multi-optima and single optimum benchmark functions, namely Rosenbrock and Rastrigin2.",
                "Graphical representation of gradient trajectories using different optimization algorithms (in particular, SGD, RMSprop, Adam, AdamW, diffGrad, AdaBelief AngularGradcos and AngularGradtan) by considering the Rosenbrock function.",
                "A similar situation is found in ResNeXt29 and DLA, where the proposed methods are pretty close to the best OA reached by AdaBelief and SGDM, respectively.",
                "\u2020 IS REPORTED IN [51], \u2021 IS REPORTED IN [46], \u00a7 IS REPORTED IN [47]",
                "In this regard, AdamW, AngularGradtan and AngularGradcos have relatively smooth and uniform tra-\n10\njectory, reaching a solution near to the global minimum, while AdaBelief is noisier than other three optimization methods.",
                "To justify our theory, we model the optimization problem as a regression one over three one-dimensional non-convex functions, performing optimization over these functions using SGDM, Adam, diffGrad, AdaBelief, AngularGradcos and AngularGradtan.",
                "Indeed, the proposed AngularGrad and the other optimizers (SGDM, RMSprop, Adam, AdamW, diffGrad and AdaBelief) are evaluated with the same settings as those described in the first experiment.",
                "2(c) clearly shows that Adam and AdaBelief overshoots the global minimum at \u03b8 = \u22120.3 due to the high momentum gained, and finally converges at \u03b8 = 0.2.",
                "On the other hand, instead of using the exponential moving average (EMA) of g2t , AdaBelief [47] uses the EMA of (gt\u2212mt)2 as st and the update direction for AdaBelief is mt/ \u221a st."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "187c02ac75fd893f46eee1cf9dce0858cde8e3b7",
                "externalIds": {
                    "ArXiv": "2105.10190",
                    "CorpusId": 235125608
                },
                "corpusId": 235125608,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/187c02ac75fd893f46eee1cf9dce0858cde8e3b7",
                "title": "AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks",
                "abstract": "Convolutional neural networks (CNNs) are trained using stochastic gradient descent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam) optimizer has become very popular due to its adaptive momentum, which tackles the dying gradient problem of SGD. Nevertheless, existing optimizers are still unable to exploit the optimization curvature information efficiently. This paper proposes a new AngularGrad optimizer that considers the behavior of the direction/angle of consecutive gradients. This is the first attempt in the literature to exploit the gradient angular information apart from its magnitude. The proposed AngularGrad generates a score to control the step size based on the gradient angular information of previous iterations. Thus, the optimization steps become smoother as a more accurate step size of immediate past gradients is captured through the angular information. Two variants of AngularGrad are developed based on the use of Tangent or Cosine functions for computing the gradient angular information. Theoretically, AngularGrad exhibits the same regret bound as Adam for convergence purposes. Nevertheless, extensive experiments conducted on benchmark data sets against state-of-the-art methods reveal a superior performance of AngularGrad. The source code will be made publicly available at: https://github.com/mhaut/AngularGrad.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107286805",
                        "name": "S. K. Roy"
                    },
                    {
                        "authorId": "31715636",
                        "name": "Mercedes Eugenia Paoletti"
                    },
                    {
                        "authorId": "21248079",
                        "name": "J. M. Haut"
                    },
                    {
                        "authorId": "34992579",
                        "name": "S. Dubey"
                    },
                    {
                        "authorId": "39746893",
                        "name": "Purushottam Kar"
                    },
                    {
                        "authorId": "41222008",
                        "name": "A. Plaza"
                    },
                    {
                        "authorId": "1759420",
                        "name": "B. Chaudhuri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For training all models in this section, an Adabelief optimizer has been used [45]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "878463976b3863f5d4fc5df38cbe8dce98277a43",
                "externalIds": {
                    "ArXiv": "2105.06421",
                    "DOI": "10.2139/ssrn.4395579",
                    "CorpusId": 251402296
                },
                "corpusId": 251402296,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/878463976b3863f5d4fc5df38cbe8dce98277a43",
                "title": "Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation",
                "abstract": "In this paper, at first, the impact of ImageNet pre-training on fine-grained Facial Emotion Recognition (FER) is investigated which shows that when enough augmentations on images are applied, training from scratch provides better result than fine-tuning on ImageNet pre-training. Next, we propose a method to improve fine-grained and in-the-wild FER, called Hybrid Multi-Task Learning (HMTL). HMTL uses Self-Supervised Learning (SSL) as an auxiliary task during classical Supervised Learning (SL) in the form of Multi-Task Learning (MTL). Leveraging SSL during training can gain additional information from images for the primary fine-grained SL task. We investigate how proposed HMTL can be used in the FER domain by designing two customized version of common pre-text task techniques, puzzling and in-painting. We achieve state-of-the-art results on the AffectNet benchmark via two types of HMTL, without utilizing pre-training on additional data. Experimental results on the common SSL pre-training and proposed HMTL demonstrate the difference and superiority of our work. However, HMTL is not only limited to FER domain. Experiments on two types of fine-grained facial tasks, i.e., head pose estimation and gender recognition, reveals the potential of using HMTL to improve fine-grained facial representation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2091909928",
                        "name": "Mahdi Pourmirzaei"
                    },
                    {
                        "authorId": "1801348",
                        "name": "G. Montazer"
                    },
                    {
                        "authorId": "2091913488",
                        "name": "Farzaneh Esmaili"
                    }
                ]
            }
        },
        {
            "contexts": [
                "and Adabelief may potentially generate improvements in learning accuracy compared to the Adam optimiser (Keskar & Socher, 2017; Zhuang et al., 2020).",
                "Researchers suggest that Stochastic Gradient (SGD) and Adabelief may potentially generate improvements in learning accuracy compared to the Adam optimiser (Keskar & Socher, 2017; Zhuang et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "dcd574ad017f1d050ee0380aad1d2db3b4e1750b",
                "externalIds": {
                    "MAG": "3162884997",
                    "DOI": "10.1080/17543266.2021.1925355",
                    "CorpusId": 236595129
                },
                "corpusId": 236595129,
                "publicationVenue": {
                    "id": "ffea35a4-2dd3-47a2-a045-bb04dcf9af26",
                    "name": "International Journal of Fashion Design, Technology and Education",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Fash Des Technol Educ"
                    ],
                    "issn": "1754-3274",
                    "url": "http://www.informaworld.com/openurl?genre=journal&issn=1754-3266",
                    "alternate_urls": [
                        "http://www.tandfonline.com/toc/tfdt20/current",
                        "http://www.tandfonline.com/loi/tfdt20"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dcd574ad017f1d050ee0380aad1d2db3b4e1750b",
                "title": "Automatic defect detection for fabric printing using a deep convolutional neural network",
                "abstract": "ABSTRACT Defect detection is a crucial step in textile and apparel quality control. An efficient defect detection system can ensure the overall quality of the processes and products that are acceptable to consumers. Existing techniques for real-time defect detection tend to vary according to unique manufacturing processes, focal defects and computational algorithms. Although the need is high, research related to automatic printed fabric defect detection processes is not prevalent in academic literatures. This research proposes a novel methodology that demonstrates the application of convolutional neural network (CNN) to classify printing defects based on the fabric images collected from industries. The research also integrated visual geometric group (VGG), DenseNet, Inception and Xception deep learning networks to compare model performance. The results exhibit that the VGG-based models perform better compared to a simple CNN model, suggesting promise for automatic defect detection (ADD) of printed fabrics that can improve profitability in fashion supply chains.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153395610",
                        "name": "Samit Chakraborty"
                    },
                    {
                        "authorId": "33516977",
                        "name": "Marguerite Moore"
                    },
                    {
                        "authorId": "1441433418",
                        "name": "Lisa Parrillo-Chapman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "L(u0; \u03b8, \u03c6) = 30\u2211 i=1 [ (\u00b5i \u2212 \u00b5\u0302i)2 + (\u03c32i \u2212 \u03c3\u03022i )2 ] + \u03bbrRE\n(17)\nThe models were trained using AdaBelief Optimizer (Zhuang et al., 2020) with a learning rate of 0.01 for 250 iterations.",
                "The models were trained using AdaBelief Optimizer (Zhuang et al., 2020) with a learning rate of 0."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "97f378bdb4d9d66cf0c2351dde0737f869a8cbda",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-03918",
                    "ArXiv": "2105.03918",
                    "CorpusId": 234337701
                },
                "corpusId": 234337701,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/97f378bdb4d9d66cf0c2351dde0737f869a8cbda",
                "title": "Opening the Blackbox: Accelerating Neural Differential Equations by Regularizing Internal Solver Heuristics",
                "abstract": "Democratization of machine learning requires architectures that automatically adapt to new problems. Neural Differential Equations (NDEs) have emerged as a popular modeling framework by removing the need for ML practitioners to choose the number of layers in a recurrent model. While we can control the computational cost by choosing the number of layers in standard architectures, in NDEs the number of neural network evaluations for a forward pass can depend on the number of steps of the adaptive ODE solver. But, can we force the NDE to learn the version with the least steps while not increasing the training cost? Current strategies to overcome slow prediction require high order automatic differentiation, leading to significantly higher training time. We describe a novel regularization method that uses the internal cost heuristics of adaptive differential equation solvers combined with discrete adjoint sensitivities to guide the training process towards learning NDEs that are easier to solve. This approach opens up the blackbox numerical analysis behind the differential equation solver\u2019s algorithm and directly uses its local error estimates and stiffness heuristics as cheap and accurate cost estimates. We incorporate our method without any change in the underlying NDE framework and show that our method extends beyond Ordinary Differential Equations to accommodate Neural Stochastic Differential Equations. We demonstrate how our approach can halve the prediction time and, unlike other methods which can increase the training time by an order of magnitude, we demonstrate similar reduction in training times. Together this showcases how the knowledge embedded within Indian Institute of Technology Kanpur Julia Computing Massachusetts Institute of Technology Pumas AI University of Maryland Baltimore. Correspondence to: Avik Pal <avikpal@cse.iitk.ac.in>, Christopher Rackauckas <crackauc@mit.edu>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). Figure 1. Training and Prediction Performance of Regularized NDEs We obtain an average training and prediction speedup of 1.45x and 1.84x respectively for our best model on supervised classification and time series problems. state-of-the-art equation solvers can be used to enhance machine learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9308965",
                        "name": "Avik Pal"
                    },
                    {
                        "authorId": "20859037",
                        "name": "Yingbo Ma"
                    },
                    {
                        "authorId": "2543935",
                        "name": "Viral B. Shah"
                    },
                    {
                        "authorId": "150956309",
                        "name": "Chris Rackauckas"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2eb9afe904c14b24ffa2b35a6377a19cd57e5c6f",
                "externalIds": {
                    "DBLP": "conf/sp/Woods21",
                    "ArXiv": "2105.13114",
                    "DOI": "10.1109/SPW53761.2021.00031",
                    "CorpusId": 233747269
                },
                "corpusId": 233747269,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2eb9afe904c14b24ffa2b35a6377a19cd57e5c6f",
                "title": "RL-GRIT: Reinforcement Learning for Grammar Inference",
                "abstract": "When working to understand usage of a data format, examples of the data format are often more representative than the format\u2019s specification. For example, two different applications might use very different JSON representations, or two PDF-writing applications might make use of very different areas of the PDF specification to realize the same rendered content. The complexity arising from these distinct origins can lead to large, difficult-to-understand attack surfaces, presenting a security concern when considering both exfiltration and data schizophrenia. Grammar inference can aid in describing the practical language generator behind examples of a data format. However, most grammar inference research focuses on natural language, not data formats, and fails to support crucial features such as type recursion. We propose a novel set of mechanisms for grammar inference, RL-GRIT1, and apply them to understanding de facto data formats. After reviewing existing grammar inference solutions, it was determined that a new, more flexible scaffold could be found in Reinforcement Learning (RL). Within this work, we lay out the many algorithmic changes required to adapt RL from its traditional, sequential-time environment to the highly interdependent environment of parsing. The result is an algorithm which can demonstrably learn recursive control structures in simple data formats, and can extract meaningful structure from fragments of the PDF format. Whereas prior work in grammar inference focused on either regular languages or constituency parsing, we show that RL can be used to surpass the expressiveness of both classes, and offers a clear path to learning context-sensitive languages. The proposed algorithm can serve as a building block for understanding the ecosystems of de facto data formats.1RL-GRIT may be pronounced as \u201cReal Grit.\u201d",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "86941640",
                        "name": "Walt Woods"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "dcebcb5680be1989de2b54fa155f1c12dadfc241",
                "externalIds": {
                    "DBLP": "journals/jei/ShaoL21",
                    "DOI": "10.1117/1.JEI.30.3.033021",
                    "CorpusId": 235600167
                },
                "corpusId": 235600167,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dcebcb5680be1989de2b54fa155f1c12dadfc241",
                "title": "TAMNet: two attention modules-based network on facial expression recognition under uncertainty",
                "abstract": "Abstract. Facial expression recognition (FER) has applications in many scenarios, making it a valuable research direction. However, due to the uncertainty of real-world images, their recognition accuracy is not satisfying. To deal with this problem, we propose a two attention modules-based network that uses two different attention modules to extract features. The main classifier uses the self-attention module (SAM), and the auxiliary classifier uses the channel enhancement module. At the same time, the classification results of the two classifiers are constrained by variance regularization to suppress uncertainty. In addition, we use knowledge distillation to further optimize the predicted results using the mutual information between the teacher network and the student network. Our model achieves an optimal accuracy rate of 88.75% in the RAF-DB dataset and 80.59% in the FERPlus dataset, which is occluded with the lower half being visible.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111877513",
                        "name": "Jie Shao"
                    },
                    {
                        "authorId": "2112602044",
                        "name": "Yan Luo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "31a5fbf5bac3737820b5e9900c4eae6dfb1f7b5c",
                "externalIds": {
                    "ArXiv": "2104.13790",
                    "DBLP": "journals/tnn/ZhouHCWHL23",
                    "DOI": "10.1109/TNNLS.2022.3143554",
                    "CorpusId": 235368156,
                    "PubMed": "35271450"
                },
                "corpusId": 235368156,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31a5fbf5bac3737820b5e9900c4eae6dfb1f7b5c",
                "title": "FastAdaBelief: Improving Convergence Rate for Belief-Based Adaptive Optimizers by Exploiting Strong Convexity",
                "abstract": "AdaBelief, one of the current best optimizers, demonstrates superior generalization ability over the popular Adam algorithm by viewing the exponential moving average of observed gradients. AdaBelief is theoretically appealing in which it has a data-dependent <inline-formula> <tex-math notation=\"LaTeX\">$O(\\sqrt {T})$ </tex-math></inline-formula> regret bound when objective functions are convex, where <inline-formula> <tex-math notation=\"LaTeX\">$T$ </tex-math></inline-formula> is a time horizon. It remains, however, an open problem whether the convergence rate can be further improved without sacrificing its generalization ability. To this end, we make the first attempt in this work and design a novel optimization algorithm called FastAdaBelief that aims to exploit its strong convexity in order to achieve an even faster convergence rate. In particular, by adjusting the step size that better considers strong convexity and prevents fluctuation, our proposed FastAdaBelief demonstrates excellent generalization ability and superior convergence. As an important theoretical contribution, we prove that FastAdaBelief attains a data-dependent <inline-formula> <tex-math notation=\"LaTeX\">$O(\\log T)$ </tex-math></inline-formula> regret bound, which is substantially lower than AdaBelief in strongly convex cases. On the empirical side, we validate our theoretical analysis with extensive experiments in scenarios of strong convexity and nonconvexity using three popular baseline models. Experimental results are very encouraging: FastAdaBelief converges the quickest in comparison to all mainstream algorithms while maintaining an excellent generalization ability, in cases of both strong convexity or nonconvexity. FastAdaBelief is, thus, posited as a new benchmark model for the research community.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145498691",
                        "name": "Yangfan Zhou"
                    },
                    {
                        "authorId": "5380819",
                        "name": "Kaizhu Huang"
                    },
                    {
                        "authorId": "2116394152",
                        "name": "Cheng Cheng"
                    },
                    {
                        "authorId": "2108084717",
                        "name": "Xuguang Wang"
                    },
                    {
                        "authorId": "144664815",
                        "name": "A. Hussain"
                    },
                    {
                        "authorId": "2120101540",
                        "name": "Xin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "75be78bda395c7a5b898b4a0d644c25fb4124f93",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-13790",
                    "MAG": "3158712583",
                    "CorpusId": 233423331
                },
                "corpusId": 233423331,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/75be78bda395c7a5b898b4a0d644c25fb4124f93",
                "title": "FastAdaBelief: Improving Convergence Rate for Belief-based Adaptive Optimizer by Strong Convexity",
                "abstract": "The AdaBelief algorithm demonstrates superior generalization ability to the Adam algorithm by viewing the exponential moving average of observed gradients. AdaBelief is proved to have a data-dependent $O(\\sqrt{T})$ regret bound when objective functions are convex, where $T$ is a time horizon. However, it remains to be an open problem on how to exploit strong convexity to further improve the convergence rate of AdaBelief. To tackle this problem, we present a novel optimization algorithm under strong convexity, called FastAdaBelief. We prove that FastAdaBelief attains a data-dependant $O(\\log T)$ regret bound, which is substantially lower than AdaBelief. In addition, the theoretical analysis is validated by extensive experiments performed on open datasets (i.e., CIFAR-10 and Penn Treebank) for image classification and language modeling.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145498691",
                        "name": "Yangfan Zhou"
                    },
                    {
                        "authorId": "5380819",
                        "name": "Kaizhu Huang"
                    },
                    {
                        "authorId": "2116394152",
                        "name": "Cheng Cheng"
                    },
                    {
                        "authorId": "2108084717",
                        "name": "Xuguang Wang"
                    },
                    {
                        "authorId": "2120101540",
                        "name": "Xin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use AdaBelief (Zhuang et al. 2020) in combination with look-ahead optimizer (Zhang et al.",
                "We use AdaBelief (Zhuang et al. 2020) in combination with look-ahead optimizer (Zhang et al. 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4769caa0e3a3bde5ac74b6c3edcec7972020f03a",
                "externalIds": {
                    "ArXiv": "2104.08665",
                    "CorpusId": 237581664
                },
                "corpusId": 237581664,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4769caa0e3a3bde5ac74b6c3edcec7972020f03a",
                "title": "Higher Order Recurrent Space-Time Transformer for Video Action Prediction",
                "abstract": "Endowing visual agents with predictive capability is a key step towards video intelligence at scale. The predominant modeling paradigm for this is sequence learning, mostly implemented through LSTMs. Feed-forward Transformer architectures have replaced recurrent model designs in ML applications of language processing and also partly in computer vision. In this paper we investigate on the competitiveness of Transformer-style architectures for video predictive tasks. To do so we propose HORST, a novel higher order recurrent layer design whose core element is a spatial-temporal decomposition of self-attention for video. HORST achieves state of the art competitive performance on Something-Something early action recognition and EPIC-Kitchens action anticipation, showing evidence of predictive capability that we attribute to our recurrent higher order design of self-attention.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "27084141",
                        "name": "Tsung-Ming Tai"
                    },
                    {
                        "authorId": "3144258",
                        "name": "G. Fiameni"
                    },
                    {
                        "authorId": "2115293259",
                        "name": "Cheng-Kuang Lee"
                    },
                    {
                        "authorId": "1717522",
                        "name": "O. Lanz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Models were optimized using the AdaBelief optimizer (Zhuang et al., 2020), with a learning rate of 4 \u00b7 10\u22125 and batch size of 24."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "18ad12c8ca8297fa517fd7322efdbb2def31f8da",
                "externalIds": {
                    "ArXiv": "2104.06645",
                    "DBLP": "journals/corr/abs-2104-06645",
                    "CorpusId": 233231551
                },
                "corpusId": 233231551,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/18ad12c8ca8297fa517fd7322efdbb2def31f8da",
                "title": "Jointly Learning Truth-Conditional Denotations and Groundings using Parallel Attention",
                "abstract": "We present a model that jointly learns the denotations of words together with their groundings using a truth-conditional semantics. Our model builds on the neurosymbolic approach of Mao et al. (2019), learning to ground objects in the CLEVR dataset (Johnson et al., 2017) using a novel parallel attention mechanism. The model achieves state of the art performance on visual question answering, learning to detect and ground objects with question performance as the only training signal. We also show that the model is able to learn flexible non-canonical groundings just by adjusting answers to questions in the training set.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2819425",
                        "name": "Leon Bergen"
                    },
                    {
                        "authorId": "3335364",
                        "name": "Dzmitry Bahdanau"
                    },
                    {
                        "authorId": "1400877758",
                        "name": "T. O\u2019Donnell"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The network is trained with the AdaBelief optimizer [39] for 750 epochs with a learning rate of 0."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "351ecac93c8833e632a18b6cb3d5c8effdb2b3a6",
                "externalIds": {
                    "PubMedCentral": "8489024",
                    "DOI": "10.1523/ENEURO.0122-21.2021",
                    "CorpusId": 232224579,
                    "PubMed": "34518364"
                },
                "corpusId": 232224579,
                "publicationVenue": {
                    "id": "6c38e807-0b8c-433f-919b-9fa30f6fe6a5",
                    "name": "eNeuro",
                    "issn": "2373-2822",
                    "url": "https://epub.uni-regensburg.de/40454/",
                    "alternate_urls": [
                        "http://eneuro.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/351ecac93c8833e632a18b6cb3d5c8effdb2b3a6",
                "title": "MEYE: Web App for Translational and Real-Time Pupillometry",
                "abstract": "Abstract Pupil dynamics alterations have been found in patients affected by a variety of neuropsychiatric conditions, including autism. Studies in mouse models have used pupillometry for phenotypic assessment and as a proxy for arousal. Both in mice and humans, pupillometry is noninvasive and allows for longitudinal experiments supporting temporal specificity; however, its measure requires dedicated setups. Here, we introduce a convolutional neural network that performs online pupillometry in both mice and humans in a web app format. This solution dramatically simplifies the usage of the tool for the nonspecialist and nontechnical operators. Because a modern web browser is the only software requirement, this choice is of great interest given its easy deployment and setup time reduction. The tested model performances indicate that the tool is sensitive enough to detect both locomotor-induced and stimulus-evoked pupillary changes, and its output is comparable to state-of-the-art commercial devices.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49034768",
                        "name": "Raffaele Mazziotti"
                    },
                    {
                        "authorId": "1996072",
                        "name": "F. Carrara"
                    },
                    {
                        "authorId": "153727291",
                        "name": "A. Viglione"
                    },
                    {
                        "authorId": "117651232",
                        "name": "L. Lupori"
                    },
                    {
                        "authorId": "8845616",
                        "name": "L. L. Verde"
                    },
                    {
                        "authorId": "2056902192",
                        "name": "A. Benedetto"
                    },
                    {
                        "authorId": "2061405339",
                        "name": "Giulia Ricci"
                    },
                    {
                        "authorId": "114118762",
                        "name": "G. Sagona"
                    },
                    {
                        "authorId": "144514869",
                        "name": "G. Amato"
                    },
                    {
                        "authorId": "145110694",
                        "name": "T. Pizzorusso"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a102c5a5d13e8524406a2a87ad640a9c086117cb",
                "externalIds": {
                    "MAG": "3139181973",
                    "DOI": "10.1007/s40435-021-00785-5",
                    "CorpusId": 233709013
                },
                "corpusId": 233709013,
                "publicationVenue": {
                    "id": "03c42de9-e1fc-4c76-8e63-5813d7f59424",
                    "name": "International Journal of Dynamics and Control",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Dyn Control"
                    ],
                    "issn": "2195-268X",
                    "url": "https://www.springer.com/engineering/mechanics/journal/40435",
                    "alternate_urls": [
                        "https://link.springer.com/journal/40435"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a102c5a5d13e8524406a2a87ad640a9c086117cb",
                "title": "An AB-CNN intelligent fault location recognition model for induction motor",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3071688",
                        "name": "Lingzhi Yi"
                    },
                    {
                        "authorId": "2112315163",
                        "name": "Xiu Xu"
                    },
                    {
                        "authorId": "2116671882",
                        "name": "Wenxin Yu"
                    },
                    {
                        "authorId": "2203376828",
                        "name": "Xuanjian Xu"
                    },
                    {
                        "authorId": "2113205332",
                        "name": "Tao Sun"
                    },
                    {
                        "authorId": "2089766434",
                        "name": "Ganlin Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Regarding the utilization of flatness and concavity of DL loss functions, the AdaBelief algorithm of [47] is worth mentioning."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "55f5f6de1c8d9751c57c0f1e9b15d1d7f61f470b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-03570",
                    "ArXiv": "2103.03570",
                    "DOI": "10.1007/s11063-021-10705-5",
                    "CorpusId": 232135085
                },
                "corpusId": 232135085,
                "publicationVenue": {
                    "id": "03101d6e-e317-48fe-ab55-f82ed4f0727f",
                    "name": "Neural Processing Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Process Lett"
                    ],
                    "issn": "1370-4621",
                    "url": "https://link.springer.com/journal/11063"
                },
                "url": "https://www.semanticscholar.org/paper/55f5f6de1c8d9751c57c0f1e9b15d1d7f61f470b",
                "title": "Second-Order Step-Size Tuning of SGD for Non-Convex Optimization",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "133745714",
                        "name": "Camille Castera"
                    },
                    {
                        "authorId": "24369169",
                        "name": "J. Bolte"
                    },
                    {
                        "authorId": "51007162",
                        "name": "C'edric F'evotte"
                    },
                    {
                        "authorId": "144797102",
                        "name": "Edouard Pauwels"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To train the network, we used AdaBelief [23] as optimizer.",
                "To train the network, we used AdaBelief [28] as optimizer."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "223d4b0bb6435bc39fd9c25bce5a1e7aa7e8fc55",
                "externalIds": {
                    "DOI": "10.1002/cyto.a.24467",
                    "CorpusId": 232081668,
                    "PubMed": "34089228"
                },
                "corpusId": 232081668,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/223d4b0bb6435bc39fd9c25bce5a1e7aa7e8fc55",
                "title": "Machine learning for cell classification and neighborhood analysis in glioma tissue",
                "abstract": "Multiplexed and spatially resolved single-cell analyses that intend to study tissue heterogeneity and cell organization invariably face as a first step the challenge of cell classification. Accuracy and reproducibility are important for the down-stream process of counting cells, quantifying cell-cell interactions, and extracting information on disease-specific localized cell niches. Novel staining techniques make it possible to visualize and quantify large numbers of cell-specific molecular markers in parallel. However, due to variations in sample handling and artefacts from staining and scanning, cells of the same type may present different marker profiles both within and across samples. We address multiplexed immunofluorescence data from tissue microarrays of low grade gliomas and present a methodology using two different machine learning architectures and features insensitive to illumination to perform cell classification. The fully automated cell classification provides a measure of confidence for the decision and requires a comparably small annotated dataset for training, which can be created using freely available tools. Using the proposed method, we reached an accuracy of 83.1% on cell classification without the need for standardization of samples. Using our confidence measure, cells with low-confidence classifications could be excluded, pushing the classification accuracy to 94.5%. Next, we used the cell classification results to search for cell niches with an unsupervised learning approach based on graph neural networks. We show that the approach can re-detect specialized tissue niches in previously published data, and that our proposed cell classification leads to niche definitions that may be relevant for sub-groups of glioma, if applied to larger datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40519546",
                        "name": "L. Solorzano"
                    },
                    {
                        "authorId": "2051741812",
                        "name": "Lina Wik"
                    },
                    {
                        "authorId": "4173786",
                        "name": "T. O. Bontell"
                    },
                    {
                        "authorId": "2145199363",
                        "name": "Yuyu Wang"
                    },
                    {
                        "authorId": "5126805",
                        "name": "Anna H. Klemm"
                    },
                    {
                        "authorId": "10691789",
                        "name": "Johan \u00d6fverstedt"
                    },
                    {
                        "authorId": "1725097",
                        "name": "A. Jakola"
                    },
                    {
                        "authorId": "3919780",
                        "name": "A. \u00d6stman"
                    },
                    {
                        "authorId": "1705606",
                        "name": "Carolina W\u00e4hlby"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "935a34fe8cf16a2296942d49b9ab43d0dab5e687",
                "externalIds": {
                    "ArXiv": "2102.10343",
                    "CorpusId": 246904889
                },
                "corpusId": 246904889,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/935a34fe8cf16a2296942d49b9ab43d0dab5e687",
                "title": "Measuring the Transferability of $\\ell_\\infty$ Attacks by the $\\ell_2$ Norm",
                "abstract": "Deep neural networks could be fooled by adversarial examples with trivial differences to original samples. To keep the difference imperceptible in human eyes, researchers bound the adversarial perturbations by the $\\ell_\\infty$ norm, which is now commonly served as the standard to align the strength of different attacks for a fair comparison. However, we propose that using the $\\ell_\\infty$ norm alone is not sufficient in measuring the attack strength, because even with a fixed $\\ell_\\infty$ distance, the $\\ell_2$ distance also greatly affects the attack transferability between models. Through the discovery, we reach more in-depth understandings towards the attack mechanism, i.e., several existing methods attack black-box models better partly because they craft perturbations with 70% to 130% larger $\\ell_2$ distances. Since larger perturbations naturally lead to better transferability, we thereby advocate that the strength of attacks should be simultaneously measured by both the $\\ell_\\infty$ and $\\ell_2$ norm. Our proposal is firmly supported by extensive experiments on ImageNet dataset from 7 attacks, 4 white-box models, and 9 black-box models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111636895",
                        "name": "Sizhe Chen"
                    },
                    {
                        "authorId": "2035647651",
                        "name": "Qinghua Tao"
                    },
                    {
                        "authorId": "17064832",
                        "name": "Zhixing Ye"
                    },
                    {
                        "authorId": "47932717",
                        "name": "Xiaolin Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For MSA, we use the AdaBelief optimizer [25] to update parameters with the gradient; though other optimizers such as SGD can be used, we found AdaBelief converges faster in practice."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7dcbd26734797a3aa428411073c18b94c72708cf",
                "externalIds": {
                    "DBLP": "conf/ipmi/ZhuangDTPVD21",
                    "ArXiv": "2102.11013",
                    "DOI": "10.1007/978-3-030-78191-0_5",
                    "CorpusId": 231985476
                },
                "corpusId": 231985476,
                "publicationVenue": {
                    "id": "8c4f911f-84c7-4e8e-87ed-96b3d4c55774",
                    "name": "Information Processing in Medical Imaging",
                    "type": "conference",
                    "alternate_names": [
                        "Inf Process Med Imaging",
                        "IPMI"
                    ],
                    "url": "https://en.wikipedia.org/wiki/Information_Processing_in_Medical_Imaging"
                },
                "url": "https://www.semanticscholar.org/paper/7dcbd26734797a3aa428411073c18b94c72708cf",
                "title": "Multiple-shooting adjoint method for whole-brain dynamic causal modeling",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46251934",
                        "name": "Juntang Zhuang"
                    },
                    {
                        "authorId": "5507046",
                        "name": "N. Dvornek"
                    },
                    {
                        "authorId": "1688323",
                        "name": "S. Tatikonda"
                    },
                    {
                        "authorId": "1932911",
                        "name": "X. Papademetris"
                    },
                    {
                        "authorId": "2827899",
                        "name": "P. Ventola"
                    },
                    {
                        "authorId": "145947161",
                        "name": "J. Duncan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2013 We introduce AdaBelief optimizer [13] into iterative gradient attack to form AdaBelief Iterative Fast Gradient Method.",
                "For ABI-FGM, we follow the default settings in [13] with the stability coefficient \u03b4 = 10\u221214, the decay factors \u03b21 = 0.",
                "AdaBelief [13] is an adaptive learning rate optimization algorithm, which can be easily modified from Adam [30] without additional parameters.",
                "In terms of the advantages of AdaBelief over other optimization algorithms, a large number of experiments have been carried out, detailed and sufficient experiments are shown in [13]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d1d4a3f52edb60ef97a3257a398ca9e60c9fed46",
                "externalIds": {
                    "ArXiv": "2102.03726",
                    "DBLP": "journals/corr/abs-2102-03726",
                    "DOI": "10.1007/s10489-022-03469-5",
                    "CorpusId": 231846700
                },
                "corpusId": 231846700,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d1d4a3f52edb60ef97a3257a398ca9e60c9fed46",
                "title": "Adversarial example generation with adabelief optimizer and crop invariance",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143787140",
                        "name": "Bo Yang"
                    },
                    {
                        "authorId": "2153524934",
                        "name": "Hengwei Zhang"
                    },
                    {
                        "authorId": "2108471978",
                        "name": "Yuchen Zhang"
                    },
                    {
                        "authorId": "2113473946",
                        "name": "Kaiyong Xu"
                    },
                    {
                        "authorId": "2128724312",
                        "name": "Jin-dong Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "548fd9d631acb99d19dea54a291c91c1caaa1013",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-02128",
                    "ArXiv": "2102.02128",
                    "DOI": "10.1002/int.22720",
                    "CorpusId": 231786477
                },
                "corpusId": 231786477,
                "publicationVenue": {
                    "id": "05528bac-d212-46a6-9c84-314d4bd77368",
                    "name": "International Journal of Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Intell Syst"
                    ],
                    "issn": "0884-8173",
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/36062",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/1098111X"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/548fd9d631acb99d19dea54a291c91c1caaa1013",
                "title": "IWA: Integrated gradient\u2010based\u00a0white\u2010box attacks for fooling deep neural networks",
                "abstract": "The widespread application of deep neural network (DNN) techniques is being challenged by adversarial examples\u2014the legitimate input added with imperceptible and well\u2010designed perturbation that can fool DNNs easily in the DNN testing/deploying stage. Previous white\u2010box adversarial example generation algorithms used the Jacobian gradient information to add the perturbation. This imprecise and inexplicit information can cause unnecessary perturbation when generating adversarial examples. This paper aims to address this issue. We first propose to apply the more informative and distilled gradient information, namely, integrated gradient, to generate adversarial examples. To further make the perturbation more imperceptible, we propose to employ the restriction combination of L 0 and L 1 / L 2 second, which can restrict the total perturbation and the perturbation points simultaneously. Meanwhile, to address the nondifferentiable problem of L 1 , we explore a proximal operation of L 1 third. On the basis of these three works, we propose two Integrated gradient\u2010based White\u2010box Adversarial example generation algorithms (IWA): Integrated gradient\u2010based Finite Point Attack (IFPA) and Integrated gradient\u2010based Universe Attack (IUA). IFPA is suitable for situations where there are a determined number of points to be perturbed. IUA is suitable for situations where no perturbation point number is preset to obtain more adversarial examples. We verify the effectiveness of the proposed algorithms on both structured and unstructured data sets, and compare them with five baseline generation algorithms. The results show that our proposed algorithms craft adversarial examples with more imperceptible perturbation and satisfactory crafting rate. L 2 restriction is suitable for unstructured data sets and L 1 restriction performs better in the structured data set.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108734934",
                        "name": "Yixiang Wang"
                    },
                    {
                        "authorId": "9184144",
                        "name": "Jiqiang Liu"
                    },
                    {
                        "authorId": "108766082",
                        "name": "Xiaolin Chang"
                    },
                    {
                        "authorId": "1721064",
                        "name": "J. Misic"
                    },
                    {
                        "authorId": "1741013",
                        "name": "V. Mi\u0161i\u0107"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For optimization on model parameters, we tried SGD [38], Adam [39] and Adabelief [40] as optimizers, and find that Adabelief gives the best optimized model with highest evaluation performance and most stable convergence during training and testing.",
                "For the hyperparameters configuration, we followed the default settings of HAG-Net and selected Adabelief optimizer with learning rate \u03b7 = 0.001, \u03be = 10\u221216 , (\u03b20, \u03b21) = (0.9, 0.999) and weight decay \u03bb = 0.0001."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "05574a41342af0cd3d53239f65981e989be5e395",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-01649",
                    "ArXiv": "2102.01649",
                    "CorpusId": 231749575
                },
                "corpusId": 231749575,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/05574a41342af0cd3d53239f65981e989be5e395",
                "title": "Heterogeneous Graph based Deep Learning for Biomedical Network Link Prediction",
                "abstract": "Multi-scale biomedical knowledge networks are expanding with emerging experimental technologies that generates multi-scale biomedical big data. Link prediction is increasingly used especially in bipartite biomedical networks to identify hidden biological interactions and relationshipts between key entities such as compounds, targets, gene and diseases. We propose a Graph Neural Networks (GNN) method, namely Graph Pair based Link Prediction model (GPLP), for predicting biomedical network links simply based on their topological interaction information. In GPLP, 1-hop subgraphs extracted from known network interaction matrix is learnt to predict missing links. To evaluate our method, three heterogeneous biomedical networks were used, i.e. Drug-Target Interaction network (DTI), Compound-Protein Interaction network (CPI) from NIH Tox21, and Compound-Virus Inhibition network (CVI). Our proposed GPLP method significantly outperforms over the state-of-the-art baselines. In addition, different network incompleteness is analysed with our devised protocol, and we also design an effective approach to improve the model robustness towards incomplete networks. Our method demonstrates the potential applications in other biomedical networks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2890064",
                        "name": "Jinjiang Guo"
                    },
                    {
                        "authorId": "2155869735",
                        "name": "Jie Li"
                    },
                    {
                        "authorId": "8700232",
                        "name": "Dawei Leng"
                    },
                    {
                        "authorId": "6836662",
                        "name": "Lurong Pan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Szegedy et al.[4] were the first to exploit gradient information to generate adversarial examples, and the proposed white-box attack is called Fast Gradient Sign Method (FGSM).",
                "AdaBelief Iterative Fast Gradient Sign Method AdaBelief optimizer is proposed to solve the problem that adaptive optimization methods cannot generalize as well as stochastic gradient descent(SGD) optimization methods [12].",
                "Concretely, we propose an iterative method to improve the transferability of adversarial examples in the black-box setting and maintain the success rate in the white-box setting: AdaBelief iterative Fast Gradient Sign Method [12].",
                "In this section, we first describe how to combine AdaBelief optimizer with iterative Fast Gradient Sign Method.",
                "In this section, we give a detailed explanation of basic notations and Fast Gradient Sign Method (FGSM).",
                "There exist two families of gradient descent algorithms: accelerated stochastic gradient descent (SGD) family, such as momentum[13] and Nesterov[14]; adaptive learning rate family[12], such as Adam[15], Adadelta[16]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7645492c3663c0008a763f916f02b288593d7cc2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-09930",
                    "ArXiv": "2101.09930",
                    "CorpusId": 231698408
                },
                "corpusId": 231698408,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7645492c3663c0008a763f916f02b288593d7cc2",
                "title": "Generalizing Adversarial Examples by AdaBelief Optimizer",
                "abstract": "Recent research has proved that deep neural networks (DNNs) are vulnerable to adversarial examples\u2014the legitimate input added with imperceptible and well-designed perturbations can fool DNNs easily in the testing stage. However, most of the existing adversarial attacks are difficult to fool adversarially trained models. To solve this issue, we propose an AdaBelief iterative Fast Gradient Sign Method (AB-FGSM) to generalize adversarial examples. By integrating AdaBelief optimization algorithm to I-FGSM, we believe that the generalization of adversarial examples will be improved, relying on the strong generalization of AdaBelief optimizer. To validate the effectiveness and transferability of adversarial examples generated by our proposed AB-FGSM, we conduct the white-box and black-box attacks on various single models and ensemble models. Compared with state-of-the-art attack methods, our proposed method can generate adversarial examples effectively in the white-box setting, and the transfer rate is 7%~21% higher than latest attack methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108734934",
                        "name": "Yixiang Wang"
                    },
                    {
                        "authorId": "9184144",
                        "name": "Jiqiang Liu"
                    },
                    {
                        "authorId": "108766082",
                        "name": "Xiaolin Chang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the foreseeable future, we plan to improve deep learning classifiers by stacking multiple layers, and further optimize the hyperparameters, and possibly extend the study to include the very recent adabelief optimizer [65]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "62e2194dda52e13ddd64b8f9a12efcfa999d686c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-05626",
                    "ArXiv": "2101.05626",
                    "CorpusId": 231603127
                },
                "corpusId": 231603127,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/62e2194dda52e13ddd64b8f9a12efcfa999d686c",
                "title": "Eating Garlic Prevents COVID-19 Infection: Detecting Misinformation on the Arabic Content of Twitter",
                "abstract": "The rapid growth of social media content during the current pandemic provides useful tools for disseminating information which has also become a root for misinformation. Therefore, there is an urgent need for fact-checking and effective techniques for detecting misinformation in social media. In this work, we study the misinformation in the Arabic content of Twitter. We construct a large Arabic dataset related to COVID-19 misinformation and gold-annotate the tweets into two categories: misinformation or not. Then, we apply eight different traditional and deep machine learning models, with different features including word embeddings and word frequency. The word embedding models (\\textsc{FastText} and word2vec) exploit more than two million Arabic tweets related to COVID-19. Experiments show that optimizing the area under the curve (AUC) improves the models' performance and the Extreme Gradient Boosting (XGBoost) presents the highest accuracy in detecting COVID-19 misinformation online.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "87727257",
                        "name": "S. Alqurashi"
                    },
                    {
                        "authorId": "1724715137",
                        "name": "Btool Hamoui"
                    },
                    {
                        "authorId": "2408242",
                        "name": "Abdulaziz S. Alashaikh"
                    },
                    {
                        "authorId": "3143907",
                        "name": "Ahmad Alhindi"
                    },
                    {
                        "authorId": "2451113",
                        "name": "Eisa A. Alanazi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ef0c9fe8263ff76154c46ceb8fa530d482e580ee",
                "externalIds": {
                    "ArXiv": "2012.14059",
                    "DBLP": "journals/corr/abs-2012-14059",
                    "CorpusId": 229678480
                },
                "corpusId": 229678480,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ef0c9fe8263ff76154c46ceb8fa530d482e580ee",
                "title": "Convolutional Neural Networks in Multi-Class Classification of Medical Data",
                "abstract": "We report applications of Convolutional Neural Networks (CNN) to multi-classification classification of a large medical data set (Diabetes 130-US hospitals for years 1999-2008 dataset). We work on multi-classification of the patient\u2019s readmission, which resulted in classification of three classes: 0, <30, or > 30 days. We discuss in detail how changes in the CNN model and the data pre-processing impact the classification results. In the end, we introduce an ensemble model that consists of both deep learning (CNN) and shallow learning models (Gradient Boosting). The method achieves Accuracy of 64.93%, the highest three-class classification accuracy we achieved in this study. Our results also show that CNN and the ensemble consistently obtain a higher Recall than Precision. The highest Recall is 68.87, whereas the highest Precision is 65.04.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109264708",
                        "name": "Yuanzheng Hu"
                    },
                    {
                        "authorId": "145368345",
                        "name": "Marina Sokolova"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "de21dfb081ebea3ebe0d3af1e70dca42af748a83",
                "externalIds": {
                    "DBLP": "journals/ml/LiWZC22",
                    "ArXiv": "2012.13760",
                    "DOI": "10.1007/s10994-022-06227-3",
                    "CorpusId": 237355021
                },
                "corpusId": 237355021,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/de21dfb081ebea3ebe0d3af1e70dca42af748a83",
                "title": "Variance reduction on general adaptive stochastic mirror descent",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2145235979",
                        "name": "Wenjie Li"
                    },
                    {
                        "authorId": "2108404523",
                        "name": "Zhanyu Wang"
                    },
                    {
                        "authorId": "2129513965",
                        "name": "Yichen Zhang"
                    },
                    {
                        "authorId": "2153217331",
                        "name": "Guang Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "48d3e2815410ec850f2bae5f48eeccabd13bc3ee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-06244",
                    "ArXiv": "2012.06244",
                    "MAG": "3111365927",
                    "CorpusId": 228376375
                },
                "corpusId": 228376375,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/48d3e2815410ec850f2bae5f48eeccabd13bc3ee",
                "title": "The Implicit Bias for Adaptive Optimization Algorithms on Homogeneous Neural Networks",
                "abstract": "Despite their overwhelming capacity to overfit, deep neural networks trained by specific optimization algorithms tend to generalize relatively well to unseen data. Recently, researchers explained it by investigating the implicit bias of optimization algorithms. A remarkable progress is the work [18], which proves gradient descent (GD) maximizes the margin of homogeneous deep neural networks. Except the first-order optimization algorithms like GD, adaptive algorithms such as AdaGrad, RMSProp and Adam are popular owing to its rapid training process. Meanwhile, numerous works have provided empirical evidence that adaptive methods may suffer from poor generalization performance. However, theoretical explanation for the generalization of adaptive optimization algorithms is still lacking. In this paper, we study the implicit bias of adaptive optimization algorithms on homogeneous neural networks. In particular, we study the convergent direction of parameters when they are optimizing the logistic loss. We prove that the convergent direction of RMSProp is the same with GD, while for AdaGrad, the convergent direction depends on the adaptive conditioner. Technically, we provide a unified framework to analyze convergent direction of adaptive optimization algorithms by constructing novel and nontrivial adaptive gradient flow and surrogate margin. The theoretical findings explain the superiority on generalization of exponential moving average strategy that is adopted by RMSProp and Adam. To the best of knowledge, it is the first work to study the convergent direction of adaptive optimizations on non-linear deep neural networks",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1390816671",
                        "name": "Bohan Wang"
                    },
                    {
                        "authorId": "47580728",
                        "name": "Qi Meng"
                    },
                    {
                        "authorId": "2154939268",
                        "name": "Wei Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To overcome these limitations, our proposed FCMRDpA enhances MBGD-RDA by replacing AdaBound with Powerball AdaBelief, which combines Powerball gradients [20], [21] and AdaBelief [22] so that the gradients and the learning rates are simultaneously adaptively changed.",
                "Moreover, unlike AdaBound [18] that determines the lower and upper bounds of the learning rates solely by the number of mini-batch iterations, AdaBelief [22] takes into consideration the curvature information of the loss function and adapts the learning rate by the belief in observed gradients."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "1be6c0f4bf12f5090283c025b53836cc12d0de85",
                "externalIds": {
                    "MAG": "3110456311",
                    "DBLP": "journals/isci/ShiWGZCW21",
                    "ArXiv": "2012.00060",
                    "DOI": "10.1016/J.INS.2021.05.084",
                    "CorpusId": 227238948
                },
                "corpusId": 227238948,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/1be6c0f4bf12f5090283c025b53836cc12d0de85",
                "title": "FCM-RDpA: TSK Fuzzy Regression Model Construction Using Fuzzy C-Means Clustering, Regularization, DropRule, and Powerball AdaBelief",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2110399546",
                        "name": "Zhenhua Shi"
                    },
                    {
                        "authorId": "144855927",
                        "name": "Dongrui Wu"
                    },
                    {
                        "authorId": "46181113",
                        "name": "Chenfeng Guo"
                    },
                    {
                        "authorId": "2112729493",
                        "name": "Changming Zhao"
                    },
                    {
                        "authorId": "145634433",
                        "name": "Yuqi Cui"
                    },
                    {
                        "authorId": "49450967",
                        "name": "Fei-Yue Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our model is implemented with PyTorch, and AdaBelief [53] is adapted as an optimizer."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8b1a09189acaa09e3a3da8507800ce5ed44a61f6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-12527",
                    "ArXiv": "2011.12527",
                    "MAG": "3108049163",
                    "DOI": "10.1007/s10489-022-04072-4",
                    "CorpusId": 227162430
                },
                "corpusId": 227162430,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8b1a09189acaa09e3a3da8507800ce5ed44a61f6",
                "title": "Match them up: visually explainable few-shot image classification",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2153213890",
                        "name": "Bowen Wang"
                    },
                    {
                        "authorId": "47681301",
                        "name": "Liangzhi Li"
                    },
                    {
                        "authorId": "1840437995",
                        "name": "Manisha Verma"
                    },
                    {
                        "authorId": "1789677",
                        "name": "Yuta Nakashima"
                    },
                    {
                        "authorId": "1738501775",
                        "name": "R. Kawasaki"
                    },
                    {
                        "authorId": "144829054",
                        "name": "H. Nagahara"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We train for 100 epochs with the AdaBeliefe (Zhuang et al., 2020) optimizer with a batch size of 128 using PyTorch (Paszke et al.",
                "We train for 100 epochs with the AdaBeliefe (Zhuang et al., 2020) optimizer with a batch size of 128 using PyTorch (Paszke et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0c45669e92711298d4b494e4a29e41f9134b0e79",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-02952",
                    "MAG": "3095857158",
                    "ArXiv": "2011.02952",
                    "CorpusId": 226254242
                },
                "corpusId": 226254242,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0c45669e92711298d4b494e4a29e41f9134b0e79",
                "title": "Generalized Negative Correlation Learning for Deep Ensembling",
                "abstract": "Ensemble algorithms offer state of the art performance in many machine learning applications. A common explanation for their excellent performance is due to the bias-variance decomposition of the mean squared error which shows that the algorithm's error can be decomposed into its bias and variance. Both quantities are often opposed to each other and ensembles offer an effective way to manage them as they reduce the variance through a diverse set of base learners while keeping the bias low at the same time. Even though there have been numerous works on decomposing other loss functions, the exact mathematical connection is rarely exploited explicitly for ensembling, but merely used as a guiding principle. In this paper, we formulate a generalized bias-variance decomposition for arbitrary twice differentiable loss functions and study it in the context of Deep Learning. We use this decomposition to derive a Generalized Negative Correlation Learning (GNCL) algorithm which offers explicit control over the ensemble's diversity and smoothly interpolates between the two extremes of independent training and the joint training of the ensemble. We show how GNCL encapsulates many previous works and discuss under which circumstances training of an ensemble of Neural Networks might fail and what ensembling method should be favored depending on the choice of the individual networks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "31934577",
                        "name": "Sebastian Buschj\u00e4ger"
                    },
                    {
                        "authorId": "32421394",
                        "name": "Lukas Pfahler"
                    },
                    {
                        "authorId": "1752599",
                        "name": "K. Morik"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c4c946e1d1aa62081b1a0d7d42057c00fdaf6992",
                "externalIds": {
                    "ArXiv": "2011.02150",
                    "MAG": "3097356789",
                    "CorpusId": 226246268
                },
                "corpusId": 226246268,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c4c946e1d1aa62081b1a0d7d42057c00fdaf6992",
                "title": "EAdam Optimizer: How $\\epsilon$ Impact Adam",
                "abstract": "Many adaptive optimization methods have been proposed and used in deep learning, in which Adam is regarded as the default algorithm and widely used in many deep learning frameworks. Recently, many variants of Adam, such as Adabound, RAdam and Adabelief, have been proposed and show better performance than Adam. However, these variants mainly focus on changing the stepsize by making differences on the gradient or the square of it. Motivated by the fact that suitable damping is important for the success of powerful second-order optimizers, we discuss the impact of the constant $\\epsilon$ for Adam in this paper. Surprisingly, we can obtain better performance than Adam simply changing the position of $\\epsilon$. Based on this finding, we propose a new variant of Adam called EAdam, which doesn't need extra hyper-parameters or computational costs. We also discuss the relationships and differences between our method and Adam. Finally, we conduct extensive experiments on various popular tasks and models. Experimental results show that our method can bring significant improvement compared with Adam. Our code is available at https://github.com/yuanwei2019/EAdam-optimizer.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2110750097",
                        "name": "Wei Yuan"
                    },
                    {
                        "authorId": "144947757",
                        "name": "Kai-Xin Gao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "05d86d0af6067141d42103b7c292fc1f42e0bc2c",
                "externalIds": {
                    "DOI": "10.1109/AIAM50918.2020.00007",
                    "CorpusId": 234477415
                },
                "corpusId": 234477415,
                "publicationVenue": {
                    "id": "d565b490-9e7c-4669-8bf2-7a9915d88a01",
                    "name": "International Conference on Artificial Intelligence and Advanced Manufacturing",
                    "type": "conference",
                    "alternate_names": [
                        "AIAM",
                        "Int Conf Artif Intell Adv Manuf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/05d86d0af6067141d42103b7c292fc1f42e0bc2c",
                "title": "Recognition of important Korean text structure based on Reinforcement Learning and Self-attention mechanism",
                "abstract": "Aiming at the problem that the artificial tagging of Korean corpus is too time-consuming and laborious, and it is difficult for minority languages to integrate with various resources. We intend to construct an effective Korean structural representation from the perspective of representation learning to improve the effectiveness of subsequent natural language processing tasks. Combining deep reinforcement learning with Self-attention mechanism, we propose a hierarchical self-attention model (Hierarchically Structured Korean, HS-K). By using the Actor-Critic idea in reinforcement learning, the model takes the text classification effect as the label feedback of reinforcement learning, and transforms the text structure division task into the sequence decision task. The experimental results show that the model can identify the important text structure of Korean which closer the manual tagging, and it has a friendly auxiliary effect on Korean informatization and intelligentialize.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "79381277",
                        "name": "Feiyang Yang"
                    },
                    {
                        "authorId": "46316880",
                        "name": "Yahui Zhao"
                    },
                    {
                        "authorId": "88231316",
                        "name": "Xu Cui"
                    },
                    {
                        "authorId": "37205707",
                        "name": "Rong-yi Cui"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Subsequently, a number of techniques have emerged to theoretically justify and algorithmically improve Adam, including AMSGrad (Reddi et al., 2018), AdaBound (Luo et al., 2019), RAdam (Liu et al., 2020) and AdaBelief (Zhuang et al., 2020).",
                ", 2020), AdaBelief (Zhuang et al., 2020), and AdaHessian (Yao et al.",
                "To make Apollo scale-invariant, we modify this rectification operation by incorporating a term similar to the gradient \u201cbelief\u201d (Zhuang et al., 2020):",
                "5 Convergence Analysis Similar to previous work (Reddi et al., 2018; Chen et al., 2019; Zhuang et al., 2020), we omit the initialization bias correction step, i.",
                "In addition, following Zhuang et al. (2020), we also tuned for AdaBelief (for Adam and RAdam, we fixed = 1e\u22128).",
                "Similar to previous work (Reddi et al., 2018; Chen et al., 2019; Zhuang et al., 2020), we omit the initialization bias correction step, i.e. we use mt = \u03b2tmt\u22121 + (1\u2212 \u03b2t)gt, 0 < \u03b2t < 1, \u2200t \u2208 [T ].",
                "The five baseline methods we compare with are SGD with momentum (Bottou and Bousquet, 2008), Adam (Kingma and Ba, 2015), Rectified Adam (RAdam) (Liu et al., 2020), AdaBelief (Zhuang et al., 2020), and AdaHessian (Yao et al., 2020).",
                "\u2026\u03c3) = max(|Bt|, \u03c3)\nTo make Apollo scale-invariant, we modify this rectification operation by incorporating a term similar to the gradient \u201cbelief\u201d (Zhuang et al., 2020):\nDt = rectify(Bt, \u03c3) = max(|Bt|, \u03c3\u2016gt+1 \u2212 gt\u2016\u221e) (31)\nIt is not hard to prove that Apollo with the rectification in (31) is\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "30b45ed379235a838d1e7b75f26cd3b4ad8e1466",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2009-13586",
                    "MAG": "3089447378",
                    "ArXiv": "2009.13586",
                    "CorpusId": 221995901
                },
                "corpusId": 221995901,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/30b45ed379235a838d1e7b75f26cd3b4ad8e1466",
                "title": "Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization",
                "abstract": "In this paper, we introduce Apollo, a quasi-Newton method for nonconvex stochastic optimization, which dynamically incorporates the curvature of the loss function by approximating the Hessian via a diagonal matrix. Importantly, the update and storage of the diagonal approximation of Hessian is as efficient as adaptive first-order optimization methods with linear complexity for both time and memory. To handle nonconvexity, we replace the Hessian with its rectified absolute value, which is guaranteed to be positive-definite. Experiments on three tasks of vision and language show that Apollo achieves significant improvements over other stochastic optimization methods, including SGD and variants of Adam, in term of both convergence speed and generalization performance. The implementation of the algorithm is available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2378954",
                        "name": "Xuezhe Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The optimizer is the Adabelief-optimizer [41] with eps 1e \u2212 16, betas (0.9, 0.999), weight decay 1e \u2212 4 and learning rate 2e \u2212 5.",
                "The optimizer is the Adabelief-optimizer [41] with eps 1e \u2212 16, betas (0."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "070018779627497346183703b9c3e5cfe02c4041",
                "externalIds": {
                    "DBLP": "conf/nips/MaLZH21",
                    "ArXiv": "2009.06891",
                    "CorpusId": 235358779
                },
                "corpusId": 235358779,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/070018779627497346183703b9c3e5cfe02c4041",
                "title": "Global-aware Beam Search for Neural Abstractive Summarization",
                "abstract": "This study develops a calibrated beam-based algorithm with awareness of the global attention distribution for neural abstractive summarization, aiming to improve the local optimality problem of the original beam search in a rigorous way. Specifically, a novel global protocol is proposed based on the attention distribution to stipulate how a global optimal hypothesis should attend to the source. A global scoring mechanism is then developed to regulate beam search to generate summaries in a near-global optimal fashion. This novel design enjoys a distinctive property, i.e., the global attention distribution could be predicted before inference, enabling step-wise improvements on the beam search through the global scoring mechanism. Extensive experiments on nine datasets show that the global (attention)-aware inference significantly improves state-of-the-art summarization models even using empirical hyper-parameters. The algorithm is also proven robust as it remains to generate meaningful texts with corrupted attention distributions. The codes and a comprehensive set of examples are available.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1485278258",
                        "name": "Ye Ma"
                    },
                    {
                        "authorId": "2061995596",
                        "name": "Zixun Lan"
                    },
                    {
                        "authorId": "48373979",
                        "name": "Lu Zong"
                    },
                    {
                        "authorId": "5380819",
                        "name": "Kaizhu Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Adabelief optimizer [64] can achieve fast convergence, good generalization and training stability by adapting the stepsize according to the \u201cbelief\u201d in the current gradient direction."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "08a4ee16f83d3781e5b9be644d3380c0a121a69b",
                "externalIds": {
                    "ArXiv": "2008.07277",
                    "DBLP": "journals/mlc/JieGVT22",
                    "DOI": "10.1007/s13042-022-01625-4",
                    "CorpusId": 234364560
                },
                "corpusId": 234364560,
                "publicationVenue": {
                    "id": "a0c45882-7c78-4f0c-8886-d3481ba02586",
                    "name": "International Journal of Machine Learning and Cybernetics",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Mach Learn Cybern"
                    ],
                    "issn": "1868-8071",
                    "url": "http://www.springer.com/engineering/mathematical/journal/13042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/13042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/08a4ee16f83d3781e5b9be644d3380c0a121a69b",
                "title": "Adaptive hierarchical hyper-gradient descent",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "103756614",
                        "name": "Renlong Jie"
                    },
                    {
                        "authorId": "32278515",
                        "name": "Junbin Gao"
                    },
                    {
                        "authorId": "39962145",
                        "name": "A. Vasnev"
                    },
                    {
                        "authorId": "2941842",
                        "name": "Minh-Ngoc Tran"
                    }
                ]
            }
        },
        {
            "contexts": [
                "999) \u03b5 10\u22128 7 ADABELIEF (Zhuang et al., 2020) \u03b1 10\u22123 LU(10\u22124, 1) 3 \u03b21 0.",
                ", 2017) AdaBelief (Zhuang et al., 2020) L4Adam/L4Momentum (Rol\u00ednek & Martius, 2018) AdaBlock (Yun et al.",
                "ADAM, NADAM and RADAM, as well as AMSBOUND, ADABOUND and ADABELIEF all have white or blue rows on several (but not all"
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "0b72faba82f0b652a5a082b57872c1c8ecaf7e9a",
                "externalIds": {
                    "ArXiv": "2007.01547",
                    "DBLP": "conf/icml/SchmidtSH21",
                    "MAG": "3040620603",
                    "CorpusId": 220347631
                },
                "corpusId": 220347631,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0b72faba82f0b652a5a082b57872c1c8ecaf7e9a",
                "title": "Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers",
                "abstract": "Choosing the optimizer is among the most crucial decisions of deep learning engineers, and it is not an easy one. The growing literature now lists literally hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often done according to personal anecdotes. In this work, we aim to replace these anecdotes, if not with evidence, then at least with heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost 35 000 individual runs, we contribute the following three points: Optimizer performance varies greatly across tasks. We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. While we can not identify an individual optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally provided competitive results in our experiments. This subset includes popular favorites and some less well-known contenders. We have open-sourced all our experimental results, making it available to use as well-tuned baselines when evaluating novel optimization methods and therefore reducing the necessary computational efforts.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1456064291",
                        "name": "Robin M. Schmidt"
                    },
                    {
                        "authorId": "144223390",
                        "name": "Frank Schneider"
                    },
                    {
                        "authorId": "2517795",
                        "name": "Philipp Hennig"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3f6a7fffbf3c96c99743f1e01f8dd86d9a9851fa",
                "externalIds": {
                    "DBLP": "conf/icml/XieWZSS22",
                    "ArXiv": "2006.15815",
                    "CorpusId": 248986834
                },
                "corpusId": 248986834,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3f6a7fffbf3c96c99743f1e01f8dd86d9a9851fa",
                "title": "Adaptive Inertia: Disentangling the Effects of Adaptive Learning Rate and Momentum",
                "abstract": "Adaptive Moment Estimation (Adam), which combines Adaptive Learning Rate and Momentum, would be the most popular stochastic optimizer for accelerating the training of deep neural networks. However, it is empirically known that Adam often generalizes worse than Stochastic Gradient Descent (SGD). The purpose of this paper is to unveil the mystery of this behavior in the diffusion theoretical framework. Specifically, we disentangle the effects of Adaptive Learning Rate and Momentum of the Adam dynamics on saddle-point escaping and flat minima selection. We prove that Adaptive Learning Rate can escape saddle points efficiently, but cannot select flat minima as SGD does. In contrast, Momentum provides a drift effect to help the training process pass through saddle points, and almost does not affect flat minima selection. This partly explains why SGD (with Momentum) generalizes better, while Adam generalizes worse but converges faster. Furthermore, motivated by the analysis, we design a novel adaptive optimization framework named Adaptive Inertia, which uses parameter-wise adaptive inertia to accelerate the training and provably favors flat minima as well as SGD. Our extensive experiments demonstrate that the proposed adaptive inertia method can generalize significantly better than SGD and conventional adaptive gradient methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "30014947",
                        "name": "Zeke Xie"
                    },
                    {
                        "authorId": "2108082210",
                        "name": "Xinrui Wang"
                    },
                    {
                        "authorId": "2973831",
                        "name": "Huishuai Zhang"
                    },
                    {
                        "authorId": "73355331",
                        "name": "Issei Sato"
                    },
                    {
                        "authorId": "67154907",
                        "name": "Masashi Sugiyama"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We conclude this section with a few remarks: 1) In practice our algorithm can be implemented just as easily with ADAM instead of SGD, as in some of our experiments (alternately, one may also be able to substitute other optimization algorithms such as Momentum SGD (Polyak, 1964), ADAGrad (Duchi et al., 2011), or Adabelief (Zhuang et al., 2020) for gradient updates).",
                "\u2026can be implemented just as easily with ADAM instead of SGD, as in some of our experiments (alternately, one may also be able to substitute other optimization algorithms such as Momentum SGD (Polyak, 1964), ADAGrad (Duchi et al., 2011), or Adabelief (Zhuang et al., 2020) for gradient updates).",
                ", 2011), or Adabelief (Zhuang et al., 2020) for gradient updates)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f4ce6ec7ab9073bfcacb9ef96219a9603149603a",
                "externalIds": {
                    "ArXiv": "2006.12376",
                    "DBLP": "conf/icml/KeswaniMSV22",
                    "CorpusId": 250244135
                },
                "corpusId": 250244135,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f4ce6ec7ab9073bfcacb9ef96219a9603149603a",
                "title": "A Convergent and Dimension-Independent Min-Max Optimization Algorithm",
                "abstract": "We study a variant of a recently introduced min-max optimization framework where the max-player is constrained to update its parameters in a greedy manner until it reaches a first-order stationary point. Our equilibrium definition for this framework depends on a proposal distribution which the min-player uses to choose directions in which to update its parameters. We show that, given a smooth and bounded nonconvex-nonconcave objective function, access to any proposal distribution for the min-player\u2019s updates, and stochastic gradient oracle for the max-player, our algorithm converges to the aforementioned approximate local equilibrium in a number of iterations that does not depend on the dimension. The equilibrium point found by our algorithm depends on the proposal distribution, and when applying our algorithm to train GANs we choose the proposal distribution to be a distribution of stochastic gradients. We empirically evaluate our algorithm on challenging nonconvex-nonconcave test-functions and loss functions arising in GAN training. Our algorithm converges on these test functions and, when used to train GANs, trains stably on synthetic and real-world datasets and avoids mode collapse.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35604162",
                        "name": "Vijay Keswani"
                    },
                    {
                        "authorId": "3263071",
                        "name": "Oren Mangoubi"
                    },
                    {
                        "authorId": "2274555",
                        "name": "Sushant Sachdeva"
                    },
                    {
                        "authorId": "1810064",
                        "name": "Nisheeth K. Vishnoi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To evaluate the e\u21b5ectiveness of MaxVA for image classification, we compare with SGD, Adam, LaProp [53] and AdaBelief [52] in training ResNet18 [16] on CIFAR10, CIFAR100 and ImageNet.",
                "Results of other meethods are from the AdaBelief paper [52].",
                "Same as other adaptive methods such as Adam and the recently proposed AdaBelief [52], we adopt this assumption throughout training.",
                "Note our results with ResNet18 is better than the recent AdaBelief\u2019s results with ResNet34 on CIFAR10/CIFAR100 (95.51/79.32 vs. 95.30/77.30 approximately), as well as AdaBelief with ResNet18 on ImageNet (70.16 vs. 70.08) [52].",
                "\u21e4: The results of AdaBelief are from their paper [52] with a ResNet34, while our results are with ResNet18.",
                "Starting from a similar motivation of adapting to the curvature, the recent work AdaBelief [52] directly estimates the exponential running average of the gradient deviation to compute the adaptive step sizes."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3ec7a41e40a6d9283c8903260254343ff5c0c737",
                "externalIds": {
                    "DBLP": "conf/pkdd/ZhuCGHLG21",
                    "ArXiv": "2006.11918",
                    "DOI": "10.1007/978-3-030-86523-8_38",
                    "CorpusId": 235732393
                },
                "corpusId": 235732393,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3ec7a41e40a6d9283c8903260254343ff5c0c737",
                "title": "MaxVA: Fast Adaptation of Step Sizes by Maximizing Observed Variance of Gradients",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2115801862",
                        "name": "Chenfei Zhu"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "40070055",
                        "name": "Furong Huang"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    },
                    {
                        "authorId": "1962083",
                        "name": "T. Goldstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Same as other adaptive methods such as ADAM and the recently proposed AdaBelief (Zhuang et al., 2020), we use this assumption throughout training.",
                "08) (Zhuang et al., 2020).",
                "Starting from a similar motivation of adapting to the curvature, the recent work AdaBelief (Zhuang et al., 2020) directly estimates the exponential running average of the gradient deviation to compute the adaptive step sizes."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3e95270c40ae2d8ff0be6866dfb648fa739b6441",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-11918",
                    "MAG": "3036735840",
                    "CorpusId": 219966750
                },
                "corpusId": 219966750,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3e95270c40ae2d8ff0be6866dfb648fa739b6441",
                "title": "Adaptive Learning Rates with Maximum Variation Averaging",
                "abstract": "Adaptive gradient methods such as RMSProp and Adam use exponential moving estimate of the squared gradient to compute element-wise adaptive step sizes and handle noisy gradients. However, Adam can have undesirable convergence behavior in some problems due to unstable or extreme adaptive learning rates. Methods such as AMSGrad and AdaBound have been proposed to stabilize the adaptive learning rates of Adam in the later stage of training, but they do not outperform Adam in some practical tasks such as training Transformers. In this paper, we propose an adaptive learning rate rule in which the running mean squared gradient is replaced by a weighted mean, with weights chosen to maximize the estimated variance of each coordinate. This gives a worst-case estimate for the local gradient variance, taking smaller steps when large curvatures or noisy gradients are present, resulting in more desirable convergence behavior than Adam. We analyze and demonstrate the improved efficacy of our adaptive averaging approach on image classification, neural machine translation and natural language understanding tasks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1431754650",
                        "name": "Chen Zhu"
                    },
                    {
                        "authorId": "145215470",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "40070055",
                        "name": "Furong Huang"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    },
                    {
                        "authorId": "1962083",
                        "name": "T. Goldstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026strategies to improve Adam through e.g. improved convergence (Reddi et al., 2018), warmup learning rate (Liu et al., 2020), moving average (Zhang et al., 2019b), Nesterov momentum (Dozat, 2016), rectified weight decay (Loshchilov & Hutter, 2019), and variance of gradients (Zhuang et al., 2020).",
                ", 2019b), Nesterov momentum (Dozat, 2016), rectified weight decay (Loshchilov & Hutter, 2019), and variance of gradients (Zhuang et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "67a06313cd84c8f5713892f4708358d256eb99c9",
                "externalIds": {
                    "MAG": "3092043977",
                    "DBLP": "conf/iclr/HeoCOHYKUH21",
                    "CorpusId": 222295578
                },
                "corpusId": 222295578,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/67a06313cd84c8f5713892f4708358d256eb99c9",
                "title": "AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights",
                "abstract": "Normalization techniques are a boon for modern deep learning. They let weights converge more quickly with often better generalization performances. It has been argued that the normalization-induced scale invariance among the weights provides an advantageous ground for gradient descent (GD) optimizers: the effective step sizes are automatically reduced over time, stabilizing the overall training procedure. It is often overlooked, however, that the additional introduction of momentum in GD optimizers results in a far more rapid reduction in effective step sizes for scale-invariant weights, a phenomenon that has not yet been studied and may have caused unwanted side effects in the current practice. This is a crucial issue because arguably the vast majority of modern deep neural networks consist of (1) momentum-based GD (e.g. SGD or Adam) and (2) scale-invariant parameters. In this paper, we verify that the widely-adopted combination of the two ingredients lead to the premature decay of effective step sizes and sub-optimal model performances. We propose a simple and effective remedy, SGDP and AdamP: get rid of the radial component, or the norm-increasing direction, at each optimizer step. Because of the scale invariance, this modification only alters the effective step sizes without changing the effective update directions, thus enjoying the original convergence properties of GD optimizers. Given the ubiquity of momentum GD and scale invariance in machine learning, we have evaluated our methods against the baselines on 13 benchmarks. They range from vision tasks like classification (e.g. ImageNet), retrieval (e.g. CUB and SOP), and detection (e.g. COCO) to language modelling (e.g. WikiText) and audio classification (e.g. DCASE) tasks. We verify that our solution brings about uniform gains in those benchmarks. Source code is available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3086596",
                        "name": "Byeongho Heo"
                    },
                    {
                        "authorId": "2647582",
                        "name": "Sanghyuk Chun"
                    },
                    {
                        "authorId": "2390510",
                        "name": "Seong Joon Oh"
                    },
                    {
                        "authorId": "2086576",
                        "name": "Dongyoon Han"
                    },
                    {
                        "authorId": "2151587",
                        "name": "Sangdoo Yun"
                    },
                    {
                        "authorId": "2276429",
                        "name": "Gyuwan Kim"
                    },
                    {
                        "authorId": "2847986",
                        "name": "Youngjung Uh"
                    },
                    {
                        "authorId": "2577039",
                        "name": "Jung-Woo Ha"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f1d527d069cd370428a40ec58bff5e85cd7f9715",
                "externalIds": {
                    "ArXiv": "2003.01247",
                    "CorpusId": 236169710
                },
                "corpusId": 236169710,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f1d527d069cd370428a40ec58bff5e85cd7f9715",
                "title": "Iterative Averaging in the Quest for Best Test Error",
                "abstract": "We analyse and explain the increased generalisation performance of iterate averaging using a Gaussian process perturbation model between the true and batch risk surface on the high dimensional quadratic. We derive three phenomena \\latestEdits{from our theoretical results:} (1) The importance of combining iterate averaging (IA) with large learning rates and regularisation for improved regularisation. (2) Justification for less frequent averaging. (3) That we expect adaptive gradient methods to work equally well, or better, with iterate averaging than their non-adaptive counterparts. Inspired by these results\\latestEdits{, together with} empirical investigations of the importance of appropriate regularisation for the solution diversity of the iterates, we propose two adaptive algorithms with iterate averaging. These give significantly better results compared to stochastic gradient descent (SGD), require less tuning and do not require early stopping or validation set monitoring. We showcase the efficacy of our approach on the CIFAR-10/100, ImageNet and Penn Treebank datasets on a variety of modern and classical network architectures.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "10746977",
                        "name": "Diego Granziol"
                    },
                    {
                        "authorId": "2146720825",
                        "name": "Xingchen Wan"
                    },
                    {
                        "authorId": "7641268",
                        "name": "Samuel Albanie"
                    },
                    {
                        "authorId": "143841496",
                        "name": "Stephen J. Roberts"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief (named for adapting stepsizes by the belief in observed gradients) was presented in [26], which uses vn := \u03b4vn\u22121 + (1\u2212 \u03b4)(G(xn, \u03ben)\u2212mn) (G(xn, \u03ben) \u2212 mn) in place of vn in (3)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "29ade647d7bbba28eebb835f40e909f69496a94e",
                "externalIds": {
                    "MAG": "3008382482",
                    "ArXiv": "2002.09647",
                    "DBLP": "journals/tcyb/Iiduka22",
                    "DOI": "10.1109/TCYB.2021.3107415",
                    "CorpusId": 211259252,
                    "PubMed": "34495862"
                },
                "corpusId": 211259252,
                "publicationVenue": {
                    "id": "404813e7-95da-4137-be14-2ba73d2df4fd",
                    "name": "IEEE Transactions on Cybernetics",
                    "alternate_names": [
                        "IEEE Trans Cybern"
                    ],
                    "issn": "2168-2267",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6221036",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/29ade647d7bbba28eebb835f40e909f69496a94e",
                "title": "Appropriate Learning Rates of Adaptive Learning Rate Optimization Algorithms for Training Deep Neural Networks",
                "abstract": "This article deals with nonconvex stochastic optimization problems in deep learning. Appropriate learning rates, based on theory, for adaptive-learning-rate optimization algorithms (e.g., Adam and AMSGrad) to approximate the stationary points of such problems are provided. These rates are shown to allow faster convergence than previously reported for these algorithms. Specifically, the algorithms are examined in numerical experiments on text and image classification and are shown in experiments to perform better with constant learning rates than algorithms using diminishing learning rates.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2018304",
                        "name": "H. Iiduka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "CIFAR-10\nTest Err%\nCIFAR-100\nTest Err %\nImageNet\nVal Err %\nPenn Treebank\nTest BPC \u2193\nPenn Treebank\nTest BPC \u2193\nCIFAR-10\nFID \u2193\nModel WRN 28-10 WRN 28-10 ResNet-50 3xLSTM(300) 3xLSTM(1000) GGAN\nSGD 3.86 (0.08) 19.05 (0.24) 24.01 1.404 1.237 (0.000) 133.0\nAdam 3.64 (0.06) 18.96 (0.21) 23.45 1.377 1.182 (0.000) 43.0\nAMSGrad 3.90 (0.17) 18.97 (0.09) 23.46 1.385 1.187 (0.001) 41.3\nAdaBound 5.40 (0.24) 22.76 (0.17) 27.99 \u2014 2.891 (0.041) 247.3\nAdaShift 4.08 (0.11) 18.88 (0.06) OOM 1.395 1.199 (0.001) 43.7\nRAdam 3.89 (0.09) 19.15 (0.13) 23.60 \u2014 1.349 (0.003) 42.5\nAdaBelief 3.98 (0.07) 19.08 (0.09) 24.11 1.377 1.198 (0.000) 44.8\nAdamW 4.11 (0.17) 20.13 (0.22) 26.70 1.401 1.227 (0.003) \u2014\nAvaGrad 3.80 (0.02) 18.76 (0.20) 23.58 1.375 1.179 (0.000) 35.3\nAvaGradW 3.97 (0.02) 19.04 (0.37) 23.49 1.375 1.175 (0.000) \u2014\nachieving an improvement of 7.7 FID over Adam (35.3 against 43.0).",
                "Morever, recently proposed adaptive methods such as AdaBelief [48] and RAdam [25] claim success while underperforming SGD on ImageNet.",
                "Since \u01eb affects AdaBelief differently and its official codebase recommends values as low as 10\u221216 for some tasks 2, we adopt a search space where candidate values for \u01eb are smaller by a factor of 10\u22128 i.e. starting from 10\u221216 instead of 10\u22128.",
                "Most strikingly, Adam outperforms AdaBound, RAdam, and AdaBelief: sophisticated methods whose motivation lies in improving the performance of adaptive methods.",
                "For all experiments we consider the following popular adaptive methods: Adam [21], AMSGrad [34], AdaBound [27], AdaShift [47], RAdam [25], AdaBelief [48], and AdamW [26].",
                "We also observe that AdaBound, RAdam, and AdaBelief all visibly underperform Adam in this setting where adaptivity (small \u01eb) is advantageous, even given extensive hyperparameter tuning."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "614a5afd870d734642df04fa758fca2e12958856",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1912-01823",
                    "ArXiv": "1912.01823",
                    "MAG": "2994406485",
                    "DOI": "10.1109/CVPR46437.2021.01602",
                    "CorpusId": 208617767
                },
                "corpusId": 208617767,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/614a5afd870d734642df04fa758fca2e12958856",
                "title": "Domain-Independent Dominance of Adaptive Methods",
                "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. When training GANs, AvaGrad improves upon existing optimizers.1",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1387999547",
                        "name": "Pedro H. P. Savarese"
                    },
                    {
                        "authorId": "145689002",
                        "name": "David A. McAllester"
                    },
                    {
                        "authorId": "36366640",
                        "name": "Sudarshan Babu"
                    },
                    {
                        "authorId": "145854440",
                        "name": "M. Maire"
                    }
                ]
            }
        },
        {
            "contexts": [
                "During the last decade, many wellknown SGD methods which are incorporated with adaptive learning rates have been proposed by the deep learning community, which include but are not limited to AdaGrad [8], RMSProp [28], Adam [12], AMSGrad [19], Adabelief [41] and Adabound [15]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "65802c8212177d3227fede3523aa118521aec8ec",
                "externalIds": {
                    "ArXiv": "1811.04187",
                    "MAG": "2998419562",
                    "DBLP": "journals/ijon/WangLZ22",
                    "DOI": "10.1016/j.neucom.2022.02.039",
                    "CorpusId": 210902769
                },
                "corpusId": 210902769,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/65802c8212177d3227fede3523aa118521aec8ec",
                "title": "Accelerated Gradient-free Neural Network Training by Multi-convex Alternating Optimization",
                "abstract": null,
                "year": 2018,
                "authors": [
                    {
                        "authorId": "2120473483",
                        "name": "Junxiang Wang"
                    },
                    {
                        "authorId": "143796160",
                        "name": "Fuxun Yu"
                    },
                    {
                        "authorId": "114741360",
                        "name": "Xiangyi Chen"
                    },
                    {
                        "authorId": "2116734918",
                        "name": "Liang Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c99be6b5cd24ae05f60256989efbefc7252c7717",
                "externalIds": {
                    "DBLP": "conf/iclr/ZhouXY23",
                    "CorpusId": 253187683
                },
                "corpusId": 253187683,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c99be6b5cd24ae05f60256989efbefc7252c7717",
                "title": "Win: Weight-Decay-Integrated Nesterov Acceleration for Adaptive Gradient Algorithms",
                "abstract": "Training deep networks on increasingly large-scale datasets is computationally challenging. In this work, we explore the problem of \u201c how to accelerate the convergence of adaptive gradient algorithms in a general manner \u201d, and aim at pro-viding practical insights to boost the training ef\ufb01ciency. To this end, we propose an effective Weight-decay-Integrated Nesterov acceleration (Win) for adaptive algorithms to enhance their convergence speed. Taking AdamW and Adam as examples, we minimize a dynamical loss per iteration which combines the vanilla training loss and a dynamic regularizer inspired by proximal point method (PPM) to improve the convexity of the problem. Then we respectively use the \ufb01rst- and second-order Taylor approximations of vanilla loss to update the variable twice while \ufb01xing the above dynamic regularization brought by PPM. In this way, we arrive at our Win acceleration (like Nesterov acceleration) for AdamW and Adam that uses a conservative step and a reckless step to update twice and then linearly combines these two updates for acceleration. Next, we extend this Win acceleration to LAMB and SGD. Our transparent acceleration derivation could provide insights for other accelerated methods and their integration into adaptive algorithms. Besides, we prove the convergence of Win-accelerated adaptive algorithms and justify their convergence superiority over their non-accelerated counterparts by taking AdamW and Adam as examples. Experimental results testify the faster convergence speed and superior performance of our Win-accelerated AdamW, Adam, LAMB and SGD over their vanilla counterparts on vision classi\ufb01cation tasks and language modeling tasks with CNN and Transformer backbones.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153245275",
                        "name": "Pan Zhou"
                    },
                    {
                        "authorId": "2543387",
                        "name": "Xingyu Xie"
                    },
                    {
                        "authorId": "2186749683",
                        "name": "Shuicheng Yan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, different optimizers will affect the model sensitivity and learning accuracy [16-19]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f34a7a2abcc89283de6c2533be2de680fbfb7d7f",
                "externalIds": {
                    "DOI": "10.14569/ijacsa.2023.0140624",
                    "CorpusId": 259311435
                },
                "corpusId": 259311435,
                "publicationVenue": {
                    "id": "20a3a2f3-532a-4f04-9f3d-1e268e100872",
                    "name": "International Journal of Advanced Computer Science and Applications",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Adv Comput Sci Appl"
                    ],
                    "issn": "2156-5570",
                    "url": "http://sites.google.com/site/ijacsa2010/",
                    "alternate_urls": [
                        "http://thesai.org/Publication/Default.aspx",
                        "https://thesai.org/Publications/IJACSA"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f34a7a2abcc89283de6c2533be2de680fbfb7d7f",
                "title": "Hierarchical Convolutional Neural Networks using CCP-3 Block Architecture for Apparel Image Classification",
                "abstract": "In fashion applications, deep learning has been applied automatically to recognize and classify the apparel images under the massive visual data, emerged on social networks. To classify the apparel correctly and quickly is challenging due to a variety of apparel features and complexity of the classification. Recently, the hierarchical convolutional neural networks (H\u2013CNN) with the VGGNet architecture was proposed to classify the fashion-MNIST datasets. However, the VGGNet (many layers) required many filters (in the convolution layer) and many neurons (in the fully connected layer), leading to computational complexity and long training-time. Therefore, this paper proposes to classify the apparel images by the H\u2013CNN in cooperated with the new shallow-layer CCP-3-Block architecture, where each building block consists of two convolutional layers (CC) and one pooling layer (P). In the CCP3-Block, the number of layers can be reduced (in the network), the number of filters (in the convolution layer), and the number of neurons (in the fully connected layer), while adding a new connection between the convolution layer and the pooling layer plus a batch-normalization technique before passing the activation so that networks can learn independently and train quickly. Moreover, dropout techniques were utilized in the feature mapping and fully connected to reduce overfitting, and the optimizer adaptive moment estimation was utilized to solve the decaying of gradients, which can improve the networkperformance. The experimental results showed that the improved H\u2013CNN model with our CCP-3-Block outperformed the recent H\u2013CNN model with the VGGNet in terms of decreased loss, increased accuracy, and faster training. Keywords\u2014Convolutional neural networks (CNN), hierarchical CNN (H-CNN), CCP-3 block (two convolutional layers (CC) and one pooling layer (P) per block), apparel image classification, fashion applications",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220978492",
                        "name": "Natthamon Chamnong"
                    },
                    {
                        "authorId": "2246634",
                        "name": "J. Werapun"
                    },
                    {
                        "authorId": "2437093",
                        "name": "Anantaporn Hanskunatai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e1bb84b3e6e094b24040cf25f0fba41a4c92b6b7",
                "externalIds": {
                    "DOI": "10.11591/ijai.v12.i4.pp1695-1703",
                    "CorpusId": 260595996
                },
                "corpusId": 260595996,
                "publicationVenue": {
                    "id": "6dcf82ac-7373-43d7-9939-cdc8dcc3fd2f",
                    "name": "IAES International Journal of Artificial Intelligence (IJ-AI)",
                    "type": "journal",
                    "alternate_names": [
                        "IAES Int J Artif Intell",
                        "IAES International Journal of Artificial Intelligence",
                        "IAES Int J Artif Intell (IJ-AI"
                    ],
                    "issn": "2089-4872",
                    "url": "http://iaesjournal.com/online/index.php/IJAI"
                },
                "url": "https://www.semanticscholar.org/paper/e1bb84b3e6e094b24040cf25f0fba41a4c92b6b7",
                "title": "Attention gated encoder-decoder for ultrasonic signal denoising",
                "abstract": "Ultrasound imaging is one of the most widely used non-destructive testingmethods. The transducer emits pulses that travel through the imaged samplesand are reflected by echo-forming impedance. The resulting ultrasonic signalsusually contain noise. Most of the traditional noise reduction algorithmsrequire high skills and prior knowledge of noise distribution, which has acrucial impact on their performances. As a result, these methods generallyyield a loss of information, significantly influencing the final data and deeplylimiting both sensitivity and resolution of imaging devices in medical andindustrial applications. In the present study, a denoising method based on anattention-gated convolutional autoencoder is proposed to fill this gap. Toevaluate its performance, the suggested protocol is compared to widely usedmethods such as butterworth filtering (BF), discrete wavelet transforms(DWT), principal component analysis (PCA), and convolutional autoencoder(CAE) methods. Results proved that better denoising can be achievedespecially when the original signal-to-noise ratio (SNR) is very low and thesound waves\u2019 traces are distorted by noise. Moreover, the initial SNR wasimproved by up to 30 dB and the resulting Pearson correlation coefficient wasmaintained over 99% even for ultrasonic signals with poor initial SNR.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2229446351",
                        "name": "N. Mansouri"
                    },
                    {
                        "authorId": "2816702",
                        "name": "G. Khaissidi"
                    },
                    {
                        "authorId": "3198314",
                        "name": "G. Despaux"
                    },
                    {
                        "authorId": "2295994",
                        "name": "M. Mrabti"
                    },
                    {
                        "authorId": "16096917",
                        "name": "E. Cl\u00e9zio"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[26], one of the state-of-the-art (SOTA) first-order optimizers, introduced a new second-order momentum using a squared error between the current gradient and the first-order momentum, which is formulated as",
                "[48], AdaBelief [26], diffGrad [27], AngularGrad [49], and SAdam [28], as the comparison models.",
                "For example, AdaBelief [26], diffGrad [27],"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0e175e0dedacef698dd9e6105a967af652288c27",
                "externalIds": {
                    "DBLP": "journals/access/KimC23d",
                    "DOI": "10.1109/ACCESS.2023.3300034",
                    "CorpusId": 260409311
                },
                "corpusId": 260409311,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0e175e0dedacef698dd9e6105a967af652288c27",
                "title": "Advanced First-Order Optimization Algorithm With Sophisticated Search Control for Convolutional Neural Networks",
                "abstract": "As the performance of computing devices such as graphics processing units (GPUs) has improved dramatically, many deep neural network models, especially convolutional neural networks (CNNs), have been widely applied to various applications such as image classification, semantic segmentation, and object recognition. However, effective first-order optimization methods for CNNs have rarely been studied, although many CNN models have been successfully developed. Accordingly, this paper investigates various advanced adaptive solution search methods and proposes a new first-order optimization algorithm for CNNs called Adam-ASC. Our approach uses four sophisticated adaptive solution search methods to adjust its search strength in the complicated large-dimensional weight solution space spanned by a loss function. At the same time, we explain how they can be combined compensatively to form a complete optimizer with a detailed implementation. From the experiments, we found that our Adam-ASC can significantly improve the image recognition performance of practical CNNs in both the image classification and segmentation tasks. These experimental results show that the four fundamental methods of Adam-ASC and their compensative combination strategy play a crucial role in training CNNs by effectively finding their optimal weights.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111836747",
                        "name": "K. Kim"
                    },
                    {
                        "authorId": "35714151",
                        "name": "Y. Choi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9816172ee33da525cc193d4daa87ba34b114ed40",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-12979",
                    "DOI": "10.48550/arXiv.2307.12979",
                    "CorpusId": 260357267
                },
                "corpusId": 260357267,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9816172ee33da525cc193d4daa87ba34b114ed40",
                "title": "An Isometric Stochastic Optimizer",
                "abstract": "The Adam optimizer is the standard choice in deep learning applications. I propose a simple explanation of Adam\u2019s success: it makes each parameter\u2019s step size independent of the norms of the other parameters. Based on this principle I derive Iso, a new optimizer which makes the norm of a parameter\u2019s update invariant to the application of any linear transformation to its inputs and outputs. I develop a variant of Iso called IsoAdam that allows optimal hyperparameters to be transferred from Adam, and demonstrate that IsoAdam obtains a speedup over Adam when training a small Transformer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119404536",
                        "name": "Jacob Jackson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The models were trained using the AdaBelief [Zhuang et al., 2020] optimization algorithm with a learning rate of 1\u00d7 10\u22123 and a batch size of 256, and \u03b1 = 10.0.",
                "The model was trained for around 4 hours on a NVIDIA 3090 gpu with a learning rate of 1\u00d7 10\u22124\nTrue density Learned density Learned rank\nwith the AdaBelief [Zhuang et al., 2020] optimization algorithm and a batch size of 2048 and \u03b1 = 5.0.",
                "\u2026Jki(g(x)) 2 ) + \u03b3||f(g(x))\u2212 x||2, k \u223c Uniform(1, . . . , 10)\n(79)\nObjective1iNF = \u2211 x\u2208D \u2212 log pz(g(x)) + 1 2 log |J(g(x))TJ(g(x))|+ \u03b3||f(g(x))\u2212 x||2 (80)\nFor both models we set \u03b3 = 10, used a batch size of 64, learning rate of 1\u00d7 10\u22124 and the AdaBelief [Zhuang et al., 2020] optimization algorithm.",
                "The objectives we optimized were: Objective1iPF = \u2211 x\u2208D \u2212 log pz(g(x)) + dim(z) 2 log (\u2211 i Jki(g(x)) 2 ) + \u03b3||f(g(x))\u2212 x||2, k \u223c Uniform(1, . . . , 10)\n(79)\nObjective1iNF = \u2211 x\u2208D \u2212 log pz(g(x)) + 1 2 log |J(g(x))TJ(g(x))|+ \u03b3||f(g(x))\u2212 x||2 (80)\nFor both models we set \u03b3 = 10, used a batch size of 64, learning rate of 1\u00d7 10\u22124 and the AdaBelief [Zhuang et al., 2020] optimization algorithm."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fd0eefc8c57e68011198a2708775756c6d966f44",
                "externalIds": {
                    "CorpusId": 248748028
                },
                "corpusId": 248748028,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fd0eefc8c57e68011198a2708775756c6d966f44",
                "title": "P RINCIPAL MANIFOLD FLOWS",
                "abstract": "Normalizing \ufb02ows map an independent set of latent variables to their samples using a bijective transformation. Despite the exact correspondence between samples and latent variables, their high level relationship is not well understood. In this paper we characterize the geometric structure of \ufb02ows using principal manifolds and understand the relationship between latent variables and samples using contours. We introduce a novel class of normalizing \ufb02ows, called principal manifold \ufb02ows (PF), whose contours are its principal manifolds, and a variant for injective \ufb02ows (iPF) that is more ef\ufb01cient to train than regular injective \ufb02ows. PFs can be constructed using any \ufb02ow architecture, are trained with a regularized maximum likelihood objective and can perform density estimation on all of their principal manifolds. In our experiments we show that PFs and iPFs are able to learn the principal manifolds over a variety of datasets. Additionally, we show that PFs can perform density estimation on data that lie on a manifold with variable dimensionality, which is not possible with existing normalizing \ufb02ows.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2165094125",
                        "name": "Manifold Flows"
                    },
                    {
                        "authorId": "2056450967",
                        "name": "Edmond Cunningham"
                    },
                    {
                        "authorId": "36119737",
                        "name": "Adam D. Cobb"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Shown in Figure 17, this visual difference is more apparent when looking at pixel attacks using AdaBelief on these four different pre-trainings.",
                "In the pixel attacks using AdaBelief on AdaBelief pixel-trained model, contours and edges are clearly visible and the edits to the texture are smoother and more consistent.",
                "Specifically, as shown in Table 23, AdaBelief provides a significant improvement to PixelAT (0.71 to ImageNet, 1.72 in ImageNet-R) and a marginal improvement to PyramidAT (0.08 to ImageNet, 0.98 in ImageNet-R).",
                "Currently, AdaBelief does not provide such visible changes or improvements to pyramid.",
                "However after testing multiple optimizers (Adam [26], AdaBelief [66]), we observe significantly different behavior from AdaBelief.",
                "As shown in Figure 16, we also observe significant visual difference in the pixel attacks on the pixel-trained model with AdaBelief."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "58467f7aacd37fd5519dad26002a6247a6227670",
                "externalIds": {
                    "CorpusId": 249917179
                },
                "corpusId": 249917179,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/58467f7aacd37fd5519dad26002a6247a6227670",
                "title": "Pyramid Adversarial Training Improves ViT Performance",
                "abstract": "Description Scale s determines the size of the patch that an individual perturbation parameter will be applied to; e.g. for s=16, we learn and add a single adv parameter to each non-overlapping patch of size 16x16. The application is equivalent to a nearest neighbor resize on the 14x14 adv tensor to the image size of 224x224 and then addition. The scales s and multipliers ms used by PyramidAT are hyperparameters. Code We provide a minimal implementation of our technique in Fig. 1.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2094809887",
                        "name": "ViT"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The models were trained using the AdaBelief (Zhuang et al., 2020) optimization algorithm with a learning rate of 1\u00d7 10\u22123 and a batch size of 256, and \u03b1 = 10.0.",
                "The models were trained using the AdaBelief (Zhuang et al., 2020) optimization algorithm with a learning rate of 1\u00d7 10\u22123 and a batch size of 256, and \u03b1 = 10.",
                "For both models we set \u03b3 = 10, used a batch size of 64, learning rate of 1\u00d7 10\u22124 and the AdaBelief (Zhuang et al., 2020) optimization algorithm.",
                "The\nTrue density Learned density Learned rank\nmodel was trained for around 4 hours on a NVIDIA 3090 gpu with a learning rate of 1\u00d7 10\u22124 with the AdaBelief (Zhuang et al., 2020) optimization algorithm and a batch size of 2048 and \u03b1 = 5.0.",
                "\u2026Jki(g(x)) 2 ) + \u03b3||f(g(x))\u2212 x||2, k \u223c Uniform(1, . . . , 10)\n(79)\nObjective1iNF = \u2211 x\u2208D \u2212 log pz(g(x)) + 1 2 log |J(g(x))TJ(g(x))|+ \u03b3||f(g(x))\u2212 x||2 (80)\nFor both models we set \u03b3 = 10, used a batch size of 64, learning rate of 1\u00d7 10\u22124 and the AdaBelief (Zhuang et al., 2020) optimization algorithm.",
                "The objectives we optimized were:\nObjective1iPCF = \u2211 x\u2208D \u2212 log pz(g(x)) + dim(z) 2 log (\u2211 i Jki(g(x)) 2 ) + \u03b3||f(g(x))\u2212 x||2, k \u223c Uniform(1, . . . , 10)\n(79)\nObjective1iNF = \u2211 x\u2208D \u2212 log pz(g(x)) + 1 2 log |J(g(x))TJ(g(x))|+ \u03b3||f(g(x))\u2212 x||2 (80)\nFor both models we set \u03b3 = 10, used a batch size of 64, learning rate of 1\u00d7 10\u22124 and the AdaBelief (Zhuang et al., 2020) optimization algorithm.",
                "model was trained for around 4 hours on a NVIDIA 3090 gpu with a learning rate of 1\u00d7 10\u22124 with the AdaBelief (Zhuang et al., 2020) optimization algorithm and a batch size of 2048 and \u03b1 = 5."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1c33c42ffebe49ea0253cacff1ce61347945314d",
                "externalIds": {
                    "DBLP": "conf/icml/CunninghamCJ22",
                    "CorpusId": 250340982
                },
                "corpusId": 250340982,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1c33c42ffebe49ea0253cacff1ce61347945314d",
                "title": "Principal Component Flows",
                "abstract": "Normalizing \ufb02ows map an independent set of latent variables to their samples using a bijective transformation. Despite the exact correspondence between samples and latent variables, their high level relationship is not well understood. In this paper we characterize the geometric structure of \ufb02ows using principal manifolds and understand the relationship between latent variables and samples using contours. We introduce a novel class of normalizing \ufb02ows, called principal component \ufb02ows (PCF), whose contours are its principal manifolds, and a variant for injective \ufb02ows (iPCF) that is more ef\ufb01cient to train than regular injective \ufb02ows. PCFs can be constructed using any \ufb02ow architecture, are trained with a regularized maximum likelihood objective and can perform density estimation on all of their principal manifolds. In our experiments we show that PCFs and iPCFs are able to learn the principal manifolds over a variety of datasets. Additionally, we show that PCFs can perform density estimation on data that lie on a manifold with variable dimensionality, which is not possible with existing normalizing \ufb02ows.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056450967",
                        "name": "Edmond Cunningham"
                    },
                    {
                        "authorId": "36119737",
                        "name": "Adam D. Cobb"
                    },
                    {
                        "authorId": "37747652",
                        "name": "Susmit Jha"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model SGDM AdamW [21] RAdam [19] Ranger Adabelief [47] AdaHessian [38] Apollo [23] W-SGDM W-Adam R18 77.",
                "The compared methods include the representative and state-of-the-art DNN optimizers, including SGDM, AdamW [21], RAdam [19], Ranger [19, 45, 40] and Adabelief [47], AdaHessian1 [38] and Apollo [23].",
                "Based on Adam, Adabelief [47] considers the belief of observed gradient to adjust the adaptive learning rates.",
                "We refer to the strategies in [47] to set the learning rate and weight decay.",
                "Model SGDM AdamW [21] RAdam [19] Ranger Adabelief [47] AdaHessian [38] Apollo [23] W-SGDM W-Adam R18 70.",
                ", ReLU [25]), batch normalization (BN) [13], gradient clipping [26, 27], adaptive learning rate optimizers [3, 14, 47], and so on.",
                "Only Adabelief outperforms SGDM but it is still much worse than W-SGDM and W-Adam.",
                "The compared methods include the representative and state-of-the-art DNN optimizers, including SGDM, AdamW [21], RAdam [19], Ranger [19, 45, 40] and Adabelief [47], AdaHessian(1) [38] and Apollo [23].",
                "Model SGDM AdamW [21] RAdam [19] Ranger Adabelief [47] AdaHessian [38] Apollo [23] W-SGDM W-Adam\nR18 70.47 70.01 69.92 69.35 70.08 70.08 70.39 71.43 (\u21910.96) 71.59(\u21911.58) R50 76.31 76.02 76.12 75.95 76.22 - 76.32 77.48(\u21911.17) 76.83 (\u21910.81)\n0 20 40 60 80 100\nepoch\n0.4\n0.5\n0.6\n0.7\n0.8\nA cc\nu ra\ncy (%\n)\nTraining\nSGDM & R18 W-SGDM & R18 SGDM & R50 W-SGDM & R50\n0 20 40 60 80 100\nepoch\n0.4\n0.5\n0.6\n0.7\n0.8\nA cc\nu ra\ncy (%\n)\nValidation\nSGDM & R18 W-SGDM & R18 SGDM & R50 W-SGDM & R50\n0 20 40 60 80 100\nepoch\n0.4\n0.5\n0.6\n0.7\n0.8\nA cc\nu ra\ncy (%\n)\nTraining\nAdamW & R18 W-Adam & R18 AdamW & R50 W-Adam & R50\n0 20 40 60 80 100\nepoch\n0.4\n0.5\n0.6\n0.7\n0.8\nA cc\nu ra\ncy (%\n)\nValidation\nAdamW & R18 W-Adam & R18 AdamW & R50 W-Adam & R50\nFig."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4d3f81e6b5f8a8ab141815f99bffb70beab140a4",
                "externalIds": {
                    "DBLP": "conf/eccv/Yong022",
                    "DOI": "10.1007/978-3-031-20050-2_20",
                    "CorpusId": 253270096
                },
                "corpusId": 253270096,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/4d3f81e6b5f8a8ab141815f99bffb70beab140a4",
                "title": "An Embedded Feature Whitening Approach to Deep Neural Network Optimization",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7906116",
                        "name": "Hongwei Yong"
                    },
                    {
                        "authorId": "2152836933",
                        "name": "Lei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To be specific, DRAG obtains more than 0.5% generalization accuracy gain over AdaBelief [24] on most tasks.",
                "We compare our algorithm DRAG with some popular deep learning optimizers, including SGD [13], Adam [6], AdamW [9], AdaBound [11], AdaBelief [24], RAdam [8], Yogi [21].",
                "[24] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan.",
                "5% generalization accuracy gain over AdaBelief [24] on most tasks.",
                "The experimental setting is borrowed from AdaBelief [24] and we also use their default setting for all the hyperparameters.",
                "[24] and make the following necessary assumption.",
                "All results except DRAG and SGD are reported by Adabelief [24].",
                "We follow the exact experimental setting in Adabelief [24] and use their default hyperparameters except for SGD."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "590d82752e814d1516e542df9e48464d944840a2",
                "externalIds": {
                    "CorpusId": 254238821
                },
                "corpusId": 254238821,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/590d82752e814d1516e542df9e48464d944840a2",
                "title": "Dimension-Reduced Adaptive Gradient Method",
                "abstract": "Adaptive gradient methods, such as Adam, have shown faster convergence speed than SGD across various kinds of network models at the expense of inferior generalization performance. In this work, we proposed a Dimension-Reduced Adaptive Gradient Method (DRAG) to eliminate the generalization gap. DRAG makes an elegant combination of SGD and Adam by adopting a trust-region like framework. We observe that 1) Adam adjusts stepsizes for each gradient coordinate according to some loss curvature, and indeed decomposes the n -dimensional gradient into n standard basis directions to search; 2) SGD uniformly scales gradient for all gradient coordinates and actually has only one descent direction to minimize. Accordingly, DRAG reduces the high de-gree of freedom of Adam and also improves the flexibility of SGD via optimizing the loss along k ( \u226a n ) descent directions, e.g. the gradient direction and momentum direction used in this work. Then per iteration, DRAG finds the best stepsizes for k descent directions by solving a trust-region subproblem whose computational overhead is negligible since the trust-region subproblem is low-dimensional, e.g. k = 2 in this work. DRAG is compatible with the common deep learning training pipeline without introducing extra hyper-parameters and with negligible extra computation. Moreover, we prove the convergence property of DRAG for non-convex stochastic problems that often occur in deep learning training. Experimental results on representative benchmarks testify the fast convergence speed and also superior generalization of DRAG.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2193480017",
                        "name": "LI Jingyang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020)) and AdaBelief ((Zhuang et al., 2020)) have illustrated the state-of-the-art (SOTA) learning performance.",
                "E[(g \u2212m)2], in the update based on the same logic outlined in ((Zhuang et al., 2020)).",
                "\u2026\u221a T 4(1\u2212 \u03b2)\u03b1 d\u2211 i=1 v 1/2 T,i + 1 2 T\u22121\u2211 t=1 D2 \u03b1t d\u2211 i=1 v 1/2 t,i\n+ \u03b1\n\u221a T T\u22121\u2211 t=1 d\u2211 i=1 \u03b2T\u2212kg2k,i\n+ (1 + \u03b2)\u03b1\n\u221a 1 + log(T \u2212 1)\n2(1\u2212 \u03b2) d\u2211 i=1 \u2225\u2225g21:T\u22121,i\u2225\u22252 (31)\nNote the similarity between this regret bound and the one derived by (Reddi et al., 2018) and by (Zhuang et al., 2020) using AMSGrad.",
                "Among these, in our knowledge, RAdam ((Liu et al., 2020)) and AdaBelief ((Zhuang et al., 2020)) have illustrated the state-of-the-art (SOTA) learning performance.",
                "AdaBelief ((Zhuang et al., 2020)), which is similar to AdaTerm with \u03bd \u2192\u221e, was also tested, but was excluded from Figs.",
                "Results of benchmark problems In all the experiments, we compare the following six optimizers: Adam ((Kingma and Ba, 2014)), AdaBelief ((Zhuang et al., 2020)), and RAdam ((Gulcehre et al.",
                "Unfortunately, the second-order momentum vt is still based on the regular EMA, i.e. vt = \u03b22mt\u22121 + (1\u2212 \u03b22)st where st is a function of the squared gradient, e.g. st = g2t for Adam ((Kingma and Ba, 2014)) and st = (gt \u2212mt)2 for AdaBelief ((Zhuang et al., 2020)).",
                "Note that AdaBelief ((Zhuang et al., 2020)) could also remove it.",
                "In all the experiments, we compare the following six optimizers: Adam ((Kingma and Ba, 2014)), AdaBelief ((Zhuang et al., 2020)), and RAdam ((Gulcehre et al.,\n2017)) as the state-of-the-art optimizers in the cases without noise; t-Adam ((Ilboudo et al., 2020)) and At-Adam ((Ilboudo et al., 2021))\u2026",
                "st = g(2) t for Adam ((Kingma and Ba, 2014)) and st = (gt \u2212mt)(2) for AdaBelief ((Zhuang et al., 2020))."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "50bb7365e827d92f038134d5b4e5a28ae55afc6c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-06714",
                    "DOI": "10.2139/ssrn.4294401",
                    "CorpusId": 246016405
                },
                "corpusId": 246016405,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/50bb7365e827d92f038134d5b4e5a28ae55afc6c",
                "title": "AdaTerm: Adaptive T-Distribution Estimated Robust Moments towards Noise-Robust Stochastic Gradient Optimizer",
                "abstract": "With deep learning applications becoming more practical, practitioners are inevitably faced with datasets corrupted by a variety of noise such as measurement errors, mislabeling and estimated surrogate inputs/outputs, which can have negative impacts on the optimization results. As a safety net, it is natural to improve the robustness to noise of the optimization algorithm which updates the network parameters in the final process of learning. Previous works revealed that the first momentum used in Adam-like stochastic gradient descent optimizers can be modified based on the Student\u2019s t-distribution to produce updates robust to noise. In this paper, we propose AdaTerm which derives not only the first momentum but also all of the involved statistics based on the Student\u2019s t-distribution, providing for the first time a unified treatment of the optimization process under the t-distribution statistical model. When the computed gradients statistically appear to be aberrant, AdaTerm excludes them from the update and reinforce its robustness for subsequent updates; otherwise, it normally updates the network parameters and relaxes its robustness for the following updates. With this noise-adaptive behavior, AdaTerm\u2019s excellent learning performance was confirmed via typical optimization problems with several cases where the noise ratio is different and/or unknown. In addition, we proved a new general trick for deriving a theoretical regret bound without AMSGrad. Equal contribution Division of Information Science, Nara Institute of Science and Technology, Nara, Japan National Institute of Informatics/The Graduate University for Advanced Studies, SOKENDAI. Correspondence to: Wendyam Eric Lionel Ilboudo <ilboudo.wendyam eric.in1@is.naist.jp>, Taisuke Kobayashi<kobayashi@nii.ac.jp>, Takamitsu Matsubara <takam-m@is.naist.jp>.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1515577860",
                        "name": "Wendyam Eric Lionel Ilboudo"
                    },
                    {
                        "authorId": "5216888",
                        "name": "Taisuke Kobayashi"
                    },
                    {
                        "authorId": "2025991",
                        "name": "Kenji Sugimoto"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, the past few years, new optimisers, such as the AdaBelief [54], have shown promising results in deep learning, thus, it was crucial to test these TABLE 7.",
                "Furthermore, the past few years, new optimisers, such as the AdaBelief [54], have shown promising results in deep learning, thus, it was crucial to test these\navailable options for the proposed network.",
                "As shown by the results in Table 8, AdaBelief does not perform well in the proposed framework, with an SDR of 5.96 dB, and the RMSProp underperforms as well, with SDR of 7.63 dB, thus we excluded them from the final implementation."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e784f4b44f10e691d884079b8db9ac78ac41860a",
                "externalIds": {
                    "DBLP": "journals/access/SgourosBM22",
                    "DOI": "10.1109/ACCESS.2022.3221766",
                    "CorpusId": 253534550
                },
                "corpusId": 253534550,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e784f4b44f10e691d884079b8db9ac78ac41860a",
                "title": "An Efficient Short-Time Discrete Cosine Transform and Attentive MultiResUNet Framework for Music Source Separation",
                "abstract": "The music source separation problem, where the task at hand is to estimate the audio components that are present in a mixture, has been at the centre of research activity for a long time. In more recent frameworks, the problem is tackled by creating deep learning models, which attempt to extract information from each component by using Short-Time Fourier Transform (STFT) spectrograms as input. Most approaches assume that one source is present at each time-frequency point, which allows to allocate this point from the mixture to the desired source. Since this assumption is strong and is reported not to hold in practice, there is a problem that arises from the use of the magnitude of the STFT as input to these networks, which is the absence of the Fourier phase information during the separated source reconstruction. The recovery of the Fourier phase information is neither easily tractable, nor computationally efficient to estimate. In this paper, we propose a novel Attentive MultiResUNet architecture, that uses real-valued Short-Time Discrete Cosine Transform data as inputs. This step avoids the phase recovery problem, by estimating the appropriate values within the network itself, rather than employing complex estimation or post-processing algorithms. The proposed novel network features a U-Net type structure with residual skip connections and an attention mechanism that correlates the skip connection and the decoder output at the previous level. The proposed network is used for the first time in source separation and is more computationally efficient than state-of-the-art separation networks and features favourable performance compared to the state-of-the-art with a fraction of the computational cost.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3451610",
                        "name": "Thomas Sgouros"
                    },
                    {
                        "authorId": "2190881065",
                        "name": "Angelos Bousis"
                    },
                    {
                        "authorId": "1879125",
                        "name": "N. Mitianoudis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, some practical variations have been proposed to make it work better in real life, such as AdaGrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), AdaBelief (Zhuang et al., 2020), and SGD-momentum (Liu et al., 2020), even though their theoretical convergence have not been shown to be better.",
                ", 2020), Adam (Kingma & Ba, 2014), and AdaBelief (Zhuang et al., 2020).",
                ", 2011), Adam (Kingma & Ba, 2014), AdaBelief (Zhuang et al., 2020), and SGD-momentum (Liu et al.",
                "In this section, we show the performance of SDRS with proposed efficient implementation on both classification and regression tasks, and compare it with some widely used alternatives such as SGD-momentum (Liu et al., 2020), Adam (Kingma & Ba, 2014), and AdaBelief (Zhuang et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "1bbda32a7549d84237d4ba9fd55920f150de5f72",
                "externalIds": {
                    "DBLP": "journals/tmlr/BuminH22",
                    "CorpusId": 252739086
                },
                "corpusId": 252739086,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1bbda32a7549d84237d4ba9fd55920f150de5f72",
                "title": "Stochastic Douglas-Rachford Splitting for Regularized Empirical Risk Minimization: Convergence, Mini-batch, and Implementation",
                "abstract": "In this paper, we study the stochastic Douglas-Rachford splitting (SDRS) for general empirical risk minimization (ERM) problems with regularization. Our first contribution is to close the theoretical gap by proving its convergence for both convex and strongly convex problems; the convergence rates are O (1 / \u221a t ) and O (1 /t ), respectively. Since SDRS reduces to the stochastic proximal point algorithm (SPPA) when there is no regularization, it is pleasing to see the result matches that of SPPA, under the same mild conditions. We also propose the mini-batch version of SDRS that handles multiple samples simultaneously while maintaining the same efficiency as that of a single one, which is not a straight-forward extension in the context of stochastic proximal algorithms. We show that the mini-batch SDRS again enjoys the same convergence rate. Furthermore, we demonstrate that, for some of the canonical regularized ERM problems, each iteration of SDRS can be efficiently calculated either in closed form or in close to closed form via bisection\u2014the resulting complexity is identical to, for example, the stochastic (sub)gradient method. Experiments on real data demonstrate its effectiveness in terms of convergence compared to SGD and its variants.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1394621662",
                        "name": "Ayseg\u00fcl Bumin"
                    },
                    {
                        "authorId": "2349460",
                        "name": "Kejun Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fb2debff906f6977e9189eba068c4536632caa63",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-11989",
                    "CorpusId": 246411372
                },
                "corpusId": 246411372,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fb2debff906f6977e9189eba068c4536632caa63",
                "title": "Using Constant Learning Rate of Two Time-Scale Update Rule for Training Generative Adversarial Networks",
                "abstract": "Previous numerical results have shown that a two time-scale update rule (TTUR) using constant learning rates is practically useful for training generative adversarial networks (GANs). Meanwhile, a theoretical analysis of TTUR to find a stationary local Nash equilibrium of a Nash equilibrium problem with two players, a discriminator and a generator, has been given using decaying learning rates. In this paper, we give a theoretical analysis of TTUR using constant learning rates to bridge the gap between theory and practice. In particular, we show that, for TTUR using constant learning rates, the number of steps needed to find a stationary local Nash equilibrium decreases as the batch size increases. We also provide numerical results to support our theoretical analyzes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1659997847",
                        "name": "Naoki Sato"
                    },
                    {
                        "authorId": "2018304",
                        "name": "H. Iiduka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We tried SGD [32], Adam [33] and Adabelief [34] to optimize model parameters, and find that Adam provides the best optimal model with highest evaluation performance and most stable convergence during training and testing."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "166f9a0bd1be72e01e3506eb19f416dec72d6df9",
                "externalIds": {
                    "CorpusId": 247025851
                },
                "corpusId": 247025851,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/166f9a0bd1be72e01e3506eb19f416dec72d6df9",
                "title": "LIGANDFORMER: A GRAPH NEURAL NETWORK FOR PREDICTING LIGAND PROPERTY WITH ROBUST INTERPRETATION",
                "abstract": "Robust and efficient interpretation of QSAR methods is quite useful to validate AI prediction rationales with subjective opinion (chemist or biologist expertise), understand sophisticated chemical or biological process mechanisms, and provide heuristic ideas for structure optimization in pharmaceutical industry. For this purpose, we construct a multi-layer self-attention based Graph Neural Network framework, namely Ligandformer, for predicting ligand property with interpretation. Ligandformer integrates attention maps on ligand structure from different network blocks. The integrated attention map reflects the machine\u2019s local interest on compound structure, and indicates the relationship between predicted compound property and its structure. This work mainly contributes to three aspects: 1. Ligandformer directly opens the black-box of deep learning methods, providing local prediction rationales on chemical structures. 2. Ligandformer gives robust prediction in different experimental rounds, overcoming the ubiquitous prediction instability of deep learning methods. 3. Ligandformer can be generalized to predict different chemical or biological properties with high performance. Furthermore, Ligandformer can simultaneously output specific property score and visible attention map on structure, which can support researchers to investigate chemical or biological property and optimize structure efficiently. Our framework outperforms over counterparts in terms of accuracy, robustness and generalization, and can be applied in complex system study.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157958199",
                        "name": "Jinjiang Guo"
                    },
                    {
                        "authorId": "2157149882",
                        "name": "Qi Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0ce2477bdc3d4bdd2abd14e8ef1a089612850ad0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-13273",
                    "DOI": "10.48550/arXiv.2203.13273",
                    "CorpusId": 247749001
                },
                "corpusId": 247749001,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0ce2477bdc3d4bdd2abd14e8ef1a089612850ad0",
                "title": "On Exploiting Layerwise Gradient Statistics for Effective Training of Deep Neural Networks",
                "abstract": "\u2014Adam and AdaBelief compute and make use of elementwise adaptive stepsizes in training deep neural networks (DNNs) by tracking the exponential moving average (EMA) of the squared-gradient g 2 t and the squared prediction error ( m t \u2212 g t ) 2 , respectively, where m t is the \ufb01rst momentum at iteration t and can be viewed as a prediction of g t . In this work, we investigate if layerwise gradient statistics can be expoited in Adam and AdaBelief to allow for more effective training of DNNs. We address the above research question in two steps. Firstly, we slightly modify Adam and AdaBelief by introducing layerwise adaptive stepsizes in their update procedures via either pre- or post-processing. Our empirical results indicate that the slight modi\ufb01cation produces comparable performance for training VGG and ResNet models over CIFAR10 and CIFAR100, suggesting that layer-wise gradient statistics play an important role towards the success of Adam and AdaBelief for at least certian DNN tasks. In the second step, we propose Aida , a new optimisation method, with the objective that the elementwise stepsizes within each layer have signi\ufb01cantly smaller statistical variances, and the layerwise average stepsizes are much more compact across all the layers. Motivated by the fact that ( m t \u2212 g t ) 2 in AdaBelief is conservative in comparison to g 2 t in Adam in terms of layerwise statistical averages and variances, Aida is designed by tracking a more conservative function of m t and g t than ( m t \u2212 g t ) 2 via layerwise vector projections. Experimental results show that Aida produces either competitive or better performance with respect to a number of existing methods including Adam and AdaBelief for a set of challenging DNN tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46172209",
                        "name": "Guoqiang Zhang"
                    },
                    {
                        "authorId": "48341306",
                        "name": "K. Niwa"
                    },
                    {
                        "authorId": "144015910",
                        "name": "W. Kleijn"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0204b2848438e34eaf2b299d2bf7a6aa796e026a",
                "externalIds": {
                    "CorpusId": 248986964
                },
                "corpusId": 248986964,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0204b2848438e34eaf2b299d2bf7a6aa796e026a",
                "title": "Schr\u00a8odinger dynamics of undulatory locomotion",
                "abstract": "Spectral mode representations play an essential role in various areas of physics, from quantum mechanics to \ufb02uid turbulence, but they are not yet extensively used to characterize and describe the behavioral dynamics of living systems. Here, we show that mode-based linear models inferred from experimental live-imaging data can provide an accurate low-dimensional description of undulatory locomotion in worms, robots, and snakes. By incorporating physical symmetries and known biological constraints into the dynamical model, we \ufb01nd that the shape dynamics are generically governed by Schr\u00a8odinger equations in mode space. Similar to quantum systems, the eigenstates of the biophysical Hamiltonians enable the e\ufb03cient classi\ufb01cation and di\ufb00erentiation of locomotion behaviors in natural, simulated, and robotic organisms. While our analysis focuses on a widely studied class of biophysical locomotion phenomena, the underlying approach generalizes to other physical or living systems that permit a mode representation subject to geometric shape constraints.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2166049296",
                        "name": "Alexander E. Cohen"
                    },
                    {
                        "authorId": "146085118",
                        "name": "Alasdair D. Hastewell"
                    },
                    {
                        "authorId": "143820834",
                        "name": "Sreeparna Pradhan"
                    },
                    {
                        "authorId": "14269940",
                        "name": "S. Flavell"
                    },
                    {
                        "authorId": "1798934",
                        "name": "J. Dunkel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired by Zhuang et al. (2020), we compare the optimization trajectories for various loss functions.",
                "As discussed in Zhuang et al. (2020), the loss functions in Figure 1 are simple, yet they give important clues for the local behavior in deep learning optimization."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7772f1bc4b3e868ac6e56720617c33e2a2288e3d",
                "externalIds": {
                    "DBLP": "conf/aistats/YunLY22",
                    "CorpusId": 248923861
                },
                "corpusId": 248923861,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7772f1bc4b3e868ac6e56720617c33e2a2288e3d",
                "title": "AdaBlock: SGD with Practical Block Diagonal Matrix Adaptation for Deep Learning",
                "abstract": "We introduce AdaBlock , a class of adaptive gradient methods that extends popular approaches such as Adam by adopting the simple and natural idea of using block-diagonal matrix adaption to e\ufb00ectively utilize structural characteristics of deep learning archi-tectures. Unlike other quadratic or block-diagonal approaches, AdaBlock has complete freedom to select block-diagonal groups, providing a wider trade-o\ufb00 applicable even to extremely high-dimensional problems. We provide convergence and generalization error bounds for AdaBlock , and study both theoretically and empirically the impact of the block size on the bounds and advantages over usual diagonal approaches. In addition, we propose a randomized layer-wise variant of Adablock to further reduce computations and memory footprint, and devise an e\ufb03cient spectrum-clipping scheme for AdaBlock to bene\ufb01t from Sgd \u2019s superior generalization performance. Extensive experiments on several deep learning tasks demonstrate the bene\ufb01ts of block diagonal adaptation compared to adaptive diagonal methods, vanilla Sgd , as well as modi\ufb01ed versions of full-matrix adaptation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "80332204",
                        "name": "Jihun Yun"
                    },
                    {
                        "authorId": "145611269",
                        "name": "A. Lozano"
                    },
                    {
                        "authorId": "1720494",
                        "name": "Eunho Yang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bcb280ce0b47d645349ea37775ceaf41c74180bf",
                "externalIds": {
                    "CorpusId": 251734819
                },
                "corpusId": 251734819,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bcb280ce0b47d645349ea37775ceaf41c74180bf",
                "title": "V ARIATIONAL I NFERENCE FOR D ISCRIMINATIVE L EARNING WITH G ENERATIVE M ODELING OF F EA TURE I NCOMPLETION",
                "abstract": "We are concerned with the problem of distributional prediction with incomplete features: The goal is to estimate the distribution of target variables given feature vectors with some of the elements missing. A typical approach to this problem is to perform missing-value imputation and regression, simultaneously or sequen-tially, which we call the generative approach. Another approach is to perform regression after appropriately encoding missing values into the feature, which we call the discriminative approach. In comparison, the generative approach is more robust to the feature corruption while the discriminative approach is more favor-able to maximize the performance of prediction. In this study, we propose a hybrid method to take the best of both worlds. Our method utilizes the black-box variational inference framework so that it can be applied to a wide variety of modern machine learning models, including the variational autoencoders. We also con\ufb01rmed the effectiveness of the proposed method empirically.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2171833",
                        "name": "Kohei Miyaguchi"
                    },
                    {
                        "authorId": "37860384",
                        "name": "Takayuki Katsuki"
                    },
                    {
                        "authorId": "2064013801",
                        "name": "Akira Koseki"
                    },
                    {
                        "authorId": "150320144",
                        "name": "T. Iwamori"
                    }
                ]
            }
        },
        {
            "contexts": [
                "439 We determine the perceptrons using the AdaBelief optimizer 440 [43]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9390b444ae4a101aef0c2f0e2a6f2d0cf0dc0c68",
                "externalIds": {
                    "DBLP": "journals/tgrs/BykovGTV22",
                    "DOI": "10.1109/TGRS.2022.3202609",
                    "CorpusId": 251936180
                },
                "corpusId": 251936180,
                "publicationVenue": {
                    "id": "70628d6a-97aa-4571-9701-bc0eb3989c32",
                    "name": "IEEE Transactions on Geoscience and Remote Sensing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Geosci Remote Sens"
                    ],
                    "issn": "0196-2892",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=36",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9390b444ae4a101aef0c2f0e2a6f2d0cf0dc0c68",
                "title": "Inhomogeneous Anisotropic Analysis of the Available Water Content of the Upper Soil Layer According to Ground-Based and Remote Sensing on the Territory of Russia",
                "abstract": "The Hydrometeorological Center of Russia receives agrometeorological information from about 950 stations one time per ten days and the remote sensing Advanced Scatterometer (ASCAT) data from three Meteorological Operational (MetOp) satellites. We suggest a combined objective analysis (OA) of the available water content based on the available water content measurements at agrometeorological stations and on remote sensing data. The new version of OA is constructed using two neural networks and the backpropagation of error to learn it simultaneously. The first neural network is used to convert the ASCAT data into the available water content values, and the second network is used to estimate the inhomogeneities of soil moisture fields. We use the optimal interpolation (OI) method for assimilation of the ground-based data. In the new version, we evaluate the correlation functions (CFs) of inhomogeneous non-Gaussian fields, not from sample statistics but from machine learning methods. The method takes into account the combining of various datasets: ASCAT data, Food and Agriculture Organization (FAO) soil types, European Space Agency (ESA) GlobCover, and National Center for Atmospheric Research (NCAR) climate data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "102891321",
                        "name": "Ph. L. Bykov"
                    },
                    {
                        "authorId": "40576421",
                        "name": "V. Gordin"
                    },
                    {
                        "authorId": "49845084",
                        "name": "L. L. Tarasova"
                    },
                    {
                        "authorId": "2183400257",
                        "name": "Evgenii V. Vasilenko"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other related works are efforts on handcrafted optimizers [5, 13, 15, 22, 27, 36, 49].",
                "Despite a tremendous number of new optimizers introduced in recent years [2, 5, 26, 49], Adam [22], proposed in 2014, and its variant with decoupled weight decay, AdamW [27], are still the de facto standard optimizers for most deep neural networks, especially the recently proposed convolution-free or hybrid vision models, e."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "903217d739b11d593545d63656a7fcec133b342b",
                "externalIds": {
                    "CorpusId": 252760472
                },
                "corpusId": 252760472,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/903217d739b11d593545d63656a7fcec133b342b",
                "title": "Evolved Optimizer for Vision",
                "abstract": "We present an optimizer, Hero-Lion ( Evo L ved S i gn M o me n tum ), discovered by evolutionary search from basic math operations in the AutoML-Hero project. It keeps track of only the momentum and leverages the sign operation to calculate the update to the weights. Despite the simplicity, Hero-Lion outperforms the commonly used optimizer, such as AdamW, AdafactorW, and SGD with momentum, for training a variety of architectures on different tasks. Notably, it improves the accuracy of Vision Transformer for up to 2% when trained from scratch on ImageNet. When used in pre-training with larger data and model sizes, Hero-Lion still outperforms AdamW and AdafactorW, and can save 2-5x compute. On JFT-300M, ViT-L/16 trained by Hero-Lion matches the accuracy of the previous ViT-H/14 trained by AdamW. By replacing AdafactorW with Hero-Lion, we improve the ImageNet accuracy of ViT-G/14, pre-trained on JFT-3B, from 90.45% to 90.71%. Besides, Hero-Lion improves the contrastive pre-training of multi-modal Transformers by achieving \u223c 1% gain of ImageNet zero-shot accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143737082",
                        "name": "Xiangning Chen"
                    },
                    {
                        "authorId": "145246869",
                        "name": "Chen Liang"
                    },
                    {
                        "authorId": "2110408964",
                        "name": "Da Huang"
                    },
                    {
                        "authorId": "2892780",
                        "name": "Esteban Real"
                    },
                    {
                        "authorId": "2187205985",
                        "name": "Yao Liu"
                    },
                    {
                        "authorId": "2150032395",
                        "name": "Kai-Yi Wang"
                    },
                    {
                        "authorId": "1793529",
                        "name": "Cho-Jui Hsieh"
                    },
                    {
                        "authorId": "2141538599",
                        "name": "Yifeng Lu"
                    },
                    {
                        "authorId": "1397917613",
                        "name": "Quoc V. Le"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, it is shown that SGD can usually achieve much lower validation loss [87] compared to Adam when trained for enough number of iterations."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "137bfaf2d180a01eb94b2a19140d40702ffe8d96",
                "externalIds": {
                    "CorpusId": 253181144
                },
                "corpusId": 253181144,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/137bfaf2d180a01eb94b2a19140d40702ffe8d96",
                "title": "Dense Captioning Objects in 3D Environments using Natural Language",
                "abstract": "This thesis tackles the problem of dense captioning objects in 3D environments. In this task, we are given a 3D environment, and our aim is to first detect objects and then describe them using natural language. Previously, many works have addressed the problem of image captioning as well as dense captioning in 3D environments. However, no prior work has thoroughly investigated and compared the quality of generated captions from aspects such as the choice of visual input. In this thesis, we first introduce a 3D dense captioning pipeline, and then we show how it compares against prior work. Our investigations show that captioning objects in 3D leads to higher quality captions (compared to captioning with 2D visual inputs). We further show that simple modifications in the type of visual input (e.g. addition of depth to 2D single view images) and careful choice of optimization settings (e.g. optimizer, learning rate, and end-to-end training) can drastically improve the performance of the 2D captioning, and even outperform the 3D captioning on some evaluation metrics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064252419",
                        "name": "A. Gholami"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0ab2aa1b22a194ca66fb3a1b491e90199e6e91c6",
                "externalIds": {
                    "CorpusId": 253754162
                },
                "corpusId": 253754162,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0ab2aa1b22a194ca66fb3a1b491e90199e6e91c6",
                "title": "THE MINISTRY OF SCIENCE AND HIGHER EDUCATION OF THE RUSSIAN FEDERATION",
                "abstract": "The conductivity and dielectric permittivity of the underlying surface are part of the initial data necessary for calculating the energy parameters of radio tracks. The lack of complete information about the electrical characteristics of the Earth surface indicates the relevance of the researches. The technique of forming global digital maps of the electrical characteristics of the underlying surface in the very low frequency range points to two main aspects: the formation of information about the electrical characteristics of the continental and oceanic parts of the Earth surface. We obtained the global conductivity map of the continental part of the Earth surface by digitizing the soil conductivity atlas. We calculated a regression function, on the base of which a global map of the dielectric permittivity of the soil has been formed. Using the methods described in the ITU recommendations, we obtained global maps of the conductivity and permittivity of the oceanic part of the Earth surface based on data of the temperature and salinity of the world ocean. With the help of the developed data consolidation algorithm, we transformed the intermediate results into global digital maps of electrical characteristics of the Earth surface in the very low frequency range.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50847790",
                        "name": "O. L. Balysheva"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The AdaBelief optimizer was selected because it has both the fast convergence characteristics of the Adam optimizer and the good generalization capability of the SGD [55].",
                "AdaBelief optimizer was selected because it has both the fast convergence characteristics of the Adam optimizer and the good generalization capability of the SGD [55]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "985363be50d00694ea53547b97c17729a64b671d",
                "externalIds": {
                    "DBLP": "journals/access/LiXS22",
                    "DOI": "10.1109/ACCESS.2022.3222364",
                    "CorpusId": 253539363
                },
                "corpusId": 253539363,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/985363be50d00694ea53547b97c17729a64b671d",
                "title": "A Decoupled Head and Coordinate Attention Detection Method for Ship Targets in SAR Images",
                "abstract": "Currently, deep learning-based synthetic aperture radar (SAR) image ship target detection methods have been widely used in the field of SAR image ship detection. However, these methods suffer from high model complexity and poor performance when detecting small dense targets. To address this problem, this paper proposes a ship target detection algorithm based on the improved YOLO (You Only Look Once) algorithm. In addition, considering the real-time requirements and computational constraints in mobile applications, the YOLOv4 network is modified to make it more lightweight. Moreover, decoupled head and coordinate attention are introduced to preserve YOLOv4\u2019s superb detection performance as much as possible after lightweighting it. First, as the detection head of the YOLOv4 degrades the performance, this study decouples the classification and regression tasks. Second, since the channel attention mechanism ignores the spatial position information, coordinate attention is used to obtain long-range dependencies and accurate position information in the spatial domain. Moreover, the effects of the coordinate attention mechanism in different hierarchical YOLOv4 structures are analyzed. Furthermore, on the basis of the YOLOv4 backbone, another lightweight backbone is added to the model structure to improve model detection performance. Experimental results on the SAR ship detection dataset (SSDD) and the high-resolution SAR images dataset (HRSID) demonstrate that the proposed method can achieve high detection accuracy in complex scenes. The proposed lightweight model has fewer parameters compared to the original YOLOv4 structure. Furthermore, two massive SAR images are used to confirm the proposed model\u2019s migration application performance. The experimental results demonstrate that the proposed model has a strong migration ability and can be used in maritime monitoring.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2190935023",
                        "name": "Qinzuo Li"
                    },
                    {
                        "authorId": "35141608",
                        "name": "Dengjun Xiao"
                    },
                    {
                        "authorId": "2190839723",
                        "name": "Fangying Shi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d896d88a7bd4718ffe1696c49dcc7055235586d4",
                "externalIds": {
                    "DOI": "10.3788/irla20210697",
                    "CorpusId": 258034649
                },
                "corpusId": 258034649,
                "publicationVenue": {
                    "id": "24e05ba3-4c4a-4e17-8134-2c99807898be",
                    "name": "Infrared and Laser Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Infrared Laser Eng"
                    ],
                    "issn": "1007-2276"
                },
                "url": "https://www.semanticscholar.org/paper/d896d88a7bd4718ffe1696c49dcc7055235586d4",
                "title": "\u7a7a\u95f4\u5149\u6ce2\u524d\u7578\u53d8\u6821\u6b63\u4e2dSPGD\u65b9\u6cd5\u7684\u81ea\u9002\u5e94\u4f18\u5316",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2133663538",
                        "name": "\u8d75\u8f89 Zhao Hui"
                    },
                    {
                        "authorId": "2205946866",
                        "name": "\u909d\u51ef\u8fbe Kuang Kaida"
                    },
                    {
                        "authorId": "2206487505",
                        "name": "\u5415\u5178\u6977 Lv Diankai"
                    },
                    {
                        "authorId": "2206144549",
                        "name": "\u4f59\u5b5f\u6d01 Yu Mengjie"
                    },
                    {
                        "authorId": "2134430960",
                        "name": "\u5b89\u9759 An Jing"
                    },
                    {
                        "authorId": "2205946931",
                        "name": "\u5f20\u5929\u9a90 Zhang Tianqi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c5c37b94a468c98aacbe179b62a0256f4b81b9b1",
                "externalIds": {
                    "DBLP": "conf/nips/WeiBLY22",
                    "CorpusId": 258509364
                },
                "corpusId": 258509364,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c5c37b94a468c98aacbe179b62a0256f4b81b9b1",
                "title": "A Variant of Anderson Mixing with Minimal Memory Size",
                "abstract": "Anderson mixing (AM) is a useful method that can accelerate \ufb01xed-point iterations by exploring the information from historical iterations. Despite its numerical success in various applications, the memory requirement in AM remains a bottleneck when solving large-scale optimization problems in a resource-limited machine. To address this problem, we propose a novel variant of AM method, called Min-AM, by storing only one vector pair, that is the minimal memory size requirement in AM. Our method forms a symmetric approximation to the inverse Hessian matrix and is proved to be equivalent to the full-memory Type-I AM for solving strongly convex quadratic optimization. Moreover, for general nonlinear optimization problems, we establish the convergence properties of Min-AM under reasonable assumptions and show that the mixing parameters can be adaptively chosen by estimating the eigenvalues of the Hessian. Finally, we extend Min-AM to solve stochastic programming problems. Experimental results on logistic regression and network training problems validate the effectiveness of the proposed Min-AM.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113261072",
                        "name": "Fu Wei"
                    },
                    {
                        "authorId": "3183763",
                        "name": "Chenglong Bao"
                    },
                    {
                        "authorId": "2152801239",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "2149520603",
                        "name": "Guang Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is obvious that AdaBelief has the largest number of samples of small errors.",
                "On the other hand, the calculation speed of each epoch of AdaBelief is also significantly faster than the other\n66974 VOLUME 9, 2021\ntwo optimizers (computation time is reduced by 24.8% and 6.9% respectively compared with Adam and SGD).",
                "Compared with the traditional Adam in load forecasting tasks, the AdaBelief optimizer has a faster convergence rate, better stability, obvious accuracy advantages and stronger normalization.",
                "The so-called AdaBelief [36] refers to adjusting the training stride according to the Belief in the gradient direction.",
                "B. EXPERIMENT 1: THE INFLUENCE OF AdaBelief OPTIMIZER AND ATTENTION MECHANISM ON LOAD FORECASTING In this experiment, we use the same learning rate (lr = 1e-2) and the same training epoch (n = 60) to test the changes of loss function of TCN-GRU in the states of AdaBelief, Adam, and SGD.",
                "Otherwise, to further improve the performance of the model, this article also uses two techniques to make innovations: AdaBelief optimizer and Attention mechanism.",
                "Theoretically, AdaBelief mainly modifies the adaptive learning rate tuning item in Adam and considers curvature information.",
                "In addition, combined with the results of various metrics in Table 5, Adam has a large prediction error at this learning rate (RMSE>2000MW), and the training effect is obviously inferior to AdaBelief and SGD.",
                "C. AdaBelief OPTIMIZER The so-called AdaBelief [36] refers to adjusting the training stride according to the Belief in the gradient direction.",
                "After determining AdaBelief as the optimizer of the proposed method, this article also sets up an experiment to judge whether the Attention mechanism can improve the performance of the model.",
                "We have added different optimizer comparison experiments, compared the performance of Adam, SGD (stochastic gradient descent) [52], and AdaBelief in terms of training loss and prediction accuracy.",
                "In general, in this load forecasting task, the AdaBelief optimizer has only 1.24% on MAPE, which brings a very outstanding performance improvement.",
                "Finally, we use an improved optimizer AdaBelief and Attention mechanism to further improve the accuracy and efficiency of short-term load forecasting.",
                "With the support of AdaBelief optimizer and Attention mechanism, the proposed TCN-GRU model improves the accuracy and efficiency of short-term load forecasting.",
                "It is worth noting that the deep learning models all utilize the AdaBelief optimizer and Attention mechanism.",
                "The state-of-the-art AdaBelief optimizer based on Adam is adopted to greatly improve the accuracy and efficiency of model operation.",
                "Consequently, AdaBelief replaces vt in Adam with st . vt and st are EMA of gt2 and (gt \u2212Mt )2 respectively."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "cc38f0548f40a4b4bf524d1ba442415ee3c83585",
                "externalIds": {
                    "MAG": "3158245581",
                    "DBLP": "journals/access/ShiWSWZW21",
                    "DOI": "10.1109/ACCESS.2021.3076313",
                    "CorpusId": 233990913
                },
                "corpusId": 233990913,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cc38f0548f40a4b4bf524d1ba442415ee3c83585",
                "title": "Short-Term Load Forecasting Based on Adabelief Optimized Temporal Convolutional Network and Gated Recurrent Unit Hybrid Neural Network",
                "abstract": "To fully mine the relationship between temporal features in load data, improve the accuracy and efficiency of short-term load forecasting and overcome the difficulties caused by load nonlinearity and volatility in accurate load forecasting. In this paper, a hybrid neural network short-term load forecasting model based on temporal convolutional network (TCN) and gated recurrent unit (GRU) is proposed. Firstly, the correlation between meteorological features and load is measured with the distance correlation coefficient, and the fixed-length sliding time window method is used to reconstruct the features. Next, temporal convolutional network is adopted to extract the hidden historical information and time relationship including meteorological features, electricity price, etc., and a better-performing gated recurrent unit is utilized for perdition. Furthermore, the state-of-the-art AdaBelief optimizer and Attention mechanism are utilized to enhance the prediction accuracy and efficiency. The effectiveness and superiority of the proposed model are verified by load and weather data from Spain and PJM power system data. Short-term load forecasting results in different periods and comprehensive comparisons with the performance of different models show that the proposed model can provide accurate load forecasting results rather quickly. The highlights of this paper are that temporal convolutional network and gated recurrent unit are combined for load forecasting for the first time, and the forecasting performance is improved by the novel optimizer AdaBelief and feature selection based on distance correlation coefficient.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108661350",
                        "name": "Hanhong Shi"
                    },
                    {
                        "authorId": "2152513076",
                        "name": "Lei Wang"
                    },
                    {
                        "authorId": "1710827",
                        "name": "R. Scherer"
                    },
                    {
                        "authorId": "144433006",
                        "name": "M. Wo\u017aniak"
                    },
                    {
                        "authorId": "2109991428",
                        "name": "Pengchao Zhang"
                    },
                    {
                        "authorId": "2149191769",
                        "name": "Wei Wei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Traing by 200 epochs with ResNet-34 as backbone on CIFAR-10, our experiments show that AdaMomentum and SGDM can reach over 96% accuracy, while in Zhuang et al. (2020) the accuracy of SGDM is only around 94% .",
                "This contradicts the result reported in Zhuang et al. (2020), where they claim AdaBelief can achieve better accuray than SGDM.",
                "4https://github.com/pytorch/fairseq 5https://github.com/juntang-zhuang/SNGAN-AdaBelief",
                "Considering the fact that the loss function f is convex in area C and we do not take stochastic noise into account in this\nillustrative example as in Zhuang et al. (2020), we have\ng2t+1,i \u2264 g 2 t,i (4)\nThen we have at time t + 1,\nm2t+1,i = (\u03b21mt,i + (1\u2212 \u03b21)gt+1,i) 2\n(i) \u2265(\u03b21gt,i + (1\u2212 \u03b21)gt+1,i)2\u2026",
                "For hyperparameter tuning, we perform grid search to choose the best hyperparameters for all the baseline algorithms following Zhuang et al. (2020).",
                "We compare our proposed optimizer with seven state-of-the-art (SOTA) optimizers: SGDM (Sutskever et al., 2013), Adam (Kingma and Ba, 2014), AdamW (Loshchilov and Hutter, 2017), Yogi (Reddi et al., 2018), AdaBound (Luo et al., 2019), RAdam (Liu et al., 2019) and AdaBelief (Zhuang et al., 2020).",
                "Although these toy examples are simple, they give hints to the behavior of optimizers in complex deep learning tasks as they can be viewed as the local dynamics which occur frequently in deep learning (Zhuang et al., 2020).",
                "For RAdam, AdaBelief and AdaMomentum, the optimal hyperparameter values are: stepsize \u03b1 = 0.001, \u03b21 = 0.9, \u03b22 = 0.999 and weight decay parameter value is 10\u22122.",
                "Similar to Reddi et al. (2019); Luo et al. (2019); Zhuang et al. (2020), we omit the bias correction in the algorithm procedure for simplicity and the following analysis applies to the de-biased version as well.",
                "AdaBelief (Zhuang et al., 2020) adapts stepsizes by the belief in the observed gradients.",
                "In testing phase, AdaMomentum can exhibit performance almost as good as SGDM (slightly better than SGDM on CIFAR-10 and worse than SGDM on CIFAR-100) and far exceeds other adaptive gradient method baselines, including the recent proposed AdaBelief (Zhuang et al., 2020) optimizer.",
                "We believe this stems from the fact that Zhuang et al. (2020) did not take an appropriate stepsize annealing strategy or tune the hyperparameters well."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "8b2ffb143ed51bae6093a07650d994ac05be6242",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-11514",
                    "CorpusId": 235593324
                },
                "corpusId": 235593324,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8b2ffb143ed51bae6093a07650d994ac05be6242",
                "title": "Adapting Stepsizes by Momentumized Gradients Improves Optimization and Generalization",
                "abstract": "Adaptive gradient methods, such as Adam, have achieved tremendous success in machine learning. Scaling gradients by square roots of the running averages of squared past gradients, such methods are able to attain rapid training of modern deep neural networks. Nevertheless, they are observed to generalize worse than stochastic gradient descent (SGD) and tend to be trapped in local minima at an early stage during training. Intriguingly, we discover that substituting the gradient in the preconditioner term with the momentumized version in Adam can well solve the issues. The intuition is that gradient with momentum contains more accurate directional information and therefore its second moment estimation is a better choice for scaling than raw gradient\u2019s. Thereby we propose AdaMomentum as a new optimizer reaching the goal of training faster while generalizing better. We further develop a theory to back up the improvement in optimization and generalization and provide convergence guarantee under both convex and nonconvex settings. Extensive experiments on various models and tasks demonstrate that AdaMomentum exhibits comparable performance to SGD on vision tasks, and achieves state-ofthe-art results consistently on other tasks including language processing.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116758319",
                        "name": "Yizhou Wang"
                    },
                    {
                        "authorId": "2110042283",
                        "name": "Yue-yue Kang"
                    },
                    {
                        "authorId": "12282768",
                        "name": "Can Qin"
                    },
                    {
                        "authorId": "2143969442",
                        "name": "Yi Xu"
                    },
                    {
                        "authorId": "2197900626",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": "2129519081",
                        "name": "Yulun Zhang"
                    },
                    {
                        "authorId": "46956675",
                        "name": "Y. Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u00b7 In our experiment, using the AdaBelief[15] optimizer can speed up training, and the learning rate setting is not greater than 1e-4.",
                "[15] Juntang Zhuang, Tommy Tang, Sekhar Tatikonda, Nicha Dvornek, et al. AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients.",
                "\u00b7 In our experiment, using the AdaBelief[15] optimizer can speed up training, and the learning\nrate setting is not greater than 1e-4.",
                "[15] Juntang Zhuang, Tommy Tang, Sekhar Tatikonda, Nicha Dvornek, et al.",
                "For the optimizer, we did not use AdamW[14] or Adam[20] but chose AdaBelief[15], in whichh weight_decay is set to 1e-4, weight_decouple and rectify are both set to True."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3ff354527a92ae39094bd6f5aa319401ccccbe98",
                "externalIds": {
                    "CorpusId": 235740591
                },
                "corpusId": 235740591,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3ff354527a92ae39094bd6f5aa319401ccccbe98",
                "title": "Simple Video Instance Segmentation with ResNet and Transformer",
                "abstract": "Video instance segmentation(VIS) is a new vision task that has emerged in recent years and is processed by deep learning algorithms. It uses continuous video frames as input, generally ranging from a few frames to dozens of frames. Therefore, this segmentation task requires a large batch and large GPU memory for training. Based on recent VIS model VisTR, we propose a competitive VIS model called SimpleVTR in this paper. SimpleVTR trade off and optimizes the computing resources and effects of end-to-end video instance segmentation algorithm. While reducing the computing resources, the model effect can also be well maintained. First, we used one RTX3090 (24G) and one RTX1080Ti (11G) to continuously experiment and optimize the VIS model, and finally determined a model that can train only with one RTX1080Ti (11 G). At the same time, it achieved a score of 31.9 AP on the testing of Video Instance Segmentation in the YouTuBe VOS 2021 Challenge track 2. In the course of the experiment, we summarized some proposals and end-to-end training methods that should be followed.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116112934",
                        "name": "Wenbo Li"
                    },
                    {
                        "authorId": "36553195",
                        "name": "Jianzhen Tang"
                    },
                    {
                        "authorId": "2118596445",
                        "name": "Yujia Xie"
                    },
                    {
                        "authorId": "9166005",
                        "name": "Jian-liang Lan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Particularly, SGD, RMSprop, Adam, AdamW, diffGrad, AdaBelief AngularGradcos and AngularGradtan have been considered.",
                "On the other hand, instead of using the exponential moving average (EMA) of g2t , AdaBelief [47] uses the EMA of (gt \u2212mt)2 as st and the update direction for AdaBelief is mt/ \u221a st.",
                "For instance, in VGG16, the AngularGrad is quite close to SGDM and AdaBelief, whilst the rest of the optimization methods (Adam, AdamW, DiffGrad and RMSprop) are at least two points behind the best OA.",
                "However, the loss landscapes of diffGrad, AdaBelief, AngularGradcos and AngularGradtan are more or less uniform in shape.",
                "In this regard, AdamW, AngularGradtan and AngularGradcos have relatively smooth and uniform trajectory, reaching a solution near to the global minimum, while AdaBelief is noisier than other three optimization methods.",
                "On the other hand, instead of using the exponential moving average (EMA) of g(2) t , AdaBelief [47] uses the EMA of (gt \u2212mt)(2) as st and the update direction for AdaBelief is mt/ \u221a st.",
                "A similar situation is found in ResNeXt29 and DLA, where the proposed methods are pretty close to the best OA reached by AdaBelief and SGDM, respectively.",
                "To justify our theory, we model the optimization problem as a regression one over three one-dimensional non-convex functions, performing optimization over these functions using SGDM, Adam, diffGrad, AdaBelief, AngularGradcos and AngularGradtan."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e37673a1700d7efa915f5c0a7ff03e4c96ce1a57",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-10190",
                    "CorpusId": 261703051
                },
                "corpusId": 261703051,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e37673a1700d7efa915f5c0a7ff03e4c96ce1a57",
                "title": "AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks",
                "abstract": "Convolutional neural networks (CNNs) are trained using stochastic gradient descent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam) optimizer has become very popular due to its adaptive momentum, which tackles the dying gradient problem of SGD. Nevertheless, existing optimizers are still unable to exploit the optimization curvature information efficiently. This paper proposes a new AngularGrad optimizer that considers the behavior of the direction/angle of consecutive gradients. This is the first attempt in the literature to exploit the gradient angular information apart from its magnitude. The proposed AngularGrad generates a score to control the step size based on the gradient angular information of previous iterations. Thus, the optimization steps become smoother as a more accurate step size of immediate past gradients is captured through the angular information. Two variants of AngularGrad are developed based on the use of Tangent or Cosine functions for computing the gradient angular information. Theoretically, AngularGrad exhibits the same regret bound as Adam for convergence purposes. Nevertheless, extensive experiments conducted on benchmark data sets against state-of-the-art methods reveal a superior performance of AngularGrad. The source code will be made publicly available at: https://github.com/mhaut/AngularGrad.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1842981",
                        "name": "S. K. Roy"
                    },
                    {
                        "authorId": "31715636",
                        "name": "Mercedes Eugenia Paoletti"
                    },
                    {
                        "authorId": "21248079",
                        "name": "J. M. Haut"
                    },
                    {
                        "authorId": "34992579",
                        "name": "S. Dubey"
                    },
                    {
                        "authorId": "2125343461",
                        "name": "P. Kar"
                    },
                    {
                        "authorId": "2239169601",
                        "name": "Antonio J. Plaza"
                    },
                    {
                        "authorId": "2239129151",
                        "name": "B. B. Chaudhuri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Zhuang, J., Tang, T., Ding, Y., Tatikonda, S., Dvornek, N., Papademetris, X., and Duncan, J. S. AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients.",
                "Zhou, Y., Huang, K., Cheng, C., Wang, X., and Liu, X. FastAdaBelief: Improving Convergence Rate for Belief-based Adaptive Optimizer by Strong Convexity. arXiv preprint: 2104.13790, 2021a.\nZhou, Y., Li, X., and Banerjee, A. Noisy Truncated SGD: Optimization and Generalization. arXiv preprint: 2103.00075, 2021b.\nZhou, Z., Zhang, Q., Lu, G., Wang, H., Zhang, W., and Yu, Y. AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods.",
                ", 2017) AdaBelief (Zhuang et al., 2020) L4Adam/L4Momentum (Rol\u00ednek & Martius, 2018) AdaBlock (Yun et al."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "d406339244d904ee5b89f2091ce60d9fac3d6822",
                "externalIds": {
                    "CorpusId": 236923668
                },
                "corpusId": 236923668,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d406339244d904ee5b89f2091ce60d9fac3d6822",
                "title": "Descending through a Crowded Valley",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "118741557",
                        "name": "A. List"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For DNNs, existing analysis on optimization mainly focuses on the training [Kingma and Ba, 2015, Zhuang et al., 2020] rather than their performance in attacks.",
                "AdaBelief [Zhuang et al., 2020]\nmk+1 = \u03b21 \u00b7mk +(1\u2212\u03b21) \u00b7g(xk), \u03bd k+1 = \u03b22 \u00b7\u03bd k +(1\u2212\u03b22) \u00b7 (g(xk)\u2212mk+1)2,\ngk+1 = mk+1\u221a\n\u03bd k+1 +\u03b4 ."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "143c26b10d495aa48f499baf5e98e5603873bcc2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-10343",
                    "CorpusId": 231986235
                },
                "corpusId": 231986235,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/143c26b10d495aa48f499baf5e98e5603873bcc2",
                "title": "Going Far Boosts Attack Transferability, but Do Not Do It",
                "abstract": "Deep Neural Networks (DNNs) could be easily fooled by Adversarial Examples (AEs) with an imperceptible difference to original ones in human eyes. Also, the AEs from attacking one surrogate DNN tend to cheat other black-box DNNs as well, i.e., the attack transferability. Existing works reveal that adopting certain optimization algorithms in attack improves transferability, but the underlying reasons have not been thoroughly studied. In this paper, we investigate the impacts of optimization on attack transferability by comprehensive experiments concerning 7 optimization algorithms, 4 surrogates, and 9 black-box models. Through the thorough empirical analysis from three perspectives, we surprisingly \ufb01nd that the varied transferability of AEs from optimization algorithms is strongly related to the corresponding Root Mean Square Error (RMSE) from their original samples. On such a basis, one could simply approach high transferability by attacking until RMSE decreases, which motives us to propose a LArge RMSE Attack (LARA). Although LARA signi\ufb01cantly improves transferability by 20%, it is insuf\ufb01cient to exploit the vulnerability of DNNs, leading to a natural urge that the strength of all attacks should be measured by both the widely used (cid:96) \u221e bound and the RMSE addressed in this paper, so that tricky enhancement of transferability would be avoided.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150293336",
                        "name": "Sizhe Chen"
                    },
                    {
                        "authorId": "2035647651",
                        "name": "Qinghua Tao"
                    },
                    {
                        "authorId": "17064832",
                        "name": "Zhixing Ye"
                    },
                    {
                        "authorId": "47932717",
                        "name": "Xiaolin Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use AdaBelief [50] in combination with look-ahead optimizer [51]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0853d542d7102eceaec9504f55b5b7702b9a8a83",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-08665",
                    "CorpusId": 233296791
                },
                "corpusId": 233296791,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0853d542d7102eceaec9504f55b5b7702b9a8a83",
                "title": "Higher Order Recurrent Space-Time Transformer",
                "abstract": "Endowing visual agents with predictive capability is a key step towards video intelligence at scale. The predominant modeling paradigm for this is sequence learning, mostly implemented through LSTMs. Feed-forward Transformer architectures have replaced recurrent model designs in ML applications of language processing and also partly in computer vision. In this paper we investigate on the competitiveness of Transformer-style architectures for video predictive tasks. To do so we propose HORST, a novel higher order recurrent layer design whose core element is a spatial-temporal decomposition of self-attention for video. HORST achieves state of the art competitive performance on Something-Something-V2 early action recognition and EPIC-Kitchens-55 action anticipation, without exploiting a task specific design. We believe this is promising evidence of causal predictive capability that we attribute to our recurrent higher order design of self-attention.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "27084141",
                        "name": "Tsung-Ming Tai"
                    },
                    {
                        "authorId": "3144258",
                        "name": "G. Fiameni"
                    },
                    {
                        "authorId": "2115293259",
                        "name": "Cheng-Kuang Lee"
                    },
                    {
                        "authorId": "1717522",
                        "name": "O. Lanz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For training all models in this section, an Adabelief optimizer has been used [43]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8c34613a9c123246802e079fa0feb0709f090087",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-06421",
                    "CorpusId": 234482860
                },
                "corpusId": 234482860,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8c34613a9c123246802e079fa0feb0709f090087",
                "title": "Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation",
                "abstract": "Over the past few years, best SSL methods, gradually moved from the pre-text task learning to the Contrastive learning. But contrastive methods have some drawbacks which could not be solved completely, such as performing poor on fine-grained visual tasks compare to supervised learning methods. In this study, at first, the impact of ImageNet pre-training on fine-grained Facial Expression Recognition (FER) was tested. It could be seen from the results that training from scratch is better than ImageNet fine-tuning at stronger augmentation levels. After that, a framework was proposed for standard Supervised Learning (SL), called Hybrid Multi-Task Learning (HMTL) which merged Self-Supervised as auxiliary task to the SL training setting. Leveraging Self-Supervised Learning (SSL) can gain additional information from input data than labels which can help the main fine-grained SL task. It is been investigated how this method could be used for FER by designing two customized version of common pre-text techniques, Jigsaw puzzling and in-painting. The state-of-the-art was reached on AffectNet via two types of HMTL, without utilizing pre-training on additional datasets. Moreover, we showed the difference between SS pre-training and HMTL to demonstrate superiority of proposed method. Furthermore, the impact of proposed method was shown on two other fine-grained facial tasks, Head Poses estimation and Gender Recognition, which concluded to reduce in error rate by 11% and 1% respectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2091909928",
                        "name": "Mahdi Pourmirzaei"
                    },
                    {
                        "authorId": "2091913488",
                        "name": "Farzaneh Esmaili"
                    },
                    {
                        "authorId": "1801348",
                        "name": "G. Montazer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following [100], we test our method with one of the most popular models, Wasserstein GAN [3] with gradient penalty (WGAN-GP) [24].",
                "The adaptive selection of the update step-size has been based on several principles, including: the local sharpness of the loss function [91], incorporating a line search approach [80, 53, 49], the gradient change speed [16], the Barzilai-Borwein method [76], a \u201cbelief\u201d in the current gradient direction [100], the linearization of the loss [62], the per-component unweighted mean of all historical gradients [11], handling noise by preconditioning based on a covariance matrix [35], the adaptive and momental bounds [14], decorrelating the second moment and gradient terms [99], the importance weights [40], the layer-wise adaptation strategy [90], the gradient scale invariance [55], multiple learning rates [66], controlling the increase in effective learning [93], learning the update-step size [87], looking ahead at the sequence of fast weights generated by another optimizer [98]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3c8fbb954308da70413988bce0989abff78c87ca",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-03331",
                    "CorpusId": 235755407
                },
                "corpusId": 235755407,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3c8fbb954308da70413988bce0989abff78c87ca",
                "title": "KaFiStO: A Kalman Filtering Framework for Stochastic Optimization",
                "abstract": "Optimization is often cast as a deterministic problem, where the solution is found through some iterative procedure such as gradient descent. However, when training neural networks the loss function changes over (iteration) time due to the randomized selection of a subset of the samples. This randomization turns the optimization problem into a stochastic one. We propose to consider the loss as a noisy observation with respect to some reference optimum. This interpretation of the loss allows us to adopt Kalman filtering as an optimizer, as its recursive formulation is designed to estimate unknown parameters from noisy measurements. Moreover, we show that the Kalman Filter dynamical model for the evolution of the unknown parameters can be used to capture the gradient dynamics of advanced methods such as Momentum and Adam. We call this stochastic optimization method KaFiStO. KaFiStO is an easy to implement, scalable, and efficient method to train neural networks. We show that it also yields parameter estimates that are on par with or better than existing optimization algorithms across several neural network architectures and machine learning tasks, such as computer vision and language modeling.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "87459556",
                        "name": "A. Davtyan"
                    },
                    {
                        "authorId": "2117714519",
                        "name": "Sepehr Sameni"
                    },
                    {
                        "authorId": "28319879",
                        "name": "L. Cerkezi"
                    },
                    {
                        "authorId": "41016678",
                        "name": "Givi Meishvili"
                    },
                    {
                        "authorId": "48657002",
                        "name": "Adam Bielski"
                    },
                    {
                        "authorId": "145646305",
                        "name": "P. Favaro"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use Adam (Kingma and Ba, 2014) and AdaBelief (Zhuang et al., 2020) as optimizers on Twitter and Weibo datasets, respectively, to seek the optimal parameters of our model."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a852e1d3adae5179ca75abe1b93b4dcb410cc082",
                "externalIds": {
                    "DBLP": "conf/acl/WuZZWX21",
                    "ACL": "2021.findings-acl.226",
                    "DOI": "10.18653/v1/2021.findings-acl.226",
                    "CorpusId": 236478188
                },
                "corpusId": 236478188,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a852e1d3adae5179ca75abe1b93b4dcb410cc082",
                "title": "Multimodal Fusion with Co-Attention Networks for Fake News Detection",
                "abstract": "Fake news with textual and visual contents has a better story-telling ability than text-only contents, and can be spread quickly with social media. People can be easily deceived by such fake news, and traditional expert identi\ufb01cation is labor-intensive. Therefore, automatic detection of multimodal fake news has become a new hot-spot issue. A shortcoming of existing approaches is their inability to fuse multi-modality features effectively. They simply concatenate unimodal features without considering inter-modality relations. Inspired by the way people read news with image and text, we propose a novel Multimodal Co-Attention Networks (MCAN) to better fuse textual and visual features for fake news detection. Ex-tensive experiments conducted on two real-world datasets demonstrate that MCAN can learn inter-dependencies among multimodal features and outperforms state-of-the-art methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2149121852",
                        "name": "Yang Wu"
                    },
                    {
                        "authorId": "2105515515",
                        "name": "Pengwei Zhan"
                    },
                    {
                        "authorId": "2129519432",
                        "name": "Yunjian Zhang"
                    },
                    {
                        "authorId": "2109120341",
                        "name": "Liming Wang"
                    },
                    {
                        "authorId": "40285789",
                        "name": "Zhen Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief (Zhuang et al., 2020) mk+1 = \u03b21 \u00b7mk + (1\u2212 \u03b21) \u00b7 g(xk), \u03bdk+1 = \u03b22 \u00b7 \u03bdk + (1\u2212 \u03b22) \u00b7 (g(xk)\u2212mk+1)(2), gk+1 = mk+1 \u221a \u03bdk+1 + \u03b4 .",
                "AdaBelief (Zhuang et al., 2020)\nmk+1 = \u03b21 \u00b7mk + (1\u2212 \u03b21) \u00b7 g(xk),\n\u03bdk+1 = \u03b22 \u00b7 \u03bdk + (1\u2212 \u03b22) \u00b7 (g(xk)\u2212mk+1)2, gk+1 = mk+1\u221a \u03bdk+1 + \u03b4 ."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1818abeab5a2bef708ac1726af061c855d68588e",
                "externalIds": {
                    "CorpusId": 237396198
                },
                "corpusId": 237396198,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1818abeab5a2bef708ac1726af061c855d68588e",
                "title": "Measuring `\u221e Attacks by the `2 Norm",
                "abstract": "Deep Neural Networks (DNNs) could be easily fooled by Adversarial Examples (AEs) with the imperceptible difference to original samples in human eyes. To keep the difference imperceptible, the existing attacking bound the adversarial perturbations by the `\u221e norm, which is then served as the standard to align different attacks for a fair comparison. However, when investigating attack transferability, i.e., the capability of the AEs from attacking one surrogate DNN to cheat other black-box DNN, we find that ar X iv :2 10 2. 10 34 3v 2 [ cs .L G ] 1 S ep 2 02 1 only using the `\u221e norm is not sufficient to measure the attack strength, according to our comprehensive experiments concerning 7 transfer-based attacks, 4 white-box surrogate models, and 9 black-box victim models. Specifically, we find that the `2 norm greatly affects the transferability in `\u221e attacks. Since larger-perturbed AEs naturally bring about better transferability, we advocate that the strength of all attacks should be measured by both the widely used `\u221e and also the `2 norm. Despite the intuitiveness of our conclusion and advocacy, they are very necessary for the community, because common evaluations (bounding only the `\u221e norm) allow tricky enhancements of the \u201cattack transferability\u201d by increasing the \u201cattack strength\u201d (`2 norm) as shown by our simple counter-example method, and the good transferability of several existing methods may be due to their large `2 distances.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111636895",
                        "name": "Sizhe Chen"
                    },
                    {
                        "authorId": "2035647651",
                        "name": "Qinghua Tao"
                    },
                    {
                        "authorId": "17064832",
                        "name": "Zhixing Ye"
                    },
                    {
                        "authorId": "47932717",
                        "name": "Xiaolin Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "00ebcb1edee5b9132488e87284cbc6f15e661c9d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-12504",
                    "CorpusId": 237940355
                },
                "corpusId": 237940355,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/00ebcb1edee5b9132488e87284cbc6f15e661c9d",
                "title": "Curvature Injected Adaptive Momentum Optimizer for Convolutional Neural Networks",
                "abstract": "In this paper, we propose a new approach, hereafter referred as AdaInject, for the gradient descent optimizers by injecting the curvature information with adaptive momentum. Specifically, the curvature information is used as a weight to inject the second order moment in the update rule. The curvature information is captured through the short-term parameter history. The AdaInject approach boosts the parameter update by exploiting the curvature information. The proposed approach is generic in nature and can be integrated with any existing adaptive momentum stochastic gradient descent optimizers. The effectiveness of the AdaInject optimizer is tested using a theoretical analysis as well as through toy examples. We also show the convergence property of the proposed injection based optimizer. Further, we depict the efficacy of the AdaInject approach through extensive experiments in conjunction with the state-of-the-art optimizers, i.e., AdamInject, diffGradInject, RadamInject, and AdaBeliefInject on four benchmark datasets. Different CNN models are used in the experiments. A highest improvement in the top-1 classification error rate of 16.54% is observed using diffGradInject optimizer with ResNeXt29 model over the CIFAR10 dataset. Overall, we observe very promising performance improvement of existing optimizers with the proposed AdaInject approach.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34992579",
                        "name": "S. Dubey"
                    },
                    {
                        "authorId": "153037548",
                        "name": "S. H. Shabbeer Basha"
                    },
                    {
                        "authorId": "2108384213",
                        "name": "S. Singh"
                    },
                    {
                        "authorId": "1759420",
                        "name": "B. Chaudhuri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use a batch size of 12 and Adabelief [Zhuang et al., 2020] optimizer with a weight decay of 1e-4."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fc9eb8a85d70314a3d0276d0d5ac126f3f8586bf",
                "externalIds": {
                    "DBLP": "conf/nips/YanCD21",
                    "CorpusId": 245019393
                },
                "corpusId": 245019393,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fc9eb8a85d70314a3d0276d0d5ac126f3f8586bf",
                "title": "Skipping the Frame-Level: Event-Based Piano Transcription With Neural Semi-CRFs",
                "abstract": "Piano transcription systems are typically optimized to estimate pitch activity at each frame of audio. They are often followed by carefully designed heuristics and post-processing algorithms to estimate note events from the frame-level predictions. Recent methods have also framed piano transcription as a multi-task learning problem, where the activation of different stages of a note event are estimated independently. These practices are not well aligned with the desired outcome of the task, which is the specification of note intervals as holistic events, rather than the aggregation of disjoint observations. In this work, we propose a novel formulation of piano transcription, which is optimized to directly predict note events. Our method is based on Semi-Markov Conditional Random Fields (semi-CRF), which produce scores for intervals rather than individual frames. When formulating piano transcription in this way, we eliminate the need to rely on disjoint frame-level estimates for different stages of a note event. We conduct experiments on the MAESTRO dataset and demonstrate that the proposed model surpasses the current state-of-the-art for piano transcription. Our results suggest that the semiCRF output layer, while still quadratic in complexity, is a simple, fast and wellperforming solution for event-based prediction, and may lead to similar success in other areas which currently rely on frame-level estimates.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115359584",
                        "name": "Yujia Yan"
                    },
                    {
                        "authorId": "1397115189",
                        "name": "Frank Cwitkowitz"
                    },
                    {
                        "authorId": "3270912",
                        "name": "Z. Duan"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", \u2217, \u2020, \u2021 are respectively reported in [1], [23], [61], [62]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7b6aa14207eb54964fa97899460dc6367766b1ef",
                "externalIds": {
                    "DBLP": "conf/nips/ZhouYYFY21",
                    "CorpusId": 245121851
                },
                "corpusId": 245121851,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7b6aa14207eb54964fa97899460dc6367766b1ef",
                "title": "Towards Understanding Why Lookahead Generalizes Better Than SGD and Beyond",
                "abstract": "To train networks, lookahead algorithm [1] updates its fast weights k times via an inner-loop optimizer before updating its slow weights once by using the latest fast weights. Any optimizer, e.g. SGD, can serve as the inner-loop optimizer, and the derived lookahead generally enjoys remarkable test performance improvement over the vanilla optimizer. But theoretical understandings on the test performance improvement of lookahead remain absent yet. To solve this issue, we theoretically justify the advantages of lookahead in terms of the excess risk error which measures the test performance. Speci\ufb01cally, we prove that lookahead using SGD as its inner-loop optimizer can better balance the optimization error and generalization error to achieve smaller excess risk error than vanilla SGD on (strongly) convex problems and nonconvex problems with Polyak-\u0141ojasiewicz condition which has been observed/proved in neural networks. Moreover, we show the stagewise optimization strategy [2] which decays learning rate several times during training can also bene\ufb01t lookahead in improving its optimization and generalization errors on strongly convex problems. Finally, we propose a stagewise locally-regularized lookahead (SLRLA) algorithm which sums up the vanilla objective and a local regularizer to minimize at each stage and provably enjoys optimization and generalization improvement over the conventional (stagewise) lookahead. Experimental results on CIFAR10/100 and ImageNet testify its advantages. Codes is available at https://github.com/sail-sg/SLRLA-optimizer .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2153245275",
                        "name": "Pan Zhou"
                    },
                    {
                        "authorId": "143619442",
                        "name": "Hanshu Yan"
                    },
                    {
                        "authorId": "2115844592",
                        "name": "Xiao-Tong Yuan"
                    },
                    {
                        "authorId": "1698982",
                        "name": "Jiashi Feng"
                    },
                    {
                        "authorId": "143653681",
                        "name": "Shuicheng Yan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[34] to enhance the learning of wider representations with fewer parameters.",
                "In this study, TMwas based on the work by [34], with the attention mechanism described in [30]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "a1e84863e6abbc43ce3367ba303ca370cbfa0813",
                "externalIds": {
                    "CorpusId": 245495664
                },
                "corpusId": 245495664,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a1e84863e6abbc43ce3367ba303ca370cbfa0813",
                "title": "NovelPredictionModel forCOVID-19 inSaudiArabiaBasedonan LSTM Algorithm",
                "abstract": "'e rapid emergence of the novel SARS-CoV-2 poses a challenge and has attracted worldwide attention. Artificial intelligence (AI) can be used to combat this pandemic and control the spread of the virus. In particular, deep learning-based time-series techniques are used to predict worldwide COVID-19 cases for short-term and medium-term dependencies using adaptive learning. 'is study aimed to predict daily COVID-19 cases and investigate the critical factors that increase the transmission rate of this outbreak by examining different influential factors. Furthermore, the study analyzed the effectiveness of COVID-19 preventionmeasures. A fully connected deep neural network, long short-termmemory (LSTM), and transformermodel were used as the AImodels for the prediction of new COVID-19 cases. Initially, data preprocessing and feature extraction were performed using COVID-19 datasets from Saudi Arabia.'e performance metrics for all models were computed, and the results were subjected to comparative analysis to detect the most reliable model. Additionally, statistical hypothesis analysis and correlation analysis were performed on the COVID-19 datasets by including features such as daily mobility, total cases, people fully vaccinated per hundred, weekly hospital admissions per million, intensive care unit patients, and new deaths per million.'e results show that the LSTM algorithm had the highest accuracy of all the algorithms and an error of less than 2%. 'e findings of this study contribute to our understanding of COVID-19 containment. 'is study also provides insights into the prevention of future outbreaks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3066841",
                        "name": "Eman H. Alkhammash"
                    },
                    {
                        "authorId": "7399020",
                        "name": "H. Algethami"
                    },
                    {
                        "authorId": "51337869",
                        "name": "R. Alshahrani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AdaBelief optimizer [14] was used to train models with the learning rate of 0."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "84ddd1abe13adf16717dfe64939b2377798f562c",
                "externalIds": {
                    "CorpusId": 246076022
                },
                "corpusId": 246076022,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/84ddd1abe13adf16717dfe64939b2377798f562c",
                "title": "A COMBINATION OF VARIOUS NEURAL NETWORKS FOR SOUND EVENT LOCALIZATION AND DETECTION Technical Report",
                "abstract": "This technical report describes our approach to the DCASE 2021 task 3: Sound Event Localization and Detection (SELD). We propose a network architecture, a combination of various network layers, which can yield the optimal performance for the SELD task. Furthermore, we propose which augmentation techniques to use to boost the performance of our proposed model with a limited train dataset. In order to further improve the performance, several techniques were applied at training and post-processing stages, such as adaptive gradient clipping, ensemble techniques, and class-wise dynamic thresholds. Evaluation results on the development dataset showed that the proposed approach outperformed the existing baseline model of the task.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2017943901",
                        "name": "Daniel Rho"
                    },
                    {
                        "authorId": "2108155403",
                        "name": "Seungjin Lee"
                    },
                    {
                        "authorId": "2150662791",
                        "name": "Jinhyeok Park"
                    },
                    {
                        "authorId": "2155195741",
                        "name": "Taesoo Kim"
                    },
                    {
                        "authorId": "144419967",
                        "name": "Jiho Chang"
                    },
                    {
                        "authorId": "2813905",
                        "name": "J. Ko"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this implementation, the DAM is trained for 160 epochs using AdaBelief [11] optimizer."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1234d030ba105d8fae9fe34f21474f2ff19ddaf4",
                "externalIds": {
                    "CorpusId": 247613192
                },
                "corpusId": 247613192,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1234d030ba105d8fae9fe34f21474f2ff19ddaf4",
                "title": "Learning Compact Representations of Neural Networks using DiscriminAtive Masking (DAM)",
                "abstract": "A central goal in deep learning is to learn compact representations of features at every layer of a neural network, which is useful for both unsupervised representation learning and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of fine-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. We show that our proposed DAM approach has remarkably good performance over a diverse range of applications in representation learning and structured pruning, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification. We also theoretically show that the learning objective of DAM is directly related to minimizing the L0 norm of the masking layer. All of our codes and datasets are available https://github.com/jayroxis/ dam-pytorch.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49221613",
                        "name": "Jie Bu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ca82905710eeda6c12d8b3017589cb93de824135",
                "externalIds": {
                    "CorpusId": 251920951
                },
                "corpusId": 251920951,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ca82905710eeda6c12d8b3017589cb93de824135",
                "title": "R OBUST D EEP N EURAL N ETWORKS FOR H ETEROGENEOUS T ABULAR D ATA",
                "abstract": "Although deep neural networks (DNNs) constitute the state-of-the-art in many tasks based on image, audio, or text data, their performance on heterogeneous, tabular data is typically inferior to that of decision tree ensembles. To bridge the gap between the difficulty of DNNs to handle tabular data and leverage the flexibility of deep learning under input heterogeneity, we propose DeepTLF, a framework for deep tabular learning. The core idea of our method is to transform the heterogeneous input data into homogeneous data to boost the performance of DNNs considerably. For the transformation step, we develop a novel knowledge distillations approach, TreeDrivenEncoder, which exploits the structure of decision trees trained on the available heterogeneous data to map the original input vectors onto homogeneous vectors that a DNN can use to improve the predictive performance. Through extensive and challenging experiments on various real-world datasets, we demonstrate that the DeepTLF pipeline leads to higher predictive performance. On average, our framework shows 19.6% performance improvement in comparison to DNNs. The DeepTLF code is publicly available.",
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "191551b639a0028542a952a6e35dec17cfd9ed07",
                "externalIds": {
                    "DBLP": "journals/access/KhanJK21",
                    "DOI": "10.1109/ACCESS.2021.3096976",
                    "CorpusId": 236184361
                },
                "corpusId": 236184361,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/191551b639a0028542a952a6e35dec17cfd9ed07",
                "title": "Adadb: Adaptive Diff-Batch Optimization Technique for Gradient Descent",
                "abstract": "Gradient descent is the workhorse of deep neural networks. Gradient descent has the disadvantage of slow convergence. The famous way to overcome slow convergence is to use momentum. Momentum effectively increases the learning factor of gradient descent. Recently, many approaches have been proposed to control the momentum for better optimization towards global minima, such as Adam, diffGrad, and AdaBelief. Adam decreases the momentum by dividing it with square root of moving averages of squared past gradients or second moment. The sudden decrease in the second moment often results in the overshoot of the gradient from the minima and then settle at the closest minima. DiffGrad decreases this problem by using a friction constant based on the difference of current gradient and immediate past gradient in Adam. The friction constant further decreases the momentum and results in slow convergence. AdaBelief adapts the step size according to the belief in the current gradient direction. Another famous way of fast convergence is to increase the batch size adaptively. This paper proposes a new optimization technique named adaptive diff-batch or adadb that removes the problem of overshooting gradient in Adam, slow convergence in diffGrad, and combines the methods with adaptive batch size for further increase in convergence rate. The proposed technique uses the friction constant based on the past three differences of gradients rather than one as in diffGrad and a condition to decide the use of friction constant. The proposed technique has outperformed the Adam, diffGrad, and AdaBelief optimizers on synthetic complex non-convex functions and real-world datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115773771",
                        "name": "M. U. S. Khan"
                    },
                    {
                        "authorId": "48952500",
                        "name": "M. Jawad"
                    },
                    {
                        "authorId": "1740261",
                        "name": "S. Khan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a2b9e95f13f5ea8f5b4d3bb4f913b2ddafa96df3",
                "externalIds": {
                    "DBLP": "journals/access/ZhuI21",
                    "DOI": "10.1109/ACCESS.2021.3120749",
                    "CorpusId": 240004738
                },
                "corpusId": 240004738,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a2b9e95f13f5ea8f5b4d3bb4f913b2ddafa96df3",
                "title": "Unified Algorithm Framework for Nonconvex Stochastic Optimization in Deep Neural Networks",
                "abstract": "This paper presents a unified algorithmic framework for nonconvex stochastic optimization, which is needed to train deep neural networks. The unified algorithm includes the existing adaptive-learning-rate optimization algorithms, such as Adaptive Moment Estimation (Adam), Adaptive Mean Square Gradient (AMSGrad), Adam with weighted gradient and dynamic bound of learning rate (GWDC), AMSGrad with weighted gradient and dynamic bound of learning rate (AMSGWDC), and Adapting stepsizes by the belief in observed gradients (AdaBelief). The paper also gives convergence analyses of the unified algorithm for constant and diminishing learning rates. When using a constant learning rate, the algorithm can approximate a stationary point of a nonconvex stochastic optimization problem. When using a diminishing rate, it converges to a stationary point of the problem. Hence, the analyses lead to the finding that the existing adaptive-learning-rate optimization algorithms can be applied to nonconvex stochastic optimization in deep neural networks in theory. Additionally, this paper provides numerical results showing that the unified algorithm can train deep neural networks in practice. Moreover, it provides numerical comparisons for unconstrained minimization using benchmark functions of the unified algorithm with certain heuristic intelligent optimization algorithms. The numerical comparisons show that a teaching-learning-based optimization algorithm and the unified algorithm perform well.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48269686",
                        "name": "Yini Zhu"
                    },
                    {
                        "authorId": "2018304",
                        "name": "H. Iiduka"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fbf6ed7873160b7b57b20551a60e933ca093492b",
                "externalIds": {
                    "CorpusId": 259343794
                },
                "corpusId": 259343794,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fbf6ed7873160b7b57b20551a60e933ca093492b",
                "title": "A Cascaded Speech Enhancement for Hearing Aids in Noisy-Reverberant Conditions",
                "abstract": "Hearing aid users suffer from poor listening experiences under noise and reverberation. This paper introduces a cascaded speech enhancement system to improve the intelligibility and perception of hearing impairments in noisy-reverberant environments. The system consists of three main parts: a deep learning-based noise reduction, a weighted prediction error-based dereverberation, and a personalized dynamic equalization. The proposed enhancement method is in cooperation with a hearing aid simulator for objective and subjective evaluations. In terms of modi\ufb01ed binaural short-time objective intelligibility (MBSTOI), the proposed method outperforms the baseline on the test dataset in different noisy and reverberant conditions. The subjective listening test shows that our scheme obtains a lower word recognition rate under noise and speech interference than other teams.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2221126364",
                        "name": "Clarity"
                    },
                    {
                        "authorId": "2145309576",
                        "name": "Xi Chen"
                    },
                    {
                        "authorId": "48081503",
                        "name": "Yupeng Shi"
                    },
                    {
                        "authorId": "2093592031",
                        "name": "Wei Xiao"
                    },
                    {
                        "authorId": "2406948",
                        "name": "Tingzhao Wu"
                    },
                    {
                        "authorId": "47446699",
                        "name": "Meng Wang"
                    },
                    {
                        "authorId": "2052100290",
                        "name": "Shidong Shang"
                    },
                    {
                        "authorId": "35813117",
                        "name": "Nengheng Zheng"
                    },
                    {
                        "authorId": "12675852",
                        "name": "Q. Meng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several works try to improve the performance of adaptive optimization algorithms such as AdamW (Loshchilov & Hutter, 2018), AdaBound (Luo et al., 2018), AdaBelief (Zhuang et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e90092fc12b45344c01ea650f2a4d43bbbe1f2bf",
                "externalIds": {
                    "CorpusId": 236909609
                },
                "corpusId": 236909609,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e90092fc12b45344c01ea650f2a4d43bbbe1f2bf",
                "title": "The Implicit Regularization for Adaptive Optimization Algorithms on Homogeneous Neural Networks",
                "abstract": "Despite their overwhelming capacity to overfit, deep neural networks trained by specific optimization algorithms tend to generalize well to unseen data. Recently, researchers explained it by investigating the implicit regularization effect of optimization algorithms. A remarkable progress is the work (Lyu & Li, 2019), which proves gradient descent (GD) maximizes the margin of homogeneous deep neural networks. Except GD, adaptive algorithms such as AdaGrad, RMSProp and Adam are popular owing to their rapid training process. However, theoretical guarantee for the generalization of adaptive optimization algorithms is still lacking. In this paper, we study the implicit regularization of adaptive optimization algorithms when they are optimizing the logistic loss on homogeneous deep neural networks. We prove that adaptive algorithms that adopt exponential moving average strategy in conditioner (such as Adam and RMSProp) can maximize the margin of the neural network, while AdaGrad that directly sums historical squared gradients in conditioner can not. It indicates superiority on generalization of exponential moving average strategy in the design of the conditioner. Technically, we provide a unified framework to analyze convergent direction of adaptive optimization algorithms by constructing novel adaptive gradient flow and surrogate margin. Our experiments can well support the theoretical findings on convergent direction of adaptive optimization algorithms.",
                "year": null,
                "authors": [
                    {
                        "authorId": "1390816671",
                        "name": "Bohan Wang"
                    },
                    {
                        "authorId": "47580728",
                        "name": "Qi Meng"
                    },
                    {
                        "authorId": "2154939268",
                        "name": "Wei Chen"
                    },
                    {
                        "authorId": "2110264337",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "251723490",
                "publicationVenue": null,
                "url": null,
                "title": "An Adam Convergence Analysis",
                "abstract": null,
                "year": 2022,
                "authors": []
            }
        }
    ]
}