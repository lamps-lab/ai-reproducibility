{
    "offset": 0,
    "data": [
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[20] Matthew O\u2019Shaughnessy, Gregory Canal, Marissa Connor, Christopher Rozell, and Mark Davenport."
            ],
            "citingPaper": {
                "paperId": "0f59aa3f9ff8e070c828c0a36886aeb4bc3aed5c",
                "externalIds": {
                    "ArXiv": "2309.16928",
                    "CorpusId": 263310415
                },
                "corpusId": 263310415,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0f59aa3f9ff8e070c828c0a36886aeb4bc3aed5c",
                "title": "Learning to Receive Help: Intervention-Aware Concept Embedding Models",
                "abstract": "Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories at train-time. This conditions IntCEMs to effectively select and receive concept interventions when deployed at test-time. Our experiments show that IntCEMs significantly outperform state-of-the-art concept-interpretable models when provided with test-time concept interventions, demonstrating the effectiveness of our approach.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "13201073",
                        "name": "M. Zarlenga"
                    },
                    {
                        "authorId": "2055306799",
                        "name": "Katherine M. Collins"
                    },
                    {
                        "authorId": "1729912",
                        "name": "Krishnamurthy Dvijotham"
                    },
                    {
                        "authorId": "145689461",
                        "name": "Adrian Weller"
                    },
                    {
                        "authorId": "34591435",
                        "name": "Z. Shams"
                    },
                    {
                        "authorId": "1708741",
                        "name": "M. Jamnik"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "782c75e244b28233ff070737d545db352eed7d80",
                "externalIds": {
                    "PubMedCentral": "10550829",
                    "DOI": "10.1038/s41586-023-06541-3",
                    "CorpusId": 262085881,
                    "PubMed": "37730990"
                },
                "corpusId": 262085881,
                "publicationVenue": {
                    "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                    "name": "Nature",
                    "type": "journal",
                    "issn": "0028-0836",
                    "url": "https://www.nature.com/",
                    "alternate_urls": [
                        "http://www.nature.com/nature/",
                        "https://www.nature.com/nature/",
                        "http://www.nature.com/nature/archive/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/782c75e244b28233ff070737d545db352eed7d80",
                "title": "Cingulate dynamics track depression recovery with deep brain stimulation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47750013",
                        "name": "Sankaraleengam (Sankar) Alagapan"
                    },
                    {
                        "authorId": "145306000",
                        "name": "K. Choi"
                    },
                    {
                        "authorId": "2243441394",
                        "name": "Stephen Heisig"
                    },
                    {
                        "authorId": "1398307278",
                        "name": "P. Riva-Posse"
                    },
                    {
                        "authorId": "145810331",
                        "name": "A. Crowell"
                    },
                    {
                        "authorId": "2626949",
                        "name": "V. Tiruvadi"
                    },
                    {
                        "authorId": "1679867138",
                        "name": "Mosadoluwa Obatusin"
                    },
                    {
                        "authorId": "3968846",
                        "name": "A. Veerakumar"
                    },
                    {
                        "authorId": "2243428987",
                        "name": "Allison C Waters"
                    },
                    {
                        "authorId": "2243431153",
                        "name": "Robert E Gross"
                    },
                    {
                        "authorId": "13155081",
                        "name": "Sin\u00e9ad Quinn"
                    },
                    {
                        "authorId": "32824523",
                        "name": "Lydia Denison"
                    },
                    {
                        "authorId": "2243444606",
                        "name": "Matthew O'Shaughnessy"
                    },
                    {
                        "authorId": "73753677",
                        "name": "Marissa Connor"
                    },
                    {
                        "authorId": "49440303",
                        "name": "Gregory H. Canal"
                    },
                    {
                        "authorId": "38247487",
                        "name": "J. Cha"
                    },
                    {
                        "authorId": "2243453397",
                        "name": "Rachel Hershenberg"
                    },
                    {
                        "authorId": "6416523",
                        "name": "T. Nauvel"
                    },
                    {
                        "authorId": "51266128",
                        "name": "Faical Isbaine"
                    },
                    {
                        "authorId": "2184608345",
                        "name": "M. Afzal"
                    },
                    {
                        "authorId": "2855516",
                        "name": "M. Figee"
                    },
                    {
                        "authorId": "16718273",
                        "name": "B. Kopell"
                    },
                    {
                        "authorId": "47078511",
                        "name": "R. Butera"
                    },
                    {
                        "authorId": "2245080",
                        "name": "H. Mayberg"
                    },
                    {
                        "authorId": "2145678648",
                        "name": "C. Rozell"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "9d8e5e44edb85302e246a38fa354d5d3f97f407f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-06248",
                    "ArXiv": "2308.06248",
                    "DOI": "10.48550/arXiv.2308.06248",
                    "CorpusId": 260866112
                },
                "corpusId": 260866112,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9d8e5e44edb85302e246a38fa354d5d3f97f407f",
                "title": "FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods",
                "abstract": "The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural models. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its automatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evaluation protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explanations into a common space of part importances, we can analyze a variety of different explanation types in a single common framework. Using our tools, we report results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2230789042",
                        "name": "Robin Hesse"
                    },
                    {
                        "authorId": "1412432168",
                        "name": "Simone Schaub-Meyer"
                    },
                    {
                        "authorId": "145920814",
                        "name": "S. Roth"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Although there are some works [28, 37] that can be used to extract causal explanations, they often make strict assumptions about the underlying data format, so they cannot be compared fairly, and we put the comparison in Appendix E."
            ],
            "citingPaper": {
                "paperId": "26f6a4fd94a6cf4aef6a1acbfcbc336e9ad7097e",
                "externalIds": {
                    "DBLP": "conf/kdd/WuWL0C23",
                    "DOI": "10.1145/3580305.3599240",
                    "CorpusId": 259282277
                },
                "corpusId": 259282277,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/26f6a4fd94a6cf4aef6a1acbfcbc336e9ad7097e",
                "title": "A Causality Inspired Framework for Model Interpretation",
                "abstract": "This paper introduces a unified causal lens for understanding representative model interpretation methods. We show that their explanation scores align with the concept of average treatment effect in causal inference, which allows us to evaluate their relative strengths and limitations from a unified causal perspective. Based on our observations, we outline the major challenges in applying causal inference to model interpretation, including identifying common causes that can be generalized across instances and ensuring that explanations provide a complete causal explanation of model predictions. We then present CIMI, a Causality-Inspired Model Interpreter, which addresses these challenges. Our experiments show that CIMI provides more faithful and generalizable explanations with improved sampling efficiency, making it particularly suitable for larger pretrained models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2000383268",
                        "name": "Chenwang Wu"
                    },
                    {
                        "authorId": "2108045320",
                        "name": "Xiting Wang"
                    },
                    {
                        "authorId": "1862782",
                        "name": "Defu Lian"
                    },
                    {
                        "authorId": "2110971997",
                        "name": "Xing Xie"
                    },
                    {
                        "authorId": "2113754294",
                        "name": "Enhong Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Recent works (Pearl 2009; O\u2019Shaughnessy et al. 2020) introduce the information flow to measure the causal influence of the learned representation on the output of the predictor.",
                "2021a), and causal explanations (O\u2019Shaughnessy et al. 2020; Holzinger et al. 2022), etc."
            ],
            "citingPaper": {
                "paperId": "1f1b6f5dcd5f802f2782ef15629be57202e3b629",
                "externalIds": {
                    "DBLP": "conf/aaai/SunSJ023",
                    "DOI": "10.1609/aaai.v37i2.25334",
                    "CorpusId": 259581127
                },
                "corpusId": 259581127,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1f1b6f5dcd5f802f2782ef15629be57202e3b629",
                "title": "Learning Event-Relevant Factors for Video Anomaly Detection",
                "abstract": "Most video anomaly detection methods discriminate events that deviate from normal patterns as anomalies. However, these methods are prone to interferences from event-irrelevant factors, such as background textures and object scale variations, incurring an increased false detection rate. In this paper, we propose to explicitly learn event-relevant factors to eliminate the interferences from event-irrelevant factors on anomaly predictions. To this end, we introduce a causal generative model to separate the event-relevant factors and event-irrelevant ones in videos, and learn the prototypes of event-relevant factors in a memory augmentation module. We design a causal objective function to optimize the causal generative model and develop a counterfactual learning strategy to guide anomaly predictions, which increases the influence of the event-relevant factors. The extensive experiments show the effectiveness of our method for video anomaly detection.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "41172535",
                        "name": "Che Sun"
                    },
                    {
                        "authorId": "2195668886",
                        "name": "Chenrui Shi"
                    },
                    {
                        "authorId": "7415267",
                        "name": "Yunde Jia"
                    },
                    {
                        "authorId": "150352923",
                        "name": "Yuwei Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "7eddac2807fa24901625b680bf7f2eec650e555a",
                "externalIds": {
                    "DOI": "10.1002/gcc.23177",
                    "CorpusId": 259153587,
                    "PubMed": "37314068"
                },
                "corpusId": 259153587,
                "publicationVenue": {
                    "id": "e1f5462e-d422-4897-a279-6ed82add8fcb",
                    "name": "Genes, Chromosomes and Cancer",
                    "type": "journal",
                    "alternate_names": [
                        "Gene Chromosom Cancer"
                    ],
                    "issn": "1045-2257",
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/38250",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/10982264",
                        "http://www3.interscience.wiley.com/journal/38250/home"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7eddac2807fa24901625b680bf7f2eec650e555a",
                "title": "Machine learning in computational histopathology: Challenges and opportunities",
                "abstract": "Digital histopathological images, high\u2010resolution images of stained tissue samples, are a vital tool for clinicians to diagnose and stage cancers. The visual analysis of patient state based on these images are an important part of oncology workflow. Although pathology workflows have historically been conducted in laboratories under a microscope, the increasing digitization of histopathological images has led to their analysis on computers in the clinic. The last decade has seen the emergence of machine learning, and deep learning in particular, a powerful set of tools for the analysis of histopathological images. Machine learning models trained on large datasets of digitized histopathology slides have resulted in automated models for prediction and stratification of patient risk. In this review, we provide context for the rise of such models in computational histopathology, highlight the clinical tasks they have found success in automating, discuss the various machine learning techniques that have been applied to this domain, and underscore open problems and opportunities.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2170575070",
                        "name": "Michael Cooper"
                    },
                    {
                        "authorId": "121500114",
                        "name": "Zongliang Ji"
                    },
                    {
                        "authorId": "2066130378",
                        "name": "R. Krishnan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "e2df2e85960cde1b2fff49c489f27f2bede25b55",
                "externalIds": {
                    "ArXiv": "2305.18362",
                    "DBLP": "conf/ijcai/XuFAS23",
                    "DOI": "10.48550/arXiv.2305.18362",
                    "CorpusId": 258967383
                },
                "corpusId": 258967383,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e2df2e85960cde1b2fff49c489f27f2bede25b55",
                "title": "Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs",
                "abstract": "A concept-based classifier can explain the decision process of a deep learning model by human understandable concepts in image classification problems. However, sometimes concept-based explanations may cause false positives, which misregards unrelated concepts as important for the prediction task. Our goal is to find the statistically significant concept for classification to prevent misinterpretation. In this study, we propose a method using a deep learning model to learn the image concept and then using the knockoff sample to select the important concepts for prediction by controlling the False Discovery Rate (FDR) under a certain value. We evaluate the proposed method in our experiments on both synthetic and real data. Also, it shows that our method can control the FDR properly while selecting highly interpretable concepts to improve the trustworthiness of the model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144464012",
                        "name": "Kaiwen Xu"
                    },
                    {
                        "authorId": "16348694",
                        "name": "Kazuto Fukuchi"
                    },
                    {
                        "authorId": "1721701",
                        "name": "Youhei Akimoto"
                    },
                    {
                        "authorId": "1733719",
                        "name": "Jun Sakuma"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "For instance, a similar relationship between data structure and feature redundancy has been reported in various domains, including time series Radovic et al. [2017] and graph structures Liu et al."
            ],
            "citingPaper": {
                "paperId": "6a8ec7b39423d912ed58ea9d9911eec0a91d8c43",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-13836",
                    "ArXiv": "2304.13836",
                    "DOI": "10.48550/arXiv.2304.13836",
                    "CorpusId": 258352437
                },
                "corpusId": 258352437,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6a8ec7b39423d912ed58ea9d9911eec0a91d8c43",
                "title": "On Pitfalls of RemOve-And-Retrain: Data Processing Inequality Perspective",
                "abstract": "Approaches for appraising feature importance approximations, alternatively referred to as attribution methods, have been established across an extensive array of contexts. The development of resilient techniques for performance benchmarking constitutes a critical concern in the sphere of explainable deep learning. This study scrutinizes the dependability of the RemOve-And-Retrain (ROAR) procedure, which is prevalently employed for gauging the performance of feature importance estimates. The insights gleaned from our theoretical foundation and empirical investigations reveal that attributions containing lesser information about the decision function may yield superior results in ROAR benchmarks, contradicting the original intent of ROAR. This occurrence is similarly observed in the recently introduced variant RemOve-And-Debias (ROAD), and we posit a persistent pattern of blurriness bias in ROAR attribution metrics. Our findings serve as a warning against indiscriminate use on ROAR metrics. The code is available as open source.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217315691",
                        "name": "J. Song"
                    },
                    {
                        "authorId": "32665890",
                        "name": "Keumgang Cha"
                    },
                    {
                        "authorId": "11027268",
                        "name": "Junghoon Seo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "edcc75fbdbfea988c759df08e1f3e42f9621e0db",
                "externalIds": {
                    "DBLP": "journals/eswa/ZhangCWL23",
                    "DOI": "10.1016/j.eswa.2023.120214",
                    "CorpusId": 258307893
                },
                "corpusId": 258307893,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/edcc75fbdbfea988c759df08e1f3e42f9621e0db",
                "title": "Density-based reliable and robust explainer for counterfactual explanation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2157987371",
                        "name": "Songming Zhang"
                    },
                    {
                        "authorId": "2155500094",
                        "name": "Xiaofeng Chen"
                    },
                    {
                        "authorId": "2131811021",
                        "name": "Shiping Wen"
                    },
                    {
                        "authorId": "2145408261",
                        "name": "Zhongshan Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Neural network (NN) models enriched with causal knowledge have demonstrated their ability to achieve robustness [36], invariance [27, 9], and provide interpretable explanations for human understanding [3, 26, 17].",
                "In training such NN models imbued with causal knowledge, two primary tasks emerge: (1) acquiring a comprehension of causal relationships between input and output neurons [15, 21, 17], and (2) validating and explaining the acquired causal relationships [3, 16, 26]."
            ],
            "citingPaper": {
                "paperId": "4632627d57898278804cabd5ef5b600e9d82aba8",
                "externalIds": {
                    "ArXiv": "2303.13850",
                    "CorpusId": 257756923
                },
                "corpusId": 257756923,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4632627d57898278804cabd5ef5b600e9d82aba8",
                "title": "Towards Learning and Explaining Indirect Causal Effects in Neural Networks",
                "abstract": "Recently, there has been a growing interest in learning and explaining causal effects within Neural Network (NN) models. By virtue of NN architectures, previous approaches consider only direct and total causal effects assuming independence among input variables. We view an NN as a structural causal model (SCM) and extend our focus to include indirect causal effects by introducing feedforward connections among input neurons. We propose an ante-hoc method that captures and maintains direct, indirect, and total causal effects during NN model training. We also propose an algorithm for quantifying learned causal effects in an NN model and efficient approximation strategies for quantifying causal effects in high-dimensional data. Extensive experiments conducted on synthetic and real-world datasets demonstrate that the causal effects learned by our ante-hoc method better approximate the ground truth effects compared to existing methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2212649843",
                        "name": "Abbaavaram Gowtham Reddy"
                    },
                    {
                        "authorId": "2052114813",
                        "name": "Saketh Bachu"
                    },
                    {
                        "authorId": "48560783",
                        "name": "Harsh Nilesh Pathak"
                    },
                    {
                        "authorId": "123684147",
                        "name": "Ben Godfrey"
                    },
                    {
                        "authorId": "1699429",
                        "name": "V. Balasubramanian"
                    },
                    {
                        "authorId": "71015765",
                        "name": "V. Varshaneya"
                    },
                    {
                        "authorId": "2212537008",
                        "name": "Satya Narayanan Kar"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "They often employ causal graphs, gradient-descent, discriminative and evolutionary algorithms to generate contrastive examples (CEs) while satisfying feasibility constraints (Ustun, Spangher, and Liu 2019; O\u2019Shaughnessy et al. 2020; Goyal et al. 2019)."
            ],
            "citingPaper": {
                "paperId": "38819202e99e5174d15be8521727d711bf1b4629",
                "externalIds": {
                    "ArXiv": "2301.07941",
                    "DBLP": "journals/corr/abs-2301-07941",
                    "DOI": "10.48550/arXiv.2301.07941",
                    "CorpusId": 255999810
                },
                "corpusId": 255999810,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/38819202e99e5174d15be8521727d711bf1b4629",
                "title": "CEnt: An Entropy-based Model-agnostic Explainability Framework to Contrast Classifiers' Decisions",
                "abstract": "Current interpretability methods focus on explaining a particular model's decision through present input features. Such methods do not inform the user of the sufficient conditions that alter these decisions when they are not desirable. Contrastive explanations circumvent this problem by providing explanations of the form\"If the feature $X>x$, the output $Y$ would be different''. While different approaches are developed to find contrasts; these methods do not all deal with mutability and attainability constraints. In this work, we present a novel approach to locally contrast the prediction of any classifier. Our Contrastive Entropy-based explanation method, CEnt, approximates a model locally by a decision tree to compute entropy information of different feature splits. A graph, G, is then built where contrast nodes are found through a one-to-many shortest path search. Contrastive examples are generated from the shortest path to reflect feature splits that alter model decisions while maintaining lower entropy. We perform local sampling on manifold-like distances computed by variational auto-encoders to reflect data density. CEnt is the first non-gradient-based contrastive method generating diverse counterfactuals that do not necessarily exist in the training data while satisfying immutability (ex. race) and semi-immutability (ex. age can only change in an increasing direction). Empirical evaluation on four real-world numerical datasets demonstrates the ability of CEnt in generating counterfactuals that achieve better proximity rates than existing methods without compromising latency, feasibility, and attainability. We further extend CEnt to imagery data to derive visually appealing and useful contrasts between class labels on MNIST and Fashion MNIST datasets. Finally, we show how CEnt can serve as a tool to detect vulnerabilities of textual classifiers.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "8160667",
                        "name": "Julia El Zini"
                    },
                    {
                        "authorId": "50245581",
                        "name": "Mohamad Mansour"
                    },
                    {
                        "authorId": "144707373",
                        "name": "M. Awad"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In practice, one can also use the MI term I(\u03b1;Y ) as suggested in O\u2019Shaughnessy et al. (2020).",
                "Note that, although we do not use \u201cdo\u201d operator to introduce intervention, O\u2019Shaughnessy et al. (2020) shows that I (\u03b1\u2192 Y |do (\u03b2)) = I (\u03b1;Y |\u03b2) from the rules of do-calculus."
            ],
            "citingPaper": {
                "paperId": "108c960f7d8dafdbc4c41ab7e23610a5959ae2fc",
                "externalIds": {
                    "ArXiv": "2301.01642",
                    "DBLP": "journals/corr/abs-2301-01642",
                    "DOI": "10.48550/arXiv.2301.01642",
                    "CorpusId": 255415863
                },
                "corpusId": 255415863,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/108c960f7d8dafdbc4c41ab7e23610a5959ae2fc",
                "title": "CI-GNN: A Granger Causality-Inspired Graph Neural Network for Interpretable Brain Network-Based Psychiatric Diagnosis",
                "abstract": "There is a recent trend to leverage the power of graph neural networks (GNNs) for brain-network based psychiatric diagnosis, which,in turn, also motivates an urgent need for psychiatrists to fully understand the decision behavior of the used GNNs. However, most of the existing GNN explainers are either post-hoc in which another interpretive model needs to be created to explain a well-trained GNN, or do not consider the causal relationship between the extracted explanation and the decision, such that the explanation itself contains spurious correlations and suffers from weak faithfulness. In this work, we propose a granger causality-inspired graph neural network (CI-GNN), a built-in interpretable model that is able to identify the most influential subgraph (i.e., functional connectivity within brain regions) that is causally related to the decision (e.g., major depressive disorder patients or healthy controls), without the training of an auxillary interpretive network. CI-GNN learns disentangled subgraph-level representations {\\alpha} and \\b{eta} that encode, respectively, the causal and noncausal aspects of original graph under a graph variational autoencoder framework, regularized by a conditional mutual information (CMI) constraint. We theoretically justify the validity of the CMI regulation in capturing the causal relationship. We also empirically evaluate the performance of CI-GNN against three baseline GNNs and four state-of-the-art GNN explainers on synthetic data and three large-scale brain disease datasets. We observe that CI-GNN achieves the best performance in a wide range of metrics and provides more reliable and concise explanations which have clinical evidence.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152804328",
                        "name": "Kaizhong Zheng"
                    },
                    {
                        "authorId": "2462771",
                        "name": "Shujian Yu"
                    },
                    {
                        "authorId": "2199175116",
                        "name": "Ba-dong Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[30] developed a method for causal explaining black-box models.",
                "Simplifying causal metric for training the selector: We choose RED, which is an information theoretic causal strength measure to train the selector as it satisfies: \uf0b7 It can capture the non-linear, and complex relationship between input and output variables that is common in black-box DL models [30]."
            ],
            "citingPaper": {
                "paperId": "309a92ce69b83c4685f6a6b653d65bf9af463039",
                "externalIds": {
                    "DOI": "10.1109/ICSMD57530.2022.10058059",
                    "CorpusId": 257519061
                },
                "corpusId": 257519061,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/309a92ce69b83c4685f6a6b653d65bf9af463039",
                "title": "Instance-Wise Causal Feature Selection Explainer for Rotating Machinery Fault Diagnosis",
                "abstract": "Artificial neural networks in prognostics and health management (PHM), especially in intelligent fault diagnosis (IFD) have made great progress but possess black-box nature, leading to lack of interpretability and weak robustness when facing complex environment variations. When environment changes, the model tends to make wrong decisions leading to a cost, especially for major equipment if easily trusted by the users. Researchers have made studies on eXplainable Artificial Intelligence (XAI) based IFD to better understand the models. Most of them express their interpretability in the way of drawing gradient-based saliency maps to show where the model focuses on, which is of little consideration for causal effect and not sparse enough without quantitative metrics. To address these issues, we design an XAI method that utilizes a neural network as an instance-wise feature selector to select frequency bands that have stronger causal strength with the diagnosis result than others and further explain the diagnosis model. We quantify causal strength with the relative entropy distance (RED) and treat the simplified RED as the objective function for the optimization of the selector model. Finally, our experiments demonstrate the superiority of our method over another algorithm L2X measured by post-hoc accuracy (PHA), variant average causal effect (ACE), and vision plots.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2211594615",
                        "name": "Chang Guo"
                    },
                    {
                        "authorId": "2128012430",
                        "name": "Zuogang Shang"
                    },
                    {
                        "authorId": "32222804",
                        "name": "Jiaxin Ren"
                    },
                    {
                        "authorId": "48634325",
                        "name": "Zhibin Zhao"
                    },
                    {
                        "authorId": "1906484",
                        "name": "Shibin Wang"
                    },
                    {
                        "authorId": "46772519",
                        "name": "Xuefeng Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Debugging and Explainability: Most of the existing works on explainability of deep networks focus on inspecting the decisions for a single image [2,8,9,15,33,37,40,41,53,56, 62,63,65,69]."
            ],
            "citingPaper": {
                "paperId": "8aa9f90d8bb3cebc6362eae1e4a764b44f697b60",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-09859",
                    "ArXiv": "2211.09859",
                    "DOI": "10.48550/arXiv.2211.09859",
                    "CorpusId": 253708121
                },
                "corpusId": 253708121,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8aa9f90d8bb3cebc6362eae1e4a764b44f697b60",
                "title": "Data-Centric Debugging: mitigating model failures via targeted data collection",
                "abstract": "Deep neural networks can be unreliable in the real world when the training set does not adequately cover all the settings where they are deployed. Focusing on image classification, we consider the setting where we have an error distribution $\\mathcal{E}$ representing a deployment scenario where the model fails. We have access to a small set of samples $\\mathcal{E}_{sample}$ from $\\mathcal{E}$ and it can be expensive to obtain additional samples. In the traditional model development framework, mitigating failures of the model in $\\mathcal{E}$ can be challenging and is often done in an ad hoc manner. In this paper, we propose a general methodology for model debugging that can systemically improve model performance on $\\mathcal{E}$ while maintaining its performance on the original test set. Our key assumption is that we have access to a large pool of weakly (noisily) labeled data $\\mathcal{F}$. However, naively adding $\\mathcal{F}$ to the training would hurt model performance due to the large extent of label noise. Our Data-Centric Debugging (DCD) framework carefully creates a debug-train set by selecting images from $\\mathcal{F}$ that are perceptually similar to the images in $\\mathcal{E}_{sample}$. To do this, we use the $\\ell_2$ distance in the feature space (penultimate layer activations) of various models including ResNet, Robust ResNet and DINO where we observe DINO ViTs are significantly better at discovering similar images compared to Resnets. Compared to LPIPS, we find that our method reduces compute and storage requirements by 99.58\\%. Compared to the baselines that maintain model performance on the test set, we achieve significantly (+9.45\\%) improved results on the debug-heldout sets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Sahil Singla"
                    },
                    {
                        "authorId": "2188780004",
                        "name": "A. Chegini"
                    },
                    {
                        "authorId": "104644443",
                        "name": "Mazda Moayeri"
                    },
                    {
                        "authorId": "2191521621",
                        "name": "Soheil Feiz"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "7a42c0c6d231985f2b17a78616e6680bef545434",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-03113",
                    "ArXiv": "2207.03113",
                    "DOI": "10.48550/arXiv.2207.03113",
                    "CorpusId": 250334344
                },
                "corpusId": 250334344,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7a42c0c6d231985f2b17a78616e6680bef545434",
                "title": "An Additive Instance-Wise Approach to Multi-class Model Interpretation",
                "abstract": "Interpretable machine learning offers insights into what factors drive a certain prediction of a black-box system. A large number of interpreting methods focus on identifying explanatory input features, which generally fall into two main categories: attribution and selection. A popular attribution-based approach is to exploit local neighborhoods for learning instance-specific explainers in an additive manner. The process is thus inefficient and susceptible to poorly-conditioned samples. Meanwhile, many selection-based methods directly optimize local feature distributions in an instance-wise training framework, thereby being capable of leveraging global information from other inputs. However, they can only interpret single-class predictions and many suffer from inconsistency across different settings, due to a strict reliance on a pre-defined number of features selected. This work exploits the strengths of both methods and proposes a framework for learning local explanations simultaneously for multiple target classes. Our model explainer significantly outperforms additive and instance-wise counterparts on faithfulness with more compact and comprehensible explanations. We also demonstrate the capacity to select stable and important features through extensive experiments on various data sets and black-box model architectures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Vy Vo"
                    },
                    {
                        "authorId": "2147319231",
                        "name": "Van-Anh Nguyen"
                    },
                    {
                        "authorId": "145301586",
                        "name": "Trung Le"
                    },
                    {
                        "authorId": "2536742",
                        "name": "Quan Hung Tran"
                    },
                    {
                        "authorId": "2561045",
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "authorId": "1811469",
                        "name": "S. \u00c7amtepe"
                    },
                    {
                        "authorId": "1400659302",
                        "name": "Dinh Q. Phung"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "At the same time, the language of causality is advocated as a precise and powerful way of extracting explanations [18].",
                "We follow an information-theoretical approach to measure the flow of information [18] to quantify faithfulness."
            ],
            "citingPaper": {
                "paperId": "299d9193e2843c0bed4513d4a647f19206bebb2d",
                "externalIds": {
                    "ArXiv": "2207.01917",
                    "DBLP": "journals/corr/abs-2207-01917",
                    "DOI": "10.48550/arXiv.2207.01917",
                    "CorpusId": 250280171
                },
                "corpusId": 250280171,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/299d9193e2843c0bed4513d4a647f19206bebb2d",
                "title": "GLANCE: Global to Local Architecture-Neutral Concept-based Explanations",
                "abstract": "Most of the current explainability techniques focus on capturing the importance of features in input space. However, given the complexity of models and data-generating processes, the resulting explanations are far from being \u2018complete\u2019, in that they lack an indication of feature interactions and visualization of their \u2018effect\u2019. In this work, we propose a novel twin-surrogate explainability framework to explain the decisions made by any CNN-based image classi\ufb01er (irrespective of the architec-ture). For this, we \ufb01rst disentangle latent features from the classi\ufb01er, followed by aligning these features to observed/human-de\ufb01ned \u2018context\u2019 features. These aligned features form semantically meaningful concepts that are used for extracting a causal graph depicting the \u2018perceived\u2019 data-generating process, describing the inter- and intra-feature interactions between unobserved latent features and observed \u2018context\u2019 features. This causal graph serves as a global model from which local explanations of different forms can be extracted. Speci\ufb01cally, we provide a generator to visualize the \u2018effect\u2019 of interactions among features in latent space and draw feature importance therefrom as local explanations. Our framework utilizes adversarial knowledge distillation to faithfully learn a representation from the classi\ufb01ers\u2019 latent space and use it for extracting visual explanations. We use the styleGAN-v2 architecture with an additional regularization term to enforce disentanglement and alignment. We demonstrate and evaluate explanations obtained with our framework on Morpho-MNIST and on the FFHQ human faces dataset. Our framework is available at https://github.com/koriavinash1/GLANCE-Explanations",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35982249",
                        "name": "A. Kori"
                    },
                    {
                        "authorId": "1709824",
                        "name": "Ben Glocker"
                    },
                    {
                        "authorId": "49973505",
                        "name": "Francesca Toni"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026the behaviors of black-box machine learning (ML) models has drawn significant attention (Ghorbani et al. 2019; Kim et al. 2018; Koh et al. 2020; Pedapati et al. 2020; Jeyakumar et al. 2020; Heskes et al. 2020; O\u2019Shaughnessy et al. 2020; Heskes et al. 2020; Huai et al. 2019; Yao et al. 2021).",
                "Recently, interpreting and understanding the behaviors of black-box machine learning (ML) models has drawn significant attention (Ghorbani et al. 2019; Kim et al. 2018; Koh et al. 2020; Pedapati et al. 2020; Jeyakumar et al. 2020; Heskes et al. 2020; O\u2019Shaughnessy et al. 2020; Heskes et al. 2020; Huai et al. 2019; Yao et al. 2021)."
            ],
            "citingPaper": {
                "paperId": "6b29cb5f89926cfaa001a9239dec82bd865cb350",
                "externalIds": {
                    "DBLP": "conf/aaai/HuaiLMYZ22",
                    "DOI": "10.1609/aaai.v36i6.20651",
                    "CorpusId": 250291769
                },
                "corpusId": 250291769,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6b29cb5f89926cfaa001a9239dec82bd865cb350",
                "title": "Towards Automating Model Explanations with Certified Robustness Guarantees",
                "abstract": "Providing model explanations has gained significant popularity recently. In contrast with the traditional feature-level model explanations, concept-based explanations can provide explanations in the form of high-level human concepts. However, existing concept-based explanation methods implicitly follow a two-step procedure that involves human intervention. Specifically, they first need the human to be involved to define (or extract) the high-level concepts, and then manually compute the importance scores of these identified concepts in a post-hoc way. This laborious process requires significant human effort and resource expenditure due to manual work, which hinders their large-scale deployability. In practice, it is challenging to automatically generate the concept-based explanations without human intervention due to the subjectivity of defining the units of concept-based interpretability. In addition, due to its data-driven nature, the interpretability itself is also potentially susceptible to malicious manipulations. Hence, our goal in this paper is to free human from this tedious process, while ensuring that the generated explanations are provably robust to adversarial perturbations. We propose a novel concept-based interpretation method, which can not only automatically provide the prototype-based concept explanations but also provide certified robustness guarantees for the generated prototype-based explanations. We also conduct extensive experiments on real-world datasets to verify the desirable properties of the proposed method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2925985",
                        "name": "Mengdi Huai"
                    },
                    {
                        "authorId": "1709583",
                        "name": "Jinduo Liu"
                    },
                    {
                        "authorId": "49434299",
                        "name": "Chenglin Miao"
                    },
                    {
                        "authorId": "41075660",
                        "name": "Liuyi Yao"
                    },
                    {
                        "authorId": "92994851",
                        "name": "Aidong Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "5671934320ebcdf0de28cbe27a5b8d0797bcfdde",
                "externalIds": {
                    "ArXiv": "2206.11529",
                    "DBLP": "journals/corr/abs-2206-11529",
                    "DOI": "10.48550/arXiv.2206.11529",
                    "CorpusId": 249953914
                },
                "corpusId": 249953914,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5671934320ebcdf0de28cbe27a5b8d0797bcfdde",
                "title": "Explanatory causal effects for model agnostic explanations",
                "abstract": "This paper studies the problem of estimating the contributions of features to the prediction of a speci\ufb01c instance by a machine learning model and the overall contribution of a feature to the model. The causal effect of a feature (variable) on the predicted outcome re\ufb02ects the contribution of the feature to a prediction very well. A challenge is that most existing causal effects cannot be estimated from data without a known causal graph. In this paper, we de\ufb01ne an explanatory causal effect based on a hypothetical ideal experiment. The de\ufb01nition brings several bene\ufb01ts to model agnostic explanations. First, explanations are transparent and have causal meanings. Second, the explanatory causal effect estimation can be data driven. Third, the causal effects provide both a local explanation for a speci\ufb01c prediction and a global explanation showing the overall importance of a feature in a predictive model. We further propose a method using individual and combined variables based on explanatory causal effects for explanations. We show the de\ufb01nition and the method work with experiments on some real-world data sets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1786185",
                        "name": "Jiuyong Li"
                    },
                    {
                        "authorId": "2068938999",
                        "name": "Ha Xuan Tran"
                    },
                    {
                        "authorId": "3141770",
                        "name": "T. Le"
                    },
                    {
                        "authorId": "2146017365",
                        "name": "Lin Liu"
                    },
                    {
                        "authorId": "46330674",
                        "name": "Kui Yu"
                    },
                    {
                        "authorId": "2108374005",
                        "name": "Jixue Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Variational auto-encoders (VAEs) have shown promising results in learning causal [37] and interpretable [2] representations or interception of interpretable attributes [13]."
            ],
            "citingPaper": {
                "paperId": "da135c5294aca324ce7ff3e0d30d7a1e1ed74f83",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-05257",
                    "ArXiv": "2206.05257",
                    "DOI": "10.48550/arXiv.2206.05257",
                    "CorpusId": 249605335
                },
                "corpusId": 249605335,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/da135c5294aca324ce7ff3e0d30d7a1e1ed74f83",
                "title": "Explaining Image Classifiers Using Contrastive Counterfactuals in Generative Latent Spaces",
                "abstract": "Despite their high accuracies, modern complex image classifiers cannot be trusted for sensitive tasks due to their unknown decision-making process and potential biases. Counterfactual explanations are very effective in providing transparency for these black-box algorithms. Nevertheless, generating counterfactuals that can have a consistent impact on classifier outputs and yet expose interpretable feature changes is a very challenging task. We introduce a novel method to generate causal and yet interpretable counterfactual explanations for image classifiers using pretrained generative models without any re-training or conditioning. The generative models in this technique are not bound to be trained on the same data as the target classifier. We use this framework to obtain contrastive and causal sufficiency and necessity scores as global explanations for black-box classifiers. On the task of face attribute classification, we show how different attributes influence the classifier output by providing both causal and contrastive feature attributions, and the corresponding counterfactual images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46650151",
                        "name": "Kamran Alipour"
                    },
                    {
                        "authorId": "2003163970",
                        "name": "Aditya Lahiri"
                    },
                    {
                        "authorId": "3419364",
                        "name": "E. Adeli"
                    },
                    {
                        "authorId": "2124624117",
                        "name": "Babak Salimi"
                    },
                    {
                        "authorId": "1694780",
                        "name": "M. Pazzani"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "21a453cae786eb8a45e3ae06c5ec096ddd97c7b2",
                "externalIds": {
                    "ArXiv": "2205.07234",
                    "DBLP": "journals/corr/abs-2205-07234",
                    "DOI": "10.48550/arXiv.2205.07234",
                    "CorpusId": 248811128
                },
                "corpusId": 248811128,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/21a453cae786eb8a45e3ae06c5ec096ddd97c7b2",
                "title": "Clinical outcome prediction under hypothetical interventions - a representation learning framework for counterfactual reasoning",
                "abstract": "Most machine learning (ML) models are devel-oped for prediction only; offering no option for causal interpretation of their predictions or pa-rameters/properties. This can hamper the health systems\u2019 ability to employ ML models in clinical decision-making processes, where the need and desire for predicting outcomes under hypothetical interventions (i.e., counterfactual reason-ing/explanation) is high. In this research, we in-troduce a new representation learning framework (i.e., partial concept bottleneck), which considers the provision of counterfactual explanations as an embedded property of the risk model. Despite architectural changes necessary for jointly optimising for prediction accuracy and counterfactual reasoning, the accuracy of our approach is comparable to prediction-only models. Our results suggest that our proposed framework has the potential to help researchers and clinicians im-prove personalised care (e.g., by investigating the hypothetical differential effects of interventions).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110468525",
                        "name": "Yikuan Li"
                    },
                    {
                        "authorId": "84377381",
                        "name": "M. Mamouei"
                    },
                    {
                        "authorId": "2109307053",
                        "name": "Shishir Rao"
                    },
                    {
                        "authorId": "26931273",
                        "name": "A. Hassaine"
                    },
                    {
                        "authorId": "4547530",
                        "name": "D. Canoy"
                    },
                    {
                        "authorId": "1690572",
                        "name": "Thomas Lukasiewicz"
                    },
                    {
                        "authorId": "3296992",
                        "name": "K. Rahimi"
                    },
                    {
                        "authorId": "1402005227",
                        "name": "G. Salimi-Khorshidi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "5c27465c799fccb16138edd6f0ce6db879234f4d",
                "externalIds": {
                    "ArXiv": "2204.12037",
                    "DBLP": "journals/ijautcomp/LiuWYLL22",
                    "PubMedCentral": "9638478",
                    "DOI": "10.1007/s11633-022-1362-z",
                    "CorpusId": 248572531
                },
                "corpusId": 248572531,
                "publicationVenue": {
                    "id": "1caabc5e-b06a-4ba8-bccd-8d3b71322232",
                    "name": "Machine Intelligence Research",
                    "alternate_names": [
                        "Mach Intell Res"
                    ],
                    "issn": "2731-538X"
                },
                "url": "https://www.semanticscholar.org/paper/5c27465c799fccb16138edd6f0ce6db879234f4d",
                "title": "Causal Reasoning Meets Visual Representation Learning: A Prospective Study",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47909156",
                        "name": "Y. Liu"
                    },
                    {
                        "authorId": "2163540971",
                        "name": "Yushen Wei"
                    },
                    {
                        "authorId": "2152844433",
                        "name": "Hongyu Yan"
                    },
                    {
                        "authorId": "144958813",
                        "name": "Guanbin Li"
                    },
                    {
                        "authorId": "2151739092",
                        "name": "Liang Lin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "The work in [36] relies on Variational Autoencoders [24], but is limited by experiments on artificial toy datasets.",
                "After localizing the features through the two methods, we perturb the salient features and observe the change in the classifier\u2019s output on these new images, a commonly employed metric [36, 37].",
                "Generative models have been proposed to visualize classifiers [26, 31, 36, 43]."
            ],
            "citingPaper": {
                "paperId": "3e4851a761c6cb3b72ae7e04e6c52fdead6d4b35",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-05376",
                    "ArXiv": "2204.05376",
                    "DOI": "10.1109/CVPRW56347.2022.00331",
                    "CorpusId": 248118862
                },
                "corpusId": 248118862,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3e4851a761c6cb3b72ae7e04e6c52fdead6d4b35",
                "title": "medXGAN: Visual Explanations for Medical Classifiers through a Generative Latent Space",
                "abstract": "Despite the surge of deep learning in the past decade, some users are skeptical to deploy these models in practice due to their black-box nature. Specifically, in the medical space where there are severe potential repercussions, we need to develop methods to gain confidence in the models\u2019 decisions. To this end, we propose a novel medical imaging generative adversarial framework, medXGAN (medical eXplanation GAN), to visually explain what a medical classifier focuses on in its binary predictions. By encoding domain knowledge of medical images, we are able to disentangle anatomical structure and pathology, leading to fine-grained visualization through latent interpolation. Furthermore, we optimize the latent space such that interpolation explains how the features contribute to the classifier\u2019s output. Our method outperforms baselines such as Gradient-Weighted Class Activation Mapping (Grad-CAM) and Integrated Gradients in localization and explanatory ability. Additionally, a combination of the medXGAN with Integrated Gradients can yield explanations more robust to noise. The project page with code is available at: https://avdravid.github.io/medXGANpage/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "116990729",
                        "name": "Amil Dravid"
                    },
                    {
                        "authorId": "16766276",
                        "name": "Florian Schiffers"
                    },
                    {
                        "authorId": "40206014",
                        "name": "Boqing Gong"
                    },
                    {
                        "authorId": "144842935",
                        "name": "A. Katsaggelos"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "There are several viable formalisms of causality, such as structural causal models [18, 19], Granger causality [3, 11], and causal Bayesian networks [19].",
                "Causality, therefore, has been a plausible language for answering such questions [11, 18].",
                "Following existing works [11, 18], we also evaluate the Log-odds difference to illustrate the fidelity of generated explanations in a more statistical view."
            ],
            "citingPaper": {
                "paperId": "cfd1ba68e0dee2bbf86dd202e79587599939539c",
                "externalIds": {
                    "DBLP": "conf/cvpr/LinL0022",
                    "ArXiv": "2203.15209",
                    "DOI": "10.1109/CVPR52688.2022.01336",
                    "CorpusId": 247778655
                },
                "corpusId": 247778655,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cfd1ba68e0dee2bbf86dd202e79587599939539c",
                "title": "OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks",
                "abstract": "This paper proposes a new eXplanation framework, called OrphicX, for generating causal explanations for any graph neural networks (GNNs) based on learned latent causal factors. Specifically, we construct a distinct generative model and design an objective function that encourages the generative model to produce causal, compact, and faithful explanations. This is achieved by isolating the causal factors in the latent space of graphs by maximizing the information flow measurements. We theoretically analyze the cause-effect relationships in the proposed causal graph, identify node attributes as confounders between graphs and GNN predictions, and circumvent such confounder effect by leveraging the backdoor adjustment formula. Our framework is compatible with any GNNs, and it does not require access to the process by which the target GNN produces its predictions. In addition, it does not rely on the linear-independence assumption of the explained features, nor require prior knowledge on the graph learning tasks. We show a proof-of-concept of OrphicX on canonical classification problems on graph data. In particular, we analyze the explanatory subgraphs obtained from explanations for molecular graphs (i.e., Mutag) and quantitatively evaluate the explanation performance with frequently occurring subgraph patterns. Empirically, we show that OrphicX can effectively identify the causal semantics for generating causal explanations, significantly outperforming its alternatives11This project is supported by the Internal Research Fund at The Hong Kong Polytechnic University P0035763. HW is partially supported by NSF Grant IIS-2127918 and an Amazon Faculty Research Award..",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2526241",
                        "name": "Wanyu Lin"
                    },
                    {
                        "authorId": "2105547066",
                        "name": "Hao Lan"
                    },
                    {
                        "authorId": "39483391",
                        "name": "Hao Wang"
                    },
                    {
                        "authorId": "2145520273",
                        "name": "Baochun Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Interpretability: Most of the existing works on post-hoc interpretability techniques focus on inspecting the decisions for a single image (Zeiler & Fergus, 2014; Mahendran & Vedaldi, 2016; Dosovitskiy & Brox, 2016; Yosinski et al., 2016; Nguyen et al., 2016; Adebayo et al., 2018; Zhou et al., 2018; Chang et al., 2019; Olah et al., 2018; Yeh et al., 2019; Carter et al., 2019; O\u2019Shaughnessy et al., 2019; Sturmfels et al., 2020; Verma et al., 2020).",
                "\u20262014; Mahendran & Vedaldi, 2016; Dosovitskiy & Brox, 2016; Yosinski et al., 2016; Nguyen et al., 2016; Adebayo et al., 2018; Zhou et al., 2018; Chang et al., 2019; Olah et al., 2018; Yeh et al., 2019; Carter et al., 2019; O\u2019Shaughnessy et al., 2019; Sturmfels et al., 2020; Verma et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "71854d12f72a2d62105b2e0f3e24fb6cada55f7c",
                "externalIds": {
                    "ArXiv": "2203.15566",
                    "DBLP": "journals/corr/abs-2203-15566",
                    "DOI": "10.48550/arXiv.2203.15566",
                    "CorpusId": 247778575
                },
                "corpusId": 247778575,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/71854d12f72a2d62105b2e0f3e24fb6cada55f7c",
                "title": "Core Risk Minimization using Salient ImageNet",
                "abstract": "Deep neural networks can be unreliable in the real world especially when they heavily use spurious features for their predictions. Recently, Singla&Feizi (2022) introduced the Salient Imagenet dataset by annotating and localizing core and spurious features of ~52k samples from 232 classes of Imagenet. While this dataset is useful for evaluating the reliance of pretrained models on spurious features, its small size limits its usefulness for training models. In this work, we first introduce the Salient Imagenet-1M dataset with more than 1 million soft masks localizing core and spurious features for all 1000 Imagenet classes. Using this dataset, we first evaluate the reliance of several Imagenet pretrained models (42 total) on spurious features and observe that: (i) transformers are more sensitive to spurious features compared to Convnets, (ii) zero-shot CLIP transformers are highly susceptible to spurious features. Next, we introduce a new learning paradigm called Core Risk Minimization (CoRM) whose objective ensures that the model predicts a class using its core features. We evaluate different computational approaches for solving CoRM and achieve significantly higher (+12%) core accuracy (accuracy when non-core regions corrupted using noise) with no drop in clean accuracy compared to models trained via Empirical Risk Minimization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144190575",
                        "name": "Sahil Singla"
                    },
                    {
                        "authorId": "104644443",
                        "name": "Mazda Moayeri"
                    },
                    {
                        "authorId": "34389431",
                        "name": "S. Feizi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We are conducting a data collection study to observe the real-world practicality and performance of an adaptive DBS algorithm in patients with PD to observe the resulting LFP dynamics under adaptive DBS in 10 patients (Oyama et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "133c94b00cac751874132540cd20c6c665ca4497",
                "externalIds": {
                    "PubMedCentral": "8931265",
                    "DOI": "10.3389/fnhum.2022.813387",
                    "CorpusId": 247223676,
                    "PubMed": "35308605"
                },
                "corpusId": 247223676,
                "publicationVenue": {
                    "id": "f1d65bf7-0efb-4099-9607-0904a843a6ec",
                    "name": "Frontiers in Human Neuroscience",
                    "type": "journal",
                    "alternate_names": [
                        "Front Hum Neurosci"
                    ],
                    "issn": "1662-5161",
                    "url": "http://www.frontiersin.org/human_neuroscience",
                    "alternate_urls": [
                        "http://www.frontiersin.org/humanneuroscience/",
                        "https://www.frontiersin.org/journals/human-neuroscience",
                        "http://journal.frontiersin.org/journal/human-neuroscience"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/133c94b00cac751874132540cd20c6c665ca4497",
                "title": "Proceedings of the Ninth Annual Deep Brain Stimulation Think Tank: Advances in Cutting Edge Technologies, Artificial Intelligence, Neuromodulation, Neuroethics, Pain, Interventional Psychiatry, Epilepsy, and Traumatic Brain Injury",
                "abstract": "DBS Think Tank IX was held on August 25\u201327, 2021 in Orlando FL with US based participants largely in person and overseas participants joining by video conferencing technology. The DBS Think Tank was founded in 2012 and provides an open platform where clinicians, engineers and researchers (from industry and academia) can freely discuss current and emerging deep brain stimulation (DBS) technologies as well as the logistical and ethical issues facing the field. The consensus among the DBS Think Tank IX speakers was that DBS expanded in its scope and has been applied to multiple brain disorders in an effort to modulate neural circuitry. After collectively sharing our experiences, it was estimated that globally more than 230,000 DBS devices have been implanted for neurological and neuropsychiatric disorders. As such, this year\u2019s meeting was focused on advances in the following areas: neuromodulation in Europe, Asia and Australia; cutting-edge technologies, neuroethics, interventional psychiatry, adaptive DBS, neuromodulation for pain, network neuromodulation for epilepsy and neuromodulation for traumatic brain injury.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145718995",
                        "name": "J. Wong"
                    },
                    {
                        "authorId": "143829487",
                        "name": "G. Deuschl"
                    },
                    {
                        "authorId": "50811771",
                        "name": "R. Wolke"
                    },
                    {
                        "authorId": "1888474",
                        "name": "H. Bergman"
                    },
                    {
                        "authorId": "2802652",
                        "name": "M. Muthuraman"
                    },
                    {
                        "authorId": "2017347",
                        "name": "S. Groppa"
                    },
                    {
                        "authorId": "145246942",
                        "name": "S. Sheth"
                    },
                    {
                        "authorId": "1393590220",
                        "name": "H. Bronte-Stewart"
                    },
                    {
                        "authorId": "1557335021",
                        "name": "K. B. Wilkins"
                    },
                    {
                        "authorId": "9185126",
                        "name": "M. Petrucci"
                    },
                    {
                        "authorId": "2157052345",
                        "name": "Emilia F. Lambert"
                    },
                    {
                        "authorId": "1918787549",
                        "name": "Y. Kehnemouyi"
                    },
                    {
                        "authorId": "2020143",
                        "name": "P. Starr"
                    },
                    {
                        "authorId": "3118667",
                        "name": "S. Little"
                    },
                    {
                        "authorId": "3206547",
                        "name": "J. Ans\u00f3"
                    },
                    {
                        "authorId": "41066595",
                        "name": "R. Gilron"
                    },
                    {
                        "authorId": "4123617",
                        "name": "L. Poree"
                    },
                    {
                        "authorId": "2677312",
                        "name": "G. Kalamangalam"
                    },
                    {
                        "authorId": "2898104",
                        "name": "G. Worrell"
                    },
                    {
                        "authorId": "2113624821",
                        "name": "K. Miller"
                    },
                    {
                        "authorId": "2536346",
                        "name": "N. Schiff"
                    },
                    {
                        "authorId": "3073857",
                        "name": "C. Butson"
                    },
                    {
                        "authorId": "2102382",
                        "name": "J. Henderson"
                    },
                    {
                        "authorId": "31351949",
                        "name": "J. Judy"
                    },
                    {
                        "authorId": "1398440727",
                        "name": "A. Ramirez-Zamora"
                    },
                    {
                        "authorId": "38780809",
                        "name": "K. Foote"
                    },
                    {
                        "authorId": "2841533",
                        "name": "P. Silburn"
                    },
                    {
                        "authorId": "47681789",
                        "name": "Luming Li"
                    },
                    {
                        "authorId": "2492382",
                        "name": "G. Oyama"
                    },
                    {
                        "authorId": "46222943",
                        "name": "H. Kamo"
                    },
                    {
                        "authorId": "153777312",
                        "name": "S. Sekimoto"
                    },
                    {
                        "authorId": "48039185",
                        "name": "N. Hattori"
                    },
                    {
                        "authorId": "145469189",
                        "name": "J. Giordano"
                    },
                    {
                        "authorId": "3381821",
                        "name": "Diane DiEuliis"
                    },
                    {
                        "authorId": "6917517",
                        "name": "J. Shook"
                    },
                    {
                        "authorId": "2157051906",
                        "name": "Darin D. Doughtery"
                    },
                    {
                        "authorId": "2179271",
                        "name": "A. Widge"
                    },
                    {
                        "authorId": "2245080",
                        "name": "H. Mayberg"
                    },
                    {
                        "authorId": "38247487",
                        "name": "J. Cha"
                    },
                    {
                        "authorId": "145306000",
                        "name": "K. Choi"
                    },
                    {
                        "authorId": "46254997",
                        "name": "S. Heisig"
                    },
                    {
                        "authorId": "1679867138",
                        "name": "Mosadoluwa Obatusin"
                    },
                    {
                        "authorId": "6422202",
                        "name": "E. Opri"
                    },
                    {
                        "authorId": "47361297",
                        "name": "S. Kaufman"
                    },
                    {
                        "authorId": "4951341",
                        "name": "P. Shirvalkar"
                    },
                    {
                        "authorId": "2145678648",
                        "name": "C. Rozell"
                    },
                    {
                        "authorId": "47750013",
                        "name": "Sankaraleengam (Sankar) Alagapan"
                    },
                    {
                        "authorId": "7021113",
                        "name": "R. Raike"
                    },
                    {
                        "authorId": "2663065",
                        "name": "H. Bokil"
                    },
                    {
                        "authorId": "2157050970",
                        "name": "David Green"
                    },
                    {
                        "authorId": "1840350",
                        "name": "M. Okun"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "199f8fbf558d4eadaa86bdf4201f560ca69a0bf0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-07728",
                    "ArXiv": "2202.07728",
                    "DOI": "10.1109/CVPR52729.2023.01550",
                    "CorpusId": 246867069
                },
                "corpusId": 246867069,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/199f8fbf558d4eadaa86bdf4201f560ca69a0bf0",
                "title": "Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis",
                "abstract": "A plethora of attribution methods have recently been developed to explain deep neural networks. These methods use different classes of perturbations (e.g, occlusion, blurring, masking, etc) to estimate the importance of individual image pixels to drive a model's decision. Nevertheless, the space of possible perturbations is vast and current attribution methods typically require significant computation time to accurately sample the space in order to achieve high-quality explanations. In this work, we introduce EVA (Explaining using Verified Perturbation Analysis) - the first explainability method which comes with guarantees that an entire set of possible perturbations has been exhaustively searched. We leverage recent progress in verified perturbation analysis methods to directly propagate bounds through a neural network to exhaustively probe a - potentially infinite-size - set of perturbations in a single forward pass. Our approach takes advantage of the beneficial properties of verified perturbation analysis, i.e., time efficiency and guaranteed complete - sampling agnostic - coverage of the perturbation space - to identify image pixels that drive a model's decision. We evaluate EVA systematically and demonstrate state-of-the-art results on multiple benchmarks. Our code isfreely available: github.com/deel-ai/formal-explainability",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1935310411",
                        "name": "Thomas Fel"
                    },
                    {
                        "authorId": "2812151",
                        "name": "M\u00e9lanie Ducoffe"
                    },
                    {
                        "authorId": "51879352",
                        "name": "David Vigouroux"
                    },
                    {
                        "authorId": "2117570481",
                        "name": "R'emi Cadene"
                    },
                    {
                        "authorId": "49880353",
                        "name": "Mikael Capelle"
                    },
                    {
                        "authorId": "11882108",
                        "name": "C. Nicodeme"
                    },
                    {
                        "authorId": "1981539",
                        "name": "Thomas Serre"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "7caaafd5a3ee033c98e792c7ea5b699d005753d5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08164",
                    "ArXiv": "2201.08164",
                    "DOI": "10.1145/3583558",
                    "CorpusId": 246063780
                },
                "corpusId": 246063780,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7caaafd5a3ee033c98e792c7ea5b699d005753d5",
                "title": "From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI",
                "abstract": "The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the past 7 years at major AI and ML conferences that introduce an XAI method. We find that one in three papers evaluate exclusively with anecdotal evidence, and one in five papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark, and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training to optimize for accuracy and interpretability simultaneously.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "17698891",
                        "name": "Meike Nauta"
                    },
                    {
                        "authorId": "52019849",
                        "name": "Jan Trienes"
                    },
                    {
                        "authorId": "66163851",
                        "name": "Shreyasi Pathak"
                    },
                    {
                        "authorId": "13407092",
                        "name": "Elisa Nguyen"
                    },
                    {
                        "authorId": "2066935841",
                        "name": "Michelle Peters"
                    },
                    {
                        "authorId": "2150574981",
                        "name": "Yasmin Schmitt"
                    },
                    {
                        "authorId": "3044872",
                        "name": "J\u00f6rg Schl\u00f6tterer"
                    },
                    {
                        "authorId": "1711719",
                        "name": "M. V. Keulen"
                    },
                    {
                        "authorId": "145566115",
                        "name": "C. Seifert"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026has been a surge in using the ideas of causality to improve the learning and explanation capabilities of deep learning models in recent years (O' Shaughnessy et al. 2020; Suter et al. 2019; Goyal et al. 2019a,b; Chattopadhyay et al. 2019; Janzing 2019; Zmigrod et al. 2019; Pitis, Creager, and\u2026"
            ],
            "citingPaper": {
                "paperId": "b4845ef70a5fd4d786038d0360ff32fe2cab8593",
                "externalIds": {
                    "DBLP": "conf/aaai/ReddyLB22",
                    "ArXiv": "2112.05746",
                    "DOI": "10.1609/aaai.v36i7.20781",
                    "CorpusId": 245117679
                },
                "corpusId": 245117679,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b4845ef70a5fd4d786038d0360ff32fe2cab8593",
                "title": "On Causally Disentangled Representations",
                "abstract": "Representation learners that disentangle factors of variation have already proven to be important in addressing various real world concerns such as fairness and interpretability. Initially consisting of unsupervised models with independence assumptions, more recently, weak supervision and correlated features have been explored, but without a causal view of the generative process. In contrast, we work under the regime of a causal generative process where generative factors are either independent or can be potentially confounded by a set of observed or unobserved confounders. We present an analysis of disentangled representations through the notion of disentangled causal process. We motivate the need for new metrics and datasets to study causal disentanglement and propose two evaluation metrics and a dataset. We show that our metrics capture the desiderata of disentangled causal process. Finally we perform an empirical study on state of the art disentangled representation learners using our metrics and dataset to evaluate them from causal perspective.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110653264",
                        "name": "Abbavaram Gowtham Reddy"
                    },
                    {
                        "authorId": "2144866290",
                        "name": "L. BeninGodfrey"
                    },
                    {
                        "authorId": "1699429",
                        "name": "V. Balasubramanian"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "explanations where decisions about single images are inspected (Zeiler & Fergus, 2014; Yosinski et al., 2016; Dosovitskiy & Brox, 2016; Mahendran & Vedaldi, 2016; Nguyen et al., 2016; Zhou et al., 2018; Olah et al., 2018; Adebayo et al., 2018; Chang et al., 2019; Carter et al., 2019; Yeh et al., 2019; Sturmfels et al., 2020; O\u2019Shaughnessy et al., 2019; Verma et al., 2020).",
                "\u20262014; Yosinski et al., 2016; Dosovitskiy & Brox, 2016; Mahendran & Vedaldi, 2016; Nguyen et al., 2016; Zhou et al., 2018; Olah et al., 2018; Adebayo et al., 2018; Chang et al., 2019; Carter et al., 2019; Yeh et al., 2019; Sturmfels et al., 2020; O\u2019Shaughnessy et al., 2019; Verma et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "5f893ad86470cb935d702f980f5af8d8e013c7ae",
                "externalIds": {
                    "ArXiv": "2110.04301",
                    "DBLP": "conf/iclr/0002F22",
                    "CorpusId": 244116787
                },
                "corpusId": 244116787,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5f893ad86470cb935d702f980f5af8d8e013c7ae",
                "title": "Salient ImageNet: How to discover spurious features in Deep Learning?",
                "abstract": "we identify spurious or core neural features (penultimate layer neurons of a robust model) via limited human supervision (e.g., using top 5 activating images per feature). We then show that these neural feature annotations generalize extremely well to many more images without any human supervision. We use the activation maps for these neural features as the soft masks to highlight spurious or core visual features. Using this methodology, we introduce the Salient Imagenet dataset containing core and spurious masks for a large set of samples from Imagenet. Using this dataset, we show that several popular Imagenet models rely heavily on various spurious features in their predictions, in-dicating the standard accuracy alone is not suf\ufb01cient to fully assess model perfor-1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144190575",
                        "name": "Sahil Singla"
                    },
                    {
                        "authorId": "34389431",
                        "name": "S. Feizi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "62c2fe262327bf1b2964a260c75ef447f42e9d23",
                "externalIds": {
                    "ArXiv": "2110.03485",
                    "DBLP": "journals/corr/abs-2110-03485",
                    "DOI": "10.1007/978-3-031-19775-8_26",
                    "CorpusId": 238419170
                },
                "corpusId": 238419170,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/62c2fe262327bf1b2964a260c75ef447f42e9d23",
                "title": "Cartoon Explanations of Image Classifiers",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48189362",
                        "name": "S. Kolek"
                    },
                    {
                        "authorId": "2153960546",
                        "name": "Duc Anh Nguyen"
                    },
                    {
                        "authorId": "2265243",
                        "name": "R. Levie"
                    },
                    {
                        "authorId": "143627859",
                        "name": "Joan Bruna"
                    },
                    {
                        "authorId": "3125779",
                        "name": "Gitta Kutyniok"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2026is not given, one needs to discover useful concepts for the explanation, e.g., Ghorbani et al. (2019) used segmentation and clustering, Yeh et al. (2020) retrained the classifier with a prototypical concept layer, O\u2019Shaughnessy et al. (2020) learned the generative model with a causal objective.",
                "O\u2019Shaughnessy et al. (2020) argued that compared to other metrics such as average causal effect (ACE) (Holland 1988), analysis of variance (ANOVA) (Lewontin 1974), information flow is more suitable to capture complex and nonlinear causal dependence between variables.",
                "Recently, O\u2019Shaughnessy et al. (2020) proposed a learning framework that encourages the causal effect of certain latent factors on the classifier output to learn a latent representation that has causality on the prediction.",
                "Specifically, the standard VAE model and also O\u2019Shaughnessy et al. (2020) assumes the independence of latent factors, which is believed to encourage\nmeaningful disentanglement via a factorized prior distribution.",
                "Next, we compare to O\u2019Shaughnessy et al. (2020), in which we used a VAE model with ten continuous factors and encouraged three factors to have causal effects on predicted classes.",
                "Inspired by O\u2019Shaughnessy et al. (2020), we employs a generative model to learn the data distribution while encouraging the causal influence of certain latent factors.",
                "2020) or a latent factor of a generative model (O\u2019Shaughnessy et al. 2020; Goyal et al. 2020).",
                "The definition of concept are various, e.g., a direction in the activation space (Kim et al. 2018; Ghorbani et al. 2019), a prototypical activation vector (Yeh et al. 2020) or a latent factor of a generative model (O\u2019Shaughnessy et al. 2020; Goyal et al. 2020)."
            ],
            "citingPaper": {
                "paperId": "5697da09cdfd3ce7768b126ced6b5a0b82e4e884",
                "externalIds": {
                    "ArXiv": "2109.04518",
                    "DBLP": "conf/aaai/TranFAS22",
                    "DOI": "10.1609/aaai.v36i9.21195",
                    "CorpusId": 237485333
                },
                "corpusId": 237485333,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5697da09cdfd3ce7768b126ced6b5a0b82e4e884",
                "title": "Unsupervised Causal Binary Concepts Discovery with VAE for Black-box Model Explanation",
                "abstract": "We aim to explain a black-box classifier with the form: \"data X is classified as class Y because X has A, B and does not have C\" in which A, B, and C are high-level concepts. The challenge is that we have to discover in an unsupervised manner a set of concepts, i.e., A, B and C, that is useful for explaining the classifier. We first introduce a structural generative model that is suitable to express and discover such concepts. We then propose a learning process that simultaneously learns the data distribution and encourages certain concepts to have a large causal influence on the classifier output. Our method also allows easy integration of user's prior knowledge to induce high interpretability of concepts. Finally, using multiple datasets, we demonstrate that the proposed method can discover useful concepts for explanation in this form.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2072581644",
                        "name": "Thien Q. Tran"
                    },
                    {
                        "authorId": "16348694",
                        "name": "Kazuto Fukuchi"
                    },
                    {
                        "authorId": "1721701",
                        "name": "Youhei Akimoto"
                    },
                    {
                        "authorId": "1733719",
                        "name": "Jun Sakuma"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "0b3e119248286aeedc95330ab7b67999f522d574",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-09003",
                    "ArXiv": "2108.09003",
                    "DOI": "10.1007/s00521-023-08423-1",
                    "CorpusId": 237259799
                },
                "corpusId": 237259799,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0b3e119248286aeedc95330ab7b67999f522d574",
                "title": "Explainable reinforcement learning for broad-XAI: a conceptual framework and survey",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3327913",
                        "name": "Richard Dazeley"
                    },
                    {
                        "authorId": "1990124",
                        "name": "P. Vamplew"
                    },
                    {
                        "authorId": "152480455",
                        "name": "Francisco Cruz"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "fdc781007356ae4739300157638e69e9ae3e848b",
                "externalIds": {
                    "ArXiv": "2108.09159",
                    "DBLP": "journals/corr/abs-2108-09159",
                    "DOI": "10.1007/978-3-031-01333-1_19",
                    "CorpusId": 237260132
                },
                "corpusId": 237260132,
                "publicationVenue": {
                    "id": "c85dfc25-bcef-4719-9997-f41ad334d998",
                    "name": "International Symposium on Intelligent Data Analysis",
                    "type": "conference",
                    "alternate_names": [
                        "IDA",
                        "Int Symp Intell Data Anal"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1517"
                },
                "url": "https://www.semanticscholar.org/paper/fdc781007356ae4739300157638e69e9ae3e848b",
                "title": "VAE-CE: Visual Contrastive Explanation using Disentangled VAEs",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "151413080",
                        "name": "Y. Poels"
                    },
                    {
                        "authorId": "2266428",
                        "name": "Vlado Menkovski"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "As such, they usually require knowledge of the system of causal structural equations [20, 16, 25, 41] or the causal graph [26]."
            ],
            "citingPaper": {
                "paperId": "7b700dbbdc38b714916a93065c1d11ea16728e7c",
                "externalIds": {
                    "DBLP": "conf/nips/PawelczykBHRK21",
                    "ArXiv": "2108.00783",
                    "CorpusId": 236772193
                },
                "corpusId": 236772193,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7b700dbbdc38b714916a93065c1d11ea16728e7c",
                "title": "CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms",
                "abstract": "Counterfactual explanations provide means for prescriptive model explanations by suggesting actionable feature changes (e.g., increase income) that allow individuals to achieve favorable outcomes in the future (e.g., insurance approval). Choosing an appropriate method is a crucial aspect for meaningful counterfactual explanations. As documented in recent reviews, there exists a quickly growing literature with available methods. Yet, in the absence of widely available opensource implementations, the decision in favor of certain models is primarily based on what is readily available. Going forward - to guarantee meaningful comparisons across explanation methods - we present CARLA (Counterfactual And Recourse LibrAry), a python library for benchmarking counterfactual explanation methods across both different data sets and different machine learning models. In summary, our work provides the following contributions: (i) an extensive benchmark of 11 popular counterfactual explanation methods, (ii) a benchmarking framework for research on future counterfactual explanation methods, and (iii) a standardized set of integrated evaluation measures and data sets for transparent and extensive comparisons of these methods. We have open-sourced CARLA and our experimental results on Github, making them available as competitive baselines. We welcome contributions from other research groups and practitioners.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "89583148",
                        "name": "Martin Pawelczyk"
                    },
                    {
                        "authorId": "2122328520",
                        "name": "Sascha Bielawski"
                    },
                    {
                        "authorId": "1720621",
                        "name": "J. V. D. Heuvel"
                    },
                    {
                        "authorId": "47934005",
                        "name": "Tobias Richter"
                    },
                    {
                        "authorId": "1686448",
                        "name": "G. Kasneci"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Another direction is to build a simpler causal tree that approximates the predictions of a more complicated network and can be said to explain its behavior (Shaughnessy et al. 2020)."
            ],
            "citingPaper": {
                "paperId": "6e2047ee30fb09cf62775ef9e476415904936c99",
                "externalIds": {
                    "DBLP": "conf/aaai/TadepalliR21",
                    "DOI": "10.1609/aaai.v35i11.17175",
                    "CorpusId": 235349284
                },
                "corpusId": 235349284,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6e2047ee30fb09cf62775ef9e476415904936c99",
                "title": "PAC Learning of Causal Trees with Latent Variables",
                "abstract": "Learning causal models with latent variables from observational and experimental data is an important problem. In this paper we present a polynomial-time algorithm that PAC learns the structure and parameters of a rooted tree-structured causal network of bounded degree where the internal nodes of the tree cannot be observed or manipulated. Our algorithm is the first of its kind to provably learn the structure and parameters of tree-structured causal models with latent internal variables from random examples and active experiments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1729906",
                        "name": "P. Tadepalli"
                    },
                    {
                        "authorId": "145107462",
                        "name": "Stuart J. Russell"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The closest method in spirit to the explanations provided by our StylEx approach is [20], though their method only works on small images, and Figure 3."
            ],
            "citingPaper": {
                "paperId": "0981cbb415a602b599a3549282429ca3acf35a85",
                "externalIds": {
                    "ArXiv": "2104.13369",
                    "DBLP": "conf/iccv/LangGYWEHFIGIM21",
                    "MAG": "3159635074",
                    "DOI": "10.1109/ICCV48922.2021.00073",
                    "CorpusId": 233407984
                },
                "corpusId": 233407984,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/0981cbb415a602b599a3549282429ca3acf35a85",
                "title": "Explaining in Style: Training a GAN to explain a classifier in StyleSpace",
                "abstract": "Image classification models can depend on multiple different semantic attributes of the image. An explanation of the decision of the classifier needs to both discover and visualize these properties. Here we present StylEx, a method for doing this, by training a generative model to specifically explain multiple attributes that underlie classifier decisions. A natural source for such attributes is the StyleSpace of StyleGAN, which is known to generate semantically meaningful dimensions in the image. However, because standard GAN training is not dependent on the classifier, it may not represent those attributes which are important for the classifier decision, and the dimensions of StyleSpace may represent irrelevant at-tributes. To overcome this, we propose a training procedure for a StyleGAN, which incorporates the classifier model, in order to learn a classifier-specific StyleSpace. Explanatory attributes are then selected from this space. These can be used to visualize the effect of changing multiple attributes per image, thus providing image-specific explanations. We apply StylEx to multiple domains, including animals, leaves, faces and retinal images. For these, we show how an image can be modified in different ways to change its classifier output. Our results show that the method finds attributes that align well with semantic ones, generate meaningful image-specific explanations, and are human-interpretable as measured in user-studies.1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49618488",
                        "name": "Oran Lang"
                    },
                    {
                        "authorId": "52164591",
                        "name": "Yossi Gandelsman"
                    },
                    {
                        "authorId": "2065978700",
                        "name": "Michal Yarom"
                    },
                    {
                        "authorId": "2042084",
                        "name": "Yoav Wald"
                    },
                    {
                        "authorId": "1684677",
                        "name": "G. Elidan"
                    },
                    {
                        "authorId": "1809983",
                        "name": "A. Hassidim"
                    },
                    {
                        "authorId": "1768236",
                        "name": "W. Freeman"
                    },
                    {
                        "authorId": "2094770",
                        "name": "Phillip Isola"
                    },
                    {
                        "authorId": "1786843",
                        "name": "A. Globerson"
                    },
                    {
                        "authorId": "144611617",
                        "name": "M. Irani"
                    },
                    {
                        "authorId": "2138834",
                        "name": "Inbar Mosseri"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2022 It can capture the non-linear, complex relationship between input and output variables in a black-box model [13].",
                "(iii) Causal influence based methods: This is a recent line of work in which causal influence is quantified using some measure and this is used for generating feature attribution maps[4, 14, 15, 13]."
            ],
            "citingPaper": {
                "paperId": "f462b16213e59c270abd0bbca03ef88a1ab1c076",
                "externalIds": {
                    "ArXiv": "2104.12759",
                    "DBLP": "journals/corr/abs-2104-12759",
                    "DOI": "10.1109/CVPRW53098.2021.00194",
                    "CorpusId": 233394103
                },
                "corpusId": 233394103,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f462b16213e59c270abd0bbca03ef88a1ab1c076",
                "title": "Instance-wise Causal Feature Selection for Model Interpretation",
                "abstract": "We formulate a causal extension to the recently introduced paradigm of instance-wise feature selection to explain black-box visual classifiers. Our method selects a subset of input features that has the greatest causal effect on the model\u2019s output. We quantify the causal influence of a subset of features by the Relative Entropy Distance measure. Under certain assumptions this is equivalent to the conditional mutual information between the selected subset and the output variable. The resulting causal selections are sparser and cover salient objects in the scene. We show the efficacy of our approach on multiple vision datasets by measuring the post-hoc accuracy and Average Causal Effect of selected features on the model\u2019s output.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "122698764",
                        "name": "Pranoy Panda"
                    },
                    {
                        "authorId": "2105284868",
                        "name": "Sai Srinivas Kancheti"
                    },
                    {
                        "authorId": "1699429",
                        "name": "V. Balasubramanian"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "9cb50651718a8f51c60f84eb71cc406d499bf50d",
                "externalIds": {
                    "ArXiv": "2012.03369",
                    "CorpusId": 260507536
                },
                "corpusId": 260507536,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9cb50651718a8f51c60f84eb71cc406d499bf50d",
                "title": "Proactive Pseudo-Intervention: Causally Informed Contrastive Learning For Interpretable Vision Models",
                "abstract": "Deep neural networks excel at comprehending complex visual signals, delivering on par or even superior performance to that of human experts. However, ad-hoc visual explanations of model decisions often reveal an alarming level of reliance on exploiting non-causal visual cues that strongly correlate with the target label in training data. As such, deep neural nets suffer compromised generalization to novel inputs collected from different sources, and the reverse engineering of their decision rules offers limited interpretability. To overcome these limitations, we present a novel contrastive learning strategy called {\\it Proactive Pseudo-Intervention} (PPI) that leverages proactive interventions to guard against image features with no causal relevance. We also devise a novel causally informed salience mapping module to identify key image pixels to intervene, and show it greatly facilitates model interpretability. To demonstrate the utility of our proposals, we benchmark on both standard natural images and challenging medical image datasets. PPI-enhanced models consistently deliver superior performance relative to competing solutions, especially on out-of-domain predictions and data integration from heterogeneous sources. Further, our causally trained saliency maps are more succinct and meaningful relative to their non-causal counterparts.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2152689526",
                        "name": "Dong Wang"
                    },
                    {
                        "authorId": "2109410509",
                        "name": "Yuewei Yang"
                    },
                    {
                        "authorId": "46387857",
                        "name": "Chenyang Tao"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "2109077977",
                        "name": "Liqun Chen"
                    },
                    {
                        "authorId": "1411683893",
                        "name": "Fanjie Kong"
                    },
                    {
                        "authorId": "145153424",
                        "name": "Ricardo Henao"
                    },
                    {
                        "authorId": "2185755024",
                        "name": "L. Carin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Very recently, exploratory effort has been made to leverage the tools from counterfactual reasoning [22] and causal analysis [41] to derive visual explanations, but do not lend insights back to model training."
            ],
            "citingPaper": {
                "paperId": "784e22c365c76113fd68eb15a16f45130c3d9e51",
                "externalIds": {
                    "MAG": "3111409922",
                    "DBLP": "journals/corr/abs-2012-03369",
                    "CorpusId": 227333988
                },
                "corpusId": 227333988,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/784e22c365c76113fd68eb15a16f45130c3d9e51",
                "title": "Proactive Pseudo-Intervention: Causally Informed Contrastive Learning For Interpretable Vision Models",
                "abstract": "Deep neural networks have shown significant promise in comprehending complex visual signals, delivering performance on par or even superior to that of human experts. However, these models often lack a mechanism for interpreting their predictions, and in some cases, particularly when the sample size is small, existing deep learning solutions tend to capture spurious correlations that compromise model generalizability on unseen inputs. In this work, we propose a contrastive causal representation learning strategy that leverages proactive interventions to identify causally-relevant image features, called Proactive Pseudo-Intervention (PPI). This approach is complemented with a causal salience map visualization module, i.e., Weight Back Propagation (WBP), that identifies important pixels in the raw input image, which greatly facilitates the interpretability of predictions. To validate its utility, our model is benchmarked extensively on both standard natural images and challenging medical image datasets. We show this new contrastive causal representation learning model consistently improves model performance relative to competing solutions, particularly for out-of-domain predictions or when dealing with data integration from heterogeneous sources. Further, our causal saliency maps are more succinct and meaningful relative to their non-causal counterparts.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2152689526",
                        "name": "Dong Wang"
                    },
                    {
                        "authorId": "2109410509",
                        "name": "Yuewei Yang"
                    },
                    {
                        "authorId": "46387857",
                        "name": "Chenyang Tao"
                    },
                    {
                        "authorId": "1411683893",
                        "name": "Fanjie Kong"
                    },
                    {
                        "authorId": "51030446",
                        "name": "R. Henao"
                    },
                    {
                        "authorId": "145006560",
                        "name": "L. Carin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "6116cde7fc65eff29a1b8118b034f87909f2237f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-01750",
                    "MAG": "3107423523",
                    "ArXiv": "2012.01750",
                    "DOI": "10.1109/CVPR46437.2021.01266",
                    "CorpusId": 227255149
                },
                "corpusId": 227255149,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6116cde7fc65eff29a1b8118b034f87909f2237f",
                "title": "Understanding Failures of Deep Networks via Robust Feature Extraction",
                "abstract": "Traditional evaluation metrics for learned models that report aggregate scores over a test set are insufficient for surfacing important and informative patterns of failure over features and instances. We introduce and study a method aimed at characterizing and explaining failures by identifying visual attributes whose presence or absence results in poor performance. In distinction to previous work that relies upon crowdsourced labels for visual attributes, we leverage the representation of a separate robust model to extract interpretable features and then harness these features to identify failure modes. We further propose a visualization method aimed at enabling humans to understand the meaning encoded in such features and we test the comprehensibility of the features. An evaluation of the methods on the ImageNet dataset demonstrates that: (i) the proposed workflow is effective for discovering important failure modes, (ii) the visualization techniques help humans to understand the extracted features, and (iii) the extracted insights can assist engineers with error analysis and debugging.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144190575",
                        "name": "Sahil Singla"
                    },
                    {
                        "authorId": "2571049",
                        "name": "Besmira Nushi"
                    },
                    {
                        "authorId": "47973411",
                        "name": "S. Shah"
                    },
                    {
                        "authorId": "1783184",
                        "name": "Ece Kamar"
                    },
                    {
                        "authorId": "145479841",
                        "name": "E. Horvitz"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "3a24bfb77ed271fef948058e414850f89b0955a7",
                "externalIds": {
                    "DBLP": "conf/icml/KohNTMPKL20",
                    "ArXiv": "2007.04612",
                    "MAG": "3041871339",
                    "CorpusId": 220424448
                },
                "corpusId": 220424448,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3a24bfb77ed271fef948058e414850f89b0955a7",
                "title": "Concept Bottleneck Models",
                "abstract": "We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like \u201cthe existence of bone spurs\u201d, as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts (\u201cbone spurs\u201d) or bird attributes (\u201cwing color\u201d). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2572525",
                        "name": "Pang Wei Koh"
                    },
                    {
                        "authorId": "2117824274",
                        "name": "Thao Nguyen"
                    },
                    {
                        "authorId": "103603264",
                        "name": "Y. S. Tang"
                    },
                    {
                        "authorId": "1776721",
                        "name": "Stephen Mussmann"
                    },
                    {
                        "authorId": "145192191",
                        "name": "E. Pierson"
                    },
                    {
                        "authorId": "3351164",
                        "name": "Been Kim"
                    },
                    {
                        "authorId": "145419642",
                        "name": "Percy Liang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Generative causal explanations of black box classifiers [41] are built by learning the latent factors involved in a classification, which are then included in a causal model."
            ],
            "citingPaper": {
                "paperId": "be5bfe6026a6c9d88367332a4a2d142fec5f6851",
                "externalIds": {
                    "CorpusId": 259300842
                },
                "corpusId": 259300842,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/be5bfe6026a6c9d88367332a4a2d142fec5f6851",
                "title": "Explaining Classifiers\u2019 Outputs with Causal Models and Argumentation",
                "abstract": "We introduce a conceptualisation for generating argumentation frameworks (AFs) from causal models for the purpose of forging explanations for models\u2019 outputs. The conceptualisation is based on reinterpreting properties of semantics of AFs as explanation moulds, which are means for characterising argumentative relations. We demonstrate our methodology by reinterpreting the property of bi-variate reinforcement in bipolar AFs, showing how the extracted bipolar AFs may be used as relation-based explanations for the outputs of causal models. We then evaluate our method empirically when the causal models represent (Bayesian and neural network) machine learning models for classification. The results show advantages over a popular approach from the literature, both in highlighting specific relationships between feature and classification variables and in generating counterfactual explanations with respect to a commonly used metric.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1911563",
                        "name": "Antonio Rago"
                    },
                    {
                        "authorId": "2154616820",
                        "name": "Fabrizio Russo"
                    },
                    {
                        "authorId": "1810684853",
                        "name": "Emanuele Albini"
                    },
                    {
                        "authorId": "49973505",
                        "name": "Francesca Toni"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Recent work has focused on learning causal graphs [12] or generating causal natural language explanations [9] from images."
            ],
            "citingPaper": {
                "paperId": "ecf0ee0b148f780562aeaeef8e64edbdf5b5342f",
                "externalIds": {
                    "CorpusId": 262688948
                },
                "corpusId": 262688948,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ecf0ee0b148f780562aeaeef8e64edbdf5b5342f",
                "title": "Compositional Visual Causal Reasoning with Language Prompts",
                "abstract": ". We propose a new visual causal reasoning framework that leverages compositional visual representations and language prompts to reason about counterfactuals. Our model learns to decompose visual scenes into objects and events, represent them compositionally, and generate natural language explanations describing potential causal relationships between them. These explanations are then used to infer counter-factuals in response to language prompts. We show that compositional visual representations, when combined with causal language explanations and prompting, can improve performance on visual causal reasoning tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2246523331",
                        "name": "Ipsit Mantri"
                    },
                    {
                        "authorId": "2219692593",
                        "name": "Nevasini Sasikumar"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "07ef59360ae4deac815b96b5ed1f8315959156bf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-12037",
                    "DOI": "10.48550/arXiv.2204.12037",
                    "CorpusId": 248392088
                },
                "corpusId": 248392088,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/07ef59360ae4deac815b96b5ed1f8315959156bf",
                "title": "Causal Reasoning with Spatial-temporal Representation Learning: A Prospective Study",
                "abstract": "Spatial-temporal representation learning is ubiquitous in various real-world applications, including visual comprehension, video understanding, multi-modal analysis, human-computer interaction, and urban computing. Due to the emergence of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal data in big data era, the lack of interpretability, robustness, and out-of-distribution generalization are becoming the challenges of the existing visual models. The majority of the existing methods tend to \ufb01t the original data/variable distributions and ignore the essential causal relations behind the multi-modal knowledge, which lacks an uni\ufb01ed guidance and analysis about why modern spatial-temporal representation learning methods are easily collapse into data bias and have limited generalization and cognitive abilities. Inspired by the strong inference ability of human-level agents, recent years have therefore witnessed great e\ufb00ort in developing causal reasoning paradigms to realize robust representation and model learning with good cognitive ability. In this paper, we conduct a comprehensive review of existing causal reasoning methods for spatial-temporal representation learning, covering fundamental theories, models, and datasets. The limitations of current methods and datasets are also discussed. Moreover, we propose some primary challenges, opportunities, and future research directions for benchmarking causal reasoning algorithms in spatial-temporal representation learning. This paper aims to provide a comprehensive overview of this emerging \ufb01eld, attract attention, encourage discussions, bring to the forefront the urgency of developing novel causal reasoning methods, publicly available benchmarks, and consensus-building standards for reliable spatial-temporal representation learning and related real-world applications more e\ufb03ciently.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46399556",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "2163540971",
                        "name": "Yushen Wei"
                    },
                    {
                        "authorId": "2111314931",
                        "name": "Hongfei Yan"
                    },
                    {
                        "authorId": "144958813",
                        "name": "Guanbin Li"
                    },
                    {
                        "authorId": "2148303324",
                        "name": "Liang Lin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "com/trends/explore?date=all&q=mnist [124, 198, 199, 200, 201, 202]"
            ],
            "citingPaper": {
                "paperId": "52184ce524aa30ec485808f240d58fb7bc37f96d",
                "externalIds": {
                    "CorpusId": 252367597
                },
                "corpusId": 252367597,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/52184ce524aa30ec485808f240d58fb7bc37f96d",
                "title": "A S URVEY OF D EEP C AUSAL M ODELS A BSTRACT",
                "abstract": "The concept of causality plays an important role in human cognition . In the past few decades, causal inference has been well developed in many \ufb01elds, such as computer science, medicine, economics, and education. With the advancement of deep learning techniques, it has been increasingly used in causal inference against counterfactual data. Typically, deep causal models map the characteristics of covariates to a representation space and then design various objective optimization functions to estimate counterfactual data unbiasedly based on the different optimization methods. This paper focuses on the survey of the deep causal models, and its core contributions are as follows: 1) we provide relevant metrics under multiple treatments and continuous-dose treatment; 2) we incorporate a comprehensive overview of deep causal models from both temporal development and method classi\ufb01cation perspectives; 3) we assist a detailed and comprehensive classi\ufb01cation and analysis of relevant datasets and source code.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118208232",
                        "name": "Zongyu Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "com/trends/explore?date=all&q=mnist [124, 198, 199, 200, 201, 202]"
            ],
            "citingPaper": {
                "paperId": "ecd18261dd5c2022253568852f7726bff5bc557c",
                "externalIds": {
                    "CorpusId": 252411866
                },
                "corpusId": 252411866,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ecd18261dd5c2022253568852f7726bff5bc557c",
                "title": "A SURVEY OF DEEP CAUSAL MODELS",
                "abstract": "The concept of causality plays an important role in human cognition . In the past few decades, causal inference has been well developed in many fields, such as computer science, medicine, economics, and education. With the advancement of deep learning techniques, it has been increasingly used in causal inference against counterfactual data. Typically, deep causal models map the characteristics of covariates to a representation space and then design various objective optimization functions to estimate counterfactual data unbiasedly based on the different optimization methods. This paper focuses on the survey of the deep causal models, and its core contributions are as follows: 1) we provide relevant metrics under multiple treatments and continuous-dose treatment; 2) we incorporate a comprehensive overview of deep causal models from both temporal development and method classification perspectives; 3) we assist a detailed and comprehensive classification and analysis of relevant datasets and source code.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46637795",
                        "name": "Zheng Hua Zhu"
                    },
                    {
                        "authorId": "145181674",
                        "name": "Zhenyu Guo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "de68a4d5da64034326a636794fe7164f3436fe1f",
                "externalIds": {
                    "DBLP": "conf/ijcai/YuGP21",
                    "DOI": "10.24963/ijcai.2021/633",
                    "CorpusId": 237100635
                },
                "corpusId": 237100635,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/de68a4d5da64034326a636794fe7164f3436fe1f",
                "title": "Information-Theoretic Methods in Deep Neural Networks: Recent Advances and Emerging Opportunities",
                "abstract": "We present a review on the recent advances and emerging opportunities around the theme of analyzing deep neural networks (DNNs) with information-theoretic methods. We first discuss popular information-theoretic quantities and their estimators. We then introduce recent developments on information-theoretic learning principles (e.g., loss functions, regularizers and objectives) and their parameterization with DNNs. We finally briefly review current usages of informationtheoretic concepts in a few modern machine learning problems and list a few emerging opportunities.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2462771",
                        "name": "Shujian Yu"
                    },
                    {
                        "authorId": "1912613",
                        "name": "L. S. Giraldo"
                    },
                    {
                        "authorId": "143961030",
                        "name": "J. Pr\u00edncipe"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "f25edfa0e26c9e4be4b10eed9c7b5db095335cda",
                "externalIds": {
                    "CorpusId": 237258219
                },
                "corpusId": 237258219,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f25edfa0e26c9e4be4b10eed9c7b5db095335cda",
                "title": "[Re] Replication Study of \u2019Generative causal explanations of black-box classi\ufb01ers\u2019",
                "abstract": "We verify the outcome of the methodology proposed in the article, which attempts to provide post-hoc causal explanations for black-box classifiers through causal reference. This is achieved by replicating the code step by step, according to the descriptions in the paper. All the claims in the paper have been examined, and we provide additional metric to evaluate the portability, expressive power, algorithmic complexity and the data fidelity of their framework. We have further extended their analyses to consider all benchmark datasets used, confirming results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2175695991",
                        "name": "Ivo Verhoeven"
                    },
                    {
                        "authorId": "2176047961",
                        "name": "Xinyi Chen"
                    },
                    {
                        "authorId": "2115785253",
                        "name": "Qi-Cheng Hu"
                    },
                    {
                        "authorId": "1486496131",
                        "name": "Mario S. Holubar"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "d7d80da41ad557ee5887787efd8da4b65622065e",
                "externalIds": {
                    "CorpusId": 237263074
                },
                "corpusId": 237263074,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d7d80da41ad557ee5887787efd8da4b65622065e",
                "title": "[Re] Generative causal explanations of black-box classifiers",
                "abstract": "Explainability of black-box classifiers is an important aspect of neural models that often is non-existent. Classifiers 2 made for tasks such as object recognition and decision making often lack transparency which causes vulnerability 3 being overlooked [1]. Without insight into the reasons behind a decision made by a neural model, potential security 4 risks or classification mistakes can be missed [1]. Multiple solutions have been posed to solve this problem. An 5 example is the method designed by O\u2019Shaugnessy et al. [2]. The authors design a learning framework that leverages a 6 generative model and information-theoretic measures of causal influence. The objective function encourages both the 7 generative model to faithfully represent the data distribution and the latent factors to have a large causal influence on 8 the classifier output. In this study, the reproducibility of the method developed by O\u2019Shaugnessy et al. is tested. Several 9 claims are challenged to ensure the validity of the method. Furthermore, the method is extended to test generalizability. 10 It was found that the claims are not as strong as the authors suggested and the method is not as easily generalizable 11 as expected. However, for the task described in the original study, the method is completely reproducible, and thus a 12 valid contribution to machine learning innovation. 13",
                "year": 2021,
                "authors": []
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "2aa9347fdd2943b54171846db567fe9ad054d267",
                "externalIds": {
                    "CorpusId": 237263101
                },
                "corpusId": 237263101,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2aa9347fdd2943b54171846db567fe9ad054d267",
                "title": "Reproducibility report Generative causal explanations of black-box classifiers",
                "abstract": "The paper by O\u2019Shaughnessy et al. (2020) claims to have developed a method to disentangle the latent space of 3 generative models during training. The latent space then consists of variables with causal influence and variables with 4 non-causal influence. These can then be used as explanations of the generative model. These models will be reproduced 5 with the goal of examining their latent space and confirming if they serve as sufficiently reliable explanations. 6",
                "year": 2021,
                "authors": []
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "Most of these efforts have focused on local explanations where decisions about single images are inspected (Zeiler & Fergus, 2014; Yosinski et al., 2016; Dosovitskiy & Brox, 2016; Mahendran & Vedaldi, 2016; Nguyen et al., 2016; Zhou et al., 2018; Olah et al., 2018; Adebayo et al., 2018; Chang et al., 2019; Carter et al., 2019; Yeh et al., 2019; Sturmfels et al., 2020; O\u2019Shaughnessy et al., 2019; Verma et al., 2020).",
                "\u20262014; Yosinski et al., 2016; Dosovitskiy & Brox, 2016; Mahendran & Vedaldi, 2016; Nguyen et al., 2016; Zhou et al., 2018; Olah et al., 2018; Adebayo et al., 2018; Chang et al., 2019; Carter et al., 2019; Yeh et al., 2019; Sturmfels et al., 2020; O\u2019Shaughnessy et al., 2019; Verma et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "468b9ec2bc817bb9e567e300ae16e05160ab24df",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-04301",
                    "CorpusId": 238582635
                },
                "corpusId": 238582635,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/468b9ec2bc817bb9e567e300ae16e05160ab24df",
                "title": "Causal ImageNet: How to discover spurious features in Deep Learning?",
                "abstract": "A key reason for the lack of reliability of deep neural networks in the real world is their heavy reliance on spurious input features that are causally unrelated to the true label. Focusing on image classifications, we define causal attributes as the set of visual features that are always a part of the object while spurious attributes are the ones that are likely to co-occur with the object but not a part of it (e.g., attribute \u201cfingers\u201d for class \u201cband aid\u201d). Traditional methods for discovering spurious features either require extensive human annotations (thus, not scalable), or are useful on specific models. In this work, we introduce a scalable framework to discover a subset of spurious and causal visual attributes used in inferences of a general model and localize them on a large number of images with minimal human supervision. Our methodology is based on this key idea: to identify spurious or causal visual attributes used in model predictions, we identify spurious or causal neural features (penultimate layer neurons of a robust model) via limited human supervision (e.g., using top 5 activating images per feature). We then show that these neural feature annotations generalize extremely well to many more images without any human supervision. We use the activation maps for these neural features as the soft masks to highlight spurious or causal visual attributes. Using this methodology, we introduce the Causal Imagenet dataset containing causal and spurious masks for a large set of samples from Imagenet. We assess the performance of several popular Imagenet models and show that they rely heavily on various spurious features in their predictions. 1 ar X iv :2 11 0. 04 30 1v 2 [ cs .L G ] 6 N ov 2 02 1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144190575",
                        "name": "Sahil Singla"
                    },
                    {
                        "authorId": "34389431",
                        "name": "S. Feizi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Generative causal explanations of black box classifiers [37] are built by learning the latent factors involved in a classification, which are then included in a causal model."
            ],
            "citingPaper": {
                "paperId": "7964afeb903c913845143876e2bf4495daccff15",
                "externalIds": {
                    "DBLP": "conf/aiia/0001RABT21",
                    "CorpusId": 246869030
                },
                "corpusId": 246869030,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7964afeb903c913845143876e2bf4495daccff15",
                "title": "Forging Argumentative Explanations from Causal Models",
                "abstract": "We introduce a conceptualisation for generating argumentation frameworks (AFs) from causal models for the purpose of forging explanations for models\u2019 outputs. The conceptualisation is based on reinterpreting properties of semantics of AFs as explanation moulds, which are means for characterising argumentative relations. We demonstrate our methodology by reinterpreting the property of bi-variate reinforcement in bipolar AFs, showing how the extracted bipolar AFs may be used as relation-based explanations for the outputs of causal models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1911563",
                        "name": "Antonio Rago"
                    },
                    {
                        "authorId": "2154616820",
                        "name": "Fabrizio Russo"
                    },
                    {
                        "authorId": "1810684853",
                        "name": "Emanuele Albini"
                    },
                    {
                        "authorId": "1710592",
                        "name": "P. Baroni"
                    },
                    {
                        "authorId": "49973505",
                        "name": "Francesca Toni"
                    }
                ]
            }
        }
    ]
}