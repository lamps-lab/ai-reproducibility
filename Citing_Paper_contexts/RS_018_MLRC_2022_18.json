{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5531dd953832d86253d08b739eaa9312c7c6f610",
                "externalIds": {
                    "ArXiv": "2309.17337",
                    "DOI": "10.1145/3617694.3623259",
                    "CorpusId": 263311003
                },
                "corpusId": 263311003,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5531dd953832d86253d08b739eaa9312c7c6f610",
                "title": "Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools",
                "abstract": "While algorithmic fairness is a thriving area of research, in practice, mitigating issues of bias often gets reduced to enforcing an arbitrarily chosen fairness metric, either by enforcing fairness constraints during the optimization step, post-processing model outputs, or by manipulating the training data. Recent work has called on the ML community to take a more holistic approach to tackle fairness issues by systematically investigating the many design choices made through the ML pipeline, and identifying interventions that target the issue's root cause, as opposed to its symptoms. While we share the conviction that this pipeline-based approach is the most appropriate for combating algorithmic unfairness on the ground, we believe there are currently very few methods of \\emph{operationalizing} this approach in practice. Drawing on our experience as educators and practitioners, we first demonstrate that without clear guidelines and toolkits, even individuals with specialized ML knowledge find it challenging to hypothesize how various design choices influence model behavior. We then consult the fair-ML literature to understand the progress to date toward operationalizing the pipeline-aware approach: we systematically collect and organize the prior work that attempts to detect, measure, and mitigate various sources of unfairness through the ML pipeline. We utilize this extensive categorization of previous contributions to sketch a research agenda for the community. We hope this work serves as the stepping stone toward a more comprehensive set of resources for ML researchers, practitioners, and students interested in exploring, designing, and testing pipeline-oriented approaches to algorithmic fairness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2249533192",
                        "name": "Emily Black"
                    },
                    {
                        "authorId": "2249533181",
                        "name": "Rakshit Naidu"
                    },
                    {
                        "authorId": "1791498",
                        "name": "R. Ghani"
                    },
                    {
                        "authorId": "6783324",
                        "name": "Kit T. Rodolfa"
                    },
                    {
                        "authorId": "2249533466",
                        "name": "Daniel E. Ho"
                    },
                    {
                        "authorId": "2249539262",
                        "name": "Hoda Heidari"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "41d70ca636726feee45887063c8c884efb12f22a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-03299",
                    "ArXiv": "2308.03299",
                    "DOI": "10.48550/arXiv.2308.03299",
                    "CorpusId": 260681163
                },
                "corpusId": 260681163,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/41d70ca636726feee45887063c8c884efb12f22a",
                "title": "My Model is Unfair, Do People Even Care? Visual Design Affects Trust and Perceived Bias in Machine Learning",
                "abstract": "Machine learning technology has become ubiquitous, but, unfortunately, often exhibits bias. As a consequence, disparate stakeholders need to interact with and make informed decisions about using machine learning models in everyday systems. Visualization technology can support stakeholders in understanding and evaluating trade-offs between, for example, accuracy and fairness of models. This paper aims to empirically answer\"Can visualization design choices affect a stakeholder's perception of model bias, trust in a model, and willingness to adopt a model?\"Through a series of controlled, crowd-sourced experiments with more than 1,500 participants, we identify a set of strategies people follow in deciding which models to trust. Our results show that men and women prioritize fairness and performance differently and that visual design choices significantly affect that prioritization. For example, women trust fairer models more often than men do, participants value fairness more when it is explained using text than as a bar chart, and being explicitly told a model is biased has a bigger impact than showing past biased performance. We test the generalizability of our results by comparing the effect of multiple textual and visual design choices and offer potential explanations of the cognitive mechanisms behind the difference in fairness perception and trust. Our research guides design considerations to support future work developing visualization systems for machine learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2180800367",
                        "name": "Aimen Gaba"
                    },
                    {
                        "authorId": "2163391892",
                        "name": "Zhanna Kaufman"
                    },
                    {
                        "authorId": "2230104672",
                        "name": "Jason Chueng"
                    },
                    {
                        "authorId": "2230100848",
                        "name": "Marie Shvakel"
                    },
                    {
                        "authorId": "2228405650",
                        "name": "Kyle Wm. Hall"
                    },
                    {
                        "authorId": "2932798",
                        "name": "Yuriy Brun"
                    },
                    {
                        "authorId": "2230093725",
                        "name": "Cindy Xiong Bearfield"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u201d The sources of unfairness are many, including data sampling bias or under-representation [16, 70, 15, 7], data labeling bias [60, 65, 26], model architecture (or feature representation) [2, 47, 68, 56, 66, 39, 55, 41], distribution shift [23, 17, 50, 27] etc."
            ],
            "citingPaper": {
                "paperId": "f7aa171a55347ab8c61eceb8a922b54f0e04d4eb",
                "externalIds": {
                    "ArXiv": "2306.17828",
                    "DBLP": "journals/corr/abs-2306-17828",
                    "DOI": "10.48550/arXiv.2306.17828",
                    "CorpusId": 259309082
                },
                "corpusId": 259309082,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f7aa171a55347ab8c61eceb8a922b54f0e04d4eb",
                "title": "Understanding Unfairness via Training Concept Influence",
                "abstract": "Knowing the causes of a model's unfairness helps practitioners better understand their data and algorithms. This is an important yet relatively unexplored task. We look into this problem through the lens of the training data - one of the major sources of unfairness. We ask the following questions: how would a model's fairness performance change if, in its training data, some samples (1) were collected from a different (e.g. demographic) group, (2) were labeled differently, or (3) some features were changed? In other words, we quantify the fairness influence of training samples by counterfactually intervening and changing samples based on predefined concepts, i.e. data attributes such as features (X), labels (Y), or sensitive attributes (A). To calculate a training sample's influence on the model's unfairness w.r.t a concept, we first generate counterfactual samples based on the concept, i.e. the counterfactual versions of the sample if the concept were changed. We then calculate the resulting impact on the unfairness, via influence function, if the counterfactual samples were used in training. Our framework not only helps practitioners understand the observed unfairness and repair their training data, but also leads to many other applications, e.g. detecting mislabeling, fixing imbalanced representations, and detecting fairness-targeted poisoning attacks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3460027",
                        "name": "Yuanshun Yao"
                    },
                    {
                        "authorId": "2152797134",
                        "name": "Yang Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Within the domain of algorithmic fairness, our work is related to recent investigations into the effects of distribution shift, or data mismeasurement, on fair learning [17, 33, 37, 38]."
            ],
            "citingPaper": {
                "paperId": "89959d1c87b83202913636ed6bd8d39fc2fc4a52",
                "externalIds": {
                    "DBLP": "conf/fat/EstornellDLV23",
                    "DOI": "10.1145/3593013.3594006",
                    "CorpusId": 259139757
                },
                "corpusId": 259139757,
                "publicationVenue": {
                    "id": "cdc83875-a82d-445c-b097-cbe91afe99a8",
                    "name": "Conference on Fairness, Accountability and Transparency",
                    "type": "conference",
                    "alternate_names": [
                        "FAccT",
                        "Conf Fairness Account Transpar"
                    ],
                    "url": "https://facctconference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/89959d1c87b83202913636ed6bd8d39fc2fc4a52",
                "title": "Group-Fair Classification with Strategic Agents",
                "abstract": "The use of algorithmic decision making systems in domains which impact the financial, social, and political well-being of people has created a demand for these to be \u201cfair\u201d under some accepted notion of equity. This demand has in turn inspired a large body of work focused on the development of fair learning algorithms which are then used in lieu of their conventional counterparts. Most analysis of such fair algorithms proceeds from the assumption that the people affected by the algorithmic decisions are represented as immutable feature vectors. However, strategic agents may possess both the ability and the incentive to manipulate this observed feature vector in order to attain a more favorable outcome. We explore the impact that strategic agent behavior can have on group-fair classification. We find that in many settings strategic behavior can lead to fairness reversal, with a conventional classifier exhibiting higher fairness than a classifier trained to satisfy group fairness. Further, we show that fairness reversal occurs as a result of a group-fair classifier becoming more selective, achieving fairness largely by excluding individuals from the advantaged group. In contrast, if group fairness is achieved by the classifier becoming more inclusive, fairness reversal does not occur.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "103056375",
                        "name": "Andrew Estornell"
                    },
                    {
                        "authorId": "40583483",
                        "name": "Sanmay Das"
                    },
                    {
                        "authorId": "40457423",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "1699600",
                        "name": "Yevgeniy Vorobeychik"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d85d7a205dfdde6cc41f46141756d14348557218",
                "externalIds": {
                    "DOI": "10.1038/s41551-023-01056-8",
                    "CorpusId": 259277694,
                    "PubMed": "37380750"
                },
                "corpusId": 259277694,
                "publicationVenue": {
                    "id": "5619586e-de5a-4bc3-ac80-04dd8530d80c",
                    "name": "Nature Biomedical Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Nat Biomed Eng"
                    ],
                    "issn": "2157-846X",
                    "url": "http://www.nature.com/natbiomedeng/"
                },
                "url": "https://www.semanticscholar.org/paper/d85d7a205dfdde6cc41f46141756d14348557218",
                "title": "Algorithmic fairness in artificial intelligence for medicine and healthcare",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108279369",
                        "name": "Richard J. Chen"
                    },
                    {
                        "authorId": "2109623647",
                        "name": "Judy J. Wang"
                    },
                    {
                        "authorId": "25259989",
                        "name": "Drew F. K. Williamson"
                    },
                    {
                        "authorId": "2242468870",
                        "name": "Tiffany Y. Chen"
                    },
                    {
                        "authorId": "1959705",
                        "name": "Jana Lipkov\u00e1"
                    },
                    {
                        "authorId": "16184125",
                        "name": "Ming Y. Lu"
                    },
                    {
                        "authorId": "2060422236",
                        "name": "S. Sahai"
                    },
                    {
                        "authorId": "37122655",
                        "name": "Faisal Mahmood"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For software properties such as fairness [3], [7], [8], [14], [22] and safety [52], complementary approaches provide high-confidence, probabilistic guarantees based on statistical tests and confidence bounds [2], [16], [21], [31], [52]."
            ],
            "citingPaper": {
                "paperId": "1c42b6c3dd048f4141ac0841589b711c14748491",
                "externalIds": {
                    "DBLP": "conf/icse/AgrawalFKRZZSRB23",
                    "DOI": "10.1109/ICSE-Companion58688.2023.00018",
                    "CorpusId": 254280267
                },
                "corpusId": 254280267,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1c42b6c3dd048f4141ac0841589b711c14748491",
                "title": "PRoofster: Automated Formal Verification",
                "abstract": "Formal verification is an effective but extremely work-intensive method of improving software quality. Verifying the correctness of software systems often requires significantly more effort than implementing them in the first place, despite the existence of proof assistants, such as Coq, aiding the process. Recent work has aimed to fully automate the synthesis of formal verification proofs, but little tool support exists for practitioners. This paper presents oofster, a web-based tool aimed at assisting developers with the formal verification process via proof synthesis. oofster inputs a Coq theorem specifying a property of a software system and attempts to automatically synthesize a formal proof of the correctness of that property. When it is unable to produce a proof, oofster outputs the proof-space search tree its synthesis explored, which can guide the developer to provide a hint to enable oofster to synthesize the proof. oofster runs online at https://proofster.cs.umass.edu/ and a video demonstrating oofster is available at https://youtu.be/xQAi66IRfwI/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2193591435",
                        "name": "Arpan Agrawal"
                    },
                    {
                        "authorId": "104884254",
                        "name": "E. First"
                    },
                    {
                        "authorId": "2163391892",
                        "name": "Zhanna Kaufman"
                    },
                    {
                        "authorId": "2193624271",
                        "name": "Tom Reichel"
                    },
                    {
                        "authorId": "2145401634",
                        "name": "Shizhuo Zhang"
                    },
                    {
                        "authorId": "2193708528",
                        "name": "Timothy Zhou"
                    },
                    {
                        "authorId": "1406432713",
                        "name": "Alex Sanchez-Stern"
                    },
                    {
                        "authorId": "35064164",
                        "name": "T. Ringer"
                    },
                    {
                        "authorId": "2932798",
                        "name": "Yuriy Brun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "empirically compared Seldonian algorithms to other fairnessaware ML algorithms [19], [31], [45].",
                "contextual bandits [31], the setting where the training data and deployment data come from different distributions [19], and to enforce measures of long-term fairness [48], suggesting future extensions of the Seldonian Toolkit."
            ],
            "citingPaper": {
                "paperId": "f0f60b1a381db0a54b9877d5bb410ea98b2bc540",
                "externalIds": {
                    "DBLP": "conf/icse/HoagKSTB23",
                    "DOI": "10.1109/ICSE-Companion58688.2023.00035",
                    "CorpusId": 259282095
                },
                "corpusId": 259282095,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f0f60b1a381db0a54b9877d5bb410ea98b2bc540",
                "title": "Seldonian Toolkit: Building Software with Safe and Fair Machine Learning",
                "abstract": "We present the Seldonian Toolkit, which enables software engineers to integrate provably safe and fair machine learning algorithms into their systems. Software systems that use data and machine learning are routinely deployed in a wide range of settings from medical applications, autonomous vehicles, the criminal justice system, and hiring processes. These systems, however, can produce unsafe and unfair behavior, such as suggesting potentially fatal medical treatments, making racist or sexist predictions, or facilitating radicalization and polarization. To reduce these undesirable behaviors, software engineers need the ability to easily integrate their machine-learning-based systems with domain-specific safety and fairness requirements defined by domain experts, such as doctors and hiring managers. The Seldonian Toolkit provides special machine learning algorithms that enable software engineers to incorporate such expert-defined requirements of safety and fairness into their systems, while provably guaranteeing those requirements will be satisfied. A video demonstrating the Seldonian Toolkit is available at https://youtu.be/wHR-hDm9jX4/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144486207",
                        "name": "Austin Hoag"
                    },
                    {
                        "authorId": "2073944141",
                        "name": "James E. Kostas"
                    },
                    {
                        "authorId": "2065418856",
                        "name": "B. C. Silva"
                    },
                    {
                        "authorId": "143640165",
                        "name": "P. Thomas"
                    },
                    {
                        "authorId": "2932798",
                        "name": "Yuriy Brun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Probabilistic verification has successfully provided guarantees for such properties for machine learning systems [27, 57, 89, 96]."
            ],
            "citingPaper": {
                "paperId": "9f8ac6ee3760ab202e492c733362e5bfc6763934",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-04910",
                    "ArXiv": "2303.04910",
                    "DOI": "10.48550/arXiv.2303.04910",
                    "CorpusId": 257427444
                },
                "corpusId": 257427444,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9f8ac6ee3760ab202e492c733362e5bfc6763934",
                "title": "Baldur: Whole-Proof Generation and Repair with Large Language Models",
                "abstract": "Formally verifying software properties is a highly desirable but labor-intensive task. Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time, and using that model to search through the space of possible proofs. This paper introduces a new method to automate formal verification: We use large language models, trained on natural language text and code and fine-tuned on proofs, to generate whole proofs for theorems at once, rather than one step at a time. We combine this proof generation model with a fine-tuned repair model to repair generated proofs, further increasing proving power. As its main contributions, this paper demonstrates for the first time that: (1) Whole-proof generation using transformers is possible and is as effective as search-based techniques without requiring costly search. (2) Giving the learned model additional context, such as a prior failed proof attempt and the ensuing error message, results in proof repair and further improves automated proof generation. (3) We establish a new state of the art for fully automated proof synthesis. We reify our method in a prototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL theorems and their proofs. In addition to empirically showing the effectiveness of whole-proof generation, repair, and added context, we show that Baldur improves on the state-of-the-art tool, Thor, by automatically generating proofs for an additional 8.7% of the theorems. Together, Baldur and Thor can prove 65.7% of the theorems fully automatically. This paper paves the way for new research into using large language models for automating formal verification.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "104884254",
                        "name": "E. First"
                    },
                    {
                        "authorId": "1800714",
                        "name": "Markus N. Rabe"
                    },
                    {
                        "authorId": "35064164",
                        "name": "T. Ringer"
                    },
                    {
                        "authorId": "2932798",
                        "name": "Yuriy Brun"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "There are some works [Rezaei et al., 2021, Schrouff et al., 2022, An et al., 2022, Singh et al., 2021, Giguere et al., 2022] that aim to solve fairness under various distribution shifts."
            ],
            "citingPaper": {
                "paperId": "2e223c5c5b6dfd8df6261de0d2cf55d560882369",
                "externalIds": {
                    "ArXiv": "2303.03300",
                    "DBLP": "journals/corr/abs-2303-03300",
                    "DOI": "10.48550/arXiv.2303.03300",
                    "CorpusId": 257365367
                },
                "corpusId": 257365367,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2e223c5c5b6dfd8df6261de0d2cf55d560882369",
                "title": "Weight Perturbation Can Help Fairness under Distribution Shift",
                "abstract": "Fairness in machine learning has attracted increasing attention in recent years. The fairness methods improving algorithmic fairness for in-distribution data may not perform well under distribution shift. In this paper, we first theoretically demonstrate the inherent connection between distribution shift, data perturbation, and weight perturbation. Subsequently, we analyze the sufficient conditions to guarantee fairness (i.e., low demographic parity) for the target dataset, including fairness for the source dataset, and low prediction difference between the source and target dataset for each sensitive attribute group. Motivated by these sufficient conditions, we propose robust fairness regularization (RFR) by considering the worst case within the weight perturbation ball for each sensitive attribute group. In this way, the maximization problem can be simplified as two forward and two backward propagations for each update of model parameters. We evaluate the effectiveness of our proposed RFR algorithm on synthetic and real distribution shifts across various datasets. Experimental results demonstrate that RFR achieves better fairness-accuracy trade-off performance compared with several baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47653902",
                        "name": "Zhimeng Jiang"
                    },
                    {
                        "authorId": "50017230",
                        "name": "Xiaotian Han"
                    },
                    {
                        "authorId": "1791983892",
                        "name": "Hongye Jin"
                    },
                    {
                        "authorId": "32780441",
                        "name": "Guanchu Wang"
                    },
                    {
                        "authorId": "49648991",
                        "name": "Na Zou"
                    },
                    {
                        "authorId": "2193021044",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fe9643de9dfed3dbd20e55e8c25c190adee4a742",
                "externalIds": {
                    "DOI": "10.1016/j.ejdp.2023.100031",
                    "CorpusId": 257862909
                },
                "corpusId": 257862909,
                "publicationVenue": {
                    "id": "aa4bb990-78cd-423a-9f4b-3baee3da5da7",
                    "name": "EURO Journal on Decision Processes",
                    "type": "journal",
                    "alternate_names": [
                        "EURO J Decis Process"
                    ],
                    "issn": "2193-9438",
                    "url": "https://www.springer.com/business+&+management/operations+research/journal/40070",
                    "alternate_urls": [
                        "https://link.springer.com/journal/40070"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fe9643de9dfed3dbd20e55e8c25c190adee4a742",
                "title": "Fairkit, Fairkit, on the Wall, Who\u2019s the Fairest of Them All? Supporting Fairness-Related Decision-Making",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111408682",
                        "name": "Brittany Johnson"
                    },
                    {
                        "authorId": "2039664663",
                        "name": "Jesse Bartola"
                    },
                    {
                        "authorId": "40555739",
                        "name": "Rico Angell"
                    },
                    {
                        "authorId": "19425012",
                        "name": "Sam Witty"
                    },
                    {
                        "authorId": "2066220310",
                        "name": "Stephen Giguere"
                    },
                    {
                        "authorId": "2932798",
                        "name": "Yuriy Brun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "The ability to measure the stability of chosen fairness metrics across dynamically specified bias factors can be a great first step towards safe deployment of fair classifiers, similar to recent works like Shifty [30].",
                "Recent works have studied fair classification subject to these distribution shifts and proposed solutions under reasonable assumptions on the data distribution [10, 12, 18, 19, 21, 30, 43, 54, 57, 59]."
            ],
            "citingPaper": {
                "paperId": "ed8a223012f5fd67448c9370192de66309f0be95",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-05906",
                    "ArXiv": "2302.05906",
                    "DOI": "10.48550/arXiv.2302.05906",
                    "CorpusId": 256827369
                },
                "corpusId": 256827369,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ed8a223012f5fd67448c9370192de66309f0be95",
                "title": "On Testing and Comparing Fair classifiers under Data Bias",
                "abstract": "In this paper, we consider a theoretical model for injecting data bias, namely, under-representation and label bias (Blum&Stangl, 2019). We theoretically and empirically study its effect on the accuracy and fairness of fair classifiers. Theoretically, we prove that the Bayes optimal group-aware fair classifier on the original data distribution can be recovered by simply minimizing a carefully chosen reweighed loss on the bias-injected distribution. Through extensive experiments on both synthetic and real-world datasets (e.g., Adult, German Credit, Bank Marketing, COMPAS), we empirically audit pre-, in-, and post-processing fair classifiers from standard fairness toolkits for their fairness and accuracy by injecting varying amounts of under-representation and label bias in their training data (but not the test data). Our main observations are: (1) The fairness and accuracy of many standard fair classifiers degrade severely as the bias injected in their training data increases, (2) A simple logistic regression model trained on the right data can often outperform, in both accuracy and fairness, most fair classifiers trained on biased training data, and (3) A few, simple fairness techniques (e.g., reweighing, exponentiated gradients) seem to offer stable accuracy and fairness guarantees even when their training data is injected with under-representation and label bias. Our experiments also show how to integrate a measure of data bias risk in the existing fairness dashboards for real-world deployments",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110360195",
                        "name": "Mohit Sharma"
                    },
                    {
                        "authorId": "144576872",
                        "name": "A. Deshpande"
                    },
                    {
                        "authorId": "1753278",
                        "name": "R. Shah"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "The above assumptions are used in the theoretical analyses of the previous works (Maity et al., 2021; Giguere et al., 2022).",
                "The reason is that Shifty is selecting the final model among the candidates that were already trained on the original training data, whereas ours trains a new model on the improved (pre-processed) data.",
                ", 2021), Shifty (Giguere et al., 2022), and our framework.",
                "6, we add a new baseline called Shifty (Giguere et al., 2022), which focuses on the distribution shift most relevant to ours.",
                "Our framework thus works best when the x distribution does not change, but does not strictly require this condition unlike other previous works on fairness under different types of shifts (Maity et al., 2021; Giguere et al., 2022).",
                "\u2022 Similarly, the demographic shift (Giguere et al., 2022) assumes that the joint probabilities of x and y on the training and deployment distributions are identical (i.",
                "Shifty first trains candidate models on the training data and selects only the models showing high fairness in the shifted deployment data.",
                "To give a favorable condition to Shifty, we assume that Shifty knows the exact test distribution.",
                "Fairness-specific shifts (Maity et al., 2021; An et al., 2022; Giguere et al., 2022; Schrouff et al., 2022) handle group (z) changes, as z is especially correlated with fair training.",
                "Another study (Giguere et al., 2022) designs a new test method to serve a fair model under another distribution change called demographic shifts, where the subgroup distribution may change \u2013 see an empirical comparison with our work in Sec.",
                "\u2022 Similarly, the demographic shift (Giguere et al., 2022) assumes that the joint probabilities of x and y on the training and deployment distributions are identical (i.e., Prtrain(x = x, y = y|z = z) = Prtest(x = x, y = y|z = z)).",
                "As a result, both ours and Shifty improve the fairness of the in-processing-only baseline, but ours shows better fairness than Shifty while achieving similar or higher accuracy.",
                "Table 9 shows the accuracy and fairness performances of the in-processing-only baseline FairBatch, Shifty, and our framework w.r.t. a single metric (DP) and multiple metrics (DP & EO) in the synthetic and COMPAS datasets used in Tables 1 and 2."
            ],
            "citingPaper": {
                "paperId": "b40bccd71db0247639555963368333cad7f9d7cd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-02323",
                    "ArXiv": "2302.02323",
                    "DOI": "10.48550/arXiv.2302.02323",
                    "CorpusId": 256615602
                },
                "corpusId": 256615602,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b40bccd71db0247639555963368333cad7f9d7cd",
                "title": "Improving Fair Training under Correlation Shifts",
                "abstract": "Model fairness is an essential element for Trustworthy AI. While many techniques for model fairness have been proposed, most of them assume that the training and deployment data distributions are identical, which is often not true in practice. In particular, when the bias between labels and sensitive groups changes, the fairness of the trained model is directly influenced and can worsen. We make two contributions for solving this problem. First, we analytically show that existing in-processing fair algorithms have fundamental limits in accuracy and group fairness. We introduce the notion of correlation shifts, which can explicitly capture the change of the above bias. Second, we propose a novel pre-processing step that samples the input data to reduce correlation shifts and thus enables the in-processing approaches to overcome their limitations. We formulate an optimization problem for adjusting the data ratio among labels and sensitive groups to reflect the shifted correlation. A key benefit of our approach lies in decoupling the roles of pre- and in-processing approaches: correlation adjustment via pre-processing and unfairness mitigation on the processed data via in-processing. Experiments show that our framework effectively improves existing in-processing fair algorithms w.r.t. accuracy and fairness, both on synthetic and real datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "30840932",
                        "name": "Yuji Roh"
                    },
                    {
                        "authorId": "2115495251",
                        "name": "Kangwook Lee"
                    },
                    {
                        "authorId": "3288247",
                        "name": "Steven Euijong Whang"
                    },
                    {
                        "authorId": "47808468",
                        "name": "Changho Suh"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Robust fairness is also well studied (Mehrotra & Vishnoi, 2022; Ma et al., 2022; Chai & Wang, 2022; An et al., 2022; Giguere et al., 2022; Jiang et al., 2023), such as under distribution shift and with limited sensitive attributes."
            ],
            "citingPaper": {
                "paperId": "8dc4f76559e3d603412d4ba7b3af118233942b05",
                "externalIds": {
                    "ArXiv": "2301.13443",
                    "CorpusId": 256504813
                },
                "corpusId": 256504813,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8dc4f76559e3d603412d4ba7b3af118233942b05",
                "title": "Retiring $\\Delta$DP: New Distribution-Level Metrics for Demographic Parity",
                "abstract": "Demographic parity is the most widely recognized measure of group fairness in machine learning, which ensures equal treatment of different demographic groups. Numerous works aim to achieve demographic parity by pursuing the commonly used metric $\\Delta DP$. Unfortunately, in this paper, we reveal that the fairness metric $\\Delta DP$ can not precisely measure the violation of demographic parity, because it inherently has the following drawbacks: i) zero-value $\\Delta DP$ does not guarantee zero violation of demographic parity, ii) $\\Delta DP$ values can vary with different classification thresholds. To this end, we propose two new fairness metrics, Area Between Probability density function Curves (ABPC) and Area Between Cumulative density function Curves (ABCC), to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic groups. Thus our proposed new metrics enjoy: i) zero-value ABCC/ABPC guarantees zero violation of demographic parity; ii) ABCC/ABPC guarantees demographic parity while the classification thresholds are adjusted. We further re-evaluate the existing fair models with our proposed fairness metrics and observe different fairness behaviors of those models under the new metrics. The code is available at https://github.com/ahxt/new_metric_for_demographic_parity",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50017230",
                        "name": "Xiaotian Han"
                    },
                    {
                        "authorId": "47653902",
                        "name": "Zhimeng Jiang"
                    },
                    {
                        "authorId": "1791983892",
                        "name": "Hongye Jin"
                    },
                    {
                        "authorId": "47781070",
                        "name": "Zirui Liu"
                    },
                    {
                        "authorId": "49648991",
                        "name": "Na Zou"
                    },
                    {
                        "authorId": "2145778781",
                        "name": "Qifan Wang"
                    },
                    {
                        "authorId": "2148950326",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2021), demographic shift (Giguere et al., 2022), prior probability shift (Biswas & Mukherjee, 2021)) that may be violated in practice.",
                "\u2026and many also imposed rather strong assumptions on distributional shifts (e.g., covariate shifts (Singh et al., 2021; Coston et al., 2019; Rezaei et al., 2021), demographic shift (Giguere et al., 2022), prior probability shift (Biswas & Mukherjee, 2021)) that may be violated in practice."
            ],
            "citingPaper": {
                "paperId": "8b4a80daed8331c051e7daf1ae8bc3c9ee3829fb",
                "externalIds": {
                    "PubMedCentral": "10246117",
                    "DBLP": "journals/corr/abs-2301-13323",
                    "ArXiv": "2301.13323",
                    "DOI": "10.48550/arXiv.2301.13323",
                    "CorpusId": 256416262,
                    "PubMed": "37292471"
                },
                "corpusId": 256416262,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8b4a80daed8331c051e7daf1ae8bc3c9ee3829fb",
                "title": "Fairness and Accuracy Under Domain Generalization",
                "abstract": "As machine learning (ML) algorithms are increasingly used in high-stakes applications, concerns have arisen that they may be biased against certain social groups. Although many approaches have been proposed to make ML models fair, they typically rely on the assumption that data distributions in training and deployment are identical. Unfortunately, this is commonly violated in practice and a model that is fair during training may lead to an unexpected outcome during its deployment. Although the problem of designing robust ML models under dataset shifts has been widely studied, most existing works focus only on the transfer of accuracy. In this paper, we study the transfer of both fairness and accuracy under domain generalization where the data at test time may be sampled from never-before-seen domains. We first develop theoretical bounds on the unfairness and expected loss at deployment, and then derive sufficient conditions under which fairness and accuracy can be perfectly transferred via invariant representation learning. Guided by this, we design a learning algorithm such that fair ML models learned with training data still have high fairness and accuracy when deployment environments change. Experiments on real-world data validate the proposed algorithm. Model implementation is available at https://github.com/pth1993/FATDM.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40421524",
                        "name": "Thai-Hoang Pham"
                    },
                    {
                        "authorId": "1845782244",
                        "name": "Xueru Zhang"
                    },
                    {
                        "authorId": "2157210606",
                        "name": "Ping Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "It is widely recognized in the literature that learning a fair classifier without any auxiliary information about the target distribution or the data collection process is practically impossible [18, 22, 30, 71, 81]."
            ],
            "citingPaper": {
                "paperId": "3c5d703f78a18c404e00abfebcfcde55669aa56e",
                "externalIds": {
                    "ArXiv": "2212.10839",
                    "DOI": "10.14778/3611479.3611498",
                    "CorpusId": 256598382
                },
                "corpusId": 256598382,
                "publicationVenue": {
                    "id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e",
                    "name": "Proceedings of the VLDB Endowment",
                    "type": "journal",
                    "alternate_names": [
                        "Proceedings of The Vldb Endowment",
                        "Proc VLDB Endow",
                        "Proc Vldb Endow"
                    ],
                    "issn": "2150-8097",
                    "url": "http://dl.acm.org/toc.cfm?id=J1174",
                    "alternate_urls": [
                        "http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3c5d703f78a18c404e00abfebcfcde55669aa56e",
                "title": "Consistent Range Approximation for Fair Predictive Modeling",
                "abstract": "This paper proposes a novel framework for certifying the fairness of predictive models trained on biased data. It draws from query answering for incomplete and inconsistent databases to formulate the problem of consistent range approximation (CRA) of fairness queries for a predictive model on a target population. The framework employs background knowledge of the data collection process and biased data, working with or without limited statistics about the target population, to compute a range of answers for fairness queries. Using CRA, the framework builds predictive models that are certifiably fair on the target population, regardless of the availability of external data during training. The framework's efficacy is demonstrated through evaluations on real data, showing substantial improvement over existing state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117052998",
                        "name": "Jiongli Zhu"
                    },
                    {
                        "authorId": "2663974",
                        "name": "Sainyam Galhotra"
                    },
                    {
                        "authorId": "2197526839",
                        "name": "Nazanin Sabri"
                    },
                    {
                        "authorId": "2124624117",
                        "name": "Babak Salimi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "[21] Stephen Giguere, Blossom Metevier, Bruno Castro da Silva, Yuriy Brun, Philip Thomas, and Scott Niekum.",
                "We would like to note that the procedure that first chooses a candidate set of tuning parameters and then selects the best one has been commonly used in machine learning, such as in Seldonian algorithm framework to control safety and fairness [43, 21, 48], the Learn then Test framework for risk control [3], and in high-dimensional statistics [47]."
            ],
            "citingPaper": {
                "paperId": "2fa570b35fc0b443c4e0b39ae6dd4978e6803f66",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-15072",
                    "ArXiv": "2211.15072",
                    "DOI": "10.48550/arXiv.2211.15072",
                    "CorpusId": 254044657
                },
                "corpusId": 254044657,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2fa570b35fc0b443c4e0b39ae6dd4978e6803f66",
                "title": "FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee",
                "abstract": "Algorithmic fairness plays an increasingly critical role in machine learning research. Several group fairness notions and algorithms have been proposed. However, the fairness guarantee of existing fair classification methods mainly depends on specific data distributional assumptions, often requiring large sample sizes, and fairness could be violated when there is a modest number of samples, which is often the case in practice. In this paper, we propose FaiREE, a fair classification algorithm that can satisfy group fairness constraints with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfy various group fairness notions (e.g., Equality of Opportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal accuracy. These theoretical guarantees are further supported by experiments on both synthetic and real data. FaiREE is shown to have favorable performance over state-of-the-art algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192665769",
                        "name": "Puheng Li"
                    },
                    {
                        "authorId": "2114312765",
                        "name": "James Y. Zou"
                    },
                    {
                        "authorId": "10537441",
                        "name": "Linjun Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Indeed, those works define an upper-bound of the generalisation loss of either the error [28, 119], the fairness guarantees [98, 41] or both [72] characterised by the training loss and/or unfairness, model complexity and confidence.",
                "[41] considered that the distribution shift is caused by demographic shift, and assumed that the demographic proportions (i.",
                "The dissimilarity between training and deployment environments can significantly deteriorate and potentially cause harm in fairness-critical applications [17, 27, 41]."
            ],
            "citingPaper": {
                "paperId": "7b5cbc587ee7c51945f6b6fecb3ff5ddce52e68c",
                "externalIds": {
                    "ArXiv": "2211.07530",
                    "DBLP": "journals/corr/abs-2211-07530",
                    "DOI": "10.48550/arXiv.2211.07530",
                    "CorpusId": 253510328
                },
                "corpusId": 253510328,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7b5cbc587ee7c51945f6b6fecb3ff5ddce52e68c",
                "title": "A Survey on Preserving Fairness Guarantees in Changing Environments",
                "abstract": "Human lives are increasingly being affected by the outcomes of automated decision-making systems and it is essential for the latter to be, not only accurate, but also fair. The literature of algorithmic fairness has grown considerably over the last decade, where most of the approaches are evaluated under the strong assumption that the train and test samples are independently and identically drawn from the same underlying distribution. However, in practice, dissimilarity between the training and deployment environments exists, which compromises the performance of the decision-making algorithm as well as its fairness guarantees in the deployment data. There is an emergent research line that studies how to preserve fairness guarantees when the data generating processes differ between the source (train) and target (test) domains, which is growing remarkably. With this survey, we aim to provide a wide and unifying overview on the topic. For such purpose, we propose a taxonomy of the existing approaches for fair classification under distribution shift, highlight benchmarking alternatives, point out the relation with other similar research fields and eventually, identify future venues of research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2190750645",
                        "name": "Ainhize Barrainkua"
                    },
                    {
                        "authorId": "51130398",
                        "name": "Paula Gordaliza"
                    },
                    {
                        "authorId": "144762651",
                        "name": "J. A. Lozano"
                    },
                    {
                        "authorId": "1704531",
                        "name": "Novi Quadrianto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "about the in-field distribution is known) [18]."
            ],
            "citingPaper": {
                "paperId": "dbf9f58779eb554c7be22113bf78e9df78853519",
                "externalIds": {
                    "DBLP": "conf/sigsoft/Brun22",
                    "DOI": "10.1145/3549034.3570200",
                    "CorpusId": 253368880
                },
                "corpusId": 253368880,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dbf9f58779eb554c7be22113bf78e9df78853519",
                "title": "The promise and perils of using machine learning when engineering software (keynote paper)",
                "abstract": "Machine learning has radically changed what computing can accomplish, including the limits of what software engineering can do. I will discuss recent software engineering advances machine learning has enabled, from automatically repairing software bugs to data-driven software systems that automatically learn to make decisions. Unfortunately, with the promises of these new technologies come serious perils. For example, automatically generated program patches can break as much functionality as they repair. And self-learning, data-driven software can make decisions that result in unintended consequences, including unsafe, racist, or sexist behavior. But to build solutions to these shortcomings we may need to look no further than machine learning itself. I will introduce multiple ways machine learning can help verify software properties, leading to higher-quality systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2932798",
                        "name": "Yuriy Brun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "While recent work explores methods to transfer fairness [54, 48, 23], most considered settings fall into subpopulation shifts.",
                "[23] Stephen Giguere, Blossom Metevier, Yuriy Brun, Philip S.",
                "[16] uses reweighting to deal with fairness problem under covariate shift and [23] uses reweighting together with a fairness test to guarantee the fairness under demographic shift."
            ],
            "citingPaper": {
                "paperId": "a197a8aff0af542ce75ecf900ae178e13c0b1656",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-12796",
                    "ArXiv": "2206.12796",
                    "DOI": "10.48550/arXiv.2206.12796",
                    "CorpusId": 250072167
                },
                "corpusId": 250072167,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a197a8aff0af542ce75ecf900ae178e13c0b1656",
                "title": "Transferring Fairness under Distribution Shifts via Fair Consistency Regularization",
                "abstract": "The increasing reliance on ML models in high-stakes tasks has raised a major concern on fairness violations. Although there has been a surge of work that improves algorithmic fairness, most of them are under the assumption of an identical training and test distribution. In many real-world applications, however, such an assumption is often violated as previously trained fair models are often deployed in a different environment, and the fairness of such models has been observed to collapse. In this paper, we study how to transfer model fairness under distribution shifts, a widespread issue in practice. We conduct a fine-grained analysis of how the fair model is affected under different types of distribution shifts and find that domain shifts are more challenging than subpopulation shifts. Inspired by the success of self-training in transferring accuracy under domain shifts, we derive a sufficient condition for transferring group fairness. Guided by it, we propose a practical algorithm with a fair consistency regularization as the key component. A synthetic dataset benchmark, which covers all types of distribution shifts, is deployed for experimental verification of the theoretical findings. Experiments on synthetic and real datasets including image and tabular data demonstrate that our approach effectively transfers fairness and accuracy under various distribution shifts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49640821",
                        "name": "Bang An"
                    },
                    {
                        "authorId": "2173680156",
                        "name": "Zora Che"
                    },
                    {
                        "authorId": "52184822",
                        "name": "Mucong Ding"
                    },
                    {
                        "authorId": "40070055",
                        "name": "Furong Huang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8a58b886297649da28e70a4476db1093c254946b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-11423",
                    "ArXiv": "2206.11423",
                    "DOI": "10.48550/arXiv.2206.11423",
                    "CorpusId": 249953531
                },
                "corpusId": 249953531,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8a58b886297649da28e70a4476db1093c254946b",
                "title": "Input-agnostic Certified Group Fairness via Gaussian Parameter Smoothing",
                "abstract": "Only recently, researchers attempt to provide classification algorithms with provable group fairness guarantees. Most of these algorithms suffer from harassment caused by the requirement that the training and deployment data follow the same distribution. This paper proposes an input-agnostic certified group fairness algorithm, FairSmooth, for improving the fairness of classification models while maintaining the remarkable prediction accuracy. A Gaussian parameter smoothing method is developed to transform base classifiers into their smooth versions. An optimal individual smooth classifier is learnt for each group with only the data regarding the group and an overall smooth classifier for all groups is generated by averaging the parameters of all the individual smooth ones. By leveraging the theory of nonlinear functional analysis, the smooth classifiers are reformulated as output functions of a Nemytskii operator. Theoretical analysis is conducted to derive that the Nemytskii operator is smooth and induces a Frechet differentiable smooth manifold. We theoretically demonstrate that the smooth manifold has a global Lipschitz constant that is independent of the domain of the input data, which derives the input-agnostic certified group fairness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "103340106",
                        "name": "Jiayin Jin"
                    },
                    {
                        "authorId": "2118690556",
                        "name": "Zeru Zhang"
                    },
                    {
                        "authorId": "2145499198",
                        "name": "Yang Zhou"
                    },
                    {
                        "authorId": "3008832",
                        "name": "Lingfei Wu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "These guarantees can even extend to settings when the distribution of the training data is different from that of the data to which the model is applied [13]."
            ],
            "citingPaper": {
                "paperId": "b403d7b0e6b85f5b9f8f511c5ffe49e620a0f06c",
                "externalIds": {
                    "DBLP": "conf/icse/JohnsonB22",
                    "DOI": "10.1145/3510454.3516830",
                    "CorpusId": 246973496
                },
                "corpusId": 246973496,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b403d7b0e6b85f5b9f8f511c5ffe49e620a0f06c",
                "title": "Fairkit-learn: A Fairness Evaluation and Comparison Toolkit",
                "abstract": "Advances in how we build and use software, specifically the integration of machine learning for decision making, have led to widespread concern around model and software fairness. We present fairkit-learn, an interactive Python toolkit designed to support data scientists\u2019 ability to reason about and understand model fairness. We outline how fairkit-learn can support model training, evaluation, and comparison and describe the potential benefit that comes with using fairkit-learn in comparison to the state-of-the-art. Fairkit-learn is open source at https://go.gmu.edu/fairkit-learn/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143777369",
                        "name": "Brittany Johnson"
                    },
                    {
                        "authorId": "2932798",
                        "name": "Yuriy Brun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", that the component will not exhibit racist or sexist behavior when applied to future inputs [5, 46, 64, 96]."
            ],
            "citingPaper": {
                "paperId": "22ea3cd56bfbad81e7a7f8d024e710d06f2a2e5d",
                "externalIds": {
                    "ArXiv": "2103.06091",
                    "DBLP": "journals/tosem/BrunLSME23",
                    "DOI": "10.1145/3571850",
                    "CorpusId": 232170324
                },
                "corpusId": 232170324,
                "publicationVenue": {
                    "id": "0730105a-4941-449f-9450-28cba8ae056b",
                    "name": "ACM Transactions on Software Engineering and Methodology",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Softw Eng Methodol"
                    ],
                    "issn": "1049-331X",
                    "url": "http://www.acm.org/pubs/contents/journals/tosem/",
                    "alternate_urls": [
                        "https://tosem.acm.org/",
                        "http://tosem.acm.org/",
                        "http://www.acm.org/pubs/tosem/",
                        "http://portal.acm.org/tosem"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/22ea3cd56bfbad81e7a7f8d024e710d06f2a2e5d",
                "title": "Blindspots in Python and Java APIs Result in Vulnerable Code",
                "abstract": "Blindspots in APIs can cause software engineers to introduce vulnerabilities, but such blindspots are, unfortunately, common. We study the effect APIs with blindspots have on developers in two languages by replicating a 109-developer, 24-Java-API controlled experiment. Our replication applies to Python and involves 129 new developers and 22 new APIs. We find that using APIs with blindspots statistically significantly reduces the developers\u2019 ability to correctly reason about the APIs in both languages, but that the effect is more pronounced for Python. Interestingly, for Java, the effect increased with complexity of the code relying on the API, whereas for Python, the opposite was true. This suggests that Python developers are less likely to notice potential for vulnerabilities in complex code than in simple code, whereas Java developers are more likely to recognize the extra complexity and apply more care, but are more careless with simple code. Whether the developers considered API uses to be more difficult, less clear, and less familiar did not have an effect on their ability to correctly reason about them. Developers with better long-term memory recall were more likely to correctly reason about APIs with blindspots, but short-term memory, processing speed, episodic memory, and memory span had no effect. Surprisingly, professional experience and expertise did not improve the developers\u2019 ability to reason about APIs with blindspots across both languages, with long-term professionals with many years of experience making mistakes as often as relative novices. Finally, personality traits did not significantly affect the Python developers\u2019 ability to reason about APIs with blindspots, but less extroverted and more open developers were better at reasoning about Java APIs with blindspots. Overall, our findings suggest that blindspots in APIs are a serious problem across languages, and that experience and education alone do not overcome that problem, suggesting that tools are needed to help developers recognize blindspots in APIs as they write code that uses those APIs.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2932798",
                        "name": "Yuriy Brun"
                    },
                    {
                        "authorId": "143677437",
                        "name": "Tian Lin"
                    },
                    {
                        "authorId": "2052356053",
                        "name": "J. Somerville"
                    },
                    {
                        "authorId": "1740861427",
                        "name": "Elisha M Myers"
                    },
                    {
                        "authorId": "32293759",
                        "name": "Natalie C. Ebner"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "259840344",
                "publicationVenue": null,
                "url": null,
                "title": "P R PPPP RRRRRR oofster: Automated Formal Veri\ufb01cation",
                "abstract": null,
                "year": null,
                "authors": []
            }
        }
    ]
}