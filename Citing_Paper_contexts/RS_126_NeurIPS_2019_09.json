{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2e7aecfefb8f008a6371a3b9eddacf7b827b63ae",
                "externalIds": {
                    "PubMedCentral": "10501567",
                    "DOI": "10.1371/journal.pone.0291545",
                    "CorpusId": 261884250,
                    "PubMed": "37708154"
                },
                "corpusId": 261884250,
                "publicationVenue": {
                    "id": "0aed7a40-85f3-4c66-9e1b-c1556c57001b",
                    "name": "PLoS ONE",
                    "type": "journal",
                    "alternate_names": [
                        "Plo ONE",
                        "PLOS ONE",
                        "PLO ONE"
                    ],
                    "issn": "1932-6203",
                    "url": "https://journals.plos.org/plosone/",
                    "alternate_urls": [
                        "http://www.plosone.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2e7aecfefb8f008a6371a3b9eddacf7b827b63ae",
                "title": "Sample-efficient multi-agent reinforcement learning with masked reconstruction",
                "abstract": "Deep reinforcement learning (DRL) is a powerful approach that combines reinforcement learning (RL) and deep learning to address complex decision-making problems in high-dimensional environments. Although DRL has been remarkably successful, its low sample efficiency necessitates extensive training times and large amounts of data to learn optimal policies. These limitations are more pronounced in the context of multi-agent reinforcement learning (MARL). To address these limitations, various studies have been conducted to improve DRL. In this study, we propose an approach that combines a masked reconstruction task with QMIX (M-QMIX). By introducing a masked reconstruction task as an auxiliary task, we aim to achieve enhanced sample efficiency\u2014a fundamental limitation of RL in multi-agent systems. Experiments were conducted using the StarCraft II micromanagement benchmark to validate the effectiveness of the proposed method. We used 11 scenarios comprising five easy, three hard, and three very hard scenarios. We particularly focused on using a limited number of time steps for each scenario to demonstrate the improved sample efficiency. Compared to QMIX, the proposed method is superior in eight of the 11 scenarios. These results provide strong evidence that the proposed method is more sample-efficient than QMIX, demonstrating that it effectively addresses the limitations of DRL in multi-agent systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2241417337",
                        "name": "Jung In Kim"
                    },
                    {
                        "authorId": "2116466706",
                        "name": "Young Jae Lee"
                    },
                    {
                        "authorId": "2198592815",
                        "name": "Jongkook Heo"
                    },
                    {
                        "authorId": "2109377105",
                        "name": "J. Park"
                    },
                    {
                        "authorId": "2133383668",
                        "name": "Jaehoon Kim"
                    },
                    {
                        "authorId": "2198614994",
                        "name": "Sae Rin Lim"
                    },
                    {
                        "authorId": "2242086104",
                        "name": "Jinyong Jeong"
                    },
                    {
                        "authorId": "2183723706",
                        "name": "Seoung Bum Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, some researchers [14] have put forward a new state representation technique which exploits the spatial-temporal nature of visual observations in an RL setting."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "06e21f9e53495457f84b4295e0f85c312781228f",
                "externalIds": {
                    "DBLP": "journals/tciaig/ZhaoSZZL23",
                    "DOI": "10.1109/TG.2022.3164470",
                    "CorpusId": 247959723
                },
                "corpusId": 247959723,
                "publicationVenue": {
                    "id": "c8bd8d97-dbc6-4af1-aa49-9a852cb59f96",
                    "name": "IEEE Transactions on Games",
                    "alternate_names": [
                        "IEEE Trans Game"
                    ],
                    "issn": "2475-1502",
                    "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=7782673",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7782673"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/06e21f9e53495457f84b4295e0f85c312781228f",
                "title": "Improving Deep Reinforcement Learning With Mirror Loss",
                "abstract": "Recent years have witnessed the great breakthrough of deep reinforcement learning (DRL) in various artificial intelligence applications, but the training process needs a very large amount of samples and huge computational costs. To alleviate the low sample efficiency issue, one feasible solution is to improve the state representation learning. We uncover that the agents, trained by the original DRL algorithm, face severe performance degradation in mirrored game environments. As mirror symmetry is an important property of the environment, the poor performance in the mirrored situation indicates that the agents are not fully aware of the essence of the environment. In order to handle this problem and make use of the property to attain better state representation, we propose a mirror loss, which serves as an auxiliary module to bring mirror symmetry representation to the DRL agent. It is model-agnostic and prompts the DRL agent to make logically consistent mirrored actions in the mirrored environment. We conduct experiments on OpenAI Gym Atari environments and a more complex reinforcement learning task, Mahjong AI, and the results demonstrate the efficiency and versatility of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "7818567",
                        "name": "Jian Zhao"
                    },
                    {
                        "authorId": "2161465218",
                        "name": "Weide Shu"
                    },
                    {
                        "authorId": "2151271582",
                        "name": "Youpeng Zhao"
                    },
                    {
                        "authorId": "38272296",
                        "name": "Wen-gang Zhou"
                    },
                    {
                        "authorId": "2108508109",
                        "name": "Houqiang Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In particular, we follow the linear probing protocol introduced in [22] and run simple linear regression on game-state variables following the protocol of [21].",
                "We conduct linear probing tasks [22] on the derived content and style embeddings (using the 3D-SSL and Gen11 datasets, respectively) to recover relevant game state information (i."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b26fafffe8aa4e7c88294dabfa657b9c8c47e700",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-11141",
                    "ArXiv": "2307.11141",
                    "DOI": "10.48550/arXiv.2307.11141",
                    "CorpusId": 260091544
                },
                "corpusId": 260091544,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b26fafffe8aa4e7c88294dabfa657b9c8c47e700",
                "title": "Towards General Game Representations: Decomposing Games Pixels into Content and Style",
                "abstract": "On-screen game footage contains rich contextual information that players process when playing and experiencing a game. Learning pixel representations of games can benefit artificial intelligence across several downstream tasks including game-playing agents, procedural content generation, and player modelling. The generalizability of these methods, however, remains a challenge, as learned representations should ideally be shared across games with similar game mechanics. This could allow, for instance, game-playing agents trained on one game to perform well in similar games with no re-training. This paper explores how generalizable pre-trained computer vision encoders can be for such tasks, by decomposing the latent space into content embeddings and style embeddings. The goal is to minimize the domain gap between games of the same genre when it comes to game content critical for downstream tasks, and ignore differences in graphical style. We employ a pre-trained Vision Transformer encoder and a decomposition technique based on game genres to obtain separate content and style embeddings. Our findings show that the decomposed embeddings achieve style invariance across multiple games while still maintaining strong content extraction capabilities. We argue that the proposed decomposition of content and style offers better generalization capacities across game environments independently of the downstream task.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "37498103",
                        "name": "C. Trivedi"
                    },
                    {
                        "authorId": "2091878621",
                        "name": "Konstantinos Makantasis"
                    },
                    {
                        "authorId": "1713331",
                        "name": "Antonios Liapis"
                    },
                    {
                        "authorId": "1686193",
                        "name": "Georgios N. Yannakakis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2f650ef6219917934b52db91275bb7569a1ed282",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-01403",
                    "ArXiv": "2307.01403",
                    "DOI": "10.48550/arXiv.2307.01403",
                    "CorpusId": 259341905
                },
                "corpusId": 259341905,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2f650ef6219917934b52db91275bb7569a1ed282",
                "title": "Learning to Communicate using Contrastive Learning",
                "abstract": "Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144299297",
                        "name": "Y. Lo"
                    },
                    {
                        "authorId": "39599054",
                        "name": "B. Sengupta"
                    },
                    {
                        "authorId": "1412004702",
                        "name": "J. Foerster"
                    },
                    {
                        "authorId": "41020834",
                        "name": "Michael Noukhovitch"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In case when K > 1, because if E[Zt+K , [Zt, Ut, ..., Ut+K\u22121]] = E[St+K , [St, At, ..., At+K\u22121]], then for any 1 \u2264 k \u2264 K, E[Zt+k, [Zt, Ut, ..., Ut+k\u22121]] = E[St+k, [St, At, ..., At+k\u22121]], including K = 1, by Data processing Inequality.",
                "Then, our method aims to maximize the mutual information between representations of current states paired with action sequences and representations of the corresponding future states: JTACO = I(Zt+K ; [Zt, Ut, ..., Ut+K\u22121]) (2) Here, K \u2265 1 is a fixed hyperparameter for the prediction horizon.",
                ", Ut+K\u22121]) (2) Here, K \u2265 1 is a fixed hyperparameter for the prediction horizon.",
                "CPC (47), ST-DIM (2), and ATC (44) integrate temporal relationships into the contrastive loss by maximizing mutual information between current state representations (or state histories encoded by LSTM in CPC) and future state representations.",
                "Let K \u2208 N+, and JTACO = I(Zt+K ; [Zt, Ut, ..., Ut+K\u22121]).",
                "In CURL (33), it treats augmented states as positive pairs, but it neglects the temporal dependency of MDP. CPC (47), ST-DIM (2), and ATC (44) integrate temporal relationships into the contrastive loss by maximizing mutual information between current state representations (or state histories encoded by LSTM in CPC) and future state representations.",
                "This theorem guarantees that if our mutual information objective Equation (2) is maximized, then for any two state-action pairs (s1, a1) and (s2, a2) with equivalent state and action representations, their optimal action-value functions,Q\u2217(s1, a1) andQ\u2217(s2, a2), will be equal.",
                "This is because p(Rt|St = s1, At = a1) = p(Rt|St = s2, At = a2) by Equation (18) as p(zt|St = s1) = p(zt|St = s2), p(ut|At = a1) = p(ut|At = a2).",
                "This theorem guarantees that if our mutual information objective Equation (2) is maximized, then for any two state-action pairs (s1, a1) and (s2, a2) with equivalent state and action representations, their optimal action-value functions,Q(s1, a1) andQ(s2, a2), will be equal.",
                "Subsequent works, including CPC (23), ST-DIM (2), and ATC (44), made progress to rectify this by integrating temporal elements into the contrastive loss, linking pairs of observations with short temporal intervals.",
                "= x)dz\nProof of Theorem 3.1: Based on the graphical model, it is clear that\nmax \u03c6,\u03c8 I(Zt+K , [Zt, Ut, ..., Ut+K\u22121]) = I(St+K ; [St, At, ..., At+K\u22121]) (16)\nNow define the random variable of return-to-go Rt such that\nRt = H\u2212t\u2211 k=0 \u03b3kRt+k (17)\nBased on Proposition E.1, because\nI(Zt+K ;Zt, Ut:t+K\u22121) = I(St+K ;St, At:t+K\u22121)\nwe could conclude that\nI(Rt+K ;Zt, Ut:t+K\u22121) = I(Rt+K ;St, At:t+K\u22121) (18)\nNow applying Proposition E.2, we get\nEp(zt,ut:t+K\u22121|St=s,At:t+K\u22121=at:t+K\u22121)[p(Rt|Zt, Ut:t+K\u22121)] = p(Rt|St = s,At:t+K\u22121) (19)\nAs a result, when K = 1, for any reward function r, given a state-action pair (s1, a1), (s2, a2) such that \u03c6(s1) = \u03c6(s2), \u03c8(a1) = \u03c8(a2), we have Qr(s1, a1) = Ep(Rt|St=s1,At=a1)[Rt] = Ep(Rt|St=s2,At=a2)[Rt]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "157cbfb5837711ac242ed7eeeb2aa6f3bff68184",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-13229",
                    "ArXiv": "2306.13229",
                    "DOI": "10.48550/arXiv.2306.13229",
                    "CorpusId": 259244013
                },
                "corpusId": 259244013,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/157cbfb5837711ac242ed7eeeb2aa6f3bff68184",
                "title": "TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning",
                "abstract": "Despite recent progress in reinforcement learning (RL) from raw pixel data, sample inefficiency continues to present a substantial obstacle. Prior works have attempted to address this challenge by creating self-supervised auxiliary tasks, aiming to enrich the agent's learned representations with control-relevant information for future state prediction. However, these objectives are often insufficient to learn representations that can represent the optimal policy or value function, and they often consider tasks with small, abstract discrete action spaces and thus overlook the importance of action representation learning in continuous control. In this paper, we introduce TACO: Temporal Action-driven Contrastive Learning, a simple yet powerful temporal contrastive learning approach that facilitates the concurrent acquisition of latent state and action representations for agents. TACO simultaneously learns a state and an action representation by optimizing the mutual information between representations of current states paired with action sequences and representations of the corresponding future states. Theoretically, TACO can be shown to learn state and action representations that encompass sufficient information for control, thereby improving sample efficiency. For online RL, TACO achieves 40% performance boost after one million environment interaction steps on average across nine challenging visual continuous control tasks from Deepmind Control Suite. In addition, we show that TACO can also serve as a plug-and-play module adding to existing offline visual RL methods to establish the new state-of-the-art performance for offline visual RL across offline datasets with varying quality.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108815599",
                        "name": "Ruijie Zheng"
                    },
                    {
                        "authorId": "48630770",
                        "name": "Xiyao Wang"
                    },
                    {
                        "authorId": "120738683",
                        "name": "Yanchao Sun"
                    },
                    {
                        "authorId": "2118867113",
                        "name": "Shuang Ma"
                    },
                    {
                        "authorId": "2110117732",
                        "name": "Jieyu Zhao"
                    },
                    {
                        "authorId": "3286703",
                        "name": "Huazhe Xu"
                    },
                    {
                        "authorId": "2200167546",
                        "name": "Hal Daum'e"
                    },
                    {
                        "authorId": "40070055",
                        "name": "Furong Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Schwarzer et al. (2020) and Anand et al. (2019) did the same in the Atari game-playing environments."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "300bf71c102fe349a082482f791ba15795fe69cb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-12554",
                    "ArXiv": "2306.12554",
                    "DOI": "10.48550/arXiv.2306.12554",
                    "CorpusId": 259224572
                },
                "corpusId": 259224572,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/300bf71c102fe349a082482f791ba15795fe69cb",
                "title": "Improving Long-Horizon Imitation Through Instruction Prediction",
                "abstract": "Complex, long-horizon planning and its combinatorial nature pose steep challenges for learning-based agents. Difficulties in such settings are exacerbated in low data regimes where over-fitting stifles generalization and compounding errors hurt accuracy. In this work, we explore the use of an often unused source of auxiliary supervision: language. Inspired by recent advances in transformer-based models, we train agents with an instruction prediction loss that encourages learning temporally extended representations that operate at a high level of abstraction. Concretely, we demonstrate that instruction modeling significantly improves performance in planning environments when training with a limited number of demonstrations on the BabyAI and Crafter benchmarks. In further analysis we find that instruction modeling is most important for tasks that require complex reasoning, while understandably offering smaller gains in environments that require simple plans. More details and code can be found at \\url{https://github.com/jhejna/instruction-prediction}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2122700519",
                        "name": "Joey Hejna"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    },
                    {
                        "authorId": "34026610",
                        "name": "Lerrel Pinto"
                    }
                ]
            }
        },
        {
            "contexts": [
                "game that is considered, according to the annotations provided in (Anand et al., 2019).",
                "Moreover, we postprocess this additional information by only selecting the subset of variables that are relevant to the\ngame that is considered, according to the annotations provided in (Anand et al., 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8df5487087f2fa0291acdd510c89f2c78adbc034",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-11488",
                    "ArXiv": "2306.11488",
                    "DOI": "10.48550/arXiv.2306.11488",
                    "CorpusId": 259202914
                },
                "corpusId": 259202914,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8df5487087f2fa0291acdd510c89f2c78adbc034",
                "title": "Informed POMDP: Leveraging Additional Information in Model-Based RL",
                "abstract": "In this work, we generalize the problem of learning through interaction in a POMDP by accounting for eventual additional information available at training time. First, we introduce the informed POMDP, a new learning paradigm offering a clear distinction between the training information and the execution observation. Next, we propose an objective for learning a sufficient statistic from the history for the optimal control that leverages this information. We then show that this informed objective consists of learning an environment model from which we can sample latent trajectories. Finally, we show for the Dreamer algorithm that the convergence speed of the policies is sometimes greatly improved on several environments by using this informed environment model. Those results and the simplicity of the proposed adaptation advocate for a systematic consideration of eventual additional information when learning in a POMDP using model-based RL.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2131349333",
                        "name": "Gaspard Lambrechts"
                    },
                    {
                        "authorId": "1583086071",
                        "name": "Adrien Bolland"
                    },
                    {
                        "authorId": "1751167",
                        "name": "D. Ernst"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The distance reward \ud835\udc45dist requires the agent location at each step, we propose to either use a specifically trained object detector, which was tested with a pre-trained FasterRCNN model [25] fine-tuned on 100 manually labelled training examples, or to use the RAM state labels provided via the AtariARI Wrapper [1].",
                "The distance reward Rdist requires the agent location at each step, we propose to either use a specifically trained object detector, which was tested with a pre-trained FasterRCNN model [25] fine-tuned on 100 manually labelled training examples, or to use the RAM state labels provided via the AtariARI Wrapper [1].",
                "The required state information can be extracted directly from the ALE via an environment wrapper called Atari Annotated RAM Interface (AtariARI) [1]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1d6a31eec49742240ea5d9a666ec0beb6dc79333",
                "externalIds": {
                    "ArXiv": "2306.11483",
                    "DBLP": "journals/corr/abs-2306-11483",
                    "DOI": "10.48550/arXiv.2306.11483",
                    "CorpusId": 259202873
                },
                "corpusId": 259202873,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1d6a31eec49742240ea5d9a666ec0beb6dc79333",
                "title": "Int-HRL: Towards Intention-based Hierarchical Reinforcement Learning",
                "abstract": "While deep reinforcement learning (RL) agents outperform humans on an increasing number of tasks, training them requires data equivalent to decades of human gameplay. Recent hierarchical RL methods have increased sample efficiency by incorporating information inherent to the structure of the decision problem but at the cost of having to discover or use human-annotated sub-goals that guide the learning process. We show that intentions of human players, i.e. the precursor of goal-oriented decisions, can be robustly predicted from eye gaze even for the long-horizon sparse rewards task of Montezuma's Revenge - one of the most challenging RL tasks in the Atari2600 game suite. We propose Int-HRL: Hierarchical RL with intention-based sub-goals that are inferred from human eye gaze. Our novel sub-goal extraction pipeline is fully automatic and replaces the need for manual sub-goal annotation by human experts. Our evaluations show that replacing hand-crafted sub-goals with automatically extracted intentions leads to a HRL agent that is significantly more sample efficient than previous methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2128736668",
                        "name": "Anna Penzkofer"
                    },
                    {
                        "authorId": "39734851",
                        "name": "Simon Schaefer"
                    },
                    {
                        "authorId": "51251584",
                        "name": "Florian Strohm"
                    },
                    {
                        "authorId": "31944767",
                        "name": "Mihai B\u00e2ce"
                    },
                    {
                        "authorId": "2864731",
                        "name": "Stefan Leutenegger"
                    },
                    {
                        "authorId": "3194727",
                        "name": "A. Bulling"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To tackle these problems, Anand et al. (2019) have proposed AtariARI, a wrapper around some Atari gymnasium environments, that augments the information dictionary returned at each step with the RAM position of some information.",
                "Nevertheless, only a few methods regarding OCR, like Anand et al. (2019) (who also introduced AtariARI), Li et al. (2017) and Lin et al. (2020b), have been tested on Atari games so far.",
                "Nevertheless, only a few methods regarding OCR, like Anand et al. (2019) (who also introduced AtariARI), Li et al.",
                "Contrary to Anand et al. (2019), which consulted commented disassemblies or source code of different Atari games for their AtariARI framework, we used other analytical techniques to understand and find where the information is stored in the RAM.",
                "Nevertheless, only a few methods regarding OCR, like Anand et al. (2019) (who also introduced AtariARI), Li et al. (2017) and Lin et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "15c278aef68dcda620f8139c7a0bb66490c18101",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-08649",
                    "ArXiv": "2306.08649",
                    "DOI": "10.48550/arXiv.2306.08649",
                    "CorpusId": 259164792
                },
                "corpusId": 259164792,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/15c278aef68dcda620f8139c7a0bb66490c18101",
                "title": "OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments",
                "abstract": "Cognitive science and psychology suggest that object-centric representations of complex scenes are a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep reinforcement learning approaches rely on only pixel-based representations that do not capture the compositional properties of natural scenes. For this, we need environments and datasets that allow us to work and evaluate object-centric approaches. We present OCAtari, a set of environment that provides object-centric state representations of Atari games, the most-used evaluation framework for deep RL approaches. OCAtari also allows for RAM state manipulations of the games to change and create specific or even novel situations. The code base for this work is available at github.com/k4ntz/OC_Atari.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051175030",
                        "name": "Quentin Delfosse"
                    },
                    {
                        "authorId": "2215812879",
                        "name": "Jannis Bl\u00fcml"
                    },
                    {
                        "authorId": "2220098610",
                        "name": "Bjarne Gregori"
                    },
                    {
                        "authorId": "2116865178",
                        "name": "Sebastian Sztwiertnia"
                    },
                    {
                        "authorId": "2113406566",
                        "name": "K. Kersting"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To prevent representational collapse, recent studies have employed the contrastive learning (Anand et al., 2019; Laskin et al., 2020; Oord et al., 2018; Stooke et al., 2021) or architectural designs including the use of batch-normalization and stop-gradient operation (Schwarzer et al., 2020; 2021).",
                "To prevent representational collapse, recent studies have employed the contrastive learning (Anand et al., 2019; Laskin et al., 2020; Oord et al., 2018; Stooke et al., 2021) or architectural designs including the use of batch-normalization and stop-gradient operation (Schwarzer et al.",
                "To prevent representational collapse, contrastive learning (Anand et al., 2019; Laskin et al., 2020; Stooke et al., 2021; Oord et al., 2018) or batch-normalization with stop-gradient operation (Schwarzer et al.",
                "To prevent representational collapse, contrastive learning (Anand et al., 2019; Laskin et al., 2020; Stooke et al., 2021; Oord et al., 2018) or batch-normalization with stop-gradient operation (Schwarzer et al., 2020; 2021) are commonly employed in unsupervised representation learning."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1c73d712dd614d2eebf06f55229348f8e8b46b47",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-05637",
                    "ArXiv": "2306.05637",
                    "DOI": "10.48550/arXiv.2306.05637",
                    "CorpusId": 259129641
                },
                "corpusId": 259129641,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1c73d712dd614d2eebf06f55229348f8e8b46b47",
                "title": "On the Importance of Feature Decorrelation for Unsupervised Representation Learning in Reinforcement Learning",
                "abstract": "Recently, unsupervised representation learning (URL) has improved the sample efficiency of Reinforcement Learning (RL) by pretraining a model from a large unlabeled dataset. The underlying principle of these methods is to learn temporally predictive representations by predicting future states in the latent space. However, an important challenge of this approach is the representational collapse, where the subspace of the latent representations collapses into a low-dimensional manifold. To address this issue, we propose a novel URL framework that causally predicts future states while increasing the dimension of the latent manifold by decorrelating the features in the latent space. Through extensive empirical studies, we demonstrate that our framework effectively learns predictive representations without collapse, which significantly improves the sample efficiency of state-of-the-art URL methods on the Atari 100k benchmark. The code is available at https://github.com/dojeon-ai/SimTPR.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2163406260",
                        "name": "Hojoon Lee"
                    },
                    {
                        "authorId": "2110019663",
                        "name": "Ko-tik Lee"
                    },
                    {
                        "authorId": "2140537743",
                        "name": "Dongyoon Hwang"
                    },
                    {
                        "authorId": "2162205515",
                        "name": "Hyunho Lee"
                    },
                    {
                        "authorId": "7613239",
                        "name": "ByungKun Lee"
                    },
                    {
                        "authorId": "1795455",
                        "name": "J. Choo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "and contrastive representation learning [38; 1; 34; 24; 36; 27]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "57b55c24c03525998379990faecbf2bc23a3f6e1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-03360",
                    "ArXiv": "2306.03360",
                    "DOI": "10.48550/arXiv.2306.03360",
                    "CorpusId": 259088862
                },
                "corpusId": 259088862,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/57b55c24c03525998379990faecbf2bc23a3f6e1",
                "title": "Vid2Act: Activate Offline Videos for Visual RL",
                "abstract": "Pretraining RL models on offline video datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV, sidesteps the accompanied action records in offline datasets and instead focuses on pretraining a task-irrelevant, action-free world model within the source domains. We present Vid2Act, a model-based RL method that learns to transfer valuable action-conditioned dynamics and potentially useful action demonstrations from offline to online settings. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the domain relevance for both dynamics representation transfer and policy transfer. Specifically, we train the world models to generate a set of time-varying task similarities using a domain-selective knowledge distillation loss. These similarities serve two purposes: (i) adaptively transferring the most useful source knowledge to facilitate dynamics learning, and (ii) learning to replay the most relevant source actions to guide the target policy. We demonstrate the advantages of Vid2Act over the action-free visual RL pretraining method in both Meta-World and DeepMind Control Suite.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2027156855",
                        "name": "Minting Pan"
                    },
                    {
                        "authorId": "2219666710",
                        "name": "Yitao Zheng"
                    },
                    {
                        "authorId": "2108270774",
                        "name": "Wendong Zhang"
                    },
                    {
                        "authorId": "3530540",
                        "name": "Yunbo Wang"
                    },
                    {
                        "authorId": "2157189845",
                        "name": "Xiaokang Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While feature learning of this type has found some success in the RL setting, it has been mainly limited to vision-based environments [Jaderberg et al., 2017, Oord et al., 2018, Anand et al., 2019, Laskin et al., 2020, Stooke et al., 2021, Yarats et al., 2022]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "487b7090af70989dde3a0e16bc5d014eb5dfd498",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-02451",
                    "ArXiv": "2306.02451",
                    "DOI": "10.48550/arXiv.2306.02451",
                    "CorpusId": 259075901
                },
                "corpusId": 259075901,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/487b7090af70989dde3a0e16bc5d014eb5dfd498",
                "title": "For SALE: State-Action Representation Learning for Deep Reinforcement Learning",
                "abstract": "In the field of reinforcement learning (RL), representation learning is a proven tool for complex image-based tasks, but is often overlooked for environments with low-level states, such as physical control problems. This paper introduces SALE, a novel approach for learning embeddings that model the nuanced interaction between state and action, enabling effective representation learning from low-level states. We extensively study the design space of these embeddings and highlight important design considerations. We integrate SALE and an adaptation of checkpoints for RL into TD3 to form the TD7 algorithm, which significantly outperforms existing continuous control algorithms. On OpenAI gym benchmark tasks, TD7 has an average performance gain of 276.7% and 50.7% over TD3 at 300k and 5M time steps, respectively, and works in both the online and offline settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "14637819",
                        "name": "Scott Fujimoto"
                    },
                    {
                        "authorId": "24064144",
                        "name": "Wei-Di Chang"
                    },
                    {
                        "authorId": "143757896",
                        "name": "Edward James Smith"
                    },
                    {
                        "authorId": "2046135",
                        "name": "S. Gu"
                    },
                    {
                        "authorId": "144368601",
                        "name": "Doina Precup"
                    },
                    {
                        "authorId": "2462512",
                        "name": "D. Meger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In [1], the representations for RL algorithms are learned by maximizing mutual",
                "While the existing methods [1, 10, 38, 40, 43, 44, 46] feeds the stacked frames to the encoder at once, which can be viewed as an early fusion [32], our method generates the set of the latent representations individually with the encoder.",
                "To address this challenge, a number of deep RL approaches [1,10,38,40,43,44,46] leverage the recent advance of self-supervised learning which effectively extracts highlevel features from raw pixels in an unsupervised fashion."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ddb320f3925b4865b0b04aa397964dcc71c1c65f",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChoiLSJSM23",
                    "DOI": "10.1109/CVPR52729.2023.01447",
                    "CorpusId": 261060722
                },
                "corpusId": 261060722,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ddb320f3925b4865b0b04aa397964dcc71c1c65f",
                "title": "Local-Guided Global: Paired Similarity Representation for Visual Reinforcement Learning",
                "abstract": "Recent vision-based reinforcement learning (RL) methods have found extracting high-level features from raw pixels with self-supervised learning to be effective in learning policies. However, these methods focus on learning global representations of images, and disregard local spatial structures present in the consecutively stacked frames. In this paper, we propose a novel approach, termed self-supervised Paired Similarity Representation Learning (PSRL) for effectively encoding spatial structures in an unsupervised manner. Given the input frames, the latent volumes are first generated individually using an encoder, and they are used to capture the variance in terms of local spatial structures, i.e., correspondence maps among multiple frames. This enables for providing plenty of fine-grained samples for training the encoder of deep RL. We further attempt to learn the global semantic representations in the action aware transform module that predicts future state representations using action vectors as a medium. The proposed method imposes similarity constraints on the three latent volumes; transformed query representations by estimated pixel-wise correspondence, predicted query representations from the action aware transform model, and target representations of future state, guiding action aware transform with locality-inherent volume. Experimental results on complex tasks in Atari Games and DeepMind Control Suite demonstrate that the RL methods are significantly boosted by the proposed self-supervised learning of paired similarity representations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1971492765",
                        "name": "Hyesong Choi"
                    },
                    {
                        "authorId": "2110726838",
                        "name": "Hunsang Lee"
                    },
                    {
                        "authorId": "66401313",
                        "name": "Wonil Song"
                    },
                    {
                        "authorId": "9535835",
                        "name": "Sangryul Jeon"
                    },
                    {
                        "authorId": "144442279",
                        "name": "K. Sohn"
                    },
                    {
                        "authorId": "2065130",
                        "name": "Dongbo Min"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous SSL methods in [2] have used a single output head with 256 hidden units.",
                "results of using a single output head with 256 hidden units, which are taken from [2].",
                "The performance of DIM-UA and other SRL methods is evaluated on the 19 games of the AtariARI benchmark.",
                "State representation learning (SRL) [2, 16, 22] focuses on learning representations from input data that are typically collected in a reinforcement learning (RL) environment.",
                "There are five categories of state variables in AtariARI [2], which are agent localization (Agent Loc.",
                "The data for pretraining and probing are collected by an RL agent running a certain number of steps using a random policy since it was found that the samples collected by a random policy could be more favorable than those collected by policy gradient policies for the SSL methods [2].",
                "We experiment with our UA paradigm using the SRL algorithm ST-DIM [2], and propose DIM-UA.",
                "(2) We compare DIM-UA with other SRL algorithms using samples collected from 19 Atari games of the AtariARI benchmark and illustrate that our algorithm achieves the best performance in terms of F1 scores and accuracy.",
                "Our contribution is summarized as follows: (1) We modify the state-of-the-art SRL algorithm ST-DIM [2] with our UA paradigm and introduce a new algorithm called DIM-UA.",
                "We present our UA paradigm for representation learning using an unbalanced atlas and design a new SRL method based on the UA paradigm.",
                "We demonstrate that our paradigm improves the state-of-the-art SRL methods significantly by following the SSL experimental pipeline and comparing the probe F1 scores and accuracy."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5164dc3cfed3da81dc960c341afde13c29bd8032",
                "externalIds": {
                    "ArXiv": "2305.10267",
                    "DBLP": "journals/corr/abs-2305-10267",
                    "DOI": "10.48550/arXiv.2305.10267",
                    "CorpusId": 258740745
                },
                "corpusId": 258740745,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5164dc3cfed3da81dc960c341afde13c29bd8032",
                "title": "State Representation Learning Using an Unbalanced Atlas",
                "abstract": "The manifold hypothesis posits that high-dimensional data often lies on a lower-dimensional manifold and that utilizing this manifold as the target space yields more efficient representations. While numerous traditional manifold-based techniques exist for dimensionality reduction, their application in self-supervised learning has witnessed slow progress. The recent MSIMCLR method combines manifold encoding with SimCLR but requires extremely low target encoding dimensions to outperform SimCLR, limiting its applicability. This paper introduces a novel learning paradigm using an unbalanced atlas (UA), capable of surpassing state-of-the-art self-supervised learning approaches. We meticulously investigated and engineered the DeepInfomax with an unbalanced atlas (DIM-UA) method by systematically adapting the Spatiotemporal DeepInfomax (ST-DIM) framework to align with our proposed UA paradigm, employing rigorous scientific methodologies throughout the process. The efficacy of DIM-UA is demonstrated through training and evaluation on the Atari Annotated RAM Interface (AtariARI) benchmark, a modified version of the Atari 2600 framework that produces annotated image samples for representation learning. The UA paradigm improves the existing algorithm significantly when the number of target encoding dimensions grows. For instance, the mean F1 score averaged over categories of DIM-UA is ~75% compared to ~70% of ST-DIM when using 16384 hidden units.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2117876649",
                        "name": "Li Meng"
                    },
                    {
                        "authorId": "49052946",
                        "name": "Morten Goodwin"
                    },
                    {
                        "authorId": "145191958",
                        "name": "A. Yazidi"
                    },
                    {
                        "authorId": "145608672",
                        "name": "P. Engelstad"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026policy gradient\nPolicy Gradient Methods in the Presence of Symmetries and State Abstractions\nlearning (Hjelm et al., 2018; Liu and Abbeel, 2021), or self-supervised learning (Anand et al., 2019; Sinha et al., 2021; Hansen et al., 2020; Hansen and Wang, 2021; Fan et al., 2021).",
                ", 2018; Liu and Abbeel, 2021), or self-supervised learning (Anand et al., 2019; Sinha et al., 2021; Hansen et al., 2020; Hansen and Wang, 2021; Fan et al., 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3960ea842b8b89819f80cf2cd77adc321ff744a1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-05666",
                    "ArXiv": "2305.05666",
                    "DOI": "10.48550/arXiv.2305.05666",
                    "CorpusId": 258564493
                },
                "corpusId": 258564493,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3960ea842b8b89819f80cf2cd77adc321ff744a1",
                "title": "Policy Gradient Methods in the Presence of Symmetries and State Abstractions",
                "abstract": "Reinforcement learning on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of MDP homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual control tasks from the DeepMind Control Suite. Our method's ability to utilize MDP homomorphisms for representation learning leads to improved performance, and the visualizations of the latent space clearly demonstrate the structure of the learned abstraction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1784317",
                        "name": "P. Panangaden"
                    },
                    {
                        "authorId": "1417282075",
                        "name": "S. Rezaei-Shoshtari"
                    },
                    {
                        "authorId": "2004617613",
                        "name": "Rosie Zhao"
                    },
                    {
                        "authorId": "2462512",
                        "name": "D. Meger"
                    },
                    {
                        "authorId": "144368601",
                        "name": "Doina Precup"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44aae5283d29c894e7d2573d99f65be4bb6c1e41",
                "externalIds": {
                    "DBLP": "journals/csr/ParkK23",
                    "DOI": "10.1016/j.cosrev.2023.100548",
                    "CorpusId": 257316030
                },
                "corpusId": 257316030,
                "publicationVenue": {
                    "id": "3aa92b7f-af7a-4ebd-8925-1152710bfbc7",
                    "name": "Computer Science Review",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Sci Rev"
                    ],
                    "issn": "1574-0137",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/710138/description#description"
                },
                "url": "https://www.semanticscholar.org/paper/44aae5283d29c894e7d2573d99f65be4bb6c1e41",
                "title": "Visual language integration: A survey and open challenges",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2115232998",
                        "name": "Sang-Min Park"
                    },
                    {
                        "authorId": "2160604766",
                        "name": "Young-Gab Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another body of related work on decoupling representation learning from RL primarily revolves around the use of contrastive learning (Anand et al., 2019; Wu et al., 2019; Stooke et al., 2021; Schwarzer et al., 2021b; Erraqabi et al., 2022).",
                "Anand et al. (2019) proposed ST-DIM, a collection of temporal contrastive losses operating on image patches from environmental observations."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a2b2abdb4566beee1e29f19bc6914ce67680acf3",
                "externalIds": {
                    "ArXiv": "2304.12567",
                    "DBLP": "conf/iclr/FarebrotherGALG23",
                    "DOI": "10.48550/arXiv.2304.12567",
                    "CorpusId": 254199184
                },
                "corpusId": 254199184,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a2b2abdb4566beee1e29f19bc6914ce67680acf3",
                "title": "Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks",
                "abstract": "Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well understood; in practice, however, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treated as an essentially infinite source of information about the environment. Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations, focusing on the setting where the number of tasks and the size of the agent's network are simultaneously increased. For this purpose, we derive a new family of auxiliary tasks based on the successor measure. These tasks are easy to implement and have appealing theoretical properties. Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extending Mahadevan&Maggioni (2007)'s proto-value functions to deep reinforcement learning -- accordingly, we call the resulting object proto-value networks. Through a series of experiments on the Arcade Learning Environment, we demonstrate that proto-value networks produce rich features that may be used to obtain performance comparable to established algorithms, using only linear approximation and a small number (~4M) of interactions with the environment's reward function.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2066946221",
                        "name": "Jesse Farebrother"
                    },
                    {
                        "authorId": "1580438318",
                        "name": "Joshua Greaves"
                    },
                    {
                        "authorId": "29767024",
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "authorId": "153892869",
                        "name": "Charline Le Lan"
                    },
                    {
                        "authorId": "2558463",
                        "name": "Ross Goroshin"
                    },
                    {
                        "authorId": "39163115",
                        "name": "P. S. Castro"
                    },
                    {
                        "authorId": "1792298",
                        "name": "Marc G. Bellemare"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3eb75f5f35a7963b46ad4c43c68082283c9c0489",
                "externalIds": {
                    "ArXiv": "2304.00046",
                    "DBLP": "journals/corr/abs-2304-00046",
                    "DOI": "10.48550/arXiv.2304.00046",
                    "CorpusId": 257913696
                },
                "corpusId": 257913696,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3eb75f5f35a7963b46ad4c43c68082283c9c0489",
                "title": "Accelerating exploration and representation learning with offline pre-training",
                "abstract": "Sequential decision-making agents struggle with long horizon tasks, since solving them requires multi-step reasoning. Most reinforcement learning (RL) algorithms address this challenge by improved credit assignment, introducing memory capability, altering the agent's intrinsic motivation (i.e. exploration) or its worldview (i.e. knowledge representation). Many of these components could be learned from offline data. In this work, we follow the hypothesis that exploration and representation learning can be improved by separately learning two different models from a single offline dataset. We show that learning a state representation using noise-contrastive estimation and a model of auxiliary reward separately from a single collection of human demonstrations can significantly improve the sample efficiency on the challenging NetHack benchmark. We also ablate various components of our experimental setting and highlight crucial insights.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "18111246",
                        "name": "Bogdan Mazoure"
                    },
                    {
                        "authorId": "12139064",
                        "name": "Jake Bruce"
                    },
                    {
                        "authorId": "144368601",
                        "name": "Doina Precup"
                    },
                    {
                        "authorId": "2276554",
                        "name": "R. Fergus"
                    },
                    {
                        "authorId": "2212836908",
                        "name": "Ankit Anand"
                    }
                ]
            }
        },
        {
            "contexts": [
                "CURL [31] proposed a contrastive loss between two different data-augmentation of the same observation, while CPC [32] and ST-DIM [16] proposed different variations of temporal contrastive loss between two augmented observations separated by small time steps.",
                "To further improve sample efficiency, we use contrastive representation learning by maximizing the temporal mutual information between embeddings of consecutive time steps [16], [17], [18]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "99435b8d230ee465c3b6947e6f8b7aaa02477038",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-03787",
                    "ArXiv": "2303.03787",
                    "DOI": "10.48550/arXiv.2303.03787",
                    "CorpusId": 257377943
                },
                "corpusId": 257377943,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/99435b8d230ee465c3b6947e6f8b7aaa02477038",
                "title": "Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and Contrastive Learning",
                "abstract": "Model-based reinforcement learning (MBRL) with real-time planning has shown great potential in locomotion and manipulation control tasks. However, the existing planning methods, such as the Cross-Entropy Method (CEM), do not scale well to complex high-dimensional environments. One of the key reasons for underperformance is the lack of exploration, as these planning methods only aim to maximize the cumulative extrinsic reward over the planning horizon. Furthermore, planning inside the compact latent space in the absence of observations makes it challenging to use curiosity-based intrinsic motivation. We propose Curiosity CEM (CCEM), an improved version of the CEM algorithm for encouraging exploration via curiosity. Our proposed method maximizes the sum of state-action Q values over the planning horizon, in which these Q values estimate the future extrinsic and intrinsic reward, hence encouraging reaching novel observations. In addition, our model uses contrastive representation learning to efficiently learn latent representations. Experiments on image-based continuous control tasks from the DeepMind Control suite show that CCEM is by a large margin more sample-efficient than previous MBRL algorithms and compares favorably with the best model-free RL methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40391142",
                        "name": "Mostafa Kotb"
                    },
                    {
                        "authorId": "1798067",
                        "name": "C. Weber"
                    },
                    {
                        "authorId": "1736513",
                        "name": "S. Wermter"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We experimented with different self-supervised losses (MSE, ST-DIM [1] and VICReg [8]), covering both contrastive and non-contrastive approaches.",
                "Self-supervised learning also started to be used in the field of state representation learning [28], it has proven to be a suitable method for creating the feature space [1] and has also found its use in reinforcement learning [48, 17].",
                "SND-STD method uses the Spatio-Temporal DeepInfoMax (ST-DIM) algorithm [1] (the simple diagram can be found in Figure 4) leveraging multiclass N -pair losses [46]:",
                "The details of this algorithm are provided in [1].",
                "Our experiments revealed that if the ST-DIM algorithm works on an\nincomplete dataset that takes on new samples (the authors probably did not test it in such conditions), there is an instability and an exponential increase of activity in the feature space at certain moments.",
                "SND-STD method uses the Spatio-Temporal DeepInfoMax (ST-DIM) algorithm [1] (the simple diagram can be found in Figure 4) leveraging multiclass N -pair losses [46]:\nLGL = \u2212 I\u2211 i=1 J\u2211 j=1 log exp(gi,j)\u2211 s\u2217t\u2208Snext exp(gi,j)\n(5)\nLLL = \u2212 I\u2211 i=1 J\u2211 j=1 log exp(fi,j)\u2211 s\u2217t\u2208Snext exp(fi,j)\n(6)\nwhere f(.)",
                "Our version uses the state st and its successor st+1 (the same as ST-DIM, the simple diagram can be found in Figure 4) instead of two augmentations of the same state.",
                "7) of the ST-DIM algorithm itself has an expansive effect and the addition of another expansive term led to problems with the uncontrolled expansion of the feature space."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "038a6c388eb8fe2608e98875fda93367a13d326d",
                "externalIds": {
                    "ArXiv": "2302.11563",
                    "DBLP": "journals/corr/abs-2302-11563",
                    "DOI": "10.48550/arXiv.2302.11563",
                    "CorpusId": 257078726
                },
                "corpusId": 257078726,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/038a6c388eb8fe2608e98875fda93367a13d326d",
                "title": "Exploration by self-supervised exploitation",
                "abstract": "Reinforcement learning can solve decision-making problems and train an agent to behave in an environment according to a predesigned reward function. However, such an approach becomes very problematic if the reward is too sparse and the agent does not come across the reward during the environmental exploration. The solution to such a problem may be in equipping the agent with an intrinsic motivation, which will provide informed exploration, during which the agent is likely to also encounter external reward. Novelty detection is one of the promising branches of intrinsic motivation research. We present Self-supervised Network Distillation (SND), a class of internal motivation algorithms based on the distillation error as a novelty indicator, where the target model is trained using self-supervised learning. We adapted three existing self-supervised methods for this purpose and experimentally tested them on a set of ten environments that are considered difficult to explore. The results show that our approach achieves faster growth and higher external reward for the same training time compared to the baseline models, which implies improved exploration in a very sparse reward environment.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9450324",
                        "name": "Matej Pech\u00e1c"
                    },
                    {
                        "authorId": "3072132",
                        "name": "M. Chovanec"
                    },
                    {
                        "authorId": "153742031",
                        "name": "I. Farka\u0161"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ST-DIM [21] uses temporal and contrastive losses to operate on local features of the intermediate layer within the encoderwithout data augmentation.",
                "ST-DIM [21] uses temporal and contrastive losses to operate on local features of the intermediate layer within theencoderwithoutdataaugmentation."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1d55f7985ecf57098ed78ce53c1aab095cc62e83",
                "externalIds": {
                    "DOI": "10.1007/s40747-023-00995-8",
                    "CorpusId": 257142510
                },
                "corpusId": 257142510,
                "publicationVenue": {
                    "id": "d30c9917-b233-46f4-a644-8e5cdf6d6c5e",
                    "name": "Complex & Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Complex  Intell Syst"
                    ],
                    "issn": "2199-4536",
                    "url": "https://link.springer.com/journal/40747",
                    "alternate_urls": [
                        "http://link.springer.com/journal/40747"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1d55f7985ecf57098ed78ce53c1aab095cc62e83",
                "title": "Efficient state representation with artificial potential fields for reinforcement learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152631261",
                        "name": "Hao Jiang"
                    },
                    {
                        "authorId": "2153703762",
                        "name": "Shengze Li"
                    },
                    {
                        "authorId": "2108952578",
                        "name": "Jieyuan Zhang"
                    },
                    {
                        "authorId": "46759169",
                        "name": "Yuqi Zhu"
                    },
                    {
                        "authorId": "2152776128",
                        "name": "Xinhai Xu"
                    },
                    {
                        "authorId": "40639829",
                        "name": "Donghong Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2), (vi) predictive information [138,150,151], (vii) bisimulation [137,152] and (viii) asymmetric training by taking advantage of simulation [153]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9e13ffe77bc1ae88dc9e89ddf6fdaa12f15c165f",
                "externalIds": {
                    "PubMedCentral": "10007406",
                    "DBLP": "journals/sensors/KadiT23",
                    "DOI": "10.3390/s23052389",
                    "CorpusId": 257122700,
                    "PubMed": "36904597"
                },
                "corpusId": 257122700,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9e13ffe77bc1ae88dc9e89ddf6fdaa12f15c165f",
                "title": "Data-Driven Robotic Manipulation of Cloth-like Deformable Objects: The Present, Challenges and Future Prospects",
                "abstract": "Manipulating cloth-like deformable objects (CDOs) is a long-standing problem in the robotics community. CDOs are flexible (non-rigid) objects that do not show a detectable level of compression strength while two points on the article are pushed towards each other and include objects such as ropes (1D), fabrics (2D) and bags (3D). In general, CDOs\u2019 many degrees of freedom (DoF) introduce severe self-occlusion and complex state\u2013action dynamics as significant obstacles to perception and manipulation systems. These challenges exacerbate existing issues of modern robotic control methods such as imitation learning (IL) and reinforcement learning (RL). This review focuses on the application details of data-driven control methods on four major task families in this domain: cloth shaping, knot tying/untying, dressing and bag manipulation. Furthermore, we identify specific inductive biases in these four domains that present challenges for more general IL and RL algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2209521933",
                        "name": "Halid Abdulrahim Kadi"
                    },
                    {
                        "authorId": "1754945",
                        "name": "K. Terzic"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5704b6ccdccba5645421daef67e1651dbc373b9b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-04449",
                    "ArXiv": "2302.04449",
                    "DOI": "10.48550/arXiv.2302.04449",
                    "CorpusId": 256697185
                },
                "corpusId": 256697185,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5704b6ccdccba5645421daef67e1651dbc373b9b",
                "title": "Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals",
                "abstract": "High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. Auxiliary reward is then provided to a standard A2C RL agent, when interaction is detected. When assisted by our design, A2C improves on 4 games in the Atari environment with sparse rewards, and requires 1000x less training frames compared to the previous SOTA Agent 57 on Skiing, the hardest game in Atari.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46220633",
                        "name": "Yue Wu"
                    },
                    {
                        "authorId": "2166103953",
                        "name": "Yewen Fan"
                    },
                    {
                        "authorId": "28130078",
                        "name": "P. Liang"
                    },
                    {
                        "authorId": "1746466",
                        "name": "A. Azaria"
                    },
                    {
                        "authorId": "152244300",
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "authorId": "40975594",
                        "name": "Tom Michael Mitchell"
                    }
                ]
            }
        },
        {
            "contexts": [
                "reconstructing the observation (to ignore irrelevant visual aspects) [162, 51, 76, 67, 109], to predicting forward dynamics (to capture what constrains movement) [162, 67] or inverse dynamics (to recover actions from observations) [126], to enforcing behavioural similarity between observations [173, 59, 11], to contrastive losses [158, 94, 8, 154], and many",
                "Feature Embedding (Unsupervised) \u03c6R(oR) = ~ \u03c6R(oR) \u2208 Rd May learn wrong disentangled information d, Low by design If disentangled information complete, easy If disentangled information incomplete, hard May be interpretable to designer [97, 154, 145, 173, 76, 67, 94, 8, 70]"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "db20bd3bb82d1011ce704d440d8c2578f665e6e1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-01928",
                    "ArXiv": "2302.01928",
                    "DOI": "10.48550/arXiv.2302.01928",
                    "CorpusId": 256598067
                },
                "corpusId": 256598067,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/db20bd3bb82d1011ce704d440d8c2578f665e6e1",
                "title": "Aligning Robot and Human Representations",
                "abstract": "To act in the world, robots rely on a representation of salient task aspects: for example, to carry a cup of coffee, a robot must consider movement efficiency and cup orientation in its behaviour. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. their representations must be aligned with humans'. In this survey, we pose that current reward and imitation learning approaches suffer from representation misalignment, where the robot's learned representation does not capture the human's representation. We suggest that because humans will be the ultimate evaluator of robot performance in the world, it is critical that we explicitly focus our efforts on aligning learned task representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. To do so, we mathematically define the problem, identify its key desiderata, and situate current robot learning methods within this formalism. We conclude the survey by suggesting future directions for exploring open challenges.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3489195",
                        "name": "Andreea Bobu"
                    },
                    {
                        "authorId": "100953200",
                        "name": "Andi Peng"
                    },
                    {
                        "authorId": "33932184",
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "authorId": "143873972",
                        "name": "J. Shah"
                    },
                    {
                        "authorId": "2745001",
                        "name": "A. Dragan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "+20 ] (8) To train the critic, we symlog transform the targets R t and then twohot encode them into a soft label for the softmax distribution produced by the critic."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-04104",
                    "ArXiv": "2301.04104",
                    "DOI": "10.48550/arXiv.2301.04104",
                    "CorpusId": 255569874
                },
                "corpusId": 255569874,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e",
                "title": "Mastering Diverse Domains through World Models",
                "abstract": "General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision-making problems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35006479",
                        "name": "Danijar Hafner"
                    },
                    {
                        "authorId": "31143488",
                        "name": "J. Pa\u0161ukonis"
                    },
                    {
                        "authorId": "2503659",
                        "name": "Jimmy Ba"
                    },
                    {
                        "authorId": "2542999",
                        "name": "T. Lillicrap"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8fb54b12983491434ff119f2c6d931c03e1994f3",
                "externalIds": {
                    "DBLP": "journals/nn/LeeKKPK23",
                    "DOI": "10.1016/j.neunet.2022.12.018",
                    "CorpusId": 255284750,
                    "PubMed": "36587439"
                },
                "corpusId": 255284750,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8fb54b12983491434ff119f2c6d931c03e1994f3",
                "title": "STACoRe: Spatio-temporal and action-based contrastive representations for reinforcement learning in Atari",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116466706",
                        "name": "Young Jae Lee"
                    },
                    {
                        "authorId": "2133383668",
                        "name": "Jaehoon Kim"
                    },
                    {
                        "authorId": "134419323",
                        "name": "Mingu Kwak"
                    },
                    {
                        "authorId": "2110425522",
                        "name": "Young Joon Park"
                    },
                    {
                        "authorId": "2144239880",
                        "name": "S. Kim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "866c0e7c33e6bb5dde0097bc86ab2d875d586699",
                "externalIds": {
                    "ArXiv": "2212.13819",
                    "DBLP": "journals/corr/abs-2212-13819",
                    "DOI": "10.48550/arXiv.2212.13819",
                    "CorpusId": 255186582
                },
                "corpusId": 255186582,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/866c0e7c33e6bb5dde0097bc86ab2d875d586699",
                "title": "Don't do it: Safer Reinforcement Learning With Rule-based Guidance",
                "abstract": "During training, reinforcement learning systems interact with the world without considering the safety of their actions. When deployed into the real world, such systems can be dan- gerous and cause harm to their surroundings. Often, danger-ous situations can be mitigated by de\ufb01ning a set of rules that the system should not violate under any conditions. For example, in robot navigation, one safety rule would be to avoid colliding with surrounding objects and people. In this work, we de\ufb01ne safety rules in terms of the relationships between the agent and objects and use them to prevent reinforcement learning systems from performing potentially harmful actions. We propose a new safe epsilon-greedy algorithm that uses safety rules to override agents\u2019 actions if they are con- sidered to be unsafe. In our experiments, we show that a safe epsilon-greedy policy signi\ufb01cantly increases the safety of the agent during training, improves the learning ef\ufb01ciency resulting in much faster convergence, and achieves better perfor- mance than the base model.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144941958",
                        "name": "Ekaterina Nikonova"
                    },
                    {
                        "authorId": "2111673212",
                        "name": "Cheng Xue"
                    },
                    {
                        "authorId": "1680174",
                        "name": "Jochen Renz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several works have addressed the problem of finding compact state representations from raw sensory streams in a model-free MDP setting [3], [4], [5], [6]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dc58a8b6cdadf9b30bf5594a058441b70ab599d2",
                "externalIds": {
                    "DBLP": "conf/cdc/KhadkeG22",
                    "DOI": "10.1109/CDC51059.2022.9993177",
                    "CorpusId": 255599409
                },
                "corpusId": 255599409,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/dc58a8b6cdadf9b30bf5594a058441b70ab599d2",
                "title": "Sparsity Inducing System Representations for Policy Decompositions",
                "abstract": "Policy Decomposition (PoDec) is a framework that lessens the curse of dimensionality when deriving policies to optimal control problems. For a given system representation, i.e. the state variables and control inputs describing a system, PoDec generates strategies to decompose the joint optimization of policies for all control inputs. Thereby, policies for different inputs are derived in a decoupled or cascaded fashion and as functions of some subsets of the state variables, leading to reduction in computation. However, the choice of system representation is crucial as it dictates the suboptimality of the resulting policies. We present a heuristic method to find a representation more amenable to decomposition. Our approach is based on the observation that every decomposition enforces a sparsity pattern in the resulting policies at the cost of optimality and a representation that already leads to a sparse optimal policy is likely to produce decompositions with lower suboptimalities. As the optimal policy is not known we construct a system representation that sparsifies its LQR approximation. For a simplified biped, a 4 degree-of-freedom manipulator, and a quadcopter, we discover decompositions that offer 10% reduction in trajectory costs over those identified by vanilla PoDec. Moreover, the decomposition policies produce trajectories with substantially lower costs compared to policies obtained from state-of-the-art reinforcement learning algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30623582",
                        "name": "Ashwin Khadke"
                    },
                    {
                        "authorId": "1702216",
                        "name": "H. Geyer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some of the recent unsupervised state representations are Time Contrastive Network (TCN) [32], conditional models of Noise Contrastive Estimation (CM-NCE) [33], SpatioTemporal Deep InfoMax (ST-DIM) [18], Balanced View Spatial Deep InfoMax (BVS-DIM) [22], CURL [6] and augmented temporal contrast (ATC) [21].",
                "Some researchers used state variables of video games as an evaluation methods such as [18] and [22] while other used reinforcement learning tasks such as [6] and [21].",
                "In ST-DIM, \u03c4 = 1 while in ATC they used \u03c4 = 3.",
                "There in no modification of equation 5 since it is adjusted with p = 2 \u2217 (0.5\u2212 r) when we have r   0.5\nxbmn(i, j) :=\n{ x\u22171(i, j) if Ci,j = true\nxmin(i, j) otherwise (8)\nxbmx(i, j) :=\n{ x\u22172(i, j) if Ci,j = true\nxmax(i, j) otherwise (9)\nIn ST-DIM and ATC the temporally close frames are taken as the view of one another.",
                "However, learning in ST-DIM and ATC is limited to time sequenced samples, and we hypothesize the success of representation learning in ST-DIM and ATC depends on the similarity factor of these temporal observations.",
                "Video games and reinforcement learning based control tasks are useful candidates for evaluating representation learning algorithms [12], [18]\u2013[21].",
                "Devising a generic way of evaluation of the general goodness of the representation is however essential [18].",
                "On the other hand, ST-DIM and BVS-DIM made evaluation of the learned representations by using state variables of the game environments.",
                "Since \u03c4 is considered as a factor of spatio-temporal similarity-contrast, there was no possible values to choose for \u03c4 other than integers in ST-DIM and ATC."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "08539144092acd73bd333d9767305a3f38bcf7f1",
                "externalIds": {
                    "DBLP": "conf/icmla/MengistuACL22",
                    "DOI": "10.1109/ICMLA55696.2022.00273",
                    "CorpusId": 257721317
                },
                "corpusId": 257721317,
                "publicationVenue": {
                    "id": "f6752838-f268-4a1b-87e7-c5f30a36713c",
                    "name": "International Conference on Machine Learning and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Mach Learn Appl",
                        "ICMLA"
                    ],
                    "url": "http://www.icmla-conference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/08539144092acd73bd333d9767305a3f38bcf7f1",
                "title": "Balancing Similarity-Contrast in Unsupervised Representation Learning: Evaluation with Reinforcement Learning",
                "abstract": "In this paper, we provided an unsupervised contrastive representation learning method which uses contrastive views in which both spatial and temporal similarity-contrast is balanced. The balanced views are created by taking pixels from the anchor sample and any randomly selected negative sample and balancing the ratio of number of pixels taken from the anchor and the negative. Then these balanced views are paired with the anchor to create the positive contrastive views and all other samples paired with the anchor are taken as negative contrastive views. We made the evaluation using reinforcement learning tasks on Atari games and Deep Mind Control suites (DMControl). Our evaluations on 26 Atari games and six DMControl tasks show that the proposed method is superior in learning spatio-temporally evolving factors of the environment by capturing the relevant task controlling generative factors from the agents\u2019 raw observations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154838118",
                        "name": "Menore Tekeba Mengistu"
                    },
                    {
                        "authorId": "2212612899",
                        "name": "Getachew Alemu"
                    },
                    {
                        "authorId": "32730304",
                        "name": "Pierre Chevaillier"
                    },
                    {
                        "authorId": "1758175",
                        "name": "P. D. Loor"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Various unsupervised/self-supervised auxiliary objectives like autoencoders [34, 45, 83], forward [31] and inverse dynamics [56], spatio-temporal mutual information maximization [2, 35], contrastive learning [32, 33, 66, 85], derive supervision from the agent\u2019s own experience.",
                "[2] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C\u00f4t\u00e9, and R."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9e69459f91dd699881b19554b692290e96c35aad",
                "externalIds": {
                    "ArXiv": "2212.01186",
                    "DBLP": "journals/corr/abs-2212-01186",
                    "DOI": "10.48550/arXiv.2212.01186",
                    "CorpusId": 254220950
                },
                "corpusId": 254220950,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9e69459f91dd699881b19554b692290e96c35aad",
                "title": "A General Purpose Supervisory Signal for Embodied Agents",
                "abstract": "Training effective embodied AI agents often involves manual reward engineering, expert imitation, specialized components such as maps, or leveraging additional sensors for depth and localization. Another approach is to use neural architectures alongside self-supervised objectives which encourage better representation learning. In practice, there are few guarantees that these self-supervised objectives encode task-relevant information. We propose the Scene Graph Contrastive (SGC) loss, which uses scene graphs as general-purpose, training-only, supervisory signals. The SGC loss does away with explicit graph decoding and instead uses contrastive learning to align an agent's representation with a rich graphical encoding of its environment. The SGC loss is generally applicable, simple to implement, and encourages representations that encode objects' semantics, relationships, and history. Using the SGC loss, we attain significant gains on three embodied tasks: Object Navigation, Multi-Object Navigation, and Arm Point Navigation. Finally, we present studies and analyses which demonstrate the ability of our trained representation to encode semantic cues about the environment.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153866008",
                        "name": "Kunal Pratap Singh"
                    },
                    {
                        "authorId": "145704057",
                        "name": "Jordi Salvador"
                    },
                    {
                        "authorId": "20745881",
                        "name": "Luca Weihs"
                    },
                    {
                        "authorId": "2684226",
                        "name": "Aniruddha Kembhavi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some common auxiliary tasks in RL include: (1) future prediction [26, 15, 46, 13, 32], which predicts an agent\u2019s future state conditioned on its current state and the actions taken, (2) inverse dynamic [15, 46, 32], which predicts the actions taken between two states, and (3) contrastive learning [3, 36], which applies contrastive learning to refine the state representation."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "91cbeb715435252fade1284b343b8850934760d9",
                "externalIds": {
                    "ArXiv": "2211.11116",
                    "DBLP": "journals/corr/abs-2211-11116",
                    "DOI": "10.1109/WACV56688.2023.00116",
                    "CorpusId": 253443969
                },
                "corpusId": 253443969,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/91cbeb715435252fade1284b343b8850934760d9",
                "title": "Structure-Encoding Auxiliary Tasks for Improved Visual Representation in Vision-and-Language Navigation",
                "abstract": "In Vision-and-Language Navigation (VLN), researchers typically take an image encoder pre-trained on ImageNet without fine-tuning on the environments that the agent will be trained or tested on. However, the distribution shift between the training images from ImageNet and the views in the navigation environments may render the ImageNet pre-trained image encoder suboptimal. Therefore, in this paper, we design a set of structure-encoding auxiliary tasks (SEA) that leverage the data in the navigation environments to pre-train and improve the image encoder. Specifically, we design and customize (1) 3D jigsaw, (2) traversability prediction, and (3) instance classification to pre-train the image encoder. Through rigorous ablations, our SEA pre-trained features are shown to better encode structural information of the scenes, which ImageNet pre-trained features fail to properly encode but is crucial for the target navigation task. The SEA pre-trained features can be easily plugged into existing VLN agents without any tuning. For example, on Test-Unseen environments, the VLN agents combined with our SEA pre-trained features achieve absolute success rate improvement of 12% for Speaker-Follower [14], 5% for Env-Dropout [37], and 4% for AuxRN [50].",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47387175",
                        "name": "Chia-Wen Kuo"
                    },
                    {
                        "authorId": "2112464124",
                        "name": "Chih-Yao Ma"
                    },
                    {
                        "authorId": "50196944",
                        "name": "Judy Hoffman"
                    },
                    {
                        "authorId": "145276578",
                        "name": "Z. Kira"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This protocol is related to the one used in [1], but we probe not only the encoder output, but also the predictor output."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ed009b7423dcfec47708fb5817ec4955e4265757",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-10831",
                    "ArXiv": "2211.10831",
                    "DOI": "10.48550/arXiv.2211.10831",
                    "CorpusId": 253735342
                },
                "corpusId": 253735342,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ed009b7423dcfec47708fb5817ec4955e4265757",
                "title": "Joint Embedding Predictive Architectures Focus on Slow Features",
                "abstract": "Many common methods for learning a world model for pixel-based environments use generative architectures trained with pixel-level reconstruction objectives. Recently proposed Joint Embedding Predictive Architectures (JEPA) offer a reconstruction-free alternative. In this work, we analyze performance of JEPA trained with VICReg and SimCLR objectives in the fully offline setting without access to rewards, and compare the results to the performance of the generative architecture. We test the methods in a simple environment with a moving dot with various background distractors, and probe learned representations for the dot's location. We find that JEPA methods perform on par or better than reconstruction when distractor noise changes every time step, but fail when the noise is fixed. Furthermore, we provide a theoretical explanation for the poor performance of JEPA-based methods with fixed noise, highlighting an important limitation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2162736903",
                        "name": "Vlad Sobal"
                    },
                    {
                        "authorId": "2191646240",
                        "name": "V. JyothirS"
                    },
                    {
                        "authorId": "2191610598",
                        "name": "Siddhartha Jalagam"
                    },
                    {
                        "authorId": "3422899",
                        "name": "Nicolas Carion"
                    },
                    {
                        "authorId": "1979489",
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "authorId": "1688882",
                        "name": "Yann LeCun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These games are the only two from our datasets with a fixed number of objects, which allows for a very simple policy architecture, and for which perfect information obtained from the RAM using [1] is available.",
                "The first was to use the RAM hacking approach of AtariARI [1].",
                "[1] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C\u00f4t\u00e9, and R."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6abe1fb523b4a7fc829cba0015f1ed7294f0a217",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-09771",
                    "ArXiv": "2211.09771",
                    "DOI": "10.1007/978-3-031-43421-1_36",
                    "CorpusId": 253581540
                },
                "corpusId": 253581540,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6abe1fb523b4a7fc829cba0015f1ed7294f0a217",
                "title": "Boosting Object Representation Learning via Motion and Object Continuity",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2051175030",
                        "name": "Quentin Delfosse"
                    },
                    {
                        "authorId": "1486503614",
                        "name": "Wolfgang Stammer"
                    },
                    {
                        "authorId": "2191076248",
                        "name": "Thomas Rothenbacher"
                    },
                    {
                        "authorId": "2191076236",
                        "name": "Dwarak Vittal"
                    },
                    {
                        "authorId": "2066493115",
                        "name": "K. Kersting"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Representation Learning World Model (Ha & Schmidhuber, 2018) Reconstruction 3 7 ST-DIM (Anand et al., 2019) Forward Pixel Prediction 3 7 ATC (Stooke et al.",
                "These approaches usually incorporate temporal information, aiming to distinguish between sequential and non-sequential states (Anand et al., 2019; Stooke et al., 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c90a33f1f0049d524e9b5b3174d35611fd9a8096",
                "externalIds": {
                    "ArXiv": "2211.03959",
                    "DBLP": "journals/corr/abs-2211-03959",
                    "DOI": "10.48550/arXiv.2211.03959",
                    "CorpusId": 253397510
                },
                "corpusId": 253397510,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096",
                "title": "Pretraining in Deep Reinforcement Learning: A Survey",
                "abstract": "The past few years have seen rapid progress in combining reinforcement learning (RL) with deep learning. Various breakthroughs ranging from games to robotics have spurred the interest in designing sophisticated RL algorithms and systems. However, the prevailing workflow in RL is to learn tabula rasa, which may incur computational inefficiency. This precludes continuous deployment of RL algorithms and potentially excludes researchers without large-scale computing resources. In many other areas of machine learning, the pretraining paradigm has shown to be effective in acquiring transferable knowledge, which can be utilized for a variety of downstream tasks. Recently, we saw a surge of interest in Pretraining for Deep RL with promising results. However, much of the research has been based on different experimental settings. Due to the nature of RL, pretraining in this field is faced with unique challenges and hence requires new design principles. In this survey, we seek to systematically review existing works in pretraining for deep reinforcement learning, provide a taxonomy of these methods, discuss each sub-field, and bring attention to open problems and future directions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114115577",
                        "name": "Zhihui Xie"
                    },
                    {
                        "authorId": "41123614",
                        "name": "Zichuan Lin"
                    },
                    {
                        "authorId": "1723423332",
                        "name": "Junyou Li"
                    },
                    {
                        "authorId": "1491648207",
                        "name": "Shuai Li"
                    },
                    {
                        "authorId": "2055648566",
                        "name": "Deheng Ye"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "832b192561c749216c1b69e161cf15b2d8f0235d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-04367",
                    "ArXiv": "2211.04367",
                    "DOI": "10.48550/arXiv.2211.04367",
                    "CorpusId": 253398075
                },
                "corpusId": 253398075,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/832b192561c749216c1b69e161cf15b2d8f0235d",
                "title": "Much Easier Said Than Done: Falsifying the Causal Relevance of Linear Decoding Methods",
                "abstract": "Linear classifier probes are frequently utilized to better understand how neural networks function. Researchers have approached the problem of determining unit importance in neural networks by probing their learned, internal representations. Linear classifier probes identify highly selective units as the most important for network function. Whether or not a network actually relies on high selectivity units can be tested by removing them from the network using ablation. Surprisingly, when highly selective units are ablated they only produce small performance deficits, and even then only in some cases. In spite of the absence of ablation effects for selective neurons, linear decoding methods can be effectively used to interpret network function, leaving their effectiveness a mystery. To falsify the exclusive role of selectivity in network function and resolve this contradiction, we systematically ablate groups of units in subregions of activation space. Here, we find a weak relationship between neurons identified by probes and those identified by ablation. More specifically, we find that an interaction between selectivity and the average activity of the unit better predicts ablation performance deficits for groups of units in AlexNet, VGG16, MobileNetV2, and ResNet101. Linear decoders are likely somewhat effective because they overlap with those units that are causally important for network function. Interpretability methods could be improved by focusing on causally important units.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "17494973",
                        "name": "L. Hayne"
                    },
                    {
                        "authorId": "46419708",
                        "name": "Abhijit Suresh"
                    },
                    {
                        "authorId": "2190174707",
                        "name": "Hunar Jain"
                    },
                    {
                        "authorId": "2108767151",
                        "name": "Rahul Kumar"
                    },
                    {
                        "authorId": "33898466",
                        "name": "R. M. Carter"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, recent work has shown significance of learning representations through self supervised objectives in RL [3, 55], often as a pre-training phase [56, 70], which can help for both exploration [36] and control [71].",
                "[3] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C\u00f4t\u00e9, and R."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5af8cc56be44bbc741c3701c65dd354c20addc28",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-00247",
                    "ArXiv": "2211.00247",
                    "DOI": "10.48550/arXiv.2211.00247",
                    "CorpusId": 253244398
                },
                "corpusId": 253244398,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5af8cc56be44bbc741c3701c65dd354c20addc28",
                "title": "Discrete Factorial Representations as an Abstraction for Goal Conditioned Reinforcement Learning",
                "abstract": "Goal-conditioned reinforcement learning (RL) is a promising direction for training agents that are capable of solving multiple tasks and reach a diverse set of objectives. How to \\textit{specify} and \\textit{ground} these goals in such a way that we can both reliably reach goals during training as well as generalize to new goals during evaluation remains an open area of research. Defining goals in the space of noisy and high-dimensional sensory inputs poses a challenge for training goal-conditioned agents, or even for generalization to novel goals. We propose to address this by learning factorial representations of goals and processing the resulting representation via a discretization bottleneck, for coarser goal specification, through an approach we call DGRL. We show that applying a discretizing bottleneck can improve performance in goal-conditioned RL setups, by experimentally evaluating this method on tasks ranging from maze environments to complex robotic navigation and manipulation. Additionally, we prove a theorem lower-bounding the expected return on out-of-distribution goals, while still allowing for specifying goals with expressive combinatorial structure.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "18014232",
                        "name": "Riashat Islam"
                    },
                    {
                        "authorId": "11262459",
                        "name": "Hongyu Zang"
                    },
                    {
                        "authorId": "1996705",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "49071560",
                        "name": "Alex Lamb"
                    },
                    {
                        "authorId": "1392876047",
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "authorId": "2177762717",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "144100820",
                        "name": "R. Laroche"
                    },
                    {
                        "authorId": "1865800402",
                        "name": "Y. Bengio"
                    },
                    {
                        "authorId": "15032777",
                        "name": "R\u00e9mi Tachet des Combes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We mostly used the default parameters from the Atari agent in (Hafner et al., 2020) but increased the RSSM recurrent state size and tuned the KL and entropy scales to the new environment.",
                "Deep reinforcement learning (RL) has made tremendous progress in recent years, outperforming humans on Atari games (Mnih et al., 2015; Badia et al., 2020), board games (Silver et al., 2016; Schrittwieser et al., 2019), and advances in robot learning (Akkaya et al., 2019; Wu et al., 2022).",
                "The differences from the parameters of the original DreamerV2 Atari model (Table D.1 in Hafner et al. (2020)) are shown in bold face.",
                "Unsupervised representations are commonly evaluated by probing (Oord et al., 2018; Chen et al., 2020; Gregor et al., 2019; Anand et al., 2019), where a separate network is trained to predict relevant properties from the frozen representations.",
                "A standardized probe benchmark is available for Atari (Anand et al., 2019), but those tasks require almost no memory."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5f164c00d9dcdc5da72b9f71a802eabc4d2e8e68",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-13383",
                    "ArXiv": "2210.13383",
                    "DOI": "10.48550/arXiv.2210.13383",
                    "CorpusId": 253097769
                },
                "corpusId": 253097769,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5f164c00d9dcdc5da72b9f71a802eabc4d2e8e68",
                "title": "Evaluating Long-Term Memory in 3D Mazes",
                "abstract": "Intelligent agents need to remember salient information to reason in partially-observed environments. For example, agents with a first-person view should remember the positions of relevant objects even if they go out of view. Similarly, to effectively navigate through rooms agents need to remember the floor plan of how rooms are connected. However, most benchmark tasks in reinforcement learning do not test long-term memory in agents, slowing down progress in this important research direction. In this paper, we introduce the Memory Maze, a 3D domain of randomized mazes specifically designed for evaluating long-term memory in agents. Unlike existing benchmarks, Memory Maze measures long-term memory separate from confounding agent abilities and requires the agent to localize itself by integrating information over time. With Memory Maze, we propose an online reinforcement learning benchmark, a diverse offline dataset, and an offline probing evaluation. Recording a human player establishes a strong baseline and verifies the need to build up and retain memories, which is reflected in their gradually increasing rewards within each episode. We find that current algorithms benefit from training with truncated backpropagation through time and succeed on small mazes, but fall short of human performance on the large mazes, leaving room for future algorithmic designs to be evaluated on the Memory Maze.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31143488",
                        "name": "J. Pa\u0161ukonis"
                    },
                    {
                        "authorId": "2542999",
                        "name": "T. Lillicrap"
                    },
                    {
                        "authorId": "35006479",
                        "name": "Danijar Hafner"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[28] systematically evaluate the importance of key representational biases encoded by DQN\u2019s network by proposing simple linear representations."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6600be062e90ac6834dab1b33f5459087df14d2b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-11262",
                    "ArXiv": "2210.11262",
                    "DOI": "10.48550/ARXIV.2210.11262",
                    "CorpusId": 253018610
                },
                "corpusId": 253018610,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6600be062e90ac6834dab1b33f5459087df14d2b",
                "title": "RMBench: Benchmarking Deep Reinforcement Learning for Robotic Manipulator Control",
                "abstract": "Reinforcement learning is applied to solve actual complex tasks from high-dimensional, sensory inputs. The last decade has developed a long list of reinforcement learning algorithms. Recent progress benefits from deep learning for raw sensory signal representation. One question naturally arises: how well do they perform concerning different robotic manipulation tasks? Benchmarks use objective performance metrics to offer a scientific way to compare algorithms. In this paper, we present RMBench, the first benchmark for robotic manipulations, which have high-dimensional continuous action and state spaces. We implement and evaluate reinforcement learning algorithms that directly use observed pixels as inputs. We report their average performance and learning curves to show their performance and stability of training. Our study concludes that none of the studied algorithms can handle all tasks well, soft Actor-Critic outperforms most algorithms in average reward and stability, and an algorithm combined with data augmentation may facilitate learning policies. Our code is publicly available at https://github.com/xiangyanfei212/RMBench-2022, including all benchmark tasks and studied algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2036280391",
                        "name": "Yanfei Xiang"
                    },
                    {
                        "authorId": "2154562538",
                        "name": "Xin Wang"
                    },
                    {
                        "authorId": "2122826299",
                        "name": "Shu Hu"
                    },
                    {
                        "authorId": "2145924097",
                        "name": "Bin Zhu"
                    },
                    {
                        "authorId": "2174006572",
                        "name": "Xiao-lin Huang"
                    },
                    {
                        "authorId": "2120827116",
                        "name": "Xi Wu"
                    },
                    {
                        "authorId": "1491799130",
                        "name": "Siwei Lyu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "When learning a predictive information representation between a state, action pair and its subsequent state, the learning task is equivalent to modeling environment dynamics [4].",
                "While existing studies of learning predictive information representations in RL [2, 3, 4, 8, 9] have largely been limited to learning single-task specialist agents in simulated environments such as DM-Control [10] and Atari [11], our hypotheses can be seen as extending the generalization results in Lee et al.",
                "Previous studies [17, 2, 4, 3, 9, 18, 8, 19] have shown that predictive information [1] is an effective auxiliary or representation learning objective for RL agents or planning."
            ],
            "intents": [
                "background",
                "result"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2763b888dbff47c7c6e095287453edb94941cf82",
                "externalIds": {
                    "ArXiv": "2210.08217",
                    "DBLP": "conf/corl/LeeXLWF022",
                    "DOI": "10.48550/arXiv.2210.08217",
                    "CorpusId": 252918390
                },
                "corpusId": 252918390,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2763b888dbff47c7c6e095287453edb94941cf82",
                "title": "PI-QT-Opt: Predictive Information Improves Multi-Task Robotic Reinforcement Learning at Scale",
                "abstract": "The predictive information, the mutual information between the past and future, has been shown to be a useful representation learning auxiliary loss for training reinforcement learning agents, as the ability to model what will happen next is critical to success on many control tasks. While existing studies are largely restricted to training specialist agents on single-task settings in simulation, in this work, we study modeling the predictive information for robotic agents and its importance for general-purpose agents that are trained to master a large repertoire of diverse skills from large amounts of data. Specifically, we introduce Predictive Information QT-Opt (PI-QT-Opt), a QT-Opt agent augmented with an auxiliary loss that learns representations of the predictive information to solve up to 297 vision-based robot manipulation tasks in simulation and the real world with a single set of parameters. We demonstrate that modeling the predictive information significantly improves success rates on the training tasks and leads to better zero-shot transfer to unseen novel tasks. Finally, we evaluate PI-QT-Opt on real robots, achieving substantial and consistent improvement over QT-Opt in multiple experimental settings of varying environments, skills, and multi-task configurations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145145412",
                        "name": "Kuang-Huei Lee"
                    },
                    {
                        "authorId": "9961095",
                        "name": "Ted Xiao"
                    },
                    {
                        "authorId": "2598515",
                        "name": "Adrian Li"
                    },
                    {
                        "authorId": "3202367",
                        "name": "Paul Wohlhart"
                    },
                    {
                        "authorId": "33091759",
                        "name": "Ian S. Fischer"
                    },
                    {
                        "authorId": "2161346119",
                        "name": "Yao Lu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c0b7fb0066e8c1a5a2b4e4856135650eeef7702",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-06964",
                    "ArXiv": "2210.06964",
                    "DOI": "10.48550/arXiv.2210.06964",
                    "CorpusId": 252873572
                },
                "corpusId": 252873572,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0c0b7fb0066e8c1a5a2b4e4856135650eeef7702",
                "title": "Causality-driven Hierarchical Structure Discovery for Reinforcement Learning",
                "abstract": "Hierarchical reinforcement learning (HRL) effectively improves agents' exploration efficiency on tasks with sparse reward, with the guide of high-quality hierarchical structures (e.g., subgoals or options). However, how to automatically discover high-quality hierarchical structures is still a great challenge. Previous HRL methods can hardly discover the hierarchical structures in complex environments due to the low exploration efficiency by exploiting the randomness-driven exploration paradigm. To address this issue, we propose CDHRL, a causality-driven hierarchical reinforcement learning framework, leveraging a causality-driven discovery instead of a randomness-driven exploration to effectively build high-quality hierarchical structures in complicated environments. The key insight is that the causalities among environment variables are naturally fit for modeling reachable subgoals and their dependencies and can perfectly guide to build high-quality hierarchical structures. The results in two complex environments, 2D-Minecraft and Eden, show that CDHRL significantly boosts exploration efficiency with the causality-driven paradigm.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2072713784",
                        "name": "Shaohui Peng"
                    },
                    {
                        "authorId": "2125531721",
                        "name": "Xin Hu"
                    },
                    {
                        "authorId": "2118404461",
                        "name": "Rui Zhang"
                    },
                    {
                        "authorId": "2055742585",
                        "name": "Ke Tang"
                    },
                    {
                        "authorId": "47093626",
                        "name": "Jiaming Guo"
                    },
                    {
                        "authorId": "2187686017",
                        "name": "Qi Yi"
                    },
                    {
                        "authorId": "2118229954",
                        "name": "Rui Chen"
                    },
                    {
                        "authorId": "22066021",
                        "name": "Xishan Zhang"
                    },
                    {
                        "authorId": "1678776",
                        "name": "Zidong Du"
                    },
                    {
                        "authorId": "3353457",
                        "name": "Ling Li"
                    },
                    {
                        "authorId": "145461472",
                        "name": "Qi Guo"
                    },
                    {
                        "authorId": "7377735",
                        "name": "Yunji Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered.",
                "Along similar lines, (56; 2; 54; 19; 21; 18; 36; 1) have begun to explore contrastive systems and binary classifiers to do imitation learning and RL.",
                "One must (1) generate a good idea, (2) conduct mathematical analyses or experiments, (3) write a paper, and finally, (4) respond to reviewers in a satisfactory manner."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c4bac7f78d47b7ba2d21f86d0d9139cad9d4217",
                "externalIds": {
                    "ArXiv": "2210.05845",
                    "CorpusId": 257365537
                },
                "corpusId": 257365537,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0c4bac7f78d47b7ba2d21f86d0d9139cad9d4217",
                "title": "ConSpec: honing in on critical steps for rapid learning and generalization in RL",
                "abstract": "In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on critical steps. This algorithm, which we call contrastive introspection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of these prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (1) They enable rapid identification of all the critical steps. (2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon than it is to prospectively predict reward at every step taken in the environment. Altogether, ConSpec improves learning in a diverse set of RL tasks, including both those with explicit, discrete critical steps and those with complex, continuous critical steps.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1612977414",
                        "name": "Chen Sun"
                    },
                    {
                        "authorId": "91848984",
                        "name": "Wannan Yang"
                    },
                    {
                        "authorId": "2187058673",
                        "name": "Thomas Jiralerspong"
                    },
                    {
                        "authorId": "2210796846",
                        "name": "Dane Malenfant"
                    },
                    {
                        "authorId": "2166566550",
                        "name": "Benjamin Alsbury-Nealy"
                    },
                    {
                        "authorId": "1865800402",
                        "name": "Y. Bengio"
                    },
                    {
                        "authorId": "38498866",
                        "name": "B. Richards"
                    }
                ]
            }
        },
        {
            "contexts": [
                "CPC [73] and ST-DIM [30] use temporal contrastive losses to maximize the MI between the previous state embedding and a future embedding several time steps later, but they both do not leverage DA to transform the observations.",
                "Instead of maximizing the MI between the current state and the future state using the InfoNCE loss [73, 30, 38], SPR [23] produces state representations by minimizing the prediction error between the true future states and the predicted future states using an explicit multi-step DM."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "336eea1e32e2b93c63f97931710d3de54d05a336",
                "externalIds": {
                    "ArXiv": "2210.04561",
                    "DBLP": "journals/corr/abs-2210-04561",
                    "DOI": "10.48550/arXiv.2210.04561",
                    "CorpusId": 252780594
                },
                "corpusId": 252780594,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/336eea1e32e2b93c63f97931710d3de54d05a336",
                "title": "A Comprehensive Survey of Data Augmentation in Visual Reinforcement Learning",
                "abstract": "Visual reinforcement learning (RL), which makes decisions directly from high-dimensional visual inputs, has demonstrated significant potential in various domains. However, deploying visual RL techniques in the real world remains challenging due to their low sample efficiency and large generalization gaps. To tackle these obstacles, data augmentation (DA) has become a widely used technique in visual RL for acquiring sample-efficient and generalizable policies by diversifying the training data. This survey aims to provide a timely and essential review of DA techniques in visual RL in recognition of the thriving development in this field. In particular, we propose a unified framework for analyzing visual RL and understanding the role of DA in it. We then present a principled taxonomy of the existing augmentation techniques used in visual RL and conduct an in-depth discussion on how to better leverage augmented data in different scenarios. Moreover, we report a systematic empirical evaluation of DA-based techniques in visual RL and conclude by highlighting the directions for future research. As the first comprehensive survey of DA in visual RL, this work is expected to offer valuable guidance to this emerging field.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2171444073",
                        "name": "Guozheng Ma"
                    },
                    {
                        "authorId": "72682891",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2156151359",
                        "name": "Zhecheng Yuan"
                    },
                    {
                        "authorId": "2155638834",
                        "name": "Xueqian Wang"
                    },
                    {
                        "authorId": "2055907880",
                        "name": "Bo Yuan"
                    },
                    {
                        "authorId": "2140448089",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "generate fully imagined trajectories [39], model multi-agent interactions [22], learn competitive policies through selfplay [23], imagine goals in goal-conditioned policies [40], [41], meta-reinforcement learning [42], and offline reinforcement learning [43]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10fe28d8b7b3d54d9a2a445838c8bf77afbdf0cb",
                "externalIds": {
                    "ArXiv": "2210.01249",
                    "DBLP": "journals/corr/abs-2210-01249",
                    "DOI": "10.48550/arXiv.2210.01249",
                    "CorpusId": 252693056
                },
                "corpusId": 252693056,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/10fe28d8b7b3d54d9a2a445838c8bf77afbdf0cb",
                "title": "LOPR: Latent Occupancy PRediction using Generative Models",
                "abstract": "Environment prediction frameworks are integral for autonomous vehicles, enabling safe navigation in dynamic environments. LiDAR generated occupancy grid maps (L-OGMs) offer a robust bird's eye-view scene representation that facilitates joint scene predictions without relying on manual labeling unlike commonly used trajectory prediction frameworks. Prior approaches have optimized deterministic L-OGM prediction architectures directly in grid cell space. While these methods have achieved some degree of success in prediction, they occasionally grapple with unrealistic and incorrect predictions. We claim that the quality and realism of the forecasted occupancy grids can be enhanced with the use of generative models. We propose a framework that decouples occupancy prediction into: representation learning and stochastic prediction within the learned latent space. Our approach allows for conditioning the model on other available sensor modalities such as RGB-cameras and high definition maps. We demonstrate that our approach achieves state-of-the-art performance and is readily transferable between different robotic platforms on the real-world NuScenes, Waymo Open, and a custom dataset we collected on an experimental vehicle platform.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2052773838",
                        "name": "Bernard Lange"
                    },
                    {
                        "authorId": "30112153",
                        "name": "Masha Itkina"
                    },
                    {
                        "authorId": "79262652",
                        "name": "Mykel J. Kochenderfer"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dce5272415051d8b6a8e038be8c85ced5544869f",
                "externalIds": {
                    "ArXiv": "2209.14932",
                    "DBLP": "journals/corr/abs-2209-14932",
                    "DOI": "10.48550/arXiv.2209.14932",
                    "CorpusId": 252595886
                },
                "corpusId": 252595886,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dce5272415051d8b6a8e038be8c85ced5544869f",
                "title": "Contrastive Unsupervised Learning of World Model with Invariant Causal Features",
                "abstract": "In this paper we present a world model , which learns causal features using the invariance principle. In particular, we use contrastive unsupervised learning to learn the invariant causal features, which enforces invariance across augmentations of irrelevant parts or styles of the observation. The world-model-based reinforcement learning methods independently optimize representation learning and the policy. Thus na\u00efve contrastive loss implementation collapses due to a lack of supervisory signals to the representation learning module. We propose an intervention invariant auxiliary task to mitigate this issue. Specifically, we utilize depth prediction to explicitly enforce the invariance and use data augmentation as style intervention on the RGB observation space. Our design leverages unsupervised representation learning to learn the world model with invariant causal features. Our proposed method significantly outperforms current state-of-the-art model-based and model-free reinforcement learning methods on out-of-distribution point navigation tasks on the iGibson dataset. Moreover, our proposed model excels at the sim-to-real transfer of our perception learning module. 1",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143840956",
                        "name": "Rudra P. K. Poudel"
                    },
                    {
                        "authorId": "3072183",
                        "name": "Harit Pandya"
                    },
                    {
                        "authorId": "1745672",
                        "name": "R. Cipolla"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In classical RL [26], where behaviour is learned from a given reward function, mutual information objectives are commonly used to find compact state representations that increase performance by discarding irrelevant information [29, 3, 37, 24, 22]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "36b741e5d098dfc6cd44ea1ff94cbda3fb9734be",
                "externalIds": {
                    "ArXiv": "2209.12093",
                    "DBLP": "journals/corr/abs-2209-12093",
                    "DOI": "10.48550/arXiv.2209.12093",
                    "CorpusId": 252531787
                },
                "corpusId": 252531787,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/36b741e5d098dfc6cd44ea1ff94cbda3fb9734be",
                "title": "Learn what matters: cross-domain imitation learning with task-relevant embeddings",
                "abstract": "We study how an autonomous agent learns to perform a task from demonstrations in a different domain, such as a different environment or different agent. Such cross-domain imitation learning is required to, for example, train an artificial agent from demonstrations of a human expert. We propose a scalable framework that enables cross-domain imitation learning without access to additional demonstrations or further domain knowledge. We jointly train the learner agent's policy and learn a mapping between the learner and expert domains with adversarial training. We effect this by using a mutual information criterion to find an embedding of the expert's state space that contains task-relevant information and is invariant to domain specifics. This step significantly simplifies estimating the mapping between the learner and expert domains and hence facilitates end-to-end learning. We demonstrate successful transfer of policies between considerably different domains, without extra supervision such as additional demonstrations, and in situations where other methods fail.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2120160070",
                        "name": "Tim Franzmeyer"
                    },
                    {
                        "authorId": "143635540",
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "authorId": "143848064",
                        "name": "Jo\u00e3o F. Henriques"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another possible path would be using previously proposed benchmarks like the AtariARI benchmark (Anand et al., 2020), which tries to evaluate representations using the RAM states as ground truth labels.",
                "Another possible path would be using previously proposed benchmarks like the AtariARI benchmark Anand et al. (2020), which tries to evaluate representations using the RAM states as ground truth labels.",
                "Pretraining representations Previous work has explored, similarly to our approach, pretraining representations using self-supervised methods which led to great data-efficiency improvements in the fine-tuning phase (Schwarzer et al., 2021b; Zhan et al., 2020) or superior results in evaluation tasks, like AtariARI (Anand et al., 2020).",
                "\u2026unlike images from datasets like ImageNet or MSCOCO, observations from reinforcement learning environments share similarities in more dimensions, for example, time (Stooke et al., 2021; Anand et al., 2020), semantics (Fan et al., 2022; Zhong et al., 2022), and behavior (Agarwal et al., 2021a).",
                "\u2026work has explored, similarly to our approach, pretraining representations using self-supervised methods which led to great data-efficiency improvements in the fine-tuning phase (Schwarzer et al., 2021b; Zhan et al., 2020) or superior results in evaluation tasks, like AtariARI (Anand et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9e2cba3255320ee093105bc18b18fd0609f39d8e",
                "externalIds": {
                    "ArXiv": "2209.10901",
                    "DBLP": "journals/corr/abs-2209-10901",
                    "DOI": "10.48550/arXiv.2209.10901",
                    "CorpusId": 252439214
                },
                "corpusId": 252439214,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9e2cba3255320ee093105bc18b18fd0609f39d8e",
                "title": "Pretraining the Vision Transformer using self-supervised methods for vision based Deep Reinforcement Learning",
                "abstract": "The Vision Transformer architecture has shown to be competitive in the computer vision (CV) space where it has dethroned convolution-based networks in several benchmarks. Nevertheless, convolutional neural networks (CNN) remain the preferential architecture for the representation module in reinforcement learning. In this work, we study pretraining a Vision Transformer using several state-of-the-art self-supervised methods and assess the quality of the learned representations. To show the importance of the temporal dimension in this context we propose an extension of VICReg to better capture temporal relations between observations by adding a temporal order verification task. Our results show that all methods are effective in learning useful representations and avoiding representational collapse for observations from Atari Learning Environment (ALE) which leads to improvements in data efficiency when we evaluated in reinforcement learning (RL). Moreover, the encoder pretrained with the temporal order verification task shows the best results across all experiments, with richer representations, more focused attention maps and sparser representation vectors throughout the layers of the encoder, which shows the importance of exploring such similarity dimension. With this work, we hope to provide some insights into the representations learned by ViT during a self-supervised pretraining with observations from RL environments and which properties arise in the representations that lead to the best-performing agents. The source code will be available at: https://github.com/mgoulao/TOV-VICReg",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "52166049",
                        "name": "Manuel Goul\u00e3o"
                    },
                    {
                        "authorId": "2158460715",
                        "name": "Arlindo L. Oliveira"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several works have addressed the problem of finding compact state representations from raw sensory streams in a model-free MDP setting [3], [4], [5], [6]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b684c23c838931203a35d187d92923ef91d1c1ea",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-07039",
                    "ArXiv": "2209.07039",
                    "DOI": "10.48550/arXiv.2209.07039",
                    "CorpusId": 252280546
                },
                "corpusId": 252280546,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b684c23c838931203a35d187d92923ef91d1c1ea",
                "title": "Sparsity Inducing Representations for Policy Decompositions",
                "abstract": "\u2014Policy Decomposition (PoDec) is a framework that lessens the curse of dimensionality when deriving policies to optimal control problems. For a given system representation, i.e. the state variables and control inputs describing a system, PoDec generates strategies to decompose the joint optimization of policies for all control inputs. Thereby, policies for different inputs are derived in a decoupled or cascaded fashion and as functions of some subsets of the state variables, leading to reduction in computation. However, the choice of system representation is crucial as it dictates the suboptimality of the resulting policies. We present a heuristic method to \ufb01nd a representation more amenable to decomposition. Our approach is based on the observation that every decomposition enforces a sparsity pattern in the resulting policies at the cost of optimality and a representation that already leads to a sparse optimal policy is likely to produce decompositions with lower suboptimalities. As the optimal policy is not known we construct a system representation that sparsi\ufb01es its LQR approximation. For a simpli\ufb01ed biped, a 4 degree-of-freedom manipulator, and a quadcopter, we discover decompositions that offer 10% reduction in trajectory costs over those identi\ufb01ed by vanilla PoDec. Moreover, the decomposition policies produce trajectories with substantially lower costs compared to policies obtained from state-of-the-art reinforcement learning algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30623582",
                        "name": "Ashwin Khadke"
                    },
                    {
                        "authorId": "1702216",
                        "name": "H. Geyer"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2d1b83626d9f55b94335d24037a253a9ac35d63d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-07364",
                    "ArXiv": "2209.07364",
                    "DOI": "10.48550/arXiv.2209.07364",
                    "CorpusId": 252283953
                },
                "corpusId": 252283953,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2d1b83626d9f55b94335d24037a253a9ac35d63d",
                "title": "Continuous MDP Homomorphisms and Homomorphic Policy Gradient",
                "abstract": "Abstraction has been widely studied as a way to improve the efficiency and generalization of reinforcement learning algorithms. In this paper, we study abstraction in the continuous-control setting. We extend the definition of MDP homomorphisms to encompass continuous actions in continuous state spaces. We derive a policy gradient theorem on the abstract MDP, which allows us to leverage approximate symmetries of the environment for policy optimization. Based on this theorem, we propose an actor-critic algorithm that is able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. We demonstrate the effectiveness of our method on benchmark tasks in the DeepMind Control Suite. Our method's ability to utilize MDP homomorphisms for representation learning leads to improved performance when learning from pixel observations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1417282075",
                        "name": "S. Rezaei-Shoshtari"
                    },
                    {
                        "authorId": "2004617613",
                        "name": "Rosie Zhao"
                    },
                    {
                        "authorId": "1784317",
                        "name": "P. Panangaden"
                    },
                    {
                        "authorId": "2462512",
                        "name": "D. Meger"
                    },
                    {
                        "authorId": "144368601",
                        "name": "Doina Precup"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our work is also related to unsupervised representation learning methods based on mutual information estimation [33], [34]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c9b5412bcef781b001222a8952c104af84889f5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-05333",
                    "ArXiv": "2209.05333",
                    "DOI": "10.48550/arXiv.2209.05333",
                    "CorpusId": 252199426
                },
                "corpusId": 252199426,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0c9b5412bcef781b001222a8952c104af84889f5",
                "title": "Self-supervised Sequential Information Bottleneck for Robust Exploration in Deep Reinforcement Learning",
                "abstract": "Effective exploration is critical for reinforcement learning agents in environments with sparse rewards or high-dimensional state-action spaces. Recent works based on state-visitation counts, curiosity and entropy-maximization generate intrinsic reward signals to motivate the agent to visit novel states for exploration. However, the agent can get distracted by perturbations to sensor inputs that contain novel but task-irrelevant information, e.g. due to sensor noise or changing background. In this work, we introduce the sequential information bottleneck objective for learning compressed and temporally coherent representations by modelling and compressing sequential predictive information in time-series observations. For efficient exploration in noisy environments, we further construct intrinsic rewards that capture task-relevant state novelty based on the learned representations. We derive a variational upper bound of our sequential information bottleneck objective for practical optimization and provide an information-theoretic interpretation of the derived upper bound. Our experiments on a set of challenging image-based simulated control tasks show that our method achieves better sample efficiency, and robustness to both white noise and natural video backgrounds compared to state-of-art methods based on curiosity, entropy maximization and information-gain.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "113024804",
                        "name": "Bang You"
                    },
                    {
                        "authorId": "48086634",
                        "name": "Jingming Xie"
                    },
                    {
                        "authorId": "49070272",
                        "name": "Youping Chen"
                    },
                    {
                        "authorId": "2107720654",
                        "name": "Jan Peters"
                    },
                    {
                        "authorId": "7931507",
                        "name": "O. Arenz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This objective has previously been used in the context of augmented multiscale DIM (AMDIM) [41], cross-modal DIM (CM-DIM) [51], and spatio-temporal DIM (ST-DIM) [52].",
                "Previously, augmented multiscale DIM (AMDIM) [41], cross-modal DIM (CM-DIM) [50, 51], and spatio-temporal DIM (ST-DIM) [52] used local intermediate representation of convolutional layers to capture multi-scale relationships between multiple views, modalities or time frames.",
                "The CC objective has been used in AMDIM [41], CM-DIM [51], and ST-DIM [52]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e21578b7c51a8f6dc0dc4da31683f74fdfc64ee0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-02876",
                    "ArXiv": "2209.02876",
                    "DOI": "10.48550/arXiv.2209.02876",
                    "CorpusId": 252111020
                },
                "corpusId": 252111020,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e21578b7c51a8f6dc0dc4da31683f74fdfc64ee0",
                "title": "Self-supervised multimodal neuroimaging yields predictive representations for a spectrum of Alzheimer's phenotypes",
                "abstract": "Recent neuroimaging studies that focus on predicting brain disorders via modern machine learning approaches commonly include a single modality and rely on supervised over-parameterized models.However, a single modality provides only a limited view of the highly complex brain. Critically, supervised models in clinical settings lack accurate diagnostic labels for training. Coarse labels do not capture the long-tailed spectrum of brain disorder phenotypes, which leads to a loss of generalizability of the model that makes them less useful in diagnostic settings. This work presents a novel multi-scale coordinated framework for learning multiple representations from multimodal neuroimaging data. We propose a general taxonomy of informative inductive biases to capture unique and joint information in multimodal self-supervised fusion. The taxonomy forms a family of decoder-free models with reduced computational complexity and a propensity to capture multi-scale relationships between local and global representations of the multimodal inputs. We conduct a comprehensive evaluation of the taxonomy using functional and structural magnetic resonance imaging (MRI) data across a spectrum of Alzheimer's disease phenotypes and show that self-supervised models reveal disorder-relevant brain regions and multimodal links without access to the labels during pre-training. The proposed multimodal self-supervised learning yields representations with improved classification performance for both modalities. The concomitant rich and flexible unsupervised deep learning framework captures complex multimodal relationships and provides predictive performance that meets or exceeds that of a more narrow supervised classification analysis. We present elaborate quantitative evidence of how this framework can significantly advance our search for missing links in complex brain disorders.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "26920432",
                        "name": "A. Fedorov"
                    },
                    {
                        "authorId": "146605948",
                        "name": "E. Geenjaar"
                    },
                    {
                        "authorId": "2218256077",
                        "name": "Lei Wu"
                    },
                    {
                        "authorId": "8118056",
                        "name": "Tristan Sylvain"
                    },
                    {
                        "authorId": "6659971",
                        "name": "T. DeRamus"
                    },
                    {
                        "authorId": "39175553",
                        "name": "M. Luck"
                    },
                    {
                        "authorId": "4342974",
                        "name": "Maria B. Misiura"
                    },
                    {
                        "authorId": "40482726",
                        "name": "R. Devon Hjelm"
                    },
                    {
                        "authorId": "2122479",
                        "name": "S. Plis"
                    },
                    {
                        "authorId": "2067236261",
                        "name": "V. Calhoun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[49] introduced a new contrastive loss to learn better representation in a fully unsupervised setting, while in [45], the authors adapted the BYOL objective [50] for learning the visual encoder in parallel with the RL objective, which led to state-of-the-art results on the Atari benchmark."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "023cb3905ce58656b551f07e5165acab5551f63d",
                "externalIds": {
                    "PubMedCentral": "9460564",
                    "DBLP": "journals/sensors/LuuVNY22",
                    "DOI": "10.3390/s22176504",
                    "CorpusId": 251962867,
                    "PubMed": "36080961"
                },
                "corpusId": 251962867,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/023cb3905ce58656b551f07e5165acab5551f63d",
                "title": "Visual Pretraining via Contrastive Predictive Model for Pixel-Based Reinforcement Learning",
                "abstract": "In an attempt to overcome the limitations of reward-driven representation learning in vision-based reinforcement learning (RL), an unsupervised learning framework referred to as the visual pretraining via contrastive predictive model (VPCPM) is proposed to learn the representations detached from the policy learning. Our method enables the convolutional encoder to perceive the underlying dynamics through a pair of forward and inverse models under the supervision of the contrastive loss, thus resulting in better representations. In experiments with a diverse set of vision control tasks, by initializing the encoders with VPCPM, the performance of state-of-the-art vision-based RL algorithms is significantly boosted, with 44% and 10% improvement for RAD and DrQ at 100 steps, respectively. In comparison to the prior unsupervised methods, the performance of VPCPM matches or outperforms all the baselines. We further demonstrate that the learned representations successfully generalize to the new tasks that share a similar observation and action space.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145491883",
                        "name": "T. Luu"
                    },
                    {
                        "authorId": "1680408",
                        "name": "Thang Vu"
                    },
                    {
                        "authorId": "2117824129",
                        "name": "Thanh Nguyen"
                    },
                    {
                        "authorId": "2154412756",
                        "name": "C. D. Yoo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "5) and momentum encoders [83] were used in combination with SAC and Proximal Policy Optimization (PPO) [84] by [48], SAC and RainbowDQN [85] by CURL [44], distributional Q-learning by [86], and PPO by [87] for continuous and discrete control problems."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8f5b9f3af25d7f38886ecd3589531e0752e57012",
                "externalIds": {
                    "ArXiv": "2208.14226",
                    "DBLP": "journals/corr/abs-2208-14226",
                    "DOI": "10.48550/arXiv.2208.14226",
                    "CorpusId": 251928947
                },
                "corpusId": 251928947,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8f5b9f3af25d7f38886ecd3589531e0752e57012",
                "title": "Unsupervised Representation Learning in Deep Reinforcement Learning: A Review",
                "abstract": "This review addresses the problem of learning abstract representations of the measurement data in the context of Deep Reinforcement Learning (DRL). While the data are often ambiguous, high-dimensional, and complex to interpret, many dynamical systems can be effectively described by a low-dimensional set of state variables. Discovering these state variables from the data is a crucial aspect for improving the data efficiency, robustness and generalization of DRL methods, tackling the \\textit{curse of dimensionality}, and bringing interpretability and insights into black-box DRL. This review provides a comprehensive and complete overview of unsupervised representation learning in DRL by describing the main Deep Learning tools used for learning representations of the world, providing a systematic view of the method and principles, summarizing applications, benchmarks and evaluation strategies, and discussing open challenges and future directions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "118468227",
                        "name": "N. Botteghi"
                    },
                    {
                        "authorId": "145788256",
                        "name": "M. Poel"
                    },
                    {
                        "authorId": "2065340953",
                        "name": "C. Brune"
                    }
                ]
            }
        },
        {
            "contexts": [
                "STDIM [3], ATC [46] and BVS-DIM [35] incorporate temporal information in their contrastive objective, adapting similar techniques from the unsupervised video representation learning [45].",
                "Some prior work [41, 21, 3] evaluate the quality of their pretrained representations by probing for ground truth state variables such as agent/object locations and game scores."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b62f6f765f033c1f023c4a424a20571564e61d97",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-12345",
                    "ArXiv": "2208.12345",
                    "DOI": "10.48550/arXiv.2208.12345",
                    "CorpusId": 251881409
                },
                "corpusId": 251881409,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b62f6f765f033c1f023c4a424a20571564e61d97",
                "title": "Light-weight probing of unsupervised representations for Reinforcement Learning",
                "abstract": "Unsupervised visual representation learning offers the opportunity to leverage large corpora of unlabeled trajectories to form useful visual representations, which can benefit the training of reinforcement learning (RL) algorithms. However, evaluating the fitness of such representations requires training RL algorithms which is computationally intensive and has high variance outcomes. To alleviate this issue, we design an evaluation protocol for unsupervised RL representations with lower variance and up to 600x lower computational cost. Inspired by the vision community, we propose two linear probing tasks: predicting the reward observed in a given state, and predicting the action of an expert in a given state. These two tasks are generally applicable to many RL domains, and we show through rigorous experimentation that they correlate strongly with the actual downstream control performance on the Atari100k Benchmark. This provides a better method for exploring the space of pretraining algorithms without the need of running RL evaluations for every setting. Leveraging this framework, we further improve existing self-supervised learning (SSL) recipes for RL, highlighting the importance of the forward model, the size of the visual backbone, and the precise formulation of the unsupervised objective.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108277713",
                        "name": "Wancong Zhang"
                    },
                    {
                        "authorId": "2148899281",
                        "name": "Anthony GX-Chen"
                    },
                    {
                        "authorId": "2162736903",
                        "name": "Vlad Sobal"
                    },
                    {
                        "authorId": "1688882",
                        "name": "Yann LeCun"
                    },
                    {
                        "authorId": "3422899",
                        "name": "Nicolas Carion"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ae40437fb4733c9c4364475fb7b86e9c35d147dc",
                "externalIds": {
                    "DBLP": "conf/cig/Feng22",
                    "DOI": "10.1109/CoG51982.2022.9893637",
                    "CorpusId": 252409929
                },
                "corpusId": 252409929,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ae40437fb4733c9c4364475fb7b86e9c35d147dc",
                "title": "Multi-goal Reinforcement Learning via Exploring Successor Matching",
                "abstract": "Multi-goal reinforcement learning (RL) agent aims at achieving and generalizing over various goals. Due to the sparsity of goal-reaching rewards, it suffers from unreliable value estimation and is thus unable to efficiently identify essential states towards specific goal-reaching. To deal with the problem, we propose Exploring Successor Matching (ESM), a framework that enables goal-conditioned policy and progressively encourages the multi-goal exploration towards the promising frontier. ESM adopts the idea of successor feature and extends it to goal-reaching successor mapping that serves as a more stable state feature under sparse rewards. After acquiring the successor mapping, it further explores intrinsic goals that are more likely to be achieved from a diverse set of states in terms of future state occupancies. Experiments on challenging manipulation tasks show that ESM deals well with sparse rewards and achieves better sample efficiency.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149117203",
                        "name": "Xiaoyun Feng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Contrastive losses (Oord et al., 2018; Anand et al., 2019; Srinivas et al., 2020; Stooke et al., 2021), which encourage similar states to be closer in embedding space, where the notion of similarity is usually defined in terms of temporal distance (Anand et al.",
                "\u2026et al., 2021), which encourage similar states to be closer in embedding space, where the notion of similarity is usually defined in terms of temporal distance (Anand et al., 2019; Sermanet et al., 2018) or image-based data augmentations (Srinivas et al., 2020), also show promising performance.",
                ", 2021), which encourage similar states to be closer in embedding space, where the notion of similarity is usually defined in terms of temporal distance (Anand et al., 2019; Sermanet et al., 2018) or image-based data augmentations (Srinivas et al.",
                "Contrastive losses (Oord et al., 2018; Anand et al., 2019; Srinivas et al., 2020; Stooke et al., 2021), which encourage similar states to be closer in embedding space, where the notion of similarity is usually defined in terms of temporal distance (Anand et al., 2019; Sermanet et al., 2018) or\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "42929aa6ebf8cdc0e7d7662751dc228de07800bb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-09515",
                    "ArXiv": "2208.09515",
                    "DOI": "10.48550/arXiv.2208.09515",
                    "CorpusId": 251719318
                },
                "corpusId": 251719318,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/42929aa6ebf8cdc0e7d7662751dc228de07800bb",
                "title": "Spectral Decomposition Representation for Reinforcement Learning",
                "abstract": "Representation learning often plays a critical role in reinforcement learning by managing the curse of dimensionality. A representative class of algorithms exploits a spectral decomposition of the stochastic transition dynamics to construct representations that enjoy strong theoretical properties in an idealized setting. However, current spectral methods suffer from limited applicability because they are constructed for state-only aggregation and derived from a policy-dependent transition kernel, without considering the issue of exploration. To address these issues, we propose an alternative spectral method, Spectral Decomposition Representation (SPEDER), that extracts a state-action abstraction from the dynamics without inducing spurious dependence on the data collection policy, while also balancing the exploration-versus-exploitation trade-off during learning. A theoretical analysis establishes the sample efficiency of the proposed algorithm in both the online and offline settings. In addition, an experimental investigation demonstrates superior performance over current state-of-the-art algorithms across several benchmarks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32453998",
                        "name": "Tongzheng Ren"
                    },
                    {
                        "authorId": "1993655237",
                        "name": "Tianjun Zhang"
                    },
                    {
                        "authorId": "87068304",
                        "name": "Lisa Lee"
                    },
                    {
                        "authorId": "49988044",
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "authorId": "50319359",
                        "name": "D. Schuurmans"
                    },
                    {
                        "authorId": "144445937",
                        "name": "Bo Dai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "becdb3a4ec34089ac8f34b647aa92365ba901db4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-14800",
                    "ArXiv": "2207.14800",
                    "DOI": "10.48550/arXiv.2207.14800",
                    "CorpusId": 250341003
                },
                "corpusId": 250341003,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/becdb3a4ec34089ac8f34b647aa92365ba901db4",
                "title": "Contrastive UCB: Provably Efficient Contrastive Self-Supervised Learning in Online Reinforcement Learning",
                "abstract": "In view of its power in extracting feature representation, contrastive self-supervised learning has been successfully integrated into the practice of (deep) reinforcement learning (RL), leading to efficient policy learning in various applications. Despite its tremendous empirical successes, the understanding of contrastive learning for RL remains elusive. To narrow such a gap, we study how RL can be empowered by contrastive learning in a class of Markov decision processes (MDPs) and Markov games (MGs) with low-rank transitions. For both models, we propose to extract the correct feature representations of the low-rank model by minimizing a contrastive loss. Moreover, under the online setting, we propose novel upper confidence bound (UCB)-type algorithms that incorporate such a contrastive loss with online RL algorithms for MDPs or MGs. We further theoretically prove that our algorithm recovers the true representations and simultaneously achieves sample efficiency in learning the optimal policy and Nash equilibrium in MDPs and MGs. We also provide empirical studies to demonstrate the efficacy of the UCB-based contrastive learning method for RL. To the best of our knowledge, we provide the first provably efficient online RL algorithm that incorporates contrastive learning for representation learning. Our codes are available at https://github.com/Baichenjia/Contrastive-UCB.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143654948",
                        "name": "Shuang Qiu"
                    },
                    {
                        "authorId": "2151977112",
                        "name": "Lingxiao Wang"
                    },
                    {
                        "authorId": "150944133",
                        "name": "Chenjia Bai"
                    },
                    {
                        "authorId": "150358650",
                        "name": "Zhuoran Yang"
                    },
                    {
                        "authorId": "50218397",
                        "name": "Zhaoran Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, object-level world-models have been used to constrain action-spaces, and states in robotics [85,86] and control domains [87,12,88]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "52fcd9178c489d3b5d056fd214de115871e69e06",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-10075",
                    "ArXiv": "2207.10075",
                    "DOI": "10.48550/arXiv.2207.10075",
                    "CorpusId": 250699195
                },
                "corpusId": 250699195,
                "publicationVenue": {
                    "id": "a8f26d13-e373-4e48-b57b-ef89bf48f4db",
                    "name": "Asian Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Comput Vis",
                        "ACCV"
                    ],
                    "url": "http://www.cvl.iis.u-tokyo.ac.jp/afcv/"
                },
                "url": "https://www.semanticscholar.org/paper/52fcd9178c489d3b5d056fd214de115871e69e06",
                "title": "Is an Object-Centric Video Representation Beneficial for Transfer?",
                "abstract": "The objective of this work is to learn an object-centric video representation, with the aim of improving transferability to novel tasks, i.e., tasks different from the pre-training task of action classification. To this end, we introduce a new object-centric video recognition model based on a transformer architecture. The model learns a set of object-centric summary vectors for the video, and uses these vectors to fuse the visual and spatio-temporal trajectory 'modalities' of the video clip. We also introduce a novel trajectory contrast loss to further enhance objectness in these summary vectors. With experiments on four datasets -- SomethingSomething-V2, SomethingElse, Action Genome and EpicKitchens -- we show that the object-centric model outperforms prior video representations (both object-agnostic and object-aware), when: (1) classifying actions on unseen objects and unseen environments; (2) low-shot learning of novel classes; (3) linear probe to other downstream tasks; as well as (4) for standard action classification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111332521",
                        "name": "Chuhan Zhang"
                    },
                    {
                        "authorId": "2110759501",
                        "name": "Ankush Gupta"
                    },
                    {
                        "authorId": "1688869",
                        "name": "Andrew Zisserman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For learning representations from image-based RL tasks, CPC (van den Oord et al., 2018), ST-DIM (Anand et al., 2019), DRIML (Mazoure et al., 2020), CURL (Laskin et al., 2020), and SPR (Schwarzer et al., 2021) learn representations by optimizing various temporal contrastive losses.",
                "To address these sample complexity demands, a recent line of work (van den Oord et al., 2018; Anand et al., 2019; Mazoure et al., 2020; Yang & Nachum, 2021) has incorporated advances in unsupervised representation learning from the supervised learning literature into developing RL agents.",
                "We compare our representational objective against CURL and SPR in Section 6, and demonstrate that under linear evaluation protocol, ours outperform both CURL and SPR. Note, we refer the readers to Schwarzer et al. (2021) for comparisons between SPR and CPC, ST-DIM, and DRIML."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "20eca4d183e9f2de55948bde9f7fea60a3337c6e",
                "externalIds": {
                    "ArXiv": "2207.05837",
                    "DBLP": "conf/icml/ChangWKS22",
                    "DOI": "10.48550/arXiv.2207.05837",
                    "CorpusId": 250340843
                },
                "corpusId": 250340843,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/20eca4d183e9f2de55948bde9f7fea60a3337c6e",
                "title": "Learning Bellman Complete Representations for Offline Policy Evaluation",
                "abstract": "We study representation learning for Offline Reinforcement Learning (RL), focusing on the important task of Offline Policy Evaluation (OPE). Recent work shows that, in contrast to supervised learning, realizability of the Q-function is not enough for learning it. Two sufficient conditions for sample-efficient OPE are Bellman completeness and coverage. Prior work often assumes that representations satisfying these conditions are given, with results being mostly theoretical in nature. In this work, we propose BCRL, which directly learns from data an approximately linear Bellman complete representation with good coverage. With this learned representation, we perform OPE using Least Square Policy Evaluation (LSPE) with linear functions in our learned representation. We present an end-to-end theoretical analysis, showing that our two-stage algorithm enjoys polynomial sample complexity provided some representation in the rich class considered is linear Bellman complete. Empirically, we extensively evaluate our algorithm on challenging, image-based continuous control tasks from the Deepmind Control Suite. We show our representation enables better OPE compared to previous representation learning methods developed for off-policy RL (e.g., CURL, SPR). BCRL achieve competitive OPE error with the state-of-the-art method Fitted Q-Evaluation (FQE), and beats FQE when evaluating beyond the initial state distribution. Our ablations show that both linear Bellman complete and coverage components of our method are crucial.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157681314",
                        "name": "Jonathan D. Chang"
                    },
                    {
                        "authorId": "2148352172",
                        "name": "Kaiwen Wang"
                    },
                    {
                        "authorId": "3174388",
                        "name": "Nathan Kallus"
                    },
                    {
                        "authorId": "144426657",
                        "name": "Wen Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Extensive work [2, 3, 12] in dissimilar domains of AI and games such as player experience modeling, general gameplaying or content generation make use of the internal state of the game [1, 9] obtained from the game engine.",
                "[1], we evaluate how well the learned representations have captured information relevant to the game"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b52e9e69c5fc2666e55b7098658d3549a8f2ecdc",
                "externalIds": {
                    "ArXiv": "2207.01289",
                    "DBLP": "journals/corr/abs-2207-01289",
                    "DOI": "10.1145/3555858.3555902",
                    "CorpusId": 250264157
                },
                "corpusId": 250264157,
                "publicationVenue": {
                    "id": "9f0494a8-8595-4796-8a3e-68c47ccdfe81",
                    "name": "International Conference on Foundations of Digital Games",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Found Digit Game",
                        "Foundations of Digital Games",
                        "FDG",
                        "Found Digit Game"
                    ],
                    "url": "http://www.foundationsofdigitalgames.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b52e9e69c5fc2666e55b7098658d3549a8f2ecdc",
                "title": "Game\u00a0State Learning via Game\u00a0Scene\u00a0Augmentation",
                "abstract": "Having access to accurate game state information is of utmost importance for any artificial intelligence task including game-playing, testing, player modeling, and procedural content generation. Self-Supervised Learning (SSL) techniques have shown to be capable of inferring accurate game state information from the high-dimensional pixel input of game footage into compressed latent representations. Contrastive Learning is a popular SSL paradigm where the visual understanding of the game\u2019s images comes from contrasting dissimilar and similar game states defined by simple image augmentation methods. In this study, we introduce a new game scene augmentation technique\u2014named GameCLR\u2014that takes advantage of the game-engine to define and synthesize specific, highly-controlled renderings of different game states, thereby, boosting contrastive learning performance. We test our GameCLR technique on images of the CARLA driving simulator environment and compare it against the popular SimCLR baseline SSL method. Our results suggest that GameCLR can infer the game\u2019s state information from game footage more accurately compared to the baseline. Our proposed approach allows us to conduct game artificial intelligence research by directly utilizing screen pixels as input.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "37498103",
                        "name": "C. Trivedi"
                    },
                    {
                        "authorId": "2091878621",
                        "name": "Konstantinos Makantasis"
                    },
                    {
                        "authorId": "1713331",
                        "name": "Antonios Liapis"
                    },
                    {
                        "authorId": "1686193",
                        "name": "Georgios N. Yannakakis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[65] that demonstrated the effectiveness of auxiliary unsupervised objectives for RL, a variety of unsupervised learning objectives have been studied, including future latent reconstruction [27, 66, 67, 68], bisimulation [69, 70], contrastive learning [71, 72, 73, 74, 75, 30, 76, 29, 77, 78], keypoint extraction [79, 80, 81, 82], world model learning [8, 9, 10, 55] and reconstruction [83, 84]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "31d629bb161d8199e18b6f2ed7e4ecbda10b6797",
                "externalIds": {
                    "DBLP": "conf/corl/SeoHLLJLA22",
                    "ArXiv": "2206.14244",
                    "DOI": "10.48550/arXiv.2206.14244",
                    "CorpusId": 250113367
                },
                "corpusId": 250113367,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31d629bb161d8199e18b6f2ed7e4ecbda10b6797",
                "title": "Masked World Models for Visual Control",
                "abstract": "Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling approach achieves state-of-the-art performance on a variety of visual robotic tasks from Meta-world and RLBench, e.g., we achieve 81.7% success rate on 50 visual robotic manipulation tasks from Meta-world, while the baseline achieves 67.9%. Code is available on the project website: https://sites.google.com/view/mwm-rl.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2067714176",
                        "name": "Younggyo Seo"
                    },
                    {
                        "authorId": "35006479",
                        "name": "Danijar Hafner"
                    },
                    {
                        "authorId": "2143855835",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "32324034",
                        "name": "Fangchen Liu"
                    },
                    {
                        "authorId": "2055291154",
                        "name": "Stephen James"
                    },
                    {
                        "authorId": "3436470",
                        "name": "Kimin Lee"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Self-supervised representation learning aims to group states together without loss of that property (Anand et al. 2019; Kipf, van der Pol, and Welling 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1632f51c8e573264730061dc5c9e7db821535bc4",
                "externalIds": {
                    "DBLP": "conf/aaai/ZhuJL0Z22",
                    "DOI": "10.1609/aaai.v36i8.20913",
                    "CorpusId": 250302464
                },
                "corpusId": 250302464,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1632f51c8e573264730061dc5c9e7db821535bc4",
                "title": "Invariant Action Effect Model for Reinforcement Learning",
                "abstract": "Good representations can help RL agents perform concise modeling of their surroundings, and thus support effective decision-making in complex environments. \n Previous methods learn good representations by imposing extra constraints on dynamics.\n However, in the causal perspective, the causation between the action and its effect is not fully considered in those methods, which leads to the ignorance of the underlying relations among the action effects on the transitions. \n Based on the intuition that the same action always causes similar effects among different states, we induce such causation by taking the invariance of action effects among states as the relation.\n By explicitly utilizing such invariance, in this paper, we show that a better representation can be learned and potentially improves the sample efficiency and the generalization ability of the learned policy. \n We propose Invariant Action Effect Model (IAEM) to capture the invariance in action effects, where the effect of an action is represented as the residual of representations from neighboring states.\n IAEM is composed of two parts:\n (1) a new contrastive-based loss to capture the underlying invariance of action effects;\n (2) an individual action effect and provides a self-adapted weighting strategy to tackle the corner cases where the invariance does not hold.\n The extensive experiments on two benchmarks, i.e. Grid-World and Atari, show that the representations learned by IAEM preserve the invariance of action effects. \n Moreover, with the invariant action effect, IAEM can accelerate the learning process by 1.6x, rapidly generalize to new environments by fine-tuning on a few components, and outperform other dynamics-based representation methods by 1.4x in limited steps.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1999326708",
                        "name": "Zhengbang Zhu"
                    },
                    {
                        "authorId": "2119326931",
                        "name": "Shengyi Jiang"
                    },
                    {
                        "authorId": "40685903",
                        "name": "Yu-Ren Liu"
                    },
                    {
                        "authorId": "144705629",
                        "name": "Yang Yu"
                    },
                    {
                        "authorId": "2119017332",
                        "name": "Kun Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a5f22a3377830e8040287726b0bf5656d26efdb3",
                "externalIds": {
                    "DBLP": "conf/icml/YuanL22",
                    "ArXiv": "2206.10442",
                    "DOI": "10.48550/arXiv.2206.10442",
                    "CorpusId": 249889776
                },
                "corpusId": 249889776,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a5f22a3377830e8040287726b0bf5656d26efdb3",
                "title": "Robust Task Representations for Offline Meta-Reinforcement Learning via Contrastive Learning",
                "abstract": "We study offline meta-reinforcement learning, a practical reinforcement learning paradigm that learns from offline data to adapt to new tasks. The distribution of offline data is determined jointly by the behavior policy and the task. Existing offline meta-reinforcement learning algorithms cannot distinguish these factors, making task representations unstable to the change of behavior policies. To address this problem, we propose a contrastive learning framework for task representations that are robust to the distribution mismatch of behavior policies in training and test. We design a bi-level encoder structure, use mutual information maximization to formalize task representation learning, derive a contrastive learning objective, and introduce several approaches to approximate the true distribution of negative pairs. Experiments on a variety of offline meta-reinforcement learning benchmarks demonstrate the advantages of our method over prior methods, especially on the generalization to out-of-distribution behavior policies. The code is available at https://github.com/PKU-AI-Edge/CORRO.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1429192914",
                        "name": "Haoqi Yuan"
                    },
                    {
                        "authorId": "2265693",
                        "name": "Zongqing Lu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "3 While most methods generate the \u201cpositive\u201d examples via data augmentation, some methods generate similar examples using different camera viewpoints of the same scene [109, 121], or by sampling examples that occur close in time within time series data [4, 95, 109, 117]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4247b93593b6ecd70262a1b1c7021dcecc26c8e0",
                "externalIds": {
                    "ArXiv": "2206.07568",
                    "DBLP": "journals/corr/abs-2206-07568",
                    "DOI": "10.48550/arXiv.2206.07568",
                    "CorpusId": 249674522
                },
                "corpusId": 249674522,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4247b93593b6ecd70262a1b1c7021dcecc26c8e0",
                "title": "Contrastive Learning as Goal-Conditioned Reinforcement Learning",
                "abstract": "In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often finds that learning representations in an end-to-end fashion is unstable and instead equip RL algorithms with additional representation learning parts (e.g., auxiliary losses, data augmentation). How can we design RL algorithms that directly acquire good representations? In this paper, instead of adding representation learning parts to an existing RL algorithm, we show (contrastive) representation learning methods can be cast as RL algorithms in their own right. To do this, we build upon prior work and apply contrastive representation learning to action-labeled trajectories, in such a way that the (inner product of) learned representations exactly corresponds to a goal-conditioned value function. We use this idea to reinterpret a prior RL method as performing contrastive learning, and then use the idea to propose a much simpler method that achieves similar performance. Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL methods achieve higher success rates than prior non-contrastive methods, including in the offline RL setting. We also show that contrastive RL outperforms prior methods on image-based tasks, without using data augmentation or auxiliary objectives.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8140754",
                        "name": "Benjamin Eysenbach"
                    },
                    {
                        "authorId": "1993655237",
                        "name": "Tianjun Zhang"
                    },
                    {
                        "authorId": "145124475",
                        "name": "R. Salakhutdinov"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Even when the state representation is trained parallel with the agent as in [27] or when unsupervised learning is used as in [1,30], the training is still time-consuming since multiple convolutional networks are trained."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7e38f22d7757486f1d77877633a36c1a6be3aed1",
                "externalIds": {
                    "ArXiv": "2206.06712",
                    "DBLP": "journals/corr/abs-2206-06712",
                    "DOI": "10.1007/978-3-031-09282-4_27",
                    "CorpusId": 249318493
                },
                "corpusId": 249318493,
                "publicationVenue": {
                    "id": "4e5ef04d-ccfb-4243-ae0f-3aa5293707c2",
                    "name": "International Conferences on Pattern Recognition and Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "ICPRAI",
                        "Int Conf Pattern Recognit Artif Intell"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7e38f22d7757486f1d77877633a36c1a6be3aed1",
                "title": "Visual Radial Basis Q-Network",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2167780371",
                        "name": "Julien Hautot"
                    },
                    {
                        "authorId": "1735928",
                        "name": "C\u00e9line Teuli\u00e8re"
                    },
                    {
                        "authorId": "2193898",
                        "name": "Nourddine Azzaoui"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This evaluation approach, called linear probing [2], has been used to evaluate representations of Atari games.",
                "[2] proposed a self-supervised learning method which utilizes",
                "application tasks in games research\u2014including gameplaying, modeling user behavior, and content generation\u2014use image representations of games containing information about critical factors describing the current state of the game [2].",
                "This approach, however, has two core limitations: first, it requires time-distributed images as the method\u2019s loss function incorporates temporal difference between the game\u2019s images and, second, it presents results on basic Atari games that are restricted to simple and abstract 2D grid environments, which are not representative of most modern era games.",
                "As mentioned earlier, recent work in self-supervised representation learning considered 2D game environments such as Atari [2].",
                "To solidify these conclusions, further investigation would be required with other contrastive and non-contrastive SSL methods such as Barlow Twins [5], SimSiam [6], VICReg [7],\nand DINO [38], as well as time-distributed SSL approaches such as ST-DIM [2].",
                "More recently, Anand et al. [2] proposed a self-supervised learning method which utilizes\nthe spatial and temporal relations between frames of different Atari games to learn important visual features of the game\u2019s image.",
                "Following the principles of [2] the evaluation takes place by quantifying the capacity of a linear model to recover or predict the internal game state variables based on the derived SSL representations.",
                "Indicatively, a number of recent studies have effectively used ConvNets with reinforcement learning for playing Atari games [18]\u2013[20] by mapping the game\u2019s raw image to an action to be performed for maximizing the game\u2019s score.",
                "be required with other contrastive and non-contrastive SSL methods such as Barlow Twins [5], SimSiam [6], VICReg [7], and DINO [38], as well as time-distributed SSL approaches such as ST-DIM [2].",
                "The internal state of Atari games is described via discrete variables, and thus linear probing in [2] evaluated the capacity of a linear model to predict the class of the internal game\nstate variables.",
                "The internal state of Atari games is described via discrete variables, and thus linear probing in [2] evaluated the capacity of a linear model to predict the class of the internal game"
            ],
            "intents": [],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "25f7aebc226f07a618fd14316e77621af5622887",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-06490",
                    "ArXiv": "2206.06490",
                    "DOI": "10.1109/CoG51982.2022.9893648",
                    "CorpusId": 249642232
                },
                "corpusId": 249642232,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/25f7aebc226f07a618fd14316e77621af5622887",
                "title": "Learning Task-Independent Game State Representations from Unlabeled Images",
                "abstract": "Self-supervised learning (SSL) techniques have been widely used to learn compact and informative representations from high-dimensional complex data. In many computer vision tasks, such as image classification, such methods achieve state-of-the-art results that surpass supervised learning approaches. In this paper, we investigate whether SSL methods can be leveraged for the task of learning accurate state representations of games, and if so, to what extent. For this purpose, we collect game footage frames and corresponding sequences of games\u2019 internal state from three different 3D games: VizDoom, the CARLA racing simulator and the Google Research Football Environment. We train an image encoder with three widely used SSL algorithms using solely the raw frames, and then attempt to recover the internal state variables from the learned representations. Our results across all three games showcase significantly higher correlation between SSL representations and the game\u2019s internal state compared to pre-trained baseline models such as ImageNet. Such findings suggest that SSL-based visual encoders can yield general\u2014not tailored to a specific task\u2014yet informative game representations solely from game pixel information. Such representations can, in turn, form the basis for boosting the performance of downstream learning tasks in games, including gameplaying, content generation and player modeling.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "37498103",
                        "name": "C. Trivedi"
                    },
                    {
                        "authorId": "2091878621",
                        "name": "Konstantinos Makantasis"
                    },
                    {
                        "authorId": "1713331",
                        "name": "Antonios Liapis"
                    },
                    {
                        "authorId": "1686193",
                        "name": "Georgios N. Yannakakis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this paper, we use a Contrastive Predictive Coding (CPC) algorithm (Oord et al., 2018) which was shown useful for finding predictive latent variables (Anand et al., 2019; Henaff, 2020; Yan et al., 2020).",
                "\u2026the CPC algorithm, (Oord et al., 2018) is used for finding predictive latent representations, which is useful for data-efficient image recognition (Henaff, 2020) and learning world-models\nthat supports robotic object manipulation (Yan et al., 2020) and playing Atari games (Anand et al., 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1100c30448df1d27a68b72a542d0a08c54f22cba",
                "externalIds": {
                    "ArXiv": "2205.01924",
                    "DBLP": "journals/corr/abs-2205-01924",
                    "DOI": "10.48550/arXiv.2205.01924",
                    "CorpusId": 248512876
                },
                "corpusId": 248512876,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1100c30448df1d27a68b72a542d0a08c54f22cba",
                "title": "Zero-Episode Few-Shot Contrastive Predictive Coding: Solving intelligence tests without prior training",
                "abstract": "Video prediction models often combine three components: an encoder from pixel space to a small latent space, a latent space prediction model, and a generative model back to pixel space. However, the large and unpredictable pixel space makes training such models difficult, requiring many training examples. We argue that finding a predictive latent variable and using it to evaluate the consistency of a future image enables data-efficient predictions because it precludes the necessity of a generative model training. To demonstrate it, we created sequence completion intelligence tests in which the task is to identify a predictably changing feature in a sequence of images and use this prediction to select the subsequent image. We show that a one-dimensional Markov Contrastive Predictive Coding (M-CPC_1D) model solves these tests efficiently, with only five examples. Finally, we demonstrate the usefulness of M-CPC_1D in solving two tasks without prior training: anomaly detection and stochastic movement video prediction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2066597096",
                        "name": "T. Barak"
                    },
                    {
                        "authorId": "2934154",
                        "name": "Y. Loewenstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u20262010; Lange et al., 2012; Yarats et al., 2021b), contrastive objectives (van den Oord et al., 2018; Laskin et al., 2020a), data augmentation (Yarats et al., 2021a), and mutual information maximization (Anand et al., 2019; Choi et al., 2021) to learn representations for downstream control."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9d836fc1c8b4eb9e3dc071c0f7cb4f169319520b",
                "externalIds": {
                    "DBLP": "conf/icml/Hansen-Estruch022",
                    "ArXiv": "2204.13060",
                    "DOI": "10.48550/arXiv.2204.13060",
                    "CorpusId": 248405793
                },
                "corpusId": 248405793,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9d836fc1c8b4eb9e3dc071c0f7cb4f169319520b",
                "title": "Bisimulation Makes Analogies in Goal-Conditioned Reinforcement Learning",
                "abstract": "Building generalizable goal-conditioned agents from rich observations is a key to reinforcement learning (RL) solving real world problems. Traditionally in goal-conditioned RL, an agent is provided with the exact goal they intend to reach. However, it is often not realistic to know the configuration of the goal before performing a task. A more scalable framework would allow us to provide the agent with an example of an analogous task, and have the agent then infer what the goal should be for its current state. We propose a new form of state abstraction called goal-conditioned bisimulation that captures functional equivariance, allowing for the reuse of skills to achieve new goals. We learn this representation using a metric form of this abstraction, and show its ability to generalize to new goals in simulation manipulation tasks. Further, we prove that this learned representation is sufficient not only for goal conditioned tasks, but is amenable to any downstream task described by a state-only reward function. Videos can be found at https://sites.google.com/view/gc-bisimulation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2163582335",
                        "name": "Philippe Hansen-Estruch"
                    },
                    {
                        "authorId": "2111672235",
                        "name": "Amy Zhang"
                    },
                    {
                        "authorId": "3422774",
                        "name": "Ashvin Nair"
                    },
                    {
                        "authorId": "2163582683",
                        "name": "Patrick Yin"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "CPC [32], CPC|Action [17] and ST-Dim [3] propose different variants of temporal contrastive losses, however these methods add complexity and require a sequence of images for training."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "68475c9ff2bb6ae0948450c252f049e7659eacd7",
                "externalIds": {
                    "ArXiv": "2204.13226",
                    "DBLP": "journals/corr/abs-2204-13226",
                    "DOI": "10.48550/arXiv.2204.13226",
                    "CorpusId": 248426942
                },
                "corpusId": 248426942,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/68475c9ff2bb6ae0948450c252f049e7659eacd7",
                "title": "Offline Visual Representation Learning for Embodied Navigation",
                "abstract": "How should we learn visual representations for embodied agents that must see and move? The status quo is tabula rasa in vivo, i.e. learning visual representations from scratch while also learning to move, potentially augmented with auxiliary tasks (e.g. predicting the action taken between two successive observations). In this paper, we show that an alternative 2-stage strategy is far more effective: (1) offline pretraining of visual representations with self-supervised learning (SSL) using large-scale pre-rendered images of indoor environments (Omnidata), and (2) online finetuning of visuomotor representations on specific tasks with image augmentations under long learning schedules. We call this method Offline Visual Representation Learning (OVRL). We conduct large-scale experiments - on 3 different 3D datasets (Gibson, HM3D, MP3D), 2 tasks (ImageNav, ObjectNav), and 2 policy learning algorithms (RL, IL) - and find that the OVRL representations lead to significant across-the-board improvements in state of art, on ImageNav from 29.2% to 54.2% (+25% absolute, 86% relative) and on ObjectNav from 18.1% to 23.2% (+5.1% absolute, 28% relative). Importantly, both results were achieved by the same visual encoder generalizing to datasets that were not seen during pretraining. While the benefits of pretraining sometimes diminish (or entirely disappear) with long finetuning schedules, we find that OVRL's performance gains continue to increase (not decrease) as the agent is trained for 2 billion frames of experience.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1838683872",
                        "name": "Karmesh Yadav"
                    },
                    {
                        "authorId": "80155427",
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "authorId": "2905057",
                        "name": "Arjun Majumdar"
                    },
                    {
                        "authorId": "51266557",
                        "name": "Vincent-Pierre Berges"
                    },
                    {
                        "authorId": "108075584",
                        "name": "Sachit Kuhar"
                    },
                    {
                        "authorId": "1746610",
                        "name": "Dhruv Batra"
                    },
                    {
                        "authorId": "14667698",
                        "name": "Alexei Baevski"
                    },
                    {
                        "authorId": "90536527",
                        "name": "Oleksandr Maksymets"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Minimizing the contrastive loss is known to be less computationally costly and can produce good representations from high-dimensional inputs (Oord et al., 2018; Anand et al., 2019; Chen et al., 2020; Laskin et al., 2020; van der Pol et al., 2020a), thus we use it here."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9efa93e2228266fb1a9faa1b55270813f3ad6c5c",
                "externalIds": {
                    "DBLP": "conf/icml/ParkBZMW22",
                    "ArXiv": "2204.11371",
                    "DOI": "10.48550/arXiv.2204.11371",
                    "CorpusId": 248377666
                },
                "corpusId": 248377666,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9efa93e2228266fb1a9faa1b55270813f3ad6c5c",
                "title": "Learning Symmetric Embeddings for Equivariant World Models",
                "abstract": "Incorporating symmetries can lead to highly data-ef\ufb01cient and generalizable models by de\ufb01ning equivalence classes of data samples related by transformations. However, characterizing how transformations act on input data is often dif\ufb01cult, limiting the applicability of equivariant models. We propose learning symmetric embedding networks (SENs) that encode an input space (e.g. images), where we do not know the effect of transformations (e.g. rotations), to a feature space that transforms in a known manner under these oper-ations. This network can be trained end-to-end with an equivariant task network to learn an explicitly symmetric representation. We validate this approach in the context of equivariant transition models with 3 distinct forms of symmetry. Our experiments demonstrate that SENs facilitate the application of equivariant networks to data with complex symmetry representations. Moreover, doing so can yield improvements in accuracy and generalization relative to both fully-equivariant and non-equivariant baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118946132",
                        "name": "Jung Yeon Park"
                    },
                    {
                        "authorId": "52228551",
                        "name": "Ondrej Biza"
                    },
                    {
                        "authorId": "2111640814",
                        "name": "Linfeng Zhao"
                    },
                    {
                        "authorId": "2086966519",
                        "name": "J.-W. van de Meent"
                    },
                    {
                        "authorId": "153401894",
                        "name": "R. Walters"
                    }
                ]
            }
        },
        {
            "contexts": [
                "which no additional annotation is needed for the representation learning, the network is trained to produce consistent embeddings for semantically similar input (positives), contrasting them with other, more distant samples (negatives) [6], [8]\u2013 [10].",
                "Following CPC concepts, ST-DIM [8] used both patch structure and temporal positions of observations, aligning nearby frames."
            ],
            "intents": [
                "methodology",
                "result"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4212a0421fce78b6a028497a788bc86d84e5b468",
                "externalIds": {
                    "ArXiv": "2204.03525",
                    "DBLP": "journals/corr/abs-2204-03525",
                    "DOI": "10.1109/ICPR56361.2022.9956553",
                    "CorpusId": 248006413
                },
                "corpusId": 248006413,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4212a0421fce78b6a028497a788bc86d84e5b468",
                "title": "Temporal Alignment for History Representation in Reinforcement Learning",
                "abstract": "Environments in Reinforcement Learning are usually only partially observable. To address this problem, a possible solution is to provide the agent with information about the past. However, providing complete observations of numerous steps can be excessive. Inspired by human memory, we propose to represent history with only important changes in the environment and, in our approach, to obtain automatically this representation using self-supervision. Our method (TempAl) aligns temporally-close frames, revealing a general, slowly varying state of the environment. This procedure is based on contrastive loss, which pulls embeddings of nearby observations to each other while pushing away other samples from the batch. It can be interpreted as a metric that captures the temporal relations of observations. We propose to combine both common instantaneous and our history representation and we evaluate TempAl on all available Atari games from the Arcade Learning Environment. TempAl surpasses the instantaneous-only baseline in 35 environments out of 49. The source code of the method and of all the experiments is available at https://github.com/htdt/tempal.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064701324",
                        "name": "Aleksandr Ermolov"
                    },
                    {
                        "authorId": "1716310",
                        "name": "E. Sangineto"
                    },
                    {
                        "authorId": "1703601",
                        "name": "N. Sebe"
                    }
                ]
            }
        },
        {
            "contexts": [
                "According to the different objectives of auxiliary tasks, existing auxiliary tasks in model-free RL frameworks can be roughly categorized into three types, reconstruction-based [7, 11], prediction-based [10, 15, 34] and contrastive-learning based [1, 18, 22].",
                "The main idea of most contrastive-learning based auxiliary tasks is to hope that the representation of anchor sample is closer to the representation of positive samples and farther from the representation of negative samples in the latent vector space [1, 2, 4, 22, 30]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0dbe051d5863e9f6f55e5be7e88ad9b0f9156bc3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-00888",
                    "ArXiv": "2204.00888",
                    "DOI": "10.1145/3511808.3557094",
                    "CorpusId": 247940026
                },
                "corpusId": 247940026,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0dbe051d5863e9f6f55e5be7e88ad9b0f9156bc3",
                "title": "Learning List-wise Representation in Reinforcement Learning for Ads Allocation with Multiple Auxiliary Tasks",
                "abstract": "With the recent prevalence of reinforcement learning (RL), there have been tremendous interests in utilizing RL for ads allocation in recommendation platforms (e.g., e-commerce and news feed sites). To achieve better allocation, the input of recent RL-based ads allocation methods is upgraded from point-wise single item to list-wise item arrangement. However, this also results in a high-dimensional space of state-action pairs, making it difficult to learn list-wise representations with good generalization ability. This further hinders the exploration of RL agents and causes poor sample efficiency. To address this problem, we propose a novel RL-based approach for ads allocation which learns better list-wise representations by leveraging task-specific signals on Meituan food delivery platform. Specifically, we propose three different auxiliary tasks based on reconstruction, prediction, and contrastive learning respectively according to prior domain knowledge on ads allocation. We conduct extensive experiments on Meituan food delivery platform to evaluate the effectiveness of the proposed auxiliary tasks. Both offline and online experimental results show that the proposed method can learn better list-wise representations and achieve higher revenue for the platform compared to the state-of-the-art baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2161243609",
                        "name": "Guogang Liao"
                    },
                    {
                        "authorId": "2108725730",
                        "name": "Zehua Wang"
                    },
                    {
                        "authorId": "2119205048",
                        "name": "Xiaowen Shi"
                    },
                    {
                        "authorId": "2107938658",
                        "name": "Xiaoxu Wu"
                    },
                    {
                        "authorId": "144114271",
                        "name": "Chuheng Zhang"
                    },
                    {
                        "authorId": "2108836630",
                        "name": "Yongkang Wang"
                    },
                    {
                        "authorId": "2144802333",
                        "name": "Xingxing Wang"
                    },
                    {
                        "authorId": "2152689873",
                        "name": "Dong Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026al., 2019; 2021), reconstruction (Yarats et al., 2021c), future representation prediction (Gelada et al., 2019; Schwarzer et al., 2021a), bisimulation (Castro, 2020; Zhang et al., 2021), and contrastive learning (Oord et al., 2018; Anand et al., 2019; Mazoure et al., 2020; Srinivas et al., 2020).",
                ", 2021), and contrastive learning (Oord et al., 2018; Anand et al., 2019; Mazoure et al., 2020; Srinivas et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1c35807e1a4c24e2013fa0a090cee9cc4716a5f5",
                "externalIds": {
                    "DBLP": "conf/icml/SeoLJA22",
                    "ArXiv": "2203.13880",
                    "DOI": "10.48550/arXiv.2203.13880",
                    "CorpusId": 247762941
                },
                "corpusId": 247762941,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1c35807e1a4c24e2013fa0a090cee9cc4716a5f5",
                "title": "Reinforcement Learning with Action-Free Pre-Training from Videos",
                "abstract": "Recent unsupervised pre-training methods have shown to be effective on language and vision domains by learning useful representations for multiple downstream tasks. In this paper, we investigate if such unsupervised pre-training methods can also be effective for vision-based reinforcement learning (RL). To this end, we introduce a framework that learns representations useful for understanding the dynamics via generative pre-training on videos. Our framework consists of two phases: we pre-train an action-free latent video prediction model, and then utilize the pre-trained representations for efficiently learning action-conditional world models on unseen environments. To incorporate additional action inputs during fine-tuning, we introduce a new architecture that stacks an action-conditional latent prediction model on top of the pre-trained action-free prediction model. Moreover, for better exploration, we propose a video-based intrinsic bonus that leverages pre-trained representations. We demonstrate that our framework significantly improves both final performances and sample-efficiency of vision-based RL in a variety of manipulation and locomotion tasks. Code is available at https://github.com/younggyoseo/apv.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2067714176",
                        "name": "Younggyo Seo"
                    },
                    {
                        "authorId": "3436470",
                        "name": "Kimin Lee"
                    },
                    {
                        "authorId": "2055291154",
                        "name": "Stephen James"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, despite the loss functions operating directly on the agent\u2019s representation, the representations themselves are rarely evaluated beyond evaluating the returns, with some exceptions (Anand et al., 2019; Zhang et al., 2020; Mazoure et al., 2021; Lyle et al., 2021).",
                "Usable annotation is completely absent from the Atari benchmark (Bellemare et al., 2013), though wrappers have been created to aid in representation learning research (e.g., Anand et al., 2019), which has meaning only within a single task \u201cgame\u201d."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "29186936422c199599fd0a24e6f209376775fce9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-10351",
                    "ArXiv": "2203.10351",
                    "DOI": "10.48550/arXiv.2203.10351",
                    "CorpusId": 247593990
                },
                "corpusId": 247593990,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/29186936422c199599fd0a24e6f209376775fce9",
                "title": "The Sandbox Environment for Generalizable Agent Research (SEGAR)",
                "abstract": "A broad challenge of research on generalization for sequential decision-making tasks in interactive environments is designing benchmarks that clearly landmark progress. While there has been notable headway, current benchmarks either do not provide suitable exposure nor intuitive control of the underlying factors, are not easy-to-implement, customizable, or extensible, or are computationally expensive to run. We built the Sandbox Environment for Generalizable Agent Research (SEGAR) with all of these things in mind. SEGAR improves the ease and accountability of generalization research in RL, as generalization objectives can be easy designed by specifying task distributions, which in turns allows the researcher to measure the nature of the generalization objective. We present an overview of SEGAR and how it contributes to these goals, as well as experiments that demonstrate a few types of research questions SEGAR can help answer.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40482726",
                        "name": "R. Devon Hjelm"
                    },
                    {
                        "authorId": "18111246",
                        "name": "Bogdan Mazoure"
                    },
                    {
                        "authorId": "2970150",
                        "name": "Florian Golemo"
                    },
                    {
                        "authorId": "1844283112",
                        "name": "F. Frujeri"
                    },
                    {
                        "authorId": "1958810",
                        "name": "M. Jalobeanu"
                    },
                    {
                        "authorId": "6247481",
                        "name": "A. Kolobov"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ceab9f894390f06986ba6969f38e0adb09b390fa",
                "externalIds": {
                    "PubMedCentral": "8932622",
                    "DOI": "10.1371/journal.pone.0265456",
                    "CorpusId": 247547327,
                    "PubMed": "35303031"
                },
                "corpusId": 247547327,
                "publicationVenue": {
                    "id": "0aed7a40-85f3-4c66-9e1b-c1556c57001b",
                    "name": "PLoS ONE",
                    "type": "journal",
                    "alternate_names": [
                        "Plo ONE",
                        "PLOS ONE",
                        "PLO ONE"
                    ],
                    "issn": "1932-6203",
                    "url": "https://journals.plos.org/plosone/",
                    "alternate_urls": [
                        "http://www.plosone.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ceab9f894390f06986ba6969f38e0adb09b390fa",
                "title": "Action-driven contrastive representation for reinforcement learning",
                "abstract": "In reinforcement learning, reward-driven feature learning directly from high-dimensional images faces two challenges: sample-efficiency for solving control tasks and generalization to unseen observations. In prior works, these issues have been addressed through learning representation from pixel inputs. However, their representation faced the limitations of being vulnerable to the high diversity inherent in environments or not taking the characteristics for solving control tasks. To attenuate these phenomena, we propose the novel contrastive representation method, Action-Driven Auxiliary Task (ADAT), which forces a representation to concentrate on essential features for deciding actions and ignore control-irrelevant details. In the augmented state-action dictionary of ADAT, the agent learns representation to maximize agreement between observations sharing the same actions. The proposed method significantly outperforms model-free and model-based algorithms in the Atari and OpenAI ProcGen, widely used benchmarks for sample-efficiency and generalization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2250540",
                        "name": "Minbeom Kim"
                    },
                    {
                        "authorId": "153199184",
                        "name": "Kyeongha Rho"
                    },
                    {
                        "authorId": "2129531153",
                        "name": "Yong-Duk Kim"
                    },
                    {
                        "authorId": "1731707",
                        "name": "Kyomin Jung"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Zhang et al., 2016; Anand et al., 2019).",
                "sigmoid (max{Wsi + b}), to predict VQA-style questions in the form \u201cis there a (size, color, material, shape) object in the image?\u201d (Zhang et al., 2016; Anand et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9aa5a492c9188bcf9441af36fed69b3be74ac1b3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-05997",
                    "ArXiv": "2203.05997",
                    "DOI": "10.48550/arXiv.2203.05997",
                    "CorpusId": 247410975
                },
                "corpusId": 247410975,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9aa5a492c9188bcf9441af36fed69b3be74ac1b3",
                "title": "Towards Self-Supervised Learning of Global and Object-Centric Representations",
                "abstract": "Self-supervision allows learning meaningful representations of natural images, which usually contain one central object. How well does it transfer to multi-entity scenes? We discuss key aspects of learning structured object-centric representations with self-supervision and validate our insights through several experiments on the CLEVR dataset. Regarding the architecture, we confirm the importance of competition for attention-based object discovery, where each image patch is exclusively attended by one object. For training, we show that contrastive losses equipped with matching can be applied directly in a latent space, avoiding pixel-based reconstruction. However, such an optimization objective is sensitive to false negatives (recurring objects) and false positives (matching errors). Careful consideration is thus required around data augmentation and negative sample selection.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "38107189",
                        "name": "Federico Baldassarre"
                    },
                    {
                        "authorId": "2622491",
                        "name": "Hossein Azizpour"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Contrastive learning (Oord et al., 2018; He et al., 2020) is always used as an unsupervised learning approach to extract useful representations from high-dimensional data, which has been applied successfully to image recognition (Henaff, 2020) and RL (Anand et al., 2019; Laskin et al., 2020).",
                ", 2020) is always used as an unsupervised learning approach to extract useful representations from high-dimensional data, which has been applied successfully to image recognition (Henaff, 2020) and RL (Anand et al., 2019; Laskin et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1e8ab4b5af3c69845183133d499e86baf56e819d",
                "externalIds": {
                    "DBLP": "journals/finr/CongLRSGH022",
                    "PubMedCentral": "8926160",
                    "DOI": "10.3389/fnbot.2022.829437",
                    "CorpusId": 247171268,
                    "PubMed": "35308311"
                },
                "corpusId": 247171268,
                "publicationVenue": {
                    "id": "de454aec-8c73-4737-bb1f-5231453ca8fa",
                    "name": "Frontiers in Neurorobotics",
                    "type": "journal",
                    "alternate_names": [
                        "Front Neurorobotics"
                    ],
                    "issn": "1662-5218",
                    "url": "https://www.frontiersin.org/journals/neurorobotics#articles",
                    "alternate_urls": [
                        "http://www.frontiersin.org/neurorobotics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1e8ab4b5af3c69845183133d499e86baf56e819d",
                "title": "Reinforcement Learning With Vision-Proprioception Model for Robot Planar Pushing",
                "abstract": "We propose a vision-proprioception model for planar object pushing, efficiently integrating all necessary information from the environment. A Variational Autoencoder (VAE) is used to extract compact representations from the task-relevant part of the image. With the real-time robot state obtained easily from the hardware system, we fuse the latent representations from the VAE and the robot end-effector position together as the state of a Markov Decision Process. We use Soft Actor-Critic to train the robot to push different objects from random initial poses to target positions in simulation. Hindsight Experience replay is applied during the training process to improve the sample efficiency. Experiments demonstrate that our algorithm achieves a pushing performance superior to a state-based baseline model that cannot be generalized to a different object and outperforms state-of-the-art policies which operate on raw image observations. At last, we verify that our trained model has a good generalization ability to unseen objects in the real world.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2069041374",
                        "name": "Lin Cong"
                    },
                    {
                        "authorId": "51436910",
                        "name": "Hongzhuo Liang"
                    },
                    {
                        "authorId": "51311032",
                        "name": "Philipp Ruppel"
                    },
                    {
                        "authorId": "66051129",
                        "name": "Yunlei Shi"
                    },
                    {
                        "authorId": "51445013",
                        "name": "M. G\u00f6rner"
                    },
                    {
                        "authorId": "1730476",
                        "name": "N. Hendrich"
                    },
                    {
                        "authorId": "2144167738",
                        "name": "Jianwei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1bb82660573bbaf01572041da16842ee2398ae39",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-00715",
                    "ArXiv": "2203.00715",
                    "DOI": "10.48550/arXiv.2203.00715",
                    "CorpusId": 247218338
                },
                "corpusId": 247218338,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1bb82660573bbaf01572041da16842ee2398ae39",
                "title": "Learning Robust Real-Time Cultural Transmission without Human Data",
                "abstract": "Figure 1 | Freeze-frames from a single episode of test-time evaluation, in chronological order from left to right. (a) Our cultural transmission agent (blue avatar) is spawned in a held-out task; (b) the agent finds a human (red avatar); (c) the agent follows the human on a rewarding path through goals while navigating terrain and obstacles; (d) the agent recalls and reproduces the demonstrated path after the human has dropped out. The numbers above the avatars indicate cumulative score across the episode. Videos of this agent are available on the website accompanying this paper.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7567594",
                        "name": "Avishkar Bhoopchand"
                    },
                    {
                        "authorId": "2156928199",
                        "name": "Bethanie Brownfield"
                    },
                    {
                        "authorId": "69041729",
                        "name": "Adrian Collister"
                    },
                    {
                        "authorId": "2152469362",
                        "name": "Agustin Dal Lago"
                    },
                    {
                        "authorId": "48779623",
                        "name": "Ashley D. Edwards"
                    },
                    {
                        "authorId": "145867385",
                        "name": "Richard Everett"
                    },
                    {
                        "authorId": "2156930381",
                        "name": "Alexandre Frechette"
                    },
                    {
                        "authorId": "121078430",
                        "name": "Y. Oliveira"
                    },
                    {
                        "authorId": "37591038",
                        "name": "Edward Hughes"
                    },
                    {
                        "authorId": "2034344309",
                        "name": "K. Mathewson"
                    },
                    {
                        "authorId": "2156929516",
                        "name": "Piermaria Mendolicchio"
                    },
                    {
                        "authorId": "2156928197",
                        "name": "Julia Pawar"
                    },
                    {
                        "authorId": "2007581998",
                        "name": "Miruna Pislar"
                    },
                    {
                        "authorId": "152710375",
                        "name": "A. Platonov"
                    },
                    {
                        "authorId": "2321181",
                        "name": "Evan Senter"
                    },
                    {
                        "authorId": "2192583463",
                        "name": "Sukhdeep Singh"
                    },
                    {
                        "authorId": "2156928090",
                        "name": "Alexander Zacherl"
                    },
                    {
                        "authorId": "2152836492",
                        "name": "Lei M. Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026Guo et al., 2020; Lee et al., 2020a;b; Schwarzer et al., 2021a; Yu et al., 2021) and contrastive learning of instance discrimination (Laskin et al., 2020b) or (spatial -) temporal discrimination (Oord\net al., 2018; Anand et al., 2019; Stooke et al., 2020; Zhu et al., 2020; Mazoure et al., 2020).",
                ", 2020b) or (spatial -) temporal discrimination (Oord et al., 2018; Anand et al., 2019; Stooke et al., 2020; Zhu et al., 2020; Mazoure et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "073ae688d98abc9b25120b039c70653a93f3ed68",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-12096",
                    "ArXiv": "2201.12096",
                    "CorpusId": 246411446
                },
                "corpusId": 246411446,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/073ae688d98abc9b25120b039c70653a93f3ed68",
                "title": "Mask-based Latent Reconstruction for Reinforcement Learning",
                "abstract": "For deep reinforcement learning (RL) from pixels, learning effective state representations is crucial for achieving high performance. However, in practice, limited experience and high-dimensional inputs prevent effective representation learning. To address this, motivated by the success of mask-based modeling in other research fields, we introduce mask-based reconstruction to promote state representation learning in RL. Specifically, we propose a simple yet effective self-supervised method, Mask-based Latent Reconstruction (MLR), to predict complete state representations in the latent space from the observations with spatially and temporally masked pixels. MLR enables better use of context information when learning state representations to make them more informative, which facilitates the training of RL agents. Extensive experiments show that our MLR significantly improves the sample efficiency in RL and outperforms the state-of-the-art sample-efficient RL methods on multiple continuous and discrete control benchmarks. Our code is available at https://github.com/microsoft/Mask-based-Latent-Reconstruction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152278376",
                        "name": "Tao Yu"
                    },
                    {
                        "authorId": "1486397342",
                        "name": "Zhizheng Zhang"
                    },
                    {
                        "authorId": "40093162",
                        "name": "Cuiling Lan"
                    },
                    {
                        "authorId": "31482866",
                        "name": "Zhibo Chen"
                    },
                    {
                        "authorId": "2146557916",
                        "name": "Yan Lu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c4cb493effafb05ebff7da3789ef092ea2e35c59",
                "externalIds": {
                    "DOI": "10.14704/web/v19i1/web19094",
                    "CorpusId": 246266586
                },
                "corpusId": 246266586,
                "publicationVenue": {
                    "id": "621ec958-6aa9-44cf-8c1b-ceb4e6932bfd",
                    "name": "Webology",
                    "issn": "1735-188X",
                    "url": "http://www.webology.itgo.com/",
                    "alternate_urls": [
                        "https://www.webology.org/",
                        "http://www.webology.ir/",
                        "http://www.webology.ir/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c4cb493effafb05ebff7da3789ef092ea2e35c59",
                "title": "Content Classification Tasks with Data Preprocessing Manifestations",
                "abstract": "Deep reinforcement learning has a major hurdle in terms of data efficiency. We solve this challenge by pretraining an encoder with unlabeled input, which is subsequently finetuned on a tiny quantity of task-specific input. We use a mixture of latent dynamics modelling and unsupervised goal-conditioned RL to encourage learning representations that capture various elements of the underlying MDP. Our approach significantly outperforms previous work combining offline representation pretraining with task-specific finetuning when limited to 100k steps of interaction on Atari games (equivalent to two hours of human experience) and compares favourably with other pretraining methods that require orders of magnitude more data. When paired with larger models and more diverse, task-aligned observational data, our methodology shows great promise, nearing human-level performance and data efficiency on Atari in the best-case scenario.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2128524824",
                        "name": "Mamoona Anam"
                    },
                    {
                        "authorId": "2151049194",
                        "name": "Dr. Kantilal P. Rane"
                    },
                    {
                        "authorId": "51070954",
                        "name": "Ali Alenezi"
                    },
                    {
                        "authorId": "2151174984",
                        "name": "Ruby Mishra"
                    },
                    {
                        "authorId": "2151048490",
                        "name": "Dr. Swaminathan Ramamurthy"
                    },
                    {
                        "authorId": "8627257",
                        "name": "Ferdin Joe John Joseph"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This idea is further explored in ATC [Stooke et al., 2021] and STDIM [Anand et al., 2019] where a temporal contrast is adopted instead.",
                ", 2021] and STDIM [Anand et al., 2019] where a temporal contrast is adopted instead."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "934474a378a268327e73eb3f73601b8eb1904714",
                "externalIds": {
                    "ArXiv": "2201.07016",
                    "DBLP": "journals/corr/abs-2201-07016",
                    "CorpusId": 246035501
                },
                "corpusId": 246035501,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/934474a378a268327e73eb3f73601b8eb1904714",
                "title": "Accelerating Representation Learning with View-Consistent Dynamics in Data-Efficient Reinforcement Learning",
                "abstract": "Learning informative representations from image-based observations is of fundamental concern in deep Reinforcement Learning (RL). However, data-inefficiency remains a significant barrier to this objective. To overcome this obstacle, we propose to accelerate state representation learning by enforcing view-consistency on the dynamics. Firstly, we introduce a formalism of Multi-view Markov Decision Process (MMDP) that incorporates multiple views of the state. Following the structure of MMDP, our method, View-Consistent Dynamics (VCD), learns state representations by training a view-consistent dynamics model in the latent space, where views are generated by applying data augmentation to states. Empirical evaluation on DeepMind Control Suite and Atari-100k demonstrates VCD to be the SoTA data-efficient algorithm on visual control tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110813546",
                        "name": "Tao Huang"
                    },
                    {
                        "authorId": "2154572465",
                        "name": "Jiacheng Wang"
                    },
                    {
                        "authorId": "46772408",
                        "name": "Xiao Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "07a07c6b9780efee9953b87526fa3400c07df49b",
                "externalIds": {
                    "DBLP": "journals/ijon/YouACP22",
                    "ArXiv": "2203.01810",
                    "DOI": "10.1016/j.neucom.2021.12.094",
                    "CorpusId": 245690620
                },
                "corpusId": 245690620,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/07a07c6b9780efee9953b87526fa3400c07df49b",
                "title": "Integrating contrastive learning with dynamic models for reinforcement learning from images",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "113024804",
                        "name": "Bang You"
                    },
                    {
                        "authorId": "7931507",
                        "name": "O. Arenz"
                    },
                    {
                        "authorId": "49070272",
                        "name": "Youping Chen"
                    },
                    {
                        "authorId": "145197867",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f20c2aee7b91ed3c72d059499a77f335faa45167",
                "externalIds": {
                    "ArXiv": "2112.15303",
                    "DBLP": "conf/aaai/Zang0W22",
                    "DOI": "10.1609/aaai.v36i8.20883",
                    "CorpusId": 245634865
                },
                "corpusId": 245634865,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f20c2aee7b91ed3c72d059499a77f335faa45167",
                "title": "SimSR: Simple Distance-based State Representation for Deep Reinforcement Learning",
                "abstract": "This work explores how to learn robust and generalizable state representation from image-based observations with deep reinforcement learning methods. Addressing the computational complexity, stringent assumptions and representation collapse challenges in existing work of bisimulation metric, we devise Simple State Representation (SimSR) operator. SimSR enables us to design a stochastic approximation method that can practically learn the mapping functions (encoders) from observations to latent representation space. In addition to the theoretical analysis and comparison with the existing work, we experimented and compared our work with recent state-of-the-art solutions in visual MuJoCo tasks. The results shows that our model generally achieves better performance and has better robustness and good generalization.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "11262459",
                        "name": "Hongyu Zang"
                    },
                    {
                        "authorId": "38645830",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "40091302",
                        "name": "Mingzhong Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "When unsupervisedly training a low-level layer, at each time step, a random action was taken of a game (Anand et al., 2019), then minibatches of size 32 were randomly extracted from a buffer of 10(6) recent frames to train the network using eq."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "aa712c0c4325eaca03c9b9669ab97adab3b01683",
                "externalIds": {
                    "DOI": "10.1101/2021.12.14.472260",
                    "CorpusId": 245273433
                },
                "corpusId": 245273433,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/aa712c0c4325eaca03c9b9669ab97adab3b01683",
                "title": "Evolutionary learning in the brain by heterosynaptic plasticity",
                "abstract": "How the brain modifies synapses to improve the performance of complicated networks remains one of the biggest mysteries in neuroscience. Canonical models suppose synaptic weights change according to pre- and post-synaptic activities (i.e., local plasticity rules), implementing gradient-descent algorithms. However, the lack of experimental evidence to confirm these models suggests that there may be important ingredients neglected by these models. For example, heterosynaptic plasticity, non-local rules mediated by inter-cellular signaling pathways, and the biological implementation of evolutionary algorithms (EA), another machine-learning paradigm that successfully trains large-scale neural networks, are seldom explored. Here we propose and systematically investigate an EA model of brain learning with non-local rules alone. Specifically, a population of agents are represented by different information routes in the brain, whose task performances are evaluated through gating on individual routes alternatively. The selection and reproduction of agents are realized by dopamine-guided heterosynaptic plasticity. Our EA model provides a framework to re-interpret the biological functions of dopamine, meta-plasticity of dendritic spines, memory replay, and the cooperative plasticity between the synapses within a dendritic neighborhood from a new and coherent aspect. Neural networks trained with the model exhibit analogous dynamics to the brain in cognitive tasks. Our EA model manifests broad competence to train spiking or analog neural networks with recurrent or feedforward architecture. Our EA model also demonstrates its powerful capability to train deep networks with biologically plausible binary weights in MNIST classification and Atari-game playing tasks with performance comparable with continuous-weight networks trained by gradient-based methods. Overall, our work leads to a fresh understanding of the brain learning mechanism unexplored by local rules and gradient-based algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2414955",
                        "name": "Zedong Bi"
                    },
                    {
                        "authorId": "2155230058",
                        "name": "Guozhang Chen"
                    },
                    {
                        "authorId": "2152325544",
                        "name": "Dongping Yang"
                    },
                    {
                        "authorId": "2145107610",
                        "name": "Yu Zhou"
                    },
                    {
                        "authorId": "2167981457",
                        "name": "Liang Tian"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", computer vision [1, 2, 3], natural language [4, 5], code [6], reinforcement learning [7, 8, 9], and graphs [10]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "607d6e0573d9147a26c134d59fdb323890a8c2b5",
                "externalIds": {
                    "DBLP": "conf/nips/LeeLLLS21",
                    "ArXiv": "2111.09613",
                    "CorpusId": 244345894
                },
                "corpusId": 244345894,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/607d6e0573d9147a26c134d59fdb323890a8c2b5",
                "title": "Improving Transferability of Representations via Augmentation-Aware Self-Supervision",
                "abstract": "Recent unsupervised representation learning methods have shown to be effective in a range of vision tasks by learning representations invariant to data augmentations such as random cropping and color jittering. However, such invariance could be harmful to downstream tasks if they rely on the characteristics of the data augmentations, e.g., location- or color-sensitive. This is not an issue just for unsupervised learning; we found that this occurs even in supervised learning because it also learns to predict the same label for all augmented samples of an instance. To avoid such failures and obtain more generalizable representations, we suggest to optimize an auxiliary self-supervised loss, coined AugSelf, that learns the difference of augmentation parameters (e.g., cropping positions, color adjustment intensities) between two randomly augmented samples. Our intuition is that AugSelf encourages to preserve augmentation-aware information in learned representations, which could be beneficial for their transferability. Furthermore, AugSelf can easily be incorporated into recent state-of-the-art representation learning methods with a negligible additional training cost. Extensive experiments demonstrate that our simple idea consistently improves the transferability of representations learned by supervised and unsupervised methods in various transfer learning scenarios. The code is available at https://github.com/hankook/AugSelf.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51122345",
                        "name": "Hankook Lee"
                    },
                    {
                        "authorId": "2208511",
                        "name": "Kibok Lee"
                    },
                    {
                        "authorId": "3436470",
                        "name": "Kimin Lee"
                    },
                    {
                        "authorId": "2118338545",
                        "name": "Honglak Lee"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also experiment with a temporal contrastive objective which treats pairs of observations close in time as positive examples and un-correlated timestamps as negative examples [2, 52, 23]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44164c068499fbe387a1765104d69a8cbc5f0327",
                "externalIds": {
                    "ArXiv": "2111.01587",
                    "DBLP": "journals/corr/abs-2111-01587",
                    "CorpusId": 240419913
                },
                "corpusId": 240419913,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/44164c068499fbe387a1765104d69a8cbc5f0327",
                "title": "Procedural Generalization by Planning with Self-Supervised World Models",
                "abstract": "One of the key promises of model-based reinforcement learning is the ability to generalize using an internal model of the world to make predictions in novel environments and tasks. However, the generalization ability of model-based agents is not well understood because existing work has focused on model-free agents when benchmarking generalization. Here, we explicitly measure the generalization ability of model-based agents in comparison to their model-free counterparts. We focus our analysis on MuZero (Schrittwieser et al., 2020), a powerful model-based agent, and evaluate its performance on both procedural and task generalization. We identify three factors of procedural generalization -- planning, self-supervised representation learning, and procedural data diversity -- and show that by combining these techniques, we achieve state-of-the art generalization performance and data efficiency on Procgen (Cobbe et al., 2019). However, we find that these factors do not always provide the same benefits for the task generalization benchmarks in Meta-World (Yu et al., 2019), indicating that transfer remains a challenge and may require different approaches than procedural generalization. Overall, we suggest that building generalizable agents requires moving beyond the single-task, model-free paradigm and towards self-supervised model-based agents that are trained in rich, procedural, multi-task environments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "12679121",
                        "name": "Ankesh Anand"
                    },
                    {
                        "authorId": "1944655894",
                        "name": "Jacob Walker"
                    },
                    {
                        "authorId": "2144417088",
                        "name": "Yazhe Li"
                    },
                    {
                        "authorId": "2136446499",
                        "name": "Eszter V'ertes"
                    },
                    {
                        "authorId": "4337102",
                        "name": "Julian Schrittwieser"
                    },
                    {
                        "authorId": "1955694",
                        "name": "Sherjil Ozair"
                    },
                    {
                        "authorId": "143947744",
                        "name": "T. Weber"
                    },
                    {
                        "authorId": "2158860",
                        "name": "Jessica B. Hamrick"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al.",
                "(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al. (2017); Thomas et al. (2019); Mahmood et al. (2020); Li et al. (2018). In this study, we propose a novel pre-training algorithm involving GNNs which we call Multinetwork InfoMax (MIM).",
                "(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al. (2017); Thomas et al. (2019); Mahmood et al. (2020); Li et al. (2018). In this study, we propose a novel pre-training algorithm involving GNNs which we call Multinetwork InfoMax (MIM). MIM uses self-supervised learning with a mutual information objective to learn data representations. These representations are then used for downstream task of classification using small labeled dataset. We show that using pre-training we can bypass the need to acquire large labelled training datasets and achieve higher classification performance than non pre-trained counter parts. Prior work on self-supervised pre-training for GNNs Bojchevski and G\u00fcnnemann (2018); Davidson et al.",
                "(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al. (2017); Thomas et al. (2019); Mahmood et al. (2020); Li et al.",
                "(2019); Devlin et al. (2018); Lugosch et al. (2019) traditionally employ unsupervised/self-supervised pre-training Erhan et al.",
                "(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al. (2017); Thomas et al.",
                "(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018).",
                "(2019); Devlin et al. (2018); Lugosch et al. (2019) traditionally employ unsupervised/self-supervised pre-training Erhan et al. (2010). Furthermore, self-supervised methods with mutual information objective are able to preform competitively with supervised methods Oord et al.",
                "(2018); Bachman et al. (2019) and are suitable for a number of applications Anand et al.",
                "(2019) and are suitable for a number of applications Anand et al. (2019); Ravanelli and Bengio (2018). Pretraining and transfer learning have also been used for neuro/brain imaging Mensch et al. (2017); Thomas et al. (2019); Mahmood et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "bd863e2ebbfc1170419aba27e560f920e9fda01c",
                "externalIds": {
                    "ArXiv": "2111.01276",
                    "DBLP": "journals/corr/abs-2111-01276",
                    "CorpusId": 240420195
                },
                "corpusId": 240420195,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bd863e2ebbfc1170419aba27e560f920e9fda01c",
                "title": "Multi network InfoMax: A pre-training method involving graph convolutional networks",
                "abstract": "Discovering distinct features and their relations from data can help us uncover valuable knowledge crucial for various tasks, e.g., classification. In neuroimaging, these features could help to understand, classify, and possibly prevent brain disorders. Model introspection of highly performant overparameterized deep learning (DL) models could help find these features and relations. However, to achieve high-performance level DL models require numerous labeled training samples ($n$) rarely available in many fields. This paper presents a pre-training method involving graph convolutional/neural networks (GCNs/GNNs), based on maximizing mutual information between two high-level embeddings of an input sample. Many of the recently proposed pre-training methods pre-train one of many possible networks of an architecture. Since almost every DL model is an ensemble of multiple networks, we take our high-level embeddings from two different networks of a model --a convolutional and a graph network--. The learned high-level graph latent representations help increase performance for downstream graph classification tasks and bypass the need for a high number of labeled data samples. We apply our method to a neuroimaging dataset for classifying subjects into healthy control (HC) and schizophrenia (SZ) groups. Our experiments show that the pre-trained model significantly outperforms the non-pre-trained model and requires $50\\%$ less data for similar performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153339670",
                        "name": "Usman Mahmood"
                    },
                    {
                        "authorId": "2895825",
                        "name": "Z. Fu"
                    },
                    {
                        "authorId": "2133976600",
                        "name": "Vince D. Calhoun"
                    },
                    {
                        "authorId": "2122479",
                        "name": "S. Plis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous works in RL have constructed various auxiliary tasks to learn better representations, such as methods based on temporal structures (Aytar et al., 2018) and local spatial structures (Anand et al., 2019).",
                ", 2018) and local spatial structures (Anand et al., 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "75c6ee0ddad05b752252f57e8dc1ede18ac13f63",
                "externalIds": {
                    "DBLP": "conf/nips/ZhangCZXQL21",
                    "ArXiv": "2110.13578",
                    "CorpusId": 239885600
                },
                "corpusId": 239885600,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/75c6ee0ddad05b752252f57e8dc1ede18ac13f63",
                "title": "Distributional Reinforcement Learning for Multi-Dimensional Reward Functions",
                "abstract": "A growing trend for value-based reinforcement learning (RL) algorithms is to capture more information than scalar value functions in the value network. One of the most well-known methods in this branch is distributional RL, which models return distribution instead of scalar value. In another line of work, hybrid reward architectures (HRA) in RL have studied to model source-specific value functions for each source of reward, which is also shown to be beneficial in performance. To fully inherit the benefits of distributional RL and hybrid reward architectures, we introduce Multi-Dimensional Distributional DQN (MD3QN), which extends distributional RL to model the joint return distribution from multiple reward sources. As a by-product of joint distribution modeling, MD3QN can capture not only the randomness in returns for each source of reward, but also the rich reward correlation between the randomness of different sources. We prove the convergence for the joint distributional Bellman operator and build our empirical algorithm by minimizing the Maximum Mean Discrepancy between joint return distribution and its Bellman target. In experiments, our method accurately models the joint return distribution in environments with richly correlated reward functions, and outperforms previous RL methods utilizing multi-dimensional reward functions in the control setting.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1570021289",
                        "name": "Pushi Zhang"
                    },
                    {
                        "authorId": "2109384652",
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "authorId": "2004786514",
                        "name": "Li Zhao"
                    },
                    {
                        "authorId": "49602517",
                        "name": "Wei Xiong"
                    },
                    {
                        "authorId": "143826491",
                        "name": "Tao Qin"
                    },
                    {
                        "authorId": "2110264337",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Adopting the self-supervised objective [26, 39, 2, 34], most approaches in this field consider learning features [18, 41] of high-dimensional (eg, image-based) states in the environment, then (1) adopt the non-parametric measurement function to acquire rewards [23, 33, 42, 53, 44, 32] or (2) enable policy transfer [23, 16, 17, 13, 22] over the learned features."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f5aeaba6a0d4df824d752bceb583883c10d2ade9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-12997",
                    "ArXiv": "2110.12997",
                    "CorpusId": 239769290
                },
                "corpusId": 239769290,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f5aeaba6a0d4df824d752bceb583883c10d2ade9",
                "title": "Unsupervised Domain Adaptation with Dynamics-Aware Rewards in Reinforcement Learning",
                "abstract": "Unsupervised reinforcement learning aims to acquire skills without prior goal representations, where an agent automatically explores an open-ended environment to represent goals and learn the goal-conditioned policy. However, this procedure is often time-consuming, limiting the rollout in some potentially expensive target environments. The intuitive approach of training in another interaction-rich environment disrupts the reproducibility of trained skills in the target environment due to the dynamics shifts and thus inhibits direct transferring. Assuming free access to a source environment, we propose an unsupervised domain adaptation method to identify and acquire skills across dynamics. Particularly, we introduce a KL regularized objective to encourage emergence of skills, rewarding the agent for both discovering skills and aligning its behaviors respecting dynamics shifts. This suggests that both dynamics (source and target) shape the reward to facilitate the learning of adaptive skills. We also conduct empirical experiments to demonstrate that our method can effectively learn skills that can be smoothly deployed in target.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Jinxin Liu"
                    },
                    {
                        "authorId": "2110770857",
                        "name": "Hao Shen"
                    },
                    {
                        "authorId": "2111224425",
                        "name": "Donglin Wang"
                    },
                    {
                        "authorId": "152567783",
                        "name": "Yachen Kang"
                    },
                    {
                        "authorId": "1391496892",
                        "name": "Qiangxing Tian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4520b71140808785b86d775fb1d0d3ceb411511d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-04935",
                    "ArXiv": "2110.04935",
                    "CorpusId": 238583248
                },
                "corpusId": 238583248,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4520b71140808785b86d775fb1d0d3ceb411511d",
                "title": "Learning Temporally-Consistent Representations for Data-Efficient Reinforcement Learning",
                "abstract": "Deep reinforcement learning (RL) agents that exist in high-dimensional state spaces, such as those composed of images, have interconnected learning burdens. Agents must learn an action-selection policy that completes their given task, which requires them to learn a representation of the state space that discerns between useful and useless information. The reward function is the only supervised feedback that RL agents receive, which causes a representation learning bottleneck that can manifest in poor sample efficiency. We present $k$-Step Latent (KSL), a new representation learning method that enforces temporal consistency of representations via a self-supervised auxiliary task wherein agents learn to recurrently predict action-conditioned representations of the state space. The state encoder learned by KSL produces low-dimensional representations that make optimization of the RL task more sample efficient. Altogether, KSL produces state-of-the-art results in both data efficiency and asymptotic performance in the popular PlaNet benchmark suite. Our analyses show that KSL produces encoders that generalize better to new tasks unseen during training, and its representations are more strongly tied to reward, are more invariant to perturbations in the state space, and move more smoothly through the temporal axis of the RL problem than other methods such as DrQ, RAD, CURL, and SAC-AE.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1914794749",
                        "name": "Trevor A. McInroe"
                    },
                    {
                        "authorId": "144824410",
                        "name": "Lukas Sch\u00e4fer"
                    },
                    {
                        "authorId": "1961238",
                        "name": "Stefano V. Albrecht"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7ad3fff81212eb345b8aced69cbb159b1e13b126",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-05442",
                    "ArXiv": "2110.05442",
                    "CorpusId": 238583494
                },
                "corpusId": 238583494,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7ad3fff81212eb345b8aced69cbb159b1e13b126",
                "title": "Neural Algorithmic Reasoners are Implicit Planners",
                "abstract": "Implicit planning has emerged as an elegant technique for combining learned models of the world with end-to-end model-free reinforcement learning. We study the class of implicit planners inspired by value iteration, an algorithm that is guaranteed to yield perfect policies in fully-specified tabular environments. We find that prior approaches either assume that the environment is provided in such a tabular form -- which is highly restrictive -- or infer\"local neighbourhoods\"of states to run value iteration over -- for which we discover an algorithmic bottleneck effect. This effect is caused by explicitly running the planning algorithm based on scalar predictions in every state, which can be harmful to data efficiency if such scalars are improperly predicted. We propose eXecuted Latent Value Iteration Networks (XLVINs), which alleviate the above limitations. Our method performs all planning computations in a high-dimensional latent space, breaking the algorithmic bottleneck. It maintains alignment with value iteration by carefully leveraging neural graph-algorithmic reasoning and contrastive self-supervised learning. Across eight low-data settings -- including classical control, navigation and Atari -- XLVINs provide significant improvements to data efficiency against value iteration-based implicit planners, as well as relevant model-free baselines. Lastly, we empirically verify that XLVINs can closely align with value iteration.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48860334",
                        "name": "Andreea Deac"
                    },
                    {
                        "authorId": "1742197495",
                        "name": "Petar Velivckovi'c"
                    },
                    {
                        "authorId": "2003637930",
                        "name": "Ognjen Milinkovi'c"
                    },
                    {
                        "authorId": "145180695",
                        "name": "Pierre-Luc Bacon"
                    },
                    {
                        "authorId": "152226504",
                        "name": "Jian Tang"
                    },
                    {
                        "authorId": "145888108",
                        "name": "Mladen Nikolic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Self-supervised learning of visual representations from unlabeled images is a fundamental task of machine learning, which establishes various applications including object recognition [1, 2], reinforcement learning [3, 4], out-of-distribution detection [5, 6], and multimodal learning [7, 8]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "37de04f1055d97acdec3d3710a8db219ba8e0273",
                "externalIds": {
                    "ArXiv": "2108.00049",
                    "DBLP": "journals/corr/abs-2108-00049",
                    "CorpusId": 236772573
                },
                "corpusId": 236772573,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/37de04f1055d97acdec3d3710a8db219ba8e0273",
                "title": "Object-aware Contrastive Learning for Debiased Scene Representation",
                "abstract": "Contrastive self-supervised learning has shown impressive results in learning visual representations from unlabeled images by enforcing invariance against different data augmentations. However, the learned representations are often contextually biased to the spurious scene correlations of different objects or object and background, which may harm their generalization on the downstream tasks. To tackle the issue, we develop a novel object-aware contrastive learning framework that first (a) localizes objects in a self-supervised manner and then (b) debias scene correlations via appropriate data augmentations considering the inferred object locations. For (a), we propose the contrastive class activation map (ContraCAM), which finds the most discriminative regions (e.g., objects) in the image compared to the other images using the contrastively trained models. We further improve the ContraCAM to detect multiple objects and entire shapes via an iterative refinement procedure. For (b), we introduce two data augmentations based on ContraCAM, object-aware random crop and background mixup, which reduce contextual and background biases during contrastive self-supervised learning, respectively. Our experiments demonstrate the effectiveness of our representation learning framework, particularly when trained under multi-object images or evaluated under the background (and distribution) shifted images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9962692",
                        "name": "Sangwoo Mo"
                    },
                    {
                        "authorId": "2115457459",
                        "name": "H. Kang"
                    },
                    {
                        "authorId": "1729571",
                        "name": "Kihyuk Sohn"
                    },
                    {
                        "authorId": "2116729195",
                        "name": "Chun-Liang Li"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This information is exposed in the RAM state of the Atari emulator, as shown in Anand et al. (2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3c91a4534d51d21ae67e4a9f9287bb2a14dc5e3b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-14226",
                    "ArXiv": "2107.14226",
                    "CorpusId": 236493649
                },
                "corpusId": 236493649,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3c91a4534d51d21ae67e4a9f9287bb2a14dc5e3b",
                "title": "Learning more skills through optimistic exploration",
                "abstract": "Unsupervised skill learning objectives (Gregor et al., 2016, Eysenbach et al., 2018) allow agents to learn rich repertoires of behavior in the absence of extrinsic rewards. They work by simultaneously training a policy to produce distinguishable latent-conditioned trajectories, and a discriminator to evaluate distinguishability by trying to infer latents from trajectories. The hope is for the agent to explore and master the environment by encouraging each skill (latent) to reliably reach different states. However, an inherent exploration problem lingers: when a novel state is actually encountered, the discriminator will necessarily not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, we derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. Our objective directly estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples, thus providing an intrinsic reward more tailored to the true objective compared to pseudocount-based methods (Burda et al., 2019). We call this exploration bonus discriminator disagreement intrinsic reward, or DISDAIN. We demonstrate empirically that DISDAIN improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we encourage researchers to treat pessimism with DISDAIN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "69925460",
                        "name": "D. Strouse"
                    },
                    {
                        "authorId": "1734809439",
                        "name": "Kate Baumli"
                    },
                    {
                        "authorId": "1393680089",
                        "name": "David Warde-Farley"
                    },
                    {
                        "authorId": "123588356",
                        "name": "V. Mnih"
                    },
                    {
                        "authorId": "35231584",
                        "name": "S. Hansen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[50], wherein data was collected by a purely random policy, which may well fail to explore many relevant regions of the games.",
                "[50]\u2014however, as we require slot-level rather than flat embeddings, the final layers of our encoder are different."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8f6cc5ec1acdf35be182b6d1a77b1972aa266479",
                "externalIds": {
                    "ArXiv": "2107.08881",
                    "DBLP": "conf/log/VelickovicBKLHP22",
                    "CorpusId": 236088008
                },
                "corpusId": 236088008,
                "publicationVenue": {
                    "id": "50534c12-f4ba-4c64-806b-01647d1baacf",
                    "name": "LOG IN",
                    "type": "journal",
                    "alternate_names": [
                        "Log in",
                        "Log",
                        "LOG"
                    ],
                    "issn": "0720-8642",
                    "alternate_issns": [
                        "1547-4690",
                        "0024-5798"
                    ],
                    "url": "https://www.log-in-verlag.de/informatische_bildung/",
                    "alternate_urls": [
                        "https://www.anycorp.com/log/about",
                        "https://www.jstor.org/journal/log",
                        "https://www.anycorp.com/",
                        "http://www.jstor.org/action/showPublication?journalCode=log",
                        "http://www.anycorp.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8f6cc5ec1acdf35be182b6d1a77b1972aa266479",
                "title": "Reasoning-Modulated Representations",
                "abstract": "Neural networks leverage robust internal representations in order to generalise. Learning them is difficult, and often requires a large training set that covers the data distribution densely. We study a common setting where our task is not purely opaque. Indeed, very often we may have access to information about the underlying system (e.g. that observations must obey certain laws of physics) that any\"tabula rasa\"neural network would need to re-learn from scratch, penalising performance. We incorporate this information into a pre-trained reasoning module, and investigate its role in shaping the discovered representations in diverse self-supervised learning settings from pixels. Our approach paves the way for a new class of representation learning, grounded in algorithmic priors.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1742197495",
                        "name": "Petar Velivckovi'c"
                    },
                    {
                        "authorId": "1471340201",
                        "name": "Matko Bovsnjak"
                    },
                    {
                        "authorId": "41016725",
                        "name": "Thomas Kipf"
                    },
                    {
                        "authorId": "2289726",
                        "name": "Alexander Lerchner"
                    },
                    {
                        "authorId": "2315504",
                        "name": "R. Hadsell"
                    },
                    {
                        "authorId": "1996134",
                        "name": "Razvan Pascanu"
                    },
                    {
                        "authorId": "1723876",
                        "name": "C. Blundell"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The learned representation should facilitate efficient downstream learning [19, 25, 26] and exhibit better generalization [27, 28]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e48ae8395d28ce3cc7990e4acd83362a65656af2",
                "externalIds": {
                    "ArXiv": "2107.05686",
                    "DBLP": "conf/iclr/TraubleDWWGWLBS22",
                    "CorpusId": 239016979
                },
                "corpusId": 239016979,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e48ae8395d28ce3cc7990e4acd83362a65656af2",
                "title": "The Role of Pretrained Representations for the OOD Generalization of RL Agents",
                "abstract": "Building sample-efficient agents that generalize out-of-distribution (OOD) in real-world settings remains a fundamental unsolved problem on the path towards achieving higher-level cognition. One particularly promising approach is to begin with low-dimensional, pretrained representations of our world, which should facilitate efficient downstream learning and generalization. By training 240 representations and over 10,000 reinforcement learning (RL) policies on a simulated robotic setup, we evaluate to what extent different properties of pretrained VAE-based representations affect the OOD generalization of downstream agents. We observe that many agents are surprisingly robust to realistic distribution shifts, including the challenging sim-to-real case. In addition, we find that the generalization performance of a simple downstream proxy task reliably predicts the generalization performance of our RL agents under a wide range of OOD settings. Such proxy tasks can thus be used to select pretrained representations that will lead to agents that generalize.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3431679",
                        "name": "Andrea Dittadi"
                    },
                    {
                        "authorId": "1749568051",
                        "name": "Frederik Trauble"
                    },
                    {
                        "authorId": "1387895034",
                        "name": "M. Wuthrich"
                    },
                    {
                        "authorId": "47804478",
                        "name": "F. Widmaier"
                    },
                    {
                        "authorId": "2871555",
                        "name": "Peter Gehler"
                    },
                    {
                        "authorId": "1724252",
                        "name": "O. Winther"
                    },
                    {
                        "authorId": "2112584507",
                        "name": "Francesco Locatello"
                    },
                    {
                        "authorId": "1936951",
                        "name": "Olivier Bachem"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    },
                    {
                        "authorId": "153125952",
                        "name": "Stefan Bauer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Anand et al. (2019) learn state representation for an RL agent in an unsupervised setting and introduce a set of probe tasks to evaluate the representation learnt by agents."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c2d69b099cecf24c577ace3a68d4816398d0400d",
                "externalIds": {
                    "ACL": "2021.sigdial-1.50",
                    "ArXiv": "2106.10622",
                    "DBLP": "journals/corr/abs-2106-10622",
                    "CorpusId": 235490333
                },
                "corpusId": 235490333,
                "publicationVenue": {
                    "id": "6a470734-72c6-4809-a07d-d34dee0df4a1",
                    "name": "SIGDIAL Conferences",
                    "type": "conference",
                    "alternate_names": [
                        "SIGDIAL",
                        "SIGDIAL Conf",
                        "Annu Meet Sp\u00e9c Interest Group Discourse Dialogue",
                        "Annual Meeting of the Special Interest Group on Discourse and Dialogue"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c2d69b099cecf24c577ace3a68d4816398d0400d",
                "title": "Do Encoder Representations of Generative Dialogue Models have sufficient summary of the Information about the task ?",
                "abstract": "Predicting the next utterance in dialogue is contingent on encoding of users\u2019 input text to generate appropriate and relevant response in data-driven approaches. Although the semantic and syntactic quality of the language generated is evaluated, more often than not, the encoded representation of input is not evaluated. As the representation of the encoder is essential for predicting the appropriate response, evaluation of encoder representation is a challenging yet important problem. In this work, we showcase evaluating the text generated through human or automatic metrics is not sufficient to appropriately evaluate soundness of the language understanding of dialogue models and, to that end, propose a set of probe tasks to evaluate encoder representation of different language encoders commonly used in dialogue models. From experiments, we observe that some of the probe tasks are easier and some are harder for even sophisticated model architectures to learn. And, through experiments we observe that RNN based architectures have lower performance on automatic metrics on text generation than transformer model but perform better than the transformer model on the probe tasks indicating that RNNs might preserve task information better than the Transformers.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "32899078",
                        "name": "Prasanna Parthasarathi"
                    },
                    {
                        "authorId": "145134884",
                        "name": "J. Pineau"
                    },
                    {
                        "authorId": "123607932",
                        "name": "Sarath Chandar"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "aee4c6aa5b726dfdc5a4c371e4a68700335d77c0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-07278",
                    "ArXiv": "2106.07278",
                    "CorpusId": 235422367
                },
                "corpusId": 235422367,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/aee4c6aa5b726dfdc5a4c371e4a68700335d77c0",
                "title": "Which Mutual-Information Representation Learning Objectives are Sufficient for Control?",
                "abstract": "Mutual information maximization provides an appealing formalism for learning representations of data. In the context of reinforcement learning (RL), such representations can accelerate learning by discarding irrelevant and redundant information, while retaining the information necessary for control. Much of the prior work on these methods has addressed the practical difficulties of estimating mutual information from samples of high-dimensional observations, while comparatively less is understood about which mutual information objectives yield representations that are sufficient for RL from a theoretical perspective. In this paper, we formalize the sufficiency of a state representation for learning and representing the optimal policy, and study several popular mutual-information based objectives through this lens. Surprisingly, we find that two of these objectives can yield insufficient representations given mild and common assumptions on the structure of the MDP. We corroborate our theoretical results with empirical experiments on a simulated game environment with visual observations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2660664",
                        "name": "Kate Rakelly"
                    },
                    {
                        "authorId": "2129458064",
                        "name": "Abhishek Gupta"
                    },
                    {
                        "authorId": "10104623",
                        "name": "Carlos Florensa"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Anand et al. (2019) show that representations from encoders trained with ST-DIM contain a great deal of information about environment states, but they do not examine whether or not representations learned via their method are, in fact, useful for reinforcement learning.",
                "Two works similar to ours, Anand et al. (2019) and Stooke et al. (2021), propose reward-free temporalcontrastive methods to pretrain representations.",
                "These factors have led to Atari\u2019s extensive use for representation learning and exploratory pretraining (Anand et al., 2019; Stooke et al., 2021; Campos et al., 2021), and specifically Atari 100k for data-efficient RL (e.",
                "These factors have led to Atari\u2019s extensive use for representation learning and exploratory pretraining (Anand et al., 2019; Stooke et al., 2021; Campos et al., 2021), and specifically Atari 100k for data-efficient RL (e.g., Kaiser et al., 2019; Kostrikov et al., 2021; Schwarzer et al., 2021),\u2026"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "0f1382cb004b4834cc3ca7824a61d0d6b86a5763",
                "externalIds": {
                    "ArXiv": "2106.04799",
                    "DBLP": "conf/nips/SchwarzerRNACHB21",
                    "CorpusId": 235377401
                },
                "corpusId": 235377401,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0f1382cb004b4834cc3ca7824a61d0d6b86a5763",
                "title": "Pretraining Representations for Data-Efficient Reinforcement Learning",
                "abstract": "Data efficiency is a key challenge for deep reinforcement learning. We address this problem by using unlabeled data to pretrain an encoder which is then finetuned on a small amount of task-specific data. To encourage learning representations which capture diverse aspects of the underlying MDP, we employ a combination of latent dynamics modelling and unsupervised goal-conditioned RL. When limited to 100k steps of interaction on Atari games (equivalent to two hours of human experience), our approach significantly surpasses prior work combining offline representation pretraining with task-specific finetuning, and compares favourably with other pretraining methods that require orders of magnitude more data. Our approach shows particular promise when combined with larger models as well as more diverse, task-aligned observational data -- approaching human-level performance and data-efficiency on Atari in our best setting. We provide code associated with this work at https://github.com/mila-iqia/SGI.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51881243",
                        "name": "Max Schwarzer"
                    },
                    {
                        "authorId": "1420542737",
                        "name": "Nitarshan Rajkumar"
                    },
                    {
                        "authorId": "41020834",
                        "name": "Michael Noukhovitch"
                    },
                    {
                        "authorId": "12679121",
                        "name": "Ankesh Anand"
                    },
                    {
                        "authorId": "1778839",
                        "name": "Laurent Charlin"
                    },
                    {
                        "authorId": "88844399",
                        "name": "Devon Hjelm"
                    },
                    {
                        "authorId": "143902541",
                        "name": "Philip Bachman"
                    },
                    {
                        "authorId": "1760871",
                        "name": "Aaron C. Courville"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2446a59a1a673a7722c5b432d063a9045cf19902",
                "externalIds": {
                    "ArXiv": "2106.05139",
                    "DBLP": "journals/corr/abs-2106-05139",
                    "CorpusId": 235376966
                },
                "corpusId": 235376966,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2446a59a1a673a7722c5b432d063a9045cf19902",
                "title": "Pretrained Encoders are All You Need",
                "abstract": "Data-efficiency and generalization are key challenges in deep learning and deep reinforcement learning as many models are trained on large-scale, domain-specific, and expensive-to-label datasets. Self-supervised models trained on large-scale uncurated datasets have shown successful transfer to diverse settings. We investigate using pretrained image representations and spatio-temporal attention for state representation learning in Atari. We also explore fine-tuning pretrained representations with self-supervised techniques, i.e., contrastive predictive coding, spatio-temporal contrastive learning, and augmentations. Our results show that pretrained representations are at par with state-of-the-art self-supervised methods trained on domain-specific data. Pretrained representations, thus, yield data and compute-efficient state representations. https://github.com/PAL-ML/PEARL_v1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110749457",
                        "name": "Mina Khan"
                    },
                    {
                        "authorId": "65885416",
                        "name": "P. Srivatsa"
                    },
                    {
                        "authorId": "2035283191",
                        "name": "Advait Rane"
                    },
                    {
                        "authorId": "2106712573",
                        "name": "Shriram Chenniappa"
                    },
                    {
                        "authorId": "2108873000",
                        "name": "Rishabh Anand"
                    },
                    {
                        "authorId": "1955694",
                        "name": "Sherjil Ozair"
                    },
                    {
                        "authorId": "1701876",
                        "name": "P. Maes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "And it has achieved remarkable success in unsupervised learing task in images [6], videos [2] and natural language processing [4]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d41e6c63b62fe2ed9e4865c2ea0cf4c7f1cd218a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-04922",
                    "ArXiv": "2106.04922",
                    "CorpusId": 235376878
                },
                "corpusId": 235376878,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d41e6c63b62fe2ed9e4865c2ea0cf4c7f1cd218a",
                "title": "Self-supervision of Feature Transformation for Further Improving Supervised Learning",
                "abstract": "Self-supervised learning, which benefits from automatically constructing labels through pre-designed pretext task, has recently been applied for strengthen supervised learning. Since previous self-supervised pretext tasks are based on input, they may incur huge additional training overhead. In this paper we find that features in CNNs can be also used for self-supervision. Thus we creatively design the \\emph{feature-based pretext task} which requires only a small amount of additional training overhead. In our task we discard different particular regions of features, and then train the model to distinguish these different features. In order to fully apply our feature-based pretext task in supervised learning, we also propose a novel learning framework containing multi-classifiers for further improvement. Original labels will be expanded to joint labels via self-supervision of feature transformations. With more semantic information provided by our self-supervised tasks, this approach can train CNNs more effectively. Extensive experiments on various supervised learning tasks demonstrate the accuracy improvement and wide applicability of our method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108775182",
                        "name": "Zilin Ding"
                    },
                    {
                        "authorId": "47796458",
                        "name": "Yuhang Yang"
                    },
                    {
                        "authorId": "2110251551",
                        "name": "Xuan Cheng"
                    },
                    {
                        "authorId": "2141775420",
                        "name": "Xiaomin Wang"
                    },
                    {
                        "authorId": "145111960",
                        "name": "Ming Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Contrastive loss objectives typically aim to distinguish either sequential states from non-sequential ones (Shelhamer et al., 2016; Anand et al., 2019; Stooke et al., 2020), real states from predicted ones (Van den Oord et al.",
                "Contrastive loss objectives typically aim to distinguish either sequential states from non-sequential ones (Shelhamer et al., 2016; Anand et al., 2019; Stooke et al., 2020), real states from predicted ones (Van den Oord et al., 2018), or determine whether two augmented views came from the same or\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d0528ebfc69bd99f98d0f965913b7c16ae5f4f71",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-04379",
                    "ArXiv": "2106.04379",
                    "CorpusId": 227520288
                },
                "corpusId": 227520288,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d0528ebfc69bd99f98d0f965913b7c16ae5f4f71",
                "title": "Learning Markov State Abstractions for Deep Reinforcement Learning",
                "abstract": "A fundamental assumption of reinforcement learning in Markov decision processes (MDPs) is that the relevant decision process is, in fact, Markov. However, when MDPs have rich observations, agents typically learn by way of an abstract state representation, and such representations are not guaranteed to preserve the Markov property. We introduce a novel set of conditions and prove that they are sufficient for learning a Markov abstract state representation. We then describe a practical training procedure that combines inverse model estimation and temporal contrastive learning to learn an abstraction that approximately satisfies these conditions. Our novel training objective is compatible with both online and offline training: it does not require a reward signal, but agents can capitalize on reward information when available. We empirically evaluate our approach on a visual gridworld domain and a set of continuous control benchmarks. Our approach learns representations that capture the underlying structure of the domain and lead to improved sample efficiency over state-of-the-art deep reinforcement learning with visual features -- often matching or exceeding the performance achieved with hand-designed compact state information.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46781095",
                        "name": "Cameron S. Allen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, CPC [38], ST-DIM [1], DRIML [34], ATC [42] and PI-SAC [31] maximize the mutual information between the current state and the future state by using InfoNCE [38, 42], Deep InfoMax [20, 1, 34], or Conditional Entropy Bottleneck [9, 31].",
                "Previous works have demonstrated that good auxiliary supervision can significantly improve agent learning, like leveraging image reconstruction [49], the prediction of future states [41, 13, 30, 40], maximizing Predictive Information [38, 1, 34, 42, 31], or promoting discrimination through contrastive learning [29, 59, 32, 25].",
                "(i) Auxiliary task based methods introduce auxiliary task to help representation learning of the states [21, 49, 41, 13, 30, 40, 38, 1, 34, 42, 31, 29, 59, 32]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d54112833080d52be2d4cd43676f6a751470b90b",
                "externalIds": {
                    "DBLP": "conf/nips/YuLZFZC21",
                    "ArXiv": "2106.04152",
                    "CorpusId": 235367636
                },
                "corpusId": 235367636,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d54112833080d52be2d4cd43676f6a751470b90b",
                "title": "PlayVirtual: Augmenting Cycle-Consistent Virtual Trajectories for Reinforcement Learning",
                "abstract": "Learning good feature representations is important for deep reinforcement learning (RL). However, with limited experience, RL often suffers from data inefficiency for training. For un-experienced or less-experienced trajectories (i.e., state-action sequences), the lack of data limits the use of them for better feature learning. In this work, we propose a novel method, dubbed PlayVirtual, which augments cycle-consistent virtual trajectories to enhance the data efficiency for RL feature representation learning. Specifically, PlayVirtual predicts future states in the latent space based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a trajectory cycle. Based on this, we augment the actions to generate a large amount of virtual state-action trajectories. Being free of groudtruth state supervision, we enforce a trajectory to meet the cycle consistency constraint, which can significantly enhance the data efficiency. We validate the effectiveness of our designs on the Atari and DeepMind Control Suite benchmarks. Our method achieves the state-of-the-art performance on both benchmarks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152278376",
                        "name": "Tao Yu"
                    },
                    {
                        "authorId": "40093162",
                        "name": "Cuiling Lan"
                    },
                    {
                        "authorId": "1634494276",
                        "name": "Wenjun Zeng"
                    },
                    {
                        "authorId": "5325186",
                        "name": "Mingxiao Feng"
                    },
                    {
                        "authorId": "31482866",
                        "name": "Zhibo Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, successor representations [32, 68] orthogonalize the features of the representation and standard temporal contrastive losses use the representation for imitation [59], end-to-end task solving [2, 62] or flat exploration [69, 28]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "988dae20df8d69869aa41097a05d821446cff621",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-03853",
                    "ArXiv": "2106.03853",
                    "DOI": "10.1109/tcds.2023.3265200",
                    "CorpusId": 235368354
                },
                "corpusId": 235368354,
                "publicationVenue": {
                    "id": "f35f148a-0a3c-45db-b610-3d89e09ddf21",
                    "name": "IEEE Transactions on Cognitive and Developmental Systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Cogn Dev Syst"
                    ],
                    "issn": "2379-8920",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274989"
                },
                "url": "https://www.semanticscholar.org/paper/988dae20df8d69869aa41097a05d821446cff621",
                "title": "DisTop: Discovering a Topological representation to learn diverse and rewarding skills",
                "abstract": "The optimal way for a deep reinforcement learning (DRL) agent to explore is to learn a set of skills that achieves a uniform distribution of states. Following this,we introduce DisTop, a new model that simultaneously learns diverse skills and focuses on improving rewarding skills. DisTop progressively builds a discrete topology of the environment using an unsupervised contrastive loss, a growing network and a goal-conditioned policy. Using this topology, a state-independent hierarchical policy can select where the agent has to keep discovering skills in the state space. In turn, the newly visited states allows an improved learnt representation and the learning loop continues. Our experiments emphasize that DisTop is agnostic to the ground state representation and that the agent can discover the topology of its environment whether the states are high-dimensional binary data, images, or proprioceptive inputs. We demonstrate that this paradigm is competitiveon MuJoCo benchmarks with state-of-the-art algorithms on both single-task dense rewards and diverse skill discovery. By combining these two aspects, we showthat DisTop achieves state-of-the-art performance in comparison with hierarchical reinforcement learning (HRL) when rewards are sparse. We believe DisTop opens new perspectives by showing that bottom-up skill discovery combined with representation learning can unlock the exploration challenge in DRL.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1387894646",
                        "name": "A. Aubret"
                    },
                    {
                        "authorId": "2335305",
                        "name": "L. Matignon"
                    },
                    {
                        "authorId": "1730965",
                        "name": "S. Hassas"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", where pixel similarities in consecutive frames in Atari games can help decode object identities [15], or the underlying objectlayout map [16]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e282a0725ff485bd97a344668fe667027d26c93",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-03121",
                    "ArXiv": "2106.03121",
                    "CorpusId": 235358778
                },
                "corpusId": 235358778,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0e282a0725ff485bd97a344668fe667027d26c93",
                "title": "End-to-End Neuro-Symbolic Architecture for Image-to-Image Reasoning Tasks",
                "abstract": "Neural models and symbolic algorithms have recently been combined for tasks requiring both perception and reasoning. Neural models ground perceptual input into a conceptual vocabulary, on which a classical reasoning algorithm is applied to generate output. A key limitation is that such neural-to-symbolic models can only be trained end-to-end for tasks where the output space is symbolic. In this paper, we study neural-symbolic-neural models for reasoning tasks that require a conversion from an image input (e.g., a partially filled sudoku) to an image output (e.g., the image of the completed sudoku). While designing such a three-step hybrid architecture may be straightforward, the key technical challenge is end-to-end training -- how to backpropagate without intermediate supervision through the symbolic component. We propose NSNnet, an architecture that combines an image reconstruction loss with a novel output encoder to generate a supervisory signal, develops update algorithms that leverage policy gradient methods for supervision, and optimizes loss using a novel subsampling heuristic. We experiment on problem settings where symbolic algorithms are easily specified: a visual maze solving task and a visual Sudoku solver where the supervision is in image form. Experiments show high accuracy with significantly less data compared to purely neural approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107063491",
                        "name": "Ananye Agarwal"
                    },
                    {
                        "authorId": "144287984",
                        "name": "P. Shenoy"
                    },
                    {
                        "authorId": "2674444",
                        "name": "Mausam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To do so, we propose Cross Trajectory Representation Learning (CTRL), which applies a novel self-supervised learning (SSL) objective to pairs of trajectories drawn from the agent\u2019s policies.",
                "The end result is an agent whose encoder maps behaviorally similar trajectories to similar representations without directly referencing reward, which we show improves ZSG performance over using pure RL or RL in conjunction with other unsupervised or SSL methods.",
                "While successful in their own way, prior works that combine SSL with RL do so by applying known SSL algorithms [e.g., from vision, 22, 42, 6, 11, 21, 19] to RL in a nearly off-the-shelf manner, predicting state representations within a given trajectory, only potentially using other trajectories as counterexamples in a contrastive loss.",
                "SSL formulates objectives by generating different views of the data, which are essentially transformed versions of the data, e.g., generated by using data augmentation or by sampling patches.",
                "Our main contributions are as follows:\n\u2022 We introduce Cross Trajectory Representation Learning (CTRL), a novel SSL algorithm for RL that defines an auxiliary objective across trajectories, drawing samples for a SSL predictive task by leveraging assignments from an online clustering algorithm.",
                "We also compare with two SSL-based auxiliary objectives: CURL [38], a common SSL baseline which contrasts augmented instances of the same state; and Proto-RL [47], which we adapt for this generalization setting and denote PPO+Sinkhorn.",
                "We then compare to several unsupervised and SSL auxiliary objectives used in conjunction with PPO.",
                "Therefore, CTRL has a second step: drawing inspiration from Mine Your Own View [MYOW, 5], it selects (mines) representational nearest neighbors from different, nearby clusters and applies a predictive SSL objective to them.",
                "A recent thorough comparison of SSL and bisimulation frameworks also suggests that self-supervised objectives can outperform bisimulation-based algorithms on offline tasks [46].",
                "A successful class of models that incorporate unsupervised objectives to improve RL use self-supervised learning (SSL) [3, 38, 27, 36, 39].",
                "CTRL does this by using a predictive SSL objective between representations of \u201csimilar\u201d trajectories drawn from RL policies."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f496a4934ae20d7da7869b4c7c5c47b61fccb299",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-02193",
                    "ArXiv": "2106.02193",
                    "CorpusId": 235353039
                },
                "corpusId": 235353039,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f496a4934ae20d7da7869b4c7c5c47b61fccb299",
                "title": "Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL",
                "abstract": "A highly desirable property of a reinforcement learning (RL) agent -- and a major difficulty for deep RL approaches -- is the ability to generalize policies learned on a few tasks over a high-dimensional observation space to similar tasks not seen during training. Many promising approaches to this challenge consider RL as a process of training two functions simultaneously: a complex nonlinear encoder that maps high-dimensional observations to a latent representation space, and a simple linear policy over this space. We posit that a superior encoder for zero-shot generalization in RL can be trained by using solely an auxiliary SSL objective if the training process encourages the encoder to map behaviorally similar observations to similar representations, as reward-based signal can cause overfitting in the encoder (Raileanu et al., 2021). We propose Cross-Trajectory Representation Learning (CTRL), a method that runs within an RL agent and conditions its encoder to recognize behavioral similarity in observations by applying a novel SSL objective to pairs of trajectories from the agent's policies. CTRL can be viewed as having the same effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use of rewards and associated overfitting risks. Our experiments ablate various components of CTRL and demonstrate that in combination with PPO it achieves better generalization performance on the challenging Procgen benchmark suite (Cobbe et al., 2020).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "18111246",
                        "name": "Bogdan Mazoure"
                    },
                    {
                        "authorId": "2110059784",
                        "name": "Ahmed M. Ahmed"
                    },
                    {
                        "authorId": "3135801",
                        "name": "Patrick MacAlpine"
                    },
                    {
                        "authorId": "40482726",
                        "name": "R. Devon Hjelm"
                    },
                    {
                        "authorId": "6247481",
                        "name": "A. Kolobov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In these experiments, we employ a probing technique similar to the one described in Anand et al. (2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ceb4e0c2ad601831c056901e9f1ab5f5500d8520",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-00266",
                    "ArXiv": "2106.00266",
                    "CorpusId": 235266015
                },
                "corpusId": 235266015,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ceb4e0c2ad601831c056901e9f1ab5f5500d8520",
                "title": "Did I do that? Blame as a means to identify controlled effects in reinforcement learning",
                "abstract": "Identifying controllable aspects of the environment has proven to be an extraordinary intrinsic motivator to reinforcement learning agents. Despite repeatedly achieving State-of-the-Art results, this approach has only been studied as a proxy to a reward-based task and has not yet been evaluated on its own. Current methods are based on action-prediction. Humans, on the other hand, assign blame to their actions to decide what they controlled. This work proposes Controlled Effect Network (CEN), an unsupervised method based on counterfactual measures of blame to identify effects on the environment controlled by the agent. CEN is evaluated in a wide range of environments showing that it can accurately identify controlled effects. Moreover, we demonstrate CEN's capabilities as intrinsic motivator by integrating it in the state-of-the-art exploration method, achieving substantially better performance than action-prediction models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1396771299",
                        "name": "Oriol Corcoll"
                    },
                    {
                        "authorId": "144846212",
                        "name": "Raul Vicente"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, (Anand et al., 2020) introduce a method that learns state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bdf807227af71f539c2802d17b757e17aa422eec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-08568",
                    "ArXiv": "2105.08568",
                    "CorpusId": 234763181
                },
                "corpusId": 234763181,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bdf807227af71f539c2802d17b757e17aa422eec",
                "title": "Fixed \u03b2-VAE Encoding for Curious Exploration in Complex 3D Environments",
                "abstract": "Curiosity is a general method for augmenting an environment reward with an intrinsic reward, which encourages exploration and is especially useful in sparse reward settings. As curiosity is calculated using next state prediction error, the type of state encoding used has a large impact on performance. Random features and inverse-dynamics features are generally preferred over VAEs based on previous results from Atari and other mostly 2D environments. However, unlike VAEs, they may not encode sufficient information for optimal behaviour, which becomes increasingly important as environments become more complex. In this paper, we use the sparse reward 3D physics environment Animal-AI, to demonstrate how a fixed $\\beta$-VAE encoding can be used effectively with curiosity. We combine this with curriculum learning to solve the previously unsolved exploration intensive detour tasks while achieving 22\\% gain in sample efficiency on the training curriculum against the next best encoding. We also corroborate the results on Atari Breakout, with our custom encoding outperforming random features and inverse-dynamics features.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2093919748",
                        "name": "A. Lehuger"
                    },
                    {
                        "authorId": "143966629",
                        "name": "Matthew Crosby"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We find that the objects extracted from the self-supervised attention masks are reasonably focused on salient objects, as compared to both the ground truth objects extracted from (Anand et al. 2019) and the Transporter (Kulkarni et al.",
                "We compute these metrics using the predicted object locations and the ground truth locations from Anand et al. (2019).",
                "We find that the objects extracted from the self-supervised attention masks are reasonably focused on salient objects, as compared to both the ground truth objects extracted from (Anand et al. 2019) and the Transporter (Kulkarni et al. 2019) method."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "38537de90fda155cd10794c42dd6e14602b2fa77",
                "externalIds": {
                    "DBLP": "conf/aaai/WuKP21",
                    "DOI": "10.1609/aaai.v35i12.17235",
                    "CorpusId": 235349100
                },
                "corpusId": 235349100,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/38537de90fda155cd10794c42dd6e14602b2fa77",
                "title": "Self-Supervised Attention-Aware Reinforcement Learning",
                "abstract": "Visual saliency has emerged as a major visualization tool for interpreting deep reinforcement learning (RL) agents. However, much of the existing research uses it as an analyzing tool rather than an inductive bias for policy learning. In this work, we use visual attention as an inductive bias for RL agents. We propose a novel self-supervised attention learning approach which can 1. learn to select regions of interest without explicit annotations, and 2. act as a plug for existing deep RL methods to improve the learning performance. We empirically show that the self-supervised attention-aware deep RL methods outperform the baselines in the context of both the rate of convergence and performance. Furthermore, the proposed self-supervised attention is not tied with specific policies, nor restricted to a specific scene. We posit that the proposed approach is a general self-supervised attention module for multi-task learning and transfer learning, and empirically validate the generalization ability of the proposed method. Finally, we show that our method learns meaningful object keypoints highlighting improvements both qualitatively and quantitatively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2119019500",
                        "name": "Haiping Wu"
                    },
                    {
                        "authorId": "38562041",
                        "name": "Khimya Khetarpal"
                    },
                    {
                        "authorId": "144368601",
                        "name": "Doina Precup"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite being a relatively new area of study, it has been the basis for many excellent works such as contrastive predictive coding [20, 43], representation learning using Deep InfoMax (DIM) [4, 21, 59] or momentum contast [18], learning invariances using Augmented Multiscale DIM [9] or Contrastive MultiView Coding [56] etc.",
                "Despite being a relatively new area of study, it has been the basis for many excellent works such as contrastive predictive coding [20, 43], representation learning using Deep InfoMax (DIM) [4, 21, 59] or momentum contast [18], learning invariances using Augmented Multiscale DIM [9] or Contrastive MultiView Coding [56] etc.\nPrevious deep learning approaches have shown promises in identifying COVID-19 cases from chest radiography images [5, 13, 14, 19, 22, 46, 61\u201364, 71, 72], please refer to Table\u00a01."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fcba323aed01717610649852dfdd2fefb2a27c00",
                "externalIds": {
                    "PubMedCentral": "8591440",
                    "MAG": "3166654090",
                    "DOI": "10.1007/s13246-021-01075-2",
                    "CorpusId": 236595019,
                    "PubMed": "34780042"
                },
                "corpusId": 236595019,
                "publicationVenue": {
                    "id": "221484d6-381d-4808-a7d4-46c0294cb2df",
                    "name": "Physical and Engineering Sciences in Medicine",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Eng Sci Med"
                    ],
                    "issn": "2662-4729",
                    "url": "https://www.springer.com/journal/13246",
                    "alternate_urls": [
                        "http://link.springer.com/journal/13246"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fcba323aed01717610649852dfdd2fefb2a27c00",
                "title": "Potential diagnosis of COVID-19 from chest X-ray and CT findings using semi-supervised learning",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10386824",
                        "name": "Pracheta Sahoo"
                    },
                    {
                        "authorId": "2004499527",
                        "name": "I. Roy"
                    },
                    {
                        "authorId": "2121813595",
                        "name": "R. Ahlawat"
                    },
                    {
                        "authorId": "2121802720",
                        "name": "Saquib Irtiza"
                    },
                    {
                        "authorId": "145155297",
                        "name": "L. Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Representation learning in RL: Recently, using unsupervised/self-supervised representation learning methods to improve sample efficiency and/or performance in RL has gained increased popularity [2, 25, 37, 39, 40]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4e9f9e1a69e1924aa9345d77510d43285900f465",
                "externalIds": {
                    "ArXiv": "2105.01060",
                    "DBLP": "journals/corr/abs-2105-01060",
                    "DOI": "10.1109/ICCV48922.2021.01024",
                    "CorpusId": 233481519
                },
                "corpusId": 233481519,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/4e9f9e1a69e1924aa9345d77510d43285900f465",
                "title": "Curious Representation Learning for Embodied Intelligence",
                "abstract": "Self-supervised representation learning has achieved remarkable success in recent years. By subverting the need for supervised labels, such approaches are able to utilize the numerous unlabeled images that exist on the Internet and in photographic datasets. Yet to build truly intelligent agents, we must construct representation learning algorithms that can learn not only from datasets but also learn from environments. An agent in a natural environment will not typically be fed curated data. Instead, it must explore its environment to acquire the data it will learn from. We propose a framework, curious representation learning (CRL), which jointly learns a reinforcement learning policy and a visual representation model. The policy is trained to maximize the error of the representation learner, and in doing so is incentivized to explore its environment. At the same time, the learned representation becomes stronger and stronger as the policy feeds it ever harder data to learn from. Our learned representations enable promising transfer to downstream navigation tasks, performing better than or comparably to ImageNet pretraining without using any supervision at all. In addition, despite being trained in simulation, our learned representations can obtain interpretable results on real images. Code is available at https://yilundu.github.io/crl/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "15394275",
                        "name": "Yilun Du"
                    },
                    {
                        "authorId": "144158271",
                        "name": "Chuang Gan"
                    },
                    {
                        "authorId": "2094770",
                        "name": "Phillip Isola"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such an interpretable and layered model of images could be beneficial for a plethora of applications like object discovery [16, 6], image edition [75, 21], future frame prediction [74], object pose estimation [57] or environment abstraction [1, 47]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "edfbb73fea9da86f6db9a3830e17ff0cee783ef2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-14575",
                    "ArXiv": "2104.14575",
                    "DOI": "10.1109/ICCV48922.2021.00852",
                    "CorpusId": 233476260
                },
                "corpusId": 233476260,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/edfbb73fea9da86f6db9a3830e17ff0cee783ef2",
                "title": "Unsupervised Layered Image Decomposition into Object Prototypes",
                "abstract": "We present an unsupervised learning framework for decomposing images into layers of automatically discovered object models. Contrary to recent approaches that model image layers with autoencoder networks, we represent them as explicit transformations of a small set of prototypical images. Our model has three main components: (i) a set of object prototypes in the form of learnable images with a transparency channel, which we refer to as sprites; (ii) differentiable parametric functions predicting occlusions and transformation parameters necessary to instantiate the sprites in a given image; (iii) a layered image formation model with occlusion for compositing these instances into complete images including background. By jointly learning the sprites and occlusion/transformation predictors to reconstruct images, our approach not only yields accurate layered image decompositions, but also identifies object categories and instance parameters. We first validate our approach by providing results on par with the state of the art on standard multiobject synthetic benchmarks (Tetrominoes, Multi-dSprites, CLEVR6). We then demonstrate the applicability of our model to real images in tasks that include clustering (SVHN, GTSRB), cosegmentation (Weizmann Horse) and object discovery from unfiltered social network images. To the best of our knowledge, our approach is the first layered image decomposition algorithm that learns an explicit and shared concept of object type, and is robust enough to be applied to real images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1643955575",
                        "name": "Tom Monnier"
                    },
                    {
                        "authorId": "2086971830",
                        "name": "Elliot Vincent"
                    },
                    {
                        "authorId": "144189388",
                        "name": "J. Ponce"
                    },
                    {
                        "authorId": "48582897",
                        "name": "Mathieu Aubry"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "88799fcabed0adf1efbb78621297163a9450f650",
                "externalIds": {
                    "DBLP": "journals/tnn/JiSGHY22",
                    "DOI": "10.1109/TNNLS.2021.3071275",
                    "CorpusId": 233349241,
                    "PubMed": "33882000"
                },
                "corpusId": 233349241,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/88799fcabed0adf1efbb78621297163a9450f650",
                "title": "A Decoder-Free Variational Deep Embedding for Unsupervised Clustering",
                "abstract": "In deep clustering frameworks, autoencoder (AE)- or variational AE-based clustering approaches are the most popular and competitive ones that encourage the model to obtain suitable representations and avoid the tendency for degenerate solutions simultaneously. However, for the clustering task, the decoder for reconstructing the original input is usually useless when the model is finished training. The encoder\u2013decoder architecture limits the depth of the encoder so that the learning capacity is reduced severely. In this article, we propose a decoder-free variational deep embedding for unsupervised clustering (DFVC). It is well known that minimizing reconstruction error amounts to maximizing a lower bound on the mutual information (MI) between the input and its representation. That provides a theoretical guarantee for us to discard the bloated decoder. Inspired by contrastive self-supervised learning, we can directly calculate or estimate the MI of the continuous variables. Specifically, we investigate unsupervised representation learning by simultaneously considering the MI estimation of continuous representations and the MI computation of categorical representations. By introducing the data augmentation technique, we incorporate the original input, the augmented input, and their high-level representations into the MI estimation framework to learn more discriminative representations. Instead of matching to a simple standard normal distribution adversarially, we use end-to-end learning to constrain the latent space to be cluster-friendly by applying the Gaussian mixture distribution as the prior. Extensive experiments on challenging data sets show that our model achieves higher performance over a wide range of state-of-the-art clustering approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50426357",
                        "name": "Q. Ji"
                    },
                    {
                        "authorId": "5305357",
                        "name": "Yanfeng Sun"
                    },
                    {
                        "authorId": "32278515",
                        "name": "Junbin Gao"
                    },
                    {
                        "authorId": "2371027",
                        "name": "Yongli Hu"
                    },
                    {
                        "authorId": "1714354",
                        "name": "Baocai Yin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The generalization of this idea as a self-supervised task(also called auxiliary-task and pretext) is a relatively new paradigm in machine learning literature [14][2]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d9faf6c690309f68580a6b32c3bdcfea869bc36f",
                "externalIds": {
                    "MAG": "3136192076",
                    "DBLP": "journals/corr/abs-2103-09588",
                    "ArXiv": "2103.09588",
                    "CorpusId": 232258119
                },
                "corpusId": 232258119,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d9faf6c690309f68580a6b32c3bdcfea869bc36f",
                "title": "An Efficient Method for the Classification of Croplands in Scarce-Label Regions",
                "abstract": "Two of the main challenges for cropland classification by satellite time-series images are insufficient ground-truth data and inaccessibility of high-quality hyperspectral images for under-developed areas. Unlabeled medium-resolution satellite images are abundant, but how to benefit from them is an open question. We will show how to leverage their potential for cropland classification using self-supervised tasks. Self-supervision is an approach where we provide simple training signals for the samples, which are apparent from the data's structure. Hence, they are cheap to acquire and explain a simple concept about the data. We introduce three self-supervised tasks for cropland classification. They reduce epistemic uncertainty, and the resulting model shows superior accuracy in a wide range of settings compared to SVM and Random Forest. Subsequently, we use the self-supervised tasks to perform unsupervised domain adaptation and benefit from the labeled samples in other regions. It is crucial to know what information to transfer to avoid degrading the performance. We show how to automate the information selection and transfer process in cropland classification even when the source and target areas have a very different feature distribution. We improved the model by about 24% compared to a baseline architecture without any labeled sample in the target domain. Our method is amenable to gradual improvement, works with medium-resolution satellite images, and does not require complicated models. Code and data are available.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38784648",
                        "name": "H. Ghaffari"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[51] introduces a new contrastive loss to improve sample efficiency in Atari benchmark."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d2d3dd6188f98e5a9102bd50045df7f33b5f7104",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-08255",
                    "ArXiv": "2103.08255",
                    "DOI": "10.1109/IROS51168.2021.9636536",
                    "CorpusId": 232233778
                },
                "corpusId": 232233778,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d2d3dd6188f98e5a9102bd50045df7f33b5f7104",
                "title": "Sample-efficient Reinforcement Learning Representation Learning with Curiosity Contrastive Forward Dynamics Model*",
                "abstract": "Developing an agent in reinforcement learning (RL) that is capable of performing complex control tasks directly from high-dimensional observation such as raw pixels is a challenge as efforts still need to be made towards improving sample efficiency and generalization of RL algorithm. This paper considers a learning framework for a Curiosity Contrastive Forward Dynamics Model (CCFDM) to achieve a more sample-efficient RL based directly on raw pixels. CCFDM incorporates a forward dynamics model (FDM) and performs contrastive learning to train its deep convolutional neural network-based image encoder (IE) to extract conducive spatial and temporal information to achieve a more sample efficiency for RL. In addition, during training, CCFDM provides intrinsic rewards, produced based on FDM prediction error, and encourages the curiosity of the RL agent to improve exploration. The diverge and less-repetitive observations provided by both our exploration strategy and data augmentation available in contrastive learning improve not only the sample efficiency but also the generalization . Performance of existing model-free RL methods such as Soft Actor-Critic built on top of CCFDM outperforms prior state-of-the-art pixel-based RL methods on the DeepMind Control Suite benchmark.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2117824129",
                        "name": "Thanh Nguyen"
                    },
                    {
                        "authorId": "145491883",
                        "name": "T. Luu"
                    },
                    {
                        "authorId": "1680408",
                        "name": "Thang Vu"
                    },
                    {
                        "authorId": "145954697",
                        "name": "C. Yoo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In contrast with [RUMS18], who use a heuristic to find the agent\u2019s position from the screen\u2019s pixels, we use the Atari annotated RAM interface wrapper [ARO19].",
                "[ARO19] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C\u00f4t\u00e9, and R Devon Hjelm."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "246147e117a6178caebe86fe2b6b94488fa26a97",
                "externalIds": {
                    "ArXiv": "2103.07945",
                    "DBLP": "journals/corr/abs-2103-07945",
                    "CorpusId": 232232799
                },
                "corpusId": 232232799,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/246147e117a6178caebe86fe2b6b94488fa26a97",
                "title": "Learning One Representation to Optimize All Rewards",
                "abstract": "We introduce the forward-backward (FB) representation of the dynamics of a reward-free Markov decision process. It provides explicit near-optimal policies for any reward speci\ufb01ed a posteriori. During an unsupervised phase, we use reward-free interactions with the environment to learn two representations via off-the-shelf deep learning methods and temporal difference (TD) learning. In the test phase, a reward representation is estimated either from reward observations or an explicit reward description (e.g., a target state). The optimal policy for that reward is directly obtained from these representations, with no planning. We assume access to an exploration scheme or replay buffer for the \ufb01rst phase. The corresponding unsupervised loss is well-principled: if training is perfect, the policies obtained are provably optimal for any reward function. With imperfect training, the sub-optimality is proportional to the unsupervised approximation error. The FB representation learns long-range relationships between states and actions, via a predictive occupancy map, without having to synthesize states as in model-based approaches. This is a step towards learning controllable agents in arbitrary black-box stochastic environments. This approach compares well to goal-oriented RL algorithms on discrete and continuous mazes, pixel-based MsPacman, and the FetchReach virtual robot arm. We also illustrate how the agent can immediately adapt to new tasks beyond goal-oriented RL. 2",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145046554",
                        "name": "Ahmed Touati"
                    },
                    {
                        "authorId": "1734570",
                        "name": "Y. Ollivier"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Anand et al. (2019) provide a method for learning representations via loss functions that operate directly on the hidden layers of encoder neural networks."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "99280faafc147284bd5f77e6365077b9788063bc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-06398",
                    "ArXiv": "2103.06398",
                    "CorpusId": 232185628
                },
                "corpusId": 232185628,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/99280faafc147284bd5f77e6365077b9788063bc",
                "title": "Analyzing the Hidden Activations of Deep Policy Networks: Why Representation Matters",
                "abstract": "We analyze the hidden activations of neural network policies of deep reinforcement learning (RL) agents and show, empirically, that it\u2019s possible to know a-priori if a state representation will lend itself to fast learning. RL agents in high-dimensional states have two main learning burdens: (1) to learn an action-selection policy and (2) to learn to discern between useful and non-useful information in a given state. By learning a latent representation of these high-dimensional states with an auxiliary model, the latter burden is effectively removed, thereby leading to accelerated training progress. We examine this phenomenon across tasks in the PyBullet Kuka environment, where an agent must learn to control a robotic gripper to pick up an object. Our analysis reveals how neural network policies learn to organize their internal representation of the state space throughout training. The results from this analysis provide three main insights into how deep RL agents learn. First, a well-organized internal representation within the policy network is a prerequisite to learning good action-selection. Second, a poor initial representation can cause an unrecoverable collapse within a policy network. Third, a good initial representation allows an agent\u2019s policy network to organize its internal representation even before any training begins.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1914794749",
                        "name": "Trevor A. McInroe"
                    },
                    {
                        "authorId": "34322211",
                        "name": "Michael Spurrier"
                    },
                    {
                        "authorId": "117499339",
                        "name": "J. Sieber"
                    },
                    {
                        "authorId": "2052548516",
                        "name": "Stephen Conneely"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The evaluation metrics are the number of ram states visited using [2] and the downstream zero shot performance on Atari game."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0295df1b9d11e6e49b20119410fd755a4d7781af",
                "externalIds": {
                    "DBLP": "conf/nips/LiuA21",
                    "ArXiv": "2103.04551",
                    "CorpusId": 232146715
                },
                "corpusId": 232146715,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0295df1b9d11e6e49b20119410fd755a4d7781af",
                "title": "Behavior From the Void: Unsupervised Active Pre-Training",
                "abstract": "We introduce a new unsupervised pre-training method for reinforcement learning called APT, which stands for Active Pre-Training. APT learns behaviors and representations by actively searching for novel states in reward-free environments. The key novel idea is to explore the environment by maximizing a non-parametric entropy computed in an abstract representation space, which avoids challenging density modeling and consequently allows our approach to scale much better in environments that have high-dimensional observations (e.g., image observations). We empirically evaluate APT by exposing task-specific reward after a long unsupervised pre-training phase. In Atari games, APT achieves human-level performance on 12 games and obtains highly competitive performance compared to canonical fully supervised RL algorithms. On DMControl suite, APT beats all baselines in terms of asymptotic performance and data efficiency and dramatically improves performance on tasks that are extremely difficult to train from scratch.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143855835",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "127d6cd7da419cf02a5ae6e84e3035323509a61b",
                "externalIds": {
                    "MAG": "3112388352",
                    "DOI": "10.1016/j.rser.2020.110618",
                    "CorpusId": 230553183
                },
                "corpusId": 230553183,
                "publicationVenue": {
                    "id": "f9c45c04-838e-413a-9737-fc5d1344c88c",
                    "name": "Renewable & Sustainable Energy Reviews",
                    "type": "journal",
                    "alternate_names": [
                        "Renew  Sustain Energy Rev"
                    ],
                    "issn": "1364-0321",
                    "url": "https://www.journals.elsevier.com/renewable-and-sustainable-energy-reviews/#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/13640321"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/127d6cd7da419cf02a5ae6e84e3035323509a61b",
                "title": "Applications of reinforcement learning in energy systems",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "29649827",
                        "name": "A. Perera"
                    },
                    {
                        "authorId": "2197201",
                        "name": "Parameswaran Kamalaruban"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018) and local spatial structure (Anand et al., 2019) has been leveraged for state representation learning via contrastive losses.",
                "Contrastive learning has seen dramatic progress recently, and been introduced to learn state representation (Oord et al., 2018; Sermanet et al., 2018; Dwibedi et al., 2018; Aytar et al., 2018; Anand et al., 2019; Srinivas et al., 2020).",
                "Temporal structure (Sermanet et al., 2018; Aytar et al., 2018) and local spatial structure (Anand et al., 2019) has been leveraged for state representation learning via contrastive losses."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f37a60d5c352b322e0a1a1e852e980a1e9e903b3",
                "externalIds": {
                    "ArXiv": "2102.10960",
                    "DBLP": "conf/iclr/LiuZZQZLYL21",
                    "CorpusId": 231985599
                },
                "corpusId": 231985599,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f37a60d5c352b322e0a1a1e852e980a1e9e903b3",
                "title": "Return-Based Contrastive Representation Learning for Reinforcement Learning",
                "abstract": "Recently, various auxiliary tasks have been proposed to accelerate representation learning and improve sample efficiency in deep reinforcement learning (RL). However, existing auxiliary tasks do not take the characteristics of RL problems into consideration and are unsupervised. By leveraging returns, the most important feedback signals in RL, we propose a novel auxiliary task that forces the learnt representations to discriminate state-action pairs with different returns. Our auxiliary loss is theoretically justified to learn representations that capture the structure of a new form of state-action abstraction, under which state-action pairs with similar return distributions are aggregated together. In low data regime, our algorithm outperforms strong baselines on complex tasks in Atari games and DeepMind Control suite, and achieves even better performance when combined with existing auxiliary tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2158130076",
                        "name": "Guoqing Liu"
                    },
                    {
                        "authorId": "144114271",
                        "name": "Chuheng Zhang"
                    },
                    {
                        "authorId": "2004786514",
                        "name": "Li Zhao"
                    },
                    {
                        "authorId": "143826491",
                        "name": "Tao Qin"
                    },
                    {
                        "authorId": "151068900",
                        "name": "Jinhua Zhu"
                    },
                    {
                        "authorId": "2151967924",
                        "name": "Jian Li"
                    },
                    {
                        "authorId": "1708598",
                        "name": "Nenghai Yu"
                    },
                    {
                        "authorId": "2110264337",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "CPC (van den Oord et al., 2018), CPA|action (Guo et al., 2018), ST-DIM (Anand et al., 2019), and DRIML (Mazoure et al., 2020) apply different forms of temporal contrastive losses to learn predictions in a latent space.",
                "CPC [30], CPC|action [10], ST-DIM [1], DRIML [19], and ATC [25] apply different forms of temporal contrastive losses to learn predictions in a latent space."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "da397e74d9a3434b2bfa735f6ea8fe1ddd884b66",
                "externalIds": {
                    "DBLP": "conf/nips/ZhengVVLS21",
                    "ArXiv": "2102.04897",
                    "CorpusId": 231855442
                },
                "corpusId": 231855442,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/da397e74d9a3434b2bfa735f6ea8fe1ddd884b66",
                "title": "Learning State Representations from Random Deep Action-conditional Predictions",
                "abstract": "Our main contribution in this work is an empirical finding that random General Value Functions (GVFs), i.e., deep action-conditional predictions -- random both in what feature of observations they predict as well as in the sequence of actions the predictions are conditioned upon -- form good auxiliary tasks for reinforcement learning (RL) problems. In particular, we show that random deep action-conditional predictions when used as auxiliary tasks yield state representations that produce control performance competitive with state-of-the-art hand-crafted auxiliary tasks like value prediction, pixel control, and CURL in both Atari and DeepMind Lab tasks. In another set of experiments we stop the gradients from the RL part of the network to the state representation learning part of the network and show, perhaps surprisingly, that the auxiliary tasks alone are sufficient to learn state representations good enough to outperform an end-to-end trained actor-critic baseline. We opensourced our code at https://github.com/Hwhitetooth/random_gvfs.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109692193",
                        "name": "Zeyu Zheng"
                    },
                    {
                        "authorId": "2300921",
                        "name": "Vivek Veeriah"
                    },
                    {
                        "authorId": "50990874",
                        "name": "Risto Vuorio"
                    },
                    {
                        "authorId": "46328485",
                        "name": "Richard L. Lewis"
                    },
                    {
                        "authorId": "2108384183",
                        "name": "Satinder Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mutual information estimation [10] has inspired a number of successful uses for single-view data, such as deep infomax (DIM) [11], contrastive predictive coding (CPC) [12]) and for multi-view data, such as augmented multiscale DIM (AMDIM) [13], contrastive multiview coding (CMC) [14], SimCLR [15]) image classification, reinforcement learning (spatio-temporal DIM (ST-DIM) [16]) and zero-shot learning (class matching DIM (CM-DIM) [17], [18]).",
                "Further, AMDIM [13], ST-DIM [16], and CMDIM [17] are referred to as XX, which captures a CrossConvolution-to-Representation relation and as CC, which captures a Convolution-to-Convolution relation.",
                "infomax (DIM) [11], contrastive predictive coding (CPC) [12]) and for multi-view data, such as augmented multiscale DIM (AMDIM) [13], contrastive multiview coding (CMC) [14], SimCLR [15]) image classification, reinforcement learning (spatio-temporal DIM (ST-DIM) [16]) and zero-shot learning (class matching DIM (CM-DIM) [17], [18])."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ab94b63006bbde90bbff829ad6cd8435ea8a97dc",
                "externalIds": {
                    "ArXiv": "2012.13623",
                    "DBLP": "conf/ichi/FedorovSGLWDKBC21",
                    "DOI": "10.1109/ICHI52183.2021.00017",
                    "CorpusId": 232428595
                },
                "corpusId": 232428595,
                "publicationVenue": {
                    "id": "4c82b394-c401-4ce2-982e-7ab79de2bd24",
                    "name": "IEEE International Conference on Healthcare Informatics",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Healthc Informatics",
                        "ICHI"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ab94b63006bbde90bbff829ad6cd8435ea8a97dc",
                "title": "Self-Supervised Multimodal Domino: in Search of Biomarkers for Alzheimer\u2019s Disease",
                "abstract": "Sensory input from multiple sources is crucial for robust and coherent human perception. Different sources contribute complementary explanatory factors. Similarly, research studies often collect multimodal imaging data, each of which can provide shared and unique information. This observation motivated the design of powerful multimodal self-supervised representation-learning algorithms. In this paper, we unify recent work on multimodal self-supervised learning under a single framework. Observing that most self-supervised methods optimize similarity metrics between a set of model components, we propose a taxonomy of all reasonable ways to organize this process. We first evaluate models on toy multimodal MNIST datasets and then apply them to a multimodal neuroimaging dataset with Alzheimer\u2019s disease patients. We find that (1) multimodal contrastive learning has significant benefits over its unimodal counterpart, (2) the specific composition of multiple contrastive objectives is critical to performance on a downstream task, (3) maximization of the similarity between representations has a regularizing effect on a neural network, which can sometimes lead to reduced downstream performance but still reveal multimodal relations. Results show that the proposed approach outperforms previous self-supervised encoder-decoder methods based on canonical correlation analysis (CCA) or the mixture-of-experts multimodal variational autoEncoder (MMVAE) on various datasets with a linear evaluation protocol. Importantly, we find a promising solution to uncover connections between modalities through a jointly shared subspace that can help advance work in our search for neuroimaging biomarkers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "26920432",
                        "name": "A. Fedorov"
                    },
                    {
                        "authorId": "8118056",
                        "name": "Tristan Sylvain"
                    },
                    {
                        "authorId": "146605948",
                        "name": "E. Geenjaar"
                    },
                    {
                        "authorId": "39175553",
                        "name": "M. Luck"
                    },
                    {
                        "authorId": "1734221",
                        "name": "Lei Wu"
                    },
                    {
                        "authorId": "6659971",
                        "name": "T. DeRamus"
                    },
                    {
                        "authorId": "2042628415",
                        "name": "Alex Kirilin"
                    },
                    {
                        "authorId": "2042627897",
                        "name": "Dmitry Bleklov"
                    },
                    {
                        "authorId": "144048760",
                        "name": "V. Calhoun"
                    },
                    {
                        "authorId": "2122479",
                        "name": "S. Plis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "480d79512315aa379157796bffb626e86d6b03af",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-13619",
                    "ArXiv": "2012.13619",
                    "DOI": "10.1109/ISBI48211.2021.9434103",
                    "CorpusId": 235208369
                },
                "corpusId": 235208369,
                "publicationVenue": {
                    "id": "a38e0d3d-6929-4868-b4e4-af8bbacf711e",
                    "name": "IEEE International Symposium on Biomedical Imaging",
                    "type": "conference",
                    "alternate_names": [
                        "ISBI",
                        "International Symposium on Biomedical Imaging",
                        "Int Symp Biomed Imaging",
                        "IEEE Int Symp Biomed Imaging"
                    ],
                    "issn": "1945-7928",
                    "alternate_issns": [
                        "1945-8452"
                    ],
                    "url": "http://www.biomedicalimaging.org/",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/480d79512315aa379157796bffb626e86d6b03af",
                "title": "On Self-Supervised Multimodal Representation Learning: An Application To Alzheimer\u2019s Disease",
                "abstract": "Introspection of deep supervised predictive models trained on functional and structural brain imaging may uncover novel markers of Alzheimer\u2019s disease (AD). However, supervised training is prone to learning from spurious features (shortcut learning), impairing its value in the discovery process. Deep unsupervised and, recently, contrastive self-supervised approaches, not biased to classification, are better candidates for the task. Their multimodal options specifically offer additional regularization via modality interactions. This paper introduces a way to exhaustively consider multimodal architectures for a contrastive self-supervised fusion of fMRI and MRI of AD patients and controls. We show that this multimodal fusion results in representations that improve the downstream classification results for both modalities. We investigate the fused self-supervised features projected into the brain space and introduce a numerically stable way to do so.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "26920432",
                        "name": "A. Fedorov"
                    },
                    {
                        "authorId": "50789878",
                        "name": "L. Wu"
                    },
                    {
                        "authorId": "8118056",
                        "name": "Tristan Sylvain"
                    },
                    {
                        "authorId": "39175553",
                        "name": "M. Luck"
                    },
                    {
                        "authorId": "6659971",
                        "name": "T. DeRamus"
                    },
                    {
                        "authorId": "2042627897",
                        "name": "Dmitry Bleklov"
                    },
                    {
                        "authorId": "2122479",
                        "name": "S. Plis"
                    },
                    {
                        "authorId": "144048760",
                        "name": "V. Calhoun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Methods for accomplishing this goal are being developed in the emerging field of state representation learning (Anand et al., 2019; Higgins et al., 2018b; Jaderberg et al., 2016; Lesort et al., 2018; van denOord et al., 2019; Srinivas et al., 2020; Zhang et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0331398e6c976f8e47842caf0d92e3f9b28653f5",
                "externalIds": {
                    "MAG": "3112349766",
                    "DOI": "10.1016/j.neuron.2020.11.021",
                    "CorpusId": 229183487,
                    "PubMed": "33326755"
                },
                "corpusId": 229183487,
                "publicationVenue": {
                    "id": "7a61412a-9a9a-487d-9613-5a1fbd879c9d",
                    "name": "Neuron",
                    "type": "journal",
                    "issn": "0896-6273",
                    "url": "https://www.cell.com/neuron/home",
                    "alternate_urls": [
                        "http://www.cell.com/neuron/home",
                        "http://www.sciencedirect.com/science/journal/08966273",
                        "http://www.neuron.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0331398e6c976f8e47842caf0d92e3f9b28653f5",
                "title": "Using deep reinforcement learning to reveal how the brain encodes abstract state-space representations in high-dimensional environments",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1744848934",
                        "name": "Logan B. Cross"
                    },
                    {
                        "authorId": "40179618",
                        "name": "Jeffrey Cockburn"
                    },
                    {
                        "authorId": "1740159",
                        "name": "Yisong Yue"
                    },
                    {
                        "authorId": "101096038",
                        "name": "J. O\u2019Doherty"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0bf1a78aeefa158a80b23fd5b57a5586a32eb7c1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-02419",
                    "MAG": "3113014242",
                    "ArXiv": "2012.02419",
                    "CorpusId": 227305788
                },
                "corpusId": 227305788,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0bf1a78aeefa158a80b23fd5b57a5586a32eb7c1",
                "title": "Planning from Pixels using Inverse Dynamics Models",
                "abstract": "Learning task-agnostic dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn latent world models by learning to predict sequences of future actions conditioned on task completion. These task-conditioned models adaptively focus modeling capacity on task-relevant dynamics, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "73775191",
                        "name": "Keiran Paster"
                    },
                    {
                        "authorId": "1683896",
                        "name": "Sheila A. McIlraith"
                    },
                    {
                        "authorId": "2503659",
                        "name": "Jimmy Ba"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 CSS [1, 3]: Single-view CSS loss using Eq.",
                "Many current CSS approaches [3, 16, 1] first take a reference input I, a positive sample I, and n \u2212 1 negative samples Ik for 1 \u2264 k < n, and then minimize the loss",
                "By contrast, popular approaches to using CSS [16, 1, 21, 3] neither learn time-invariant components nor try to reconstruct the input images.",
                "ch so is to treat a video frame close to the one of interest as positive sample, and a temporally-distant one as negative sample [1, 21].",
                "While [7] exploits depth, other works [21, 1, 3] leverage CSS, with [21, 1] using it in single-view videos."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c92868e1c11ec4af15b9f62d0c098ac1bbbfef24",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-01511",
                    "MAG": "3107714383",
                    "CorpusId": 227254342
                },
                "corpusId": 227254342,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c92868e1c11ec4af15b9f62d0c098ac1bbbfef24",
                "title": "Unsupervised Learning on Monocular Videos for 3D Human Pose Estimation",
                "abstract": "In this paper, we introduce an unsupervised feature extraction method that exploits contrastive self-supervised (CSS) learning to extract rich latent vectors from single-view videos. Instead of simply treating the latent features of nearby frames as positive pairs and those of temporally-distant ones as negative pairs as in other CSS approaches, we explicitly separate each latent vector into a time-variant component and a time-invariant one. We then show that applying CSS only to the time-variant features, while also reconstructing the input and encouraging a gradual transition between nearby and away features yields a rich latent space, well-suited for human pose estimation. Our approach outperforms other unsupervised single-view methods and match the performance of multi-view techniques.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "25056820",
                        "name": "S. Honari"
                    },
                    {
                        "authorId": "2066226610",
                        "name": "Victor Constantin"
                    },
                    {
                        "authorId": "2933543",
                        "name": "Helge Rhodin"
                    },
                    {
                        "authorId": "2862871",
                        "name": "M. Salzmann"
                    },
                    {
                        "authorId": "153918727",
                        "name": "P. Fua"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, we diverge from standard CSS formulations [8], [10], [11], [12] in the two following aspects: 1) Instead of applying the con-",
                "CSS [10], [11]: Single-view CSS loss using Eq.",
                "While [22] takes depth as input in addition to RGB, other works [10], [11], [12] leverage CSS, with [12] and [11] using it for only single-view RGB videos.",
                "In our context, one way to do so is to treat a video frame close to the one of interest as positive, and a temporally-distant one as negative [11], [12].",
                "Given a reference input I, a positive sample I\u00fe, and n 1 negative samples I k for 1 k < n, standard CSS approaches [8], [10], [11] minimize the loss"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "959999e3895a244de4e81fb13820fd405f309dbb",
                "externalIds": {
                    "DBLP": "journals/pami/HonariCRSF23",
                    "ArXiv": "2012.01511",
                    "DOI": "10.1109/TPAMI.2022.3215307",
                    "CorpusId": 252970886,
                    "PubMed": "36251908"
                },
                "corpusId": 252970886,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/959999e3895a244de4e81fb13820fd405f309dbb",
                "title": "Temporal Representation Learning on Monocular Videos for 3D Human Pose Estimation",
                "abstract": "In this article we propose an unsupervised feature extraction method to capture temporal information on monocular videos, where we detect and encode subject of interest in each frame and leverage contrastive self-supervised (CSS) learning to extract rich latent vectors. Instead of simply treating the latent features of nearby frames as positive pairs and those of temporally-distant ones as negative pairs as in other CSS approaches, we explicitly disentangle each latent vector into a time-variant component and a time-invariant one. We then show that applying contrastive loss only to the time-variant features and encouraging a gradual transition on them between nearby and away frames while also reconstructing the input, extract rich temporal features, well-suited for human pose estimation. Our approach reduces error by about 50% compared to the standard CSS strategies, outperforms other unsupervised single-view methods and matches the performance of multi-view techniques. When 2D pose is available, our approach can extract even richer latent features and improve the 3D pose estimation accuracy, outperforming other state-of-the-art weakly supervised methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "25056820",
                        "name": "S. Honari"
                    },
                    {
                        "authorId": "2066226610",
                        "name": "Victor Constantin"
                    },
                    {
                        "authorId": "2933543",
                        "name": "Helge Rhodin"
                    },
                    {
                        "authorId": "2862871",
                        "name": "M. Salzmann"
                    },
                    {
                        "authorId": "153918727",
                        "name": "P. Fua"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "17d598b439c7e5e01a2e7939ac8b926e13923ea7",
                "externalIds": {
                    "DBLP": "conf/ssci/HoYQ20",
                    "DOI": "10.1109/SSCI47803.2020.9308301",
                    "CorpusId": 230997358
                },
                "corpusId": 230997358,
                "publicationVenue": {
                    "id": "8a9e9f3b-a025-473d-801e-72cdb0653d22",
                    "name": "IEEE Symposium Series on Computational Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Symp Ser Comput Intell",
                        "SSCI"
                    ],
                    "url": "http://www.ieee-ssci.org/"
                },
                "url": "https://www.semanticscholar.org/paper/17d598b439c7e5e01a2e7939ac8b926e13923ea7",
                "title": "Achieving Human Expert Level Time Performance for Atari Games \u2013 A Causal Learning Approach",
                "abstract": "In a previous paper, we have argued that what constitutes human-level performance for Atari games must include the measurements of both the score achieved as well as the time taken to learn to achieve it. When both of these measurements approach the level of humans, the method and system will also become practical as high score and short time performance for learning can lead to practical real-world applications. Other well-known approaches, such as DeepMind\u2019s Atari game player, achieve human-level scores but the time taken to learn to play the games is many orders of magnitudes slower than that of humans, leading to inapplicability in real-world scenarios beyond the games. In our previous paper, we demonstrated the ability of a causal learning framework to learn to play the Atari game, Space Invaders, at both the score and time performance levels of a human novice. In this paper, we show that an extension of this system reported earlier leads to the ability for the system to perform at both the score and time level of a human expert. A major innovation of this paper is a \u201cchanging goal tracking\u201d system that is not only applicable to Atari games such as Space Invaders, it is also applicable to general AI systems.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2014823",
                        "name": "Seng-Beng Ho"
                    },
                    {
                        "authorId": "2045602755",
                        "name": "Xiwen Yang"
                    },
                    {
                        "authorId": "151236413",
                        "name": "Therese Quieta"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[5] by introducing regression and non-linear probing.",
                "[5] formulate the task of predicting each state variable as separate 256-way classification problem, as each byte of RAM can represent 256 possible values, regardless of the nature of each variable.",
                "[5] introduced the Atari Annotated RAM Interface (AtariARI) to enable the evaluation of state representation",
                "A final approach is to \u201cprobe\u201d the learned representations by training small regression or classification models to predict ground-truth data from learned representations [5], [13].",
                "For our investigations, we assess the quality of the encoding of important state variables for each game by employing a novel evaluation method that probes the contents of the learnt state representations using ground truth state information provided by the Atari Annotated RAM Interface [5].",
                "To evaluate the quality of the learned representations we proposed and utilised novel extensions to an evaluation method that probes the representations using the AtariARI [5].",
                "Section III describes our extensions to the approach for evaluating state representation learning methods proposed in [5].",
                "[5] used such a method for evaluating learned representations for Atari games using linear classification probes (single layer neural networks)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ad4a4d7a85231ce6501b707c794a4c13828c001c",
                "externalIds": {
                    "DBLP": "conf/ivcnz/TupperN20",
                    "DOI": "10.1109/IVCNZ51579.2020.9290609",
                    "CorpusId": 229358759
                },
                "corpusId": 229358759,
                "publicationVenue": {
                    "id": "a30271f2-1619-44e5-a1fb-b1c91b224898",
                    "name": "Image and Vision Computing New Zealand",
                    "type": "conference",
                    "alternate_names": [
                        "IVCNZ",
                        "Image Vis Comput n z"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1807"
                },
                "url": "https://www.semanticscholar.org/paper/ad4a4d7a85231ce6501b707c794a4c13828c001c",
                "title": "Evaluating Learned State Representations for Atari",
                "abstract": "Deep reinforcement learning, the combination of deep learning and reinforcement learning, has enabled the training of agents that can solve complex tasks from visual inputs. However, these methods often require prohibitive amounts of computation to obtain successful results. To improve learning efficiency, there has been a renewed focus on separating state representation and policy learning. In this paper, we investigate the quality of state representations learned by different types of autoencoders, a popular class of neural networks used for representation learning. We assess not only the quality of the representations learned by undercomplete, variational, and disentangled variational autoencoders, but also how the quality of the learned representations is affected by changes in representation size. To accomplish this, we also present a new method for evaluating learned state representations for Atari games using the Atari Annotated RAM Interface. Our findings highlight differences in the quality of state representations learned by different types of autoencoders and their robustness to reduction in representation size. Our results also demonstrate the advantage of using more sophisticated evaluation methods over assessing reconstruction quality.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1486439160",
                        "name": "Adam Tupper"
                    },
                    {
                        "authorId": "2173139",
                        "name": "K. Neshatian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Comparison to pixel-based world models Aligned with prior studies of VAE losses on Atari [3], our attempts to use a world model based on pixel reconstruction (e."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3c521453773813e2c9424999d60b395d0d90ec08",
                "externalIds": {
                    "ArXiv": "2010.13146",
                    "MAG": "3093697810",
                    "DBLP": "journals/corr/abs-2010-13146",
                    "CorpusId": 225067660
                },
                "corpusId": 225067660,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3c521453773813e2c9424999d60b395d0d90ec08",
                "title": "XLVIN: eXecuted Latent Value Iteration Nets",
                "abstract": "Value Iteration Networks (VINs) have emerged as a popular method to incorporate planning algorithms within deep reinforcement learning, enabling performance improvements on tasks requiring long-range reasoning and understanding of environment dynamics. This came with several limitations, however: the model is not incentivised in any way to perform meaningful planning computations, the underlying state space is assumed to be discrete, and the Markov decision process (MDP) is assumed fixed and known. We propose eXecuted Latent Value Iteration Networks (XLVINs), which combine recent developments across contrastive self-supervised learning, graph representation learning and neural algorithmic reasoning to alleviate all of the above limitations, successfully deploying VIN-style models on generic environments. XLVINs match the performance of VIN-like models when the underlying MDP is discrete, fixed and known, and provides significant improvements to model-free baselines across three general MDP setups.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48860334",
                        "name": "Andreea Deac"
                    },
                    {
                        "authorId": "1742197495",
                        "name": "Petar Velivckovi'c"
                    },
                    {
                        "authorId": "2003637930",
                        "name": "Ognjen Milinkovi'c"
                    },
                    {
                        "authorId": "145180695",
                        "name": "Pierre-Luc Bacon"
                    },
                    {
                        "authorId": "152226504",
                        "name": "Jian Tang"
                    },
                    {
                        "authorId": "145888108",
                        "name": "Mladen Nikolic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ST-DIM [16] and CM-DIM [17] apply this in turn to reinforcement learning and zero-shot learning respectively."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5b1e80415745c583bdcb4b62005c6e9ea67d9c50",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-10593",
                    "ArXiv": "2010.10593",
                    "MAG": "3093887848",
                    "CorpusId": 224814303
                },
                "corpusId": 224814303,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5b1e80415745c583bdcb4b62005c6e9ea67d9c50",
                "title": "Cross-Modal Information Maximization for Medical Imaging: CMIM",
                "abstract": "In hospitals, data are siloed to specific information systems that make the same information available under different modalities such as the different medical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound, etc.) and their associated radiology reports. This offers unique opportunities to obtain and use at train-time those multiple views of the same information that might not always be available at test-time. \nIn this paper, we propose an innovative framework that makes the most of available data by learning good representations of a multi-modal input that are resilient to modality dropping at test-time, using recent advances in mutual information maximization. By maximizing cross-modal information at train time, we are able to outperform several state-of-the-art baselines in two different settings, medical image classification, and segmentation. In particular, our method is shown to have a strong impact on the inference-time performance of weaker modalities.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "8118056",
                        "name": "Tristan Sylvain"
                    },
                    {
                        "authorId": "3372012",
                        "name": "Francis Dutil"
                    },
                    {
                        "authorId": "90073647",
                        "name": "T. Berthier"
                    },
                    {
                        "authorId": "66141392",
                        "name": "L. Jorio"
                    },
                    {
                        "authorId": "39175553",
                        "name": "M. Luck"
                    },
                    {
                        "authorId": "88844399",
                        "name": "Devon Hjelm"
                    },
                    {
                        "authorId": "1751762",
                        "name": "Yoshua Bengio"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The corresponding objective function can cause the model to waste its capacity on reconstructing unimportant features such as static backgrounds and noise, while ignoring visually small but important details in its learned representation (Anand et al., 2019; Kipf et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1c3bbd65f768257b924d6ea1832d2e8f2ae56b15",
                "externalIds": {
                    "DBLP": "conf/icml/GondalJRBWS21",
                    "ArXiv": "2010.07093",
                    "CorpusId": 232110968
                },
                "corpusId": 232110968,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1c3bbd65f768257b924d6ea1832d2e8f2ae56b15",
                "title": "Function Contrastive Learning of Transferable Meta-Representations",
                "abstract": "Meta-learning algorithms adapt quickly to new tasks that are drawn from the same task distribution as the training tasks. The mechanism leading to fast adaptation is the conditioning of a downstream predictive model on the inferred representation of the task's underlying data generative process, or \\emph{function}. This \\emph{meta-representation}, which is computed from a few observed examples of the underlying function, is learned jointly with the predictive model. In this work, we study the implications of this joint training on the transferability of the meta-representations. Our goal is to learn meta-representations that are robust to noise in the data and facilitate solving a wide range of downstream tasks that share the same underlying functions. To this end, we propose a decoupled encoder-decoder approach to supervised meta-learning, where the encoder is trained with a contrastive objective to find a good representation of the underlying function. In particular, our training scheme is driven by the self-supervision signal indicating whether two sets of examples stem from the same function. Our experiments on a number of synthetic and real-world datasets show that the representations we obtain outperform strong baselines in terms of downstream performance and noise robustness, even when these baselines are trained in an end-to-end manner.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51214165",
                        "name": "Muhammad Waleed Gondal"
                    },
                    {
                        "authorId": "2114034081",
                        "name": "S. Joshi"
                    },
                    {
                        "authorId": "40050919",
                        "name": "Nasim Rahaman"
                    },
                    {
                        "authorId": "153125952",
                        "name": "Stefan Bauer"
                    },
                    {
                        "authorId": "36661824",
                        "name": "Manuel W\u00fcthrich"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A problem of this objective is that it can cause the model to waste its capacity on reconstructing unimportant features, such as static backgrounds, while ignoring the visually small but important details in its learned representation (Anand et al., 2019; Kipf et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "73e9ead9fd6911cb8fdce90f6613a3abda7525e0",
                "externalIds": {
                    "MAG": "3092916550",
                    "DBLP": "journals/corr/abs-2010-07093",
                    "CorpusId": 222341444
                },
                "corpusId": 222341444,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/73e9ead9fd6911cb8fdce90f6613a3abda7525e0",
                "title": "Function Contrastive Learning of Transferable Representations",
                "abstract": "Few-shot-learning seeks to find models that are capable of fast-adaptation to novel tasks. Unlike typical few-shot learning algorithms, we propose a contrastive learning method which is not trained to solve a set of tasks, but rather attempts to find a good representation of the underlying data-generating processes (\\emph{functions}). This allows for finding representations which are useful for an entire series of tasks sharing the same function. In particular, our training scheme is driven by the self-supervision signal indicating whether two sets of samples stem from the same underlying function. Our experiments on a number of synthetic and real-world datasets show that the representations we obtain can outperform strong baselines in terms of downstream performance and noise robustness, even when these baselines are trained in an end-to-end manner.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51214165",
                        "name": "Muhammad Waleed Gondal"
                    },
                    {
                        "authorId": "2114034081",
                        "name": "S. Joshi"
                    },
                    {
                        "authorId": "40050919",
                        "name": "Nasim Rahaman"
                    },
                    {
                        "authorId": "153125952",
                        "name": "Stefan Bauer"
                    },
                    {
                        "authorId": "36661824",
                        "name": "Manuel W\u00fcthrich"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4f0a8cad6d6a8d0397ad1bd35acce6458aa7164c",
                "externalIds": {
                    "MAG": "3096655658",
                    "ArXiv": "2010.05113",
                    "DBLP": "journals/access/Le-KhacHS20",
                    "DOI": "10.1109/ACCESS.2020.3031549",
                    "CorpusId": 222291214
                },
                "corpusId": 222291214,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4f0a8cad6d6a8d0397ad1bd35acce6458aa7164c",
                "title": "Contrastive Representation Learning: A Framework and Review",
                "abstract": "Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper, we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1994194269",
                        "name": "Ph\u00fac H. L\u00ea Kh\u1eafc"
                    },
                    {
                        "authorId": "30978009",
                        "name": "G. Healy"
                    },
                    {
                        "authorId": "1680223",
                        "name": "A. Smeaton"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[1] have proposed to maximize the mutual information between representations of two consecutive frames of the environment."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b33b735352c78743707f66e525f7cca65ff207b0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-02302",
                    "MAG": "3092161856",
                    "ArXiv": "2010.02302",
                    "CorpusId": 222140945
                },
                "corpusId": 222140945,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b33b735352c78743707f66e525f7cca65ff207b0",
                "title": "Latent World Models For Intrinsically Motivated Exploration",
                "abstract": "In this work we consider partially observable environments with sparse rewards. We present a self-supervised representation learning method for image-based observations, which arranges embeddings respecting temporal distance of observations. This representation is empirically robust to stochasticity and suitable for novelty detection from the error of a predictive forward model. We consider episodic and life-long uncertainties to guide the exploration. We propose to estimate the missing information about the environment with the world model, which operates in the learned latent space. As a motivation of the method, we analyse the exploration problem in a tabular Partially Observable Labyrinth. We demonstrate the method on image-based hard exploration environments from the Atari benchmark and report significant improvement with respect to prior work. The source code of the method and all the experiments is available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2064701324",
                        "name": "Aleksandr Ermolov"
                    },
                    {
                        "authorId": "1703601",
                        "name": "N. Sebe"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In future work, we want to incorporate methods for unsupervised state representation learning (Burgess et al., 2019; Anand et al., 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "560e9a8a1b027adbefb838ca337aa73af7998ca9",
                "externalIds": {
                    "MAG": "3090370023",
                    "ArXiv": "2010.01351",
                    "DBLP": "journals/corr/abs-2010-01351",
                    "CorpusId": 222134120
                },
                "corpusId": 222134120,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/560e9a8a1b027adbefb838ca337aa73af7998ca9",
                "title": "Disentangling causal effects for hierarchical reinforcement learning",
                "abstract": "Exploration and credit assignment under sparse rewards are still challenging problems. We argue that these challenges arise in part due to the intrinsic rigidity of operating at the level of actions. Actions can precisely define how to perform an activity but are ill-suited to describe what activity to perform. Instead, causal effects are inherently composable and temporally abstract, making them ideal for descriptive tasks. By leveraging a hierarchy of causal effects, this study aims to expedite the learning of task-specific behavior and aid exploration. Borrowing counterfactual and normality measures from causal literature, we disentangle controllable effects from effects caused by other dynamics of the environment. We propose CEHRL, a hierarchical method that models the distribution of controllable effects using a Variational Autoencoder. This distribution is used by a high-level policy to 1) explore the environment via random effect exploration so that novel effects are continuously discovered and learned, and to 2) learn task-specific behavior by prioritizing the effects that maximize a given reward function. In comparison to exploring with random actions, experimental results show that random effect exploration is a more efficient mechanism and that by assigning credit to few effects rather than many actions, CEHRL learns tasks more rapidly.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1396771299",
                        "name": "Oriol Corcoll"
                    },
                    {
                        "authorId": "144846212",
                        "name": "Raul Vicente"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2018), or used as an auxiliary task to study representations for high-dimensional data (Srinivas, Laskin, and Abbeel 2020; Anand et al. 2019).",
                "In RL, it has been used to extract reward signals in latent space (Sermanet et al. 2018; Dwibedi et al. 2018), or used as an auxiliary task to study representations for high-dimensional data (Srinivas, Laskin, and Abbeel 2020; Anand et al. 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "77be65bb396cb6309b6d03023c5f71203b3a39ea",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2009-13891",
                    "MAG": "3089433505",
                    "ArXiv": "2009.13891",
                    "DOI": "10.1609/aaai.v35i8.16914",
                    "CorpusId": 221996008
                },
                "corpusId": 221996008,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/77be65bb396cb6309b6d03023c5f71203b3a39ea",
                "title": "Towards Effective Context for Meta-Reinforcement Learning: an Approach based on Contrastive Learning",
                "abstract": "Context, the embedding of previous collected trajectories, is a powerful construct for Meta-Reinforcement Learning (Meta-RL) algorithms. By conditioning on an effective context, Meta-RL policies can easily generalize to new tasks within a few adaptation steps. We argue that improving the quality of context involves answering two questions: 1. How to train a compact and sufficient encoder that can embed the task-specific information contained in prior trajectories? 2. How to collect informative trajectories of which the corresponding context reflects the specification of tasks? To this end, we propose a novel Meta-RL framework called CCM (Contrastive learning augmented Context-based Meta-RL). We first focus on the contrastive nature behind different tasks and leverage it to train a compact and sufficient context encoder. Further, we train a separate exploration policy and theoretically derive a new information-gain-based objective which aims to collect informative trajectories in a few steps. Empirically, we evaluate our approaches on common benchmarks as well as several complex sparse-reward environments. The experimental results show that CCM outperforms state-of-the-art algorithms by addressing previously mentioned problems respectively.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "81875788",
                        "name": "Haotian Fu"
                    },
                    {
                        "authorId": "31190626",
                        "name": "Hongyao Tang"
                    },
                    {
                        "authorId": "40513470",
                        "name": "Jianye Hao"
                    },
                    {
                        "authorId": "40590308",
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "1443767933",
                        "name": "Xidong Feng"
                    },
                    {
                        "authorId": "2108821147",
                        "name": "Dong Li"
                    },
                    {
                        "authorId": "2046744137",
                        "name": "Wulong Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "An example of this is Spatiotemporal DeepInfomax (ST-DIM) [2], where two mutual information objective functions are defined.",
                "Whereas the ST-DIM section is based on Anand et al.\u2019s [2] previous work and aims to encode the high level game variables.",
                "\u2019s [2] previous work and aims to encode the high level game variables."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "161b1994efc445d1f05b3f59673f4b694dfa1f44",
                "externalIds": {
                    "DBLP": "conf/fdg/FergusonDKW20",
                    "MAG": "3088606284",
                    "DOI": "10.1145/3402942.3402960",
                    "CorpusId": 221764890
                },
                "corpusId": 221764890,
                "publicationVenue": {
                    "id": "9f0494a8-8595-4796-8a3e-68c47ccdfe81",
                    "name": "International Conference on Foundations of Digital Games",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Found Digit Game",
                        "Foundations of Digital Games",
                        "FDG",
                        "Found Digit Game"
                    ],
                    "url": "http://www.foundationsofdigitalgames.org/"
                },
                "url": "https://www.semanticscholar.org/paper/161b1994efc445d1f05b3f59673f4b694dfa1f44",
                "title": "Player Style Clustering without Game Variables",
                "abstract": "Player clustering when applied to the field of video games has several potential applications. For example, the evaluation of the composition of a player base or the generation of AI agents with identified playing styles. These agents can then be used for either the testing of new game content or used directly to enhance a player\u2019s gaming experience. Most current player clustering techniques focus on the use of internal game variables. This raises two main issues: (1) the availability of game variables, as source code access is required to log them and hence limits the data sources that can be used, and (2) the choice of game variables can introduce unintended bias in the types of play style extracted. In this work, a hybrid unsupervised frame encoder and a \u2018reference-based\u2019 clustering algorithm are both proposed and combined to allow clustering from raw game play videos. It is shown that the proposed methods are most beneficial when the types of play styles are unknown.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2020453671",
                        "name": "M. Ferguson"
                    },
                    {
                        "authorId": "1693696",
                        "name": "Sam Devlin"
                    },
                    {
                        "authorId": "2380005",
                        "name": "D. Kudenko"
                    },
                    {
                        "authorId": "39547552",
                        "name": "James Alfred Walker"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ATC requires a model to associate observations from nearby time steps within the same trajectory (Anand et al., 2019).",
                "ST-DIM (Anand et al., 2019) introduced various temporal, contrastive losses, including ones that operate on \u201clocal\u201d features from an intermediate layer within the encoder, without data augmentation.",
                "We also compare against Spatio-Temporal Deep InfoMax (ST-DIM), which uses temporal contrastive losses with \u201clocal\u201d features from an intermediate convolution layer to ensure attention to the whole screen; it was shown to produce detailed game-state knowledge when applied to individual frames (Anand et al., 2019).",
                "(Mazoure et al., 2020) provided extensive analysis pertaining to InfoNCE losses on functions of successive time steps in MDPs, including local features in their auxiliary loss (DRIML) similar to ST-DIM, and finally conducted experiments using global temporal contrast of augmented observations in the Procgen (Cobbe et al., 2019) environment.",
                "\u2026against Spatio-Temporal Deep InfoMax (ST-DIM), which uses temporal contrastive losses with \u201clocal\u201d features from an intermediate convolution layer to ensure attention to the whole screen; it was shown to produce detailed game-state knowledge when applied to individual frames (Anand et al., 2019)."
            ],
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "17985b57240bfaea02a6098a7a34e71e780180eb",
                "externalIds": {
                    "MAG": "3085605093",
                    "DBLP": "journals/corr/abs-2009-08319",
                    "ArXiv": "2009.08319",
                    "CorpusId": 221761383
                },
                "corpusId": 221761383,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/17985b57240bfaea02a6098a7a34e71e780180eb",
                "title": "Decoupling Representation Learning from Reinforcement Learning",
                "abstract": "In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47541311",
                        "name": "Adam Stooke"
                    },
                    {
                        "authorId": "3436470",
                        "name": "Kimin Lee"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    },
                    {
                        "authorId": "51093256",
                        "name": "M. Laskin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such contrastive predictive coding has been successfully used to find useful latent representations of ATARI games [13] and deformable objects [14]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "065d9b26d96af92b6245f867405953e612465a39",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2009-02185",
                    "ArXiv": "2009.02185",
                    "MAG": "3083364757",
                    "CorpusId": 221507690
                },
                "corpusId": 221507690,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/065d9b26d96af92b6245f867405953e612465a39",
                "title": "Naive Artificial Intelligence",
                "abstract": "In the cognitive sciences, it is common to distinguish between crystal intelligence, the ability to utilize knowledge acquired through past learning or experience and fluid intelligence, the ability to solve novel problems without relying on prior knowledge. Using this cognitive distinction between the two types of intelligence, extensively-trained deep networks that can play chess or Go exhibit crystal but not fluid intelligence. In humans, fluid intelligence is typically studied and quantified using intelligence tests. Previous studies have shown that deep networks can solve some forms of intelligence tests, but only after extensive training. Here we present a computational model that solves intelligence tests without any prior training. This ability is based on continual inductive reasoning, and is implemented by deep unsupervised latent-prediction networks. Our work demonstrates the potential fluid intelligence of deep networks. Finally, we propose that the computational principles underlying our approach can be used to model fluid intelligence in the cognitive sciences.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2066597096",
                        "name": "T. Barak"
                    },
                    {
                        "authorId": "1929009518",
                        "name": "Yehonatan Avidan"
                    },
                    {
                        "authorId": "2934154",
                        "name": "Y. Loewenstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Simpler probing tasks (Belinkov and Glass, 2019; Jawahar et al., 2019; Anand et al., 2019) test the performance of classifiers that train on the",
                "Simpler probing tasks (Belinkov and Glass, 2019; Jawahar et al., 2019; Anand et al., 2019) test the performance of classifiers that train on the representation learnt by a model.",
                "Due to the similarity between RL and dialogue, we draw inspirations from Anand et al. (2019)\u2019s probing tasks on game playing agent.",
                "Anand et al. (2019) learn state representation for an RL agent in an unsupervised setting and introduce a set of probing tasks to evaluate the representation learnt by agents."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "39e398b2db7e4f43e05544a9c9834e59ae8c9d11",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2008-10427",
                    "MAG": "3080399309",
                    "ArXiv": "2008.10427",
                    "CorpusId": 221266021
                },
                "corpusId": 221266021,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/39e398b2db7e4f43e05544a9c9834e59ae8c9d11",
                "title": "How To Evaluate Your Dialogue System: Probe Tasks as an Alternative for Token-level Evaluation Metrics",
                "abstract": "Though generative dialogue modeling is widely seen as a language modeling task, the task demands an agent to have a complex natural language understanding of its input text to carry a meaningful interaction with an user. The automatic metrics used evaluate the quality of the generated text as a proxy to the holistic interaction of the agent. Such metrics were earlier shown to not correlate with the human judgement. In this work, we observe that human evaluation of dialogue agents can be inconclusive due to the lack of sufficient information for appropriate evaluation. The automatic metrics are deterministic yet shallow and human evaluation can be relevant yet inconclusive. To bridge this gap in evaluation, we propose designing a set of probing tasks to evaluate dialogue models. The hand-crafted tasks are aimed at quantitatively evaluating a generative dialogue model's understanding beyond the token-level evaluation on the generated text. The probing tasks are deterministic like automatic metrics and requires human judgement in their designing; benefiting from the best of both worlds. With experiments on probe tasks we observe that, unlike RNN based architectures, transformer model may not be learning to comprehend the input text despite its generated text having higher overlap with the target text.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "32899078",
                        "name": "Prasanna Parthasarathi"
                    },
                    {
                        "authorId": "145134886",
                        "name": "Joelle Pineau"
                    },
                    {
                        "authorId": "123607932",
                        "name": "Sarath Chandar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "No pre-training 78\u00b1 4 79\u00b1 7 48\u00b1 4 12\u00b1 1 57\u00b1 1 37\u00b1 4 10\u00b1 1 18\u00b1 2 Image Net 86\u00b1 1 89\u00b1 3 66\u00b1 1 12\u00b1 0 21\u00b1 3 19\u00b1 3 13\u00b1 5 23\u00b1 2 Contrastive (ST-DIM) 73\u00b1 2 84\u00b1 4 71\u00b1 3 10\u00b1 1 66\u00b1 5 49\u00b1 2 17\u00b1 0 21\u00b1 1 Forward 68\u00b1 3 84\u00b1 3 68\u00b1 8 9\u00b1 1 49\u00b1 5 37\u00b1 3 18\u00b1 5 13\u00b1 2 Inverse 73\u00b1 4 82\u00b1 2 61\u00b1 3 8\u00b1 1 83\u00b1 2 67\u00b1 8 26\u00b1 8 8\u00b1 0 Behavior Cloning (BC) 91\u00b1 1 91\u00b1 4 68\u00b1 5 8\u00b1 1 83\u00b1 3 61\u00b1 4 25\u00b1 4 8\u00b1 1 CILRS 0.8.4 [11] 97\u00b1 2 83\u00b1 0 42\u00b1 2 47 66\u00b1 2 49\u00b1 5 23\u00b1 1 64 LBC [34] 97\u00b1 1 93\u00b1 1 71\u00b1 5 N/A 100\u00b1 0 94\u00b1 3 51\u00b1 3 N/A\nTable 3: Comparison of action-based pre-training with baselines (top) and other methods from the literature (bottom).",
                "Further, our approach, improves over other recent pre-training proposals such as contrastive methods [16] and even over ImageNet (supervised) pre-training.",
                "The constrastive method, ST-DIM, shows promising results for the binary affordances but results on very poorly relative angle estimations.",
                "In fact, we include in our study a recent contrastive method, ST-DIM, designed in the context of playing Atari games [16], adapted\nto actions required for driving.",
                "In short, ST-DIM is trained to answer if two frames are consecutive or not, without any action-related information\nBinary Affordances Relative Angle (\u03c8t) Pre-training Pedestrian (hp) Vehicle (hv) Red T.L. (hr) Left Turn Straight Right Turn\nNo pre-training 26\u00b1 0 50\u00b1 1 42\u00b1 0 11.38\u00b1 0.18 1.85\u00b1 0.03 24.68\u00b1 0.03 ImageNet 37\u00b1 2 75\u00b1 0 47\u00b1 0 11.69\u00b1 0.57 2.83\u00b1 0.07 25.55\u00b1 0.10 Contrastive (ST-DIM) 47\u00b1 1 53\u00b1 0 53\u00b1 0 10.43\u00b1 0.21 2.62\u00b1 0.03 18.75\u00b1 0.23 Forward 50\u00b1 0 63\u00b1 0 60\u00b1 0 5.35\u00b1 0.03 0.52\u00b1 0.00 6.61\u00b1 0.03 Inverse 49\u00b1 0 78\u00b1 0 70\u00b1 0 3.57\u00b1 0.03 0.46\u00b1 0.00 3.78\u00b1 0.06 Behavior Cloning (BC) 47\u00b1 0 81\u00b1 0 75\u00b1 0 4.89\u00b1 0.03 1.24\u00b1 0.03 6.25\u00b1 0.10\nTable 1: Linear probing results.",
                "For ST-DIM, as expected, the difference between random and expert policy is smaller since it is not based on action.",
                "For better analysis, we divide the relative heading angle into three cases, left turn, straight and right turn,\nBinary Affordances Relative Angle (\u03c8t) Pre-training Pedestrian (hp) Vehicle (hv) Red T.L. (hr) Left Turn Straight Right Turn\nNo pre-training 26\u00b1 0 50\u00b1 1 42\u00b1 0 11.38\u00b1 0.18 1.85\u00b1 0.03 24.68\u00b1 0.03 Contrastive (ST-DIM) 41\u00b1 0 62\u00b1 1 63\u00b1 1 9.01\u00b1 0.46 2.77\u00b1 0.18 18.37\u00b1 0.45 Contrastive Random (ST-DIM) 39\u00b1 1 73\u00b1 1 47\u00b1 0 9.70\u00b1 0.41 2.98\u00b1 0.11 15.89\u00b1 0.41 Forward 50\u00b1 0 51\u00b1 0 58\u00b1 0 4.87\u00b1 0.00 0.52\u00b1 0.00 6.07\u00b1 0.06 Forward Random 20\u00b1 1 38\u00b1 0 16\u00b1 0 11.54\u00b1 0.03 1.20\u00b1 0.00 19.14\u00b1 0.00 Inverse 45\u00b1 0 66\u00b1 0 73\u00b1 0 3.02\u00b1 0.03 0.42\u00b1 0.03 5.06\u00b1 0.17 Inverse Random 26\u00b1 0 49\u00b1 0 59\u00b1 0 8.50\u00b1 0.53 1.45\u00b1 0.03 13.14\u00b1 0.34\nTable 2: Linear probing results comparing encoders trained with random policy training data versus expert demonstration data.",
                "Moreover, for the Inverse, Forward, and ST-DIM strategies, we have included seldom variants which require to collect additional \u223c 20 hours of image sequences in T1.",
                "Both models also clearly outperform the constrastive-based baseline (ST-DIM) in new town.",
                "In fact, we include in our study a recent contrastive method, ST-DIM, designed in the context of playing Atari games [16], adapted",
                "We will see how action-based representation learning outperforms ST-DIM as a self-supervised representation learning strategy to infer affordances.",
                "In addition, we have incorporated ST-DIM [16], a contrastive representation learning baseline used by agents playing Atari games."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2eb64c9dc6feea15e4c7b913e004e3cc520793b4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2008-09417",
                    "MAG": "3081354920",
                    "ArXiv": "2008.09417",
                    "CorpusId": 221246443
                },
                "corpusId": 221246443,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2eb64c9dc6feea15e4c7b913e004e3cc520793b4",
                "title": "Action-Based Representation Learning for Autonomous Driving",
                "abstract": "Human drivers produce a vast amount of data which could, in principle, be used to improve autonomous driving systems. Unfortunately, seemingly straightforward approaches for creating end-to-end driving models that map sensor data directly into driving actions are problematic in terms of interpretability, and typically have significant difficulty dealing with spurious correlations. Alternatively, we propose to use this kind of action-based driving data for learning representations. Our experiments show that an affordance-based driving model pre-trained with this approach can leverage a relatively small amount of weakly annotated imagery and outperform pure end-to-end driving models, while being more interpretable. Further, we demonstrate how this strategy outperforms previous methods based on learning inverse dynamics models as well as other methods based on heavy human supervision (ImageNet).",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2122803588",
                        "name": "Yi Xiao"
                    },
                    {
                        "authorId": "2962623",
                        "name": "Felipe Codevilla"
                    },
                    {
                        "authorId": "1972076",
                        "name": "C. Pal"
                    },
                    {
                        "authorId": "2110294430",
                        "name": "Antonio M. L\u00f3pez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We see, the pre-trained MILC models outperform NPT and also ST-DIM based pre-trained models.",
                "Note, with very few samples, models based on the pre-trained MILC (FPT and UFPT) outperform the un-pre-trained models (NPT), ST-DIM models, autoencoder based models.",
                "We compare MILC with ST-DIM based pre-training shown in [24].",
                "ST-DIM [1] has been used for pre-training on unrelated data with subsequent use for classification [24].",
                "ST-DIM based pre-training model [24] performs reasonably well compared to autoencoder and NPT models, however, MILC steadily outperforms ST-DIM.",
                "They were shown to benefit structural MRI analysis [10], learn useful representations from the frames in Atari games [1] and for speaker identification [28]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e73f179b9348335cfaa00e1270ca9c6e5a607f22",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-16041",
                    "MAG": "3046675286",
                    "ArXiv": "2007.16041",
                    "DOI": "10.1007/978-3-030-59728-3_40",
                    "CorpusId": 220919981
                },
                "corpusId": 220919981,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e73f179b9348335cfaa00e1270ca9c6e5a607f22",
                "title": "Whole MILC: generalizing learned dynamics across tasks, datasets, and populations",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "153339670",
                        "name": "Usman Mahmood"
                    },
                    {
                        "authorId": "2116362588",
                        "name": "Md. Mahfuzur Rahman"
                    },
                    {
                        "authorId": "26920432",
                        "name": "A. Fedorov"
                    },
                    {
                        "authorId": "144482146",
                        "name": "N. Lewis"
                    },
                    {
                        "authorId": "2895825",
                        "name": "Z. Fu"
                    },
                    {
                        "authorId": "144048760",
                        "name": "V. Calhoun"
                    },
                    {
                        "authorId": "2122479",
                        "name": "S. Plis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Contrastive learning is a\npopular type of self-supervised learning, which has proven effective for video [Gordon et al., 2020, Knights et al., 2020] and reinforcement learning tasks [Oord et al., 2018, Anand et al., 2019, Srinivas et al., 2020, Mazoure et al., 2020, Schwarzer et al., 2020]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "04ea5fe569cd48c588b7fe420965016ed4ddd312",
                "externalIds": {
                    "ArXiv": "2007.13278",
                    "DBLP": "journals/corr/abs-2007-13278",
                    "MAG": "3045266435",
                    "CorpusId": 220794015
                },
                "corpusId": 220794015,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/04ea5fe569cd48c588b7fe420965016ed4ddd312",
                "title": "Representation Learning with Video Deep InfoMax",
                "abstract": "Self-supervised learning has made unsupervised pretraining relevant again for difficult computer vision tasks. The most effective self-supervised methods involve prediction tasks based on features extracted from diverse views of the data. DeepInfoMax (DIM) is a self-supervised method which leverages the internal structure of deep networks to construct such views, forming prediction tasks between local features which depend on small patches in an image and global features which depend on the whole image. In this paper, we extend DIM to the video domain by leveraging similar structure in spatio-temporal networks, producing a method we call Video Deep InfoMax(VDIM). We find that drawing views from both natural-rate sequences and temporally-downsampled sequences yields results on Kinetics-pretrained action recognition tasks which match or outperform prior state-of-the-art methods that use more costly large-time-scale transformer models. We also examine the effects of data augmentation and fine-tuning methods, accomplishingSoTA by a large margin when training only on the UCF-101 dataset.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40482726",
                        "name": "R. Devon Hjelm"
                    },
                    {
                        "authorId": "143902541",
                        "name": "Philip Bachman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[2] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C\u00f4t\u00e9, and R Devon Hjelm.",
                "The linear probing setting is widely studied in literature [1, 21, 10] and applied in practice [4, 2]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3ac13e3dc212fba60835b8917e02a3179eb3465a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-12446",
                    "ArXiv": "2007.12446",
                    "MAG": "3044709878",
                    "CorpusId": 220769218
                },
                "corpusId": 220769218,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3ac13e3dc212fba60835b8917e02a3179eb3465a",
                "title": "Transferred Discrepancy: Quantifying the Difference Between Representations",
                "abstract": "Understanding what information neural networks capture is an essential problem in deep learning, and studying whether different models capture similar features is an initial step to achieve this goal. Previous works sought to define metrics over the feature matrices to measure the difference between two models. However, different metrics sometimes lead to contradictory conclusions, and there has been no consensus on which metric is suitable to use in practice. In this work, we propose a novel metric that goes beyond previous approaches. Recall that one of the most practical scenarios of using the learned representations is to apply them to downstream tasks. We argue that we should design the metric based on a similar principle. For that, we introduce the transferred discrepancy (TD), a new metric that defines the difference between two representations based on their downstream-task performance. Through an asymptotic analysis, we show how TD correlates with downstream tasks and the necessity to define metrics in such a task-dependent fashion. In particular, we also show that under specific conditions, the TD metric is closely related to previous metrics. Our experiments show that TD can provide fine-grained information for varied downstream tasks, and for the models trained from different initializations, the learned features are not the same in terms of downstream-task predictions. We find that TD may also be used to evaluate the effectiveness of different training strategies. For example, we demonstrate that the models trained with proper data augmentations that improve the generalization capture more similar features in terms of TD, while those with data augmentations that hurt the generalization will not. This suggests a training strategy that leads to more robust representation also trains models that generalize better.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "152134566",
                        "name": "Yunzhen Feng"
                    },
                    {
                        "authorId": "3265543",
                        "name": "Runtian Zhai"
                    },
                    {
                        "authorId": "1391126980",
                        "name": "Di He"
                    },
                    {
                        "authorId": "24952249",
                        "name": "Liwei Wang"
                    },
                    {
                        "authorId": "145496882",
                        "name": "Bin Dong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is different from some other approaches [1, 30] that use contrastive methods (e.",
                "This work falls in another group, where future prediction is used as an auxiliary or representation learning method for model-free RL agents [1, 10, 14, 22, 29, 30, 33, 34, 35, 37]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4470a1cb348dc15bcec0020930dec6a1d3cb4376",
                "externalIds": {
                    "DBLP": "conf/nips/LeeFLGLCG20",
                    "ArXiv": "2007.12401",
                    "MAG": "3099621306",
                    "CorpusId": 220769202
                },
                "corpusId": 220769202,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4470a1cb348dc15bcec0020930dec6a1d3cb4376",
                "title": "Predictive Information Accelerates Learning in RL",
                "abstract": "The Predictive Information is the mutual information between the past and the future, I(X_past; X_future). We hypothesize that capturing the predictive information is useful in RL, since the ability to model what will happen next is necessary for success on many tasks. To test our hypothesis, we train Soft Actor-Critic (SAC) agents from pixels with an auxiliary task that learns a compressed representation of the predictive information of the RL environment dynamics using a contrastive version of the Conditional Entropy Bottleneck (CEB) objective. We refer to these as Predictive Information SAC (PI-SAC) agents. We show that PI-SAC agents can substantially improve sample efficiency over challenging baselines on tasks from the DM Control suite of continuous control environments. We evaluate PI-SAC agents by comparing against uncompressed PI-SAC agents, other compressed and uncompressed agents, and SAC agents directly trained from pixels. Our implementation is given on GitHub.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1863953",
                        "name": "Kuang-Huei Lee"
                    },
                    {
                        "authorId": "2057616290",
                        "name": "Ian Fischer"
                    },
                    {
                        "authorId": "1738283984",
                        "name": "Anthony Z. Liu"
                    },
                    {
                        "authorId": "1857914",
                        "name": "Yijie Guo"
                    },
                    {
                        "authorId": "1697141",
                        "name": "Honglak Lee"
                    },
                    {
                        "authorId": "1729041",
                        "name": "J. Canny"
                    },
                    {
                        "authorId": "1687120",
                        "name": "S. Guadarrama"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following (Anand et al., 2019), we train our\nmodel with 100,000 frames acquired with a random agent on the Atari games; an additional 50,000 frames are used for training and testing the evaluation probes.",
                "As a result, we aim to learn structured, object-centric slot representations harnessing time and using a self-supervised time-contrastive signal similar to (Anand et al., 2019; Hyvarinen & Morioka, 2017) to learn each object\u2019s representation, but also a \u201cslot contrastive\u201d signal as an attempt to force each slot to capture a unique object compared to the other slots.",
                "This is the intuition that motivates time-contrastive losses (Hyvarinen & Morioka, 2017; Anand et al., 2019; Sermanet et al., 2018); learning state representations that make it easy to predict the temporal distance between states, will potentially ensure that these representations capture time\u2026",
                "However, most of these techniques involve generative models, which have two issues: wasted capacity on modelling spurious background pixels (Oord et al., 2018) and inability to capture small objects (Anand et al., 2019).",
                "This is the intuition that motivates time-contrastive losses (Hyvarinen & Morioka, 2017; Anand et al., 2019; Sermanet et al., 2018); learning state representations that make it easy to predict the temporal distance between states, will potentially ensure that these representations capture time dependent features.",
                "Slot Accuracy For slot accuracy, we use linear probing, a technique commonly used in self-supervised learning (Anand et al., 2019; Hjelm et al., 2018; Chen et al., 2020) and disentangling (Locatello et al.",
                ", 2018) and contrastive approaches (Hyvarinen & Morioka, 2017; Oord et al., 2018; Anand et al., 2019) have begun to harness time in their self-supervised signal.",
                "The architectural details of all models are similar to (Anand et al., 2019).",
                "As a result, many self-supervised pretext approaches (Misra et al., 2016; Aytar et al., 2018) and contrastive approaches (Hyvarinen & Morioka, 2017; Oord et al., 2018; Anand et al., 2019) have begun to harness time in their self-supervised signal.",
                "\u2026we aim to learn structured, object-centric slot representations harnessing time and using a self-supervised time-contrastive signal similar to (Anand et al., 2019; Hyvarinen & Morioka, 2017) to learn each object\u2019s representation, but also a \u201cslot contrastive\u201d signal as an attempt to force\u2026",
                "For evaluation, we use labels from the AtariARI dataset (Anand et al., 2019), restricting ourselves to labels that correspond to the x or y coordinates of objects.",
                ", 2018) and inability to capture small objects (Anand et al., 2019).",
                "Slot Accuracy For slot accuracy, we use linear probing, a technique commonly used in self-supervised learning (Anand et al., 2019; Hjelm et al., 2018; Chen et al., 2020) and disentangling (Locatello et al., 2018)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "148af1bdbd8a93e085b34827d88994949be1f771",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-09294",
                    "ArXiv": "2007.09294",
                    "MAG": "3042343816",
                    "CorpusId": 220646882
                },
                "corpusId": 220646882,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/148af1bdbd8a93e085b34827d88994949be1f771",
                "title": "Slot Contrastive Networks: A Contrastive Approach for Representing Objects",
                "abstract": "Unsupervised extraction of objects from low-level visual data is an important goal for further progress in machine learning. Existing approaches for representing objects without labels use structured generative models with static images. These methods focus a large amount of their capacity on reconstructing unimportant background pixels, missing low contrast or small objects. Conversely, we present a new method that avoids losses in pixel space and over-reliance on the limited signal a static image provides. Our approach takes advantage of objects' motion by learning a discriminative, time-contrastive loss in the space of slot representations, attempting to force each slot to not only capture entities that move, but capture distinct objects from the other slots. Moreover, we introduce a new quantitative evaluation metric to measure how \"diverse\" a set of slot vectors are, and use it to evaluate our model on 20 Atari games.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3159503",
                        "name": "E. Racah"
                    },
                    {
                        "authorId": "123607932",
                        "name": "Sarath Chandar"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018), ST-DIM (Anand et al., 2019) and DRIML (Mazoure et al.",
                "CPC (Oord et al., 2018), CPC|Action (Guo et al., 2018), ST-DIM (Anand et al., 2019) and DRIML (Mazoure et al., 2020) propose to optimize various temporal contrastive losses in reinforcement learning environments."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc",
                "externalIds": {
                    "DBLP": "conf/iclr/SchwarzerAGHCB21",
                    "ArXiv": "2007.05929",
                    "CorpusId": 222163237
                },
                "corpusId": 222163237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc",
                "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations",
                "abstract": "While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, Self-Predictive Representations(SPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. The code associated with this work is available at https://github.com/mila-iqia/spr",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51881243",
                        "name": "Max Schwarzer"
                    },
                    {
                        "authorId": "12679121",
                        "name": "Ankesh Anand"
                    },
                    {
                        "authorId": "46186660",
                        "name": "Rishab Goel"
                    },
                    {
                        "authorId": "40482726",
                        "name": "R. Devon Hjelm"
                    },
                    {
                        "authorId": "1760871",
                        "name": "Aaron C. Courville"
                    },
                    {
                        "authorId": "143902541",
                        "name": "Philip Bachman"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018), ST-DIM (Anand et al., 2019) and DRIML (Mazoure et al.",
                "CPC (Oord et al., 2018), CPC|Action (Guo et al., 2018), ST-DIM (Anand et al., 2019) and DRIML (Mazoure et al., 2020) propose to optimize various temporal contrastive losses in reinforcement learning environments."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f7400c6c7a75d7559ea3cc996c1da7259a88fb61",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-05929",
                    "MAG": "3041890730",
                    "CorpusId": 220496194
                },
                "corpusId": 220496194,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f7400c6c7a75d7559ea3cc996c1da7259a88fb61",
                "title": "Data-Efficient Reinforcement Learning with Momentum Predictive Representations",
                "abstract": "While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, Momentum Predictive Representations (MPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters, and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.444 on Atari in a setting limited to 100K steps of environment interaction, which is a 66% relative improvement over the previous state-of-the-art. Moreover, even in this limited data regime, MPR exceeds expert human scores on 6 out of 26 games.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51881243",
                        "name": "Max Schwarzer"
                    },
                    {
                        "authorId": "12679121",
                        "name": "Ankesh Anand"
                    },
                    {
                        "authorId": "46186660",
                        "name": "Rishab Goel"
                    },
                    {
                        "authorId": "40482726",
                        "name": "R. Devon Hjelm"
                    },
                    {
                        "authorId": "1760871",
                        "name": "Aaron C. Courville"
                    },
                    {
                        "authorId": "143902541",
                        "name": "Philip Bachman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the process of improving sample efficiency, we address several important questions over prior work in auxiliary self-supervised learning, from both the supervised [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] and reinforcement learning paradigms [16, 17, 18, 19, 20, 21, 22, 23, 24, 25].",
                "ing [17, 19], or spatio-temporal mutual information maximization [20, 23] derive supervision from the agent\u2019s own experience.",
                "In contrast to prior work, which focus on simpler and non-photorealistic environments [17, 19, 20, 21, 22], we focus on visually complex, photorealistic environments from the Gibson 3D scans [30]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "33b5a3f31518c1ba8773dd2a996e5b77ed6cdbd7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-04561",
                    "MAG": "3041621552",
                    "ArXiv": "2007.04561",
                    "CorpusId": 220424436
                },
                "corpusId": 220424436,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/33b5a3f31518c1ba8773dd2a996e5b77ed6cdbd7",
                "title": "Auxiliary Tasks Speed Up Learning PointGoal Navigation",
                "abstract": "PointGoal Navigation is an embodied task that requires agents to navigate to a specified point in an unseen environment. Wijmans et al. showed that this task is solvable but their method is computationally prohibitive, requiring 2.5 billion frames and 180 GPU-days. In this work, we develop a method to significantly increase sample and time efficiency in learning PointNav using self-supervised auxiliary tasks (e.g. predicting the action taken between two egocentric observations, predicting the distance between two observations from a trajectory,etc.).We find that naively combining multiple auxiliary tasks improves sample efficiency,but only provides marginal gains beyond a point. To overcome this, we use attention to combine representations learnt from individual auxiliary tasks. Our best agent is 5x faster to reach the performance of the previous state-of-the-art, DD-PPO, at 40M frames, and improves on DD-PPO's performance at40M frames by 0.16 SPL. Our code is publicly available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1801888368",
                        "name": "Joel Ye"
                    },
                    {
                        "authorId": "1746610",
                        "name": "Dhruv Batra"
                    },
                    {
                        "authorId": "8405939",
                        "name": "Erik Wijmans"
                    },
                    {
                        "authorId": "2313517",
                        "name": "Abhishek Das"
                    }
                ]
            }
        },
        {
            "contexts": [
                "of a sequence of T = 4 time-steps times the number of features provided by the ATARIARI interface [1].",
                "This could be alleviated by using predictive models with strong inductive biases or unsupervised representation learners, such as deep-infomax [1] or similar noise-contrastive methods."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2a1c874abc278d66927c05123414b71fd7224266",
                "externalIds": {
                    "ArXiv": "2007.04862",
                    "MAG": "3041531722",
                    "DBLP": "journals/corr/abs-2007-04862",
                    "CorpusId": 220425342
                },
                "corpusId": 220425342,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2a1c874abc278d66927c05123414b71fd7224266",
                "title": "Attention or memory? Neurointerpretable agents in space and time",
                "abstract": "In neuroscience, attention has been shown to bidirectionally interact with reinforcement learning (RL) processes. This interaction is thought to support dimensionality reduction of task representations, restricting computations to relevant features. However, it remains unclear whether these properties can translate into real algorithmic advantages for artificial agents, especially in dynamic environments. We design a model incorporating a self-attention mechanism that implements task-state representations in semantic feature-space, and test it on a battery of Atari games. To evaluate the agent's selective properties, we add a large volume of task-irrelevant features to observations. In line with neuroscience predictions, self-attention leads to increased robustness to noise compared to benchmark models. Strikingly, this self-attention mechanism is general enough, such that it can be naturally extended to implement a transient working-memory, able to solve a partially observable maze task. Lastly, we highlight the predictive quality of attended stimuli. Because we use semantic observations, we can uncover not only which features the agent elects to base decisions on, but also how it chooses to compile more complex, relational features from simpler ones. These results formally illustrate the benefits of attention in deep RL and provide evidence for the interpretability of self-attention mechanisms.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1802185503",
                        "name": "Lennart Bramlage"
                    },
                    {
                        "authorId": "34439822",
                        "name": "A. Cortese"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To assess this, we use NEAT to evolve policies for playing Atari 2600 games from the recently released Atari Annotated RAM Interface (Atari ARI) [2].",
                "The Atari ARI [2] provides RAM annotations for 22 of the Atari 2600 games supported by the OpenAI Gym toolkit1."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6241c1813d889d95405ae2c26fe1c6377143e5ed",
                "externalIds": {
                    "DBLP": "conf/gecco/TupperN20",
                    "MAG": "3046259686",
                    "DOI": "10.1145/3377929.3390072",
                    "CorpusId": 220404993
                },
                "corpusId": 220404993,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6241c1813d889d95405ae2c26fe1c6377143e5ed",
                "title": "Evolving neural network agents to play atari games with compact state representations",
                "abstract": "Recent success in solving hard reinforcement learning problems can be partly credited to the use of deep neural networks, which can extract high-level features and learn compact state representations from high-dimensional inputs, such as images. However, the large networks required to learn both state representation and policy using this approach limit the effectiveness and benefits of neuroevolution methods that have proven effective at solving simpler problems in the past. One potential solution to this problem is to separate state representation and policy learning and only apply neuroevolution to the latter. We extend research following this approach by evolving small policy networks for Atari games using NEAT, that learn from compact state representations provided by the recently released Atari Annotated RAM Interface (Atari ARI). Our results show that it is possible to evolve agents that exceed expert human performance using these compact state representations, and that, for some games, successful policy networks can be evolved that contain only a few or even no hidden nodes.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1486439160",
                        "name": "Adam Tupper"
                    },
                    {
                        "authorId": "2173139",
                        "name": "K. Neshatian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, Spatiotemporal Deep Infomax (ST-DIM) [27] leverages temporal and spatial information in expert demonstrations to learn the state representations."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "97007d147fd81f86f001895ea1ac0a8c26e32878",
                "externalIds": {
                    "DBLP": "conf/icmcs/ZhaoZZZL20",
                    "MAG": "3034359401",
                    "DOI": "10.1109/ICME46284.2020.9102924",
                    "CorpusId": 221120785
                },
                "corpusId": 221120785,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/97007d147fd81f86f001895ea1ac0a8c26e32878",
                "title": "State Representation Learning For Effective Deep Reinforcement Learning",
                "abstract": "Recent years have witnessed the great success of deep reinforcement learning (DRL) on a variety of vision games. Although DNN has demonstrated strong power in representation learning, such capacity is under-explored in most DRL works whose focus is usually on optimization solvers. In fact, we discover that the state feature learning is the main obstacle for further improvement of DRL algorithms. To address this issue, we propose a new state representation learning scheme with our Adjacent State Consistency Loss (ASC Loss). The loss is defined based on the hypothesis that there are fewer changes between adjacent states than that of far apart ones, since scenes in videos generally evolve smoothly. In this paper, we exploit ASC loss as an assistant of RL loss in the training phase to boost the state feature learning. We conduct evaluation on Atari games and MuJoCo continuous control tasks, which demonstrates that our method is superior to OpenAI baselines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "7818567",
                        "name": "Jian Zhao"
                    },
                    {
                        "authorId": "38272296",
                        "name": "Wen-gang Zhou"
                    },
                    {
                        "authorId": "2088210977",
                        "name": "Tianyu Zhao"
                    },
                    {
                        "authorId": "2118116281",
                        "name": "Yun Zhou"
                    },
                    {
                        "authorId": "2108508109",
                        "name": "Houqiang Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Contrastive losses have also been constructed from the rules of physics in robotics tasks (Jonschkowski and Brock, 2015), have been applied to Atari models (Anand et al., 2019), and have combined with the above object-oriented approach (Kipf et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1c6435cb353271f3cb87b27ccc6df5b727d55f26",
                "externalIds": {
                    "ArXiv": "2006.16712",
                    "MAG": "3038822267",
                    "DBLP": "journals/corr/abs-2006-16712",
                    "DOI": "10.1561/9781638280576",
                    "CorpusId": 220265929
                },
                "corpusId": 220265929,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1c6435cb353271f3cb87b27ccc6df5b727d55f26",
                "title": "Model-based Reinforcement Learning: A Survey",
                "abstract": "Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a key challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two key sections, we also discuss the potential benefits of model-based RL, like enhanced data efficiency, targeted exploration, and improved stability. Along the survey, we also draw connections to several related RL fields, like hierarchical RL and transfer, and other research disciplines, like behavioural psychology. Altogether, the survey presents a broad conceptual overview of planning-learning combinations for MDP optimization.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "13477045",
                        "name": "T. Moerland"
                    },
                    {
                        "authorId": "1735303",
                        "name": "J. Broekens"
                    },
                    {
                        "authorId": "1689001",
                        "name": "C. Jonker"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Domains with rich observations include raw images or video frames from video games (Anand et al., 2019), robotics environments (Higgins et al., 2017), and renderings of classic control problems (Watter et al., 2015), and deep learning methods have enabled success in this space. Srinivas et al. (2020) use a contrastive learning approach to extract state representations from pixels.",
                "Domains with rich observations include raw images or video frames from video games (Anand et al., 2019), robotics environments (Higgins et al., 2017), and renderings of classic control problems (Watter et al., 2015), and deep learning methods have enabled success in this space. Srinivas et al. (2020) use a contrastive learning approach to extract state representations from pixels. Ha & Schmidhuber (2018) learn low-dimensional representations and dynamics which simple linear policies to achieve effective control. Hafner et al. (2019a) utilize latent imagination to learn behaviors that achieve high performance in terms of reward and sample-efficiency on several visual control tasks.",
                "Domains with rich observations include raw images or video frames from video games (Anand et al., 2019), robotics environments (Higgins et al.",
                "Domains with rich observations include raw images or video frames from video games (Anand et al., 2019), robotics environments (Higgins et al., 2017), and renderings of classic control problems (Watter et al., 2015), and deep learning methods have enabled success in this space. Srinivas et al. (2020) use a contrastive learning approach to extract state representations from pixels. Ha & Schmidhuber (2018) learn low-dimensional representations and dynamics which simple linear policies to achieve effective control.",
                "Domains with rich observations include raw images or video frames from video games (Anand et al., 2019), robotics environments (Higgins et al., 2017), and renderings of classic control problems (Watter et al., 2015), and deep learning methods have enabled success in this space."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7c58b1490564c05fe3d71cad37589ad4dc06d805",
                "externalIds": {
                    "MAG": "3037364847",
                    "ArXiv": "2006.16128",
                    "DBLP": "conf/icml/Frandsen0L22",
                    "CorpusId": 220250135
                },
                "corpusId": 220250135,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7c58b1490564c05fe3d71cad37589ad4dc06d805",
                "title": "Extracting Latent State Representations with Linear Dynamics from Rich Observations",
                "abstract": "Recently, many reinforcement learning techniques were shown to have provable guarantees in the simple case of linear dynamics, especially in problems like linear quadratic regulators. However, in practice, many reinforcement learning problems try to learn a policy directly from rich, high dimensional representations such as images. Even if there is an underlying dynamics that is linear in the correct latent representations (such as position and velocity), the rich representation is likely to be nonlinear and can contain irrelevant features. In this work we study a model where there is a hidden linear subspace in which the dynamics is linear. For such a model we give an efficient algorithm for extracting the linear subspace with linear dynamics. We then extend our idea to extracting a nonlinear mapping, and empirically verify the effectiveness of our approach in simple settings with rich observations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51165619",
                        "name": "Abraham Frandsen"
                    },
                    {
                        "authorId": "144804200",
                        "name": "Rong Ge"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such approaches may be viewed as postulating concrete latent relations: g(st, st+1) = c , where g is the squared distance between st and st+1 for Lcont, and a more complicated relation for [2].",
                "[2] provides such a framework for ATARI games, but it is not aimed at continuous control.",
                "A related heuristic from [2] maximizes mutual information between parts of consecutive latent states.",
                "using continuity [1], mutual information with prior states [2], consistency with a forward or inverse model (see [3] for a survey)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7f95a64dab58bc6287ad344d604f04fbd5a2c750",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-08718",
                    "MAG": "3035601566",
                    "ArXiv": "2006.08718",
                    "CorpusId": 219708311
                },
                "corpusId": 219708311,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7f95a64dab58bc6287ad344d604f04fbd5a2c750",
                "title": "Analytic Manifold Learning: Unifying and Evaluating Representations for Continuous Control",
                "abstract": "We address the problem of learning reusable state representations from streaming high-dimensional observations. This is important for areas like Reinforcement Learning (RL), which yields non-stationary data distributions during training. We make two key contributions. First, we propose an evaluation suite that measures alignment between latent and true low-dimensional states. We benchmark several widely used unsupervised learning approaches. This uncovers the strengths and limitations of existing approaches that impose additional constraints/objectives on the latent space. Our second contribution is a unifying mathematical formulation for learning latent relations. We learn analytic relations on source domains, then use these relations to help structure the latent space when learning on target domains. This formulation enables a more general, flexible and principled way of shaping the latent space. It formalizes the notion of learning independent relations, without imposing restrictive simplifying assumptions or requiring domain-specific information. We present mathematical properties, concrete algorithms for implementation and experimental validation of successful learning and transfer of latent relations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "39534622",
                        "name": "Rika Antonova"
                    },
                    {
                        "authorId": "1975747",
                        "name": "Maksim Maydanskiy"
                    },
                    {
                        "authorId": "1731490",
                        "name": "D. Kragic"
                    },
                    {
                        "authorId": "1693696",
                        "name": "Sam Devlin"
                    },
                    {
                        "authorId": "1380228856",
                        "name": "Katja Hofmann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the auxiliary objective, we follow a variant of Deep InfoMax [DIM, Hjelm et al., 2018, Anand et al., 2019, Bachman et al., 2019], and train the encoder to maximize the mutual information (MI) between local and global \u201cviews\u201d of tuples (st, at, st+k).",
                "\u2026typically either taken from different \u201clocations\u201d of the data [e.g., spatial patches or temporal locations, see Hjelm et al., 2018, Oord et al., 2018, Anand et al., 2019, H\u00e9naff et al., 2019] or obtained through data augmentation [Wu et al., 2018, He et al., 2019, Bachman et al., 2019, Tian et\u2026",
                "Our work, DRIML, predicts future states conditioned on the current state-action pair at multiple scales, drawing upon ideas encapsulated in Augmented Multiscale Deep InfoMax [AMDIM, Bachman et al., 2019] and Spatio-Temporal DIM [ST-DIM, Anand et al., 2019].",
                "L G\n] 1\n6 N\nov 2\nwith model-like properties, we consider a self-supervised objective derived from variants of Deep InfoMax [DIM, Hjelm et al., 2018, Bachman et al., 2019, Anand et al., 2019]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "27a7df880e9c4ccd87cb88cccb131e2b4687567f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-07217",
                    "MAG": "3104132897",
                    "ArXiv": "2006.07217",
                    "CorpusId": 219635788
                },
                "corpusId": 219635788,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/27a7df880e9c4ccd87cb88cccb131e2b4687567f",
                "title": "Deep Reinforcement and InfoMax Learning",
                "abstract": "We begin with the hypothesis that a model-free agent whose representations are predictive of properties of future states (beyond expected rewards) will be more capable of solving and adapting to new RL problems. To test that hypothesis, we introduce an objective based on Deep InfoMax (DIM) which trains the agent to predict the future by maximizing the mutual information between its internal representation of successive timesteps. We test our approach in several synthetic settings, where it successfully learns representations that are predictive of the future. Finally, we augment C51, a strong RL baseline, with our temporal DIM objective and demonstrate improved performance on a continual learning task and on the recently introduced Procgen environment.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "18111246",
                        "name": "Bogdan Mazoure"
                    },
                    {
                        "authorId": "15032777",
                        "name": "R\u00e9mi Tachet des Combes"
                    },
                    {
                        "authorId": "33554869",
                        "name": "T. Doan"
                    },
                    {
                        "authorId": "143902541",
                        "name": "Philip Bachman"
                    },
                    {
                        "authorId": "40482726",
                        "name": "R. Devon Hjelm"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The InfoMax principal has been extended to graphs [44], [40] and for state representation in reinforcement learning [2]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3ce566b993f213b8f2d47d0203b70eca111bd01f",
                "externalIds": {
                    "DBLP": "conf/eccv/Sanghi20",
                    "MAG": "3097823560",
                    "ArXiv": "2006.02598",
                    "DOI": "10.1007/978-3-030-58526-6_37",
                    "CorpusId": 219304526
                },
                "corpusId": 219304526,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/3ce566b993f213b8f2d47d0203b70eca111bd01f",
                "title": "Info3D: Representation Learning on 3D Objects using Mutual Information Maximization and Contrastive Learning",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1557900502",
                        "name": "Aditya Sanghi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[10] provides such framework for ATARI games, but it is not aimed for continuous control."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ab293ea9b6e88a81646bd9fe717614e8a752edb1",
                "externalIds": {
                    "MAG": "3037555919",
                    "CorpusId": 221742276
                },
                "corpusId": 221742276,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ab293ea9b6e88a81646bd9fe717614e8a752edb1",
                "title": "Benchmarking Unsupervised Representation Learning for Continuous Control",
                "abstract": "We address the problem of learning reusable state representations from a non-stationary stream of high-dimensional observations. This is important for areas that employ Reinforcement Learning (RL), which yields non-stationary data distributions during training. Unsupervised approaches can be trained on such data streams to produce low-dimensional latent embeddings, which could be reused on domains with different dynamics and rewards. However, there is a need to adequately evaluate the quality of the resulting representations. We propose an evaluation suite that measures alignment between the learned latent states and the true low-dimensional states. Using this suite, we benchmark several widely used unsupervised learning approaches. This uncovers the strengths and limitations of existing approaches that impose additional constraints/assumptions on the latent space.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "39534622",
                        "name": "Rika Antonova"
                    },
                    {
                        "authorId": "1693696",
                        "name": "Sam Devlin"
                    },
                    {
                        "authorId": "1380228856",
                        "name": "Katja Hofmann"
                    },
                    {
                        "authorId": "1731490",
                        "name": "D. Kragic"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f0bf1a304a473c70b683b79a0a3d16b887ad7f80",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-06369",
                    "ArXiv": "2005.06369",
                    "MAG": "3024970298",
                    "CorpusId": 218613693
                },
                "corpusId": 218613693,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f0bf1a304a473c70b683b79a0a3d16b887ad7f80",
                "title": "Progressive growing of self-organized hierarchical representations for exploration",
                "abstract": "Designing agent that can autonomously discover and learn a diversity of structures and skills in unknown changing environments is key for lifelong machine learning. A central challenge is how to learn incrementally representations in order to progressively build a map of the discovered structures and re-use it to further explore. To address this challenge, we identify and target several key functionalities. First, we aim to build lasting representations and avoid catastrophic forgetting throughout the exploration process. Secondly we aim to learn a diversity of representations allowing to discover a \"diversity of diversity\" of structures (and associated skills) in complex high-dimensional environments. Thirdly, we target representations that can structure the agent discoveries in a coarse-to-fine manner. Finally, we target the reuse of such representations to drive exploration toward an \"interesting\" type of diversity, for instance leveraging human guidance. Current approaches in state representation learning rely generally on monolithic architectures which do not enable all these functionalities. Therefore, we present a novel technique to progressively construct a Hierarchy of Observation Latent Models for Exploration Stratification, called HOLMES. This technique couples the use of a dynamic modular model architecture for representation learning with intrinsically-motivated goal exploration processes (IMGEPs). The paper shows results in the domain of automated discovery of diverse self-organized patterns, considering as testbed the experimental framework from Reinke et al. (2019).",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51306370",
                        "name": "Mayalen Etcheverry"
                    },
                    {
                        "authorId": "1720664",
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "authorId": "9252114",
                        "name": "Chris Reinke"
                    }
                ]
            }
        },
        {
            "contexts": [
                "long-studied, fast-growing field in embodied AI research (Schmidhuber, 1990; Ha and Schmidhuber, 2018; Hamrick, 2019; Anand et al., 2019; Kipf et al., 2020), and recently also in learned executors for neural programming (Kant, 2018).",
                "Outside of NLU, learning structured world models is a long-studied, fast-growing field in embodied AI research (Schmidhuber, 1990; Ha and Schmidhuber, 2018; Hamrick, 2019; Anand et al., 2019; Kipf et al., 2020), and recently also in learned executors for neural programming (Kant, 2018)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7925351a4039dae7c2fd5d5d12df7f52f8464ce3",
                "externalIds": {
                    "MAG": "3034654962",
                    "DBLP": "journals/corr/abs-2005-00311",
                    "ArXiv": "2005.00311",
                    "ACL": "2020.acl-main.559",
                    "DOI": "10.18653/v1/2020.acl-main.559",
                    "CorpusId": 218470187
                },
                "corpusId": 218470187,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/7925351a4039dae7c2fd5d5d12df7f52f8464ce3",
                "title": "Language (Re)modelling: Towards Embodied Language Understanding",
                "abstract": "While natural language understanding (NLU) is advancing rapidly, today\u2019s technology differs from human-like language understanding in fundamental ways, notably in its inferior efficiency, interpretability, and generalization. This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL). According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction. This position paper argues that the use of grounding by metaphoric reasoning and simulation will greatly benefit NLU systems, and proposes a system architecture along with a roadmap towards realizing this vision.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "24364033",
                        "name": "Ronen Tamari"
                    },
                    {
                        "authorId": "1683047517",
                        "name": "Chen Shani"
                    },
                    {
                        "authorId": "2041698667",
                        "name": "Tom Hope"
                    },
                    {
                        "authorId": "1903312",
                        "name": "Miriam R. L. Petruck"
                    },
                    {
                        "authorId": "2769805",
                        "name": "Omri Abend"
                    },
                    {
                        "authorId": "1805894",
                        "name": "Dafna Shahaf"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4cf5067a8d4393b911873c580152d78a8b9c8be3",
                "externalIds": {
                    "MAG": "2995372087",
                    "DBLP": "conf/iclr/RahamanWGRB20",
                    "CorpusId": 213644165
                },
                "corpusId": 213644165,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4cf5067a8d4393b911873c580152d78a8b9c8be3",
                "title": "Learning the Arrow of Time for Problems in Reinforcement Learning",
                "abstract": "We humans have an innate understanding of the asymmetric progression of time, which we use to efficiently and safely perceive and manipulate our environment. Drawing inspiration from that, we approach the problem of learning an arrow of time in a Markov (Decision) Process. We illustrate how a learned arrow of time can capture salient information about the environment, which in turn can be used to measure reachability, detect side-effects and to obtain an intrinsic reward signal. Finally, we propose a simple yet effective algorithm to parameterize the problem at hand and learn an arrow of time with a function approximator (here, a deep neural network). Our empirical results span a selection of discrete and continuous environments, and demonstrate for a class of stochastic processes that the learned arrow of time agrees reasonably well with a well known notion of an arrow of time due to Jordan, Kinderlehrer and Otto (1998).",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40050919",
                        "name": "Nasim Rahaman"
                    },
                    {
                        "authorId": "153201456",
                        "name": "Steffen Wolf"
                    },
                    {
                        "authorId": "1996705",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "150282484",
                        "name": "Roman Remme"
                    },
                    {
                        "authorId": "1751762",
                        "name": "Yoshua Bengio"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The learned weights of the encoder of the VAE are frozen, and the latent input is used to train the policy for reset-free RL.",
                "Lastly, we compare algorithm performance with two ablations: running R3L without the perturbation controller (\u201cVICE + VAE\u201d) and without the unsupervised learning (\u201cR3L w/o VAE\u201d).",
                "We then compare with prior reset-free RL algorithms (Eysenbach et al., 2018) that explicitly learn a reset controller to alternate goals in the state space (\u201cReset Controller + VAE\u201d).",
                "Fig 8 compares the performance of our method without supervised learning (\u201cR3L w/o VAE\u201d) in the real world against a baseline that uses SAC for vision-based RL from raw pixels, VICE for providing rewards, and running reset-free (denoted as \u201cVICE\u201d).",
                "Note that we use a VAE as an instantiation of representation learning techniques that works well in the domains we considered, but other more sophisticated density models proposed in prior work may also be substituted in place of the VAE (Lee et al., 2019; Hjelm et al., 2019; Anand et al., 2019).",
                "B.0.1 HYPERPARAMETERS\nGeneral Standard deviation update coefficient 0.99 Image Sizes [(16, 16, 3), (32, 32, 3), (64, 64, 3)] SAC Learning Rate 3e-4 \u03b3 0.99 Batch Size 256 Convnet Filters [(64, 64, 64), (16, 32, 64)] Stride (2, 2) Kernel Sizes (3, 3) Pooling [MaxPool2D, None] Actor/Critic FC Layers [(512, 512), (256, 256, 256)] VICE nVICE [1, 5, 10] Batch Size 128 Learning Rate 1e-4 Mixup \u03b1 Uniform(0, 1) Convnet Filters [(64, 64, 64), (16, 32, 64)] Stride (2, 2) Kernel Sizes (3, 3) Pooling [MaxPool2D, None] FC Layers [(512, 512), (256, 256, 256)] RND Learning Rate 3e-4 Batch Size 256 Convnet Filters (16, 32, 64) Stride (2, 2) Kernel Sizes (3, 3) Pooling [MaxPool2D, None] FC Layers [(512, 512), (256, 256, 256)] VAE Learning Rate 1e-4 Batch Size 256 Encoder (Convnet) Filters (64, 64, 32) Latent Dimension [8, 16, 32, 64] \u03b2 [1e-3, 0.1, 0.5, 1, 10] Stride (2, 2) Kernel Sizes (3, 3) Pooling [MaxPool2D, None]\nThe ranges of values listed above represent the hyperparameters we searched over, and the bolded values are what we use in the Section 6 experiments.",
                "B.0.4 VAE\nWe train a standard beta-VAE to maximize the evidence lower bound, given by:\nEz\u223cq\u03c6(z|x)[p\u03b8(x|z)]\u2212 \u03b2DKL(q\u03c6(z|x) || p\u03b8(z))\nTo collect training data, we sampled random states in the observation space.",
                "Note that we use a VAE as an instantiation of representation learning techniques that works well in the domains we considered, but other more sophisticated density models proposed in prior work may also be substituted in place of the VAE Lee et al. (2019); Hjelm et al. (2019); Anand et al. (2019).",
                "We therefore aim to convert the vision-based learning problem into one that more closely resembles state-based learning, by training a variational autoencoder (VAE, Kingma & Welling (2013)) and sharing the latent-variable representation across the actor and critic networks (refer to Appendix B for more details)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2d85f63a193cd88741c8398ceb98c55e1e89387d",
                "externalIds": {
                    "ArXiv": "2004.12570",
                    "MAG": "3020712699",
                    "DBLP": "journals/corr/abs-2004-12570",
                    "CorpusId": 212877887
                },
                "corpusId": 212877887,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2d85f63a193cd88741c8398ceb98c55e1e89387d",
                "title": "The Ingredients of Real-World Robotic Reinforcement Learning",
                "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2117693732",
                        "name": "Henry Zhu"
                    },
                    {
                        "authorId": "2152813716",
                        "name": "Justin Yu"
                    },
                    {
                        "authorId": "2129458064",
                        "name": "Abhishek Gupta"
                    },
                    {
                        "authorId": "145718344",
                        "name": "Dhruv Shah"
                    },
                    {
                        "authorId": "41016704",
                        "name": "Kristian Hartikainen"
                    },
                    {
                        "authorId": "1899992",
                        "name": "Avi Singh"
                    },
                    {
                        "authorId": "2109446216",
                        "name": "Vikash Kumar"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The fact that we can treat a cell as an object allows us to evaluate our error-correcting strategy as proof of concept, since unsupervised object detection for control tasks is still an nascent area of research [23]\u2013[25]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cf6f9d7c4f3c20f88208988b3dfa962452f98a3d",
                "externalIds": {
                    "DBLP": "conf/cig/OvalleL20",
                    "ArXiv": "2004.07155",
                    "MAG": "3016965513",
                    "DOI": "10.1109/CoG47356.2020.9231599",
                    "CorpusId": 215768881
                },
                "corpusId": 215768881,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cf6f9d7c4f3c20f88208988b3dfa962452f98a3d",
                "title": "Bootstrapped model learning and error correction for planning with uncertainty in model-based RL",
                "abstract": "Having access to a forward model enables the use of planning algorithms such as Monte Carlo Tree Search and Rolling Horizon Evolution. Where a model is unavailable, a natural aim is to learn a model that reflects accurately the dynamics of the environment. In many situations it might not be possible and minimal glitches in the model may lead to poor performance and failure. This paper explores the problem of model misspecification through uncertainty-aware reinforcement learning agents. We propose a bootstrapped multi-headed neural network that learns the distribution of future states and rewards. We experiment with a number of schemes to extract the most likely predictions. Moreover, we also introduce a global error correction filter that applies high-level constraints guided by the context provided through the predictive distribution. We illustrate our approach on Minipacman. The evaluation demonstrates that when dealing with imperfect models, our methods exhibit increased performance and stability, both in terms of model accuracy and in its use within a planning algorithm.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2139138",
                        "name": "Alvaro Ovalle"
                    },
                    {
                        "authorId": "145815031",
                        "name": "S. Lucas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Contrastive learning has been used to extract reward signals in the latent space (Sermanet et al., 2018; Dwibedi et al., 2018; WardeFarley et al., 2018); and study representation learning on Atari games by Anand et al. (2019).",
                ", 2018); and study representation learning on Atari games by Anand et al. (2019). World Models for sample-efficiency: While joint learning of an auxiliary unsupervised task with model-free RL is one way to improve the sample-efficiency of agents, there has also been another line of research that has tried to learn world models of the environment and use them to sample rollouts and plan."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "79e14a09ff070e06ab9df598ccd885b929164ef9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-04136",
                    "MAG": "3015437096",
                    "ArXiv": "2004.04136",
                    "CorpusId": 215415964
                },
                "corpusId": 215415964,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/79e14a09ff070e06ab9df598ccd885b929164ef9",
                "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
                "abstract": "We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "41207614",
                        "name": "A. Srinivas"
                    },
                    {
                        "authorId": "51093256",
                        "name": "M. Laskin"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[2] use the NCE loss to discriminate between temporally near frames and temporally far frames of ATARI gameplay but do not compare across games."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "98b6ee83697d1e722ec60d693f1c8b33b6f566fa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-07990",
                    "MAG": "3012540512",
                    "ArXiv": "2003.07990",
                    "CorpusId": 212747934
                },
                "corpusId": 212747934,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/98b6ee83697d1e722ec60d693f1c8b33b6f566fa",
                "title": "Watching the World Go By: Representation Learning from Unlabeled Videos",
                "abstract": "Recent single image unsupervised representation learning techniques show remarkable success on a variety of tasks. The basic principle in these works is instance discrimination: learning to differentiate between two augmented versions of the same image and a large batch of unrelated images. Networks learn to ignore the augmentation noise and extract semantically meaningful representations. Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial ways and are not aligned with how objects actually change e.g. occlusion, deformation, viewpoint change. In this paper, we argue that videos offer this natural augmentation for free. Videos can provide entirely new views of objects, show deformation, and even connect semantically similar but visually distinct concepts. We propose Video Noise Contrastive Estimation, a method for using unlabeled video to learn strong, transferable single image representations. We demonstrate improvements over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks. Code and the Random Related Video Views dataset are available at this https URL",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "152462964",
                        "name": "Daniel Gordon"
                    },
                    {
                        "authorId": "2883417",
                        "name": "Kiana Ehsani"
                    },
                    {
                        "authorId": "145197953",
                        "name": "D. Fox"
                    },
                    {
                        "authorId": "143787583",
                        "name": "Ali Farhadi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "41c883754ae1983e94bff6c3ec3d40306cffe06e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-07233",
                    "ArXiv": "2003.07233",
                    "MAG": "3010968977",
                    "CorpusId": 212725562
                },
                "corpusId": 212725562,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/41c883754ae1983e94bff6c3ec3d40306cffe06e",
                "title": "The TrojAI Software Framework: An OpenSource tool for Embedding Trojans into Deep Learning Models",
                "abstract": "In this paper, we introduce the TrojAI software framework, an open source set of Python tools capable of generating triggered (poisoned) datasets and associated deep learning (DL) models with trojans at scale. We utilize the developed framework to generate a large set of trojaned MNIST classifiers, as well as demonstrate the capability to produce a trojaned reinforcement-learning model using vector observations. Results on MNIST show that the nature of the trigger, training batch size, and dataset poisoning percentage all affect successful embedding of trojans. We test Neural Cleanse against the trojaned MNIST models and successfully detect anomalies in the trained models approximately $18\\%$ of the time. Our experiments and workflow indicate that the TrojAI software framework will enable researchers to easily understand the effects of various configurations of the dataset and training hyperparameters on the generated trojaned deep learning model, and can be used to rapidly and comprehensively test new trojan detection methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "38081739",
                        "name": "Kiran Karra"
                    },
                    {
                        "authorId": "32339320",
                        "name": "C. Ashcraft"
                    },
                    {
                        "authorId": "29880725",
                        "name": "Neil Fendley"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They yield good performance for learning image [13, 31, 33, 35, 45, 52, 58, 70, 71, 77] and video [3, 28, 34, 46, 49, 54, 66, 68, 82] representations, and circumvent the need to explicitly specify what information needs to be discarded via a designed task."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8ddbaa34c574124a91fa3bc217e232e17668e84c",
                "externalIds": {
                    "DBLP": "conf/iccv/PatrickAKFHZV21",
                    "ArXiv": "2003.04298",
                    "DOI": "10.1109/ICCV48922.2021.00944",
                    "CorpusId": 239998405
                },
                "corpusId": 239998405,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/8ddbaa34c574124a91fa3bc217e232e17668e84c",
                "title": "On Compositions of Transformations in Contrastive Self-Supervised Learning",
                "abstract": "In the image domain, excellent representations can be learned by inducing invariance to content-preserving transformations via noise contrastive learning. In this paper, we generalize contrastive learning to a wider set of transformations, and their compositions, for which either invariance or distinctiveness is sought. We show that it is not immediately obvious how existing methods such as SimCLR can be extended to do so. Instead, we introduce a number of formal requirements that all contrastive formulations must satisfy, and propose a practical construction which satisfies these requirements. In order to maximise the reach of this analysis, we express all components of noise contrastive formulations as the choice of certain generalized transformations of the data (GDTs), including data sampling. We then consider videos as an example of data in which a large variety of transformations are applicable, accounting for the extra modalities \u2013 for which we analyze audio and text \u2013 and the dimension of time. We find that being invariant to certain transformations and distinctive to others is critical to learning effective video representations, improving the state-of-the-art for multiple benchmarks by a large margin, and even surpassing supervised pretraining. Code and pretrained models are available1.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1379929116",
                        "name": "Mandela Patrick"
                    },
                    {
                        "authorId": "47792365",
                        "name": "Yuki M. Asano"
                    },
                    {
                        "authorId": "145592791",
                        "name": "Polina Kuznetsova"
                    },
                    {
                        "authorId": "25576460",
                        "name": "Ruth C. Fong"
                    },
                    {
                        "authorId": "143848064",
                        "name": "Jo\u00e3o F. Henriques"
                    },
                    {
                        "authorId": "1681543",
                        "name": "G. Zweig"
                    },
                    {
                        "authorId": "1687524",
                        "name": "A. Vedaldi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "34b142fb11636445f8eca36e02941c3d981c3623",
                "externalIds": {
                    "MAG": "3010323908",
                    "DBLP": "journals/corr/abs-2003-01384",
                    "CorpusId": 211818004
                },
                "corpusId": 211818004,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/34b142fb11636445f8eca36e02941c3d981c3623",
                "title": "Self-Supervised Object-Level Deep Reinforcement Learning",
                "abstract": "Current deep reinforcement learning approaches incorporate minimal prior knowledge about the environment, limiting computational and sample efficiency. We incorporate a few object-based priors that humans are known to use: \"Infants divide perceptual arrays into units that move as connected wholes, that move separately from one another, that tend to maintain their size and shape over motion, and that tend to act upon each other only on contact\" [Spelke]. We propose a probabilistic object-based model of environments and use human object priors to develop an efficient self-supervised algorithm for maximum likelihood estimation of the model parameters from observations and for inferring objects directly from the perceptual stream. We then use object features and incorporate object-contact priors to improve the sample efficiency our object-based RL agent.We evaluate our approach on a subset of the Atari benchmarks, and learn up to four orders of magnitude faster than the standard deep Q-learning network, rendering rapid desktop experiments in this domain feasible. To our knowledge, our system is the first to learn any Atari task in fewer environment interactions than humans.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "27377925",
                        "name": "William Agnew"
                    },
                    {
                        "authorId": "1740213",
                        "name": "Pedro M. Domingos"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "22669600d84bae70bad081e9acdeacc00ce766bc",
                "externalIds": {
                    "ArXiv": "2003.01384",
                    "MAG": "3088302932",
                    "CorpusId": 221978382
                },
                "corpusId": 221978382,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/22669600d84bae70bad081e9acdeacc00ce766bc",
                "title": "Relevance-Guided Modeling of Object Dynamics for Reinforcement Learning",
                "abstract": "Current deep reinforcement learning approaches incorporate minimal prior knowledge about the environment, limiting computational and sample efficiency. Objects provide a succinct and causal description of the world, and several recent works have studied unsupervised object representation learning using priors and losses over static object properties like visual consistency. However, object dynamics and interaction are critical cues for objectness. In addition, extensive research has shown humans have a working memory limited to only a small number of task relevant objects. In this paper we propose a framework for reasoning about object dynamics and behavior to rapidly determine minimal and task-specific object representations. We show the need for this reasoning over object behavior and dynamics by introducing a suite of RGBD MuJoCo object collection and avoidance tasks that, while intuitive and visually simple, confound state of the art unsupervised object representation learning algorithms. We also demonstrate the potential of this framework on a number of Atari games, using our object representation and standard RL and planning algorithms to learn over 10,000x faster than standard deep RL algorithms, and faster even than human players.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "27377925",
                        "name": "William Agnew"
                    },
                    {
                        "authorId": "1740213",
                        "name": "Pedro M. Domingos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several works have previously explored information-theoretic approaches for representation learning in the reinforcement learning context (Nachum et al., 2018; Anand et al., 2019; Lu et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "96f60a0085f4fb81c7b7726653d42c5c14d97e04",
                "externalIds": {
                    "MAG": "3035685037",
                    "DBLP": "conf/icml/ShuNCPTGEB20",
                    "ArXiv": "2003.01086",
                    "CorpusId": 211678253
                },
                "corpusId": 211678253,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/96f60a0085f4fb81c7b7726653d42c5c14d97e04",
                "title": "Predictive Coding for Locally-Linear Control",
                "abstract": "High-dimensional observations and unknown dynamics are major challenges when applying optimal control to many real-world decision making tasks. The Learning Controllable Embedding (LCE) framework addresses these challenges by embedding the observations into a lower dimensional latent space, estimating the latent dynamics, and then performing control directly in the latent space. To ensure the learned latent dynamics are predictive of next-observations, all existing LCE approaches decode back into the observation space and explicitly perform next-observation prediction---a challenging high-dimensional task that furthermore introduces a large number of nuisance parameters (i.e., the decoder) which are discarded during control. In this paper, we propose a novel information-theoretic LCE approach and show theoretically that explicit next-observation prediction can be replaced with predictive coding. We then use predictive coding to develop a decoder-free LCE model whose latent dynamics are amenable to locally-linear control. Extensive experiments on benchmark tasks show that our model reliably learns a controllable latent space that leads to superior performance when compared with state-of-the-art LCE baselines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1978777",
                        "name": "Rui Shu"
                    },
                    {
                        "authorId": "2116225171",
                        "name": "Tung D. Nguyen"
                    },
                    {
                        "authorId": "1819830",
                        "name": "Yinlam Chow"
                    },
                    {
                        "authorId": "2067955931",
                        "name": "Tu Pham"
                    },
                    {
                        "authorId": "38685176",
                        "name": "Khoat Than"
                    },
                    {
                        "authorId": "1678622",
                        "name": "M. Ghavamzadeh"
                    },
                    {
                        "authorId": "2490652",
                        "name": "Stefano Ermon"
                    },
                    {
                        "authorId": "145365341",
                        "name": "H. Bui"
                    }
                ]
            }
        },
        {
            "contexts": [
                "based on self-supervision that use alternatives to reconstruction of input states [1, 2, 4, 11, 30, 40, 53].",
                "Similar losses have often been used in related work [2, 11, 13, 30, 40], which we compare in Section 5.",
                "Certain works focus on predicting the next state using a contrastive loss [2, 30, 40], disregarding the reward function."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7d2c7517a59669108588b98de1876f54ca23f6b2",
                "externalIds": {
                    "MAG": "3008165446",
                    "DBLP": "journals/corr/abs-2002-11963",
                    "ArXiv": "2002.11963",
                    "CorpusId": 211532413
                },
                "corpusId": 211532413,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7d2c7517a59669108588b98de1876f54ca23f6b2",
                "title": "Plannable Approximations to MDP Homomorphisms: Equivariance under Actions",
                "abstract": "This work exploits action equivariance for representation learning in reinforcement learning. Equivariance under actions states that transitions in the input space are mirrored by equivalent transitions in latent space, while the map and transition functions should also commute. We introduce a contrastive loss function that enforces action equivariance on the learned representations. We prove that when our loss is zero, we have a homomorphism of a deterministic Markov Decision Process (MDP). Learning equivariant maps leads to structured latent spaces, allowing us to build a model on which we plan through value iteration. We show experimentally that for deterministic MDPs, the optimal policy in the abstract MDP can be successfully lifted to the original MDP. Moreover, the approach easily adapts to changes in the goal states. Empirically, we show that in such MDPs, we obtain better representations in fewer epochs compared to representation learning approaches using reconstructions, while generalizing better to new goals than model-free approaches.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3468383",
                        "name": "Elise van der Pol"
                    },
                    {
                        "authorId": "41016725",
                        "name": "Thomas Kipf"
                    },
                    {
                        "authorId": "1799949",
                        "name": "F. Oliehoek"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For self-supervised pre-training we use Spatio-Temporal DeepInfoMax [23] to maximize predictability between current latent state and future spatial state and between consecutive spatial states (for example, on encoded time points of the resting-state fMRI (rsfMRI)).",
                "Furthermore, it influences the neuroimaging field for classification of progression to Alzheimer\u2019s disease from sMRI [22], learning useful representation of the states from the frames in Atari games [23] and also from the speech chunks for speaker identification [24]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c910b1e15078bbe90e56608133ae09cd57ef41fa",
                "externalIds": {
                    "MAG": "2992805138",
                    "DBLP": "journals/corr/abs-1912-03130",
                    "ArXiv": "1912.03130",
                    "CorpusId": 208857665
                },
                "corpusId": 208857665,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c910b1e15078bbe90e56608133ae09cd57ef41fa",
                "title": "Learnt dynamics generalizes across tasks, datasets, and populations",
                "abstract": "Differentiating multivariate dynamic signals is a difficult learning problem as the feature space may be large yet often only a few training examples are available. Traditional approaches to this problem either proceed from handcrafted features or require large datasets to combat the m >> n problem. In this paper, we show that the source of the problem---signal dynamics---can be used to our advantage and noticeably improve classification performance on a range of discrimination tasks when training data is scarce. We demonstrate that self-supervised pre-training guided by signal dynamics produces embedding that generalizes across tasks, datasets, data collection sites, and data distributions. We perform an extensive evaluation of this approach on a range of tasks including simulated data, keyword detection problem, and a range of functional neuroimaging data, where we show that a single embedding learnt on healthy subjects generalizes across a number of disorders, age groups, and datasets.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "153339670",
                        "name": "Usman Mahmood"
                    },
                    {
                        "authorId": "2107445560",
                        "name": "M. M. Rahman"
                    },
                    {
                        "authorId": "26920432",
                        "name": "A. Fedorov"
                    },
                    {
                        "authorId": "2895825",
                        "name": "Z. Fu"
                    },
                    {
                        "authorId": "144048760",
                        "name": "V. Calhoun"
                    },
                    {
                        "authorId": "2122479",
                        "name": "S. Plis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "19b924dd9121f01165276a7afb764cf394acb80b",
                "externalIds": {
                    "MAG": "2995627237",
                    "DBLP": "journals/corr/abs-1911-12247",
                    "ArXiv": "1911.12247",
                    "CorpusId": 208310100
                },
                "corpusId": 208310100,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/19b924dd9121f01165276a7afb764cf394acb80b",
                "title": "Contrastive Learning of Structured World Models",
                "abstract": "A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "41016725",
                        "name": "Thomas Kipf"
                    },
                    {
                        "authorId": "3468383",
                        "name": "Elise van der Pol"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "386620b2572266baad688b288807f61e5a84719e",
                "externalIds": {
                    "MAG": "3034408737",
                    "DBLP": "conf/icml/CacciaBCP20",
                    "CorpusId": 226901884
                },
                "corpusId": 226901884,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/386620b2572266baad688b288807f61e5a84719e",
                "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
                "abstract": "We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the compression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learning environments.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "51889580",
                        "name": "Lucas Caccia"
                    },
                    {
                        "authorId": "1829344",
                        "name": "Eugene Belilovsky"
                    },
                    {
                        "authorId": "1750641",
                        "name": "Massimo Caccia"
                    },
                    {
                        "authorId": "145134886",
                        "name": "Joelle Pineau"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "72a6af9acffae0908e27fcbde2a399dce8930be1",
                "externalIds": {
                    "MAG": "2985391456",
                    "ArXiv": "1911.06813",
                    "DBLP": "journals/corr/abs-1911-06813",
                    "CorpusId": 208139025
                },
                "corpusId": 208139025,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/72a6af9acffae0908e27fcbde2a399dce8930be1",
                "title": "Transfer Learning of fMRI Dynamics",
                "abstract": "As a mental disorder progresses, it may affect brain structure, but brain function expressed in brain dynamics is affected much earlier. Capturing the moment when brain dynamics express the disorder is crucial for early diagnosis. The traditional approach to this problem via training classifiers either proceeds from handcrafted features or requires large datasets to combat the $m>>n$ problem when a high dimensional fMRI volume only has a single label that carries learning signal. Large datasets may not be available for a study of each disorder, or rare disorder types or sub-populations may not warrant for them. In this paper, we demonstrate a self-supervised pre-training method that enables us to pre-train directly on fMRI dynamics of healthy control subjects and transfer the learning to much smaller datasets of schizophrenia. Not only we enable classification of disorder directly based on fMRI dynamics in small data but also significantly speed up the learning when possible. This is encouraging evidence of informative transfer learning across datasets and diagnostic categories.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "153339670",
                        "name": "Usman Mahmood"
                    },
                    {
                        "authorId": "2116362588",
                        "name": "Md. Mahfuzur Rahman"
                    },
                    {
                        "authorId": "26920432",
                        "name": "A. Fedorov"
                    },
                    {
                        "authorId": "2895825",
                        "name": "Z. Fu"
                    },
                    {
                        "authorId": "2122479",
                        "name": "S. Plis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Copyright 2020 by the author(s).\nsentation learning for images (Doersch et al., 2015; Noroozi & Favaro, 2016; Larsson et al., 2017; Gidaris et al., 2018; Zhang et al., 2019a), natural language (Devlin et al., 2018), and video games (Anand et al., 2019).",
                ", 2018), and video games (Anand et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6bfa8f8b09ec21fe06d4eee28c3fb6cce879be1d",
                "externalIds": {
                    "MAG": "3037584823",
                    "DBLP": "conf/icml/LeeHS20",
                    "CorpusId": 220249782
                },
                "corpusId": 220249782,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6bfa8f8b09ec21fe06d4eee28c3fb6cce879be1d",
                "title": "Self-supervised Label Augmentation via Input Transformations",
                "abstract": "Self-supervised learning, which learns by constructing artificial labels given only the input signals, has recently gained considerable attention for learning representations with unlabeled datasets, i.e., learning without any human-annotated supervision. In this paper, we show that such a technique can be used to significantly improve the model accuracy even under fully-labeled datasets. Our scheme trains the model to learn both original and self-supervised tasks, but is different from conventional multi-task learning frameworks that optimize the summation of their corresponding losses. Our main idea is to learn a single unified task with respect to the joint distribution of the original and self-supervised labels, i.e., we augment original labels via self-supervision of input transformation. This simple, yet effective approach allows to train models easier by relaxing a certain invariant constraint during learning the original and self-supervised tasks simultaneously. It also enables an aggregated inference which combines the predictions from different augmentations to improve the prediction accuracy. Furthermore, we propose a novel knowledge transfer technique, which we refer to as self-distillation, that has the effect of the aggregated inference in a single (faster) inference. We demonstrate the large accuracy improvement and wide applicability of our framework on various fully-supervised settings, e.g., the few-shot and imbalanced classification scenarios.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "51122345",
                        "name": "Hankook Lee"
                    },
                    {
                        "authorId": "35788904",
                        "name": "Sung Ju Hwang"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2018); Antonova et al. (2019) embed action sequences with a VAE."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4d5f904f923e5e031fb500a9e9ef7699ea9283de",
                "externalIds": {
                    "DBLP": "conf/icml/AmosY20",
                    "ArXiv": "1909.12830",
                    "MAG": "3034999548",
                    "CorpusId": 203591671
                },
                "corpusId": 203591671,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4d5f904f923e5e031fb500a9e9ef7699ea9283de",
                "title": "The Differentiable Cross-Entropy Method",
                "abstract": "We study the cross-entropy method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant that enables us to differentiate the output of CEM with respect to the objective function's parameters. In the machine learning setting this brings CEM inside of the end-to-end learning pipeline where this has otherwise been impossible. We show applications in a synthetic energy-based structured prediction task and in non-convex continuous control. In the control setting we show how to embed optimal action sequences into a lower-dimensional space. DCEM enables us to fine-tune CEM-based controllers with policy optimization.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1773498",
                        "name": "Brandon Amos"
                    },
                    {
                        "authorId": "13759615",
                        "name": "Denis Yarats"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Anand et al., 2019) use an MI objective to learn representations for Atari games."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "72896b81330c687821263975a4e23ab9661f8c7c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-08114",
                    "ArXiv": "2005.08114",
                    "MAG": "2998340141",
                    "CorpusId": 213390944
                },
                "corpusId": 213390944,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/72896b81330c687821263975a4e23ab9661f8c7c",
                "title": "Mutual Information Maximization for Robust Plannable Representations",
                "abstract": "Extending the capabilities of robotics to real-world complex, unstructured environments requires the need of developing better perception systems while maintaining low sample complexity. When dealing with high-dimensional state spaces, current methods are either model-free or model-based based on reconstruction objectives. The sample inefficiency of the former constitutes a major barrier for applying them to the real-world. The later, while they present low sample complexity, they learn latent spaces that need to reconstruct every single detail of the scene. In real environments, the task typically just represents a small fraction of the scene. Reconstruction objectives suffer in such scenarios as they capture all the unnecessary components. In this work, we present MIRO, an information theoretic representational learning algorithm for model-based reinforcement learning. We design a latent space that maximizes the mutual information with the future information while being able to capture all the information needed for planning. We show that our approach is more robust than reconstruction objectives in the presence of distractors and cluttered scenes",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "46304503",
                        "name": "Yiming Ding"
                    },
                    {
                        "authorId": "15593386",
                        "name": "I. Clavera"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Concurrent work [2] applies similar method on reinforcement learning."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0174d263d3a77bf03fce831a9a5ce2678e1959f0",
                "externalIds": {
                    "MAG": "2972780057",
                    "DBLP": "conf/iccvw/HanXZ19",
                    "ArXiv": "1909.04656",
                    "DOI": "10.1109/ICCVW.2019.00186",
                    "CorpusId": 202542591
                },
                "corpusId": 202542591,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0174d263d3a77bf03fce831a9a5ce2678e1959f0",
                "title": "Video Representation Learning by Dense Predictive Coding",
                "abstract": "The objective of this paper is self-supervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We make three contributions: First, we introduce the Dense Predictive Coding (DPC) framework for self-supervised representation learning on videos. This learns a dense encoding of spatio-temporal blocks by recurrently predicting future representations; Second, we propose a curriculum training scheme to predict further into the future with progressively less temporal context. This encourages the model to only encode slowly varying spatial-temporal signals, therefore leading to semantic representations; Third, we evaluate the approach by first training the DPC model on the Kinetics-400 dataset with self-supervised learning, and then finetuning the representation on a downstream task, i.e. action recognition. With single stream (RGB only), DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "22237490",
                        "name": "Tengda Han"
                    },
                    {
                        "authorId": "10096695",
                        "name": "Weidi Xie"
                    },
                    {
                        "authorId": "1688869",
                        "name": "Andrew Zisserman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The state variables of the Atari games are the controllers of the game dynamics and the details of these state variables are provided in (Anand et al., 2019).",
                ", 2018), TDC (Ma and Collins, 2018) and more recently, ST-DIM (Anand et al., 2019), which is the state-of-the-art baseline to our BVS-DIM method.",
                ", 2019) and ST-DIM (Anand et al., 2019) even though the first two are general purpose representation learning methods tested on real-world data-sets such as ImageNet (Deng et al.",
                "For this task, and to our knowledge, the Spatio-Temporal Deep InfoMax (ST-DIM) (Anand et al., 2019) is the state-of-the-art baseline.",
                ", 2019) and (Anand et al., 2019) for more details on representation learning through MI maximization between inputs and outputs.",
                "with InfoNCE enables the encoder to learn linearly predictable representations and helps in learning representations at the semantic level (Anand et al., 2019).",
                "In ST-DIM, the ground truth state information (a state label for every example frame generated from the game) has been annotated for each frame of 22 Atari games to make evaluation of the goodness of the representation (See (Anand et al., 2019)).",
                "However, the learning method of ST-DIM depends on the consecutive time step visual observations (Anand et al., 2019).",
                "Therefore, most recent unsupervised state representations were introduced like TCN (Sermanet et al., 2018), TDC (Ma and Collins, 2018) and more recently, ST-DIM (Anand et al., 2019), which is the state-of-the-art baseline to our BVS-DIM method.",
                "In Atari games, the crucial underlying generative factors of the environment are state variables which can be directly used to control the game dynamics or query the game information (Bellemare et al., 2013), (Anand et al., 2019).",
                "The bi-linear model in combination with InfoNCE enables the encoder to learn linearly predictable representations and helps in learning representations at the semantic level (Anand et al., 2019).",
                "See (Poole et al., 2019) and (Anand et al., 2019) for more details on representation learning through MI maximization between inputs and outputs.",
                "Our work is closely related to DIM (Hjelm et al., 2019), AMDIM (Bachman et al., 2019) and ST-DIM (Anand et al., 2019) even though the first two are general purpose representation learning methods tested on real-world data-sets such as ImageNet (Deng et al., 2009).",
                "Devising a generic way of measurement of the general goodness of the representation is essential (Anand et al., 2019)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "97d7225f5a5424ba6f37cc55b3861529fcf762df",
                "externalIds": {
                    "DBLP": "conf/icaart/MengistuACL22",
                    "DOI": "10.5220/0010785000003116",
                    "CorpusId": 246935122
                },
                "corpusId": 246935122,
                "publicationVenue": {
                    "id": "f6b96a8f-dc43-4d21-99cf-0f2532b7f01f",
                    "name": "International Conference on Agents and Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Agent Artif Intell",
                        "ICAART"
                    ],
                    "url": "http://www.icaart.org/"
                },
                "url": "https://www.semanticscholar.org/paper/97d7225f5a5424ba6f37cc55b3861529fcf762df",
                "title": "Unsupervised Learning of State Representation using Balanced View Spatial Deep InfoMax: Evaluation on Atari Games",
                "abstract": "In this paper, we present an unsupervised state representation learning of spatio-temporally evolving sequences of autonomous agents\u2019 observations. Our method uses contrastive learning through mutual information (MI) maximization between a sample and the views derived through selection of pixels from the sample and other randomly selected negative samples. Our method employs balancing MI by finding the optimal ratios of positive-to-negative pixels in these derived (constructed) views. We performed several experiments and determined the optimal ratios of positive-to-negative signals to balance the MI between a given sample and the constructed views. The newly introduced method is named as Balanced View Spatial Deep InfoMax (BVSDIM). We evaluated our method on Atari games and performed comparisons with the state-of-the-art unsupervised state representation learning baseline method. We show that our solution enables to successfully learn state representations from sparsely sampled or randomly shuffled observations. Our BVS-DIM method also marginally enhances the representation powers of encoders to capture high-level latent factors of the agents\u2019 observations when compared with the baseline method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154838118",
                        "name": "Menore Tekeba Mengistu"
                    },
                    {
                        "authorId": "40172028",
                        "name": "G. Alemu"
                    },
                    {
                        "authorId": "32730304",
                        "name": "Pierre Chevaillier"
                    },
                    {
                        "authorId": "1758175",
                        "name": "P. D. Loor"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Given a reference input I, a positive sample I, and n \u2212 1 negative samples Ik for 1 \u2264 k < n, standard CSS approaches [6], [8], [9] minimize the loss",
                "In our context, one way to do so is to treat a video frame close to the one of interest as positive, and a temporally-distant one as negative [9], [10].",
                "However, we diverge from standard CSS formulations [6], [8], [9], [10] in the two following aspects: 1) Instead of applying the contrastive loss to the entire latent space, we enforce it only on the time-variant features, as only part of the latent features should evolve over time.",
                "While [20] takes depth as input in addition to RGB, other works [8], [9], [10] leverage CSS, with [10] and [9] using it for only single-view RGB videos.",
                "\u2022 CSS [8], [9]: Single-view CSS loss using Eq."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "40645f708812f69f72d4075b3feaa6fc69d8e138",
                "externalIds": {
                    "CorpusId": 248177985
                },
                "corpusId": 248177985,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/40645f708812f69f72d4075b3feaa6fc69d8e138",
                "title": "Unsupervised Temporal Learning on Monocular Videos for 3D Human Pose Estimation Unsupervised Learning on Monocular Videos for 3D Human Pose Estimation \u2014 Supplementary Information",
                "abstract": "\u2014In this paper we propose an unsupervised learning method to extract temporal information on monocular videos, where we detect and encode subject of interest in each frame and leverage contrastive self-supervised (CSS) learning to extract rich latent vectors. Instead of simply treating the latent features of nearby frames as positive pairs and those of temporally-distant ones as negative pairs as in other CSS approaches, we explicitly disentangle each latent vector into a time-variant component and a time-invariant one. We then show that applying CSS only to the time-variant features and encouraging a gradual transition on them between nearby and away frames while also reconstructing the input, extract rich temporal features into the time-variant component, well-suited for human pose estimation. Our approach reduces error by about 50% compared to the standard CSS strategies, outperforms other unsupervised single-view methods and matches the performance of multi-view techniques.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "25056820",
                        "name": "S. Honari"
                    },
                    {
                        "authorId": "2066226610",
                        "name": "Victor Constantin"
                    },
                    {
                        "authorId": "2933543",
                        "name": "Helge Rhodin"
                    },
                    {
                        "authorId": "2862871",
                        "name": "M. Salzmann"
                    },
                    {
                        "authorId": "153918727",
                        "name": "P. Fua"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In future work, we want to incorporate methods for unsupervised state representation learning (Burgess et al., 2019; Anand et al., 2019) so CEHRL can learn from observations."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7319249a853288dce45ca331a87dce052a2abfba",
                "externalIds": {
                    "DBLP": "conf/clear2/CorcollV22",
                    "CorpusId": 246995633
                },
                "corpusId": 246995633,
                "publicationVenue": {
                    "id": "3d07319c-4f2a-4f30-b619-c295ccd29367",
                    "name": "CLEaR",
                    "type": "conference",
                    "alternate_names": [
                        "Classification of Events, Activities and Relationships",
                        "CLEAR",
                        "CLeaR",
                        "Conf Causal Learn Reason",
                        "Classif Event Act Relatsh",
                        "Conference on Causal Learning and Reasoning"
                    ],
                    "issn": "2453-7128",
                    "url": "http://www.jolace.com/publications/clear/",
                    "alternate_urls": [
                        "https://www.cclear.cc/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7319249a853288dce45ca331a87dce052a2abfba",
                "title": "Disentangling Controlled Effects for Hierarchical Reinforcement Learning",
                "abstract": "Exploration and credit assignment are still challenging problems for RL agents under sparse rewards. We argue that these challenges arise partly due to the intrinsic rigidity of operating at the level of actions. Actions can precisely define how to perform an activity but are ill-suited to describe what activity to perform. Instead, controlled effects describe transformations in the environment caused by the agent. These transformations are inherently composable and temporally abstract, making them ideal for descriptive tasks. This work introduces CEHRL1, a hierarchical method leveraging the compositional nature of controlled effects to expedite the learning of task-specific behavior and aid exploration. Borrowing counterfactual and normality measures from causal literature, CEHRL learns an implicit hierarchy of transformations an agent can perform on the environment. This hierarchy allows a high-level policy to set temporally abstract goals and, by doing so, long-horizon credit assignment. Experimental results show that using effects instead of actions provides a more efficient exploration mechanism. Moreover, by leveraging prior knowledge in the hierarchy, CEHRL assigns credit to few effects instead of many actions and consequently learns tasks more rapidly.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2119528137",
                        "name": "Oriol Corcoll"
                    },
                    {
                        "authorId": "144846212",
                        "name": "Raul Vicente"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Anand et al. (2019) compared unsupervised encoders\u2019 ability to represent various features of the state (e.g., number of opponent sprites).",
                "These classifiers can be used analogously to determine if reinforcement learning agents encode tactical and strategic conceptual information (Anand et al. 2019; McGrath et al. 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "140bbb09c80334c83476e7936fa80884c46e3608",
                "externalIds": {
                    "CorpusId": 247055686
                },
                "corpusId": 247055686,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/140bbb09c80334c83476e7936fa80884c46e3608",
                "title": "Where, When & Which Concepts Does AlphaZero Learn? Lessons from the Game of Hex",
                "abstract": "AlphaZero, an approach to reinforcement learning that couples neural networks and Monte Carlo tree search (MCTS), has produced state-of-the-art strategies for traditional board games like Chess, Go, and Hex. While researchers and game commentators have suggested that AlphaZero uses concepts humans consider important, it is unclear how these concepts are represented in the network. We investigate AlphaZero\u2019s representations in Hex using both model probing and behavioral tests. We find that the MCTS search initially finds important concepts, and then the neural network learns to encode these concepts. Concepts related to short-term end-game planning are best encoded in the final layers of the model, whereas concepts related to long-term planning are encoded in the middle layers of the model.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39774809",
                        "name": "J. Forde"
                    },
                    {
                        "authorId": "10727711",
                        "name": "Charles Lovering"
                    },
                    {
                        "authorId": "1765407",
                        "name": "G. Konidaris"
                    },
                    {
                        "authorId": "2949185",
                        "name": "Ellie Pavlick"
                    },
                    {
                        "authorId": "144885169",
                        "name": "M. Littman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Markov-CPC is a variant of the Contrastive Predictive Coding (CPC) algorithm (Oord et al., 2018) which was shown useful for finding predictive latent variables (Anand et al., 2019; Henaff, 2020; Yan et al., 2020)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3c3514502464272c57b3425fba3efece6b9254ba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-12013",
                    "DOI": "10.48550/arXiv.2205.12013",
                    "CorpusId": 249018054
                },
                "corpusId": 249018054,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3c3514502464272c57b3425fba3efece6b9254ba",
                "title": "Naive Few-Shot Learning: Sequence Consistency Evaluation",
                "abstract": "Cognitive psychologists often use the term \ufb02uid intelligence to describe the ability of humans to solve novel tasks without any prior training. In contrast to humans, deep neural networks can perform cognitive tasks only after extensive (pre-)training with a large number of relevant examples. Motivated by \ufb02uid intelligence research in the cognitive sciences, we built a benchmark task which we call sequence consistency evaluation (SCE) that can be used to address this gap. Solving the SCE task requires the ability to extract simple rules from sequences, a basic computation that in humans, is required for solving various intelligence tests. We tested untrained (naive) deep learning models in the SCE task. Speci\ufb01cally, we tested two networks that can learn latent relations, Relation Networks (RN) and Contrastive Predictive Coding (CPC). We found that the latter, which imposes a causal structure on the latent relations performs better. We then show that naive few-shot learning of sequences can be successfully used for anomaly detection in two different tasks, visual and auditory, without any prior training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2066597096",
                        "name": "T. Barak"
                    },
                    {
                        "authorId": "2934154",
                        "name": "Y. Loewenstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Zhang et al., 2016; Anand et al., 2019).",
                "sigmoid (max{Wsi + b}), to predict VQA-style questions in the form \u201cis there a (size, color, material, shape) object in the image?\u201d (Zhang et al., 2016; Anand et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a0eece3804c7d01e281d12d0a93557eea4025081",
                "externalIds": {
                    "CorpusId": 249825842
                },
                "corpusId": 249825842,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a0eece3804c7d01e281d12d0a93557eea4025081",
                "title": "T OWARDS S ELF -S UPERVISED L EARNING OF G LOBAL AND O BJECT -C ENTRIC R EPRESENTATIONS",
                "abstract": "Self-supervision allows learning meaningful representations of natural images, which usually contain one central object. How well does it transfer to multi-entity scenes? We discuss key aspects of learning structured object-centric representations with self-supervision and validate our insights through several experiments on the CLEVR dataset. Regarding the architecture, we con\ufb01rm the importance of competition for attention-based object discovery, where each image patch is exclusively attended by one object. For training, we show that contrastive losses equipped with matching can be applied directly in a latent space, avoiding pixel-based reconstruction. However, such an optimization objective is sensitive to false negatives (recurring objects) and false positives (matching errors). Careful consid-eration is thus required around data augmentation and negative sample selection.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "38107189",
                        "name": "Federico Baldassarre"
                    },
                    {
                        "authorId": "2622491",
                        "name": "Hossein Azizpour"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Along similar lines, (Srinivas et al., 2020; Anand et al., 2019; Sermanet et al., 2017; Finn et al., 2016; Ghosh et al., 2019; Eysenbach et al., 2022; Li et al., 2021; Agarwal et al., 2021) have begun to explore contrastive systems and binary classifiers to do imitation learning and reinforcement\u2026",
                "Along similar lines, (Srinivas et al., 2020; Anand et al., 2019; Sermanet et al., 2017; Finn et al., 2016; Ghosh et al., 2019; Eysenbach et al., 2022; Li et al., 2021; Agarwal et al., 2021) have begun to explore contrastive systems and binary classifiers to do imitation learning and reinforcement learning."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5b0fa605a28518c4dfb64a1427fab6b0f21c361e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-05845",
                    "DOI": "10.48550/arXiv.2210.05845",
                    "CorpusId": 252846380
                },
                "corpusId": 252846380,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5b0fa605a28518c4dfb64a1427fab6b0f21c361e",
                "title": "Contrastive introspection (ConSpec) to rapidly identify invariant steps for success",
                "abstract": "Reinforcement learning (RL) algorithms have achieved notable success in recent years, but still struggle with fundamental issues in long-term credit assignment. It remains di\ufb03cult to learn in situations where success is contingent upon multiple critical steps that are distant in time from each other and from a sparse reward; as is often the case in real life. Moreover, how RL algorithms assign credit in these di\ufb03cult situations is typically not coded in a way that can rapidly generalize to new situations. Here, we present an approach using o\ufb04ine contrastive learning, which we call contrastive introspection (ConSpec), that can be added to any existing RL algorithm and addresses both issues. In ConSpec, a contrastive loss is used during o\ufb04ine replay to identify invariances among successful episodes. This takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon than it is to prospectively predict reward at every step taken in the environment. ConSpec stores this knowledge in a collection of prototypes summarizing the intermediate states required for success. During training, arrival at any state that matches these prototypes generates an intrinsic reward that is added to any external rewards. As well, the reward shaping provided by ConSpec can be made to preserve the optimal policy of the underlying RL agent. The prototypes in ConSpec provide two key bene\ufb01ts for credit assignment: (1) They enable rapid identi\ufb01cation of all the critical states. (2) They do so in a readily interpretable manner, enabling out of distribution generalization when sensory features are altered. In summary, ConSpec is a modular system that can be added to any existing RL algorithm to improve its long-term credit assignment.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1612977414",
                        "name": "Chen Sun"
                    },
                    {
                        "authorId": "91848984",
                        "name": "Wannan Yang"
                    },
                    {
                        "authorId": "2166566550",
                        "name": "Benjamin Alsbury-Nealy"
                    },
                    {
                        "authorId": "1865800402",
                        "name": "Y. Bengio"
                    },
                    {
                        "authorId": "38498866",
                        "name": "B. Richards"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Along similar lines, (Srinivas et al., 2020; Anand et al., 2019; Sermanet et al., 2017; Finn et al., 2016; Ghosh et al., 2019; Eysenbach et al., 2022; Li et al., 2021; Agarwal et al., 2021) have begun to explore contrastive systems and binary classifiers to do imitation learning and reinforcement\u2026",
                "Along similar lines, (Srinivas et al., 2020; Anand et al., 2019; Sermanet et al., 2017; Finn et al., 2016; Ghosh et al., 2019; Eysenbach et al., 2022; Li et al., 2021; Agarwal et al., 2021) have begun to explore contrastive systems and binary classifiers to do imitation learning and reinforcement learning."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7c6c1bba3f63a11c2f614c72619e45574794e31e",
                "externalIds": {
                    "CorpusId": 253384196
                },
                "corpusId": 253384196,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7c6c1bba3f63a11c2f614c72619e45574794e31e",
                "title": "Contrastive introspection (ConSpec) to rapidly identify invariant prototypes for success in RL",
                "abstract": "Reinforcement learning (RL) algorithms have achieved notable success in recent years, but still struggle with fundamental issues in long-term credit assignment. It remains di\ufb03cult to learn in situations where success is contingent upon multiple critical steps that are distant in time from each other and from a sparse reward; as is often the case in real life. Moreover, how RL algorithms assign credit in these di\ufb03cult situations is typically not coded in a way that can rapidly generalize to new situations. Here, we present an approach using o\ufb04ine contrastive learning, which we call contrastive introspection (ConSpec), that can be added to any existing RL algorithm and addresses both issues. In ConSpec, a contrastive loss is used during o\ufb04ine replay to identify invariances among successful episodes. This takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon than it is to prospectively predict reward at every step taken in the environment. ConSpec stores this knowledge in a collection of prototypes summarizing the intermediate states required for success. During training, arrival at any state that matches these prototypes generates an intrinsic reward that is added to any external rewards. As well, the reward shaping provided by ConSpec can be made to preserve the optimal policy of the underlying RL agent. The prototypes in ConSpec provide two key bene\ufb01ts for credit assignment: (1) They enable rapid identi\ufb01cation of all the critical states. (2) They do so in a readily interpretable manner, enabling out of distribution generalization when sensory features are altered. In summary, ConSpec is a modular system that can be added to any existing RL algorithm to improve its long-term credit assignment.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1612977414",
                        "name": "Chen Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Van den Oord et al. (2018); Guo et al. (2018); Anand et al. (2019); Mazoure et al. (2020) propose different temporal contrastive losses in reinforcement learning environments."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6858bb7b139afe6c581214df1216444d3a790c8b",
                "externalIds": {
                    "DBLP": "conf/acml/WangYWL22",
                    "CorpusId": 259103208
                },
                "corpusId": 259103208,
                "publicationVenue": {
                    "id": "2486528b-036c-4f3c-953f-c574eb381d12",
                    "name": "Asian Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Mach Learn",
                        "ACML"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=40"
                },
                "url": "https://www.semanticscholar.org/paper/6858bb7b139afe6c581214df1216444d3a790c8b",
                "title": "Constrained Contrastive Reinforcement Learning",
                "abstract": "Learning to control from complex observations remains a major challenge in the application of model-based reinforcement learning (MBRL). Existing MBRL methods apply contrastive learning to replace pixel-level reconstruction, improving the performance of the latent world model. However, previous contrastive learning approaches in MBRL fail to utilize task-relevant information, making it difficult to aggregate observations with the same task-relevant information but the different task-irrelevant information in latent space. In this work, we first propose Constrained Contrastive Reinforcement Learning (C2RL), an MBRL method that learns a world model through a combination of two contrastive losses based on latent dynamics and task-relevant state abstraction respectively, utilizing reward information to accelerate model learning. Then, we propose a hyperparameter \u03b2 to balance two kinds of contrastive losses to strengthen the representation ability of the latent dynamics. The experimental results show that our approach outperforms state-ofthe-art methods in both the natural video and standard background setting on challenging DMControl tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109571709",
                        "name": "Haoyu Wang"
                    },
                    {
                        "authorId": "2150442166",
                        "name": "Xinrui Yang"
                    },
                    {
                        "authorId": "2219662882",
                        "name": "Yuhang Wang"
                    },
                    {
                        "authorId": "2498428",
                        "name": "Xuguang Lan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, multiple contrastive methods have been used to learn compact representations for predicting the next state [4, 37, 55]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dfa2c37a7dbcd738e7d6a84caecc0febec67e46e",
                "externalIds": {
                    "DBLP": "conf/bnaic/StarreLO22",
                    "DOI": "10.1007/978-3-031-39144-6_9",
                    "CorpusId": 261559137
                },
                "corpusId": 261559137,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dfa2c37a7dbcd738e7d6a84caecc0febec67e46e",
                "title": "Model-Based Reinforcement Learning with State Abstraction: A Survey",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "74186242",
                        "name": "Rolf A. N. Starre"
                    },
                    {
                        "authorId": "143888579",
                        "name": "M. Loog"
                    },
                    {
                        "authorId": "1799949",
                        "name": "F. Oliehoek"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "987cc0bab6349f264cef483cbb2519301a74fc39",
                "externalIds": {
                    "DBLP": "journals/access/XiaoCCH21",
                    "DOI": "10.1109/ACCESS.2021.3077764",
                    "CorpusId": 234788604
                },
                "corpusId": 234788604,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/987cc0bab6349f264cef483cbb2519301a74fc39",
                "title": "Baseline Model Training in Sensor-Based Human Activity Recognition: An Incremental Learning Approach",
                "abstract": "Human activity recognition (HAR) based on wearable sensors has attracted significant research attention in recent years due to its advantages in availability, accuracy, and privacy-friendliness. HAR baseline model is essentially a general-purpose classifier trained to recognized multiple activity patterns of most user types. It provides the input for subsequent steps of model personalization. Training a good baseline model is of fundamental importance because it has significant impacts on the ultimate HAR accuracy. In practice, baseline model training in HAR is a non-trivial problem that faces two challenges: insufficient training data and biased training data. This paper proposes a novel baseline model training scheme to tackle the two challenges using Deep InfoMax (DIM)-based unsupervised feature extraction and Broad Learning System (BLS)-based incremental learning, respectively. Experimental results demonstrate that the proposed scheme outperform conventional methods in terms of overall accuracy, computational efficiency, and the ability to adapt to dynamic scenarios with changing data characteristics.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49874545",
                        "name": "Jianyu Xiao"
                    },
                    {
                        "authorId": "2120477078",
                        "name": "Linlin Chen"
                    },
                    {
                        "authorId": "2118438627",
                        "name": "Haipeng Chen"
                    },
                    {
                        "authorId": "2098829730",
                        "name": "Xuemin Hong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The learned latent space should contain meaningful lower-dimensional representations of our world [10] and facilitate efficient downstream learning [11, 12], exhibit better generalization [13, 14] and increased interpretability as well as allow for causal reasoning [15].",
                "[12] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C\u00f4t\u00e9, and R Devon Hjelm."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "58b1e771bfeab48d059fe7c20de875a8c0aac08d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-16091",
                    "CorpusId": 235683368
                },
                "corpusId": 235683368,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/58b1e771bfeab48d059fe7c20de875a8c0aac08d",
                "title": "Interventional Assays for the Latent Space of Autoencoders",
                "abstract": "The encoders and decoders of autoencoders effectively project the input onto learned manifolds in the latent space and data space respectively. We propose a framework, called latent responses, for probing the learned data manifold using interventions in the latent space. Using this framework, we investigate \"holes\" in the representation to quantitatively ascertain to what extent the latent space of a trained VAE is consistent with the chosen prior. Furthermore, we use the identified structure to improve interpolation between latent vectors. We evaluate how our analyses improve the quality of the generated samples using the VAE on a variety of benchmark datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "27569366",
                        "name": "Felix Leeb"
                    },
                    {
                        "authorId": "153125952",
                        "name": "Stefan Bauer"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The works most similar to ours, Anand et al. (2019) and Stooke et al. (2020), both propose to use reward-free temporal-contrastive methods to pretrain representations.",
                "Anand et al. (2019) show that representations from encoders trained with ST-DIM contain a great deal of information about environment states, but they do not examine whether or not representations learned via their method are, in fact, useful for reinforcement learning."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fcacc984afe8d65c3c3ca9c321e3cfa9321748ad",
                "externalIds": {
                    "CorpusId": 235805391
                },
                "corpusId": 235805391,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fcacc984afe8d65c3c3ca9c321e3cfa9321748ad",
                "title": "DATA-EFFICIENT REINFORCEMENT LEARNING",
                "abstract": "Data efficiency poses a major challenge for deep reinforcement learning. We approach this issue from the perspective of self-supervised representation learning, leveraging reward-free exploratory data to pretrain encoder networks. We employ a novel combination of latent dynamics modelling and goal-reaching objectives, which exploit the inherent structure of data in reinforcement learning. We demonstrate that our method scales well with network capacity and pretraining data. When evaluated on the Atari 100k data-efficiency benchmark, our approach significantly outperforms previous methods combining unsupervised pretraining with task-specific finetuning, and approaches human-level performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2119002579",
                        "name": "Nitarshan Rajkumar"
                    },
                    {
                        "authorId": "41020834",
                        "name": "Michael Noukhovitch"
                    },
                    {
                        "authorId": "12679121",
                        "name": "Ankesh Anand"
                    },
                    {
                        "authorId": "1778839",
                        "name": "Laurent Charlin"
                    },
                    {
                        "authorId": "88844399",
                        "name": "Devon Hjelm"
                    },
                    {
                        "authorId": "143902541",
                        "name": "Philip Bachman"
                    },
                    {
                        "authorId": "1760871",
                        "name": "Aaron C. Courville"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The learned representation should facilitate efficient downstream learning [26, 29] and exhibit better generalization [30\u201332].",
                "Learning low-dimensional representations that are capturing an environments\u2019 variations for RL agents in control scenarios is also often being described as state representation learning [75]: Methods therein are typically based on autoencoders [76\u201380], video prediction [81, 82] or contrastive learning [26, 83, 84]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d7bcfc6543eefc67083ef788fd007980e3302a12",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-05686",
                    "CorpusId": 235829795
                },
                "corpusId": 235829795,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d7bcfc6543eefc67083ef788fd007980e3302a12",
                "title": "Representation Learning for Out-Of-Distribution Generalization in Reinforcement Learning",
                "abstract": "Learning data representations that are useful for various downstream tasks is a cornerstone of arti\ufb01cial intelligence. While existing methods are typically evaluated on downstream tasks such as classi\ufb01cation or generative image quality, we propose to assess representations through their usefulness in downstream control tasks, such as reaching or pushing objects. By training over 10,000 reinforcement learning policies, we extensively evaluate to what extent different representation properties affect out-of-distribution (OOD) generalization. Finally, we demonstrate zero-shot transfer of these policies from simulation to the real world, without any domain randomization or \ufb01ne-tuning. This paper aims to establish the \ufb01rst systematic characterization of the usefulness of learned representations for real-world OOD downstream tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3431679",
                        "name": "Andrea Dittadi"
                    },
                    {
                        "authorId": "72803733",
                        "name": "F. Tr\u00e4uble"
                    },
                    {
                        "authorId": "36661824",
                        "name": "Manuel W\u00fcthrich"
                    },
                    {
                        "authorId": "47804478",
                        "name": "F. Widmaier"
                    },
                    {
                        "authorId": "2871555",
                        "name": "Peter Gehler"
                    },
                    {
                        "authorId": "1724252",
                        "name": "O. Winther"
                    },
                    {
                        "authorId": "9557137",
                        "name": "Francesco Locatello"
                    },
                    {
                        "authorId": "1936951",
                        "name": "Olivier Bachem"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    },
                    {
                        "authorId": "153125952",
                        "name": "Stefan Bauer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Using the information given in [4], we only count the RAM states corresponding to the controllable avatar.",
                "the underlying RAM states in ALE), as this is a proxy for state coverage that is agnostic to the specific reward function of the game [4]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0dc16391dd10379b3500ff183c98ea0d5a879d10",
                "externalIds": {
                    "DBLP": "conf/nips/HansenDBWHOM21",
                    "CorpusId": 245022029
                },
                "corpusId": 245022029,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0dc16391dd10379b3500ff183c98ea0d5a879d10",
                "title": "Entropic Desired Dynamics for Intrinsic Control",
                "abstract": "An agent might be said, informally, to have mastery of its environment when it has maximised the effective number of states it can reliably reach. In practice, this often means maximizing the number of latent codes that can be discriminated from future states under some short time horizon (e.g. [15]). By situating these latent codes in a globally consistent coordinate system, we show that agents can reliably reach more states in the long term while still optimizing a local objective. A simple instantiation of this idea, Entropic Desired Dynamics for Intrinsic ConTrol (EDDICT), assumes fixed additive latent dynamics, which results in tractable learning and an interpretable latent space. Compared to prior methods, EDDICT\u2019s globally consistent codes allow it to be far more exploratory, as demonstrated by improved state coverage and increased unsupervised performance on hard exploration games such as Montezuma\u2019s Revenge.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35231584",
                        "name": "S. Hansen"
                    },
                    {
                        "authorId": "2755582",
                        "name": "Guillaume Desjardins"
                    },
                    {
                        "authorId": "1734809439",
                        "name": "Kate Baumli"
                    },
                    {
                        "authorId": "1393680089",
                        "name": "David Warde-Farley"
                    },
                    {
                        "authorId": "2801204",
                        "name": "N. Heess"
                    },
                    {
                        "authorId": "2217144",
                        "name": "Simon Osindero"
                    },
                    {
                        "authorId": "3255983",
                        "name": "Volodymyr Mnih"
                    }
                ]
            }
        },
        {
            "contexts": [
                "At each state visited by the agent evaluator during training, the agent\u2019s state (consisting of the avatar\u2019s x and y coordinates within the frame, and potentially also the room number in games with more than one frame in which the agent can move, such as the different rooms in Montezuma\u2019s Revenge) is extracted from the environment\u2019s RAM state using the RAM annotations provided by [1]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8f92e6ab949b3968ddaad6f41c98f1fe959ef272",
                "externalIds": {
                    "CorpusId": 247582895
                },
                "corpusId": 247582895,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8f92e6ab949b3968ddaad6f41c98f1fe959ef272",
                "title": "Entropic Desired Dynamics for Intrinsic Control: Supplemental Material",
                "abstract": "To do so, we train EDDICT alongside a standard policy maximizing task rewards, which we refer to as the task policy. While the EDDICT training procedure remains unchanged, experience for the latter is generated by a behavior policy which randomly switches between EDDICT\u2019s policy and the task policy at regular intervals (every 20 steps). The motivation is similar to recent work [8] on temporally extended -greedy: temporally coherent exploration can more rapidly cover the state space. Two separate networks were used to instantiate EDDICT and the task policy, so any potential benefits must arise from improved exploration rather than e.g. representation regularization via an auxiliary objective. We stress that this is likely to be a sub-par utilization of EDDICT, since this neglects many of it\u2019s unique characteristics (e.g. learning a smooth latent state representation). We have restricted ourselves here to avoid additional complexity and focus on evaluating the exploratory benefits of EDDICT in the most straight-forward way possible.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35231584",
                        "name": "S. Hansen"
                    },
                    {
                        "authorId": "2755582",
                        "name": "Guillaume Desjardins"
                    },
                    {
                        "authorId": "1734809439",
                        "name": "Kate Baumli"
                    },
                    {
                        "authorId": "1393680089",
                        "name": "David Warde-Farley"
                    },
                    {
                        "authorId": "2801204",
                        "name": "N. Heess"
                    },
                    {
                        "authorId": "2217144",
                        "name": "Simon Osindero"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In Anand et al. (2019), the representations for RL algorithms are learned by maximizing mutual information (Hjelm et al., 2019) across spatially and temporally distinct features of an encoder of visual observations.",
                "To address this challenge, a number of deep RL approaches (Sermanet et al., 2018; Dwibedi et al., 2018; Anand et al., 2019; Laskin et al., 2020b; Mazoure et al., 2020; Stooke et al., 2020; Schwarzer\net al., 2021) leverage the recent advance of self-supervised learning which effectively extracts\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "02cd23fbe4ddd6cdad55bdb8895314daa477516d",
                "externalIds": {
                    "CorpusId": 253023353
                },
                "corpusId": 253023353,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/02cd23fbe4ddd6cdad55bdb8895314daa477516d",
                "title": "S ELF -S UPERVISED S PATIAL R EPRESENTATIONS FOR D EEP R EINFORCEMENT L EARNING",
                "abstract": "Recent reinforcement learning (RL) methods have found extracting high-level features from raw pixels with self-supervised learning to be effective in learning policies. However, these methods focus on learning global representations of images, and disregard local spatial structures present in the consecutively stacked frames. In this paper, we propose a novel approach that learns self-supervised spatial representations (SR) for effectively encoding such spatial structures in an unsupervised manner. Given the input frames, the spatial latent volumes are first generated individually using an encoder, and they are used to capture the change in terms of spatial structures, i.e., flow maps among multiple frames. To be specific, the proposed method establishes flow vectors between two latent volumes via a supervision by the image reconstruction loss.This enables for providing plenty of local samples for training the encoder of deep RL. We further attempt to leverage the spatial representations in the self-predictive representations (SPR) method that predicts future representations using the action-conditioned transition model. The proposed method imposes similarity constraints on the three latent volumes; warped query representations by estimated flows, predicted target representations from the transition model, and target representations of future state. Experimental results on complex tasks in Atari Games and DeepMind Control Suite demonstrate that the RL methods are significantly boosted by the proposed self-supervised learning of spatial representations. The code is available at https://sites. google.com/view/iclr2022-s3r.",
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9152569fa14de37380c0010a9f4725af58dec653",
                "externalIds": {
                    "CorpusId": 211839916
                },
                "corpusId": 211839916,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9152569fa14de37380c0010a9f4725af58dec653",
                "title": "L EARNING THE A RROW OF T IME FOR P ROBLEMS IN R EINFORCEMENT L EARNING",
                "abstract": "We humans have an innate understanding of the asymmetric progression of time, which we use to efficiently and safely perceive and manipulate our environment. Drawing inspiration from that, we approach the problem of learning an arrow of time in a Markov (Decision) Process. We illustrate how a learned arrow of time can capture salient information about the environment, which in turn can be used to measure reachability, detect side-effects and to obtain an intrinsic reward signal. Finally, we propose a simple yet effective algorithm to parameterize the problem at hand and learn an arrow of time with a function approximator (here, a deep neural network). Our empirical results span a selection of discrete and continuous environments, and demonstrate for a class of stochastic processes that the learned arrow of time agrees reasonably well with a well known notion of an arrow of time due to Jordan, Kinderlehrer, and Otto (1998).",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40050919",
                        "name": "Nasim Rahaman"
                    },
                    {
                        "authorId": "153201456",
                        "name": "Steffen Wolf"
                    },
                    {
                        "authorId": "1996705",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "150282484",
                        "name": "Roman Remme"
                    },
                    {
                        "authorId": "1751762",
                        "name": "Yoshua Bengio"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, imposing continuity between consecutive states [2], maximizing mutual information with prior states [3],"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3c0b10d18e35aa0d000d9640a74b2f1f253364e6",
                "externalIds": {
                    "CorpusId": 221081478
                },
                "corpusId": 221081478,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3c0b10d18e35aa0d000d9640a74b2f1f253364e6",
                "title": "Modular Latent Space Transfer withAnalytic Manifold Learning",
                "abstract": "Sim2Real can be effective when using photorealistic simulators with accurate physics. However, the target appearance might be unknown a-priori and simulated dynamics might resemble reality only in some aspects. We propose an approach that uses simulation to help learning latent state representations without requiring a match in visual appearance or domain randomization. For this, we learn to encode the dynamics properties of simulated/source domains in a set of independent analytic relations that hold on sequences of low-dimensional simulation states. Then, we impose these relations onto the latent space when learning on the target domain (e.g. reality). Defining independence rigorously allows us to obtain modular representations and ensure that each relation captures a new aspect of the dynamics. This approach also enables transferring common properties from a set of related domains, instead of being confined to modeling a specific task. We show that our approach improves the quality of the latent space of unsupervised learners that train from non-stationary high-dimensional observations. We also outline potential for adaptive partial transfer, which would entail adapting the strength of imposing relations during transfer.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "39534622",
                        "name": "Rika Antonova"
                    },
                    {
                        "authorId": "1975747",
                        "name": "Maksim Maydanskiy"
                    },
                    {
                        "authorId": "1731490",
                        "name": "D. Kragic"
                    },
                    {
                        "authorId": "1693696",
                        "name": "Sam Devlin"
                    },
                    {
                        "authorId": "1380228856",
                        "name": "Katja Hofmann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To address this issue, the work of [36, 37] learn state representations by predicting the future in latent space with a probabilistic contrastive loss, while our work directs the representation learning by reducing error on a downstream target task."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d6c785696fd4f2d19234bb2bc45ff54689cff125",
                "externalIds": {
                    "CorpusId": 228095534
                },
                "corpusId": 228095534,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d6c785696fd4f2d19234bb2bc45ff54689cff125",
                "title": "Learning to Select the Best Forecasting Tasks for Clinical Outcome Prediction",
                "abstract": "The paradigm of \u2018pretraining\u2019 from a set of relevant auxiliary tasks and then \u2018finetuning\u2019 on a target task has been successfully applied in many different domains. However, when the auxiliary tasks are abundant, with complex relationships to the target task, using domain knowledge or searching over all possible pretraining setups is inefficient and suboptimal. To address this challenge, we propose a method to automatically select from a large set of auxiliary tasks, which yields a representation most useful to the target task. In particular, we develop an efficient algorithm that uses automatic auxiliary task selection within a nested-loop metalearning process. We have applied this algorithm to the task of clinical outcome predictions in electronic medical records, learning from a large number of selfsupervised tasks related to forecasting patient trajectories. Experiments on a real clinical dataset demonstrate the superior predictive performance of our method compared to direct supervised learning, naive pretraining and simple multitask learning, in particular in low-data scenarios when the primary task has very few examples. With detailed ablation analysis, we further show that the selection rules are interpretable and able to generalize to unseen target tasks with new data.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1556311931",
                        "name": "Yuan Xue"
                    },
                    {
                        "authorId": "2140321952",
                        "name": "Nan Du"
                    },
                    {
                        "authorId": "2468495",
                        "name": "A. Mottram"
                    },
                    {
                        "authorId": "6454443",
                        "name": "Martin G. Seneviratne"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This combines features of a Spatiotemporal DeepInfomax (ST-DIM) (Anand et al. 2019) and a Variational Autoencoder (VAE) (Kingma and Welling 2013).",
                "This is key since while the VAE section aims to purely encode visual information frame by frame, the ST-DIM section aims to learn a representation that maintains high mutual information between local features in the same spatial location in sequential frames, as well as between the global features and all the next sequential frame\u2019s local features."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "981852f31ddec854f786477cf1b4b28151b001a9",
                "externalIds": {
                    "DBLP": "conf/icccrea/FergusonDLADKW20",
                    "CorpusId": 228097272
                },
                "corpusId": 228097272,
                "publicationVenue": {
                    "id": "5758d639-a450-4152-901d-7a78c8715aa7",
                    "name": "International Conference on Innovative Computing and Cloud Computing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Control Commun  Comput India",
                        "IEEE Int Conf Cogn Comput",
                        "IEEE International Conference Computer and Communications",
                        "Int Carpathian Control Conf",
                        "Int Conf Cogn Comput [services Soc",
                        "Int Conf Comput Cybern",
                        "IEEE International Conference on Cognitive Computing",
                        "IEEE Int Conf Comput Commun",
                        "International Conference on Computer Communication",
                        "Int Conf Innov Comput Cloud Comput",
                        "International Carpathian Control Conference",
                        "International Conference on Computational Creativity",
                        "Int Conf Comput Commun",
                        "International Conference on Control Communication & Computing India",
                        "ICCC",
                        "International Conference on Computational Cybernetics",
                        "Int Conf Comput Creativity",
                        "International Conference on Cognitive Computing [Services Society]",
                        "IEEE Int Conf Commun China",
                        "IEEE International Conference on Communications in China"
                    ],
                    "url": "http://computationalcreativity.net/",
                    "alternate_urls": [
                        "http://www.icccgovernors.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/981852f31ddec854f786477cf1b4b28151b001a9",
                "title": "Automatic Similarity Detection in LEGO Ducks",
                "abstract": "The automated evaluation of creative products promises both good-and-scalable creativity assessments and new forms of visual analysis of whole corpora. Where creative works are not \u2018born digital\u2019, such automated evaluation requires fast and frugal ways of transforming them into data representations that can be meaningfully assessed with common creativity metrics like novelty. In this paper, we report the results of training a Spatiotemporal DeepInfomax Variational Autoencoder (STDIM-VAE) on a digital photo pool of 162 LEGO ducks to generate a phenotypical landscape of clusters of similar ducks and dissimilarity scores for individual ducks. Visual inspection suggests that our system produces plausible results from image pixels alone. We conclude that under certain conditions, STDIM-VAEs may provide fast and frugal ways of automatically assessing corpora of creative works.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2020453671",
                        "name": "M. Ferguson"
                    },
                    {
                        "authorId": "2127156",
                        "name": "Sebastian Deterding"
                    },
                    {
                        "authorId": "2663647",
                        "name": "Andreas Lieberoth"
                    },
                    {
                        "authorId": "2056863408",
                        "name": "Marc Malmdorf Andersen"
                    },
                    {
                        "authorId": "1693696",
                        "name": "Sam Devlin"
                    },
                    {
                        "authorId": "2380005",
                        "name": "D. Kudenko"
                    },
                    {
                        "authorId": "39547552",
                        "name": "James Alfred Walker"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mutual information estimation [8] has inspired a number of successful uses to a single (DIM [9], CPC [10]) and multi-view (AMDIM [11], CMC [12], SimCLR [13]) image classification, reinforcement learning (ST-DIM [14]) and zero-shot learning (CM-DIM [15, 16]).",
                "Further, AMDIM [11], STDIM [14] and CM-DIM [15] incorporate Cross-Local (CL) and Cross-Spatial (CS) objectives."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eff863cdeef334c94605cf505576362a67fdbd3a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-13623",
                    "CorpusId": 229678072
                },
                "corpusId": 229678072,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eff863cdeef334c94605cf505576362a67fdbd3a",
                "title": "Taxonomy of multimodal self-supervised representation learning",
                "abstract": "Sensory input from multiple sources is crucial for robust and coherent human perception. Different sources contribute complementary explanatory factors and get combined based on factors they share. This system motivated the design of powerful unsupervised representation-learning algorithms. In this paper, we unify recent work on multimodal self-supervised learning under a single framework. Observing that most selfsupervised methods optimize similarity metrics between a set of model components, we propose a taxonomy of all reasonable ways to organize this process. We empirically show on two versions of multimodal MNIST and a multimodal brain imaging dataset that (1) multimodal contrastive learning has significant benefits over its unimodal counterpart, (2) the specific composition of multiple contrastive objectives is critical to performance on a downstream task, (3) maximization of the similarity between representations has a regularizing effect on a neural network, which sometimes can lead to reduced downstream performance but still can reveal multimodal relations. Consequently, we outperform previous unsupervised encoderdecoder methods based on CCA or variational mixtures MMVAE on various datasets on linear evaluation protocol.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "26920432",
                        "name": "A. Fedorov"
                    },
                    {
                        "authorId": "8118056",
                        "name": "Tristan Sylvain"
                    },
                    {
                        "authorId": "39175553",
                        "name": "M. Luck"
                    },
                    {
                        "authorId": null,
                        "name": "Lei Wu"
                    },
                    {
                        "authorId": "6659971",
                        "name": "T. DeRamus"
                    },
                    {
                        "authorId": "2042628415",
                        "name": "Alex Kirilin"
                    },
                    {
                        "authorId": "2042627897",
                        "name": "Dmitry Bleklov"
                    },
                    {
                        "authorId": "2122479",
                        "name": "S. Plis"
                    },
                    {
                        "authorId": "144048760",
                        "name": "V. Calhoun"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "747dadfbd2014655108fe647ddb97029e8f7683f",
                "externalIds": {
                    "CorpusId": 235129129
                },
                "corpusId": 235129129,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/747dadfbd2014655108fe647ddb97029e8f7683f",
                "title": "Generalization on Text-based Games using Structured Belief Representations",
                "abstract": "Text-based games are complex, interactive simulations where a player is asked to process the text describing the underlying state of the world to issue textual commands for advancing in a game. Playing these games can be formulated as acting in a partially observable Markov decision process (POMDP), as the player needs to issue actions to reach the goal, by optimizing rewards, given textual observations that may not fully describe the underlying state. Previous art has focused on developing agents to achieve high rewards or faster convergence to the optimal policy for single games. However, with the recent advances in reinforcement learning and representation learning for language we argue it is imperative to start looking for agents that can play a set of games drawn from a distribution of games rather than single games at a time. In this work, we will be looking at TextWorld [17] as a testbed for developing generalizable policies and benchmarking them against previous work. TextWorld is a sandbox environment for training and evaluating reinforcement learning agents on text-based games. TextWorld is suitable to check the generalizability of agents as it enables us to generate hundreds of unique games with varying levels of difficulties. Difficulty in text-based games are determined by a variety of factors like the number of locations in the environment and length of the optimal walkthrough to name a few. Playing text-based games requires skills in sequential decision making and processing language. In this thesis we evaluate the learnt control policies by training them on a set of games and then observing their scores on unseen games during the training phase. We check for the quality of the policies learnt, their ability to generalize on a distribution of games and their ability to transfer on games from different distributions. We define game distributions based on the difficulty level parameterized by the number of locations in the game, number of objects, etc. We propose generalizable and transferrable policies by extracting structured information from the raw textual observations describing the state. Additionally, our agents learn these policies in a purely data-driven fashion without using any handcrafted component \u2013 a common practice found in prior work. Specifically, we learn dynamic knowledge graphs from raw text to represent our agents\u2019 beliefs. The dynamic belief graphs a) allow agents to extract relevant information from text observations and, b) act as memory to act optimally in the POMDP. Experiments on 500+ different games from the TextWorld suite show that our best agent outperforms previous baselines by an average of 24.2%.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "97824511",
                        "name": "D. Adhikari"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This approach was introduced as linear probing for classification [1], and later used in reinforcement learning [2]; therefore we call it linear probing accuracy."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ea2995884db698f064c1bfb8a926f356cd984aa3",
                "externalIds": {
                    "MAG": "3133967254",
                    "DOI": "10.7939/R3-1S8N-TM45",
                    "CorpusId": 235062649
                },
                "corpusId": 235062649,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ea2995884db698f064c1bfb8a926f356cd984aa3",
                "title": "Emergent Representations in Reinforcement Learning and Their Properties",
                "abstract": "This dissertation investigates the properties of representations learned by modern deep reinforcement learning systems. Representation learning plays an important roll in reinforcement learning. A representation contains information extracted from states\u2014the description of the current situation given by the environment. Therefore, a high-quality representation is not only essential to build a robust reinforcement learning agent but also can help improving learning efficiency. Many sub-problems of reinforcement learning, such as planning with model and directed exploration, can be solved more efficiently with a successful agent state discovery. There are a lot of representation learning algorithms that have been proposed. Much of the earlier work in representation learning for reinforcement learning focused on designing fixed-basis architectures to achieve desirable properties, such as orthogonality. In contrast, the idea behind deep reinforcement learning methods is that the agent designer should not encode representational properties, but rather that the data stream should determine the properties of the representation\u2014desired representations will emerge under appropriate training schemes. In this work, we discuss how emergent representations learned with different tasks settings, both with and without auxiliary tasks, perform on properties that people think a good representation has. This thesis (1) empirically investigates how these emergent representations relate to historical notions of good representations, and (2) provides novel insights regarding end-to-end training, the auxiliary task effect, and the utility of successor-feature targets. In particular, we will compare the ii representations learned by several standard architectures by discussing seven representational properties and studying how these properties relate to control and transfer performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2113291319",
                        "name": "Han Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, various different ideas to self-supervised exploration have been proposed [55, 56, 57, 58, 59, 60]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9b754bcba2afad0d568965ae00f3d2f782a972e7",
                "externalIds": {
                    "CorpusId": 247784324
                },
                "corpusId": 247784324,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9b754bcba2afad0d568965ae00f3d2f782a972e7",
                "title": "Ef\ufb01cient Reinforcement Learning via Self-supervised learning and Model-based methods",
                "abstract": "In recent years, Reinforcement Learning systems have led to remarkable accom-plishments. Applications of Reinforcement Learning are vast and range from optimizing the energy consumption of data-centers and more ef\ufb01cient chip design to intelligent autonomous robots. However, Reinforcement Learning algorithms still have their limitations, most notably their lack of data ef\ufb01ciency. Different solutions were proposed that aim to mitigate this problem and in principle, there are two main approaches: Self-supervised learning and Model-based methods. This paper discusses the applications and theoretical foundations of Self-supervised learning and Model-based methods in the Reinforcement Learning context and investigates their individual strengths and weaknesses as well as their commonalities and intersections. The combination of both methods might lead to even better results and therefore, this paper will conclude by proposing potential ways to unify them.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2093433306",
                        "name": "Thomas Schmied"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fb4584bc6c111e94cc507729a0942c82941d979e",
                "externalIds": {
                    "MAG": "3127352392",
                    "DOI": "10.7939/R3-1JP7-XH06",
                    "CorpusId": 234952654
                },
                "corpusId": 234952654,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fb4584bc6c111e94cc507729a0942c82941d979e",
                "title": "Learning Online-Aware Representations using Neural Networks",
                "abstract": "Learning online is essential for an agent to perform well in an ever-changing world. An agent has to learn online not only out of necessity \u2014 a nonstationary world might render past learning useless \u2014 but also because continual tracking in a temporally coherent world can result in better performance than a fixed solution. Despite the necessity of online learning, we have made little progress towards building robust online learning methods. More specifically, a scalable online representation learning method for neural network function approximators has remained elusive. In this thesis, I investigate the reasons behind this lack of progress. I propose the idea of online-aware representations \u2013 data representations explicitly optimized for online learning \u2013 and argue that existing representation learning methods do not learn such representations. I investigate if neural networks are capable of learning these representations. My results suggest that neural networks can indeed learn representations that are highly effective for online learning, but learning these representations online using gradient-based methods is challenging. More specifically, long-term credit assignment using back-propagation through time (BPTT) does not scale with the size of the problem. To address this, I propose Learning with Backtracking for slowly and continually improving representations online. The primary idea behind LwB is that while it is not possible to compute an accurate estimate of the representation update online, it is possible to verify if an update is useful online.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35722350",
                        "name": "Khurram Javed"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03b349084f7b8a23ea308b996f222cf48db91c2c",
                "externalIds": {
                    "CorpusId": 251844250
                },
                "corpusId": 251844250,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/03b349084f7b8a23ea308b996f222cf48db91c2c",
                "title": "NeurIPS 2019 Reproducibility [Re] Unsupervised Representation Learning in Atari",
                "abstract": "In this study, we performed some ablations on the main model developed in the paper Unsupervised Representation Learning in Atari [1] as part of the 2019 NeurIPS Reproducibility Challenge. In this paper, Anand et. al introduce a new learning method called SpatioTemporal DeepInfoMax (STDIM), which is an unsupervised method that aims at learning state representations by maximizing particular forms of mutual information between a series of observations. Our work focuses on recreating a subset of their results, along with hyperparameter tuning, slightly altering the STDIM learning objective, and altering the receptive field of the encoder model that Anand et. al introduceintheirarticle. WealsosuggestdirectionsforfurtherexpandingtheSTDIM method. Our results also suggest that creating an ensemble model would allow for further boosting of the effectiveness of this model.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2037849433",
                        "name": "Gabriel Alacchi"
                    },
                    {
                        "authorId": "2037852669",
                        "name": "Guillaume Lam"
                    },
                    {
                        "authorId": "2151090392",
                        "name": "Carl Perreault-Lafleur"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Anand et al. (2019) use the NCE loss to discriminate between temporally near frames and temporally far frames of ATARI gameplay but do not compare across games."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6313aed3c4e6588de76544d1ecf002ec02ea0ce2",
                "externalIds": {
                    "CorpusId": 252014961
                },
                "corpusId": 252014961,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6313aed3c4e6588de76544d1ecf002ec02ea0ce2",
                "title": "W ATCHING THE W ORLD G O B Y : R EPRESENTATION L EARNING FROM U NLABELED V IDEOS",
                "abstract": "Recent unsupervised representation learning techniques show remarkable success on many single image tasks by using instance discrimination: learning to differ-entiate between two augmented versions of the same image and a large batch of unrelated images. Prior work uses arti\ufb01cial data augmentation techniques such as cropping, and color jitter which can only affect the image in super\ufb01cial ways and are not aligned with how objects actually change e.g. occlusion, deformation, viewpoint change. We argue that videos offer this natural augmentation for free. Videos can provide entirely new views of objects, show deformation, and even connect semantically similar but visually distinct concepts. We propose Video Noise Contrastive Estimation, a method for using unlabeled video to learn strong, transferable, single image representations. We demonstrate improvements over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining , across temporal and non-temporal tasks.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "contexts": [
                "However it\u2019s unclear if this extension would yield linearly interpretable representations as effectively as the bivariate case of InfoNCE as introduced by van den Oord [5].",
                "The InfoNCE objective is to learn a score function f which maximizes the following estimate which acts as a lower bound of the mutual information between X and Y .",
                "al which contrasts the global output of the encoder at both time steps, hence creating a new global-global objective based on InfoNCE [2].",
                "al use a receptive field size of 1/16 without justification in the paper [2].",
                "Following the work of van den Oord [5] on InfoNCE, STDIM uses a bilinear score function for both of these objectives:\ngm,n(xt, xt\u2032) = \u03c6(xt) T \u00b7Wg \u00b7 \u03c6m,n(xt\u2032) and fm,n(xt, xt\u2032) = \u03c6m,n(xt)T \u00b7Wl \u00b7 \u03c6m,n(xt\u2032) (3)\nwhere \u03c6 is the output of the encoder, \u03c6m,n is the local feature vector produced by an intermediate convolution layer of the encoder at the (m,n) spatial location, and Wg,Wl are learned weights.",
                "Doing this would require extending the definition of the InfoNCE objective to three variables.",
                "STDIM simultaneously trains two InfoNCE objectives, the global-local objective (GL) and the local-local objective (LL).",
                "The methods introduced by Anand et. al [2] rely on the InfoNCE mutual information bound [5].",
                "Abstract In this study, we performed some ablations on the main model developed in the paper \"Unsupervised Representation Learning in Atari\" [2] as part of the 2019 NeurIPS Reproducibility Challenge.",
                "al consider is using a pretrained agent with -greedy exploration added in [2].",
                "This ablation is similar to an ablation by Anand et. al which contrasts the global output of the encoder at both time steps, hence creating a new global-global objective based on InfoNCE [2].",
                "The InfoNCE objective is derived from mini-batches of consecutive observations {(xt, xt+1)i}Bi=1 given by the agent\u2019s interactions with the environment.",
                "When training an encoder using these objectives, Wg and Wl are learned to find the maximum for each InfoNCE bound.",
                "al [2] rely on the InfoNCE mutual information bound [5]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "8bac8c033b17ecc22cc8c94762eead657961da1d",
                "externalIds": {
                    "CorpusId": 249338697
                },
                "corpusId": 249338697,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8bac8c033b17ecc22cc8c94762eead657961da1d",
                "title": "NeurIPS REPRODUCIBILITY CHALLENGE ABLATION TRACK",
                "abstract": "In this study, we performed some ablations on the main model developed in the paper \"Unsupervised Representation Learning in Atari\" [2] as part of the 2019 NeurIPS Reproducibility Challenge. In this paper, Anand et. al introduce a new learning method called SpatioTemporal DeepInfoMax (STDIM), which is an unsupervised method that aims at learning state representations by maximizing particular forms of mutual information between a series of observations. Our work focuses on recreating a subset of their results, along with hyperparameter tuning, slightly altering the STDIM learning objective, and altering the receptive \ufb01eld of the encoder model that Anand et. al introduce in their article. We also suggest directions for further expanding the STDIM method. Our results also suggest that creating an ensemble model would allow for further boosting of the e\ufb00ectiveness of this model.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2037852669",
                        "name": "Guillaume Lam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also discuss whether their proposed benchmark is more effective at (1) transferring knowledge between tasks (in the same environment) and (2) learning with fewer interactions."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3c1547e161beb1d0e170a5d23a592fe433dc88af",
                "externalIds": {
                    "CorpusId": 251198741
                },
                "corpusId": 251198741,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3c1547e161beb1d0e170a5d23a592fe433dc88af",
                "title": "Unsupervised Representation Learning in Atari",
                "abstract": "In this study, we reproduced the paper \"Unsupervised Representation Learning in Atari\" as part of the NeurIPS 2019 Reproducibility Challenge. The original paper presents ST-DIM, or Spatiotemporal Deep Infomax, which is an unsupervised method for learning useful state representations. ST-DIM attempts to learn an encoding that maximizes shared information between sequential game frames. The authors propose that this method allows for more effective learning of meaningful environmental features, such as the game state variables of Atari games. We review the authors\u2019 goals, claims, and results. We also discuss our process, challenges encountered, and results of our work.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "41018283",
                        "name": "Shekar Ramaswamy"
                    },
                    {
                        "authorId": "2241965323",
                        "name": "Lawrence Huang"
                    },
                    {
                        "authorId": "2242319564",
                        "name": "Kendrick Tan"
                    }
                ]
            }
        }
    ]
}