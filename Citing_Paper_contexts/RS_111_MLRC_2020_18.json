{
    "offset": 0,
    "data": [
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Motivated by prior research [5, 6], this study aims to develop a method that captures the inductive bias of energy changes in rigid bodies as external conditions vary while preserving the high-precision modeling of 6-DoF equations for rigid bodies and the high-precision forward and backward sliding along the temporal dimension.",
                "Building upon, the authors in [5, 6] proposed a deep generative model termed the Hamiltonian generative network (HGN), which can learn the Hamiltonian dynamics of continuous-time evolution systems, exhibiting features such as time reversibility and smooth temporal interpolation."
            ],
            "citingPaper": {
                "paperId": "91cec28c19dc5288f1a9f669a4a0f42ca9e566af",
                "externalIds": {
                    "DBLP": "journals/complexity/FeiLSHLJLQ23",
                    "DOI": "10.1155/2023/8882781",
                    "CorpusId": 261599042
                },
                "corpusId": 261599042,
                "publicationVenue": {
                    "id": "8bc59e8b-e251-4201-839a-ec83ae78859d",
                    "name": "Complex",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Complex Sci",
                        "International Conference on Complex Sciences"
                    ],
                    "issn": "0806-1912",
                    "alternate_issns": [
                        "1538-6848"
                    ],
                    "url": "http://wo.uio.no/as/WebObjects/nettlogg.woa/1/wa/logg?logg=5904",
                    "alternate_urls": [
                        "http://www.wikicfp.com/cfp/program?id=545",
                        "https://www.complex.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/91cec28c19dc5288f1a9f669a4a0f42ca9e566af",
                "title": "Hamiltonian Neural Network 6-DoF Rigid-Body Dynamic Modeling Based on Energy Variation Estimation",
                "abstract": "This study introduces a novel deep modeling approach that utilizes Hamiltonian neural networks to address the challenges of modeling the six degrees of freedom rigid-body dynamics induced by control inputs in various domains such as aerospace, robotics, and automotive engineering. The proposed method is based on the principles of Hamiltonian dynamics and employs an inductive bias in the form of a constructed bias for both conservative and varying energies, effectively tackling the modeling issues arising from time-varying energy in controlled rigid-body dynamics. This constructed bias captures the information regarding the changes in the rigid body\u2019s energy. The presented method not only achieves highly accurate modeling but also preserves the inherent bidirectional time-sliding inference in Hamiltonian-based modeling approaches. Experimental results demonstrate that our method outperforms existing techniques in the time-varying six degrees of freedom dynamic modeling of aircraft and missile guidance, enabling high-precision modeling and feedback correction. The findings of our research hold significant potential for the kinematic modeling of time-varying energy systems, parallel system state prediction and control, inverse motion inference, and autonomous decision-making in military applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2238340189",
                        "name": "Simiao Fei"
                    },
                    {
                        "authorId": "2238387804",
                        "name": "Huo Lin"
                    },
                    {
                        "authorId": "1978589018",
                        "name": "Zhixiao Sun"
                    },
                    {
                        "authorId": "2238400934",
                        "name": "Wang He"
                    },
                    {
                        "authorId": "2238311818",
                        "name": "Yuanjie Lu"
                    },
                    {
                        "authorId": "2238335404",
                        "name": "He Jile"
                    },
                    {
                        "authorId": "2238340425",
                        "name": "Luo Qing"
                    },
                    {
                        "authorId": "2238341209",
                        "name": "Qihang Su"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Some further leverage Hamiltonian separability [19], [21] by using a Leapfrog integrator."
            ],
            "citingPaper": {
                "paperId": "8dd7bdabbb3617eade42908ff416f54993906c34",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-01069",
                    "ArXiv": "2309.01069",
                    "DOI": "10.48550/arXiv.2309.01069",
                    "CorpusId": 261531273
                },
                "corpusId": 261531273,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8dd7bdabbb3617eade42908ff416f54993906c34",
                "title": "Separable Hamiltonian Neural Networks",
                "abstract": "The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additive separability within Hamiltonian neural networks. The first model uses additive separability to quadratically scale the amount of data for training Hamiltonian neural networks. The second model embeds additive separability within the loss function of the Hamiltonian neural network. The third model embeds additive separability through the architecture of the Hamiltonian neural network using conjoined multilayer perceptions. We empirically compare the three models against state-of-the-art Hamiltonian neural networks, and demonstrate that the separable Hamiltonian neural networks, which alleviate complexity between the state variables, are more effective at regressing the Hamiltonian and its vector field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2041872418",
                        "name": "Zi-Yu Khoo"
                    },
                    {
                        "authorId": "2237809772",
                        "name": "Jonathan Sze Choong Low"
                    },
                    {
                        "authorId": "2237808597",
                        "name": "St'ephane Bressan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "[47] use a latent Hamiltonian neural network in a VAE to learn dynamics without control, prior knowledge of the configuration-space structure or dimension.",
                "Previous efforts in learning dynamics from images [23, 47, 4, 54] consider only 2D planar systems (e.",
                "The model is compared to three baseline models: (1) an LSTM-baseline, (2) a Neural ODE [12]-baseline, and (3) the HGN [47] model.",
                "The authors of [54, 4, 47] leverage Hamiltonian and Lagrangian neural networks to learn the dynamics of 2D rigid bodies (e.",
                "We evaluate our model and compare to three baseline models: (1) recurrent model (LSTM [26]), (2) NeuralODE ([12]), (3) HGN ([47]).",
                "Previous work [54, 47, 4] has made significant progress in using physics-based priors to learn dynamics from images of 2D rigid bodies, such as a pendulum.",
                "This is a generalization of the existing literature where dynamics of canonical Hamiltonian systems are learned with the canonical symplectic form as the physics prior [23, 16, 13, 47].",
                "The combination of deep learning with physics-based priors allows models to learn dynamics from high-dimensional data such as images [54, 47, 4].",
                "[47], we use a latent Hamiltonian neural network to learn dynamics.",
                "This formulation has been used by several authors to learn unknown dynamics: the Hamiltonian structure (canonical symplectic form) is used as a physics prior and the unknown dynamics are uncovered by learning the Hamiltonian [23, 56, 47]."
            ],
            "citingPaper": {
                "paperId": "9a6b88c80decebcb322227432e3712a4af927cae",
                "externalIds": {
                    "ArXiv": "2308.14666",
                    "DBLP": "journals/corr/abs-2308-14666",
                    "DOI": "10.48550/arXiv.2308.14666",
                    "CorpusId": 259089395
                },
                "corpusId": 259089395,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9a6b88c80decebcb322227432e3712a4af927cae",
                "title": "Learning to predict 3D rotational dynamics from images of a rigid body with unknown mass distribution",
                "abstract": "In many real-world settings, image observations of freely rotating 3D rigid bodies, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics. The usefulness of standard deep learning methods is also limited because an image of a rigid body reveals nothing about the distribution of mass inside the body, which, together with initial angular velocity, is what determines how the body will rotate. We present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to $\\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion. We demonstrate the efficacy of our approach on new rotating rigid-body datasets of sequences of synthetic images of rotating objects, including cubes, prisms and satellites, with unknown uniform and non-uniform mass distributions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34819822",
                        "name": "J. Mason"
                    },
                    {
                        "authorId": "1405669277",
                        "name": "Christine Allen-Blanchette"
                    },
                    {
                        "authorId": "102569860",
                        "name": "Nick Zolman"
                    },
                    {
                        "authorId": "2178760411",
                        "name": "Elizabeth Davison"
                    },
                    {
                        "authorId": "47583055",
                        "name": "N. Leonard"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "The authors in (Toth et al. 2019; Saemundsson et al. 2020; Higgins et al. 2021) circumvent this challenge by using a standard Gaussian prior and a phase-space of substantially higher dimension than the dynamical system underlying the video.",
                "2020; Zhong and Leonard 2020) for video prediction and (Toth et al. 2019; Saemundsson et al. 2020) for variational autoencoding (VAE) (Kingma and Welling 2013) based video generation.",
                "Note that we do not compare against HGN since it is designed to model a single system, and color is considered a system parameter.",
                "As evidenced by Figure 6, our model produces qualitatively better videos than HGN.",
                "We compare the performance of our model against the HGN, and MoCoGAN baseline models on the CCC dataset.",
                "HGN (Toth et al. 2019): A VAE-based video generation\napproach in which the latent vector is interpreted as an element of the phase-space and propagated forward in time with an HNN (Greydanus, Dzamba, and Yosinski 2019) cell.",
                "2020; Zhong and Leonard 2020), or select the dimension to be arbitrarily large for model flexibility (Toth et al. 2019; Saemundsson et al. 2020).",
                "2019) and in VAE-based video generation pipelines where ground truth position-momentum values are assumed to be unknown (Toth et al. 2019; Saemundsson et al. 2020).",
                "HGN (Toth et al. 2019): A VAE-based video generation approach in which the latent vector is interpreted as an element of the phase-space and propagated forward in time with an HNN (Greydanus, Dzamba, and Yosinski 2019) cell.",
                "The work in (Toth et al. 2019) and (Gordon and Parde 2021) are most similar to ours.",
                "While videos generated with our model have a foreground more similar to that of the real data, those generated with HGN have a more similar background.",
                "There is a significant difference in the FVD score of the leading model (HGN), and our model on the Three-body case.",
                "We empirically evaluate our model on two versions of the Toy Physics dataset (Toth et al. 2019), one with constant physical parameters and colors and another where the physical parameters and colors vary.",
                "We attribute the discrepancy in FVD score to the difference in the background color of videos generated with our model and HGN.",
                "(Toth et al. 2019) use the Hamiltonian formalism as an inductive bias for VAEbased video generation.",
                "The Hamiltonian formalism has been used as an inductive bias in physics-guided video generation in several recent works (Toth et al. 2019; Saemundsson et al. 2020; Zhong and Leonard 2020; Higgins et al. 2021).",
                "HGN and MoCoGAN are trained using the hyperparameters provided in the original paper.",
                "Following HGN we evaluate all models on the version of this dataset generated without friction, with constant physical quantities across trajectories, and with constant color.",
                "We use the pytorch implementation of HGN introduced in (Rodas, Canal, and Taschin 2021)."
            ],
            "citingPaper": {
                "paperId": "98c05257d4bbc7c014f1639c1d7fa851af8c5ab1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-11216",
                    "ArXiv": "2308.11216",
                    "DOI": "10.48550/arXiv.2308.11216",
                    "CorpusId": 261064856
                },
                "corpusId": 261064856,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/98c05257d4bbc7c014f1639c1d7fa851af8c5ab1",
                "title": "Hamiltonian GAN",
                "abstract": "A growing body of work leverages the Hamiltonian formalism as an inductive bias for physically plausible neural network based video generation. The structure of the Hamiltonian ensures conservation of a learned quantity (e.g., energy) and imposes a phase-space interpretation on the low-dimensional manifold underlying the input video. While this interpretation has the potential to facilitate the integration of learned representations in downstream tasks, existing methods are limited in their applicability as they require a structural prior for the configuration space at design time. In this work, we present a GAN-based video generation pipeline with a learned configuration space map and Hamiltonian neural network motion model, to learn a representation of the configuration space from data. We train our model with a physics-inspired cyclic-coordinate loss function which encourages a minimal representation of the configuration space and improves interpretability. We demonstrate the efficacy and advantages of our approach on the Hamiltonian Dynamics Suite Toy Physics dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1405669277",
                        "name": "Christine Allen-Blanchette"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5815075c70bf5113d4ab2ab4fc078913674a27ce",
                "externalIds": {
                    "DOI": "10.1145/3611383",
                    "CorpusId": 260380494
                },
                "corpusId": 260380494,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5815075c70bf5113d4ab2ab4fc078913674a27ce",
                "title": "Machine Learning and Physics: A Survey of Integrated Models",
                "abstract": "Predictive modeling of various systems around the world is extremely essential from the physics and engineering perspectives. The recognition of different systems and the capacity to predict their future behavior can lead to numerous significant applications. For the most part, physics is frequently used to model different systems. Using physical modeling can also very well help the resolution of complexity and achieve superior performance with the emerging field of novel artificial intelligence and the challenges associated with it. Physical modeling provides data and knowledge that offer meaningful and complementary understanding about the system. So, by using enriched data and training phases, the overall general integrated model achieves enhanced accuracy. The effectiveness of hybrid physics-guided or machine learning-guided models has been validated by experimental results of diverse use cases. Increased accuracy, interpretability, and transparency are the results of such hybrid models. In this paper, we provide a detailed overview of how machine learning and physics can be integrated into an interactive approach. Regarding this, we propose a classification of possible interactions between physical modeling and machine learning techniques. Our classification includes three types of approaches: (1) physics-guided machine learning (2) machine learning-guided physics, and (3) mutually-guided physics and ML. We studied the models and specifications for each of these three approaches in-depth for this survey.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226458277",
                        "name": "Azra Seyyedi"
                    },
                    {
                        "authorId": "2793286",
                        "name": "M. Bohlouli"
                    },
                    {
                        "authorId": "2199182748",
                        "name": "SeyedEhsan Nedaaee Oskoee"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "90ede06ec7692eee8b787036e0b3ddab7f2fbed5",
                "externalIds": {
                    "DOI": "10.1117/12.2664115",
                    "CorpusId": 259187058
                },
                "corpusId": 259187058,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/90ede06ec7692eee8b787036e0b3ddab7f2fbed5",
                "title": "Skeleton-based human action recognition with a physics-augmented encoder-decoder network",
                "abstract": "Human action recognition is important for many applications such as surveillance monitoring, safety, and healthcare. As 3D body skeletons can accurately characterize body actions and are robust to camera views, we propose a 3D skeleton-based human action method. Different from the existing skeleton-based methods that use only geometric features for action recognition, we propose a physics-augmented encoder and decoder model that produces physically plausible geometric features for human action recognition. Specifically, given the input skeleton sequence, the encoder performs a spatiotemporal graph convolution to produce spatiotemporal features for both predicting human actions and estimating the generalized positions and forces of body joints. The decoder, implemented as an ODE solver, takes the joint forces and solves the Euler-Lagrangian equation to reconstruct the skeletons in the next frame. By training the model to simultaneously minimize the action classification and the 3D skeleton reconstruction errors, the encoder is ensured to produce features that are consistent with both body skeletons and the underlying body dynamics as well as being discriminative. The physics-augmented spatiotemporal features are used for human action classification. We evaluate the proposed method on NTU-RGB+D, a large-scale dataset for skeleton-based action recognition. Compared with existing methods, our method achieves higher accuracy and better generalization ability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111304212",
                        "name": "Hongjian Guo"
                    },
                    {
                        "authorId": "2220211143",
                        "name": "Alexander Aved"
                    },
                    {
                        "authorId": "2220209155",
                        "name": "Collen Roller"
                    },
                    {
                        "authorId": "2191908153",
                        "name": "Erika Ardiles-Cruz"
                    },
                    {
                        "authorId": "2143639736",
                        "name": "Q. Ji"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "There are also numerous pieces of research [22, 45, 8, 43] focusing on recovering the Hamiltonian, and predicting the dynamics of certain physical systems based on observed trajectories."
            ],
            "citingPaper": {
                "paperId": "0a4736a98546de45759c0a260e0280e94e4c2591",
                "externalIds": {
                    "ArXiv": "2306.00191",
                    "DBLP": "journals/corr/abs-2306-00191",
                    "DOI": "10.48550/arXiv.2306.00191",
                    "CorpusId": 258999543
                },
                "corpusId": 258999543,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0a4736a98546de45759c0a260e0280e94e4c2591",
                "title": "Parameterized Wasserstein Hamiltonian Flow",
                "abstract": "In this work, we propose a numerical method to compute the Wasserstein Hamiltonian flow (WHF), which is a Hamiltonian system on the probability density manifold. Many well-known PDE systems can be reformulated as WHFs. We use parameterized function as push-forward map to characterize the solution of WHF, and convert the PDE to a finite-dimensional ODE system, which is a Hamiltonian system in the phase space of the parameter manifold. We establish error analysis results for the continuous time approximation scheme in Wasserstein metric. For the numerical implementation, we use neural networks as push-forward maps. We apply an effective symplectic scheme to solve the derived Hamiltonian ODE system so that the method preserves some important quantities such as total energy. The computation is done by fully deterministic symplectic integrator without any neural network training. Thus, our method does not involve direct optimization over network parameters and hence can avoid the error introduced by stochastic gradient descent (SGD) methods, which is usually hard to quantify and measure. The proposed algorithm is a sampling-based approach that scales well to higher dimensional problems. In addition, the method also provides an alternative connection between the Lagrangian and Eulerian perspectives of the original WHF through the parameterized ODE dynamics.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119795510",
                        "name": "Hao Wu"
                    },
                    {
                        "authorId": "2108422879",
                        "name": "Shu Liu"
                    },
                    {
                        "authorId": "145315788",
                        "name": "X. Ye"
                    },
                    {
                        "authorId": "48053919",
                        "name": "Haomin Zhou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[23] Peter Toth, Danilo J Rezende, Andrew Jaegle, S\u00e9bastien Racani\u00e8re, Aleksandar Botev, and Irina Higgins.",
                "Structure-preserving dense networks: For dense networks, it is relatively straightforward to parameterize reversible dynamics, see for example: Hamiltonian neural networks [19, 20, 21, 22], Hamiltonian generative networks [23], Hamiltonian with Control (SymODEN) [24], Deep Lagrangian networks [25] and Lagrangian neural networks [26]."
            ],
            "citingPaper": {
                "paperId": "71395e0be94802971611ec88130af780d4ba90d0",
                "externalIds": {
                    "ArXiv": "2305.15616",
                    "DBLP": "journals/corr/abs-2305-15616",
                    "DOI": "10.48550/arXiv.2305.15616",
                    "CorpusId": 258887954
                },
                "corpusId": 258887954,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/71395e0be94802971611ec88130af780d4ba90d0",
                "title": "Reversible and irreversible bracket-based dynamics for deep graph neural networks",
                "abstract": "Recent works have shown that physics-inspired architectures allow the training of deep graph neural networks (GNNs) without oversmoothing. The role of these physics is unclear, however, with successful examples of both reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable results despite diametrically opposed mechanisms, and further complications arising due to empirical departures from mathematical theory. This work presents a series of novel GNN architectures based upon structure-preserving bracket-based dynamical systems, which are provably guaranteed to either conserve energy or generate positive dissipation with increasing depth. It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "115023648",
                        "name": "A. Gruber"
                    },
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "1453887310",
                        "name": "N. Trask"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In order to improve the stabilty of the HNNs, Chen et al. (2020), and Toth et al. (2020) introduced an other physical constraint to the HNNs by applying symplectic integrator for deriving states from HNNs."
            ],
            "citingPaper": {
                "paperId": "e0dfbae90cafa8756755e26e9ebd6a84df034a56",
                "externalIds": {
                    "ArXiv": "2305.01338",
                    "DBLP": "journals/corr/abs-2305-01338",
                    "DOI": "10.48550/arXiv.2305.01338",
                    "CorpusId": 258437262
                },
                "corpusId": 258437262,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e0dfbae90cafa8756755e26e9ebd6a84df034a56",
                "title": "Physics-Informed Learning Using Hamiltonian Neural Networks with Output Error Noise Models",
                "abstract": "In order to make data-driven models of physical systems interpretable and reliable, it is essential to include prior physical knowledge in the modeling framework. Hamiltonian Neural Networks (HNNs) implement Hamiltonian theory in deep learning and form a comprehensive framework for modeling autonomous energy-conservative systems. Despite being suitable to estimate a wide range of physical system behavior from data, classical HNNs are restricted to systems without inputs and require noiseless state measurements and information on the derivative of the state to be available. To address these challenges, this paper introduces an Output Error Hamiltonian Neural Network (OE-HNN) modeling approach to address the modeling of physical systems with inputs and noisy state measurements. Furthermore, it does not require the state derivatives to be known. Instead, the OE-HNN utilizes an ODE-solver embedded in the training process, which enables the OE-HNN to learn the dynamics from noisy state measurements. In addition, extending HNNs based on the generalized Hamiltonian theory enables to include external inputs into the framework which are important for engineering applications. We demonstrate via simulation examples that the proposed OE-HNNs results in superior modeling performance compared to classical HNNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2100517697",
                        "name": "Sarvin Moradi"
                    },
                    {
                        "authorId": "32472333",
                        "name": "N. Jaensson"
                    },
                    {
                        "authorId": "2215866736",
                        "name": "Roland T'oth"
                    },
                    {
                        "authorId": "1767103",
                        "name": "M. Schoukens"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "2f403d194b42d10c3a438736388c8812831b1361",
                "externalIds": {
                    "ArXiv": "2304.12944",
                    "DBLP": "conf/icml/SongKSW23",
                    "DOI": "10.48550/arXiv.2304.12944",
                    "CorpusId": 258309133
                },
                "corpusId": 258309133,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f403d194b42d10c3a438736388c8812831b1361",
                "title": "Latent Traversals in Generative Models as Potential Flows",
                "abstract": "Despite the significant recent progress in deep generative models, the underlying structure of their latent spaces is still poorly understood, thereby making the task of performing semantically meaningful latent traversals an open research challenge. Most prior work has aimed to solve this challenge by modeling latent structures linearly, and finding corresponding linear directions which result in `disentangled' generations. In this work, we instead propose to model latent structures with a learned dynamic potential landscape, thereby performing latent traversals as the flow of samples down the landscape's gradient. Inspired by physics, optimal transport, and neuroscience, these potential landscapes are learned as physically realistic partial differential equations, thereby allowing them to flexibly vary over both space and time. To achieve disentanglement, multiple potentials are learned simultaneously, and are constrained by a classifier to be distinct and semantically self-consistent. Experimentally, we demonstrate that our method achieves both more qualitatively and quantitatively disentangled trajectories than state-of-the-art baselines. Further, we demonstrate that our method can be integrated as a regularization term during training, thereby acting as an inductive bias towards the learning of structured representations, ultimately improving model likelihood on similarly structured data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152601878",
                        "name": "Yue Song"
                    },
                    {
                        "authorId": "2215270642",
                        "name": "Andy Keller"
                    },
                    {
                        "authorId": "1429806753",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d520d069f58460a8c806f7591d4498e34831d68f",
                "externalIds": {
                    "ArXiv": "2303.17824",
                    "DBLP": "journals/corr/abs-2303-17824",
                    "DOI": "10.48550/arXiv.2303.17824",
                    "CorpusId": 257900843
                },
                "corpusId": 257900843,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d520d069f58460a8c806f7591d4498e34831d68f",
                "title": "Implementation and (Inverse Modified) Error Analysis for implicitly-templated ODE-nets",
                "abstract": "We focus on learning unknown dynamics from data using ODE-nets templated on implicit numerical initial value problem solvers. First, we perform Inverse Modified error analysis of the ODE-nets using unrolled implicit schemes for ease of interpretation. It is shown that training an ODE-net using an unrolled implicit scheme returns a close approximation of an Inverse Modified Differential Equation (IMDE). In addition, we establish a theoretical basis for hyper-parameter selection when training such ODE-nets, whereas current strategies usually treat numerical integration of ODE-nets as a black box. We thus formulate an adaptive algorithm which monitors the level of error and adapts the number of (unrolled) implicit solution iterations during the training process, so that the error of the unrolled approximation is less than the current learning loss. This helps accelerate training, while maintaining accuracy. Several numerical experiments are performed to demonstrate the advantages of the proposed algorithm compared to nonadaptive unrollings, and validate the theoretical analysis. We also note that this approach naturally allows for incorporating partially known physical terms in the equations, giving rise to what is termed ``gray box\"identification.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "31588522",
                        "name": "Tom S. Bertalan"
                    },
                    {
                        "authorId": "145801437",
                        "name": "Beibei Zhu"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    },
                    {
                        "authorId": "3439407",
                        "name": "I. Kevrekidis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fc2fd34a7b38f365e905c3878681b63c8bcecbf8",
                "externalIds": {
                    "PubMedCentral": "9929450",
                    "DOI": "10.1038/s41598-023-29186-8",
                    "CorpusId": 256851744,
                    "PubMed": "36788325"
                },
                "corpusId": 256851744,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fc2fd34a7b38f365e905c3878681b63c8bcecbf8",
                "title": "Symplectic encoders for physics-constrained variational dynamics inference",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2133300239",
                        "name": "Kiran Bacsa"
                    },
                    {
                        "authorId": "67285417",
                        "name": "Zhilu Lai"
                    },
                    {
                        "authorId": "2157222439",
                        "name": "Wei Liu"
                    },
                    {
                        "authorId": "2139759982",
                        "name": "Michael Todd"
                    },
                    {
                        "authorId": "66773557",
                        "name": "E. Chatzi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Initial works related to the discovery of Lagrangian can be linked to Hamiltonian Neural Networks (HNN) [12, 13].",
                "For learning the Hamiltonian and Lagrangian directly in Cartesian coordinates, Constrained Hamiltonian Neural Networks (CHNNs) and Constrained Lagrangian Neural Networks (CLNNs) were proposed in [16]."
            ],
            "citingPaper": {
                "paperId": "4a2620bb33e4ae7ce398a443df2cdc01a0351f67",
                "externalIds": {
                    "ArXiv": "2302.04400",
                    "DBLP": "journals/corr/abs-2302-04400",
                    "DOI": "10.48550/arXiv.2302.04400",
                    "CorpusId": 256697122
                },
                "corpusId": 256697122,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4a2620bb33e4ae7ce398a443df2cdc01a0351f67",
                "title": "Discovering interpretable Lagrangian of dynamical systems from data",
                "abstract": "A complete understanding of physical systems requires models that are accurate and obeys natural conservation laws. Recent trends in representation learning involve learning Lagrangian from data rather than the direct discovery of governing equations of motion. The generalization of equation discovery techniques has huge potential; however, existing Lagrangian discovery frameworks are black-box in nature. This raises a concern about the reusability of the discovered Lagrangian. In this article, we propose a novel data-driven machine-learning algorithm to automate the discovery of interpretable Lagrangian from data. The Lagrangian are derived in interpretable forms, which also allows the automated discovery of conservation laws and governing equations of motion. The architecture of the proposed framework is designed in such a way that it allows learning the Lagrangian from a subset of the underlying domain and then generalizing for an infinite-dimensional system. The fidelity of the proposed framework is exemplified using examples described by systems of ordinary differential equations and partial differential equations where the Lagrangian and conserved quantities are known.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1389549891",
                        "name": "Tapas Tripura"
                    },
                    {
                        "authorId": "3411759",
                        "name": "S. Chakraborty"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2019) or Hamiltonian Generative Networks (Toth et al., 2020).",
                "Exploiting the Newtonian evolution in classical Mechanics, Neural Hamiltonian Flows (NHF) (Toth et al., 2020) are NF models that use Hamiltonian transformations.",
                "\u2026`(d|qT ) + log g(p)] dqT dpT + cst\n= \u222b \u03a00(q0,p0) [log \u03c00(q0) + log f(p0|q0)\u2212 log \u03c00(Tq(q0,p0))\u2212 log `(d|Tq(q0,p0))\u2212 log g(Tp(q0,p0)] dq0dp0 + cst.\n(7)\nWe can also adapt the ELBO from (Toth et al., 2020) to our inference framework:\nln\u03c00(q0) = ln \u222b \u03a00(q0,p0)dp0\n= ln\n\u222b \u03a00(q0,p0)\nf(p0|q0)\u2026",
                "The dynamics is integrated in phase-space with the Leapfrog integrator (Toth et al., 2020).",
                "Including physical prior knowledge into neural networks may be another solution to understand the model (Raissi et al., 2019; Toth et al., 2020).",
                "Such toy example, similarly studied in (Toth et al., 2020), is interesting in the sense that it will allow us to discuss various aspects, from memory usage to interpretability.",
                "By adding artificial momenta pT (Toth et al., 2020), the distribution modeled by our NHF is m(qT ) = \u222b M(qT ,pT )dpT = \u222b \u03a00(T (qT ,pT ))dpT .",
                "As generative models, they have been used to sample from 2D distributions (Toth et al., 2020).",
                "In previous work (Toth et al., 2020), authors have proposed to parameterize each potential by a neural network.",
                "They come with performance similar to the ones obtained with Real-NVPs in sampling 2D distributions (Toth et al., 2020).",
                "To alleviate these issues, Neural Hamiltonian Flows (NHF, Toth et al., 2020) is a NF technique that uses a series of Hamiltonian transformations as normalizing flows.",
                "Furthermore, even if they have been numerically shown to transfer multimodality from the target distribution to the potential energy in some cases (Toth et al., 2020), this property is not guaranteed.",
                "By adding artificial momenta pT (Toth et al., 2020), the distribution modeled by our NHF is m(qT ) = \u222b M(qT ,pT )dpT = \u222b \u03a00(T \u22121(qT ,pT ))dpT .",
                "Multiple architectures have been proposed, such as Hamiltonian Neural Networks (Greydanus et al., 2019) or Hamiltonian Generative Networks (Toth et al., 2020).",
                "If the kinetic energy is chosen to be a MLP (Toth et al., 2020), then the model contains two black-boxes that are not easy to interpret a priori, namely the kinetic and potential energies K and V ."
            ],
            "citingPaper": {
                "paperId": "4fd703e3571fe469226effb3ffc303a8bcd193e0",
                "externalIds": {
                    "ArXiv": "2302.01955",
                    "DBLP": "journals/corr/abs-2302-01955",
                    "DOI": "10.48550/arXiv.2302.01955",
                    "CorpusId": 256615358
                },
                "corpusId": 256615358,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4fd703e3571fe469226effb3ffc303a8bcd193e0",
                "title": "Fixed-kinetic Neural Hamiltonian Flows for enhanced interpretability and reduced complexity",
                "abstract": "Normalizing Flows (NF) are Generative models which are particularly robust and allow for exact sampling of the learned distribution. They however require the design of an invertible mapping, whose Jacobian determinant has to be computable. Recently introduced, Neural Hamiltonian Flows (NHF) are based on Hamiltonian dynamics-based Flows, which are continuous, volume-preserving and invertible and thus make for natural candidates for robust NF architectures. In particular, their similarity to classical Mechanics could lead to easier interpretability of the learned mapping. However, despite being Physics-inspired architectures, the originally introduced NHF architecture still poses a challenge to interpretability. For this reason, in this work, we introduce a fixed kinetic energy version of the NHF model. Inspired by physics, our approach improves interpretability and requires less parameters than previously proposed architectures. We then study the robustness of the NHF architectures to the choice of hyperparameters. We analyze the impact of the number of leapfrog steps, the integration time and the number of neurons per hidden layer, as well as the choice of prior distribution, on sampling a multimodal 2D mixture. The NHF architecture is robust to these choices, especially the fixed-kinetic energy model. Finally, we adapt NHF to the context of Bayesian inference and illustrate our method on sampling the posterior distribution of two cosmological parameters knowing type Ia supernovae observations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2204576085",
                        "name": "Vincent Souveton"
                    },
                    {
                        "authorId": "1795342",
                        "name": "A. Guillin"
                    },
                    {
                        "authorId": "74359295",
                        "name": "J. Jasche"
                    },
                    {
                        "authorId": "152410445",
                        "name": "G. Lavaux"
                    },
                    {
                        "authorId": "14788259",
                        "name": "Manon Michel"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026(Cranmer et al., 2020; Lutter et al., 2019; Zhong and Leonard, 2020; Allen-Blanchette et al., 2020; Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020), and it also enables learning control-oriented dynamics models (Zhong et al., 2020a,b; Roehrl et al.,\n2020;\u2026",
                "This approach allows for the inclusion of general forms physics knowledge into data-driven models , such as for so-called Lagrangian and Hamiltonian neural networks (Cranmer et al., 2020; Lutter et al., 2019; Zhong and Leonard, 2020; Allen-Blanchette et al., 2020; Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020), and it also enables learning control-oriented dynamics models (Zhong et al."
            ],
            "citingPaper": {
                "paperId": "55bd634767dba7fc7736186c6d03732206e0d53d",
                "externalIds": {
                    "ArXiv": "2301.03565",
                    "DBLP": "journals/corr/abs-2301-03565",
                    "DOI": "10.48550/arXiv.2301.03565",
                    "CorpusId": 255545787
                },
                "corpusId": 255545787,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/55bd634767dba7fc7736186c6d03732206e0d53d",
                "title": "Physics-Informed Kernel Embeddings: Integrating Prior System Knowledge with Data-Driven Control",
                "abstract": "Data-driven control algorithms use observations of system dynamics to construct an implicit model for the purpose of control. However, in practice, data-driven techniques often require excessive sample sizes, which may be infeasible in real-world scenarios where only limited observations of the system are available. Furthermore, purely data-driven methods often neglect useful a priori knowledge, such as approximate models of the system dynamics. We present a method to incorporate such prior knowledge into data-driven control algorithms using kernel embeddings, a nonparametric machine learning technique based in the theory of reproducing kernel Hilbert spaces. Our proposed approach incorporates prior knowledge of the system dynamics as a bias term in the kernel learning problem. We formulate the biased learning problem as a least-squares problem with a regularization term that is informed by the dynamics, that has an efficiently computable, closed-form solution. Through numerical experiments, we empirically demonstrate the improved sample efficiency and out-of-sample generalization of our approach over a purely data-driven baseline. We demonstrate an application of our method to control through a target tracking problem with nonholonomic dynamics, and on spring-mass-damper and F-16 aircraft state prediction tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057725021",
                        "name": "Adam J. Thorpe"
                    },
                    {
                        "authorId": "1796254983",
                        "name": "Cyrus Neary"
                    },
                    {
                        "authorId": "2152050811",
                        "name": "Franck Djeumou"
                    },
                    {
                        "authorId": "1722063",
                        "name": "Meeko Oishi"
                    },
                    {
                        "authorId": "3199888",
                        "name": "U. Topcu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Neural network-based methods for modeling continuous-time ODEs require derivative regression [14] or computationally expensive numerical integration to solve the ODEs [7, 40, 51, 30, 8].",
                "Neural networks, such as recurrent neural networks [16, 6], and neural ordinary differential equations (ODEs) [7], have been used for modeling black-box nonlinear dynamical systems given time-series data.",
                "Existing neural network-based models in continuous time, such as neural ODEs, require high computational cost for training since they need to backpropagate through an ODE solver or solve an adjoint ODE for each training epoch.",
                "Weak form learning [43, 9] has been proposed for efficient training of neural ODEs."
            ],
            "citingPaper": {
                "paperId": "90813609def379739274d4db8337b155ac43e48e",
                "externalIds": {
                    "ArXiv": "2212.13033",
                    "DBLP": "journals/corr/abs-2212-13033",
                    "DOI": "10.48550/arXiv.2212.13033",
                    "CorpusId": 255125509
                },
                "corpusId": 255125509,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/90813609def379739274d4db8337b155ac43e48e",
                "title": "Modeling Nonlinear Dynamics in Continuous Time with Inductive Biases on Decay Rates and/or Frequencies",
                "abstract": "We propose a neural network-based model for nonlinear dynamics in continuous time that can impose inductive biases on decay rates and/or frequencies. Inductive biases are helpful for training neural networks especially when training data are small. The proposed model is based on the Koopman operator theory, where the decay rate and frequency information is used by restricting the eigenvalues of the Koopman operator that describe linear evolution in a Koopman space. We use neural networks to find an appropriate Koopman space, which are trained by minimizing multi-step forecasting and backcasting errors using irregularly sampled time-series data. Experiments on various time-series datasets demonstrate that the proposed method achieves higher forecasting performance given a single short training sequence than the existing methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2664600",
                        "name": "Tomoharu Iwata"
                    },
                    {
                        "authorId": "1704932",
                        "name": "Y. Kawahara"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "67131cb9cba4a8c5e6db2014cbc32ebe6c4952d8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-06929",
                    "ArXiv": "2212.06929",
                    "DOI": "10.1103/PhysRevB.106.214307",
                    "CorpusId": 254636214
                },
                "corpusId": 254636214,
                "publicationVenue": {
                    "id": "52113867-f77b-4f26-a1cf-8e577dd325ea",
                    "name": "Physical review B",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev B",
                        "Phys Rev B",
                        "Physical Review B"
                    ],
                    "issn": "2469-9950",
                    "alternate_issns": [
                        "1098-0121",
                        "0556-2805"
                    ],
                    "url": "https://journals.aps.org/prb",
                    "alternate_urls": [
                        "https://journals.aps.org/prb/",
                        "http://journals.aps.org/prb/",
                        "http://prola.aps.org/",
                        "https://www.tib.eu/de/openurl/search?amp;DlicenseModel=nl&issn=1098-0121,0163-1829",
                        "http://prb.aps.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/67131cb9cba4a8c5e6db2014cbc32ebe6c4952d8",
                "title": "Generating extreme quantum scattering in graphene with machine learning",
                "abstract": "Graphene quantum dots provide a platform for manipulating electron behaviors in two-dimensional (2D) Dirac materials. Most previous works were of the\"forward\"type in that the objective was to solve various confinement, transport and scattering problems with given structures that can be generated by, e.g., applying an external electrical field. There are applications such as cloaking or superscattering where the challenging problem of inverse design needs to be solved: finding a quantum-dot structure according to certain desired functional characteristics. A brute-force search of the system configuration based directly on the solutions of the Dirac equation is computational infeasible. We articulate a machine-learning approach to addressing the inverse-design problem where artificial neural networks subject to physical constraints are exploited to replace the rigorous Dirac equation solver. In particular, we focus on the problem of designing a quantum dot structure to generate both cloaking and superscattering in terms of the scattering efficiency as a function of the energy. We construct a physical loss function that enables accurate prediction of the scattering characteristics. We demonstrate that, in the regime of Klein tunneling, the scattering efficiency can be designed to vary over two orders of magnitudes, allowing any scattering curve to be generated from a proper combination of the gate potentials. Our physics-based machine-learning approach can be a powerful design tool for 2D Dirac material-based electronics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "93112518",
                        "name": "Chen-Di Han"
                    },
                    {
                        "authorId": "144769611",
                        "name": "Y. Lai"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Here, ODESolve(P\u0398(\u00b7,u),x, t, T ) is a numerical solution to the ordinary differential equation specified by P\u0398(x,u) over the window of time [t, T ].",
                "In this work, we use fixed-timestep RK4 to evaluate ODESolve(\u00b7) in all experiments.",
                "More closely related to our work, several recent papers also study neural ODEs that have a port-Hamiltonian structure (Zhong et al., 2020; Desai et al., 2021; Eidnes et al., 2023; Duong and Atanasov, 2021).",
                "(3)\nFinally, we search for local minima of L(\u0398,D) using gradient-based techniques, where \u2207\u0398L(\u0398,D) may be computed using either direct automatic differentiation through ODESolve(\u00b7), or using the adjoint sensitivity method (Pontryagin, 1987; Chen et al., 2018).",
                "Of particular relevance to our work, Hamiltonian neural networks use the Hamiltonian formulation of dynamics to inform the structure of a neural ODE (Greydanus et al., 2019; Matsubara et al., 2020; Toth et al., 2020; Finzi et al., 2020).",
                "By using neural networks to parametrize differential equations, as opposed to directly fitting the available trajectory data, NODEs allow the user to harness an existing wealth of knowledge from applied mathematics, physics, and engineering (Djeumou et al., 2022a; Cranmer et al., 2020; Lutter et al., 2019; Gupta et al., 2020; Roehrl et al., 2020; Zhong et al., 2021b; Shi et al., 2019).",
                "The output x\u0302T := PHNN\u0398(x,u, t, T ) of the PHNN parametrized by \u0398 is then given by ODESolve(P\u0398(\u00b7,u),x, t, T ) \u2248 x + \u222b T t P\u0398(xs,u)ds, where we use the subscript notation xt to denote x(t).",
                "We note that the particular algorithm used to evaluate ODESolve(\u00b7) influences the model\u2019s accuracy and the computational cost of forward evaluations of the model (Djeumou et al., 2022b).",
                "In particular, neural ordinary differential equations (NODEs) (Chen et al., 2018) provide a natural approach to incorporate physicsbased knowledge as inductive bias in deep learning (Zhong et al., 2021a; Rackauckas et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "21bac88339555940ba094324dca70a9c6cceac77",
                "externalIds": {
                    "ArXiv": "2212.00893",
                    "DBLP": "conf/l4dc/NearyT23",
                    "DOI": "10.48550/arXiv.2212.00893",
                    "CorpusId": 254220970
                },
                "corpusId": 254220970,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/21bac88339555940ba094324dca70a9c6cceac77",
                "title": "Compositional Learning of Dynamical System Models Using Port-Hamiltonian Neural Networks",
                "abstract": "Many dynamical systems -- from robots interacting with their surroundings to large-scale multiphysics systems -- involve a number of interacting subsystems. Toward the objective of learning composite models of such systems from data, we present i) a framework for compositional neural networks, ii) algorithms to train these models, iii) a method to compose the learned models, iv) theoretical results that bound the error of the resulting composite models, and v) a method to learn the composition itself, when it is not known a priori. The end result is a modular approach to learning: neural network submodels are trained on trajectory data generated by relatively simple subsystems, and the dynamics of more complex composite systems are then predicted without requiring additional data generated by the composite systems themselves. We achieve this compositionality by representing the system of interest, as well as each of its subsystems, as a port-Hamiltonian neural network (PHNN) -- a class of neural ordinary differential equations that uses the port-Hamiltonian systems formulation as inductive bias. We compose collections of PHNNs by using the system's physics-informed interconnection structure, which may be known a priori, or may itself be learned from data. We demonstrate the novel capabilities of the proposed framework through numerical examples involving interacting spring-mass-damper systems. Models of these systems, which include nonlinear energy dissipation and control inputs, are learned independently. Accurate compositions are learned using an amount of training data that is negligible in comparison with that required to train a new model from scratch. Finally, we observe that the composite PHNNs enjoy properties of port-Hamiltonian systems, such as cyclo-passivity -- a property that is useful for control purposes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1796254983",
                        "name": "Cyrus Neary"
                    },
                    {
                        "authorId": "3199888",
                        "name": "U. Topcu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Inspired by the Hamiltonian mechanics, a Hamiltonian neural network (HNN) has been proposed, where the output represents the Hamiltonian dynamics, through which energy conservation is explicitly enforced [24].",
                "The proposed Hamiltonian generative network has been applied to density estimation, leading to a neural Hamiltonian flow [24].",
                "Scientific Knowledge Mathematical Equations [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50]\u2013[52] [53] [54] [55]",
                "Hamiltonian functionality that enforces energy conservation has attracted much attention [24], [25], [30], [31]."
            ],
            "citingPaper": {
                "paperId": "e6efc429bef27fb22c62909e55091e907d7b42ae",
                "externalIds": {
                    "ArXiv": "2212.00017",
                    "DBLP": "journals/corr/abs-2212-00017",
                    "DOI": "10.48550/arXiv.2212.00017",
                    "CorpusId": 254125341
                },
                "corpusId": 254125341,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e6efc429bef27fb22c62909e55091e907d7b42ae",
                "title": "Knowledge-augmented Deep Learning and Its Applications: A Survey",
                "abstract": ",",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "90146345",
                        "name": "Zijun Cui"
                    },
                    {
                        "authorId": "2007655125",
                        "name": "Tian Gao"
                    },
                    {
                        "authorId": "2940762",
                        "name": "Kartik Talamadupula"
                    },
                    {
                        "authorId": "2193256378",
                        "name": "Qiang Ji"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Moreover, NLD can be viewed in a line of physics inspired neural network models such as (Cranmer et al., 2020; Greydanus et al., 2019; Toth et al., 2020; Botev et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "8ed31496eea24fb64e465cea58f1a0a4a92b4470",
                "externalIds": {
                    "ArXiv": "2211.09537",
                    "DBLP": "journals/corr/abs-2211-09537",
                    "DOI": "10.48550/arXiv.2211.09537",
                    "CorpusId": 253581210
                },
                "corpusId": 253581210,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8ed31496eea24fb64e465cea58f1a0a4a92b4470",
                "title": "Neural Langevin Dynamics: towards interpretable Neural Stochastic Differential Equations",
                "abstract": "Neural Stochastic Di\ufb00erential Equations (NSDE) have been trained as both Variational Autoencoders, and as GANs. However, the resulting Stochastic Di\ufb00erential Equations can be hard to interpret or analyse due to the generic nature of the drift and di\ufb00usion \ufb01elds. By restricting our NSDE to be of the form of Langevin dynamics, and training it as a VAE, we obtain NSDEs that lend themselves to more elaborate analysis and to a wider range of visualisation techniques than a generic NSDE. More speci\ufb01cally, we obtain an energy landscape, the minima of which are in one-to-one correspondence with latent states underlying the used data. This not only allows us to detect states underlying the data dynamics in an unsupervised manner, but also to infer the distribution of time spent in each state according to the learned SDE. More in general, restricting an NSDE to Langevin dynamics enables the use of a large set of tools from computational molecular dynamics for the analysis of the obtained results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2191075252",
                        "name": "Simon M. Koop"
                    },
                    {
                        "authorId": "3266992",
                        "name": "M. Peletier"
                    },
                    {
                        "authorId": "9032664",
                        "name": "J. Portegies"
                    },
                    {
                        "authorId": "49917515",
                        "name": "V. Menkovski"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "There is a great deal of work designing specific neural architectures that naturally obey Hamiltonian equations and Lagrangian equations [67], [68], [134], [135].",
                "For PDEs with Dirichlet conditions, they sample a dataset of collocation points from \u2126 and \u2202\u2126, i.e.{xi} \u2282 \u2126\n7 Neural Solver Method Description Representatives Loss Reweighting Grad Norm GradientPathologiesPINNs [43] NTK Reweighting PINNsNTK [44] Variance Reweighting Inverse-Dirichlet PINNs [45] Novel Optimization Targets Numerical Differentiation DGM [46], CAN-PINN [47], cvPINNs [48] Variantional Formulation vPINN [49], hp-PINN [50], VarNet [51], WAN [52] Regularization gPINNs [53], Sobolev Training [54]\nNovel Architectures\nAdaptive Activation LAAF-PINNs [55], [56], SReLU [57] Feature Preprocessing Fourier Embedding [58], Prior Dictionary Embedding [59] Boundary Encoding TFC-based [60], CENN [61], PFNN [62], HCNet [63]\nSequential Architecture PhyCRNet [64], PhyLSTM [65] AR-DenseED [66], HNN [67], HGN [68] Convolutional Architecture PhyGeoNet [69], PhyCRNet [64], PPNN [70]\nDomain Decomposition XPINNs [71], cPINNs [72], FBPINNs [73], Shukla et al. [74]\nOther Learning Paradigms Transfer Learning Desai et al. [75], MF-PIDNN [76]Meta-Learning Psaros et al. [77], NRPINNs [78]\nTABLE 2: An overview of variants of PINNs.",
                "HGN\n(Toth et. al.)",
                "HGN [135] combines generative models such as variational auto-encoders (VAE) [141] and Hamiltonian neural networks to model time-dependent systems with uncertainty.",
                "Hamiltonian generative networks [135] build an autoencoder to map images to latent physical variable, and use Hamiltonian canonical equations to predict the changes in the physical variables."
            ],
            "citingPaper": {
                "paperId": "0c5c5f100dec9c758abf4dcc526a6883671fd3bd",
                "externalIds": {
                    "ArXiv": "2211.08064",
                    "DBLP": "journals/corr/abs-2211-08064",
                    "DOI": "10.48550/arXiv.2211.08064",
                    "CorpusId": 253522948
                },
                "corpusId": 253522948,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0c5c5f100dec9c758abf4dcc526a6883671fd3bd",
                "title": "Physics-Informed Machine Learning: A Survey on Problems, Methods and Applications",
                "abstract": "Recent advances of data-driven machine learning have revolutionized fields like computer vision, reinforcement learning, and many scientific and engineering domains. In many real-world and scientific problems, systems that generate data are governed by physical laws. Recent work shows that it provides potential benefits for machine learning models by incorporating the physical prior and collected data, which makes the intersection of machine learning and physics become a prevailing paradigm. By integrating the data and mathematical physics models seamlessly, it can guide the machine learning model towards solutions that are physically plausible, improving accuracy and efficiency even in uncertain and high-dimensional contexts. In this survey, we present this learning paradigm called Physics-Informed Machine Learning (PIML) which is to build a model that leverages empirical data and available physical prior knowledge to improve performance on a set of tasks that involve a physical mechanism. We systematically review the recent development of physics-informed machine learning from three perspectives of machine learning tasks, representation of physical prior, and methods for incorporating physical prior. We also propose several important open research problems based on the current trends in the field. We argue that encoding different forms of physical prior into model architectures, optimizers, inference algorithms, and significant domain-specific applications like inverse engineering design and robotic control is far from being fully explored in the field of physics-informed machine learning. We believe that the interdisciplinary research of physics-informed machine learning will significantly propel research progress, foster the creation of more effective machine learning models, and also offer invaluable assistance in addressing long-standing problems in related disciplines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "95460807",
                        "name": "Zhongkai Hao"
                    },
                    {
                        "authorId": "104037450",
                        "name": "Songming Liu"
                    },
                    {
                        "authorId": null,
                        "name": "Yichi Zhang"
                    },
                    {
                        "authorId": "2072399434",
                        "name": "Chengyang Ying"
                    },
                    {
                        "authorId": "2190929908",
                        "name": "Yao Feng"
                    },
                    {
                        "authorId": "2093561216",
                        "name": "Hang Su"
                    },
                    {
                        "authorId": "2146280496",
                        "name": "Jun Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Among them, we can cite [15, 16, 17, 18, 19, 20], to mention but a few."
            ],
            "citingPaper": {
                "paperId": "4b08d509ddc3768b332746f308720b07a818eb90",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-01873",
                    "ArXiv": "2211.01873",
                    "DOI": "10.1007/s00466-023-02296-w",
                    "CorpusId": 253264928
                },
                "corpusId": 253264928,
                "publicationVenue": {
                    "id": "f1a44872-70ad-4c33-b8bb-8618e573cf3f",
                    "name": "Computational Mechanics",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Mech"
                    ],
                    "issn": "0178-7675",
                    "url": "http://www.springer.com/466",
                    "alternate_urls": [
                        "https://link.springer.com/journal/466"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4b08d509ddc3768b332746f308720b07a818eb90",
                "title": "Port-metriplectic neural networks: thermodynamics-informed machine learning of complex physical systems",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157042146",
                        "name": "Quercus Hern'andez"
                    },
                    {
                        "authorId": "2157041576",
                        "name": "Alberto Bad'ias"
                    },
                    {
                        "authorId": "2734584",
                        "name": "F. Chinesta"
                    },
                    {
                        "authorId": "3246447",
                        "name": "E. Cueto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c824fa59f3a58fe59964703ad8390fdef0291809",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09974",
                    "ArXiv": "2210.09974",
                    "DOI": "10.48550/arXiv.2210.09974",
                    "CorpusId": 252967683
                },
                "corpusId": 252967683,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c824fa59f3a58fe59964703ad8390fdef0291809",
                "title": "Theoretical Guarantees for Permutation-Equivariant Quantum Neural Networks",
                "abstract": "Despite the great promise of quantum machine learning models, there are several challenges one must overcome before unlocking their full potential. For instance, models based on quantum neural networks (QNNs) can suffer from excessive local minima and barren plateaus in their training landscapes. Recently, the nascent field of geometric quantum machine learning (GQML) has emerged as a potential solution to some of those issues. The key insight of GQML is that one should design architectures, such as equivariant QNNs, encoding the symmetries of the problem at hand. Here, we focus on problems with permutation symmetry (i.e., the group of symmetry $S_n$), and show how to build $S_n$-equivariant QNNs. We provide an analytical study of their performance, proving that they do not suffer from barren plateaus, quickly reach overparametrization, and generalize well from small amounts of data. To verify our results, we perform numerical simulations for a graph state classification task. Our work provides the first theoretical guarantees for equivariant QNNs, thus indicating the extreme power and potential of GQML.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1441128327",
                        "name": "Louis Schatzki"
                    },
                    {
                        "authorId": "103560141",
                        "name": "Mart\u00edn Larocca"
                    },
                    {
                        "authorId": "47286584",
                        "name": "F. Sauvage"
                    },
                    {
                        "authorId": "32017809",
                        "name": "M. Cerezo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "The combination of deep learning with physics-based models allows models to learn dynamics from high-dimensional data such as images (Allen-Blanchette et al. 2020; Zhong and Leonard 2020; Toth et al. 2020).",
                "Approaches that model the dynamics of the system (Li et al. 2020; Zhong and Leonard 2020; Allen-Blanchette et al. 2020; Toth et al. 2020) learn dynamics from image-state data but only for either 2D planar systems or systems with dynamics in R, using pixel images.",
                "contributions to the study of learning dynamics from images (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2020; Allen-Blanchette et al. 2020; Zhong and Leonard 2020) have evaluated their models on pixel image sequences of 2D planar dynamics.",
                "This formulation has been used by several authors to learn unknown dynamics: the Hamiltonian structure (canonical symplectic form) is used as a physics prior and the unknown dynamics are uncovered by learning the Hamiltonian (Greydanus, Dzamba, and Yosinski 2019; Zhong, Dey, and Chakraborty 2020b; Toth et al. 2020).",
                "Their datasets include the pixel pendulum, Acrobot, cart-pole (Zhong and Leonard 2020), as well as 2-body and 3-body problems (Toth et al. 2020).",
                "Previous work (Allen-Blanchette et al. 2020; Zhong and Leonard 2020; Toth et al. 2020) has made significant progress in learning dynamics from images of planar rigid bodies."
            ],
            "citingPaper": {
                "paperId": "eba59ef649b1f8c203ff3ee5a2f630e840385e25",
                "externalIds": {
                    "ArXiv": "2209.11355",
                    "DBLP": "journals/corr/abs-2209-11355",
                    "DOI": "10.48550/arXiv.2209.11355",
                    "CorpusId": 252519570
                },
                "corpusId": 252519570,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eba59ef649b1f8c203ff3ee5a2f630e840385e25",
                "title": "Learning Interpretable Dynamics from Images of a Freely Rotating 3D Rigid Body",
                "abstract": "In many real-world settings, image observations of freely rotating 3D rigid bodies, such as satellites, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics and a lack of interpretability reduces the usefulness of standard deep learning methods. In this work, we present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to $\\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion with a learned representation of the Hamiltonian. We demonstrate the efficacy of our approach on a new rotating rigid-body dataset with sequences of rotating cubes and rectangular prisms with uniform and non-uniform density.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "34819822",
                        "name": "J. Mason"
                    },
                    {
                        "authorId": "1405669277",
                        "name": "Christine Allen-Blanchette"
                    },
                    {
                        "authorId": "102569860",
                        "name": "Nick Zolman"
                    },
                    {
                        "authorId": "2178760411",
                        "name": "Elizabeth Davison"
                    },
                    {
                        "authorId": "3301461",
                        "name": "Naomi Ehrich Leonard"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "cae2adb6642a38e2dd80ee975a65b26788a891c6",
                "externalIds": {
                    "ArXiv": "2209.00905",
                    "CorpusId": 257913495
                },
                "corpusId": 257913495,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cae2adb6642a38e2dd80ee975a65b26788a891c6",
                "title": "From latent dynamics to meaningful representations",
                "abstract": "While representation learning has been central to the rise of machine learning and artificial intelligence, a key problem remains in making the learnt representations meaningful. For this the typical approach is to regularize the learned representation through prior probability distributions. However such priors are usually unavailable or ad hoc. To deal with this, we propose a dynamics-constrained representation learning framework. Instead of using predefined probabilities, we restrict the latent representation to follow specific dynamics, which is a more natural constraint for representation learning in dynamical systems. Our belief stems from a fundamental observation in physics that though different systems can have different marginalized probability distributions, they typically obey the same dynamics, such as Newton's and Schrodinger's equations. We validate our framework for different systems including a real-world fluorescent DNA movie dataset. We show that our algorithm can uniquely identify an uncorrelated, isometric and meaningful latent representation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51224208",
                        "name": "Dedi Wang"
                    },
                    {
                        "authorId": "2108927025",
                        "name": "Yihang Wang"
                    },
                    {
                        "authorId": "2055729877",
                        "name": "Luke J. Evans"
                    },
                    {
                        "authorId": "46736401",
                        "name": "P. Tiwary"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Due to its simplicity and the elegance of the idea, HNN has been applied on a wide range of tasks and neural network architectures [11, 12, 13], and even on dissipative systems by adding a dissipation term [14, 15]."
            ],
            "citingPaper": {
                "paperId": "cb07726f2e6b9fb8ed68080f6972a72718d28b7d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-10387",
                    "ArXiv": "2208.10387",
                    "DOI": "10.48550/arXiv.2208.10387",
                    "CorpusId": 251719156
                },
                "corpusId": 251719156,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/cb07726f2e6b9fb8ed68080f6972a72718d28b7d",
                "title": "Constants of motion network",
                "abstract": "The beauty of physics is that there is usually a conserved quantity in an always-changing system, known as the constant of motion. Finding the constant of motion is important in understanding the dynamics of the system, but typically requires mathematical proficiency and manual analytical work. In this paper, we present a neural network that can simultaneously learn the dynamics of the system and the constants of motion from data. By exploiting the discovered constants of motion, it can produce better predictions on dynamics and can work on a wider range of systems than Hamiltonian-based neural networks. In addition, the training progresses of our method can be used as an indication of the number of constants of motion in a system which could be useful in studying a novel physical system.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49443038",
                        "name": "M. F. Kasim"
                    },
                    {
                        "authorId": "144833174",
                        "name": "Yi Heng Lim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "There have been a lot of follow-up works that build on the foundation of HNN and expand it to different settings including dissipative systems [5, 10], generative networks [6] and graph networks [7], while others try to improve HNN by simplifying it [11, 12].",
                "A popular workaround is to learn the Hamiltonian [2, 6, 7] or Lagrangian [3] of the energy-conserving system with a neural network, then get the dynamics as the derivatives of the learned quantity."
            ],
            "citingPaper": {
                "paperId": "e142b0e670725afc66c6d4ce926b0738f1a8ad2b",
                "externalIds": {
                    "ArXiv": "2208.02632",
                    "DBLP": "journals/corr/abs-2208-02632",
                    "DOI": "10.48550/arXiv.2208.02632",
                    "CorpusId": 251320548
                },
                "corpusId": 251320548,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e142b0e670725afc66c6d4ce926b0738f1a8ad2b",
                "title": "Unifying physical systems' inductive biases in neural ODE using dynamics constraints",
                "abstract": "Conservation of energy is at the core of many physical phenomena and dynamical systems. There have been a significant number of works in the past few years aimed at predicting the trajectory of motion of dynamical systems using neural networks while adhering to the law of conservation of energy. Most of these works are inspired by classical mechanics such as Hamiltonian and Lagrangian mechanics as well as Neural Ordinary Differential Equations. While these works have been shown to work well in specific domains respectively, there is a lack of a unifying method that is more generally applicable without requiring significant changes to the neural network architectures. In this work, we aim to address this issue by providing a simple method that could be applied to not just energy-conserving systems, but also dissipative systems, by including a different inductive bias in different cases in the form of a regularisation term in the loss function. The proposed method does not require changing the neural network architecture and could form the basis to validate a novel idea, therefore showing promises to accelerate research in this direction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144833174",
                        "name": "Yi Heng Lim"
                    },
                    {
                        "authorId": "49443038",
                        "name": "M. F. Kasim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Even Generative Neural Networks exist under this Hamiltonian prism [81]."
            ],
            "citingPaper": {
                "paperId": "25080ff7886630ea8c5ac9cb35033be89acbeb38",
                "externalIds": {
                    "ArXiv": "2207.12749",
                    "DBLP": "journals/corr/abs-2207-12749",
                    "DOI": "10.48550/arXiv.2207.12749",
                    "CorpusId": 251067028
                },
                "corpusId": 251067028,
                "publicationVenue": {
                    "id": "f9c1272f-e8c2-4e8c-bdae-fc9c2bb2cb85",
                    "name": "Archives of Computational Methods in Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Arch Comput Method Eng"
                    ],
                    "issn": "1134-3060",
                    "url": "http://www.cimne.com/arcme/",
                    "alternate_urls": [
                        "http://www.springer.com/journal/11831",
                        "https://www.springer.com/journal/11831",
                        "https://link.springer.com/journal/11831"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/25080ff7886630ea8c5ac9cb35033be89acbeb38",
                "title": "Thermodynamics of learning physical phenomena",
                "abstract": "Thermodynamics could be seen as an expression of physics at a high epistemic level. As such, its potential as an inductive bias to help machine learning procedures attain accurate and credible predictions has been recently realized in many fields. We review how thermodynamics provides helpful insights in the learning process. At the same time, we study the influence of aspects such as the scale at which a given phenomenon is to be described, the choice of relevant variables for this description or the different techniques available for the learning process.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3246447",
                        "name": "E. Cueto"
                    },
                    {
                        "authorId": "2734584",
                        "name": "F. Chinesta"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Examples of existing work for Hamiltonian systems include [10,17,18,26,32,51,76,78,79,81], and while early seminal work learned Hamiltonian vector fields without truly preserving symplecticity, later results leveraged various tools including symplectic integrator [18], composition of triangular maps [32], and generating function [17] to fix this imperfection."
            ],
            "citingPaper": {
                "paperId": "42225a54689510b22edc93b8d57d9217cae582b8",
                "externalIds": {
                    "DBLP": "journals/jcphy/LiLTY23",
                    "ArXiv": "2207.06012",
                    "DOI": "10.1016/j.jcp.2023.111952",
                    "CorpusId": 250493183
                },
                "corpusId": 250493183,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/42225a54689510b22edc93b8d57d9217cae582b8",
                "title": "NySALT: Nystr\u00f6m-type inference-based schemes adaptive to large time-stepping",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145247019",
                        "name": "Xingjie Li"
                    },
                    {
                        "authorId": "40464284",
                        "name": "F. Lu"
                    },
                    {
                        "authorId": "46699279",
                        "name": "Molei Tao"
                    },
                    {
                        "authorId": "20380455",
                        "name": "F. Ye"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "35a00c7d62120292a7bc58874f266a423171a86e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-12209",
                    "ArXiv": "2207.12209",
                    "DOI": "10.48550/arXiv.2207.12209",
                    "CorpusId": 251040981
                },
                "corpusId": 251040981,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/35a00c7d62120292a7bc58874f266a423171a86e",
                "title": "Lagrangian Density Space-Time Deep Neural Network Topology",
                "abstract": "As a network-based functional approximator, we have proposed a\"Lagrangian Density Space-Time Deep Neural Networks\"(LDDNN) topology. It is qualified for unsupervised training and learning to predict the dynamics of underlying physical science governed phenomena. The prototypical network respects the fundamental conservation laws of nature through the succinctly described Lagrangian and Hamiltonian density of the system by a given data-set of generalized nonlinear partial differential equations. The objective is to parameterize the Lagrangian density over a neural network and directly learn from it through data instead of hand-crafting an exact time-dependent\"Action solution\"of Lagrangian density for the physical system. With this novel approach, can understand and open up the information inference aspect of the\"Black-box deep machine learning representation\"for the physical dynamics of nature by constructing custom-tailored network interconnect topologies, activation, and loss/cost functions based on the underlying physical differential operators. This article will discuss statistical physics interpretation of neural networks in the Lagrangian and Hamiltonian domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3475948",
                        "name": "B. Bishnoi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "92c103f29680a312b3dd78ede234221817038cec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-12325",
                    "ArXiv": "2206.12325",
                    "DOI": "10.48550/arXiv.2206.12325",
                    "CorpusId": 250048528
                },
                "corpusId": 250048528,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/92c103f29680a312b3dd78ede234221817038cec",
                "title": "ModLaNets: Learning Generalisable Dynamics via Modularity and Physical Inductive Bias",
                "abstract": "Deep learning models are able to approximate one specific dynamical system but struggle at learning generalisable dynamics, where dynamical systems obey the same laws of physics but contain different numbers of elements (e.g., double- and triple-pendulum systems). To relieve this issue, we proposed the Modular Lagrangian Network (ModLaNet), a structural neural network framework with modularity and physical inductive bias. This framework models the energy of each element using modularity and then construct the target dynamical system via Lagrangian mechanics. Modularity is beneficial for reusing trained networks and reducing the scale of networks and datasets. As a result, our framework can learn from the dynamics of simpler systems and extend to more complex ones, which is not feasible using other relevant physics-informed neural networks. We examine our framework for modelling double-pendulum or three-body systems with small training datasets, where our models achieve the best data efficiency and accuracy performance compared with counterparts. We also reorganise our models as extensions to model multi-pendulum and multi-body systems, demonstrating the intriguing reusable feature of our framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1897570842",
                        "name": "Yupu Lu"
                    },
                    {
                        "authorId": "2108835388",
                        "name": "Shi-Min Lin"
                    },
                    {
                        "authorId": "1390855802",
                        "name": "Guanqi Chen"
                    },
                    {
                        "authorId": "1943594",
                        "name": "Jia-Yu Pan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": true,
            "contexts": [
                "[8] Peter Toth, Danilo J Rezende, Andrew Jaegle, S\u00e9bastien Racani\u00e8re, Aleksandar Botev, and Irina Higgins.",
                "The majority of related work [1, 8, 9, 10, 11] use a variational autoencoder (VAE) framework to represent the state in a latent space embedding.",
                "The Hamiltonian expresses the total energy of the system H(q,p) = T (q,p) + V (q) [1, 8].",
                "This is similar to other reported results in literature [1, 9, 8, 10].",
                "Other work uses sequences of images as input to a model [8], or a specific velocity estimator model trained to estimate velocities from a sequence of positions [26]."
            ],
            "citingPaper": {
                "paperId": "7972612f6675bc44a52f8ec9211275b2cead9948",
                "externalIds": {
                    "ArXiv": "2206.11030",
                    "DBLP": "journals/corr/abs-2206-11030",
                    "DOI": "10.48550/arXiv.2206.11030",
                    "CorpusId": 249926374
                },
                "corpusId": 249926374,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7972612f6675bc44a52f8ec9211275b2cead9948",
                "title": "KeyCLD: Learning Constrained Lagrangian Dynamics in Keypoint Coordinates from Images",
                "abstract": "We present KeyCLD, a framework to learn Lagrangian dynamics from images. Learned keypoints represent semantic landmarks in images and can directly represent state dynamics. Interpreting this state as Cartesian coordinates coupled with explicit holonomic constraints, allows expressing the dynamics with a constrained Lagrangian. Our method explicitly models kinetic and potential energy, thus allowing energy based control. We are the first to demonstrate learning of Lagrangian dynamics from images on the dm_control pendulum, cartpole and acrobot environments. This is a step forward towards learning Lagrangian dynamics from real-world images, since previous work in literature was only applied to minimalistic images with monochromatic shapes on empty backgrounds. Please refer to our project page for code and additional results: https://rdaems.github.io/keycld/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2172201770",
                        "name": "Rembert Daems"
                    },
                    {
                        "authorId": "2172162987",
                        "name": "Jeroen Taets"
                    },
                    {
                        "authorId": "8508453",
                        "name": "F. Wyffels"
                    },
                    {
                        "authorId": "3318300",
                        "name": "G. Crevecoeur"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "56119684a856c24d30c0f5f5224768021383032a",
                "externalIds": {
                    "DOI": "10.1007/s40747-022-00769-8",
                    "CorpusId": 249084540
                },
                "corpusId": 249084540,
                "publicationVenue": {
                    "id": "d30c9917-b233-46f4-a644-8e5cdf6d6c5e",
                    "name": "Complex & Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Complex  Intell Syst"
                    ],
                    "issn": "2199-4536",
                    "url": "https://link.springer.com/journal/40747",
                    "alternate_urls": [
                        "http://link.springer.com/journal/40747"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/56119684a856c24d30c0f5f5224768021383032a",
                "title": "Trajectory prediction based on conditional Hamiltonian generative network for incomplete observation image sequences",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48015884",
                        "name": "Kui Qian"
                    },
                    {
                        "authorId": "2087843319",
                        "name": "Lei Tian"
                    },
                    {
                        "authorId": "2164514204",
                        "name": "Aiguo Song"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "abstracted physical quantities that are not directly accessible from the video, more recent works directly use image data [49, 12, 22, 24, 52, 28, 59, 26, 50].",
                "For example, [16, 10] and [52] use a neural network to parameterize the Hamiltonian of a system, which relates the total energy to the change of the state.",
                "Although several learning-based approaches that infer physical models from image data have been proposed [12, 22, 24, 59, 52], existing approaches are particularly tailored towards settings with large training corpora.",
                "[59] and [52] use a variational autoencoder (VAE) to predict posterior information about the initial state and combine this with an energy based representation of the dynamics and a final decoding stage.",
                "Several of the previously mentioned works model physical systems using Lagrangian or Hamiltonian energy formulations [30, 16, 11, 10, 52, 58, 28, 59], or other general physics models [26]."
            ],
            "citingPaper": {
                "paperId": "458fa7e578f54573ce313308b215b7d8e0abe33a",
                "externalIds": {
                    "DBLP": "conf/wacv/HofherrKBC23",
                    "ArXiv": "2204.14030",
                    "DOI": "10.1109/WACV56688.2023.00213",
                    "CorpusId": 248476227
                },
                "corpusId": 248476227,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/458fa7e578f54573ce313308b215b7d8e0abe33a",
                "title": "Neural Implicit Representations for Physical Parameter Inference from a Single Video",
                "abstract": "Neural networks have recently been used to analyze diverse physical systems and to identify the underlying dynamics. While existing methods achieve impressive results, they are limited by their strong demand for training data and their weak generalization abilities to out-of-distribution data. To overcome these limitations, we propose to combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) for modelling planar physical phenomena to obtain a dynamic scene representation that can be identified directly from visual observations. Our proposed model combines several unique advantages: (i) Contrary to existing approaches that require large training datasets, we are able to identify physical parameters from only a single video. (ii) The use of neural implicit representations enables the processing of high-resolution videos and the synthesis of photo-realistic images. (iii) The embedded neural ODE has a known parametric form that allows for the identification of interpretable physical parameters, and (iv) long-term prediction in state space. (v) Furthermore, the photo-realistic rendering of novel scenes with modified physical parameters becomes possible.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "22248888",
                        "name": "F. Hofherr"
                    },
                    {
                        "authorId": "1990661065",
                        "name": "Lukas Koestler"
                    },
                    {
                        "authorId": "39600032",
                        "name": "Florian Bernard"
                    },
                    {
                        "authorId": "1695302",
                        "name": "D. Cremers"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, structural priors and constraints\nare combined for fluid prediction (Tompson et al., 2017; Raissi et al., 2020), and Hamiltonian mechanics are used to construct non-regression losses (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020) to learn basic laws of physics."
            ],
            "citingPaper": {
                "paperId": "e7a47ebb237e17aa7141c5bcab4b96d6496129df",
                "externalIds": {
                    "ArXiv": "2204.08414",
                    "DBLP": "journals/corr/abs-2204-08414",
                    "DOI": "10.48550/arXiv.2204.08414",
                    "CorpusId": 248227607
                },
                "corpusId": 248227607,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e7a47ebb237e17aa7141c5bcab4b96d6496129df",
                "title": "STONet: A Neural-Operator-Driven Spatio-temporal Network",
                "abstract": "Graph-based spatio-temporal neural networks are effective to model the spatial dependency among discrete points sampled irregularly from unstructured grids, thanks to the great expressiveness of graph neural networks. However, these models are usually spatially-transductive \u2013 only \ufb01tting the signals for discrete spatial nodes fed in models but unable to generalize to \u2018unseen\u2019 spatial points with zero-shot. In comparison, for forecasting tasks on continuous space such as temperature prediction on the earth\u2019s surface, the spatially-inductive property allows the model to generalize to any point in the spatial domain, demonstrating models\u2019 ability to learn the underlying mechanisms or physics laws of the systems, rather than simply \ufb01t the signals. Besides, in temporal domains, irregularly-sampled time series, e.g. data with missing values, urge models to be temporally-continuous. Motivated by the two issues, we pro-pose a spatio-temporal framework based on neural operators for PDEs, which learn the underlying mechanisms governing the dynamics of spatially-continuous physical quantities. Experiments show our model\u2019s improved performance on forecasting spatially-continuous physic quantities, and its superior generalization to unseen spatial points and ability to handle temporally-irregular data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Haitao Lin"
                    },
                    {
                        "authorId": "150116926",
                        "name": "Guojiang Zhao"
                    },
                    {
                        "authorId": "47767812",
                        "name": "Lirong Wu"
                    },
                    {
                        "authorId": "1390908654",
                        "name": "Stan Z. Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0f58dd549ba52d9b33d423e8ec7b545bedc27fee",
                "externalIds": {
                    "ArXiv": "2204.06362",
                    "DBLP": "journals/corr/abs-2204-06362",
                    "DOI": "10.1016/j.ymssp.2023.110535",
                    "CorpusId": 248157275
                },
                "corpusId": 248157275,
                "publicationVenue": {
                    "id": "dc4b3846-1e31-4c19-a196-e8b1d091037f",
                    "name": "Mechanical systems and signal processing",
                    "type": "journal",
                    "alternate_names": [
                        "Mech syst signal process",
                        "Mech Syst Signal Process",
                        "Mechanical Systems and Signal Processing"
                    ],
                    "issn": "0888-3270",
                    "url": "https://www.journals.elsevier.com/mechanical-systems-and-signal-processing",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/08883270",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0f58dd549ba52d9b33d423e8ec7b545bedc27fee",
                "title": "A Review of Machine Learning Methods Applied to Structural Dynamics and Vibroacoustic",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064284061",
                        "name": "Barbara Z Cunha"
                    },
                    {
                        "authorId": "81538432",
                        "name": "C. Droz"
                    },
                    {
                        "authorId": "22120716",
                        "name": "A. Zine"
                    },
                    {
                        "authorId": "72411576",
                        "name": "Ste\u0301phane Foulard"
                    },
                    {
                        "authorId": "2235570",
                        "name": "M. Ichchou"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "The idea of utilizing Hamilton\u2019s equations was successfully used to predict the dynamics of Hamiltonian systems from pixel observations [13,26,31], to build representations of molecular data [21] and it was extended to control tasks [29, 31] and metalearning [20].",
                "Many extensions of HNNs [5, 9, 25, 26] use more advanced numerical integrators combined with the Neural ODE approach [3] to model the evolution of the system state in time.",
                "The analysis [32] of several numerical integrators when applied to HNNs shows that non-symplectic integrators cannot guarantee the recovery of true Hamiltonian H and the prediction accuracy obtained with a symplectic integrator depends on the integrator accuracy order.",
                "In Hamiltonian neural networks (HNNs), the law of the energy conservation is in-built in the structure of the dynamics model and therefore it is automatically satisfied."
            ],
            "citingPaper": {
                "paperId": "1c9488198aff2fffbbe06aafe45d330a4ddb775a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-05077",
                    "ArXiv": "2204.05077",
                    "DOI": "10.48550/arXiv.2204.05077",
                    "CorpusId": 248085350
                },
                "corpusId": 248085350,
                "publicationVenue": {
                    "id": "3e64b1c1-745f-4edf-bd92-b8ef122bb49c",
                    "name": "International Conference on Artificial Neural Networks",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Artif Neural Netw",
                        "ICANN"
                    ],
                    "url": "http://www.e-nns.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1c9488198aff2fffbbe06aafe45d330a4ddb775a",
                "title": "Learning Trajectories of Hamiltonian Systems with Neural Networks",
                "abstract": "Modeling of conservative systems with neural networks is an area of active research. A popular approach is to use Hamiltonian neural networks (HNNs) which rely on the assumptions that a conservative system is described with Hamilton's equations of motion. Many recent works focus on improving the integration schemes used when training HNNs. In this work, we propose to enhance HNNs with an estimation of a continuous-time trajectory of the modeled system using an additional neural network, called a deep hidden physics model in the literature. We demonstrate that the proposed integration scheme works well for HNNs, especially with low sampling rates, noisy and irregular observations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2132485673",
                        "name": "Katsiaryna Haitsiukevich"
                    },
                    {
                        "authorId": "145096481",
                        "name": "A. Ilin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4a864d3dcbdd5d1a6ac2533be2fbff60c70637d1",
                "externalIds": {
                    "PubMedCentral": "10460398",
                    "ArXiv": "2204.04348",
                    "DOI": "10.1038/s41598-023-40766-6",
                    "CorpusId": 257772087,
                    "PubMed": "37634029"
                },
                "corpusId": 257772087,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4a864d3dcbdd5d1a6ac2533be2fbff60c70637d1",
                "title": "Neuronal diversity can improve machine learning for physics and beyond",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2162047367",
                        "name": "Anil Radhakrishnan"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "[154] proposes Neural Hamiltonian Flow (NHF), which is a powerful normalising flow model using Hamiltonian dynamics as the invertible function to model expressive densities.",
                "Energy Conservation Law [117][118][109][154][134]"
            ],
            "citingPaper": {
                "paperId": "f4eb1de428295e9743a0b4754776813df6e951da",
                "externalIds": {
                    "ArXiv": "2203.16797",
                    "DBLP": "journals/corr/abs-2203-16797",
                    "DOI": "10.48550/arXiv.2203.16797",
                    "CorpusId": 247839241
                },
                "corpusId": 247839241,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f4eb1de428295e9743a0b4754776813df6e951da",
                "title": "When Physics Meets Machine Learning: A Survey of Physics-Informed Machine Learning",
                "abstract": "Physics-informed machine learning (PIML), referring to the combination of prior knowledge of physics, which is the high level abstraction of natural phenomenons and human behaviours in the long history, with data-driven machine learning models, has emerged as an effective way to mitigate the shortage of training data, to increase models' generalizability and to ensure the physical plausibility of results. In this paper, we survey an abundant number of recent works in PIML and summarize them from three aspects: (1) motivations of PIML, (2) physics knowledge in PIML, (3) methods of physics knowledge integration in PIML. We also discuss current challenges and corresponding research opportunities in PIML.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27737939",
                        "name": "Chuizheng Meng"
                    },
                    {
                        "authorId": "145260557",
                        "name": "Sungyong Seo"
                    },
                    {
                        "authorId": "120783624",
                        "name": "Defu Cao"
                    },
                    {
                        "authorId": "2160884182",
                        "name": "Sam Griesemer"
                    },
                    {
                        "authorId": "2156649635",
                        "name": "Yan Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Hamilton Generative Model(HGN) [Toth et al., 2019] proposes a Variational Autoencoder(VAE) to accommodate high-dimensional observations (such as images), and assumes hidden states are governed by Hamiltonian system.",
                "Flow Model: HGN [Toth et al., 2019] proposes a simple modification of HNN that changes it to Neural Hamiltonian Flow (NHF) model.",
                ", 2020] Standard Quadrature Canonical/Angle No RK HGN [Toth et al., 2019] Standard Quadrature Canonical/Pixel Yes Leapfrog",
                "For graph data, Hamiltonian ODE Graph Network (HOGN) [Sanchez-Gonzalez et al., 2019] combines HNN with graph neural network by the following formulation:\nHGN(q,p) = GNu(q,p, c;\u03c6) fHOGNq\u0307,p\u0307 (q,p) \u2261 ( \u2202HGN \u2202p ,\u2212\u2202HGN \u2202q ) = (q\u0307, p\u0307)\n(q,p)n+1 = RK ( \u2206t, (q,p)n, f HOGN q\u0307,p ) ,\nwhere GN denotes a graph network and RK is Runge-Kutta integrator (can be replaced with symplectic integrators)."
            ],
            "citingPaper": {
                "paperId": "ae54927b924debd9343e6b93fca40dd58295dacb",
                "externalIds": {
                    "ArXiv": "2203.00128",
                    "DBLP": "journals/corr/abs-2203-00128",
                    "DOI": "10.48550/arXiv.2203.00128",
                    "CorpusId": 247187915
                },
                "corpusId": 247187915,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ae54927b924debd9343e6b93fca40dd58295dacb",
                "title": "Learning Neural Hamiltonian Dynamics: A Methodological Overview",
                "abstract": "The past few years have witnessed an increased interest in learning Hamiltonian dynamics in deep learning frameworks. As an inductive bias based on physical laws, Hamiltonian dynamics endow neural networks with accurate long-term prediction, interpretability, and data-efficient learning. However, Hamiltonian dynamics also bring energy conservation or dissipation assumptions on the input data and additional computational overhead. In this paper, we systematically survey recently proposed Hamiltonian neural network models, with a special emphasis on methodologies. In general, we discuss the major contributions of these models, and compare them in four overlapping directions: 1) generalized Hamiltonian system; 2) symplectic integration, 3) generalized input form, and 4) extended problem settings. We also provide an outlook of the fundamental challenges and emerging opportunities in this area.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2141931645",
                        "name": "Zhijie Chen"
                    },
                    {
                        "authorId": "2067624423",
                        "name": "Mingquan Feng"
                    },
                    {
                        "authorId": "3063894",
                        "name": "Junchi Yan"
                    },
                    {
                        "authorId": "145203884",
                        "name": "H. Zha"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "One can model the driving force of the system directly [11, 13], focus on the Hamiltonian [14, 15], or the Lagrangian [6, 16]."
            ],
            "citingPaper": {
                "paperId": "9355f1c0117dc4cb9b5fb24803083f702433ac24",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-11447",
                    "ArXiv": "2202.11447",
                    "DOI": "10.1088/1367-2630/ac7c2d",
                    "CorpusId": 247058313
                },
                "corpusId": 247058313,
                "publicationVenue": {
                    "id": "8a4f69c8-3ddc-4669-921a-79403732a17e",
                    "name": "New Journal of Physics",
                    "type": "journal",
                    "alternate_names": [
                        "New J Phys"
                    ],
                    "issn": "1367-2630",
                    "url": "http://iopscience.iop.org/1367-2630",
                    "alternate_urls": [
                        "https://iopscience.iop.org/journal/1367-2630",
                        "http://njp.org/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9355f1c0117dc4cb9b5fb24803083f702433ac24",
                "title": "Reconstruction of observed mechanical motions with artificial intelligence tools",
                "abstract": "The goal of this paper is to determine the laws of observed trajectories assuming that there is a mechanical system in the background and using these laws to continue the observed motion in a plausible way. The laws are represented by neural networks with a limited number of parameters. The training of the networks follows the extreme learning machine idea. We determine laws for different levels of embedding, thus we can represent not only the equation of motion but also the symmetries of different kinds. In the recursive numerical evolution of the system, we require the fulfillment of all the observed laws, within the determined numerical precision. In this way, we can successfully reconstruct both integrable and chaotic motions, as we demonstrate in the example of the gravity pendulum and the double pendulum.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "15823876",
                        "name": "A. Jakov\u00e1c"
                    },
                    {
                        "authorId": "98510344",
                        "name": "M. T. Kurbucz"
                    },
                    {
                        "authorId": "102897395",
                        "name": "P. P\u00f3sfay"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Invariances [38] [38] [38], [38] [41], [40], [42] [40], [84] [41], [42], [84] [84] [84]"
            ],
            "citingPaper": {
                "paperId": "7819b8c7644770faf14891b0172721caacca72a2",
                "externalIds": {
                    "ArXiv": "2202.03188",
                    "DBLP": "journals/corr/abs-2202-03188",
                    "CorpusId": 246634799
                },
                "corpusId": 246634799,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7819b8c7644770faf14891b0172721caacca72a2",
                "title": "Knowledge-Integrated Informed AI for National Security",
                "abstract": "The state of artificial intelligence technology has a rich history that dates back decades and includes two fall-outs before the explosive resurgence of today, which is credited largely to datadriven techniques. While AI technology has and continues to become increasingly mainstream with impact across domains and industries, it\u2019s not without several drawbacks, weaknesses, and potential to cause undesired effects. AI techniques are numerous with many approaches and variants, but they can be classified simply based on the degree of knowledge they capture and how much data they require; two broad categories emerge as prominent across AI to date: (1) techniques that are primarily, and often solely, data-driven while leveraging little to no knowledge and (2) techniques that primarily leverage knowledge and depend less on data. Now, a third category is starting to emerge that leverages both data and knowledge, that some refer to as \u201cinformed AI.\u201d This third category can be a game changer within the national security domain where there is ample scientific and domain-specific knowledge that stands ready to be leveraged, and where purely data-driven AI can lead to serious unwanted consequences. This report shares findings from a thorough exploration of AI approaches that exploit data as well as principled and/or practical knowledge, which we refer to as \u201cknowledge-integrated informed AI.\u201d Specifically, we review illuminating examples of knowledge integrated in deep learning and reinforcement learning pipelines, taking note of the performance gains they provide. We also discuss an apparent trade space across variants of knowledge-integrated informed AI, along with observed and prominent issues that suggest worthwhile future research directions. Most importantly, this report suggests how the advantages of knowledge-integrated informed AI stand to benefit the national security domain.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9336416",
                        "name": "Anu Myne"
                    },
                    {
                        "authorId": "144561477",
                        "name": "Kevin J. Leahy"
                    },
                    {
                        "authorId": "49902902",
                        "name": "Ryan Soklaski"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5f4f196fc6277185d85816501bb814dfaeed69e4",
                "externalIds": {
                    "ArXiv": "2207.06415",
                    "DBLP": "journals/entropy/MazzagliaVCD22",
                    "PubMedCentral": "8871280",
                    "DOI": "10.3390/e24020301",
                    "CorpusId": 247066403,
                    "PubMed": "35205595"
                },
                "corpusId": 247066403,
                "publicationVenue": {
                    "id": "8270cfe1-3713-4325-a7bd-c6a87eed889e",
                    "name": "Entropy",
                    "type": "journal",
                    "issn": "1099-4300",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606",
                    "alternate_urls": [
                        "http://www.mdpi.com/journal/entropy/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155606",
                        "https://www.mdpi.com/journal/entropy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5f4f196fc6277185d85816501bb814dfaeed69e4",
                "title": "The Free Energy Principle for Perception and Action: A Deep Learning Perspective",
                "abstract": "The free energy principle, and its corollary active inference, constitute a bio-inspired theory that assumes biological agents act to remain in a restricted set of preferred states of the world, i.e., they minimize their free energy. Under this principle, biological agents learn a generative model of the world and plan actions in the future that will maintain the agent in an homeostatic state that satisfies its preferences. This framework lends itself to being realized in silico, as it comprehends important aspects that make it computationally affordable, such as variational inference and amortized planning. In this work, we investigate the tool of deep learning to design and realize artificial agents based on active inference, presenting a deep-learning oriented presentation of the free energy principle, surveying works that are relevant in both machine learning and active inference areas, and discussing the design choices that are involved in the implementation process. This manuscript probes newer perspectives for the active inference framework, grounding its theoretical aspects into more pragmatic affairs, offering a practical guide to active inference newcomers and a starting point for deep learning practitioners that would like to investigate implementations of the free energy principle.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2098445287",
                        "name": "Pietro Mazzaglia"
                    },
                    {
                        "authorId": "2413244",
                        "name": "Tim Verbelen"
                    },
                    {
                        "authorId": "108098245",
                        "name": "Ozan \u00c7atal"
                    },
                    {
                        "authorId": "1733741",
                        "name": "B. Dhoedt"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Toth et al. (2019) introduced Hamiltonian Generative Networks (HGNs), a class of generative models that can learn time-reversible Hamiltonian dynamics in an abstract phase space, starting from image inputs.",
                "Recent work by Greydanus et al. (2019), Toth et al. (2019) and others has shown that this function can be used to incorporate physical constraints, like conservation of energy, into deep neural networks."
            ],
            "citingPaper": {
                "paperId": "0823b788f7d2b4e840e092f8d2489b44d142c3b1",
                "externalIds": {
                    "ArXiv": "2201.10085",
                    "DBLP": "journals/corr/abs-2201-10085",
                    "CorpusId": 246275946
                },
                "corpusId": 246275946,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0823b788f7d2b4e840e092f8d2489b44d142c3b1",
                "title": "Dissipative Hamiltonian Neural Networks: Learning Dissipative and Conservative Dynamics Separately",
                "abstract": "Understanding natural symmetries is key to making sense of our complex and ever-changing world. Recent work has shown that neural networks can learn such symmetries directly from data using Hamiltonian Neural Networks (HNNs). But HNNs struggle when trained on datasets where energy is not conserved. In this paper, we ask whether it is possible to identify and decompose conservative and dissipative dynamics simultaneously. We propose Dissipative Hamiltonian Neural Networks (D-HNNs), which parameterize both a Hamiltonian and a Rayleigh dissipation function. Taken together, they represent an implicit Helmholtz decomposition which can separate dissipative effects such as friction from symmetries such as conservation of energy. We train our model to decompose a damped mass-spring system into its friction and inertial terms and then show that this decomposition can be used to predict dynamics for unseen friction coefficients. Then we apply our model to real world data including a large, noisy ocean current dataset where decomposing the velocity field yields useful scientific insights.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "83196249",
                        "name": "A. Sosanya"
                    },
                    {
                        "authorId": "14851288",
                        "name": "S. Greydanus"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Some Hamiltonian methods (Toth et al., 2019; Yildiz et al., 2019) also model the dynamics of high dimensional sequential data in a latent space.",
                "Recently Toth et al. (2019) developed the Hamiltonian generative network (HGN), where they proposed to learn a Hamiltonian from image sequences.",
                "Higgins et al. (2018); Toth et al. (2019); Botev et al. (2021) have discussed the benefits of such inductive biases for learning disentangled representation."
            ],
            "citingPaper": {
                "paperId": "71ab40c75c75d41f1780436682723699a828d72c",
                "externalIds": {
                    "ArXiv": "2112.01641",
                    "DBLP": "conf/nips/0008S22",
                    "CorpusId": 252873675
                },
                "corpusId": 252873675,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/71ab40c75c75d41f1780436682723699a828d72c",
                "title": "Hamiltonian Latent Operators for content and motion disentanglement in image sequences",
                "abstract": "We introduce \\textit{HALO} -- a deep generative model utilising HAmiltonian Latent Operators to reliably disentangle content and motion information in image sequences. The \\textit{content} represents summary statistics of a sequence, and \\textit{motion} is a dynamic process that determines how information is expressed in any part of the sequence. By modelling the dynamics as a Hamiltonian motion, important desiderata are ensured: (1) the motion is reversible, (2) the symplectic, volume-preserving structure in phase space means paths are continuous and are not divergent in the latent space. Consequently, the nearness of sequence frames is realised by the nearness of their coordinates in the phase space, which proves valuable for disentanglement and long-term sequence generation. The sequence space is generally comprised of different types of dynamical motions. To ensure long-term separability and allow controlled generation, we associate every motion with a unique Hamiltonian that acts in its respective subspace. We demonstrate the utility of \\textit{HALO} by swapping the motion of a pair of sequences, controlled generation, and image rotations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2149889657",
                        "name": "Asif Khan"
                    },
                    {
                        "authorId": "1728216",
                        "name": "A. Storkey"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "385f4b919171f35824f45469a4de56cc915b7970",
                "externalIds": {
                    "ArXiv": "2111.15176",
                    "DBLP": "journals/corr/abs-2111-15176",
                    "CorpusId": 244729236
                },
                "corpusId": 244729236,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/385f4b919171f35824f45469a4de56cc915b7970",
                "title": "Learning Large-Time-Step Molecular Dynamics with Graph Neural Networks",
                "abstract": "Molecular dynamics (MD) simulation predicts the trajectory of atoms by solving Newton's equation of motion with a numeric integrator. Due to physical constraints, the time step of the integrator need to be small to maintain sufficient precision. This limits the efficiency of simulation. To this end, we introduce a graph neural network (GNN) based model, MDNet, to predict the evolution of coordinates and momentum with large time steps. In addition, MDNet can easily scale to a larger system, due to its linear complexity with respect to the system size. We demonstrate the performance of MDNet on a 4000-atom system with large time steps, and show that MDNet can predict good equilibrium and transport properties, well aligned with standard MD simulations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2053658872",
                        "name": "Tian Zheng"
                    },
                    {
                        "authorId": "2153577134",
                        "name": "Weihao Gao"
                    },
                    {
                        "authorId": "2146309022",
                        "name": "Chong Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026Generative Networks (HGNs) are capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions, and the Neural Hamiltonian Flow (NHF), which uses Hamiltonian dynamics to model expressive densities [Toth et al., 2019]."
            ],
            "citingPaper": {
                "paperId": "b7930e96318bf45b213b535f15a7a2cbf655f842",
                "externalIds": {
                    "ArXiv": "2111.15631",
                    "DBLP": "journals/corr/abs-2111-15631",
                    "CorpusId": 244729473
                },
                "corpusId": 244729473,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b7930e96318bf45b213b535f15a7a2cbf655f842",
                "title": "Neural Symplectic Integrator with Hamiltonian Inductive Bias for the Gravitational $N$-body Problem",
                "abstract": "The gravitational $N$-body problem, which is fundamentally important in astrophysics to predict the motion of $N$ celestial bodies under the mutual gravity of each other, is usually solved numerically because there is no known general analytical solution for $N>2$. Can an $N$-body problem be solved accurately by a neural network (NN)? Can a NN observe long-term conservation of energy and orbital angular momentum? Inspired by Wistom&Holman (1991)'s symplectic map, we present a neural $N$-body integrator for splitting the Hamiltonian into a two-body part, solvable analytically, and an interaction part that we approximate with a NN. Our neural symplectic $N$-body code integrates a general three-body system for $10^{5}$ steps without diverting from the ground truth dynamics obtained from a traditional $N$-body integrator. Moreover, it exhibits good inductive bias by successfully predicting the evolution of $N$-body systems that are no part of the training set.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "15590614",
                        "name": "M. Cai"
                    },
                    {
                        "authorId": "1843702",
                        "name": "S. Zwart"
                    },
                    {
                        "authorId": "29980657",
                        "name": "Damian Podareanu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Subsequent work uses invertible ResNet, optimal transport theory, among other techniques to further improve the performance of CNFs [1, 2, 4, 11, 18, 20, 44, 48, 51]."
            ],
            "citingPaper": {
                "paperId": "17773066cf244e82503e7283dcf1981bfe4ef0a6",
                "externalIds": {
                    "ArXiv": "2111.13207",
                    "DBLP": "journals/corr/abs-2111-13207",
                    "CorpusId": 244709251
                },
                "corpusId": 244709251,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/17773066cf244e82503e7283dcf1981bfe4ef0a6",
                "title": "Characteristic Neural Ordinary Differential Equations",
                "abstract": "We propose Characteristic-Neural Ordinary Differential Equations (C-NODEs), a framework for extending Neural Ordinary Differential Equations (NODEs) beyond ODEs. While NODEs model the evolution of a latent variables as the solution to an ODE, C-NODE models the evolution of the latent variables as the solution of a family of first-order quasi-linear partial differential equations (PDEs) along curves on which the PDEs reduce to ODEs, referred to as characteristic curves. This in turn allows the application of the standard frameworks for solving ODEs, namely the adjoint method. Learning optimal characteristic curves for given tasks improves the performance and computational efficiency, compared to state of the art NODE models. We prove that the C-NODE framework extends the classical NODE on classification tasks by demonstrating explicit C-NODE representable functions not expressible by NODEs. Additionally, we present C-NODE-based continuous normalizing flows, which describe the density evolution of latent variables along multiple dimensions. Empirical results demonstrate the improvements provided by the proposed method for classification and density estimation on CIFAR-10, SVHN, and MNIST datasets under a similar computational budget as the existing NODE methods. The results also provide empirical evidence that the learned curves improve the efficiency of the system through a lower number of parameters and function evaluations compared with baselines.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152775979",
                        "name": "Xingzi Xu"
                    },
                    {
                        "authorId": "2065152192",
                        "name": "Ali Hasan"
                    },
                    {
                        "authorId": "1808335",
                        "name": "Khalil Elkhalil"
                    },
                    {
                        "authorId": "143798670",
                        "name": "Jie Ding"
                    },
                    {
                        "authorId": "1780864",
                        "name": "V. Tarokh"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "To overcome these problems, we therefore combine latest advances in physics-enhanced Neural Networks based on dynamic invariants such as the Hamiltonian [1, 2, 3] or Lagrangian [4, 5] NNs with additional inductive bias to gain better predictive capabilities and to reduce the requisite training data."
            ],
            "citingPaper": {
                "paperId": "72a3e30782e715a633a35b423ac96036bd6897ab",
                "externalIds": {
                    "ArXiv": "2111.10329",
                    "DBLP": "journals/corr/abs-2111-10329",
                    "CorpusId": 244463107
                },
                "corpusId": 244463107,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/72a3e30782e715a633a35b423ac96036bd6897ab",
                "title": "Physics-enhanced Neural Networks in the Small Data Regime",
                "abstract": "Identifying the dynamics of physical systems requires a machine learning model that can assimilate observational data, but also incorporate the laws of physics. Neural Networks based on physical principles such as the Hamiltonian or Lagrangian NNs have recently shown promising results in generating extrapolative predictions and accurately representing the system's dynamics. We show that by additionally considering the actual energy level as a regularization term during training and thus using physical information as inductive bias, the results can be further improved. Especially in the case where only small amounts of data are available, these improvements can significantly enhance the predictive capability. We apply the proposed regularization term to a Hamiltonian Neural Network (HNN) and Constrained Hamiltonian Neural Network (CHHN) for a single and double pendulum, generate predictions under unseen initial conditions and report significant gains in predictive accuracy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2141595883",
                        "name": "Jonas Eichelsd\u00f6rfer"
                    },
                    {
                        "authorId": "113831649",
                        "name": "Sebastian Kaltenbach"
                    },
                    {
                        "authorId": "2415520",
                        "name": "P. Koutsourelakis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Recently, a body of work has emerged that brings these well-established principles of modelling dynamics from physics \u2013 such as the conservation of energy, and numerical formulations from the theory of differential equations \u2013 to neural network architectures [49, 20, 9, 3, 58, 8, 4, 31, 50, 43, 56, 12, 17, 24, 14, 32, 11, 45, 57, 54, 48, 21].",
                "employed by [49, 9, 45, 3, 58]), where the model is evaluated on how well it can reproduce the same trajectory length T as was used for training, albeit using test data.",
                "HGN HGN [49] is a generative model that aims to learn Hamiltonian dynamics from pixel observations x.",
                "Currently only a handful of methods exist for learning dynamics with physical priors from pixels [49, 9, 45, 3, 58].",
                "We use these measures to identify a set of hyperparameters and architectural modifications that significantly improves the performance of Hamiltonian Generative Networks (HGN) [49], an existing state of the art model for recovering Hamiltonian dynamics from pixel observations, both in terms of long time-scale predictions, and interpretability of the learnt latent space."
            ],
            "citingPaper": {
                "paperId": "785139d4fdc017fe43ee986ebd89d7bdb1211843",
                "externalIds": {
                    "ArXiv": "2111.05986",
                    "DBLP": "journals/corr/abs-2111-05986",
                    "CorpusId": 243985811
                },
                "corpusId": 243985811,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/785139d4fdc017fe43ee986ebd89d7bdb1211843",
                "title": "SyMetric: Measuring the Quality of Learnt Hamiltonian Dynamics Inferred from Vision",
                "abstract": "A recently proposed class of models attempts to learn latent dynamics from high-dimensional observations, like images, using priors informed by Hamiltonian mechanics. While these models have important potential applications in areas like robotics or autonomous driving, there is currently no good way to evaluate their performance: existing methods primarily rely on image reconstruction quality, which does not always reflect the quality of the learnt latent dynamics. In this work, we empirically highlight the problems with the existing measures and develop a set of new measures, including a binary indicator of whether the underlying Hamiltonian dynamics have been faithfully captured, which we call Symplecticity Metric or SyMetric. Our measures take advantage of the known properties of Hamiltonian dynamics and are more discriminative of the model's ability to capture the underlying dynamics than reconstruction error. Using SyMetric, we identify a set of architectural choices that significantly improve the performance of a previously proposed model for inferring latent dynamics from pixels, the Hamiltonian Generative Network (HGN). Unlike the original HGN, the new HGN++ is able to discover an interpretable phase space with physically meaningful latents on some datasets. Furthermore, it is stable for significantly longer rollouts on a diverse range of 13 datasets, producing rollouts of essentially infinite length both forward and backwards in time with no degradation in quality on a subset of the datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39051054",
                        "name": "I. Higgins"
                    },
                    {
                        "authorId": "5721359",
                        "name": "Peter Wirnsberger"
                    },
                    {
                        "authorId": "2689633",
                        "name": "Andrew Jaegle"
                    },
                    {
                        "authorId": "3436640",
                        "name": "Aleksandar Botev"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "For the HGN model we use the leap-frog integrator.",
                "What makes HGN and LGN models an attractive option is that when they do learn a dataset well, their long extrapolations are consistently good, unlike those for the other models which have larger variances (shown in brackets in Table 1).",
                "The encoder and decoder networks are implemented as convolutional modules with leaky relu activations similar to the original HGN paper.",
                "The network architecture used forF is equivalent to that used for the kinetic and potential networks in the HGN, with the only difference being that it outputs not a scalar, but a vector of the same dimensionality as the input.",
                "We implement the dynamics module with Hamiltonian (HGN) and Lagrangian (LGN) priors by using two neural networks to parameterise the kinetic energy T and the potential energy V .",
                "We use a Variational Autoencoder (VAE) [38, 39] as the basis for our models, inspired by the Hamiltonian Generative Network (HGN) [11].",
                "In terms of backward extrapolations, HGN and LGN do well.",
                "Unlike HGN and LGN that use MLPs to parametrise the functions that give rise to the time derivatives of the latent state, here the time evolution of the latent state is directly parameterised by an MLP: s\u0307=F(s) (see Figure 1).",
                "Both the HGN and LGN models show good promise, especially in terms of getting good backward extrapolation performance effectively \u201cfor free\u201d, however, they are still somewhat lagging behind the Neural ODE [TR], which implements the same inductive biases of modeling continual dynamics,\npredicting state update residuals and modelling the dynamics forward and backwards in time, but constrained in a different manner.",
                "To address this issue, a number of approaches have been proposed that augment their physics-inspired models of dynamics with encoder/decoder modules for inferring the low-dimensional states from high-dimensional pixel observations [11, 24, 25, 22, 21].",
                "This intuition has been referred to as the Hamiltonian manifold hypothesis [11], which conjectures that natural images lie on a low-dimensional manifold embedded within a high-dimensional pixel space and natural sequences of images trace out paths on this manifold that follow the equations of an abstract Hamiltonian.",
                "To reduce the plethora of architectural choices we restrict our investigation to models which are Markovian and which treat the latent representation as a single vector without making any further assumptions or introducing further inductive biases (for example some previous works treat the latent representation as a spatial image or a graph [11, 36, 37, 4])."
            ],
            "citingPaper": {
                "paperId": "09865ffe9d8f9a0438100de39bcf6131fa55ce11",
                "externalIds": {
                    "DBLP": "conf/nips/BotevJWHH21",
                    "ArXiv": "2111.05458",
                    "CorpusId": 237368146
                },
                "corpusId": 237368146,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/09865ffe9d8f9a0438100de39bcf6131fa55ce11",
                "title": "Which priors matter? Benchmarking models for learning latent dynamics",
                "abstract": "Learning dynamics is at the heart of many important applications of machine learning (ML), such as robotics and autonomous driving. In these settings, ML algorithms typically need to reason about a physical system using high dimensional observations, such as images, without access to the underlying state. Recently, several methods have proposed to integrate priors from classical mechanics into ML models to address the challenge of physical reasoning from images. In this work, we take a sober look at the current capabilities of these models. To this end, we introduce a suite consisting of 17 datasets with visual observations based on physical systems exhibiting a wide range of dynamics. We conduct a thorough and detailed comparison of the major classes of physically inspired methods alongside several strong baselines. While models that incorporate physical priors can often learn latent spaces with desirable properties, our results demonstrate that these methods fail to significantly improve upon standard techniques. Nonetheless, we find that the use of continuous and time-reversible dynamics benefits models of all classes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3436640",
                        "name": "Aleksandar Botev"
                    },
                    {
                        "authorId": "2689633",
                        "name": "Andrew Jaegle"
                    },
                    {
                        "authorId": "5721359",
                        "name": "Peter Wirnsberger"
                    },
                    {
                        "authorId": "1897926",
                        "name": "Daniel Hennes"
                    },
                    {
                        "authorId": "39051054",
                        "name": "I. Higgins"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "First, Section 4.4.1 describes how energy-conserving dynamics can be enforced by encoding the problem using Hamiltonian or Lagrangian mechanics.",
                "For instance, we relate energy-conserving numerical solvers to Hamiltonian NNs, whose goal is to encode energy conservation, and we discuss concepts such as numerical stability and solver convergence, which are crucial in long-term prediction using NNs.",
                "We start with the Hamiltonian defined as\nH (x ) = T (x ) \u2212V (x ), (14)\nwhere x = [q,p] represents the concatenated state vector of generalized coordinates q and generalized momentap.",
                "Despite their mathematical elegance, deriving analytical Hamiltonian and Lagrangian functions for complex dynamical systems is a grueling task.",
                "The main advantage of Hamiltonian [41, 124] NNs and the closely related Lagrangian [21, 77] NNs is that they naturally incorporate the preservation of energy into the network structure itself.",
                "A similar concept to that of Hamiltonian and Lagrangian NNs involves learning neural surrogates for potential energy functionsV (x ) of a dynamical system, where the primary difference with Hamiltonians and Lagrangians is that the kinetic terms are\nACM Computing Surveys, Vol. 55, No. 11, Article 236.",
                "Specifically, the goal is to train an NN to approximate the Hamiltonian/Lagrangian of the system, as shown in Figure 18.",
                "In physics, a special class of closely related functions, called Hamiltonian and Lagrangian functions, has been developed for describing the total energy of a system.",
                "4.4.1 Hamiltonian and Lagrangian Networks.",
                "To improve the performance, others have introduced various inductive biases such as Hamiltonian NODE architecture [142] or penalizing higher-order derivatives of the NODEs in the\nACM Computing Surveys, Vol. 55, No. 11, Article 236.",
                "Both Hamiltonian H and Lagrangian L are defined as a sum of total kineticT and potential energy V of the system."
            ],
            "citingPaper": {
                "paperId": "0ffddec4d715c86a9e41ca61124fb47941cf7dfd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-01495",
                    "ArXiv": "2111.01495",
                    "DOI": "10.1145/3567591",
                    "CorpusId": 240420173
                },
                "corpusId": 240420173,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0ffddec4d715c86a9e41ca61124fb47941cf7dfd",
                "title": "Constructing Neural Network Based Models for Simulating Dynamical Systems",
                "abstract": "Dynamical systems see widespread use in natural sciences like physics, biology, and chemistry, as well as engineering disciplines such as circuit analysis, computational fluid dynamics, and control. For simple systems, the differential equations governing the dynamics can be derived by applying fundamental physical laws. However, for more complex systems, this approach becomes exceedingly difficult. Data-driven modeling is an alternative paradigm that seeks to learn an approximation of the dynamics of a system using observations of the true system. In recent years, there has been an increased interest in applying data-driven modeling techniques to solve a wide range of problems in physics and engineering. This article provides a survey of the different ways to construct models of dynamical systems using neural networks. In addition to the basic overview, we review the related literature and outline the most significant challenges from numerical simulations that this modeling paradigm must overcome. Based on the reviewed literature and identified challenges, we provide a discussion on promising research areas.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2121348428",
                        "name": "C. Legaard"
                    },
                    {
                        "authorId": "1392677248",
                        "name": "Thomas Schranz"
                    },
                    {
                        "authorId": "40085274",
                        "name": "G. Schweiger"
                    },
                    {
                        "authorId": "2136392921",
                        "name": "J'an Drgovna"
                    },
                    {
                        "authorId": "101678794",
                        "name": "Basak Falay"
                    },
                    {
                        "authorId": "145746046",
                        "name": "C. Gomes"
                    },
                    {
                        "authorId": "3074923",
                        "name": "Alexandros Iosifidis"
                    },
                    {
                        "authorId": "46220390",
                        "name": "M. Abkar"
                    },
                    {
                        "authorId": "1792089",
                        "name": "P. Larsen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "[5, 17, 46] introduce non-regression loss functions inspired by Hamiltonian mechanics [19]."
            ],
            "citingPaper": {
                "paperId": "862b436e4385aa1984ceef922fad6ce9725af3e3",
                "externalIds": {
                    "ArXiv": "2110.14392",
                    "DBLP": "conf/bmvc/PourheydariRFFN22",
                    "CorpusId": 239998098
                },
                "corpusId": 239998098,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/862b436e4385aa1984ceef922fad6ce9725af3e3",
                "title": "TaylorSwiftNet: Taylor Driven Temporal Modeling for Swift Future Frame Prediction",
                "abstract": "While recurrent neural networks (RNNs) demonstrate outstanding capabilities for future video frame prediction, they model dynamics in a discrete time space, i.e., they predict the frames sequentially with a fixed temporal step. RNNs are therefore prone to accumulate the error as the number of future frames increases. In contrast, partial differential equations (PDEs) model physical phenomena like dynamics in a continuous time space. However, the estimated PDE for frame forecasting needs to be numerically solved, which is done by discretization of the PDE and diminishes most of the advantages compared to discrete models. In this work, we, therefore, propose to approximate the motion in a video by a continuous function using the Taylor series. To this end, we introduce TaylorSwiftNet, a novel convolutional neural network that learns to estimate the higher order terms of the Taylor series for a given input video. TaylorSwiftNet can swiftly predict future frames in parallel and it allows to change the temporal resolution of the forecast frames on-the-fly. The experimental results on various datasets demonstrate the superiority of our model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2135541738",
                        "name": "Mohammad Pourheydari"
                    },
                    {
                        "authorId": null,
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "authorId": "2025664854",
                        "name": "Emad Bahrami Rad"
                    },
                    {
                        "authorId": "1811235696",
                        "name": "M. Noroozi"
                    },
                    {
                        "authorId": "145689714",
                        "name": "Juergen Gall"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Introduction of physical knowledge prior [41], [42], [43], [44], [45] has also shown to enhance the modeling and control of latent dynamics."
            ],
            "citingPaper": {
                "paperId": "59924d04b20a0c38dfb7f2bda3b7d62e41dcbb53",
                "externalIds": {
                    "ArXiv": "2110.08239",
                    "DBLP": "conf/case/WangKH22",
                    "DOI": "10.1109/CASE49997.2022.9926535",
                    "CorpusId": 239009733
                },
                "corpusId": 239009733,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/59924d04b20a0c38dfb7f2bda3b7d62e41dcbb53",
                "title": "Learn Proportional Derivative Controllable Latent Space from Pixels",
                "abstract": "Recent advances in latent space dynamics model from pixels show promising progress in vision-based model predictive control (MPC). However, executing MPC in real time can be challenging due to its intensive computational cost in each timestep. We propose to introduce additional learning objectives to enforce that the learned latent space is proportional derivative controllable. In execution time, the simple PD-controller can be applied directly to the latent space encoded from pixels, to produce simple and effective control to systems with visual observations. We show that our method outperforms baseline methods to produce robust goal reaching and trajectory tracking in various environments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2127969286",
                        "name": "Weiyao Wang"
                    },
                    {
                        "authorId": "3104472",
                        "name": "Marin Kobilarov"
                    },
                    {
                        "authorId": "1678633",
                        "name": "Gregory Hager"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "51b489ece8d86e73e85e9d095b4a73620d5c2029",
                "externalIds": {
                    "ArXiv": "2110.03903",
                    "DBLP": "journals/corr/abs-2110-03903",
                    "CorpusId": 238531598
                },
                "corpusId": 238531598,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/51b489ece8d86e73e85e9d095b4a73620d5c2029",
                "title": "Kinematically consistent recurrent neural networks for learning inverse problems in wave propagation",
                "abstract": "Although machine learning (ML) is increasingly employed recently for mechanistic problems, the black-box nature of conventional ML architectures lacks the physical knowledge to infer unforeseen input conditions. This implies both severe overfitting during a dearth of training data and inadequate physical interpretability, which motivates us to propose a new kinematically consistent, physics-based ML model. In particular, we attempt to perform physically interpretable learning of inverse problems in wave propagation without suffering overfitting restrictions. Towards this goal, we employ long short-term memory (LSTM) networks endowed with a physical, hyperparameter-driven regularizer, performing penalty-based enforcement of the characteristic geometries. Since these characteristics are the kinematical invariances of wave propagation phenomena, maintaining their structure provides kinematical consistency to the network. Even with modest training data, the kinematically consistent network can reduce the $L_1$ and $L_\\infty$ error norms of the plain LSTM predictions by about 45% and 55%, respectively. It can also increase the horizon of the plain LSTM's forecasting by almost two times. To achieve this, an optimal range of the physical hyperparameter, analogous to an artificial bulk modulus, has been established through numerical experiments. The efficacy of the proposed method in alleviating overfitting, and the physical interpretability of the learning mechanism, are also discussed. Such an application of kinematically consistent LSTM networks for wave propagation learning is presented here for the first time.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "72216413",
                        "name": "Wrik Mallik"
                    },
                    {
                        "authorId": "6330065",
                        "name": "R. Jaiman"
                    },
                    {
                        "authorId": "71859411",
                        "name": "J. Jelovica"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "120fd9fb41c0e6c1b7869fbb09e3f6c6a9113086",
                "externalIds": {
                    "DBLP": "journals/ijrr/LutterP23",
                    "ArXiv": "2110.01894",
                    "DOI": "10.1177/02783649231169492",
                    "CorpusId": 238353891
                },
                "corpusId": 238353891,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/120fd9fb41c0e6c1b7869fbb09e3f6c6a9113086",
                "title": "Combining physics and deep learning to learn continuous-time dynamics models",
                "abstract": "Deep learning has been widely used within learning algorithms for robotics. One disadvantage of deep networks is that these networks are black-box representations. Therefore, the learned approximations ignore the existing knowledge of physics or robotics. Especially for learning dynamics models, these black-box models are not desirable as the underlying principles are well understood and the standard deep networks can learn dynamics that violate these principles. To learn dynamics models with deep networks that guarantee physically plausible dynamics, we introduce physics-inspired deep networks that combine first principles from physics with deep learning. We incorporate Lagrangian mechanics within the model learning such that all approximated models adhere to the laws of physics and conserve energy. Deep Lagrangian Networks (DeLaN) parametrize the system energy using two networks. The parameters are obtained by minimizing the squared residual of the Euler\u2013Lagrange differential equation. Therefore, the resulting model does not require specific knowledge of the individual system, is interpretable, and can be used as a forward, inverse, and energy model. Previously these properties were only obtained when using system identification techniques that require knowledge of the kinematic structure. We apply DeLaN to learning dynamics models and apply these models to control simulated and physical rigid body systems. The results show that the proposed approach obtains dynamics models that can be applied to physical systems for real-time control. Compared to standard deep networks, the physics-inspired models learn better models and capture the underlying structure of the dynamics.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49199170",
                        "name": "M. Lutter"
                    },
                    {
                        "authorId": "145197867",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Data-driven methods allow, essentially, to learn and explore dynamical systems from given data alone [5, 6, 7]."
            ],
            "citingPaper": {
                "paperId": "7f030362b51d2a40382099dc6d32d77696a44d94",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-09151",
                    "ArXiv": "2109.09151",
                    "DOI": "10.1016/j.jcp.2023.111911",
                    "CorpusId": 237572178
                },
                "corpusId": 237572178,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7f030362b51d2a40382099dc6d32d77696a44d94",
                "title": "Locally-symplectic neural networks for learning volume-preserving dynamics",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7832894",
                        "name": "J. Baj\u0101rs"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026works either use the Lagrangian or the Hamiltonian formulation of dynamics to inform the structure of a neural ODE, as in (Cranmer et al. 2020; Lutter, Ritter, and Peters 2019; Roehrl et al. 2020) vs. (Greydanus, Dzamba, and Yosinski 2019; Matsubara, Ishikawa, and Yaguchi 2020; Toth et al. 2020)."
            ],
            "citingPaper": {
                "paperId": "d270b89b8161fd114daca4b4ceff849423a08a30",
                "externalIds": {
                    "ArXiv": "2109.06407",
                    "DBLP": "journals/corr/abs-2109-06407",
                    "CorpusId": 237503331
                },
                "corpusId": 237503331,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d270b89b8161fd114daca4b4ceff849423a08a30",
                "title": "Neural Networks with Physics-Informed Architectures and Constraints for Dynamical Systems Modeling",
                "abstract": "Effective inclusion of physics-based knowledge into deep neural network models of dynamical systems can greatly improve data efficiency and generalization. Such a-priori knowledge might arise from physical principles (e.g., conservation laws) or from the system's design (e.g., the Jacobian matrix of a robot), even if large portions of the system dynamics remain unknown. We develop a framework to learn dynamics models from trajectory data while incorporating a-priori system knowledge as inductive bias. More specifically, the proposed framework uses physics-based side information to inform the structure of the neural network itself, and to place constraints on the values of the outputs and the internal states of the model. It represents the system's vector field as a composition of known and unknown functions, the latter of which are parametrized by neural networks. The physics-informed constraints are enforced via the augmented Lagrangian method during the model's training. We experimentally demonstrate the benefits of the proposed approach on a variety of dynamical systems -- including a benchmark suite of robotics environments featuring large state spaces, non-linear dynamics, external forces, contact forces, and control inputs. By exploiting a-priori system knowledge during training, the proposed approach learns to predict the system dynamics two orders of magnitude more accurately than a baseline approach that does not include prior knowledge, given the same training dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1475901145",
                        "name": "Franck Djeumou"
                    },
                    {
                        "authorId": "1796254983",
                        "name": "Cyrus Neary"
                    },
                    {
                        "authorId": "1709823",
                        "name": "\u00c9. Goubault"
                    },
                    {
                        "authorId": "2307593",
                        "name": "S. Putot"
                    },
                    {
                        "authorId": "3199888",
                        "name": "U. Topcu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Unsupervised system identification from vision is a recent area of research that removes the requirements for trajectory data, with approaches including unsupervised physical parameter estimation [24, 31, 40], structured latent space learning [19, 25, 32], and Hamiltonian/Lagrangian learning [18, 51, 58]."
            ],
            "citingPaper": {
                "paperId": "19408c8b95b477e359b593420077d8cebfc3d5dd",
                "externalIds": {
                    "ArXiv": "2109.05928",
                    "DBLP": "conf/l4dc/JaquesABH22",
                    "CorpusId": 237491851
                },
                "corpusId": 237491851,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/19408c8b95b477e359b593420077d8cebfc3d5dd",
                "title": "Vision-based system identification and 3D keypoint discovery using dynamics constraints",
                "abstract": "This paper introduces V-SysId, a novel method that enables simultaneous keypoint discovery, 3D system identification, and extrinsic camera calibration from an unlabeled video taken from a static camera, using only the family of equations of motion of the object of interest as weak supervision. V-SysId takes keypoint trajectory proposals and alternates between maximum likelihood parameter estimation and extrinsic camera calibration, before applying a suitable selection criterion to identify the track of interest. This is then used to train a keypoint tracking model using supervised learning. Results on a range of settings (robotics, physics, physiology) highlight the utility of this approach.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50425877",
                        "name": "Miguel Jaques"
                    },
                    {
                        "authorId": "67246377",
                        "name": "Martin Asenov"
                    },
                    {
                        "authorId": "145841847",
                        "name": "Michael Burke"
                    },
                    {
                        "authorId": "1697755",
                        "name": "Timothy M. Hospedales"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Reversible bracket formalisms (e.g. Hamiltonian/Lagrangian mechanics) have been shown effective for learning reversible dynamics (Toth et al. 2019; Cranmer et al. 2020; Lutter, Ritter, and Peters 2018; Chen et al. 2019; Jin et al. 2020; Tong et al. 2021; Zhong, Dey, and Chakraborty 2021; Chen and\u2026",
                "Parameterization techniques that preserve physical structure include Hamiltonian neural networks (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019), Lagrangian neural networks (Cranmer et al.",
                "Parameterization techniques that preserve physical\nstructure include Hamiltonian neural networks (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019), Lagrangian neural networks (Cranmer et al. 2020; Lutter, Ritter, and Peters 2018), port-Hamiltonian neural networks (Desai et al. 2021), and\u2026",
                "Hamiltonian/Lagrangian mechanics) have been shown effective for learning reversible dynamics (Toth et al. 2019; Cranmer et al. 2020; Lutter, Ritter, and Peters 2018; Chen et al. 2019; Jin et al. 2020; Tong et al. 2021; Zhong, Dey, and Chakraborty 2021; Chen and Tao 2021; Bertalan et al. 2019), while dissipative metric bracket extensions provide generalizations to irreversible dynamics in the metriplectic formalism (Lee, Trask, and Stinis 2021; Desai et al."
            ],
            "citingPaper": {
                "paperId": "66b2509feac026d77bc0d2a3066c76918b77b9d5",
                "externalIds": {
                    "ArXiv": "2109.05364",
                    "DBLP": "journals/corr/abs-2109-05364",
                    "CorpusId": 237490407
                },
                "corpusId": 237490407,
                "publicationVenue": {
                    "id": "bed36c1a-3a44-42af-bd5a-4ffda6aa9fa2",
                    "name": "Mathematical and Scientific Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "MSML",
                        "Math Sci Mach Learn"
                    ],
                    "url": "http://msml-conf.org/"
                },
                "url": "https://www.semanticscholar.org/paper/66b2509feac026d77bc0d2a3066c76918b77b9d5",
                "title": "Structure-preserving Sparse Identification of Nonlinear Dynamics for Data-driven Modeling",
                "abstract": "Discovery of dynamical systems from data forms the foundation for data-driven modeling and recently, structure-preserving geometric perspectives have been shown to provide improved forecasting, stability, and physical realizability guarantees. We present here a unification of the Sparse Identification of Nonlinear Dynamics (SINDy) formalism with neural ordinary differential equations. The resulting framework allows learning of both\"black-box\"dynamics and learning of structure preserving bracket formalisms for both reversible and irreversible dynamics. We present a suite of benchmarks demonstrating effectiveness and structure preservation, including for chaotic systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "38723595",
                        "name": "Nathaniel Trask"
                    },
                    {
                        "authorId": "153799305",
                        "name": "P. Stinis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Therefore, our future work may focus on building a stable version of ODE2VAE and increasing the interpretability of the latent representations through using arbitarary Lagrangians or Hamiltonians [10, 11], and learning disentangled latent representations with weak supervision [12]."
            ],
            "citingPaper": {
                "paperId": "a2bcbab742b25b5edfd90154aa38935e0efb1906",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-04899",
                    "ArXiv": "2108.04899",
                    "CorpusId": 236976125
                },
                "corpusId": 236976125,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a2bcbab742b25b5edfd90154aa38935e0efb1906",
                "title": "Analysis of ODE2VAE with Examples",
                "abstract": "Deep generative models aim to learn underlying distributions that generate the observed data. Given the fact that the generative distribution may be complex and intractable, deep latent variable models use probabilistic frameworks to learn more expressive joint probability distributions over the data and their low-dimensional hidden variables. Learning complex probability distributions over sequential data without any supervision is a difficult task for deep generative models. Ordinary Differential Equation Variational Auto-Encoder (ODE2VAE) is a deep latent variable model that aims to learn complex distributions over high-dimensional sequential data and their low-dimensional representations. ODE2VAE infers continuous latent dynamics of the high-dimensional input in a low-dimensional hierarchical latent space. The hierarchical organization of the continuous latent space embeds a physics-guided inductive bias in the model. In this paper, we analyze the latent representations inferred by the ODE2VAE model over three different physical motion datasets: bouncing balls, projectile motion, and simple pendulum. Through our experiments, we explore the effects of the physics-guided inductive bias of the ODE2VAE model over the learned dynamical latent representations. We show that the model is able to learn meaningful latent representations to an extent without any supervision.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2000083157",
                        "name": "Batuhan Koyuncu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "78f17d91c0d2df45d778bd8484f1260e5a2f61c5",
                "externalIds": {
                    "ArXiv": "2108.01590",
                    "DOI": "10.1103/physrevresearch.4.l042005",
                    "CorpusId": 236881209
                },
                "corpusId": 236881209,
                "publicationVenue": {
                    "id": "349f119f-f4ee-48cf-aedb-89bcb56ab8e3",
                    "name": "Physical Review Research",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Rev Res"
                    ],
                    "issn": "2643-1564",
                    "url": "https://journals.aps.org/prresearch"
                },
                "url": "https://www.semanticscholar.org/paper/78f17d91c0d2df45d778bd8484f1260e5a2f61c5",
                "title": "Temperature steerable flows and Boltzmann generators",
                "abstract": "Boltzmann generators approach the sampling problem in many-body physics by combining a normalizing flow and a statistical reweighting method to generate samples in thermodynamic equilibrium. The equilibrium distribution is usually defined by an energy function and a thermodynamic state. Here we propose temperature-steerable flows (TSF) which are able to generate a family of probability densities parametrized by a choosable temperature parameter. TSFs can be embedded in generalized ensemble sampling frameworks to sample a physical system across multiple thermodynamic states.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "116366008",
                        "name": "Manuel Dibak"
                    },
                    {
                        "authorId": "1380082499",
                        "name": "Leon Klein"
                    },
                    {
                        "authorId": "2078973772",
                        "name": "Andreas Kr\u00e4mer"
                    },
                    {
                        "authorId": "2122593997",
                        "name": "Frank No\u00e9"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Concretely, it has been shown that physicallyinformed learning biases embedded in networks, such as Hamiltonian mechanics [6, 8], Lagrangians [9, 10], Ordinary Differential Equations (ODEs) [11], physicsinformed networks [12, 13], generative networks [14], and Graph Neural Networks [15, 16] can significantly improve learning and generalization over vanilla neural networks in complex physical domains."
            ],
            "citingPaper": {
                "paperId": "ea8b902ca4c33236435dacdf30cea0b964c9c1eb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-08024",
                    "ArXiv": "2107.08024",
                    "DOI": "10.1103/PhysRevE.104.034312",
                    "CorpusId": 236034262,
                    "PubMed": "34654178"
                },
                "corpusId": 236034262,
                "publicationVenue": {
                    "id": "19842b7b-a4d1-4f9a-9714-87d878cf6e73",
                    "name": "Physical Review E",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev E",
                        "Physical review. E",
                        "Phys Rev E"
                    ],
                    "issn": "1539-3755",
                    "alternate_issns": [
                        "1550-2376",
                        "2470-0045"
                    ],
                    "url": "https://journals.aps.org/pre/",
                    "alternate_urls": [
                        "http://pre.aps.org/",
                        "http://journals.aps.org/pre/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ea8b902ca4c33236435dacdf30cea0b964c9c1eb",
                "title": "Port-Hamiltonian Neural Networks for Learning Explicit Time-Dependent Dynamical Systems",
                "abstract": "Accurately learning the temporal behavior of dynamical systems requires models with well-chosen learning biases. Recent innovations embed the Hamiltonian and Lagrangian formalisms into neural networks and demonstrate a significant improvement over other approaches in predicting trajectories of physical systems. These methods generally tackle autonomous systems that depend implicitly on time or systems for which a control signal is known a priori. Despite this success, many real world dynamical systems are nonautonomous, driven by time-dependent forces and experience energy dissipation. In this study, we address the challenge of learning from such nonautonomous systems by embedding the port-Hamiltonian formalism into neural networks, a versatile framework that can capture energy dissipation and time-dependent control forces. We show that the proposed port-Hamiltonian neural network can efficiently learn the dynamics of nonlinear physical systems of practical interest and accurately recover the underlying stationary Hamiltonian, time-dependent force, and dissipative coefficient. A promising outcome of our network is its ability to learn and predict chaotic systems such as the Duffing equation, for which the trajectories are typically hard to learn.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144446378",
                        "name": "Shaan Desai"
                    },
                    {
                        "authorId": "145324933",
                        "name": "M. Mattheakis"
                    },
                    {
                        "authorId": "144633639",
                        "name": "David Sondak"
                    },
                    {
                        "authorId": "1735677",
                        "name": "P. Protopapas"
                    },
                    {
                        "authorId": "143841496",
                        "name": "Stephen J. Roberts"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "While there is a plethora of works devoted to learning conservative phenomena, based upon Hamiltonian or Lagrangian descriptions (see, among others, [9], [56], [57], [58], [59], [60]), very little has been investigated for learning dissipative phenomena."
            ],
            "citingPaper": {
                "paperId": "cfc5695b9d8c0793b7bd767897ab8f5048f01e06",
                "externalIds": {
                    "DBLP": "journals/pami/MoyaBGCC23",
                    "ArXiv": "2106.13301",
                    "DOI": "10.1109/TPAMI.2022.3160100",
                    "CorpusId": 235652364,
                    "PubMed": "35316181"
                },
                "corpusId": 235652364,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cfc5695b9d8c0793b7bd767897ab8f5048f01e06",
                "title": "Physics Perception in Sloshing Scenes With Guaranteed Thermodynamic Consistency",
                "abstract": "Physics perception very often faces the problem that only limited data or partial measurements on the scene are available. In this work, we propose a strategy to learn the full state of sloshing liquids from measurements of the free surface. Our approach is based on recurrent neural networks (RNN) that project the limited information available to a reduced-order manifold to not only reconstruct the unknown information but also be capable of performing fluid reasoning about future scenarios in real-time. To obtain physically consistent predictions, we train deep neural networks on the reduced-order manifold that, through the employ of inductive biases, ensure the fulfillment of the principles of thermodynamics. RNNs learn from history the required hidden information to correlate the limited information with the latent space where the simulation occurs. Finally, a decoder returns data to the high-dimensional manifold, to provide the user with insightful information in the form of augmented reality. This algorithm is connected to a computer vision system to test the performance of the proposed methodology with real information, resulting in a system capable of understanding and predicting future states of the observed fluid in real-time.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "69035385",
                        "name": "B. Moya"
                    },
                    {
                        "authorId": "51181453",
                        "name": "A. Badias"
                    },
                    {
                        "authorId": "47723344",
                        "name": "D. Gonz\u00e1lez"
                    },
                    {
                        "authorId": "2734584",
                        "name": "F. Chinesta"
                    },
                    {
                        "authorId": "3246447",
                        "name": "E. Cueto"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[37] show that, instead of from state trajectories, the Hamiltonian function can be learned from high-dimensional image observations."
            ],
            "citingPaper": {
                "paperId": "5873f8ebd98fc77c8f7861b7e10464cc4c45bc71",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-12782",
                    "ArXiv": "2106.12782",
                    "DOI": "10.15607/RSS.2021.XVII.086",
                    "CorpusId": 235623870
                },
                "corpusId": 235623870,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5873f8ebd98fc77c8f7861b7e10464cc4c45bc71",
                "title": "Hamiltonian-based Neural ODE Networks on the SE(3) Manifold For Dynamics Learning and Control",
                "abstract": "Accurate models of robot dynamics are critical for safe and stable control and generalization to novel operational conditions. Hand-designed models, however, may be insufficiently accurate, even after careful parameter tuning. This motivates the use of machine learning techniques to approximate the robot dynamics over a training set of state-control trajectories. The dynamics of many robots, including ground, aerial, and underwater vehicles, are described in terms of their SE(3) pose and generalized velocity, and satisfy conservation of energy principles. This paper proposes a Hamiltonian formulation over the SE(3) manifold of the structure of a neural ordinary differential equation (ODE) network to approximate the dynamics of a rigid body. In contrast to a black-box ODE network, our formulation guarantees total energy conservation by construction. We develop energy shaping and damping injection control for the learned, potentially under-actuated SE(3) Hamiltonian dynamics to enable a unified approach for stabilization and trajectory tracking with various platforms, including pendulum, rigid-body, and quadrotor systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2344318",
                        "name": "T. Duong"
                    },
                    {
                        "authorId": "50365495",
                        "name": "Nikolay A. Atanasov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Structure preserving neural networks A thorough accounting of works embedding structurepreservation into neural networks include pioneering works for Hamiltonian neural networks [4, 40], followed by development of Lagragian neural networks [41, 5] and neural networks that mimic the action of symplectic integrators [6, 7, 8]."
            ],
            "citingPaper": {
                "paperId": "2a2dd0fa4864d34874e7b1e2e39564c3147cd71e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-12619",
                    "ArXiv": "2106.12619",
                    "CorpusId": 235623998
                },
                "corpusId": 235623998,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2a2dd0fa4864d34874e7b1e2e39564c3147cd71e",
                "title": "Machine learning structure preserving brackets for forecasting irreversible processes",
                "abstract": "Forecasting of time-series data requires imposition of inductive biases to obtain predictive extrapolation, and recent works have imposed Hamiltonian/Lagrangian form to preserve structure for systems with reversible dynamics. In this work we present a novel parameterization of dissipative brackets from metriplectic dynamical systems appropriate for learning irreversible dynamics with unknown a priori model form. The process learns generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. Furthermore, for the case of added thermal noise, we guarantee exact preservation of a fluctuation-dissipation theorem, ensuring thermodynamic consistency. We provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either\"black-box\"or penalty-based approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "38723595",
                        "name": "Nathaniel Trask"
                    },
                    {
                        "authorId": "153799305",
                        "name": "P. Stinis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To name a few, generative [28], recurrent [9] and constrained [33] versions, as well as Lagrangian Neural Networks [10] have been proposed."
            ],
            "citingPaper": {
                "paperId": "b02302d7b2e8226c9dbf161f3568b2b5a737c2f9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-11753",
                    "ArXiv": "2106.11753",
                    "DOI": "10.1016/j.jcp.2023.112495",
                    "CorpusId": 235593084
                },
                "corpusId": 235593084,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b02302d7b2e8226c9dbf161f3568b2b5a737c2f9",
                "title": "Symplectic Learning for Hamiltonian Neural Networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118633631",
                        "name": "M. David"
                    },
                    {
                        "authorId": "2074471152",
                        "name": "Florian M'ehats"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Another interesting direction to pursue would be to apply novel physics-informed neural network architectures[63, 64] to resolve Hamiltonians of systems with many degrees of freedom (such as molecules) using time-resolved HHG spectra, such as those obtained from solids driven by mid-IR fields [24]."
            ],
            "citingPaper": {
                "paperId": "5626e4474e53a39a6adfe24e17bb5038f7299daf",
                "externalIds": {
                    "ArXiv": "2106.08638",
                    "CorpusId": 263794401
                },
                "corpusId": 263794401,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5626e4474e53a39a6adfe24e17bb5038f7299daf",
                "title": "Deep neural networks for high harmonic spectroscopy in solids",
                "abstract": "Neural networks are a prominent tool for identifying and modeling complex patterns, which are otherwise hard to detect and analyze. While machine learning and neural networks have been finding applications across many areas of science and technology, their use in decoding ultrafast dynamics of quantum systems driven by strong laser fields has been limited so far. Here we use deep neural networks to analyze simulated noisy spectra of highly nonlinear optical response of a 2-dimensional gapped graphene crystal to intense few-cycle laser pulses. We show that a computationally simple 1-dimensional system provides a useful\"nursery school\"for our neural network, allowing it to be easily retrained to treat more complex systems, recovering the band structure and spectral phases of the incident few-cycle pulse with high accuracy, in spite of significant amplitude noise and phase jitter. Our results both offer a new tool for attosecond spectroscopy of quantum dynamics in solids and also open a route to developing all-solid-state devices for complete characterization of few-cycle pulses, including their nonlinear chirp and the carrier envelope phase.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "102617464",
                        "name": "N. Klimkin"
                    },
                    {
                        "authorId": "1388345850",
                        "name": "'Alvaro Jim'enez-Gal'an"
                    },
                    {
                        "authorId": "2252994417",
                        "name": "Rui E. F. Silva"
                    },
                    {
                        "authorId": "2256719260",
                        "name": "Misha Ivanov"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bb09f46438236f6e916d2ad23eca92330a41b9fb",
                "externalIds": {
                    "ArXiv": "2106.00026",
                    "DBLP": "journals/corr/abs-2106-00026",
                    "DOI": "10.1103/PhysRevE.104.055302",
                    "CorpusId": 235266182,
                    "PubMed": "34942731"
                },
                "corpusId": 235266182,
                "publicationVenue": {
                    "id": "19842b7b-a4d1-4f9a-9714-87d878cf6e73",
                    "name": "Physical Review E",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev E",
                        "Physical review. E",
                        "Phys Rev E"
                    ],
                    "issn": "1539-3755",
                    "alternate_issns": [
                        "1550-2376",
                        "2470-0045"
                    ],
                    "url": "https://journals.aps.org/pre/",
                    "alternate_urls": [
                        "http://pre.aps.org/",
                        "http://journals.aps.org/pre/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bb09f46438236f6e916d2ad23eca92330a41b9fb",
                "title": "Machine-Learning Non-Conservative Dynamics for New-Physics Detection",
                "abstract": "Energy conservation is a basic physics principle, the breakdown of which often implies new physics. This paper presents a method for data-driven \"new physics\" discovery. Specifically, given a trajectory governed by unknown forces, our neural new-physics detector (NNPhD) aims to detect new physics by decomposing the force field into conservative and nonconservative components, which are represented by a Lagrangian neural network (LNN) and an unconstrained neural network, respectively, trained to minimize the force recovery error plus a constant \u03bb times the magnitude of the predicted nonconservative force. We show that a phase transition occurs at \u03bb=1, universally for arbitrary forces. We demonstrate that NNPhD successfully discovers new physics in toy numerical experiments, rediscovering friction (1493) from a damped double pendulum, Neptune from Uranus' orbit (1846), and gravitational waves (2017) from an inspiraling orbit. We also show how NNPhD coupled with an integrator outperforms both an LNN and an unconstrained neural network for predicting the future of a damped double pendulum.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145253202",
                        "name": "Ziming Liu"
                    },
                    {
                        "authorId": "1390816671",
                        "name": "Bohan Wang"
                    },
                    {
                        "authorId": "47580728",
                        "name": "Qi Meng"
                    },
                    {
                        "authorId": "2154939268",
                        "name": "Wei Chen"
                    },
                    {
                        "authorId": "49944508",
                        "name": "M. Tegmark"
                    },
                    {
                        "authorId": "2110264337",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "36f494c27103b5870e43495f33a76861a09f7ad1",
                "externalIds": {
                    "DBLP": "conf/icra/YangSS21",
                    "DOI": "10.1109/ICRA48506.2021.9561636",
                    "CorpusId": 239039430
                },
                "corpusId": 239039430,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/36f494c27103b5870e43495f33a76861a09f7ad1",
                "title": "Learning to Propagate Interaction Effects for Modeling Deformable Linear Objects Dynamics",
                "abstract": "Modeling dynamics of deformable linear objects (DLOs), such as cables, hoses, sutures, and catheters, is an important and challenging problem for many robotic manipulation applications. In this paper, we propose the first method to model and learn full 3D dynamics of DLOs from data. Our approach is capable of capturing the complex twisting and bending dynamics of DLOs and allows local effects to propagate globally. To this end, we adapt the interaction network (IN) dynamics learning method for capturing the interaction between neighboring segments in a DLO and augment it with a recurrent model for propagating interaction effects along the length of a DLO. For learning twisting and bending dynamics in 3D, we also introduce a new suitable representation of DLO segments and their relationships. Unlike the original IN method, our model learns to propagate the effects of local interaction between neighboring segments to each segment in the chain within a single time step, without the need for iterated propagation steps. Evaluation of our model with synthetic and newly collected real-world data shows better accuracy and generalization in short-term and long- term predictions than the current state of the art. We further integrate our learned model in a model predictive control scheme and use it to successfully control the shape of a DLO. Our implementation is available at https://gitsvn-nt.oru.se/ammlab\u2013public/in\u2013bilstm.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2144145722",
                        "name": "Yuxuan Yang"
                    },
                    {
                        "authorId": "3128110",
                        "name": "J. A. Stork"
                    },
                    {
                        "authorId": "144504646",
                        "name": "Todor Stoyanov"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In the application of neural networks to model physical systems, several authors have also constructed equivariant (or invariant) models by incorporating equations of motion - in either the Hamiltonian or Lagrangian formulation of classical mechanics - to accommodate the learning of system dynamics and conservation laws [71, 72, 73]."
            ],
            "citingPaper": {
                "paperId": "f62f8e9501302a57a5656b01b9d45e4c7463d48f",
                "externalIds": {
                    "ArXiv": "2105.13926",
                    "DBLP": "journals/corr/abs-2105-13926",
                    "DOI": "10.1007/s10462-023-10502-7",
                    "CorpusId": 235248075
                },
                "corpusId": 235248075,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/f62f8e9501302a57a5656b01b9d45e4c7463d48f",
                "title": "Geometric deep learning and equivariant neural networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3315379",
                        "name": "Jan E. Gerken"
                    },
                    {
                        "authorId": "104264587",
                        "name": "J. Aronsson"
                    },
                    {
                        "authorId": "2106386126",
                        "name": "Oscar Carlsson"
                    },
                    {
                        "authorId": "3311829",
                        "name": "H. Linander"
                    },
                    {
                        "authorId": "3288234",
                        "name": "F. Ohlsson"
                    },
                    {
                        "authorId": "6387775",
                        "name": "Christoffer Petersson"
                    },
                    {
                        "authorId": "2360323",
                        "name": "D. Persson"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026connect with work on inferring dynamics with neural networks such as (Battaglia et al., 2016) in the same way as HNNs. Natural extensions of our current work can include the application on graph neural network based approaches (Sanchez-Gonzalez et al., 2019) and in flows (Toth et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "190a96980e99b398fce18bb7fa78733134d53547",
                "externalIds": {
                    "MAG": "3159022575",
                    "ArXiv": "2104.14444",
                    "DBLP": "journals/corr/abs-2104-14444",
                    "CorpusId": 233444119
                },
                "corpusId": 233444119,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/190a96980e99b398fce18bb7fa78733134d53547",
                "title": "Improving Simulations with Symmetry Control Neural Networks",
                "abstract": "The dynamics of physical systems is often constrained to lower dimensional sub-spaces due to the presence of conserved quantities. Here we propose a method to learn and exploit such symmetry constraints building upon Hamiltonian Neural Networks. By enforcing cyclic coordinates with appropriate loss functions, we find that we can achieve improved accuracy on simple classical dynamics tasks. By fitting analytic formulae to the latent variables in our network we recover that our networks are utilizing conserved quantities such as (angular) momentum.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "147611823",
                        "name": "Marc Syvaeri"
                    },
                    {
                        "authorId": "51264706",
                        "name": "S. Krippendorf"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Weak form learning of GHNNs While derivative regression [5, 15\u201317, 20] and state regression [14, 18, 21] are well-known in the deep learning literature, learning ODEs from the weak form of the governing equations has only been used in the context of sparse basis function regression as far as the authors are aware [8, 29].",
                "[21] independently developed methods for learning a Hamiltonian decomposition of an ODE.",
                "Making use of such priors allows models in this class to exhibit desirable properties by construction, such as being strictly Hamiltonian [16, 17, 21] or globally stable [15]."
            ],
            "citingPaper": {
                "paperId": "f362c3c09405ba6eb0e4a79c72279d33c369cbaa",
                "externalIds": {
                    "ArXiv": "2104.05096",
                    "DBLP": "conf/nips/CourseEN20",
                    "MAG": "3105935177",
                    "CorpusId": 227275459
                },
                "corpusId": 227275459,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f362c3c09405ba6eb0e4a79c72279d33c369cbaa",
                "title": "Weak Form Generalized Hamiltonian Learning",
                "abstract": "We present a method for learning generalized Hamiltonian decompositions of ordinary differential equations given a set of noisy time series measurements. Our method simultaneously learns a continuous time model and a scalar energy function for a general dynamical system. Learning predictive models in this form allows one to place strong, high-level, physics inspired priors onto the form of the learnt governing equations for general dynamical systems. Moreover, having shown how our method extends and unifies some previous work in deep learning with physics inspired priors, we present a novel method for learning continuous time models from the weak form of the governing equations which is less computationally taxing than standard adjoint methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2031451281",
                        "name": "Kevin Course"
                    },
                    {
                        "authorId": "41075246",
                        "name": "T. W. Evans"
                    },
                    {
                        "authorId": "144475330",
                        "name": "P. Nair"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent work on physics-based deep learning injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly conserving physical quantities, or with ground truth supervision (Asenov et al.",
                "\u2026injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al.,\n2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly conserving physical quantities, or with\u2026",
                "\u2026studied in the context of differentiable physics simulation Degrave et al. (2016); de Avila Belbute-Peres et al. (2018); Cranmer et al. (2020b); Toth et al. (2020); Greydanus et al. (2019); Sanchez-Gonzalez et al. (2019), we focus on more challenging systems which have not been studied in\u2026"
            ],
            "citingPaper": {
                "paperId": "92c89db048fb825d2c81b086d7bd82ed230f685b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-02646",
                    "ArXiv": "2104.02646",
                    "CorpusId": 233033848
                },
                "corpusId": 233033848,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/92c89db048fb825d2c81b086d7bd82ed230f685b",
                "title": "gradSim: Differentiable simulation for system identification and visuomotor control",
                "abstract": "We consider the problem of estimating an object's physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current solutions require precise 3D labels which are labor-intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. We present gradSim, a framework that overcomes the dependence on 3D supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Moreover, our unified computation graph -- spanning from the dynamics and through the rendering process -- enables learning in challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to or better than techniques that rely on precise 3D labels.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7434636",
                        "name": "Krishna Murthy Jatavallabhula"
                    },
                    {
                        "authorId": "46637939",
                        "name": "M. Macklin"
                    },
                    {
                        "authorId": "2066832277",
                        "name": "Florian Golemo"
                    },
                    {
                        "authorId": "2961618",
                        "name": "Vikram S. Voleti"
                    },
                    {
                        "authorId": "1471743441",
                        "name": "Linda Petrini"
                    },
                    {
                        "authorId": "144069571",
                        "name": "Martin Weiss"
                    },
                    {
                        "authorId": "41226293",
                        "name": "Breandan Considine"
                    },
                    {
                        "authorId": "1585278372",
                        "name": "J\u00e9r\u00f4me Parent-L\u00e9vesque"
                    },
                    {
                        "authorId": "47966782",
                        "name": "Kevin Xie"
                    },
                    {
                        "authorId": "2253410",
                        "name": "Kenny Erleben"
                    },
                    {
                        "authorId": "3198259",
                        "name": "L. Paull"
                    },
                    {
                        "authorId": "2162768",
                        "name": "F. Shkurti"
                    },
                    {
                        "authorId": "1795014",
                        "name": "D. Nowrouzezahrai"
                    },
                    {
                        "authorId": "37895334",
                        "name": "S. Fidler"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Many recent methods have used the Lagrangian formulation [14], [15] and also the closely related Hamiltonian formulation [16], [17], as physics-based priors for learning dynamics models."
            ],
            "citingPaper": {
                "paperId": "d5bda58520854f7f12f1ea2fe3a3c2898bec510c",
                "externalIds": {
                    "MAG": "3144527084",
                    "DBLP": "journals/corr/abs-2103-16432",
                    "CorpusId": 232417344
                },
                "corpusId": 232417344,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d5bda58520854f7f12f1ea2fe3a3c2898bec510c",
                "title": "Learning Deep Neural Policies with Stability Guarantees",
                "abstract": "Reinforcement learning (RL) has been successfully used to solve various robotic control tasks. However, most of the existing works do not address the issue of control stability. This is in sharp contrast to the control theory community where the well-established norm is to prove stability whenever a control law is synthesized. What makes guaranteeing stability during RL difficult is threefold: non interpretable neural network policies, unknown system dynamics and random exploration. We contribute towards solving the stable RL problem in the context of robotic manipulation that may involve physical contact with the environment. Our solution is derived from physics-based prior that originates from Lagrangian mechanics and does not involve learning any dynamics model. We show how to parameterize the resulting $\\textit{energy shaping}$ policy as a deep neural network that consists of a convex potential function and a velocity dependent damping component. Our experiments, that include a real-world peg insertion task by a 7-DOF robot, validate the proposed policy structure and demonstrate the benefits of stability in RL.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2106182626",
                        "name": "S. A. Khader"
                    },
                    {
                        "authorId": "145039722",
                        "name": "Hang Yin"
                    },
                    {
                        "authorId": "144407954",
                        "name": "P. Falco"
                    },
                    {
                        "authorId": "1731490",
                        "name": "D. Kragic"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Many recent methods have used the Lagrangian formulation [14], [15] and also the closely related Hamiltonian formulation [16], [17], as physics-based priors for learning dynamics models."
            ],
            "citingPaper": {
                "paperId": "e8734774665fb2f7e1d4b84c4c2b4d0935a78f66",
                "externalIds": {
                    "ArXiv": "2103.16432",
                    "DBLP": "journals/ral/KhaderYFK21a",
                    "DOI": "10.1109/LRA.2021.3111962",
                    "CorpusId": 237940619
                },
                "corpusId": 237940619,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e8734774665fb2f7e1d4b84c4c2b4d0935a78f66",
                "title": "Learning Deep Energy Shaping Policies for Stability-Guaranteed Manipulation",
                "abstract": "Deep reinforcement learning (DRL) has been successfully used to solve various robotic manipulation tasks. However, most of the existing works do not address the issue of control stability. This is in sharp contrast to the control theory community where the well-established norm is to prove stability whenever a control law is synthesized. What makes traditional stability analysis difficult for DRL are the uninterpretable nature of the neural network policies and unknown system dynamics. In this work, stability is obtained by deriving an interpretable deep policy structure based on the energy shaping control of Lagrangian systems. Then, stability during physical interaction with an unknown environment is established based on passivity. The result is a stability guaranteeing DRL in a model-free framework that is general enough for contact-rich manipulation tasks. With an experiment on a peg-in-hole task, we demonstrate, to the best of our knowledge, the first DRL with stability guarantee on a real robotic manipulator.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2106182626",
                        "name": "S. A. Khader"
                    },
                    {
                        "authorId": "145039722",
                        "name": "Hang Yin"
                    },
                    {
                        "authorId": "144407954",
                        "name": "P. Falco"
                    },
                    {
                        "authorId": "1731490",
                        "name": "D. Kragic"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, the network may incorporate inductive bias corresponding to the physical laws of interaction and motion [25, 6, 107, 16, 92, 32]."
            ],
            "citingPaper": {
                "paperId": "390aa4fdc4072479ceaa638b1fe586d056feeb98",
                "externalIds": {
                    "ArXiv": "2103.07292",
                    "DBLP": "journals/corr/abs-2103-07292",
                    "DOI": "10.1109/CVPR46437.2021.00808",
                    "CorpusId": 232222878
                },
                "corpusId": 232222878,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/390aa4fdc4072479ceaa638b1fe586d056feeb98",
                "title": "VDSM: Unsupervised Video Disentanglement with State-Space Modeling and Deep Mixtures of Experts",
                "abstract": "Disentangled representations support a range of downstream tasks including causal reasoning, generative modeling, and fair machine learning. Unfortunately, disentanglement has been shown to be impossible without the incorporation of supervision or inductive bias. Given that supervision is often expensive or infeasible to acquire, we choose to incorporate structural inductive bias and present an unsupervised, deep State-Space-Model for Video Disentanglement (VDSM). The model disentangles latent time-varying and dynamic factors via the incorporation of hierarchical structure with a dynamic prior and a Mixture of Experts decoder. VDSM learns separate disentangled representations for the identity of the object or person in the video, and for the action being performed. We evaluate VDSM across a range of qualitative and quantitative tasks including identity and dynamics transfer, sequence generation, Fr\u00e9chet Inception Distance, and factor classification. VDSM achieves state-of-the-art performance and exceeds adversarial methods, even when the methods use additional supervision.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51468109",
                        "name": "M. Vowels"
                    },
                    {
                        "authorId": "40163061",
                        "name": "N. C. Camgoz"
                    },
                    {
                        "authorId": "145398628",
                        "name": "R. Bowden"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026et al., 2019) and an independent work (Bertalan et al., 2019), SRNN (Chen et al., 2020), SympNets (Jin et al., 2020), and (Lutter et al., 2019; Toth et al., 2020; Zhong et al., 2020; Wu et al., 2020; Xiong et al., 2021), all of which, except SympNets, are related to learning some quantity\u2026"
            ],
            "citingPaper": {
                "paperId": "75720b44f3aedeea3e5017c58d60a1ee80dba2ec",
                "externalIds": {
                    "DBLP": "conf/icml/ChenT21",
                    "ArXiv": "2103.05632",
                    "CorpusId": 232168400
                },
                "corpusId": 232168400,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/75720b44f3aedeea3e5017c58d60a1ee80dba2ec",
                "title": "Data-driven Prediction of General Hamiltonian Dynamics via Learning Exactly-Symplectic Maps",
                "abstract": "We consider the learning and prediction of nonlinear time series generated by a latent symplectic map. A special case is (not necessarily separable) Hamiltonian systems, whose solution flows give such symplectic maps. For this special case, both generic approaches based on learning the vector field of the latent ODE and specialized approaches based on learning the Hamiltonian that generates the vector field exist. Our method, however, is different as it does not rely on the vector field nor assume its existence; instead, it directly learns the symplectic evolution map in discrete time. Moreover, we do so by representing the symplectic map via a generating function, which we approximate by a neural network (hence the name GFNN). This way, our approximation of the evolution map is always \\emph{exactly} symplectic. This additional geometric structure allows the local prediction error at each step to accumulate in a controlled fashion, and we will prove, under reasonable assumptions, that the global prediction error grows at most \\emph{linearly} with long prediction time, which significantly improves an otherwise exponential growth. In addition, as a map-based and thus purely data-driven method, GFNN avoids two additional sources of inaccuracies common in vector-field based approaches, namely the error in approximating the vector field by finite difference of the data, and the error in numerical integration of the vector field for making predictions. Numerical experiments further demonstrate our claims.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2569809",
                        "name": "Ren-Chuen Chen"
                    },
                    {
                        "authorId": "46699279",
                        "name": "Molei Tao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "(53)\nThe semiclassical neural network is a type of learnable Hamiltonian flow (Bondesan & Lamacraft, 2019; Rezende et al., 2019; Toth et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "ac08fcf6a22b58a844ceb956f77effea6e0d0f46",
                "externalIds": {
                    "DBLP": "conf/icml/BondesanW21",
                    "ArXiv": "2103.04913",
                    "CorpusId": 232147869
                },
                "corpusId": 232147869,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ac08fcf6a22b58a844ceb956f77effea6e0d0f46",
                "title": "The Hintons in your Neural Network: a Quantum Field Theory View of Deep Learning",
                "abstract": "In this work we develop a quantum field theory formalism for deep learning, where input signals are encoded in Gaussian states, a generalization of Gaussian processes which encode the agent's uncertainty about the input signal. We show how to represent linear and non-linear layers as unitary quantum gates, and interpret the fundamental excitations of the quantum model as particles, dubbed ``Hintons''. On top of opening a new perspective and techniques for studying neural networks, the quantum formulation is well suited for optical quantum computing, and provides quantum deformations of neural networks that can be run efficiently on those devices. Finally, we discuss a semi-classical limit of the quantum deformed models which is amenable to classical simulation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "6715047",
                        "name": "R. Bondesan"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "We did not choose a specific model but let fP be a trainable Hamilton\u2019s equation as in [39, 11].",
                "It may happen when the semantics of zP are not necessarily grounded, for example, when fP is a trainable Hamilton\u2019s equation like in Toth et al. (2020).",
                "Toth et al. (2020) propose Hamiltonian generative networks, where a sequence of the latent variable is governed by the Hamiltonian mechanics with a learned Hamiltonian, the encoder infers the initial condition of the sequence, and the learned decoder works as an observation function.",
                "We did not choose a specific model but let fP be a trainable Hamilton\u2019s equation as in Toth et al. (2020); Greydanus et al. (2019):\nfP\n([ pT qT ]T) = [ \u2212\u2202H\u2202q T \u2202H \u2202p T ]T , (24)\nwhere p \u2208 Rdy is a generalized position, q \u2208 Rdy is a generalized momentum, andH : Rdy \u00d7Rdy \u2192 R is a Hamiltonian.",
                "While direct comparison is impossible due to the differences of the problem settings, the baseline methods we examined (listed below) are similar to some existing methods [4, 46, 39, 20, 47].",
                "Experiments on Human Locomotion\nPhysics model We modeled fP with a trainable Hamilton\u2019s equation as in Toth et al. (2020); Greydanus et al. (2019):\nfP\n([ pT qT ]T , zP ) = [ \u2212\u2202H\u2202q T \u2202H \u2202p T ]T , (26)\nwhere p \u2208 Rdy is a generalized position, q \u2208 Rdy is a generalized momentum, andH : Rdy \u00d7Rdy \u2192 R is\u2026",
                "[39] propose a model where the latent variable sequence is governed by the Hamiltonian mechanics with a neural Hamiltonian.",
                ", when fP is a neural Hamilton\u2019s equation [39].",
                "Capability of learning such an observation function has already been investigated (e.g., Toth et al., 2020).",
                "\u2026fB works as an observation function that changes signal\u2019s modality (Greydanus et al., 2019; Lutter et al., 2019; Y\u0131ld\u0131z et al., 2019; Linial et al., 2020; Toth et al., 2020; Cranmer et al., 2020; Saemundsson et al., 2020); and\n\u2022 fB works as a learnable ensemble of physics models (Sengupta et\u2026"
            ],
            "citingPaper": {
                "paperId": "180d0ba8d544ca0f3c8646f3fcf8ecaeed308226",
                "externalIds": {
                    "DBLP": "conf/nips/TakeishiK21",
                    "ArXiv": "2102.13156",
                    "CorpusId": 232068635
                },
                "corpusId": 232068635,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/180d0ba8d544ca0f3c8646f3fcf8ecaeed308226",
                "title": "Physics-Integrated Variational Autoencoders for Robust and Interpretable Generative Modeling",
                "abstract": "Integrating physics models within machine learning models holds considerable promise toward learning robust models with improved interpretability and abilities to extrapolate. In this work, we focus on the integration of incomplete physics models into deep generative models. In particular, we introduce an architecture of variational autoencoders (VAEs) in which a part of the latent space is grounded by physics. A key technical challenge is to strike a balance between the incomplete physics and trainable components such as neural networks for ensuring that the physics part is used in a meaningful manner. To this end, we propose a regularized learning method that controls the effect of the trainable components and preserves the semantics of the physics-based latent variables as intended. We not only demonstrate generative performance improvements over a set of synthetic and real-world datasets, but we also show that we learn robust models that can consistently extrapolate beyond the training distribution in a meaningful manner. Moreover, we show that we can control the generative process in an interpretable manner.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2024905",
                        "name": "Naoya Takeishi"
                    },
                    {
                        "authorId": "1784711",
                        "name": "Alexandros Kalousis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0a890542adca2a1e7a2a59862e0446649443b674",
                "externalIds": {
                    "ArXiv": "2102.13235",
                    "DBLP": "journals/corr/abs-2102-13235",
                    "DOI": "10.1103/PhysRevResearch.3.023156",
                    "CorpusId": 232069001
                },
                "corpusId": 232069001,
                "publicationVenue": {
                    "id": "349f119f-f4ee-48cf-aedb-89bcb56ab8e3",
                    "name": "Physical Review Research",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Rev Res"
                    ],
                    "issn": "2643-1564",
                    "url": "https://journals.aps.org/prresearch"
                },
                "url": "https://www.semanticscholar.org/paper/0a890542adca2a1e7a2a59862e0446649443b674",
                "title": "Adaptable Hamiltonian neural networks",
                "abstract": "The rapid growth of research in exploiting machine learning to predict chaotic systems has revived a recent interest in Hamiltonian Neural Networks (HNNs) with physical constraints defined by the Hamilton's equations of motion, which represent a major class of physics-enhanced neural networks. We introduce a class of HNNs capable of adaptable prediction of nonlinear physical systems: by training the neural network based on time series from a small number of bifurcation-parameter values of the target Hamiltonian system, the HNN can predict the dynamical states at other parameter values, where the network has not been exposed to any information about the system at these parameter values. The architecture of the HNN differs from the previous ones in that we incorporate an input parameter channel, rendering the HNN parameter--cognizant. We demonstrate, using paradigmatic Hamiltonian systems, that training the HNN using time series from as few as four parameter values bestows the neural machine with the ability to predict the state of the target system in an entire parameter interval. Utilizing the ensemble maximum Lyapunov exponent and the alignment index as indicators, we show that our parameter-cognizant HNN can successfully predict the route of transition to chaos. Physics-enhanced machine learning is a forefront area of research, and our adaptable HNNs provide an approach to understanding machine learning with broad applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "93112518",
                        "name": "Chen-Di Han"
                    },
                    {
                        "authorId": "5899699",
                        "name": "Bryan Glaz"
                    },
                    {
                        "authorId": "39004674",
                        "name": "M. Haile"
                    },
                    {
                        "authorId": "144769611",
                        "name": "Y. Lai"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "The HNNs have been developed through combining graph networks for learning interacting systems (Sanchez-Gonzalez et al., 2019) or generative networks for learning Hamiltonian from high-dimensional data (Toth et al., 2020).",
                ", 2019) or generative networks for learning Hamiltonian from high-dimensional data (Toth et al., 2020).",
                "many useful properties of the Hamiltonian (Toth et al., 2020; Chen et al., 2020; Zhong et al., 2020a; Sanchez-Gonzalez et al., 2019; Jin et al., 2020).",
                "HNN and its variants have been shown to be effective in learning\nmany useful properties of the Hamiltonian (Toth et al., 2020; Chen et al., 2020; Zhong et al., 2020a; Sanchez-Gonzalez et al., 2019; Jin et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "87174a4df7567e52771d15f949d459791145379b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-11544",
                    "ArXiv": "2102.11544",
                    "CorpusId": 232013963
                },
                "corpusId": 232013963,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/87174a4df7567e52771d15f949d459791145379b",
                "title": "Identifying Physical Law of Hamiltonian Systems via Meta-Learning",
                "abstract": "Hamiltonian mechanics is an effective tool to represent many physical processes with concise yet well-generalized mathematical expressions. A well-modeled Hamiltonian makes it easy for researchers to analyze and forecast many related phenomena that are governed by the same physical law. However, in general, identifying a functional or shared expression of the Hamiltonian is very difficult. It requires carefully designed experiments and the researcher's insight that comes from years of experience. We propose that meta-learning algorithms can be potentially powerful data-driven tools for identifying the physical law governing Hamiltonian systems without any mathematical assumptions on the representation, but with observations from a set of systems governed by the same physical law. We show that a well meta-trained learner can identify the shared representation of the Hamiltonian by evaluating our method on several types of physical systems with various experimental settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108154935",
                        "name": "Seungjun Lee"
                    },
                    {
                        "authorId": "11325568",
                        "name": "Haesang Yang"
                    },
                    {
                        "authorId": "2517631",
                        "name": "W. Seong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In Toth et al. (2019) HNNs were extended to latent variable models."
            ],
            "citingPaper": {
                "paperId": "966a110bea593cf5bd566e4a1f250f0e53cd77ed",
                "externalIds": {
                    "ArXiv": "2102.11923",
                    "DBLP": "conf/aaai/Chen0Y22",
                    "DOI": "10.1609/aaai.v36i6.20582",
                    "CorpusId": 247597094
                },
                "corpusId": 247597094,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/966a110bea593cf5bd566e4a1f250f0e53cd77ed",
                "title": "KAM Theory Meets Statistical Learning Theory: Hamiltonian Neural Networks with Non-zero Training Loss",
                "abstract": "Many physical phenomena are described by Hamiltonian mechanics using an energy function (Hamiltonian). Recently, the Hamiltonian neural network, which approximates the Hamiltonian by a neural network, and its extensions have attracted much attention. This is a very powerful method, but theoretical studies are limited. In this study, by combining the statistical learning theory and KAM theory, we provide a theoretical analysis of the behavior of Hamiltonian neural networks when the learning error is not completely zero. A Hamiltonian neural network with non-zero errors can be considered as a perturbation from the true dynamics, and the perturbation theory of the Hamilton equation is widely known as KAM theory. To apply KAM theory, we provide a generalization error bound for Hamiltonian neural networks by deriving an estimate of the covering number of the gradient of the multi-layer perceptron, which is the key ingredient of the model. This error bound gives a sup-norm bound on the Hamiltonian that is required in the application of KAM theory.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115869301",
                        "name": "Yu-Hsueh Chen"
                    },
                    {
                        "authorId": "144872058",
                        "name": "Takashi Matsubara"
                    },
                    {
                        "authorId": "2580628",
                        "name": "Takaharu Yaguchi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "One example involves the learning of invariant quantities via their Hamiltonian or Lagrangian representations [13, 14, 15, 16, 17].",
                "A particular interest was given to the automatic learning of equivariances in dynamical systems through their Hamiltonian [14, 15, 16, 17] or Lagrangian [13] formulations."
            ],
            "citingPaper": {
                "paperId": "90fb9cbdafb2c9e00c26464c9b9e25b0b739a635",
                "externalIds": {
                    "ArXiv": "2102.09589",
                    "DBLP": "journals/corr/abs-2102-09589",
                    "CorpusId": 231979467
                },
                "corpusId": 231979467,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/90fb9cbdafb2c9e00c26464c9b9e25b0b739a635",
                "title": "A Differential Geometry Perspective on Orthogonal Recurrent Models",
                "abstract": "Recently, orthogonal recurrent neural networks (RNNs) have emerged as state-of-the-art models for learning long-term dependencies. This class of models mitigates the exploding and vanishing gradients problem by design. In this work, we employ tools and insights from differential geometry to offer a novel perspective on orthogonal RNNs. We show that orthogonal RNNs may be viewed as optimizing in the space of divergence-free vector fields. Specifically, based on a well-known result in differential geometry that relates vector fields and linear operators, we prove that every divergence-free vector field is related to a skew-symmetric matrix. Motivated by this observation, we study a new recurrent model, which spans the entire space of vector fields. Our method parameterizes vector fields via the directional derivatives of scalar functions. This requires the construction of latent inner product, gradient, and divergence operators. In comparison to state-of-the-art orthogonal RNNs, our approach achieves comparable or better results on a variety of benchmark tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1927247",
                        "name": "Omri Azencot"
                    },
                    {
                        "authorId": "2371914",
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "authorId": "1398681470",
                        "name": "M. Ben-Chen"
                    },
                    {
                        "authorId": "143884206",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "As example applications, to solve the dynamics modeling problems, some works have introduced Hamiltonian dynamics (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019).",
                "In some works, in order to solve machine learning problems such as sequence prediction or reinforcement learning, neural networks attempt to learn a data symmetry of physical systems from noisy observations directly (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019; Sanchez-Gonzalez et al., 2019).",
                "\u2026in order to solve machine learning problems such as sequence prediction or reinforcement learning, neural networks attempt to learn a data symmetry of physical systems from noisy observations directly (Greydanus et al., 2019; Toth et al., 2019; Zhong et al., 2019; Sanchez-Gonzalez et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "bc716f31b93eac93f627e04039da7f6c4d126165",
                "externalIds": {
                    "ArXiv": "2102.08759",
                    "DBLP": "conf/iclr/KawanoKSIM21",
                    "CorpusId": 231942702
                },
                "corpusId": 231942702,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bc716f31b93eac93f627e04039da7f6c4d126165",
                "title": "Group Equivariant Conditional Neural Processes",
                "abstract": "We present the group equivariant conditional neural process (EquivCNP), a metalearning method with permutation invariance in a data set as in conventional conditional neural processes (CNPs), and it also has transformation equivariance in data space. Incorporating group equivariance, such as rotation and scaling equivariance, provides a way to consider the symmetry of real-world data. We give a decomposition theorem for permutation-invariant and group-equivariant maps, which leads us to construct EquivCNPs with an infinite-dimensional latent space to handle group symmetries. In this paper, we build architecture using Lie group convolutional layers for practical implementation. We show that EquivCNP with translation equivariance achieves comparable performance to conventional CNPs in a 1D regression task. Moreover, we demonstrate that incorporating an appropriate Lie group equivariance, EquivCNP is capable of zero-shot generalization for an image-completion task by selecting an appropriate Lie group equivariance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2105938934",
                        "name": "M. Kawano"
                    },
                    {
                        "authorId": "1896666",
                        "name": "Wataru Kumagai"
                    },
                    {
                        "authorId": "41152282",
                        "name": "Akiyoshi Sannai"
                    },
                    {
                        "authorId": "1715282",
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "authorId": "49484314",
                        "name": "Y. Matsuo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Other examples of dynamical systems inspired models include the learning of invariant quantities via their Hamiltonian or Lagrangian representations [37, 22, 10, 71, 58]."
            ],
            "citingPaper": {
                "paperId": "a747905cc60cde18a8c1fc1e365e319cea0ff49b",
                "externalIds": {
                    "ArXiv": "2102.04877",
                    "DBLP": "journals/corr/abs-2102-04877",
                    "CorpusId": 231855389
                },
                "corpusId": 231855389,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a747905cc60cde18a8c1fc1e365e319cea0ff49b",
                "title": "Noisy Recurrent Neural Networks",
                "abstract": "We provide a general framework for studying recurrent neural networks (RNNs) trained by injecting noise into hidden states. Specifically, we consider RNNs that can be viewed as discretizations of stochastic differential equations driven by input data. This framework allows us to study the implicit regularization effect of general noise injection schemes by deriving an approximate explicit regularizer in the small noise regime. We find that, under reasonable assumptions, this implicit regularization promotes flatter minima; it biases towards models with more stable dynamics; and, in classification tasks, it favors models with larger classification margin. Sufficient conditions for global stability are obtained, highlighting the phenomenon of stochastic stabilization, where noise injection can improve stability during training. Our theory is supported by empirical results which demonstrate that the RNNs have improved robustness with respect to various input perturbations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "98291940",
                        "name": "S. H. Lim"
                    },
                    {
                        "authorId": "2371914",
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "authorId": "90068212",
                        "name": "Liam Hodgkinson"
                    },
                    {
                        "authorId": "1717098",
                        "name": "Michael W. Mahoney"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More general formulations such as Hamiltonian (Greydanus et al., 2019; Toth et al., 2019) and Lagrangian",
                "More general formulations such as Hamiltonian (Greydanus et al., 2019; Toth et al., 2019) and Lagrangian\nDynamics (Lutter et al., 2019) are currently restricted in terms of the dimension of the dynamical system."
            ],
            "citingPaper": {
                "paperId": "7869928bc7c5809b05760367acc2eb5fd2a0e8c7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-05834",
                    "ArXiv": "2101.05834",
                    "CorpusId": 231627780
                },
                "corpusId": 231627780,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7869928bc7c5809b05760367acc2eb5fd2a0e8c7",
                "title": "Physics-aware, probabilistic model order reduction with guaranteed stability",
                "abstract": "Given (small amounts of) time-series' data from a high-dimensional, fine-grained, multiscale dynamical system, we propose a generative framework for learning an effective, lower-dimensional, coarse-grained dynamical model that is predictive of the fine-grained system's long-term evolution but also of its behavior under different initial conditions. We target fine-grained models as they arise in physical applications (e.g. molecular dynamics, agent-based models), the dynamics of which are strongly non-stationary but their transition to equilibrium is governed by unknown slow processes which are largely inaccessible by brute-force simulations. Approaches based on domain knowledge heavily rely on physical insight in identifying temporally slow features and fail to enforce the long-term stability of the learned dynamics. On the other hand, purely statistical frameworks lack interpretability and rely on large amounts of expensive simulation data (long and multiple trajectories) as they cannot infuse domain knowledge. The generative framework proposed achieves the aforementioned desiderata by employing a flexible prior on the complex plane for the latent, slow processes, and an intermediate layer of physics-motivated latent variables that reduces reliance on data and imbues inductive bias. In contrast to existing schemes, it does not require the a priori definition of projection operators from the fine-grained description and addresses simultaneously the tasks of dimensionality reduction and model estimation. We demonstrate its efficacy and accuracy in multiscale physical systems of particle dynamics where probabilistic, long-term predictions of phenomena not contained in the training data are produced.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "113831649",
                        "name": "Sebastian Kaltenbach"
                    },
                    {
                        "authorId": "2415520",
                        "name": "P. Koutsourelakis"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In [14], the authors use the symplectic leapfrog integrator and show improved performance over a forward Euler integrator; however, this comparison does not distinguish between gains due to symplecticness and gains due to the second-order accuracy of leapfrog."
            ],
            "citingPaper": {
                "paperId": "932314b95a12478582b276b967a880d63a7a099f",
                "externalIds": {
                    "DBLP": "conf/cdc/GaliotoG20",
                    "DOI": "10.1109/CDC42340.2020.9303852",
                    "CorpusId": 220524175
                },
                "corpusId": 220524175,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/932314b95a12478582b276b967a880d63a7a099f",
                "title": "Bayesian Identification of Hamiltonian Dynamics from Symplectic Data",
                "abstract": "We propose a Bayesian probabilistic formulation for system identification of Hamiltonian systems. This approach uses an approximate marginal Markov Chain Monte Carlo algorithm to directly discover a system Hamiltonian from data. Our approach improves upon existing methods in two ways: first we encode the fact that the data generating process is symplectic directly into our learning objective, and second we utilize a learning objective that simultaneously accounts for unknown parameters, model form, and measurement noise. This objective is the log marginal posterior of a probabilistic model that embeds a symplectic and reversible integrator within an uncertain dynamical system. We demonstrate that the resulting learning problem yields dynamical systems that have improved accuracy and reduced predictive uncertainty compared to existing state-of-the-art approaches. Simulation results are shown on the H\u00e9non-Heiles Hamiltonian system.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2093676645",
                        "name": "Nicholas Galioto"
                    },
                    {
                        "authorId": "2439226",
                        "name": "A. Gorodetsky"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[71] 2020 DD NN \ue234 \u00d7 Cranmer et al.",
                "Additional details on ARMs with linear transformed NNs is found in [22, 68, 23, 72, 67, 71] (cf."
            ],
            "citingPaper": {
                "paperId": "bf079ddc422ddbdfd0511baf5e48649389f422cc",
                "externalIds": {
                    "MAG": "3169805247",
                    "DBLP": "journals/corr/abs-2012-06250",
                    "ArXiv": "2012.06250",
                    "DOI": "10.1002/gamm.202100009",
                    "CorpusId": 228376142
                },
                "corpusId": 228376142,
                "publicationVenue": {
                    "id": "b388508d-7838-440c-a44d-bb2a1bfbe204",
                    "name": "GAMM-Mitteilungen",
                    "type": "journal",
                    "alternate_names": [
                        "Gamm-mitteilungen"
                    ],
                    "issn": "0936-7195",
                    "url": "https://onlinelibrary.wiley.com/journal/15222608"
                },
                "url": "https://www.semanticscholar.org/paper/bf079ddc422ddbdfd0511baf5e48649389f422cc",
                "title": "Structured learning of rigid\u2010body dynamics: A survey and unified view from a robotics perspective",
                "abstract": "Accurate models of mechanical system dynamics are often critical for model\u2010based control and reinforcement learning. Fully data\u2010driven dynamics models promise to ease the process of modeling and analysis, but require considerable amounts of data for training and often do not generalize well to unseen parts of the state space. Combining data\u2010driven modeling with prior analytical knowledge is an attractive alternative as the inclusion of structural knowledge into a regression model improves the model's data efficiency and physical integrity. In this article, we survey supervised regression models that combine rigid\u2010body mechanics with data\u2010driven modeling techniques. We analyze the different latent functions (such as kinetic energy or dissipative forces) and operators (such as differential operators and projection matrices) underlying common descriptions of rigid\u2010body mechanics. Based on this analysis, we provide a unified view on the combination of data\u2010driven regression models, such as neural networks and Gaussian processes, with analytical model priors. Furthermore, we review and discuss key techniques for designing structured models such as automatic differentiation.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2064785715",
                        "name": "A. R. Geist"
                    },
                    {
                        "authorId": "2715093",
                        "name": "Sebastian Trimpe"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In other developments, autoencoder-based HNNs (AE-HNNs) [15] and Hamiltonian Generative Networks (HGNs) [19] were proposed to learn and predict the images of mechanical systems, which can be seen as Hamiltonian systems on manifolds embedded in high-dimensional spaces.",
                "Based on HNNs, other models were proposed to tackle problems in generative modeling [16], [19] and continuous control [20].",
                "In recent works [14]\u2013[19], the primary focus has been to solve the inverse problem, i."
            ],
            "citingPaper": {
                "paperId": "a521b4f034fc94a6ecbe6807c2183eed40320ca1",
                "externalIds": {
                    "ArXiv": "2012.03133",
                    "DBLP": "journals/corr/abs-2012-03133",
                    "MAG": "3112050848",
                    "DOI": "10.1109/TNNLS.2022.3148734",
                    "CorpusId": 227333962,
                    "PubMed": "35180089"
                },
                "corpusId": 227333962,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a521b4f034fc94a6ecbe6807c2183eed40320ca1",
                "title": "Learning Poisson systems and trajectories of autonomous systems via Poisson neural networks",
                "abstract": "We propose the Poisson neural networks (PNNs) to learn Poisson systems and trajectories of autonomous systems from data. Based on the Darboux-Lie theorem, the phase flow of a Poisson system can be written as the composition of: 1) a coordinate transformation; 2) an extended symplectic map; and 3) the inverse of the transformation. In this work, we extend this result to the unknotted trajectories of autonomous systems. We employ structured neural networks with physical priors to approximate the three aforementioned maps. We demonstrate through several simulations that PNNs are capable of handling very accurately several challenging tasks, including the motion of a particle in the electromagnetic potential, the nonlinear Schr\u00f6dinger equation, and pixel observations of the two-body problem.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "2116702527",
                        "name": "Zhen Zhang"
                    },
                    {
                        "authorId": "3439407",
                        "name": "I. Kevrekidis"
                    },
                    {
                        "authorId": "1720124",
                        "name": "G. Karniadakis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "537c9772e3e8e5cab843873a9cbaf761632ee439",
                "externalIds": {
                    "ArXiv": "2012.00429",
                    "MAG": "3106628198",
                    "CorpusId": 227239483
                },
                "corpusId": 227239483,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/537c9772e3e8e5cab843873a9cbaf761632ee439",
                "title": "Temperature-steerable flows",
                "abstract": "Boltzmann generators approach the sampling problem in many-body physics by combining a normalizing flow and a statistical reweighting method to generate samples of a physical system's equilibrium density. The equilibrium distribution is usually defined by an energy function and a thermodynamic state, such as a given temperature. Here we propose temperature-steerable flows (TSF) which are able to generate a family of probability densities parametrized by a choosable temperature parameter. TSFs can be embedded in a generalized ensemble sampling framework such as parallel tempering in order to sample a physical system across thermodynamic states, such as multiple temperatures.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "116366008",
                        "name": "Manuel Dibak"
                    },
                    {
                        "authorId": "1380082499",
                        "name": "Leon Klein"
                    },
                    {
                        "authorId": "2064855776",
                        "name": "Frank No'e"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ee9a9364e98bbaedffbe137adbaa4da8d9ee77b5",
                "externalIds": {
                    "MAG": "3109817511",
                    "DOI": "10.21203/rs.3.rs-94390/v1",
                    "CorpusId": 250536799
                },
                "corpusId": 250536799,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ee9a9364e98bbaedffbe137adbaa4da8d9ee77b5",
                "title": "Deep neural networks for high harmonic spectroscopy in solids",
                "abstract": "\n Neural networks are a prominent tool for identifying and modeling complex patterns, which are otherwise hard to detect and analyse. While machine learning and neural networks have \nbeen finding applications across many areas of science and technology, their use in decoding ultrafast \ndynamics of quantum systems driven by strong laser fields has been limited so far. Here we use deep neural networks to analyze spectra of highly nonlinear optical response \nof a crystal to intense few-cycle laser pulses. We construct a deep neural network that can efficiently utilize such spectra to resolve both the complex spectral phase of ultrashort laser pulses and simultaneously reconstruct the band structure of the crystal. Our results offer a new tool for attosecond spectroscopy of quantum dynamics and also open a route to developing all-solid-state devices for complete characterization of few-cycle pulses, including their nonlinear chirp and the carrier envelope phase.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "102617464",
                        "name": "N. Klimkin"
                    },
                    {
                        "authorId": "32106860",
                        "name": "M. Ivanov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Most of them are done in a supervised way [1, 2, 3, 4]."
            ],
            "citingPaper": {
                "paperId": "d57a9e482b7b6fca0aea228e43d529281171a9b5",
                "externalIds": {
                    "ArXiv": "2011.11891",
                    "DBLP": "journals/corr/abs-2011-11891",
                    "MAG": "3108925605",
                    "CorpusId": 227151304
                },
                "corpusId": 227151304,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d57a9e482b7b6fca0aea228e43d529281171a9b5",
                "title": "Learning Principle of Least Action with Reinforcement Learning",
                "abstract": "Nature provides a way to understand physics with reinforcement learning since nature favors the economical way for an object to propagate. In the case of classical mechanics, nature favors the object to move along the path according to the integral of the Lagrangian, called the action $\\mathcal{S}$. We consider setting the reward/penalty as a function of $\\mathcal{S}$, so the agent could learn the physical trajectory of particles in various kinds of environments with reinforcement learning. In this work, we verified the idea by using a Q-Learning based algorithm on learning how light propagates in materials with different refraction indices, and show that the agent could recover the minimal-time path equivalent to the solution obtained by Snell's law or Fermat's Principle. We also discuss the similarity of our reinforcement learning approach to the path integral formalism.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2110880315",
                        "name": "Zehao Jin"
                    },
                    {
                        "authorId": "50993009",
                        "name": "J. Lin"
                    },
                    {
                        "authorId": "2118154707",
                        "name": "Siao-Fong Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To create a Hamiltonian neural network [8,19,15,2,3]",
                "In particular, Hamiltonian neural networks [8,19,15,2, 3] exploit the symplectic structure of conservative systems to forecast dynamics that mix order and chaos [6], even in very high dimensions [16].",
                "[19] introduced Hamiltonian Generative Networks (HGN), which harnessed the Hamiltonian flow without assuming canonical coordinates, but their statistical loss function was a complicated difference of posterior and prior probability distributions."
            ],
            "citingPaper": {
                "paperId": "71079261ff66497aa75abb5b3e220c627c4d6dd5",
                "externalIds": {
                    "ArXiv": "2010.15201",
                    "MAG": "3097385874",
                    "DBLP": "journals/corr/abs-2010-15201",
                    "DOI": "10.1007/s11071-020-06185-2",
                    "CorpusId": 225103333
                },
                "corpusId": 225103333,
                "publicationVenue": {
                    "id": "10925c1c-0929-4ec5-8268-a8a52bd84631",
                    "name": "Nonlinear dynamics",
                    "type": "journal",
                    "alternate_names": [
                        "Nonlinear Dyn",
                        "Nonlinear Dynamics",
                        "Nonlinear dyn"
                    ],
                    "issn": "0924-090X",
                    "url": "http://www.springer.com/11071",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11071"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/71079261ff66497aa75abb5b3e220c627c4d6dd5",
                "title": "Forecasting Hamiltonian dynamics without canonical coordinates",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "1443437285",
                        "name": "Elliott G. Holliday"
                    },
                    {
                        "authorId": "50783104",
                        "name": "Scott T. Miller"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019; Bertalan et al. 2019), the authors model the underlying dynamics of sequential data assuming the Hamiltonian structure."
            ],
            "citingPaper": {
                "paperId": "d53fbc2bd3adba8fd9d0df1aa7d852e0e20d53ac",
                "externalIds": {
                    "ArXiv": "2010.12932",
                    "MAG": "3094142577",
                    "DBLP": "journals/corr/abs-2010-12932",
                    "CorpusId": 225070180
                },
                "corpusId": 225070180,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d53fbc2bd3adba8fd9d0df1aa7d852e0e20d53ac",
                "title": "LagNetViP: A Lagrangian Neural Network for Video Prediction",
                "abstract": "The dominant paradigms for video prediction rely on opaque transition models where neither the equations of motion nor the underlying physical quantities of the system are easily inferred. The equations of motion, as defined by Newton's second law, describe the time evolution of a physical system state and can therefore be applied toward the determination of future system states. In this paper, we introduce a video prediction model where the equations of motion are explicitly constructed from learned representations of the underlying physical quantities. To achieve this, we simultaneously learn a low-dimensional state representation and system Lagrangian. The kinetic and potential energy terms of the Lagrangian are distinctly modelled and the low-dimensional equations of motion are explicitly constructed using the Euler-Lagrange equations. We demonstrate the efficacy of this approach for video prediction on image sequences rendered in modified OpenAI gym Pendulum-v0 and Acrobot environments.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1405669277",
                        "name": "Christine Allen-Blanchette"
                    },
                    {
                        "authorId": "1491176908",
                        "name": "Sushant Veer"
                    },
                    {
                        "authorId": "1780468",
                        "name": "Anirudha Majumdar"
                    },
                    {
                        "authorId": "3301461",
                        "name": "Naomi Ehrich Leonard"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[35] developed the Hamiltonian Generative Network (HGN), learning Hamiltonian dynamics from high-dimensional observations."
            ],
            "citingPaper": {
                "paperId": "1ab8bae9fa2df31c81aac4226e12094c616f8a22",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-12636",
                    "ArXiv": "2010.12636",
                    "MAG": "3094020114",
                    "CorpusId": 225068405
                },
                "corpusId": 225068405,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1ab8bae9fa2df31c81aac4226e12094c616f8a22",
                "title": "Nonseparable Symplectic Neural Networks",
                "abstract": "Predicting the behaviors of Hamiltonian systems has been drawing increasing attention in scientific machine learning. However, the vast majority of the literature was focused on predicting separable Hamiltonian systems with their kinematic and potential energy terms being explicitly decoupled, while building data-driven paradigms to predict nonseparable Hamiltonian systems that are ubiquitous in fluid dynamics and quantum mechanics were rarely explored. The main computational challenge lies in the effective embedding of symplectic priors to describe the inherently coupled evolution of position and momentum, which typically exhibits intricate dynamics with many degrees of freedom. To solve the problem, we propose a novel neural network architecture, Nonseparable Symplectic Neural Networks (NSSNNs), to uncover and embed the symplectic structure of a nonseparable Hamiltonian system from limited observation data. The enabling mechanics of our approach is an augmented symplectic time integrator to decouple the position and momentum energy terms and facilitate their evolution. We demonstrated the efficacy and versatility of our method by predicting a wide range of Hamiltonian systems, both separable and nonseparable, including vortical flow and quantum system. We showed the unique computational merits of our approach to yield long-term, accurate, and robust predictions for large-scale Hamiltonian systems by rigorously enforcing symplectomorphism.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47722204",
                        "name": "S. Xiong"
                    },
                    {
                        "authorId": "40495066",
                        "name": "Yunjin Tong"
                    },
                    {
                        "authorId": "1491320619",
                        "name": "Xingzhe He"
                    },
                    {
                        "authorId": "3443627",
                        "name": "Cheng Yang"
                    },
                    {
                        "authorId": "14386295",
                        "name": "Shuqi Yang"
                    },
                    {
                        "authorId": "92035057",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fc674c6ea9087b654f9a37ec2396d05eceb49b73",
                "externalIds": {
                    "MAG": "3161839595",
                    "ArXiv": "2010.09785",
                    "DBLP": "journals/corr/abs-2010-09785",
                    "DOI": "10.3934/jcd.2021012",
                    "CorpusId": 224803624
                },
                "corpusId": 224803624,
                "publicationVenue": {
                    "id": "5f58c92f-0128-4372-963e-172d82dbf89a",
                    "name": "Journal of Computational Dynamics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Dyn"
                    ],
                    "issn": "2158-2491",
                    "url": "http://aimsciences.org/journal/2158-2491"
                },
                "url": "https://www.semanticscholar.org/paper/fc674c6ea9087b654f9a37ec2396d05eceb49b73",
                "title": "On Computational Poisson Geometry II: Numerical Methods",
                "abstract": "<p style='text-indent:20px;'>We present twelve numerical methods for evaluation of objects and concepts from Poisson geometry. We describe how each method works with examples, and explain how it is executed in code. These include methods that evaluate Hamiltonian and modular vector fields, compute the image under the coboundary and trace operators, the Lie bracket of differential 1\u2013forms, gauge transformations, and normal forms of Lie\u2013Poisson structures on <inline-formula><tex-math id=\"M1\">\\begin{document}$ {\\mathbf{R}^{{3}}} $\\end{document}</tex-math></inline-formula>. The complexity of each of our methods is calculated, and we include experimental verifications on examples in dimensions two and three.</p>",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1429802563",
                        "name": "M. Evangelista-Alvarado"
                    },
                    {
                        "authorId": "1412516577",
                        "name": "J. C. Ru'iz-Pantale'on"
                    },
                    {
                        "authorId": "2066256143",
                        "name": "P. Su'arez-Serrato"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "757a3c0e2a43f3d15b91c52fae32b081a6a66e3a",
                "externalIds": {
                    "ArXiv": "2010.04456",
                    "DBLP": "journals/corr/abs-2010-04456",
                    "MAG": "3092352028",
                    "DOI": "10.1088/1742-5468/ac3ae5",
                    "CorpusId": 222272443
                },
                "corpusId": 222272443,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/757a3c0e2a43f3d15b91c52fae32b081a6a66e3a",
                "title": "Augmenting physical models with deep networks for complex dynamics forecasting",
                "abstract": "Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling-based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists of decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model; no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefit generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction\u2013diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters. The code is available at https://github.com/yuan-yin/APHYNITY.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3965182",
                        "name": "V. Guen"
                    },
                    {
                        "authorId": "2109472874",
                        "name": "Yuan Yin"
                    },
                    {
                        "authorId": "1853488882",
                        "name": "J\u00e9r\u00e9mie Don\u00e0"
                    },
                    {
                        "authorId": "10771473",
                        "name": "Ibrahim Ayed"
                    },
                    {
                        "authorId": "2065044561",
                        "name": "Emmanuel de B'ezenac"
                    },
                    {
                        "authorId": "1728523",
                        "name": "Nicolas Thome"
                    },
                    {
                        "authorId": "1741426",
                        "name": "P. Gallinari"
                    }
                ]
            }
        },
        {
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "This gives us a volume-preserving normalizing flow, similar to (Dinh et al., 2015; Toth et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "7f23775c1acde900b6fb11f48f8ea8ae535495e9",
                "externalIds": {
                    "ArXiv": "2010.03242",
                    "DBLP": "conf/icml/BilosG21",
                    "CorpusId": 235683294
                },
                "corpusId": 235683294,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7f23775c1acde900b6fb11f48f8ea8ae535495e9",
                "title": "Scalable Normalizing Flows for Permutation Invariant Densities",
                "abstract": "Modeling sets is an important problem in machine learning since this type of data can be found in many domains. A promising approach defines a family of permutation invariant densities with continuous normalizing flows. This allows us to maximize the likelihood directly and sample new realizations with ease. In this work, we demonstrate how calculating the trace, a crucial step in this method, raises issues that occur both during training and inference, limiting its practicality. We propose an alternative way of defining permutation equivariant transformations that give closed form trace. This leads not only to improvements while training, but also to better final performance. We demonstrate the benefits of our approach on point processes and general set modeling.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1388111489",
                        "name": "Marin Bilos"
                    },
                    {
                        "authorId": "3075189",
                        "name": "Stephan G\u00fcnnemann"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9ee0dba2c436b7b3c260db8157b64f9c02d3f1f7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-03242",
                    "MAG": "3092469665",
                    "CorpusId": 222316279
                },
                "corpusId": 222316279,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9ee0dba2c436b7b3c260db8157b64f9c02d3f1f7",
                "title": "Equivariant Normalizing Flows for Point Processes and Sets",
                "abstract": "A point process describes how random sets of exchangeable points are generated. The points usually influence the positions of each other via attractive and repulsive forces. To model this behavior, it is enough to transform the samples from the uniform process with a sufficiently complex equivariant function. However, learning the parameters of the resulting process is challenging since the likelihood is hard to estimate and often intractable. This leads us to our proposed model - CONFET. Based on continuous normalizing flows, it allows arbitrary interactions between points while having tractable likelihood. Experiments on various real and synthetic datasets show the improved performance of our new scalable approach.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1388111489",
                        "name": "Marin Bilos"
                    },
                    {
                        "authorId": "3075189",
                        "name": "Stephan G\u00fcnnemann"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "enforcing conservation laws in subdomains [19], hyperbolic conservation laws [41], Hamiltonian mechanics [42,43], symplectic structures [44,45], Lagrangian mechanics [46] and metriplectic structures [47]) and we believe that adapting/extending ideas of these approaches potentially mitigates the limitation of data-driven surrogate modelling approaches."
            ],
            "citingPaper": {
                "paperId": "812f9a18c9768d24536bdd784d08f82336a8f28a",
                "externalIds": {
                    "MAG": "3095118158",
                    "ArXiv": "2010.14685",
                    "DBLP": "journals/corr/abs-2010-14685",
                    "DOI": "10.1098/rspa.2021.0162",
                    "CorpusId": 225094147
                },
                "corpusId": 225094147,
                "publicationVenue": {
                    "id": "b61ce141-a434-431b-a154-68fc26e348f3",
                    "name": "Proceedings of the Royal Society A",
                    "type": "journal",
                    "alternate_names": [
                        "Proc R Soc A",
                        "Proc R Soc Math Phys Eng Sci",
                        "Proceedings of The Royal Society A: Mathematical, Physical and Engineering Sciences"
                    ],
                    "issn": "1364-5021",
                    "url": "https://www.jstor.org/journal/procmathphysengi",
                    "alternate_urls": [
                        "http://rspa.royalsocietypublishing.org/content/by/year",
                        "http://rspa.royalsocietypublishing.org/about",
                        "http://rspa.royalsocietypublishing.org/",
                        "https://royalsocietypublishing.org/journal/rspa"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/812f9a18c9768d24536bdd784d08f82336a8f28a",
                "title": "Parameterized neural ordinary differential equations: applications to computational physics problems",
                "abstract": "This work proposes an extension of neural ordinary differential equations (NODEs) by introducing an additional set of ODE input parameters to NODEs. This extension allows NODEs to learn multiple dynamics specified by the input parameter instances. Our extension is inspired by the concept of parameterized ODEs, which are widely investigated in computational science and engineering contexts, where characteristics of the governing equations vary over the input parameters. We apply the proposed parameterized NODEs (PNODEs) for learning latent dynamics of complex dynamical processes that arise in computational physics, which is an essential component for enabling rapid numerical simulations for time-critical physics applications. For this, we propose an encoder\u2013decoder-type framework, which models latent dynamics as PNODEs. We demonstrate the effectiveness of PNODEs on benchmark problems from computational physics.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "35221323",
                        "name": "E. Parish"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More works on learning Hamiltonian systems can be found in [11, 34, 43] and references cited therein."
            ],
            "citingPaper": {
                "paperId": "7ef234aa60401c0fb32506979d461e85c22d6199",
                "externalIds": {
                    "MAG": "3088508210",
                    "DBLP": "journals/corr/abs-2009-13415",
                    "CorpusId": 221971369
                },
                "corpusId": 221971369,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7ef234aa60401c0fb32506979d461e85c22d6199",
                "title": "Learning Interpretable and Thermodynamically Stable Partial Differential Equations",
                "abstract": "In this work, we develop a method for learning interpretable and thermodynamically stable partial differential equations (PDEs) based on the Conservation-dissipation Formalism of irreversible thermodynamics. As governing equations for non-equilibrium flows in one dimension, the learned PDEs are parameterized by fully-connected neural networks and satisfy the conservation-dissipation principle automatically. In particular, they are hyperbolic balance laws. The training data are generated from a kinetic model with smooth initial data. Numerical results indicate that the learned PDEs can achieve good accuracy in a wide range of Knudsen numbers. Remarkably, the learned dynamics can give satisfactory results with randomly sampled discontinuous initial data although it is trained only with smooth initial data.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2391774",
                        "name": "Juntao Huang"
                    },
                    {
                        "authorId": "2116417193",
                        "name": "Zhiting Ma"
                    },
                    {
                        "authorId": "49455479",
                        "name": "Y. Zhou"
                    },
                    {
                        "authorId": "34070717",
                        "name": "W. Yong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More works on learning Hamiltonian systems can be found in [13, 37, 46] and references cited therein."
            ],
            "citingPaper": {
                "paperId": "c2672d933faa17de2f2caf9942f386464709287f",
                "externalIds": {
                    "ArXiv": "2009.13415",
                    "MAG": "3109390220",
                    "DOI": "10.1515/jnet-2021-0008",
                    "CorpusId": 227225654
                },
                "corpusId": 227225654,
                "publicationVenue": {
                    "id": "bd314ae9-81ae-45be-9cf7-860255f1c0d0",
                    "name": "Journal of Non-Equilibrium Thermodynamics",
                    "type": "journal",
                    "alternate_names": [
                        "J Non-equilibrium Thermodyn"
                    ],
                    "issn": "0340-0204",
                    "alternate_issns": [
                        "1437-4358"
                    ],
                    "url": "https://www.degruyter.com/view/j/jnet",
                    "alternate_urls": [
                        "http://www.degruyter.com/view/j/jnet"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c2672d933faa17de2f2caf9942f386464709287f",
                "title": "Learning Thermodynamically Stable and Galilean Invariant Partial Differential Equations for Non-Equilibrium Flows",
                "abstract": "Abstract In this work, we develop a method for learning interpretable, thermodynamically stable and Galilean invariant partial differential equations (PDEs) based on the conservation-dissipation formalism of irreversible thermodynamics. As governing equations for non-equilibrium flows in one dimension, the learned PDEs are parameterized by fully connected neural networks and satisfy the conservation-dissipation principle automatically. In particular, they are hyperbolic balance laws and Galilean invariant. The training data are generated from a kinetic model with smooth initial data. Numerical results indicate that the learned PDEs can achieve good accuracy in a wide range of Knudsen numbers. Remarkably, the learned dynamics can give satisfactory results with randomly sampled discontinuous initial data and Sod\u2019s shock tube problem although it is trained only with smooth initial data.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2391774",
                        "name": "Juntao Huang"
                    },
                    {
                        "authorId": "2116417193",
                        "name": "Zhiting Ma"
                    },
                    {
                        "authorId": "49455479",
                        "name": "Y. Zhou"
                    },
                    {
                        "authorId": "34070717",
                        "name": "W. Yong"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5cdf93a4a06b18eae113c506d6c1b7d20ce2f59d",
                "externalIds": {
                    "MAG": "3089156014",
                    "DOI": "10.1016/J.CSFX.2020.100046",
                    "CorpusId": 224949981
                },
                "corpusId": 224949981,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5cdf93a4a06b18eae113c506d6c1b7d20ce2f59d",
                "title": "The scaling of physics-informed machine learning with data and dimensions",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50783104",
                        "name": "Scott T. Miller"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Through TorchDyn neural differential equations and derivative models, e.g (Greydanus et al., 2019; Toth et al., 2019; Massaroli et al., 2020b; Lutter et al., 2019; Cranmer et al., 2020; Massaroli et al., 2020a; Li et al., 2020), including yet\u2013to\u2013be\u2013published combinations, can effortlessly be\u2026",
                "Notably, this allows for an out\u2013of\u2013the\u2013box definition of Hamiltonian CNFs (Toth et al., 2019) and other unpublished variants.",
                "g (Greydanus et al., 2019; Toth et al., 2019; Massaroli et al., 2020b; Lutter et al., 2019; Cranmer et al., 2020; Massaroli et al., 2020a; Li et al., 2020), including yet\u2013to\u2013be\u2013published combinations, can effortlessly be obtained by ad hoc primitives in combination with the rich PyTorch (Paszke et al.",
                "Continuous normalizing flows An additional fundamental member of the continuous\u2013 depth framework, continuous normalizing flows (CNFs) (Chen et al., 2018; Grathwohl et al., 2018) are treated as first\u2013class primitives.",
                "C on\ntr ol\nA d jo in t In te gr al\nL os s\nModel ANODE (Dupont et al., 2019)\nHigher-Order (Massaroli et al., 2020b)\nGale\u0308rkin Neural ODEs (Massaroli et al., 2020b) Stacked Neural ODEs (Massaroli et al., 2020b)\nHamiltonian (Greydanus et al., 2019)\nLagrangian (Lutter et al., 2019; Cranmer et al., 2020)\nStable Neural Flows (Massaroli et al., 2020a)\nGraph Neural ODEs (Poli et al., 2019)\nCNF (Chen et al., 2018)\nFFJORD (Grathwohl et al., 2018)\nRNODE (Finlay et al., 2020)\nFigure 3: Support in torchdyn of different SOTA models."
            ],
            "citingPaper": {
                "paperId": "7fb63e2479bc76ae72bbf67289f212a4c16f886f",
                "externalIds": {
                    "ArXiv": "2009.09346",
                    "DBLP": "journals/corr/abs-2009-09346",
                    "MAG": "3087644732",
                    "CorpusId": 221818712
                },
                "corpusId": 221818712,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7fb63e2479bc76ae72bbf67289f212a4c16f886f",
                "title": "TorchDyn: A Neural Differential Equations Library",
                "abstract": "Continuous-depth learning has recently emerged as a novel perspective on deep learning, improving performance in tasks related to dynamical systems and density estimation. Core to these approaches is the neural differential equation, whose forward passes are the solutions of an initial value problem parametrized by a neural network. Unlocking the full potential of continuous-depth models requires a different set of software tools, due to peculiar differences compared to standard discrete neural networks, e.g inference must be carried out via numerical solvers. We introduce TorchDyn, a PyTorch library dedicated to continuous-depth learning, designed to elevate neural differential equations to be as accessible as regular plug-and-play deep learning primitives. This objective is achieved by identifying and subdividing different variants into common essential components, which can be combined and freely repurposed to obtain complex compositional architectures. TorchDyn further offers step-by-step tutorials and benchmarks designed to guide researchers and contributors.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40585370",
                        "name": "Michael Poli"
                    },
                    {
                        "authorId": "90467999",
                        "name": "Stefano Massaroli"
                    },
                    {
                        "authorId": "1748130",
                        "name": "A. Yamashita"
                    },
                    {
                        "authorId": "5312699",
                        "name": "H. Asama"
                    },
                    {
                        "authorId": "2085587",
                        "name": "Jinkyoo Park"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "58792475c912ca69de98c0e8a9e5d7f9f1eea355",
                "externalIds": {
                    "MAG": "3086666652",
                    "CorpusId": 263525979
                },
                "corpusId": 263525979,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/58792475c912ca69de98c0e8a9e5d7f9f1eea355",
                "title": "Symplectic Gaussian Process Regression of Hamiltonian Flow Maps",
                "abstract": "We present an approach to construct appropriate and efficient emulators for Hamiltonian flow maps. Intended future applications are long-term tracing of fast charged particles in accelerators and magnetic plasma confinement configurations. The method is based on multi-output Gaussian process regression on scattered training data. To obtain long-term stability the symplectic property is enforced via the choice of the matrix-valued covariance function. Based on earlier work on spline interpolation we observe derivatives of the generating function of a canonical transformation. A product kernel produces an accurate implicit method, whereas a sum kernel results in a fast explicit method from this approach. Both correspond to a symplectic Euler method in terms of numerical integration. These methods are applied to the pendulum and the Henon-Heiles system and results compared to an symmetric regression with orthogonal polynomials. In the limit of small mapping times, the Hamiltonian function can be identified with a part of the generating function and thereby learned from observed time-series data of the system's evolution. Besides comparable performance of implicit kernel and spectral regression for symplectic maps, we demonstrate a substantial increase in performance for learning the Hamiltonian function compared to existing approaches.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "21574289",
                        "name": "K. Rath"
                    },
                    {
                        "authorId": "2250978448",
                        "name": "Christopher G. Albert"
                    },
                    {
                        "authorId": "2252240267",
                        "name": "Bernd Bischl"
                    },
                    {
                        "authorId": "2425907",
                        "name": "U. Toussaint"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ": Denton & Fergus (2018); Villegas et al. (2019); Weissenborn et al."
            ],
            "citingPaper": {
                "paperId": "9130668c2927ee5989854e09ad565a2dd1bd6391",
                "externalIds": {
                    "MAG": "3047211079",
                    "DBLP": "journals/corr/abs-2008-01352",
                    "ArXiv": "2008.01352",
                    "CorpusId": 220961494
                },
                "corpusId": 220961494,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9130668c2927ee5989854e09ad565a2dd1bd6391",
                "title": "PDE-Driven Spatiotemporal Disentanglement",
                "abstract": "A recent line of work in the machine learning community addresses the problem of predicting high-dimensional spatiotemporal phenomena by leveraging specific tools from the differential equations theory. Following this direction, we propose in this article a novel and general paradigm for this task based on a resolution method for partial differential equations: the separation of variables. This inspiration allows us to introduce a dynamical interpretation of spatiotemporal disentanglement. It induces a principled model based on learning disentangled spatial and temporal representations of a phenomenon to accurately predict future observations. We experimentally demonstrate the performance and broad applicability of our method against prior state-of-the-art models on physical and synthetic video datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1853488882",
                        "name": "J\u00e9r\u00e9mie Don\u00e0"
                    },
                    {
                        "authorId": "35622441",
                        "name": "Jean-Yves Franceschi"
                    },
                    {
                        "authorId": "1782552",
                        "name": "S. Lamprier"
                    },
                    {
                        "authorId": "1741426",
                        "name": "P. Gallinari"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Moreover, some of them use special ODE functions such as Hamilton\u2019s equations to incorporate physical properties to neural network structurally [15, 42, 32, 7, 40].",
                "Neural ODE and its applications [6, 5, 15, 42, 32, 7, 40], alias ODE networks (ODENs), tackle these issues by learning the governing equations, rather than the state transitions directly.",
                ", ODE [6], Hamiltonian [15, 32, 40, 42, 7], and other domain knowledge [36, 37, 27, 29].",
                "Recent works [15, 42, 32, 7, 40] apply the Hamiltonian mechanics to ODE networks, and succeed in enforcing the energy conservation as well as the accurate time evolution of classical conservative systems."
            ],
            "citingPaper": {
                "paperId": "b2c078e7416de65bbc431c24762edcf33ac9f59b",
                "externalIds": {
                    "MAG": "3044111002",
                    "DBLP": "journals/corr/abs-2007-11362",
                    "ArXiv": "2007.11362",
                    "CorpusId": 220686334
                },
                "corpusId": 220686334,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b2c078e7416de65bbc431c24762edcf33ac9f59b",
                "title": "Time-Reversal Symmetric ODE Network",
                "abstract": "Time-reversal symmetry, which requires that the dynamics of a system should not change with the reversal of time axis, is a fundamental property that frequently holds in classical and quantum mechanics. In this paper, we propose a novel loss function that measures how well our ordinary differential equation (ODE) networks comply with this time-reversal symmetry; it is formally defined by the discrepancy in the time evolutions of ODE networks between forward and backward dynamics. Then, we design a new framework, which we name as Time-Reversal Symmetric ODE Networks (TRS-ODENs), that can learn the dynamics of physical systems more sample-efficiently by learning with the proposed loss function. We evaluate TRS-ODENs on several classical dynamics, and find they can learn the desired time evolution from observed noisy and complex trajectories. We also show that, even for systems that do not possess the full time-reversal symmetry, TRS-ODENs can achieve better predictive performances over baselines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2064530354",
                        "name": "In Huh"
                    },
                    {
                        "authorId": "1720494",
                        "name": "Eunho Yang"
                    },
                    {
                        "authorId": "35788904",
                        "name": "Sung Ju Hwang"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In this space, we primarily see efforts to learn classical Hamiltonians from time series [4, 5, 9, 22, 28, 30, 38, 46, 49] as well as efforts to learn quantum Hamiltonians or potentials for timeindependent problems [2, 3, 15, 20, 24]."
            ],
            "citingPaper": {
                "paperId": "816b7fb3a9929390214ac67b999ef231c3716a5d",
                "externalIds": {
                    "MAG": "3043388595",
                    "DBLP": "journals/corr/abs-2007-09814",
                    "ArXiv": "2007.09814",
                    "DOI": "10.1007/s40435-020-00699-8",
                    "CorpusId": 220647552
                },
                "corpusId": 220647552,
                "publicationVenue": {
                    "id": "03c42de9-e1fc-4c76-8e63-5813d7f59424",
                    "name": "International Journal of Dynamics and Control",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Dyn Control"
                    ],
                    "issn": "2195-268X",
                    "url": "https://www.springer.com/engineering/mechanics/journal/40435",
                    "alternate_urls": [
                        "https://link.springer.com/journal/40435"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/816b7fb3a9929390214ac67b999ef231c3716a5d",
                "title": "Machine learning a molecular Hamiltonian for predicting electron dynamics",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1817843",
                        "name": "H. Bhat"
                    },
                    {
                        "authorId": "46178275",
                        "name": "Karnamohit Ranka"
                    },
                    {
                        "authorId": "6032525",
                        "name": "C. Isborn"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Some methods bypass this problem by learning energetic invariants of the system [16, 17, 18] or exploiting the symplectic structure of the problem [19, 20], reporting promising and interpretable results for Hamiltonian dynamics."
            ],
            "citingPaper": {
                "paperId": "26f6e9238fdf7a196e07dc41a83cc171b4c0feaa",
                "externalIds": {
                    "MAG": "3135975277",
                    "DBLP": "journals/corr/abs-2007-03758",
                    "ArXiv": "2007.03758",
                    "DOI": "10.1016/J.CMA.2021.113763",
                    "CorpusId": 220403576
                },
                "corpusId": 220403576,
                "publicationVenue": {
                    "id": "3bfaa538-a67d-47d7-bfda-6f82748e9a29",
                    "name": "Computer Methods in Applied Mechanics and Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Method Appl Mech Eng"
                    ],
                    "issn": "0045-7825",
                    "url": "https://www.journals.elsevier.com/computer-methods-in-applied-mechanics-and-engineering",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00457825"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/26f6e9238fdf7a196e07dc41a83cc171b4c0feaa",
                "title": "Deep learning of thermodynamics-aware reduced-order models from data",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145556993",
                        "name": "Quercus Hernandez"
                    },
                    {
                        "authorId": "51181453",
                        "name": "A. Badias"
                    },
                    {
                        "authorId": "47723344",
                        "name": "D. Gonz\u00e1lez"
                    },
                    {
                        "authorId": "2734584",
                        "name": "F. Chinesta"
                    },
                    {
                        "authorId": "3246447",
                        "name": "E. Cueto"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian Generative Network [6] learns Hamiltonian dynamics from images.",
                "Baselines We set up two baseline models: HGN [6] and PixelHNN [3].",
                "Hamiltonian Generative Network (HGN) [6] learns Hamiltonian dynamics from image sequences."
            ],
            "citingPaper": {
                "paperId": "45dde4ce5dc531ded482d1fa6048445141a41905",
                "externalIds": {
                    "DBLP": "conf/nips/ZhongL20",
                    "ArXiv": "2007.01926",
                    "MAG": "3038141699",
                    "CorpusId": 220363329
                },
                "corpusId": 220363329,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/45dde4ce5dc531ded482d1fa6048445141a41905",
                "title": "Unsupervised Learning of Lagrangian Dynamics from Images for Prediction and Control",
                "abstract": "Recent approaches for modelling dynamics of physical systems with neural networks enforce Lagrangian or Hamiltonian structure to improve prediction and generalization. However, these approaches fail to handle the case when coordinates are embedded in high-dimensional data such as images. We introduce a new unsupervised neural network model that learns Lagrangian dynamics from images, with interpretability that benefits prediction and control. The model infers Lagrangian dynamics on generalized coordinates that are simultaneously learned with a coordinate-aware variational autoencoder (VAE). The VAE is designed to account for the geometry of physical systems composed of multiple rigid bodies in the plane. By inferring interpretable Lagrangian dynamics, the model learns physical system properties, such as kinetic and potential energy, which enables long-term prediction of dynamics in the image space and synthesis of energy-based controllers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35367989",
                        "name": "Yaofeng Desmond Zhong"
                    },
                    {
                        "authorId": "3301461",
                        "name": "Naomi Ehrich Leonard"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0aa18ed02568c58869b3ebc4d939763813d02c39",
                "externalIds": {
                    "ArXiv": "2007.01287",
                    "MAG": "3110741030",
                    "DOI": "10.1088/1367-2630/ac0b02",
                    "CorpusId": 228096669
                },
                "corpusId": 228096669,
                "publicationVenue": {
                    "id": "8a4f69c8-3ddc-4669-921a-79403732a17e",
                    "name": "New Journal of Physics",
                    "type": "journal",
                    "alternate_names": [
                        "New J Phys"
                    ],
                    "issn": "1367-2630",
                    "url": "http://iopscience.iop.org/1367-2630",
                    "alternate_urls": [
                        "https://iopscience.iop.org/journal/1367-2630",
                        "http://njp.org/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0aa18ed02568c58869b3ebc4d939763813d02c39",
                "title": "Riemannian geometry and automatic differentiation for optimization problems of quantum physics and quantum technologies",
                "abstract": "Optimization with constraints is a typical problem in quantum physics and quantum information science that becomes especially challenging for high-dimensional systems and complex architectures like tensor networks. Here we use ideas of Riemannian geometry to perform optimization on the manifolds of unitary and isometric matrices as well as the cone of positive-definite matrices. Combining this approach with the up-to-date computational methods of automatic differentiation, we demonstrate the efficacy of the Riemannian optimization in the study of the low-energy spectrum and eigenstates of multipartite Hamiltonians, variational search of a tensor network in the form of the multiscale entanglement-renormalization ansatz, preparation of arbitrary states (including highly entangled ones) in the circuit implementation of quantum computation, decomposition of quantum gates, and tomography of quantum states. Universality of the developed approach together with the provided open source software enable one to apply the Riemannian optimization to complex quantum architectures well beyond the listed problems, for instance, to the optimal control of noisy quantum systems.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51243527",
                        "name": "Ilia A. Luchnikov"
                    },
                    {
                        "authorId": "16936643",
                        "name": "M. Krechetov"
                    },
                    {
                        "authorId": "48346810",
                        "name": "S. Filippov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "In the case of natural sciences, applying ML to physics is not new, several works have been reported (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Cranmer et al. 2020; Tong et al. 2020) where different authors have combined",
                "In particular, there is a considerable amount of literature where authors have endowed neural networks with classical Hamiltonian mechanics (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Tong et al. 2020; Iten et al. 2020; Bondesan and Lamacraft 2019; Zhong, Dey, and Chakraborty 2019; Chmiela et al. 2017); conservation of energy and irreversibility in time are the key features of such networks.",
                "In the case of natural sciences, applying ML to physics is not new, several works have been reported (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Cranmer et al. 2020; Tong et al. 2020) where different authors have combined ML with Hamilton\u2019s equations of motion to generate trajectories that obey energy conservation principles and classical physical laws.",
                "For these methods, the authors used a flow based method (Toth et al. 2019; Jimenez Rezende and Mohamed 2015) to increase the expressivity of their variational family of density matrices.",
                "In particular, there is a considerable amount of literature where authors have endowed neural networks with classical Hamiltonian mechanics (Toth et al. 2019; Greydanus, Dzamba, and Yosinski 2019; Tong et al. 2020; Iten et al. 2020; Bondesan and Lamacraft 2019; Zhong, Dey, and Chakraborty 2019;\u2026"
            ],
            "citingPaper": {
                "paperId": "890b3b42b4f988a204d990f46c67b34d6c2c8aa0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-13297",
                    "MAG": "3037780680",
                    "ArXiv": "2006.13297",
                    "CorpusId": 220041548
                },
                "corpusId": 220041548,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/890b3b42b4f988a204d990f46c67b34d6c2c8aa0",
                "title": "Learning Potentials of Quantum Systems using Deep Neural Networks",
                "abstract": "Machine Learning has wide applications in a broad range of subjects, including physics. Recent works have shown that neural networks can learn classical Hamiltonian mechanics. The results of these works motivate the following question: Can we endow neural networks with inductive biases coming from quantum mechanics and provide insights for quantum phenomena? In this work, we try answering these questions by investigating possible approximations for reconstructing the Hamiltonian of a quantum system given one of its wave--functions. Instead of handcrafting the Hamiltonian and a solution of the Schrodinger equation, we design neural networks that aim to learn it directly from our observations. We show that our method, termed Quantum Potential Neural Networks (QPNN), can learn potentials in an unsupervised manner with remarkable accuracy for a wide range of quantum systems, such as the quantum harmonic oscillator, particle in a box perturbed by an external potential, hydrogen atom, Poschl--Teller potential, and a solitary wave system. Furthermore, in the case of a particle perturbed by an external force, we also learn the perturbed wave function in a joint end-to-end manner.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2131857530",
                        "name": "Arijit Sehanobish"
                    },
                    {
                        "authorId": "2400638",
                        "name": "H. Corzo"
                    },
                    {
                        "authorId": "103697788",
                        "name": "Onur Kara"
                    },
                    {
                        "authorId": "7385683",
                        "name": "D. V. Dijk"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "This model is a combination of a Hamiltonian Neural Network [45, 46] and GN."
            ],
            "citingPaper": {
                "paperId": "643ac3ef063c77eb02a3d52637c11fe028bfae28",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-11287",
                    "MAG": "3102090196",
                    "ArXiv": "2006.11287",
                    "CorpusId": 219966125
                },
                "corpusId": 219966125,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/643ac3ef063c77eb02a3d52637c11fe028bfae28",
                "title": "Discovering Symbolic Models from Deep Learning with Inductive Biases",
                "abstract": "We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "32122523",
                        "name": "M. Cranmer"
                    },
                    {
                        "authorId": "1398105826",
                        "name": "Alvaro Sanchez-Gonzalez"
                    },
                    {
                        "authorId": "2019153",
                        "name": "P. Battaglia"
                    },
                    {
                        "authorId": "1606128526",
                        "name": "Rui Xu"
                    },
                    {
                        "authorId": "11638962",
                        "name": "K. Cranmer"
                    },
                    {
                        "authorId": "49071192",
                        "name": "D. Spergel"
                    },
                    {
                        "authorId": "21220036",
                        "name": "S. Ho"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[36] proposed Hamiltonian generative networks to learn the Hamiltonian governing the evolution of a physical system."
            ],
            "citingPaper": {
                "paperId": "4ba2adb484a8dcb712895e18d6801ce595a32e19",
                "externalIds": {
                    "MAG": "3036660750",
                    "DBLP": "conf/nips/VialardKWN20",
                    "ArXiv": "2006.10330",
                    "CorpusId": 219792967
                },
                "corpusId": 219792967,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4ba2adb484a8dcb712895e18d6801ce595a32e19",
                "title": "A Shooting Formulation of Deep Learning",
                "abstract": "Continuous-depth neural networks can be viewed as deep limits of discrete neural networks whose dynamics resemble a discretization of an ordinary differential equation (ODE). Although important steps have been taken to realize the advantages of such continuous formulations, most current techniques are not truly continuous-depth as they assume identical layers. Indeed, existing works throw into relief the myriad difficulties presented by an infinite-dimensional parameter space in learning a continuous-depth neural ODE. To this end, we introduce a shooting formulation which shifts the perspective from parameterizing a network layer-by-layer to parameterizing over optimal networks described only by a set of initial conditions. For scalability, we propose a novel particle-ensemble parametrization which fully specifies the optimal weight trajectory of the continuous-depth neural network. Our experiments show that our particle-ensemble shooting formulation can achieve competitive performance, especially on long-range forecasting tasks. Finally, though the current work is inspired by continuous-depth neural networks, the particle-ensemble shooting formulation also applies to discrete-time networks and may lead to a new fertile area of research in deep learning parametrization.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2988553",
                        "name": "Fran\u00e7ois-Xavier Vialard"
                    },
                    {
                        "authorId": "2132917",
                        "name": "R. Kwitt"
                    },
                    {
                        "authorId": "49752592",
                        "name": "Susan Wei"
                    },
                    {
                        "authorId": "1695046",
                        "name": "M. Niethammer"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian Neural Networks have been augmented to form other architectures as well, such as Hamiltonian Generative Networks [47]."
            ],
            "citingPaper": {
                "paperId": "767e50c134f136784aeee75b1448c947e19bf6ad",
                "externalIds": {
                    "MAG": "3102017980",
                    "ArXiv": "2006.12972",
                    "DBLP": "conf/nips/DiPietroXZ20",
                    "CorpusId": 219980537
                },
                "corpusId": 219980537,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/767e50c134f136784aeee75b1448c947e19bf6ad",
                "title": "Sparse Symplectically Integrated Neural Networks",
                "abstract": "We introduce Sparse Symplectically Integrated Neural Networks (SSINNs), a novel model for learning Hamiltonian dynamical systems from data. SSINNs combine fourth-order symplectic integration with a learned parameterization of the Hamiltonian obtained using sparse regression through a mathematically elegant function space. This allows for interpretable models that incorporate symplectic inductive biases and have low memory requirements. We evaluate SSINNs on four classical Hamiltonian dynamical problems: the Henon-Heiles system, nonlinearly coupled oscillators, a multi-particle mass-spring system, and a pendulum system. Our results demonstrate promise in both system prediction and conservation of energy, outperforming the current state-of-the-art black-box prediction techniques by an order of magnitude. Further, SSINNs successfully converge to true governing equations from highly limited and noisy data, demonstrating potential applicability in the discovery of new physical governing equations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1396992658",
                        "name": "Daniel M. DiPietro"
                    },
                    {
                        "authorId": "47722204",
                        "name": "S. Xiong"
                    },
                    {
                        "authorId": "92035057",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", later developed Hamiltonian Generative Network (HGN), which is capable of consistently learning Hamiltonian dynamics from high-dimensional observations without restrictive domain assumptions [38]."
            ],
            "citingPaper": {
                "paperId": "43aa01607b2327b186f68cde4186f77c7b3f1e61",
                "externalIds": {
                    "ArXiv": "2006.04178",
                    "DBLP": "journals/corr/abs-2006-04178",
                    "MAG": "3033230180",
                    "CorpusId": 219530702
                },
                "corpusId": 219530702,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/43aa01607b2327b186f68cde4186f77c7b3f1e61",
                "title": "Neural Vortex Method: from Finite Lagrangian Particles to Infinite Dimensional Eulerian Dynamics",
                "abstract": "In the field of fluid numerical analysis, there has been a long-standing problem: lacking of a rigorous mathematical tool to map from a continuous flow field to discrete vortex particles, hurdling the Lagrangian particles from inheriting the high resolution of a large-scale Eulerian solver. To tackle this challenge, we propose a novel learning-based framework, the Neural Vortex Method (NVM), which builds a neural-network description of the Lagrangian vortex structures and their interaction dynamics to reconstruct the high-resolution Eulerian flow field in a physically-precise manner. The key components of our infrastructure consist of two networks: a vortex representation network to identify the Lagrangian vortices from a grid-based velocity field and a vortex interaction network to learn the underlying governing dynamics of these finite structures. By embedding these two networks with a vorticity-to-velocity Poisson solver and training its parameters using the high-fidelity data obtained from high-resolution direct numerical simulation, we can predict the accurate fluid dynamics on a precision level that was infeasible for all the previous conventional vortex methods (CVMs). To the best of our knowledge, our method is the first approach that can utilize motions of finite particles to learn infinite dimensional dynamic systems. We demonstrate the efficacy of our method in generating highly accurate prediction results, with low computational cost, of the leapfrogging vortex rings system, the turbulence system, and the systems governed by Euler equations with different external forces.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47722204",
                        "name": "S. Xiong"
                    },
                    {
                        "authorId": "1491320619",
                        "name": "Xingzhe He"
                    },
                    {
                        "authorId": "40495066",
                        "name": "Yunjin Tong"
                    },
                    {
                        "authorId": "92035057",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "296e32a86e1711269289004b140f6d3e0f250348",
                "externalIds": {
                    "DBLP": "conf/icml/KohlerKN20",
                    "ArXiv": "2006.02425",
                    "MAG": "3034497530",
                    "CorpusId": 219260563
                },
                "corpusId": 219260563,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/296e32a86e1711269289004b140f6d3e0f250348",
                "title": "Equivariant Flows: exact likelihood generative learning for symmetric densities",
                "abstract": "Normalizing flows are exact-likelihood generative neural networks which approximately transform samples from a simple prior distribution to samples of the probability distribution of interest. Recent work showed that such generative models can be utilized in statistical mechanics to sample equilibrium states of many-body systems in physics and chemistry. To scale and generalize these results, it is essential that the natural symmetries in the probability density - in physics defined by the invariances of the target potential - are built into the flow. We provide a theoretical sufficient criterion showing that the distribution generated by equivariant normalizing flows is invariant with respect to these symmetries by design. Furthermore, we propose building blocks for flows which preserve symmetries which are usually found in physical/chemical many-body particle systems. Using benchmark systems motivated from molecular physics, we demonstrate that those symmetry preserving flows can provide better generalization capabilities and sampling efficiency.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "32602330",
                        "name": "Jonas K\u00f6hler"
                    },
                    {
                        "authorId": "1380082499",
                        "name": "Leon Klein"
                    },
                    {
                        "authorId": "1967674",
                        "name": "F. No\u00e9"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "2e0bc6cc4153025cd37d957cb896abe237502cd5",
                "externalIds": {
                    "DBLP": "conf/cvpr/JaquesBH21",
                    "ArXiv": "2006.01959",
                    "MAG": "3033735750",
                    "DOI": "10.1109/CVPR46437.2021.00443",
                    "CorpusId": 219259810
                },
                "corpusId": 219259810,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2e0bc6cc4153025cd37d957cb896abe237502cd5",
                "title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces",
                "abstract": "Learning low-dimensional latent state space dynamics models has proven powerful for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of simple and well-known PID controllers. We show that our learned dynamics model enables proportional control from pixels, dramatically simplifies and accelerates behavioural cloning of vision-based controllers, and provides interpretable goal discovery when applied to imitation learning of switching controllers from demonstration. Notably, such proportional controlability also allows for robust path following from visual demonstrations using Dynamic Movement Primitives in the learned latent space.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50425877",
                        "name": "Miguel Jaques"
                    },
                    {
                        "authorId": "145841847",
                        "name": "Michael Burke"
                    },
                    {
                        "authorId": "1697755",
                        "name": "Timothy M. Hospedales"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "09ef364eacfa362d1e9bd8d827fa7e587a028610",
                "externalIds": {
                    "MAG": "2993987425",
                    "DOI": "10.1103/PHYSREVE.101.062207",
                    "CorpusId": 208617695,
                    "PubMed": "32688545"
                },
                "corpusId": 208617695,
                "publicationVenue": {
                    "id": "19842b7b-a4d1-4f9a-9714-87d878cf6e73",
                    "name": "Physical Review E",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev E",
                        "Physical review. E",
                        "Phys Rev E"
                    ],
                    "issn": "1539-3755",
                    "alternate_issns": [
                        "1550-2376",
                        "2470-0045"
                    ],
                    "url": "https://journals.aps.org/pre/",
                    "alternate_urls": [
                        "http://pre.aps.org/",
                        "http://journals.aps.org/pre/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/09ef364eacfa362d1e9bd8d827fa7e587a028610",
                "title": "Physics-enhanced neural networks learn order and chaos.",
                "abstract": "Artificial neural networks are universal function approximators. They can forecast dynamics, but they may need impractically many neurons to do so, especially if the dynamics is chaotic. We use neural networks that incorporate Hamiltonian dynamics to efficiently learn phase space orbits even as nonlinear systems transition from order to chaos. We demonstrate Hamiltonian neural networks on a widely used dynamics benchmark, the H\u00e9non-Heiles potential, and on nonperturbative dynamical billiards. We introspect to elucidate the Hamiltonian neural network forecasting.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "1443437285",
                        "name": "Elliott G. Holliday"
                    },
                    {
                        "authorId": "50783104",
                        "name": "Scott T. Miller"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                ", see [6], [31], [32], [33], [34], and [35]."
            ],
            "citingPaper": {
                "paperId": "2a27fd522de62b66017d1161d8982a596eaa1fc2",
                "externalIds": {
                    "ArXiv": "2006.12745",
                    "DBLP": "journals/corr/abs-2006-12745",
                    "MAG": "3036013675",
                    "CorpusId": 219980655
                },
                "corpusId": 219980655,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2a27fd522de62b66017d1161d8982a596eaa1fc2",
                "title": "Learning Physical Constraints with Neural Projections",
                "abstract": "We propose a new family of neural networks to predict the behaviors of physical systems by learning their underpinning constraints. A neural projection operator liesat the heart of our approach, composed of a lightweight network with an embedded recursive architecture that interactively enforces learned underpinning constraints and predicts the various governed behaviors of different physical systems. Our neural projection operator is motivated by the position-based dynamics model that has been used widely in game and visual effects industries to unify the various fast physics simulators. Our method can automatically and effectively uncover a broad range of constraints from observation point data, such as length, angle, bending, collision, boundary effects, and their arbitrary combinations, without any connectivity priors. We provide a multi-group point representation in conjunction with a configurable network connection mechanism to incorporate prior inputs for processing complex physical systems. We demonstrated the efficacy of our approach by learning a set of challenging physical systems all in a unified and simple fashion including: rigid bodies with complex geometries, ropes with varying length and bending, articulated soft and rigid bodies, and multi-object collisions with complex boundaries.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "14386295",
                        "name": "Shuqi Yang"
                    },
                    {
                        "authorId": "1491320619",
                        "name": "Xingzhe He"
                    },
                    {
                        "authorId": "92035057",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "developed the Hamiltonian Generative Network (HGN), learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions [45]."
            ],
            "citingPaper": {
                "paperId": "7fbd3fe24323781eded5ca428ec649370bf1d03f",
                "externalIds": {
                    "DBLP": "journals/jcphy/TongXHPZ21",
                    "MAG": "3022424628",
                    "ArXiv": "2005.04986",
                    "DOI": "10.1016/j.jcp.2021.110325",
                    "CorpusId": 218580916
                },
                "corpusId": 218580916,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7fbd3fe24323781eded5ca428ec649370bf1d03f",
                "title": "Symplectic Neural Networks in Taylor Series Form for Hamiltonian Systems",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40495066",
                        "name": "Yunjin Tong"
                    },
                    {
                        "authorId": "47722204",
                        "name": "S. Xiong"
                    },
                    {
                        "authorId": "1491320619",
                        "name": "Xingzhe He"
                    },
                    {
                        "authorId": "1689207911",
                        "name": "Guanghan Pan"
                    },
                    {
                        "authorId": "92035057",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "2468c730aa140f1910260aff685d6310b0ad6404",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-13688",
                    "MAG": "3023762998",
                    "CorpusId": 216562707
                },
                "corpusId": 216562707,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2468c730aa140f1910260aff685d6310b0ad6404",
                "title": "VIGN: Variational Integrator Graph Networks",
                "abstract": "Rich, physically-informed inductive biases play an imperative role in accurately modelling the time dynamics of physical systems. In this paper, we introduce Variational Integrator Graph Networks (VIGNs), the first approach to combine a Variational Integrator (VI) inductive bias with a Graph Network (GN) and demonstrate an order of magnitude improvement in performance, both in terms of data-efficient learning and predictive accuracy, over existing methods. We show that this improvement arises because VIs induce coupled learning of generalized position and momentum updates which can be formulated as a Partitioned Runge-Kutta (PRK) method. We empirically establish that VIGN outperforms numerous methods in learning from existing datasets with noise.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144446378",
                        "name": "Shaan Desai"
                    },
                    {
                        "authorId": "143841496",
                        "name": "Stephen J. Roberts"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Hamiltonian system is one of the expressions of classical mechanics and has been applied to a wide range of physics fields from celestial mechanics to quantum field theory [2, 35, 38], and there are also important applications for machine learning [4, 23, 36, 39, 41]."
            ],
            "citingPaper": {
                "paperId": "47e50929eb9358913fcbc1f4cf50952cbe3ab799",
                "externalIds": {
                    "ArXiv": "2004.13830",
                    "DBLP": "journals/corr/abs-2004-13830",
                    "MAG": "3023117547",
                    "CorpusId": 216641670
                },
                "corpusId": 216641670,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/47e50929eb9358913fcbc1f4cf50952cbe3ab799",
                "title": "Deep Hamiltonian networks based on symplectic integrators",
                "abstract": "HNets is a class of neural networks on grounds of physical prior for learning Hamiltonian systems. This paper explains the influences of different integrators as hyper-parameters on the HNets through error analysis. If we define the network target as the map with zero empirical loss on arbitrary training data, then the non-symplectic integrators cannot guarantee the existence of the network targets of HNets. We introduce the inverse modified equations for HNets and prove that the HNets based on symplectic integrators possess network targets and the differences between the network targets and the original Hamiltonians depend on the accuracy orders of the integrators. Our numerical experiments show that the phase flows of the Hamiltonian systems obtained by symplectic HNets do not exactly preserve the original Hamiltonians, but preserve the network targets calculated; the loss of the network target for the training data and the test data is much less than the loss of the original Hamiltonian; the symplectic HNets have more powerful generalization ability and higher accuracy than the non-symplectic HNets in addressing predicting issues. Thus, the symplectic integrators are of critical importance for HNets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The Hamiltonian operator in physics is the primary tool for modeling the time evolution of systems with conserved quantities, but until recently the formalism had not been integrated with NNs. Greydanus et al. [112] design a NN architecture that naturally learns and respects energy conservation and other invariance laws in simple mass-spring or pendulum systems.",
                "For example, in NNs, weights are often initialized according to a random distribution prior to training.",
                "For example, RNNs encode temporal invariance and CNNs can implicitly encode spatial translation, rotation, and scale invariance.",
                "This is taken a step further in Toth et al. [268], where they show that not only can NNs learn the Hamiltonian, but also the abstract phase space (assumed to be known in Greydanus et al. [112]) to more effectively model expressive densities in similar physical systems and also extend more generally to other problems in physics.",
                "There is a vast amount of other work using physics-guided architecture towards solving PDEs and other PDE-related applications as well which are not included in this survey (e.g. see ICLR workshop on deep learning for differential equations ([5]))\nA recent direction also relating to conserved or invariant quantities is the incorporation of the Hamiltonian operator into NNs [64, 112, 268, 317].",
                "A similar transfer and adapt approach is seen in Lu et al. [180], but for an ensemble of NNs transferred from related tasks.",
                "More specifically, they demonstrated the encoding of translational symmetries, rotational symmetries, scale invariances, and uniform motion into NNs using customized convolutional layers in CNNs that enforce desired invariance properties.",
                "A recent direction also relating to conserved or invariant quantities is the incorporation of the Hamiltonian operator into NNs [60, 107, 255, 302].",
                "A recent approach is seen in geophysics where researchers use NNs for the waveform inversion modeling to find subsurface parameters from seismic wave data.",
                "Another common technique in inverse modeling of images (e.g. medical imaging, particle physics imaging), is the use of CNNs as deep image priors [271].",
                "[255], where they show that not only can NNs learn the Hamiltonian, but also the abstract phase space (assumed to be known in Greydanus et al.",
                "Recently, the Hamiltonian-parameterized NNs above have also been expanded into NN architectures that perform additional differential equation-based integration steps based on the derivatives approximated by the Hamiltonian network [61].",
                "This can take place in many ways, including using domain-informed convolutions for CNNs, additional domain-informed discriminators in GANs, or structures informed by the physical characteristics of the problem.",
                "In a general setting, Wang et al. [281] show how spatiotemporal models can be made more generalizable by incorporating symmetries into deep NNs.",
                "They define a parabolic CNN inspired by anisotropic filtering, a hyperbolic CNN based on Hamiltonian systems, and a second order hyperbolic CNN. Hyperbolic CNNs were found to preserve the energy in the system as intended, which set them apart from parabolic CNNs that\n, Vol. 1, No. 1, Article .",
                "Their idea is to use NNs to discover hidden signs of \"simplicity\", such as symmetry or separability in the training data, which enables breaking the massive search space into smaller ones with fewer variables to be determined.",
                "In a pioneering work by Ruthotto et al [236], three variations of CNNs are proposed to improve classifiers for images.",
                "To do this, they substitute NN layers into an unrolled version of an existing solution framework which drastically reduced the overall computational cost due to the fast forward evaluation property of NNs, but kept information of the underlying physical models of power grids and of physical constraints.",
                "Schutt et al. [245] proposes continuous-filter convolutional (cfconv) layers for CNNs to allow for modeling objects with arbitrary positions such as atoms in molecules, in contrast to objects described by Cartesian-gridded data such as images.",
                "The modular and flexible nature of NNs in particular makes them prime candidates for architecture modification.",
                "In particular, NN solvers can reduce the high computational demands of traditional numerical methods into a single forward-pass of a NN. Notably, solutions obtained via NNs are also naturally differentiable and have a closed analytic form that can be transferred to any subsequent calculations, a feature not found in more traditional solving methods [159].",
                "This is similar to the common application of pre-training in computer vision, where CNNs are often pre-trained with very large image datasets before being fine-tuned on images from the task at hand [259].",
                "Though these loss functions are mostly seen in common variants of NNs, they are also be seen in architectures such as echo state networks.",
                "Lagergren et al. [160] expand on this by using ANNs to construct the dictionary of functions."
            ],
            "citingPaper": {
                "paperId": "18b0ca3448bd9703bff86c375362b6ad34f797f7",
                "externalIds": {
                    "ArXiv": "2003.04919",
                    "DBLP": "journals/csur/WillardJXSK23",
                    "DOI": "10.1145/3514228",
                    "CorpusId": 236318599
                },
                "corpusId": 236318599,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/18b0ca3448bd9703bff86c375362b6ad34f797f7",
                "title": "Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems",
                "abstract": "There is a growing consensus that solutions to complex science and engineering problems require novel methodologies that are able to integrate traditional physics-based modeling approaches with state-of-the-art machine learning (ML) techniques. This article provides a structured overview of such techniques. Application-centric objective areas for which these approaches have been applied are summarized, and then classes of methodologies used to construct physics-guided ML models and hybrid physics-ML frameworks are described. We then provide a taxonomy of these existing techniques, which uncovers knowledge gaps and potential crossovers of methods between disciplines that can serve as ideas for future research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51439518",
                        "name": "J. Willard"
                    },
                    {
                        "authorId": "38139853",
                        "name": "X. Jia"
                    },
                    {
                        "authorId": "4632515",
                        "name": "Shaoming Xu"
                    },
                    {
                        "authorId": "1707756",
                        "name": "M. Steinbach"
                    },
                    {
                        "authorId": "2107978833",
                        "name": "Vipin Kumar"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "09e85ad84c4ed40461340ac1bd5fadbd2a5b2340",
                "externalIds": {
                    "MAG": "3010993481",
                    "DBLP": "journals/corr/abs-2003-04919",
                    "CorpusId": 212657607
                },
                "corpusId": 212657607,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/09e85ad84c4ed40461340ac1bd5fadbd2a5b2340",
                "title": "Integrating Physics-Based Modeling with Machine Learning: A Survey",
                "abstract": "In this manuscript, we provide a structured and comprehensive overview of techniques to integrate machine learning with physics-based modeling. First, we provide a summary of application areas for which these approaches have been applied. Then, we describe classes of methodologies used to construct physics-guided machine learning models and hybrid physics-machine learning frameworks from a machine learning standpoint. With this foundation, we then provide a systematic organization of these existing techniques and discuss ideas for future research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51439518",
                        "name": "J. Willard"
                    },
                    {
                        "authorId": "38139853",
                        "name": "X. Jia"
                    },
                    {
                        "authorId": "4632515",
                        "name": "Shaoming Xu"
                    },
                    {
                        "authorId": "1707756",
                        "name": "M. Steinbach"
                    },
                    {
                        "authorId": "2107978833",
                        "name": "Vipin Kumar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent work by Greydanus et al. (2019), Toth et al. (2019), and Chen et al. (2019) built on previous approaches of endowing neural networks with physical priors by demonstrating how to learn invariant quantities by approximating a Hamiltonian with a neural network.",
                "This was the core motivation behind Hamiltonian Neural Networks by Greydanus et al. (2019) and Hamiltonian Generative Networks by Toth et al. (2019)."
            ],
            "citingPaper": {
                "paperId": "1926103a9a5c9099b42652c0192b5fcda571d36f",
                "externalIds": {
                    "MAG": "3021457401",
                    "DBLP": "journals/corr/abs-2003-04630",
                    "ArXiv": "2003.04630",
                    "CorpusId": 212644628
                },
                "corpusId": 212644628,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1926103a9a5c9099b42652c0192b5fcda571d36f",
                "title": "Lagrangian Neural Networks",
                "abstract": "Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "32122523",
                        "name": "M. Cranmer"
                    },
                    {
                        "authorId": "14851288",
                        "name": "S. Greydanus"
                    },
                    {
                        "authorId": "7018631",
                        "name": "Stephan Hoyer"
                    },
                    {
                        "authorId": "2019153",
                        "name": "P. Battaglia"
                    },
                    {
                        "authorId": "49071192",
                        "name": "D. Spergel"
                    },
                    {
                        "authorId": "21220036",
                        "name": "S. Ho"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026parallel, physics and machine learning have been forging strong ties mostly based upon the Hamiltonian theory and the integration of ordinary differential equations in the latent space to describe the evolution of dynamical systems (Chen et al. (2018), Toth et al. (2019), Greydanus et al. (2019))."
            ],
            "citingPaper": {
                "paperId": "081d2ae593cb23ef0100cda759a5a4e297211382",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2002-06991",
                    "MAG": "3006126551",
                    "ArXiv": "2002.06991",
                    "CorpusId": 211133037
                },
                "corpusId": 211133037,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/081d2ae593cb23ef0100cda759a5a4e297211382",
                "title": "Learning Group Structure and Disentangled Representations of Dynamical Environments",
                "abstract": "Discovering the underlying structure of a dynamical environment involves learning representations that are interpretable and disentangled, which is a challenging task. In physics, interpretable representations of our universe and its underlying dynamics are formulated in terms of representations of groups of symmetry transformations. We propose a physics-inspired method, built upon the theory of group representation, that learns a representation of an environment structured around the transformations that generate its evolution. Experimentally, we learn the structure of explicitly symmetric environments without supervision while ensuring the interpretability of the representations. We show that the learned representations allow for accurate long-horizon predictions and further demonstrate a correlation between the quality of predictions and disentanglement in the latent space.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1402910228",
                        "name": "Robin Quessard"
                    },
                    {
                        "authorId": "2067064746",
                        "name": "T. Barrett"
                    },
                    {
                        "authorId": "37289174",
                        "name": "W. Clements"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The molecule has 66 dimensions in x, and we augment it with 66 auxiliary dimensions in a second channel v, similar to \u201cvelocities\u201d in a Hamiltonian flow framework (Toth et al., 2019), resulting in 132 dimensions total.",
                "to \u201cvelocities\u201d in a Hamiltonian flow framework [42], resulting in 132 dimensions total."
            ],
            "citingPaper": {
                "paperId": "5f27443ea5e03d1e876426df613268e68ce58891",
                "externalIds": {
                    "ArXiv": "2002.06707",
                    "DBLP": "journals/corr/abs-2002-06707",
                    "MAG": "3006502260",
                    "CorpusId": 211133098
                },
                "corpusId": 211133098,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5f27443ea5e03d1e876426df613268e68ce58891",
                "title": "Stochastic Normalizing Flows",
                "abstract": "The sampling of probability distributions specified up to a normalization constant is an important problem in both machine learning and statistical mechanics. While classical stochastic sampling methods such as Markov Chain Monte Carlo (MCMC) or Langevin Dynamics (LD) can suffer from slow mixing times there is a growing interest in using normalizing flows in order to learn the transformation of a simple prior distribution to the given target distribution. Here we propose a generalized and combined approach to sample target densities: Stochastic Normalizing Flows (SNF) -- an arbitrary sequence of deterministic invertible functions and stochastic sampling blocks. We show that stochasticity overcomes expressivity limitations of normalizing flows resulting from the invertibility constraint, whereas trainable transformations between sampling steps improve efficiency of pure MCMC/LD along the flow. By invoking ideas from non-equilibrium statistical mechanics we derive an efficient training procedure by which both the sampler's and the flow's parameters can be optimized end-to-end, and by which we can compute exact importance weights without having to marginalize out the randomness of the stochastic blocks. We illustrate the representational power, sampling efficiency and asymptotic correctness of SNFs on several benchmarks including applications to sampling molecular systems in equilibrium.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2119795510",
                        "name": "Hao Wu"
                    },
                    {
                        "authorId": "32602330",
                        "name": "Jonas K\u00f6hler"
                    },
                    {
                        "authorId": "2064855776",
                        "name": "Frank No'e"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5c8e124247df991bce0c192a506a18e37709c0c7",
                "externalIds": {
                    "ArXiv": "2001.11107",
                    "DOI": "10.1103/PhysRevE.105.065305",
                    "CorpusId": 237485607,
                    "PubMed": "35854562"
                },
                "corpusId": 237485607,
                "publicationVenue": {
                    "id": "19842b7b-a4d1-4f9a-9714-87d878cf6e73",
                    "name": "Physical Review E",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev E",
                        "Physical review. E",
                        "Phys Rev E"
                    ],
                    "issn": "1539-3755",
                    "alternate_issns": [
                        "1550-2376",
                        "2470-0045"
                    ],
                    "url": "https://journals.aps.org/pre/",
                    "alternate_urls": [
                        "http://pre.aps.org/",
                        "http://journals.aps.org/pre/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5c8e124247df991bce0c192a506a18e37709c0c7",
                "title": "Hamiltonian neural networks for solving equations\u00a0of motion.",
                "abstract": "There has been a wave of interest in applying machine learning to study dynamical systems. We present a Hamiltonian neural network that solves the differential equations\u00a0that govern dynamical systems. This is an equation-driven machine learning method where the optimization process of the network depends solely on the predicted functions without using any ground truth data. The model learns solutions that satisfy, up to an arbitrarily small error, Hamilton's equations\u00a0and, therefore, conserve the Hamiltonian invariants. The choice of an appropriate activation function drastically improves the predictability of the network. Moreover, an error analysis is derived and states that the numerical errors depend on the overall network performance. The Hamiltonian network is then employed to solve the equations\u00a0for the nonlinear oscillator and the chaotic H\u00e9non-Heiles dynamical system. In both systems, a symplectic Euler integrator requires two orders more evaluation points than the Hamiltonian network to achieve the same order of the numerical error in the predicted phase space trajectories.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145324933",
                        "name": "M. Mattheakis"
                    },
                    {
                        "authorId": "144633639",
                        "name": "David Sondak"
                    },
                    {
                        "authorId": "1735677",
                        "name": "P. Protopapas"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u20262019; Greydanus et al., 2019; Rezende et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2020; Toth et al., 2020; Zhong et al., 2020), with further applications in image prediction (Greydanus et al., 2019), generative modeling (Toth et al., 2020) and continuous control (Zhong et al., 2020).",
                "\u2026systems from data (Bertalan et al., 2019; Greydanus et al., 2019; Rezende et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2020; Toth et al., 2020; Zhong et al., 2020), with further applications in image prediction (Greydanus et al., 2019), generative modeling (Toth et al.,\u2026"
            ],
            "citingPaper": {
                "paperId": "31e0726182237d47a5eecce749bf9a1f3713e2e9",
                "externalIds": {
                    "DBLP": "journals/nn/JinZZTK20",
                    "MAG": "3081814969",
                    "DOI": "10.1016/j.neunet.2020.08.017",
                    "CorpusId": 219766063,
                    "PubMed": "32890788"
                },
                "corpusId": 219766063,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31e0726182237d47a5eecce749bf9a1f3713e2e9",
                "title": "SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "2116702527",
                        "name": "Zhen Zhang"
                    },
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    },
                    {
                        "authorId": "1720124",
                        "name": "G. Karniadakis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "k-based models have been proposed to identify the Hamiltonian systems from data (Bertalan et al., 2019; Greydanus et al., 2019; Rezende et al., 2019; Sanchez-Gonzalez et al., 2019; Chen et al., 2020; Toth et al., 2020; Zhong et al., 2020), with further applications in image prediction (Greydanus et al., 2019), generative modeling (Toth et al., 2020) and continuous control (Zhong et al., 2020). These learning model"
            ],
            "citingPaper": {
                "paperId": "51faee8c0d8c58d03bf2a3923d6d5ddc8107aeff",
                "externalIds": {
                    "ArXiv": "2001.03750",
                    "MAG": "3000406748",
                    "DBLP": "journals/corr/abs-2001-03750",
                    "CorpusId": 210164521
                },
                "corpusId": 210164521,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/51faee8c0d8c58d03bf2a3923d6d5ddc8107aeff",
                "title": "Symplectic networks: Intrinsic structure-preserving networks for identifying Hamiltonian systems",
                "abstract": "This work presents a framework of constructing the neural networks preserving the symplectic structure, so-called symplectic networks (SympNets). With the symplectic networks, we show some numerical results about (\\romannumeral1) solving the Hamiltonian systems by learning abundant data points over the phase space, and (\\romannumeral2) predicting the phase flows by learning a series of points depending on time. All the experiments point out that the symplectic networks perform much more better than the fully-connected networks that without any prior information, especially in the task of predicting which is unable to do within the conventional numerical methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "2430811",
                        "name": "Aiqing Zhu"
                    },
                    {
                        "authorId": "1720124",
                        "name": "G. Karniadakis"
                    },
                    {
                        "authorId": "1805110",
                        "name": "Yifa Tang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Learning physically relevant concepts of the two body problem was recently solved both by Hamiltonian Neural Networks [12, 13] and by very problem specific VAE architectures [14]."
            ],
            "citingPaper": {
                "paperId": "0ca2387bb31b743b6dfe3fc537390c34a226252f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1912-00656",
                    "MAG": "2989686409",
                    "CorpusId": 208527676
                },
                "corpusId": 208527676,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0ca2387bb31b743b6dfe3fc537390c34a226252f",
                "title": "Rodent: Relevance determination in ODE",
                "abstract": "From a set of observed trajectories of a partially observed system, we aim to learn its underlying (physical) process without having to make too many assumptions about the generating model. We start with a very general, over-parameterized ordinary differential equation (ODE) of order N and learn the minimal complexity of the model, by which we mean both the order of the ODE as well as the minimum number of non-zero parameters that are needed to solve the problem. The minimal complexity is found by combining the Variational Auto-Encoder (VAE) with Automatic Relevance Determination (ARD) to the problem of learning the parameters of an ODE which we call Rodent. We show that it is possible to learn not only one specific model for a single process, but a manifold of models representing harmonic signals in general.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "153546490",
                        "name": "Niklas Heim"
                    },
                    {
                        "authorId": "10192900",
                        "name": "V. \u0160m\u00eddl"
                    },
                    {
                        "authorId": "1802358",
                        "name": "T. Pevn\u00fd"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5dc913f410bcf84f4656dca6216b4ab047c0dffc",
                "externalIds": {
                    "PubMedCentral": "7652943",
                    "MAG": "2982172145",
                    "DOI": "10.1038/s41598-020-76301-0",
                    "CorpusId": 204837958,
                    "PubMed": "33168886"
                },
                "corpusId": 204837958,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5dc913f410bcf84f4656dca6216b4ab047c0dffc",
                "title": "Machine learning and serving of discrete field theories",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "145199627",
                        "name": "H. Qin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4cdda86d5bc2396d0dc766abc955b4bd4de22a24",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1910-10147",
                    "ArXiv": "1910.10147",
                    "MAG": "2993196139",
                    "CorpusId": 210077753
                },
                "corpusId": 210077753,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4cdda86d5bc2396d0dc766abc955b4bd4de22a24",
                "title": "Machine learning and serving of discrete field theories - when artificial intelligence meets the discrete universe",
                "abstract": "A method for machine learning and serving of discrete field theories in physics is developed. The learning algorithm trains a discrete field theory from a set of observational data on a spacetime lattice, and the serving algorithm uses the learned discrete field theory to predict new observations of the field for new boundary and initial conditions. The approach to learn discrete field theories overcomes the difficulties associated with learning continuous theories by artificial intelligence. The serving algorithm of discrete field theories belongs to the family of structure-preserving geometric algorithms, which have been proven to be superior to the conventional algorithms based on discretization of differential equations. The effectiveness of the method and algorithms developed is demonstrated using the examples of nonlinear oscillations and the Kepler problem. In particular, the learning algorithm learns a discrete field theory from a set of data of planetary orbits similar to what Kepler inherited from Tycho Brahe in 1601, and the serving algorithm correctly predicts other planetary orbits, including parabolic and hyperbolic escaping orbits, of the solar system without learning or knowing Newton's laws of motion and universal gravitation. The proposed algorithms are also applicable when effects of special relativity and general relativity are important. The illustrated advantages of discrete field theories relative to continuous theories in terms of machine learning compatibility are consistent with Bostrom's simulation hypothesis.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "145199627",
                        "name": "H. Qin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "03547cf81db895a448c3d0283bdfa20695ed26ab",
                "externalIds": {
                    "MAG": "3137474564",
                    "DBLP": "journals/corr/abs-1910-03193",
                    "ArXiv": "1910.03193",
                    "DOI": "10.1038/s42256-021-00302-5",
                    "CorpusId": 233822586
                },
                "corpusId": 233822586,
                "publicationVenue": {
                    "id": "6457124b-39bf-4d02-bff4-73752ff21562",
                    "name": "Nature Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Nat Mach Intell"
                    ],
                    "issn": "2522-5839",
                    "url": "https://www.nature.com/natmachintell/"
                },
                "url": "https://www.semanticscholar.org/paper/03547cf81db895a448c3d0283bdfa20695ed26ab",
                "title": "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2149373363",
                        "name": "Lu Lu"
                    },
                    {
                        "authorId": "121687074",
                        "name": "Pengzhan Jin"
                    },
                    {
                        "authorId": "2867058",
                        "name": "G. Pang"
                    },
                    {
                        "authorId": "150358909",
                        "name": "Zhongqiang Zhang"
                    },
                    {
                        "authorId": "1720124",
                        "name": "G. Karniadakis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "00c6a6bd4b5d8a66b088fa1363a865624037781a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1910-00024",
                    "ArXiv": "1910.00024",
                    "MAG": "2979026759",
                    "DOI": "10.1103/PhysRevX.10.021020",
                    "CorpusId": 203610605
                },
                "corpusId": 203610605,
                "publicationVenue": {
                    "id": "98eedf55-1e67-4c3d-a25d-79861b87ae04",
                    "name": "Physical Review X",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Rev X"
                    ],
                    "issn": "2160-3308",
                    "url": "https://journals.aps.org/prx/",
                    "alternate_urls": [
                        "http://journals.aps.org/prx/",
                        "http://prx.aps.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/00c6a6bd4b5d8a66b088fa1363a865624037781a",
                "title": "Neural Canonical Transformation with Symplectic Flows",
                "abstract": "Canonical transformation plays a fundamental role in simplifying and solving classical Hamiltonian systems. We construct flexible and powerful canonical transformations as generative models using symplectic neural networks. The model transforms physical variables towards a latent representation with an independent harmonic oscillator Hamiltonian. Correspondingly, the phase space density of the physical system flows towards a factorized Gaussian distribution in the latent space. Since the canonical transformation preserves the Hamiltonian evolution, the model captures nonlinear collective modes in the learned latent representation. We present an efficient implementation of symplectic neural coordinate transformations and two ways to train the model. The variational free energy calculation is based on the analytical form of physical Hamiltonian. While the phase space density estimation only requires samples in the coordinate space for separable Hamiltonians. We demonstrate appealing features of neural canonical transformation using toy problems including two-dimensional ring potential and harmonic chain. Finally, we apply the approach to real-world problems such as identifying slow collective modes in alanine dipeptide and conceptual compression of the MNIST dataset.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2109135534",
                        "name": "Shuo-Hui Li"
                    },
                    {
                        "authorId": "2113541017",
                        "name": "Chen Dong"
                    },
                    {
                        "authorId": "2125538501",
                        "name": "Linfeng Zhang"
                    },
                    {
                        "authorId": "48169641",
                        "name": "Lei Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Relatedly, deep-learning-based approaches for enforcing conservations laws include (1) designing neural networks that can learn arbitrary conservation laws (hyperbolic conservation laws (Raissi, Perdikaris, and Karniadakis 2019), Hamiltonian dynamics (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019; Chen et al. 2019), Lagrangian dynamics (Cranmer et al.",
                "\u2026laws (hyperbolic conservation laws (Raissi, Perdikaris, and Karniadakis 2019), Hamiltonian dynamics (Greydanus, Dzamba, and Yosinski 2019; Toth et al. 2019; Chen et al. 2019), Lagrangian dynamics (Cranmer et al. 2020)), or (2) designing a loss function or adding an extra neural network\u2026"
            ],
            "citingPaper": {
                "paperId": "02327b889e75a9d86d4351cfea7c17e730d8efa4",
                "externalIds": {
                    "ArXiv": "1909.09754",
                    "DBLP": "conf/aaai/LeeC21",
                    "MAG": "2974114687",
                    "DOI": "10.2172/1569346",
                    "CorpusId": 202718772
                },
                "corpusId": 202718772,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/02327b889e75a9d86d4351cfea7c17e730d8efa4",
                "title": "Deep Conservation: A Latent-Dynamics Model for Exact Satisfaction of Physical Conservation Laws",
                "abstract": "This work proposes an approach for latent-dynamics learning that exactly enforces physical conservation laws. The method comprises two steps. First, the method computes a low-dimensional embedding of the high-dimensional dynamical-system state using deep convolutional autoencoders. This defines a low-dimensional nonlinear manifold on which the state is subsequently enforced to evolve. Second, the method defines a latent-dynamics model that associates with the solution to a constrained optimization problem. Here, the objective function is defined as the sum of squares of conservation-law violations over control volumes within a finite-volume discretization of the problem; nonlinear equality constraints explicitly enforce conservation over prescribed subdomains of the problem. Under modest conditions, the resulting dynamics model guarantees that the time-evolution of the latent state exactly satisfies conservation laws over the prescribed subdomains.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "3041967",
                        "name": "Kookjin Lee"
                    },
                    {
                        "authorId": "33892852",
                        "name": "K. Carlberg"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8cc693a47f717ed9f680f560c6fb71514d9a5a7b",
                "externalIds": {
                    "MAG": "2966473221",
                    "ArXiv": "1907.12715",
                    "DOI": "10.1063/1.5128231",
                    "CorpusId": 198985969,
                    "PubMed": "31893645"
                },
                "corpusId": 198985969,
                "publicationVenue": {
                    "id": "30c0ded7-c8b4-473c-bbc0-f237234ac1a6",
                    "name": "Chaos",
                    "type": "journal",
                    "issn": "1054-1500",
                    "url": "http://chaos.aip.org/",
                    "alternate_urls": [
                        "https://aip.scitation.org/journal/cha"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8cc693a47f717ed9f680f560c6fb71514d9a5a7b",
                "title": "On learning Hamiltonian systems from data.",
                "abstract": "Concise, accurate descriptions of physical systems through their conserved quantities abound in the natural sciences. In data science, however, current research often focuses on regression problems, without routinely incorporating additional assumptions about the system that generated the data. Here, we propose to explore a particular type of underlying structure in the data: Hamiltonian systems, where an \"energy\" is conserved. Given a collection of observations of such a Hamiltonian system over time, we extract phase space coordinates and a Hamiltonian function of them that acts as the generator of the system dynamics. The approach employs an autoencoder neural network component to estimate the transformation from observations to the phase space of a Hamiltonian system. An additional neural network component is used to approximate the Hamiltonian function on this constructed space, and the two components are trained jointly. As an alternative approach, we also demonstrate the use of Gaussian processes for the estimation of such a Hamiltonian. After two illustrative examples, we extract an underlying phase space as well as the generating Hamiltonian from a collection of movies of a pendulum. The approach is fully data-driven and does not assume a particular form of the Hamiltonian function.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "31588522",
                        "name": "Tom S. Bertalan"
                    },
                    {
                        "authorId": "144399320",
                        "name": "Felix Dietrich"
                    },
                    {
                        "authorId": "31263739",
                        "name": "Igor Mezi'c"
                    },
                    {
                        "authorId": "3439407",
                        "name": "I. Kevrekidis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "This idea also applies to learning the conserved quantities from images (Toth et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "df0dc45dbfdc6525cb210b16d83b7a4ef873b1ca",
                "externalIds": {
                    "DBLP": "conf/iclr/LiangHZ22",
                    "CorpusId": 251648970
                },
                "corpusId": 251648970,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/df0dc45dbfdc6525cb210b16d83b7a4ef873b1ca",
                "title": "Stiffness-aware neural network for learning Hamiltonian systems",
                "abstract": "We propose stiffness-aware neural network (SANN), a new method for learning Hamiltonian dynamical systems from data. SANN identi\ufb01es and splits the training data into stiff and nonstiff portions based on a stiffness-aware index, a simple, yet effective metric we introduce to quantify the stiffness of the dynamical system. This classi\ufb01cation along with a resampling technique allows us to apply different time integration strategies such as step size adaptation to better capture the dynamical characteristics of the Hamiltonian vector \ufb01elds. We evaluate SANN on complex physical systems including a three-body problem and billiard model. We show that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods, leading to signi\ufb01cant improvement in accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "116746634",
                        "name": "Senwei Liang"
                    },
                    {
                        "authorId": "2109670338",
                        "name": "Zhongzhan Huang"
                    },
                    {
                        "authorId": "2146244223",
                        "name": "Hong Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40] exploit Lagrangian or Hamiltonian mechanics to learn an energy-conserving system based on position, momentum, and the derivatives thereof along trajectories.",
                "This problem setup is distinct from that of HNN [33], HGN [36], or Symplectic ODE-Net [35]."
            ],
            "citingPaper": {
                "paperId": "510b5f11bf1ea270f8cc33f6b16e15102bb2ee3d",
                "externalIds": {
                    "DBLP": "conf/nips/YangRNR22",
                    "CorpusId": 254994240
                },
                "corpusId": 254994240,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/510b5f11bf1ea270f8cc33f6b16e15102bb2ee3d",
                "title": "Learning Physics Constrained Dynamics Using Autoencoders",
                "abstract": "We consider the problem of estimating states ( e.g., position and velocity) and physical parameters ( e.g., friction, elasticity) from a sequence of observations when provided a dynamic equation that describes the behavior of the system. The dynamic equation can arise from first principles ( e.g., Newton\u2019s laws) and provide useful cues for learning, but its physical parameters are unknown. To address this problem, we propose a model that estimates states and physical parameters of the system using two main components. First, an autoencoder compresses a sequence of observations ( e.g., sensor measurements, pixel images) into a sequence for the state representation that is consistent with physics by including a simulation of the dynamic equation. Second, an estimator is coupled with the autoencoder to predict the values of the physical parameters. We also theoretically and empirically show that using Fourier feature mappings improves the generalization of the estimator in predicting physical parameters compared to raw state sequences when learning from high-frequency data. In our experiments on three visual and one sensor measurement tasks, our model imposes interpretability on latent states and achieves improved generalization performance for long-term prediction of system dynamics over state-of-the-art baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "11844404",
                        "name": "Tsung-Yen Yang"
                    },
                    {
                        "authorId": "2085202",
                        "name": "J. Rosca"
                    },
                    {
                        "authorId": "144958935",
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "authorId": "2133868318",
                        "name": "Peter J. Ramadge"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "These methods are successfully applied to a variety of tasks, such as the generative tasks [17] and the dynamics reconstruction [20, 15], sharing the similar design idea where utilisation of an appropriate loss function enforces the model to nearly obey the physical principles."
            ],
            "citingPaper": {
                "paperId": "e6b1a3381a758ac5105444d2a6a80e61023d3c3d",
                "externalIds": {
                    "CorpusId": 259841301
                },
                "corpusId": 259841301,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e6b1a3381a758ac5105444d2a6a80e61023d3c3d",
                "title": "Hamiltonian Neural Koopman Operator",
                "abstract": "Recently, physics-informed learning, a class of deep learning framework that in-corporates the physics priors and the observational noise-perturbed data into the neural network models, has shown outstanding performances in learning physical principles with higher accuracy, faster training speed, and better generalization ability. Here, for the Hamiltonian mechanics and using the Koopman operator theory, we propose a typical physics-informed learning framework, named as H amiltonian N eural K oopman O perator (HNKO) to learn the corresponding Koopman operator automatically satisfying the conservation laws. We analytically investigate the dimension of the manifold induced by the orthogonal transformation, and use a modified auto-encoder to identify the nonlinear coordinate transformation that is required for approximating the Koopman operator. Taking the Kepler problem as an example, we demonstrate that the proposed HNKO in robustly learning the Hamiltonian dynamics outperforms the representative methods developed in the literature. Our results suggest that feeding the prior knowledge of the underlying system and the mathematical theory appropriately to the learning framework can reinforce the capability of the deep learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2223153066",
                        "name": "Jingdong Zhang"
                    },
                    {
                        "authorId": "27716042",
                        "name": "Qunxi Zhu"
                    },
                    {
                        "authorId": "2113563142",
                        "name": "Wei Lin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We can easily extend our proposed approach to learn Hamiltonians from high-dimensional data (such as images) by combining an autoencoder with an SSGP, as in [14, 42]."
            ],
            "citingPaper": {
                "paperId": "17f9fb7be352d5849a97e3e8c39829e6b33830d6",
                "externalIds": {
                    "DBLP": "conf/nips/0002IU22",
                    "CorpusId": 258509659
                },
                "corpusId": 258509659,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/17f9fb7be352d5849a97e3e8c39829e6b33830d6",
                "title": "Symplectic Spectrum Gaussian Processes: Learning Hamiltonians from Noisy and Sparse Data",
                "abstract": "Hamiltonian mechanics is a well-established theory for modeling the time evolution of systems with conserved quantities (called Hamiltonian ), such as the total energy of the system. Recent works have parameterized the Hamiltonian by machine learning models (e.g., neural networks), allowing Hamiltonian dynamics to be obtained from state trajectories without explicit mathematical modeling. However, the performance of existing models is limited as we can observe only noisy and sparse trajectories in practice. This paper proposes a probabilistic model that can learn the dynamics of conservative or dissipative systems from noisy and sparse data. We introduce a Gaussian process that incorporates the symplectic geometric structure of Hamiltonian systems, which is used as a prior distribution for estimating Hamiltonian systems with additive dissipation. We then present its spectral representation, Symplectic Spectrum Gaussian Processes (SSGPs) , for which we newly derive random Fourier features with symplectic structures. This allows us to construct an ef\ufb01cient variational inference algorithm for training the models while simulating the dynamics via ordinary differential equation solvers. Experiments on several physical systems show that SSGP offers excellent performance in predicting dynamics that follow the energy conservation or dissipation law from noisy and sparse data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143668549",
                        "name": "Yusuke Tanaka"
                    },
                    {
                        "authorId": "2664600",
                        "name": "Tomoharu Iwata"
                    },
                    {
                        "authorId": "1735221",
                        "name": "N. Ueda"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, neural networks have been proposed [15, 16] that not only learn the dynamics of the system but also",
                "Recent research [15, 16] features artificial neural networks that incorporate Hamiltonian structure to learn"
            ],
            "citingPaper": {
                "paperId": "a14cf952777549efe62ba0c9e7fea75de84bc2bc",
                "externalIds": {
                    "CorpusId": 235484512
                },
                "corpusId": 235484512,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a14cf952777549efe62ba0c9e7fea75de84bc2bc",
                "title": "Physics enhanced neural networks predict order and chaos",
                "abstract": "Anshul Choudhary, John F. Lindner*, 2 Elliott G. Holliday, Scott T. Miller, Sudeshna Sinha, 3 and William L. Ditto Nonlinear Artificial Intelligence Laboratory, Physics Department, North Carolina State University, Raleigh, NC 27607, USA Physics Department, The College of Wooster, Wooster, OH 44691, USA Indian Institute of Science Education and Research Mohali, Knowledge City, SAS Nagar, Sector 81, Manauli PO 140 306, Punjab, India (Dated: May 29, 2021)",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "1443437285",
                        "name": "Elliott G. Holliday"
                    },
                    {
                        "authorId": "50783104",
                        "name": "Scott T. Miller"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "51d666dec6e752357d7e613258bd9064d08b3598",
                "externalIds": {
                    "MAG": "3034580688",
                    "DOI": "10.1016/b978-0-12-801238-3.11595-8",
                    "CorpusId": 224882759
                },
                "corpusId": 224882759,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/51d666dec6e752357d7e613258bd9064d08b3598",
                "title": "Organism-Wide Physiological Systems",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145002362",
                        "name": "Peter Hunter"
                    },
                    {
                        "authorId": "1705888",
                        "name": "B. Bono"
                    },
                    {
                        "authorId": "144242906",
                        "name": "D. Nickerson"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "HGN maps a sequence to a latent representation and then projects it to the phase space to unroll the dynamics using an ODE integrator with Hamilton\u2019s equation.",
                "In a very recent work, Hamiltonian generative network (HGN) [8] proposed to learn Hamiltonian from image sequences.",
                "Identifying fundamental symmetries is essential for developing expressive deep generative models (DGMs) that understand the motion constraints and can generalise beyond the training data [7, 8]."
            ],
            "citingPaper": {
                "paperId": "2d445a80f3510a8b7f92ccc2e21c35ee5af8aa57",
                "externalIds": {
                    "CorpusId": 244896126
                },
                "corpusId": 244896126,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2d445a80f3510a8b7f92ccc2e21c35ee5af8aa57",
                "title": "Hamiltonian prior to Disentangle Content and Motion in Image Sequences",
                "abstract": "We present a deep latent variable model for high dimensional sequential data. Our model factorises the latent space into content and motion variables. To model the diverse dynamics, we split the motion space into subspaces, and introduce a unique Hamiltonian operator for each subspace. The Hamiltonian formulation provides reversible dynamics that learn to constrain the motion path to conserve invariant properties. The explicit split of the motion space decomposes the Hamiltonian into symmetry groups and gives long-term separability of the dynamics. This split also means representations can be learnt that are easy to interpret and control. We demonstrate the utility of our model for swapping the motion of two videos, generating sequences of various actions from a given image and unconditional sequence generation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2149889657",
                        "name": "Asif Khan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Some Hamiltonian methods (Toth et al., 2019; Yildiz et al., 2019) also model the dynamics of high dimensional sequential data in a latent space.",
                "(Higgins et al., 2018; Toth et al., 2019; Botev et al., 2021) have discussed the benefits of such inductive biases for learning disentangled representation.",
                "Recently Toth et al. (2019) developed the Hamiltonian generative network (HGN), where they proposed to learn a Hamiltonian from image sequences."
            ],
            "citingPaper": {
                "paperId": "47046680631ce273867b51f58ff0c6a97105e982",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-01641",
                    "CorpusId": 246411653
                },
                "corpusId": 246411653,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/47046680631ce273867b51f58ff0c6a97105e982",
                "title": "Hamiltonian Operator Disentanglement of Content and Motion in Image Sequences",
                "abstract": "We introduce a deep generative model for image sequences that reliably factorise the latent space into content and motion variables. To model the diverse dynamics, we split the motion space into subspaces and introduce a unique Hamiltonian operator for each subspace. The Hamiltonian formulation provides reversible dynamics that constrain the evolution of the motion path along the low-dimensional manifold and conserves learnt invariant properties. The explicit split of the motion space decomposes the Hamiltonian into symmetry groups and gives long-term separability of the dynamics. This split also means we can learn content representations that are easy to interpret and control. We demonstrate the utility of our model by swapping the motion of two videos, generating long term sequences of various actions from a given image, unconditional sequence generation and image rotations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109256663",
                        "name": "M. A. Khan"
                    },
                    {
                        "authorId": "1728216",
                        "name": "A. Storkey"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ffc6b8443c380796f929b6810f50e1e2065220aa",
                "externalIds": {
                    "DBLP": "conf/nips/GruffazPMJD21",
                    "CorpusId": 247656167
                },
                "corpusId": 247656167,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ffc6b8443c380796f929b6810f50e1e2065220aa",
                "title": "Learning Riemannian metric for disease progression modeling",
                "abstract": "Linear mixed-effect models provide a natural baseline for estimating disease progression using longitudinal data. They provide interpretable models at the cost of modeling assumptions on the progression pro\ufb01les and their variability across subjects. A signi\ufb01cant improvement is to embed the data in a Riemannian manifold and learn patient-speci\ufb01c trajectories distributed around a central geodesic. A few interpretable parameters characterize subject trajectories at the cost of a prior choice of the metric, which determines the shape of the trajectories. We extend this approach by learning the metric from the data allowing more \ufb02exibility while keeping the interpretability. Speci\ufb01cally, we learn the metric as the push-forward of the Euclidean metric by a diffeomorphism. This diffeomorphism is estimated iteratively as the composition of radial basis functions belonging to a reproducible kernel Hilbert space. The metric update allows us to improve the forecasting of imaging and clinical biomarkers in the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) cohort. Our results compare favorably to the 56 methods benchmarked in the TADPOLE challenge.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31234484",
                        "name": "S. Gruffaz"
                    },
                    {
                        "authorId": "2114694601",
                        "name": "Pierre-Emmanuel Poulet"
                    },
                    {
                        "authorId": "2032727402",
                        "name": "Etienne Maheux"
                    },
                    {
                        "authorId": "2521020",
                        "name": "B. Jedynak"
                    },
                    {
                        "authorId": "2689496",
                        "name": "S. Durrleman"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We will also investigate the integration of a physical model that uses physics as a model prior as in [26, 22, 27]."
            ],
            "citingPaper": {
                "paperId": "8c75849cee7b50f91ad6cdcc31f31da2743de9d1",
                "externalIds": {
                    "DBLP": "conf/corl/RichardAGP21",
                    "CorpusId": 237277299
                },
                "corpusId": 237277299,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8c75849cee7b50f91ad6cdcc31f31da2743de9d1",
                "title": "Learning Behaviors through Physics-driven Latent Imagination",
                "abstract": ": Model-based reinforcement learning (MBRL) consists in learning a so- 1 called world model, a representation of the environment through interactions with 2 it, then use it to train an agent. This approach is particularly interesting in the con- 3 text of \ufb01eld robotics, as it alleviates the need to train online, and reduces the risks 4 inherent to directly training agents on real robots. However, in such approaches, the 5 world encompasses both the part related to the robot itself and the rest of the environ- 6 ment. We argue that decoupling the environment representation (for example, im- 7 ages or laser scans) from the dynamics of the physical system (that is, the robot and 8 its physical state) may be bene\ufb01cial: it can increase the \ufb02exibility of world models 9 and open the door to further robusti\ufb01cation. In this paper, we apply this concept to a 10 strong latent-agent, Dreamer. We then showcase the increase of \ufb02exibility by trans- 11 ferring the environmental part of the world model from one robot (a boat) to another 12 (a rover), simply by adapting the physical model in the imagination. We also demon- 13 strate the robustness of our method through real-world experiments on a boat. 14",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50589695",
                        "name": "Antoine Richard"
                    },
                    {
                        "authorId": "83755922",
                        "name": "St\u00e9phanie Aravecchia"
                    },
                    {
                        "authorId": "1737555",
                        "name": "M. Geist"
                    },
                    {
                        "authorId": "1689803",
                        "name": "C. Pradalier"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "87621506aefb6023a814141744bab386cee52d96",
                "externalIds": {
                    "MAG": "3140504026",
                    "DOI": "10.1587/NOLTA.12.134",
                    "CorpusId": 234201303
                },
                "corpusId": 234201303,
                "publicationVenue": {
                    "id": "8326ad98-e9ee-4518-81dd-5ccaa5331c05",
                    "name": "Nonlinear Theory and Its Applications IEICE",
                    "type": "journal",
                    "alternate_names": [
                        "Nonlinear Theory and Its Applications, IEICE",
                        "Nonlinear Theory It Appl IEICE"
                    ],
                    "issn": "2185-4106",
                    "url": "https://www.jstage.jst.go.jp/browse/nolta",
                    "alternate_urls": [
                        "https://www.jstage.jst.go.jp/browse/nolta/list/-char/en",
                        "http://www.nolta.ieice.org/data/archives/archives.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/87621506aefb6023a814141744bab386cee52d96",
                "title": "Negotiating the separatrix with machine learning",
                "abstract": ": Physics-informed machine learning has recently been shown to e\ufb03ciently learn complex trajectories of nonlinear dynamical systems, even when order and chaos coexist. However, care must be taken when one or more variables are unbounded, such as in rotations. Here we use the framework of Hamiltonian Neural Networks (HNN) to learn the complex dynamics of nonlinear single and double pendulums, which can both librate and rotate, by mapping the unbounded phase space onto a compact cylinder. We clearly demonstrate that our approach can successfully forecast the motion of these challenging systems, capable of both bounded and unbounded motion. It is also evident that HNN can yield an energy surface that closely matches the surface generated by the true Hamiltonian function. Further we observe that the relative energy error for HNN decreases as a power law with number of training pairs, with HNN clearly outperforming conventional neural networks quantitatively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50783104",
                        "name": "Scott T. Miller"
                    },
                    {
                        "authorId": "2468955",
                        "name": "J. Lindner"
                    },
                    {
                        "authorId": "145638518",
                        "name": "A. Choudhary"
                    },
                    {
                        "authorId": "2053142",
                        "name": "S. Sinha"
                    },
                    {
                        "authorId": "3067672",
                        "name": "W. Ditto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8138d2e2d01870d9a2f6eda5dee0c53d84567249",
                "externalIds": {
                    "CorpusId": 253253106
                },
                "corpusId": 253253106,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8138d2e2d01870d9a2f6eda5dee0c53d84567249",
                "title": "L EARNED S IMULATORS THAT SATISFY THE L AWS OF T HERMODYNAMICS",
                "abstract": "of in systems positive of tion scheme Energy-entropy-momentum integration schemes for general discrete non-smooth dissipative problems in thermomechanics. on",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "69035385",
                        "name": "B. Moya"
                    },
                    {
                        "authorId": "51181453",
                        "name": "A. Badias"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5b47424a98482f9263ce5ec1a6a43f6594dc427f",
                "externalIds": {
                    "CorpusId": 226312826
                },
                "corpusId": 226312826,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5b47424a98482f9263ce5ec1a6a43f6594dc427f",
                "title": "Machine Learning and Neural Networks for Field Theory",
                "abstract": "Perturbative acceleration [1\u20133]. The ability to efficiently sample from high-dimensional distributions remains a widely-pursued goal across scientific disciplines, with some noteable examples including particle physics, molecular dynamics, protein folding, population genetics, neuroscience, epidemiology, economics, ecology, climate science, and astrophysics & cosmology [4]. Recently, there has been a growing interest in developing probabilistic models that are parameterized by neural networks, and while much progress has been made in this direction [5\u201310], mitigating the critical slowing down (CSD) effect for lattice QCD remains a long-term goal of the community. Since all lattice QCD simulations are performed at finite lattice spacing a, an extrapolation to the continuum limit is required in order to accurately compute physical quantities of interest. More reliable extrapolations can be done by simulating the theory at increasingly smaller lattice spacings while keeping the physics constant. Unfortunately, this causes the correlation times of these quantities to diverge, indicating that the continuum limit is a critical point of the theory. Markov Chain Monte Carlo (MCMC) algorithms are known to encounter difficulties when simulating theories near a critical point, an issue known as the critical slowing down of the algorithm [11]. This effect can easily be seen in the topological charge Q \u2208 Z, whose auto-correlation time increases dramatically with smaller lattice spacing as configurations tend to get \u2018stuck\u2019 in distinct topological sectors, preventing an efficient exploration of the phase space. As a result, developing new sampling techniques that are able to offer improvements in efficiency through a reduction of statistical autocorrelations are highly desired. In this LOI, we describe some recent work in this area, and provide suggestions for possible future directions. Generally speaking, MCMC methods are a class of algorithms that use Markov Chains to sample from a target distribution p(x) that is often too complicated to sample from directly. Currently, the Hamiltonian Monte Carlo (HMC) algorithm is the most widely used technique for generating gauge configurations in lattice gauge theory and lattice QCD. We include below a brief overview of the approach, but refer the interested reader to [12, 13] for more details and limitations. We begin by introducing an auxiliary momentum variable v (normally distributed, independent of the position x) in order to lift the target distribution onto a joint probability distribution p(x, v) in phase space. The Markov Chain is then obtained by simulating a physical system governed by a Hamiltonian comprised of kinetic and potential energy functions, i.e. H(x, v) = U(x) + T (v). In particular, HMC operates by sampling from the canonical distribution p(x, v) = exp(H(x, v)) = p(x)p(v) by solving the equations of motion (\u1e8bi = \u2202H \u2202vi , v\u0307i = \u2212 \u2202H \u2202xi ) for a fixed period of time using a volume-preserving integrator. In practice the integration is done in discrete steps introducing some numerical error. This then requires a Metropolis accept/reject step to correct for the error. As the lattice spacing decreases, sectors of different topology become separated by large potential barriers, and simply moving along trajectories from the standard EOM become inefficient at moving between",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "98680110",
                        "name": "Sam Foreman"
                    },
                    {
                        "authorId": "40430895",
                        "name": "Xiao-Yong Jin"
                    },
                    {
                        "authorId": "9138652",
                        "name": "J. Osborn"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "587cf6001c8265dea75e42a204446fb51895e78e",
                "externalIds": {
                    "MAG": "3072670751",
                    "DOI": "10.5075/EPFL-THESIS-10257",
                    "CorpusId": 226558017
                },
                "corpusId": 226558017,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/587cf6001c8265dea75e42a204446fb51895e78e",
                "title": "Deep Generative Models and Applications",
                "abstract": "Over the past few years, there have been fundamental breakthroughs in core problems in machine learning, largely driven by advances in deep neural networks. The amount of annotated data drastically increased and supervised deep discriminative models exceeded human-level performances in certain object detection tasks [Russakovsky et al., 2015, He et al., 2015]. The increasing availability in quantity and complexity of unlabelled data also opens up exciting possibilities for the development of unsupervised learning methods. Among the family of unsupervised methods, deep generative models find numerous applications. Moreover, as real-world applications include high dimensional data, the ability of generative models to automatically learn semantically meaningful subspaces makes their advancement an essential step toward developing more efficient algorithms. Generative Adversarial Networks (GANs) are a family of unsupervised generative algorithms that have demonstrated impressive performance for data synthesis and are now used in a wide range of computer vision tasks. Despite this success, they gained a reputation for being difficult to train, which results in a time-consuming and human-involved development process to use them. In the first part of this thesis, we focus on improving the stability and the performances of GANs. Foremost, we consider an alternative training process to the standard one, named SGAN, in which several adversarial \u201clocal\u201d pairs of networks are trained independently so that a \u201cglobal\u201d supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well. Next, to further reduce the computational footprint while maintaining the stability and performance advantages of SGAN, we focus on training single pair of adversarial networks using variance reduced gradient. More precisely, we study the effect of the stochastic gradient noise on the training of generative adversarial networks (GANs) and show that it can prevent the convergence of standard game optimization methods, while the batch version converges. We address this issue with two stochastic variance-reduced gradient and extragradient optimization algorithms for GANs, named SVRG-GAN and SVRE, respectively. As batch extragradient is the only method that converges for simple examples of games, our analyses focus on SVRE, which method for a large class of games improves upon the previous convergence rates proposed in the literature. We observe empirically that SVRE performs similarly to a batch method",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2458542",
                        "name": "Tatjana Chavdarova"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In [3] and its variants[11, 1, 12, 8], the Hamiltonian function can be approximated by neural networks, H\u03b8, called Hamiltonian Neural Networks (HNN)."
            ],
            "citingPaper": {
                "paperId": "ca0bf46f11fd29d5626e49a5d17b7c4240d31c92",
                "externalIds": {
                    "CorpusId": 231773239
                },
                "corpusId": 231773239,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ca0bf46f11fd29d5626e49a5d17b7c4240d31c92",
                "title": "Meta-Learned Hamiltonian",
                "abstract": "Many physical systems governed by the same physical laws can be expressed by a well-established Hamiltonian with adapting different physical parameters. However, in general, establishing an appropriate Hamiltonian of the unknown process is a central challenge for a wide area of science and engineering problems. We suggest that meta-learning algorithms can be one of the powerful data-driven tools for identifying the shared representation of Hamiltonian of an unknown process. In our demonstration, we show that a meta-learned model, which is considered implicitly to learn the shared representation of Hamiltonian, predicts the dynamics of new systems from observing a few point dynamics of the systems.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2108154935",
                        "name": "Seungjun Lee"
                    },
                    {
                        "authorId": "2517631",
                        "name": "W. Seong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In parallel, physics and machine learning have been forging strong ties based for example on Hamiltonian theory and the integration of ordinary differential equations in the latent space to describe the evolution of dynamical systems (Chen et al. (2018); Toth et al. (2019); Greydanus et al. (2019))."
            ],
            "citingPaper": {
                "paperId": "5800c6078ba37382b220ccfac252caf1b60e4e38",
                "externalIds": {
                    "DBLP": "conf/nips/QuessardBC20",
                    "MAG": "3105142225",
                    "CorpusId": 227275384
                },
                "corpusId": 227275384,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5800c6078ba37382b220ccfac252caf1b60e4e38",
                "title": "Learning Disentangled Representations and Group Structure of Dynamical Environments",
                "abstract": "Learning disentangled representations is a key step towards effectively discovering and modelling the underlying structure of environments. In the natural sciences, physics has found great success by describing the universe in terms of symmetry preserving transformations. Inspired by this formalism, we propose a framework, built upon the theory of group representation, for learning representations of a dynamical environment structured around the transformations that generate its evolution. Experimentally, we learn the structure of explicitly symmetric environments without supervision from observational data generated by sequential interactions. We further introduce an intuitive disentanglement regularisation to ensure the interpretability of the learnt representations. We show that our method enables accurate long-horizon predictions, and demonstrate a correlation between the quality of predictions and disentanglement in the latent space.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1402910228",
                        "name": "Robin Quessard"
                    },
                    {
                        "authorId": "2067064746",
                        "name": "T. Barrett"
                    },
                    {
                        "authorId": "37289174",
                        "name": "W. Clements"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent work on physics-based deep learning injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly conserving physical quantities, or with ground truth supervision (Asenov et al.",
                "\u2026injects structure in the latent space of the dynamics using Lagrangian and Hamiltonian operators (Greydanus et al., 2019; Chen et al., 2020; Toth et al., 2020; Sanchez-Gonzalez et al., 2019; Cranmer et al., 2020b; Zhong et al., 2020), by explicitly conserving physical quantities, or with\u2026",
                "\u2026studied in the context of differentiable physics simulation Degrave et al. (2016); de Avila Belbute-Peres et al. (2018); Cranmer et al. (2020b); Toth et al. (2020); Greydanus et al. (2019); Sanchez-Gonzalez et al. (2019), we focus on more challenging systems which have not been studied in\u2026"
            ],
            "citingPaper": {
                "paperId": "36d2903f2251c14bcddceb285c75c5c10b4c1256",
                "externalIds": {
                    "CorpusId": 232084662
                },
                "corpusId": 232084662,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/36d2903f2251c14bcddceb285c75c5c10b4c1256",
                "title": "Differentiable simulation for system identification and visuomotor control \u2207 Sim : D IFFERENTIABLE SIMULATION FOR SYSTEM IDENTIFICATION AND VISUOMOTOR CONTROL",
                "abstract": "We consider the problem of estimating an object\u2019s physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current solutions require precise 3D labels which are labor-intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. We present \u2207Sim, a framework that overcomes the dependence on 3D supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Moreover, our unified computation graph \u2013 spanning from the dynamics and through the rendering process \u2013 enables learning in challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to or better than techniques that rely on precise 3D labels.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "7434636",
                        "name": "Krishna Murthy Jatavallabhula"
                    },
                    {
                        "authorId": "46637939",
                        "name": "M. Macklin"
                    },
                    {
                        "authorId": "2970150",
                        "name": "Florian Golemo"
                    },
                    {
                        "authorId": "2961618",
                        "name": "Vikram S. Voleti"
                    },
                    {
                        "authorId": "2060687770",
                        "name": "Linda"
                    },
                    {
                        "authorId": "2096957486",
                        "name": "Petrini"
                    },
                    {
                        "authorId": "144069571",
                        "name": "Martin Weiss"
                    },
                    {
                        "authorId": "41226293",
                        "name": "Breandan Considine"
                    },
                    {
                        "authorId": "1585278372",
                        "name": "J\u00e9r\u00f4me Parent-L\u00e9vesque"
                    },
                    {
                        "authorId": "47966782",
                        "name": "Kevin Xie"
                    },
                    {
                        "authorId": "2102299108",
                        "name": "Kenny"
                    },
                    {
                        "authorId": "2083161852",
                        "name": "Erleben"
                    },
                    {
                        "authorId": "3198259",
                        "name": "L. Paull"
                    },
                    {
                        "authorId": "2162768",
                        "name": "F. Shkurti"
                    },
                    {
                        "authorId": "1795014",
                        "name": "D. Nowrouzezahrai"
                    },
                    {
                        "authorId": "37895334",
                        "name": "S. Fidler"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2022 Predicting the coordinates (q,p) from images has been pursued in (Greydanus et al., 2019; Toth et al., 2019).",
                "Here this would add an additional network before our input with the target output (q,p) as was explicitly demonstrated to work in (Toth et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "ccd1a1f8b097df960ef21eff9b981fe0c306b207",
                "externalIds": {
                    "CorpusId": 233392400
                },
                "corpusId": 233392400,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ccd1a1f8b097df960ef21eff9b981fe0c306b207",
                "title": "Symmetry Control Neural Networks",
                "abstract": "This paper continues the quest for designing the optimal physics bias for neural networks predicting the dynamics of systems when the underlying dynamics shall be inferred from the data directly. The description of physical systems is greatly simplified when the underlying symmetries of the system are taken into account. In classical systems described via Hamiltonian dynamics this is achieved by using appropriate coordinates, so-called cyclic coordinates, which reveal conserved quantities directly. Without changing the Hamiltonian, these coordinates can be obtained via canonical transformations. We show that such coordinates can be searched for automatically with appropriate loss functions which naturally arise from Hamiltonian dynamics. As a proof of principle, we test our method on standard classical physics systems using synthetic and experimental data where our network identifies the conserved quantities in an unsupervised way and find improved performance on predicting the dynamics of the system compared to networks biasing just to the Hamiltonian. Effectively, these new coordinates guarantee that motion takes place on symmetry orbits in phase space, i.e. appropriate lower dimensional sub-spaces of phase space. By fitting analytic formulae we recover that our networks are utilising conserved quantities such as (angular) momentum.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", later developed Hamiltonian Generative Network (HGN), which is capable of consistently learning Hamiltonian dynamics from high-dimensional observations without restrictive domain assumptions (38)."
            ],
            "citingPaper": {
                "paperId": "62d815df1e886808d7d5aa34f7a69d37cee80bbb",
                "externalIds": {
                    "CorpusId": 236795744
                },
                "corpusId": 236795744,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/62d815df1e886808d7d5aa34f7a69d37cee80bbb",
                "title": "VORTEXNET: LEARNING COMPLEX DYNAMIC SYS-",
                "abstract": "In this paper, we present a novel physics-rooted network structure that dramatically facilitates the learning of complex dynamic systems. Our method is inspired by the Vortex Method in fluid dynamics, whose key idea lies in that, given the observed flow field, instead of describing it with a function of space and time, one can equivalently understand the observation as being caused by a number of Lagrangian particles \u2014\u2013 vortices, flowing with the field. Since the number of such vortices are much smaller than that of the Eulerian, grid discretization, this Lagrangian discretization in essence encodes the system dynamics on a compact physics-based latent space. Our method enforces such Lagrangian discretization with a Encoder\u2014Dynamics\u2014Decode network structure, and trains it with a novel three-stage curriculum learning algorithm. With data generated from the high precision Eulerian DNS method, our alorithm takes advantage of the simplifying power of the Lagrangian method while persisting the physical integrity. This method fundamentally differs from the current approaches in the field of physicsinformed learning, and provides superior results for being more versatile, yielding more physical-correctness with less data sample, and faster to compute at high precision. Beyond providing a viable way of simulating complex fluid at highprecision, our method opens up a brand new horizon for embedding knowledge prior via constructing physically-valid latent spaces, which can be applied to further research areas beyond physical simulation.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2122423450",
                        "name": "Tems With"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, some works leveraged data specific knowledge to shape the prediction function, for example imposing specific fluid dynamic [26] or Hamiltonian constraints [11, 31]."
            ],
            "citingPaper": {
                "paperId": "6bd206e93f25492c345b590f4abe51db04bd0512",
                "externalIds": {
                    "CorpusId": 262898505
                },
                "corpusId": 262898505,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6bd206e93f25492c345b590f4abe51db04bd0512",
                "title": "Bridging Dynamical Models and Deep Networks to Solve Forward and Inverse Problems",
                "abstract": "Modeling the dynamics of physical systems recently gained attention in the machine learning community. Most recent works rely on complete observations of the physical state, whereas only partial observations are available in practice. Estimating the full state dynamics is important for the understanding of the underlying phenomenon and for model based prediction. Largely unexplored from an ML viewpoint, we address in this work the estimation and forecast of a partially observed spatio-temporal system, leveraging prior dynamical knowledge. To solve both forward (forecasting) and inverse (identi\ufb01cation) problems, we bridge numerical models of partial differential equations and deep learning and introduce a dynamical regularization on the unobserved states. This constrains our estimation and improves estimation performances. The approach is validated on two simulated datasets where the dynamics is controlled and fully known.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2181917156",
                        "name": "Marie D\u00e9chelle"
                    },
                    {
                        "authorId": "1853488882",
                        "name": "J\u00e9r\u00e9mie Don\u00e0"
                    },
                    {
                        "authorId": "2249230996",
                        "name": "K\u00e9vin Plessis-Fraissard"
                    },
                    {
                        "authorId": "2237639535",
                        "name": "Patrick Gallinari"
                    },
                    {
                        "authorId": "2181918106",
                        "name": "Marina L\u00e9vy"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6338e211bedbc02ddd329ec25b8e11063ddb9b58",
                "externalIds": {
                    "CorpusId": 260420896
                },
                "corpusId": 260420896,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6338e211bedbc02ddd329ec25b8e11063ddb9b58",
                "title": "Structured learning of rigid-body dynamics: A survey and uni\ufb01ed view",
                "abstract": "Accurate models of mechanical system dynamics are often critical for model-based control and reinforcement learning. Fully data-driven dynamics models promise to ease the process of modeling and analysis, but require considerable amounts of data for training and often do not generalize well to unseen parts of the state space. Combining data-driven modelling with prior analytical knowledge is an attractive alternative as the inclusion of structural knowledge into a regression model improves the model\u2019s data e\ufb03ciency and physical integrity. In this article, we survey supervised regression models that combine rigid-body mechanics with data-driven modelling techniques. We analyze the di\ufb00erent latent functions (such as kinetic energy or dissipative forces) and operators (such as di\ufb00erential operators and projection matrices) underlying common descriptions of rigid-body mechanics. Based on this analysis, we provide a uni\ufb01ed view on the combination of data-driven regression models, such as neural networks and Gaussian processes, with analytical model priors. Further, we review and discuss key techniques for designing structured models such as automatic di\ufb00erentiation.",
                "year": null,
                "authors": [
                    {
                        "authorId": "2064785715",
                        "name": "A. R. Geist"
                    },
                    {
                        "authorId": "2715093",
                        "name": "Sebastian Trimpe"
                    }
                ]
            }
        }
    ]
}