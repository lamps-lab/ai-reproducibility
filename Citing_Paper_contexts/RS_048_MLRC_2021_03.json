{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c7b88262951d7ca1466a47549af1d9be8f30a982",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-13822",
                    "ArXiv": "2309.13822",
                    "DOI": "10.48550/arXiv.2309.13822",
                    "CorpusId": 262464583
                },
                "corpusId": 262464583,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c7b88262951d7ca1466a47549af1d9be8f30a982",
                "title": "PARTICLE: Part Discovery and Contrastive Learning for Fine-grained Recognition",
                "abstract": "We develop techniques for refining representations for fine-grained classification and segmentation tasks in a self-supervised manner. We find that fine-tuning methods based on instance-discriminative contrastive learning are not as effective, and posit that recognizing part-specific variations is crucial for fine-grained categorization. We present an iterative learning approach that incorporates part-centric equivariance and invariance objectives. First, pixel representations are clustered to discover parts. We analyze the representations from convolutional and vision transformer networks that are best suited for this task. Then, a part-centric learning step aggregates and contrasts representations of parts within an image. We show that this improves the performance on image classification and part segmentation tasks across datasets. For example, under a linear-evaluation scheme, the classification accuracy of a ResNet50 trained on ImageNet using DetCon, a self-supervised learning approach, improves from 35.4% to 42.0% on the Caltech-UCSD Birds, from 35.5% to 44.1% on the FGVC Aircraft, and from 29.7% to 37.4% on the Stanford Cars. We also observe significant gains in few-shot part segmentation tasks using the proposed technique, while instance-discriminative learning was not as effective. Smaller, yet consistent, improvements are also observed for stronger networks based on transformers.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "49194775",
                        "name": "Oindrila Saha"
                    },
                    {
                        "authorId": "35208858",
                        "name": "Subhransu Maji"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d11a500d3f0dec27f0113d0f9a0893542d55ea0c",
                "externalIds": {
                    "ArXiv": "2309.10376",
                    "DBLP": "journals/corr/abs-2309-10376",
                    "DOI": "10.48550/arXiv.2309.10376",
                    "CorpusId": 262055538
                },
                "corpusId": 262055538,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d11a500d3f0dec27f0113d0f9a0893542d55ea0c",
                "title": "Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks",
                "abstract": "Graph Neural Networks (GNNs) have become popular in Graph Representation Learning (GRL). One fundamental application is few-shot node classification. Most existing methods follow the meta learning paradigm, showing the ability of fast generalization to few-shot tasks. However, recent works indicate that graph contrastive learning combined with fine-tuning can significantly outperform meta learning methods. Despite the empirical success, there is limited understanding of the reasons behind it. In our study, we first identify two crucial advantages of contrastive learning compared to meta learning, including (1) the comprehensive utilization of graph nodes and (2) the power of graph augmentations. To integrate the strength of both contrastive learning and meta learning on the few-shot node classification tasks, we introduce a new paradigm: Contrastive Few-Shot Node Classification (COLA). Specifically, COLA employs graph augmentations to identify semantically similar nodes, which enables the construction of meta-tasks without the need for label information. Therefore, COLA can utilize all nodes to construct meta-tasks, further reducing the risk of overfitting. Through extensive experiments, we validate the essentiality of each component in our design and demonstrate that COLA achieves new state-of-the-art on all tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2239091724",
                        "name": "Jiarui Feng"
                    },
                    {
                        "authorId": "2164063663",
                        "name": "Lecheng Kong"
                    },
                    {
                        "authorId": "2244621611",
                        "name": "Dacheng Tao"
                    },
                    {
                        "authorId": "2223152252",
                        "name": "Yixin Chen"
                    },
                    {
                        "authorId": "3098251",
                        "name": "Muhan Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ce19227dba27ee7afed952ca15030535baf3aab8",
                "externalIds": {
                    "DBLP": "conf/mipr/ZhangS23",
                    "DOI": "10.1109/MIPR59079.2023.00026",
                    "CorpusId": 262131796
                },
                "corpusId": 262131796,
                "publicationVenue": {
                    "id": "eb35da90-3350-4969-9944-7744cb0bf6fb",
                    "name": "Conference on Multimedia Information Processing and Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "MIPR",
                        "Conf Multimedia Inf Process Retr"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ce19227dba27ee7afed952ca15030535baf3aab8",
                "title": "Dual-stream Multi-Modal Graph Neural Network for Few-Shot Learning",
                "abstract": "Few-shot learning aims to rapidly recognize unseen targets using only a limited number of labeled samples, which is one of the core capabilities for humans. However, existing research primarily transforms the samples into the feature space of a single modality, neglecting the hidden features within other modalities. To address this problem, we develop a Dual-stream Multi-Modal Graph Neural Network (DMMG) that leverages additional information from multi-modality for few-shot learning. We convert the representation of text and images into each other's feature space in parallel streams. Then we compare instances of text and images in different vector spaces at the same time, exploiting the potentials of each modality for few-shot classification through skipping the bottleneck of information. This approach can be extended to a wide range of metric-based few-shot learning methods. The experiments on the miniImageNet dataset demonstrate that DMMG outperforms state-of-the-art few-shot learning methods, highlighting the effectiveness of our proposed approach.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2244140837",
                        "name": "Wenli Zhang"
                    },
                    {
                        "authorId": "2244139179",
                        "name": "Luping Shi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Some methods utilize the SSL loss as supplemental losses during the supervised training process [18, 41]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "96faf0c4d31b864ba428e6a20940021858b73ae6",
                "externalIds": {
                    "ArXiv": "2307.14612",
                    "DBLP": "journals/corr/abs-2307-14612",
                    "DOI": "10.48550/arXiv.2307.14612",
                    "CorpusId": 260203369
                },
                "corpusId": 260203369,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/96faf0c4d31b864ba428e6a20940021858b73ae6",
                "title": "GenCo: An Auxiliary Generator from Contrastive Learning for Enhanced Few-Shot Learning in Remote Sensing",
                "abstract": "Classifying and segmenting patterns from a limited number of examples is a significant challenge in remote sensing and earth observation due to the difficulty in acquiring accurately labeled data in large quantities. Previous studies have shown that meta-learning, which involves episodic training on query and support sets, is a promising approach. However, there has been little attention paid to direct fine-tuning techniques. This paper repurposes contrastive learning as a pre-training method for few-shot learning for classification and semantic segmentation tasks. Specifically, we introduce a generator-based contrastive learning framework (GenCo) that pre-trains backbones and simultaneously explores variants of feature samples. In fine-tuning, the auxiliary generator can be used to enrich limited labeled data samples in feature space. We demonstrate the effectiveness of our method in improving few-shot learning performance on two key remote sensing datasets: Agriculture-Vision and EuroSAT. Empirically, our approach outperforms purely supervised training on the nearly 95,000 images in Agriculture-Vision for both classification and semantic segmentation tasks. Similarly, the proposed few-shot method achieves better results on the land-cover classification task on EuroSAT compared to the results obtained from fully supervised model training on the dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149267217",
                        "name": "Jing Wu"
                    },
                    {
                        "authorId": "2130392",
                        "name": "N. Hovakimyan"
                    },
                    {
                        "authorId": "145311472",
                        "name": "Jennifer Hobbs"
                    }
                ]
            }
        },
        {
            "intents": [
                "result"
            ],
            "contexts": [
                "This, along with other previous work in few-shot learning[11], [15], inspire us to make use of unlabeled data for few-shot class-incremental learning."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cd0b2ce979270d605f93d43a750dd45191894611",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-02790",
                    "ArXiv": "2308.02790",
                    "DOI": "10.1109/ISPDS58840.2023.10235731",
                    "CorpusId": 260682712
                },
                "corpusId": 260682712,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cd0b2ce979270d605f93d43a750dd45191894611",
                "title": "Few-Shot Class-Incremental Semantic Segmentation via Pseudo-Labeling and Knowledge Distillation",
                "abstract": "We address the problem of learning new classes for semantic segmentation models from few examples, which is challenging because of the following two reasons. Firstly, it is difficult to learn from limited novel data to capture the underlying class distribution. Secondly, it is challenging to retain knowledge for existing classes and to avoid catastrophic forgetting. For learning from limited data, we propose a pseudo-labeling strategy to augment the few-shot training annotations in order to learn novel classes more effectively. Given only one or a few images labeled with the novel classes and a much larger set of unlabeled images, we transfer the knowledge from labeled images to unlabeled images with a coarse-to-fine pseudo-labeling approach in two steps. Specifically, we first match each labeled image to its nearest neighbors in the unlabeled image set at the scene level, in order to obtain images with a similar scene layout. This is followed by obtaining pseudo-labels within this neighborhood by applying classifiers learned on the few-shot annotations. In addition, we use knowledge distillation on both labeled and unlabeled data to retain knowledge on existing classes. We integrate the above steps into a single convolutional neural network with a unified learning objective. Extensive experiments on the Cityscapes and KITTI datasets validate the efficacy of the proposed approach in the self-driving domain. Code is available from https://github.com/ChasonJiang/FSCILSS.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2115486670",
                        "name": "Chen Jiang"
                    },
                    {
                        "authorId": null,
                        "name": "Tao Wang"
                    },
                    {
                        "authorId": "2228191150",
                        "name": "Sien Li"
                    },
                    {
                        "authorId": "2115639762",
                        "name": "Jinyang Wang"
                    },
                    {
                        "authorId": "2108587624",
                        "name": "Shirui Wang"
                    },
                    {
                        "authorId": "2228715762",
                        "name": "Antonios Antoniou"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e179786cb15605c685aa6ca38549b0c884c5708",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-02238",
                    "ArXiv": "2307.02238",
                    "DOI": "10.48550/arXiv.2307.02238",
                    "CorpusId": 260444896
                },
                "corpusId": 260444896,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3e179786cb15605c685aa6ca38549b0c884c5708",
                "title": "Source Identification: A Self-Supervision Task for Dense Prediction",
                "abstract": "The paradigm of self-supervision focuses on representation learning from raw data without the need of labor-consuming annotations, which is the main bottleneck of current data-driven methods. Self-supervision tasks are often used to pre-train a neural network with a large amount of unlabeled data and extract generic features of the dataset. The learned model is likely to contain useful information which can be transferred to the downstream main task and improve performance compared to random parameter initialization. In this paper, we propose a new self-supervision task called source identification (SI), which is inspired by the classic blind source separation problem. Synthetic images are generated by fusing multiple source images and the network's task is to reconstruct the original images, given the fused images. A proper understanding of the image content is required to successfully solve the task. We validate our method on two medical image segmentation tasks: brain tumor segmentation and white matter hyperintensities segmentation. The results show that the proposed SI task outperforms traditional self-supervision tasks for dense predictions including inpainting, pixel shuffling, intensity shift, and super-resolution. Among variations of the SI task fusing images of different types, fusing images from different patients performs best.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143502445",
                        "name": "Shuai Chen"
                    },
                    {
                        "authorId": "40207506",
                        "name": "S. Kayal"
                    },
                    {
                        "authorId": "32895376",
                        "name": "Marleen de Bruijne"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b0f6f1c464700c79fa20fb7b2e3975c60208a559",
                "externalIds": {
                    "DBLP": "conf/cvpr/LiWTMZMBF23",
                    "DOI": "10.1109/CVPRW59228.2023.00503",
                    "CorpusId": 260913868
                },
                "corpusId": 260913868,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b0f6f1c464700c79fa20fb7b2e3975c60208a559",
                "title": "Improving Cross-Domain Detection with Self-Supervised Learning",
                "abstract": "Cross-Domain Detection (XDD) aims to train a domain-adaptive object detector using unlabeled images from a target domain and labeled images from a source domain. Existing approaches achieve this either by transferring the style of source images to that of target images, or by aligning the features of images from the two domains. In this paper, rather than proposing another method following the existing lines, we introduce a new framework complementary to existing methods. Our framework unifies some popular Self-Supervised Learning (SSL) techniques (e.g., rotation angle prediction, strong/weak data augmentation, mean teacher modeling) and adapts them to the XDD task. Our basic idea is to leverage the unsupervised nature of these SSL techniques and apply them simultaneously across domains (source and target) and models (student and teacher). These SSL techniques can thus serve as shared bridges that facilitate knowledge transfer between domains. More importantly, as these techniques are independently applied in each domain, they are complementary to existing domain alignment techniques that relies on interactions between domains (e.g., adversarial alignment). We perform extensive analyses on these SSL techniques and show that they significantly improve the performance of existing methods. In addition, we reach comparable or even better performance than the state-of-the-art methods when integrating our framework with an old well-established method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "94451829",
                        "name": "K. Li"
                    },
                    {
                        "authorId": "26360698",
                        "name": "Curtis Wigington"
                    },
                    {
                        "authorId": "67319819",
                        "name": "Chris Tensmeyer"
                    },
                    {
                        "authorId": "2852035",
                        "name": "Vlad I. Morariu"
                    },
                    {
                        "authorId": "7574699",
                        "name": "Handong Zhao"
                    },
                    {
                        "authorId": "1977256",
                        "name": "Varun Manjunatha"
                    },
                    {
                        "authorId": "1598478975",
                        "name": "Nikolaos Barmpalios"
                    },
                    {
                        "authorId": "46956675",
                        "name": "Y. Fu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "A model is first pretrained based on the base set using either self-supervised learning [33, 26, 42, 22, 27], meta-learning [25, 32, 14, 34, 38], or traditional supervised approaches [28, 12, 9]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9449fa15622257e81e429903535433d3a2ece6f6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-17368",
                    "ArXiv": "2305.17368",
                    "DOI": "10.48550/arXiv.2305.17368",
                    "CorpusId": 258960645
                },
                "corpusId": 258960645,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9449fa15622257e81e429903535433d3a2ece6f6",
                "title": "Instance-based Max-margin for Practical Few-shot Recognition",
                "abstract": "In order to mimic the human few-shot learning (FSL) ability better and to make FSL closer to real-world applications, this paper proposes a practical FSL (pFSL) setting. pFSL is based on unsupervised pretrained models (analogous to human prior knowledge) and recognizes many novel classes simultaneously. Compared to traditional FSL, pFSL is simpler in its formulation, easier to evaluate, more challenging and more practical. To cope with the rarity of training examples, this paper proposes IbM2, an instance-based max-margin method not only for the new pFSL setting, but also works well in traditional FSL scenarios. Based on the Gaussian Annulus Theorem, IbM2 converts random noise applied to the instances into a mechanism to achieve maximum margin in the many-way pFSL (or traditional FSL) recognition task. Experiments with various self-supervised pretraining methods and diverse many- or few-way FSL tasks show that IbM2 almost always leads to improvements compared to its respective baseline methods, and in most cases the improvements are significant. With both the new pFSL setting and novel IbM2 method, this paper shows that practical few-shot learning is both viable and promising.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2158818865",
                        "name": "Minghao Fu"
                    },
                    {
                        "authorId": "2113841905",
                        "name": "Kevin Zhu"
                    },
                    {
                        "authorId": "2155449887",
                        "name": "Jianxin Wu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "698d5651b96fa7629f8cb7d698648c9267d7d49c",
                "externalIds": {
                    "ArXiv": "2305.13752",
                    "DBLP": "journals/corr/abs-2305-13752",
                    "DOI": "10.48550/arXiv.2305.13752",
                    "CorpusId": 258841230
                },
                "corpusId": 258841230,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/698d5651b96fa7629f8cb7d698648c9267d7d49c",
                "title": "Pulling Target to Source: A New Perspective on Domain Adaptive Semantic Segmentation",
                "abstract": "Domain adaptive semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. However, existing methods primarily focus on directly learning qualified target features, making it challenging to guarantee their discrimination in the absence of target labels. This work provides a new perspective. We observe that the features learned with source data manage to keep categorically discriminative during training, thereby enabling us to implicitly learn adequate target representations by simply \\textbf{pulling target features close to source features for each category}. To this end, we propose T2S-DA, which we interpret as a form of pulling Target to Source for Domain Adaptation, encouraging the model in learning similar cross-domain features. Also, considering the pixel categories are heavily imbalanced for segmentation datasets, we come up with a dynamic re-weighting strategy to help the model concentrate on those underperforming classes. Extensive experiments confirm that T2S-DA learns a more discriminative and generalizable representation, significantly surpassing the state-of-the-art. We further show that our method is quite qualified for the domain generalization task, verifying its domain-invariant property.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48017277",
                        "name": "Haochen Wang"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "2544305",
                        "name": "Jingjing Fei"
                    },
                    {
                        "authorId": "48625175",
                        "name": "Wei Li"
                    },
                    {
                        "authorId": "49279300",
                        "name": "Liwei Wu"
                    },
                    {
                        "authorId": "2115829788",
                        "name": "Yuxi Wang"
                    },
                    {
                        "authorId": "145274329",
                        "name": "Zhaoxiang Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9fb4e8392aba28dfb0caf710b604878ae1d5b058",
                "externalIds": {
                    "DBLP": "journals/pr/ShiLHZWG23",
                    "DOI": "10.2139/ssrn.4362435",
                    "CorpusId": 257011293
                },
                "corpusId": 257011293,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9fb4e8392aba28dfb0caf710b604878ae1d5b058",
                "title": "Global- and local-aware feature augmentation with semantic orthogonality for few-shot image classification",
                "abstract": "As for few-shot image classification, recently, some works revisit the standard transfer learning paradigm, i.e., pre-training and fine-tuning, and have achieved some success. However, we find that this kind of methods heavily relies on a naive image-level data augmentation (e.g., cropping and flipping) at the fine-tuning stage, which will easily suffer from the overfitting problem because of the limited-data regime. To tackle this issue, in this paper, we attempt to perform a novel feature-level semantic augmentation at the fine-tuning stage and propose a Globaland Local-aware Feature Augmentation method (GLFA) from both the channeland spatial-wise perspectives. In addition, at the pre-training stage, we further propose a Semantic Orthogonal Learning Framework (SOLF) to make the learned feature channels more independently, orthogonal and diverse. Extensive experiments demonstrate that the proposed method can obtain significant performance improvements over the state of the arts. Code is available at https://github.com/onlyyao/GLFA-SOLF. \u00a9 2023 Elsevier Ltd. All rights reserved.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2208187727",
                        "name": "Boyao Shi"
                    },
                    {
                        "authorId": "35660603",
                        "name": "Wenbin Li"
                    },
                    {
                        "authorId": "2055851838",
                        "name": "Jing Huo"
                    },
                    {
                        "authorId": "145463313",
                        "name": "Pengfei Zhu"
                    },
                    {
                        "authorId": "46659935",
                        "name": "Lei Wang"
                    },
                    {
                        "authorId": "49658113",
                        "name": "Yang Gao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4bfdf61b6d4a9cfabf69fabf01ae9426f047a20b",
                "externalIds": {
                    "DBLP": "journals/nn/LimLLT23",
                    "DOI": "10.1016/j.neunet.2023.05.037",
                    "CorpusId": 258902424,
                    "PubMed": "37263089"
                },
                "corpusId": 258902424,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4bfdf61b6d4a9cfabf69fabf01ae9426f047a20b",
                "title": "SCL: Self-supervised contrastive learning for few-shot image classification",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2148578902",
                        "name": "Jit Yan Lim"
                    },
                    {
                        "authorId": "30039300",
                        "name": "K. Lim"
                    },
                    {
                        "authorId": "84161127",
                        "name": "C. Lee"
                    },
                    {
                        "authorId": "2162619859",
                        "name": "Yong Xuan Tan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e14ab0aaf026369578a132a67a730c95e014e096",
                "externalIds": {
                    "DBLP": "journals/cviu/WangZQLGS23",
                    "DOI": "10.1016/j.cviu.2023.103737",
                    "CorpusId": 258952088
                },
                "corpusId": 258952088,
                "publicationVenue": {
                    "id": "5fbb417b-d7a5-44e6-856d-993f0624ed9c",
                    "name": "Computer Vision and Image Understanding",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Vis Image Underst"
                    ],
                    "issn": "1077-3142",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622809/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/10773142",
                        "http://www.idealibrary.com/links/toc/cviu",
                        "https://www.journals.elsevier.com/computer-vision-and-image-understanding"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e14ab0aaf026369578a132a67a730c95e014e096",
                "title": "Cross-domain few-shot action recognition with unlabeled videos",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144796430",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "50202110",
                        "name": "Shiwei Zhang"
                    },
                    {
                        "authorId": "1750375688",
                        "name": "Zhiwu Qing"
                    },
                    {
                        "authorId": "2075456784",
                        "name": "Yiliang Lv"
                    },
                    {
                        "authorId": "40115662",
                        "name": "Changxin Gao"
                    },
                    {
                        "authorId": "1707161",
                        "name": "N. Sang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0a52f77490300be671942909e09b4ee87e7f23a0",
                "externalIds": {
                    "ArXiv": "2304.13287",
                    "DBLP": "journals/corr/abs-2304-13287",
                    "DOI": "10.48550/arXiv.2304.13287",
                    "CorpusId": 258331836
                },
                "corpusId": 258331836,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0a52f77490300be671942909e09b4ee87e7f23a0",
                "title": "ESPT: A Self-Supervised Episodic Spatial Pretext Task for Improving Few-Shot Learning",
                "abstract": "Self-supervised learning (SSL) techniques have recently been integrated into the few-shot learning (FSL) framework and have shown promising results in improving the few-shot image classification performance. However, existing SSL approaches used in FSL typically seek the supervision signals from the global embedding of every single image. Therefore, during the episodic training of FSL, these methods cannot capture and fully utilize the local visual information in image samples and the data structure information of the whole episode, which are beneficial to FSL. To this end, we propose to augment the few-shot learning objective with a novel self-supervised Episodic Spatial Pretext Task (ESPT). Specifically, for each few-shot episode, we generate its corresponding transformed episode by applying a random geometric transformation to all the images in it. Based on these, our ESPT objective is defined as maximizing the local spatial relationship consistency between the original episode and the transformed one. With this definition, the ESPT-augmented FSL objective promotes learning more transferable feature representations that capture the local spatial features of different images and their inter-relational structural information in each input episode, thus enabling the model to generalize better to new categories with only a few samples. Extensive experiments indicate that our ESPT method achieves new state-of-the-art performance for few-shot image classification on three mainstay benchmark datasets. The source code will be available at: https://github.com/Whut-YiRong/ESPT.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145872629",
                        "name": "Yi Rong"
                    },
                    {
                        "authorId": "151480853",
                        "name": "Xiongbo Lu"
                    },
                    {
                        "authorId": "1491623345",
                        "name": "Zhaoyang Sun"
                    },
                    {
                        "authorId": "9407523",
                        "name": "Yaxiong Chen"
                    },
                    {
                        "authorId": "2135639762",
                        "name": "Shengwu Xiong"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3ae9c50cd9b478c3a53b55d3e7f143186dcc11d1",
                "externalIds": {
                    "ArXiv": "2304.10093",
                    "DBLP": "journals/corr/abs-2304-10093",
                    "DOI": "10.48550/arXiv.2304.10093",
                    "CorpusId": 258236467
                },
                "corpusId": 258236467,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3ae9c50cd9b478c3a53b55d3e7f143186dcc11d1",
                "title": "Clustered-patch Element Connection for Few-shot Learning",
                "abstract": "Weak feature representation problem has influenced the performance of few-shot classification task for a long time. To alleviate this problem, recent researchers build connections between support and query instances through embedding patch features to generate discriminative representations. However, we observe that there exists semantic mismatches (foreground/ background) among these local patches, because the location and size of the target object are not fixed. What is worse, these mismatches result in unreliable similarity confidences, and complex dense connection exacerbates the problem. According to this, we propose a novel Clustered-patch Element Connection (CEC) layer to correct the mismatch problem. The CEC layer leverages Patch Cluster and Element Connection operations to collect and establish reliable connections with high similarity patch features, respectively. Moreover, we propose a CECNet, including CEC layer based attention module and distance metric. The former is utilized to generate a more discriminative representation benefiting from the global clustered-patch features, and the latter is introduced to reliably measure the similarity between pair-features. Extensive experiments demonstrate that our CECNet outperforms the state-of-the-art methods on classification benchmark. Furthermore, our CEC approach can be extended into few-shot segmentation and detection tasks, which achieves competitive performances.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "15037661",
                        "name": "Jinxiang Lai"
                    },
                    {
                        "authorId": "2187419298",
                        "name": "Siqian Yang"
                    },
                    {
                        "authorId": "2182658950",
                        "name": "Junhong Zhou"
                    },
                    {
                        "authorId": "2184570180",
                        "name": "Wenlong Wu"
                    },
                    {
                        "authorId": "46772408",
                        "name": "Xiao Chen"
                    },
                    {
                        "authorId": "2184458202",
                        "name": "Jun Liu"
                    },
                    {
                        "authorId": "2226422",
                        "name": "Bin-Bin Gao"
                    },
                    {
                        "authorId": "1978245",
                        "name": "Chengjie Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bf5fa2a6d2d2cae80f921827ca03f2f6cc3fb4ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-00353",
                    "ArXiv": "2302.00353",
                    "DOI": "10.48550/arXiv.2302.00353",
                    "CorpusId": 256459720
                },
                "corpusId": 256459720,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bf5fa2a6d2d2cae80f921827ca03f2f6cc3fb4ad",
                "title": "Towards Label-Efficient Incremental Learning: A Survey",
                "abstract": "The current dominant paradigm when building a machine learning model is to iterate over a dataset over and over until convergence. Such an approach is non-incremental, as it assumes access to all images of all categories at once. However, for many applications, non-incremental learning is unrealistic. To that end, researchers study incremental learning, where a learner is required to adapt to an incoming stream of data with a varying distribution while preventing forgetting of past knowledge. Significant progress has been made, however, the vast majority of works focus on the fully supervised setting, making these algorithms label-hungry thus limiting their real-life deployment. To that end, in this paper, we make the first attempt to survey recently growing interest in label-efficient incremental learning. We identify three subdivisions, namely semi-, few-shot- and self-supervised learning to reduce labeling efforts. Finally, we identify novel directions that can further enhance label-efficiency and improve incremental learning scalability. Project website: https://github.com/kilickaya/label-efficient-il.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2492994",
                        "name": "Mert Kilickaya"
                    },
                    {
                        "authorId": "2820687",
                        "name": "Joost van de Weijer"
                    },
                    {
                        "authorId": "47792365",
                        "name": "Yuki M. Asano"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "866724e38d023f828fdeb471ffc251af9ebe9ea6",
                "externalIds": {
                    "DBLP": "journals/kbs/ZhengWLYZY23",
                    "DOI": "10.1016/j.knosys.2023.110412",
                    "CorpusId": 257177426
                },
                "corpusId": 257177426,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/866724e38d023f828fdeb471ffc251af9ebe9ea6",
                "title": "ICCL: Independent and Correlative Correspondence Learning for few-shot image classification",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109674889",
                        "name": "Zijun Zheng"
                    },
                    {
                        "authorId": "2205889436",
                        "name": "Heng Wu"
                    },
                    {
                        "authorId": "104472491",
                        "name": "Laishui Lv"
                    },
                    {
                        "authorId": "2058204",
                        "name": "Hailiang Ye"
                    },
                    {
                        "authorId": "2115693524",
                        "name": "Changchun Zhang"
                    },
                    {
                        "authorId": "3153093",
                        "name": "Gaohang Yu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "de09e8d1dea8bdaab35a0463143e413c9aa436b8",
                "externalIds": {
                    "DBLP": "journals/ijcv/QiangLSFXW23",
                    "DOI": "10.1007/s11263-023-01760-7",
                    "CorpusId": 256438396
                },
                "corpusId": 256438396,
                "publicationVenue": {
                    "id": "939ee07c-6009-43f8-b884-69238b40659e",
                    "name": "International Journal of Computer Vision",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Comput Vis"
                    ],
                    "issn": "0920-5691",
                    "url": "https://www.springer.com/computer/image+processing/journal/11263",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11263",
                        "http://link.springer.com/journal/11263"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/de09e8d1dea8bdaab35a0463143e413c9aa436b8",
                "title": "Meta Attention-Generation Network for Cross-Granularity Few-Shot Learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2059455684",
                        "name": "Wenwen Qiang"
                    },
                    {
                        "authorId": "2118506408",
                        "name": "Jiangmeng Li"
                    },
                    {
                        "authorId": "2100573353",
                        "name": "Bing Su"
                    },
                    {
                        "authorId": "3247966",
                        "name": "Jianlong Fu"
                    },
                    {
                        "authorId": "2164317173",
                        "name": "Hui Xiong"
                    },
                    {
                        "authorId": "153693432",
                        "name": "Ji-rong Wen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, many research works have demonstrated the effectiveness of SSL on both detection and classification tasks [8, 13, 4, 14, 9], and also proven that SSL benefits the deep neural network by learning robust features representations for typical fewshot tasks [12, 24, 6, 10].",
                "al [24] augmented the unlabelled images by rotation and jigsaw puzzle."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c690e1c4a5b7642c5a9db74301063240a0d91b82",
                "externalIds": {
                    "DBLP": "conf/wacv/PanYZG23",
                    "DOI": "10.1109/WACV56688.2023.00621",
                    "CorpusId": 256651000
                },
                "corpusId": 256651000,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/c690e1c4a5b7642c5a9db74301063240a0d91b82",
                "title": "SSFE-Net: Self-Supervised Feature Enhancement for Ultra-Fine-Grained Few-Shot Class Incremental Learning",
                "abstract": "Ultra-Fine-Grained Visual Categorization (ultra-FGVC) has become a popular problem due to its great real-world potential for classifying the same or closely related species with very similar layouts. However, there present many challenges for the existing ultra-FGVC methods, firstly there are always not enough samples in the existing ultraFGVC datasets based on which the models can easily get overfitting. Secondly, in practice, we are likely to find new species that we have not seen before and need to add them to existing models, which is known as incremental learning. The existing methods solve these problems by Few-Shot Class Incremental Learning (FSCIL), but the main challenge of the FSCIL models on ultra-FGVC tasks lies in their inferior discrimination detection ability since they usually use low-capacity networks to extract features, which leads to insufficient discriminative details extraction from ultrafine-grained images. In this paper, a self-supervised feature enhancement for the few-shot incremental learning network (SSFE-Net) is proposed to solve this problem. Specifically, a self-supervised learning (SSL) and knowledge distillation (KD) framework is developed to enhance the feature extraction of the low-capacity backbone network for ultra-FGVC few-shot class incremental learning tasks. Besides, we for the first time create a series of benchmarks for FSCIL tasks on two public ultra-FGVC datasets and three normal finegrained datasets, which will facilitate the development of the Ultra-FGVC community. Extensive experimental results on public ultra-FGVC datasets and other state-of-the-art benchmarks consistently demonstrate the effectiveness of the proposed method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "38332536",
                        "name": "Zicheng Pan"
                    },
                    {
                        "authorId": "1405864516",
                        "name": "Xiaohan Yu"
                    },
                    {
                        "authorId": "48985418",
                        "name": "Miaohua Zhang"
                    },
                    {
                        "authorId": "2109355537",
                        "name": "Yongsheng Gao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f9c63c1de8de192c62d334eb97bdff30c2f6fc64",
                "externalIds": {
                    "ArXiv": "2211.15320",
                    "DBLP": "journals/corr/abs-2211-15320",
                    "DOI": "10.48550/arXiv.2211.15320",
                    "CorpusId": 254043968
                },
                "corpusId": 254043968,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f9c63c1de8de192c62d334eb97bdff30c2f6fc64",
                "title": "RankDNN: Learning to Rank for Few-shot Learning",
                "abstract": "This paper introduces a new few-shot learning pipeline that\ncasts relevance ranking for image retrieval as binary ranking\nrelation classification. In comparison to image classification,\nranking relation classification is sample efficient and\ndomain agnostic. Besides, it provides a new perspective on\nfew-shot learning and is complementary to state-of-the-art\nmethods. The core component of our deep neural network is\na simple MLP, which takes as input an image triplet encoded\nas the difference between two vector-Kronecker products,\nand outputs a binary relevance ranking order. The proposed\nRankMLP can be built on top of any state-of-the-art feature\nextractors, and our entire deep neural network is called\nthe ranking deep neural network, or RankDNN. Meanwhile,\nRankDNN can be flexibly fused with other post-processing\nmethods. During the meta test, RankDNN ranks support images\naccording to their similarity with the query samples,\nand each query sample is assigned the class label of its\nnearest neighbor. Experiments demonstrate that RankDNN\ncan effectively improve the performance of its baselines\nbased on a variety of backbones and it outperforms previous\nstate-of-the-art algorithms on multiple few-shot learning\nbenchmarks, including miniImageNet, tieredImageNet,\nCaltech-UCSD Birds, and CIFAR-FS. Furthermore, experiments\non the cross-domain challenge demonstrate the superior\ntransferability of RankDNN.The code is available at:\nhttps://github.com/guoqianyu-alberta/RankDNN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1491102079",
                        "name": "Qianyu Guo"
                    },
                    {
                        "authorId": "2192605030",
                        "name": "Hongtong Gong"
                    },
                    {
                        "authorId": "2115325401",
                        "name": "Xu Wei"
                    },
                    {
                        "authorId": "35782003",
                        "name": "Yanwei Fu"
                    },
                    {
                        "authorId": "2390300",
                        "name": "Weifeng Ge"
                    },
                    {
                        "authorId": "1841911",
                        "name": "Yizhou Yu"
                    },
                    {
                        "authorId": "2159070511",
                        "name": "Wenqiang Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Metriclearning based methods [3], [4], [6], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37] learn a good embedding and an appropriate comparison metric."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "176d9b627dbee0495db86d8e7b7961bba360845c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-16019",
                    "ArXiv": "2211.16019",
                    "DOI": "10.1109/TPAMI.2022.3223784",
                    "CorpusId": 253759250,
                    "PubMed": "36409816"
                },
                "corpusId": 253759250,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/176d9b627dbee0495db86d8e7b7961bba360845c",
                "title": "PatchMix Augmentation to Identify Causal Features in Few-Shot Learning",
                "abstract": "The task of Few-shot learning (FSL) aims to transfer the knowledge learned from base categories with sufficient labelled data to novel categories with scarce known information. It is currently an important research question and has great practical values in the real-world applications. Despite extensive previous efforts are made on few-shot learning tasks, we emphasize that most existing methods did not take into account the distributional shift caused by sample selection bias in the FSL scenario. Such a selection bias can induce spurious correlation between the semantic causal features, that are causally and semantically related to the class label, and the other non-causal features. Critically, the former ones should be invariant across changes in distributions, highly related to the classes of interest, and thus well generalizable to novel classes, while the latter ones are not stable to changes in the distribution. To resolve this problem, we propose a novel data augmentation strategy dubbed as PatchMix that can break this spurious dependency by replacing the patch-level information and supervision of the query images with random gallery images from different classes from the query ones. We theoretically show that such an augmentation mechanism, different from existing ones, is able to identify the causal features. To further make these features to be discriminative enough for classification, we propose Correlation-guided Reconstruction (CGR) and Hardness-Aware module for instance discrimination and easier discrimination between similar classes. Moreover, such a framework can be adapted to the unsupervised FSL scenario. The utility of our method is demonstrated on the state-of-the-art results consistently achieved on several benchmarks including miniImageNet, tieredImageNet, CIFAR-FS, CUB, Cars, Places and Plantae, in all settings of single-domain, cross-domain and unsupervised FSL. By studying the intra-variance property of learned features and visualizing the learned features, we further quantitatively and qualitatively show that such a promising result is due to the effectiveness in learning causal features.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2708397",
                        "name": "C. Xu"
                    },
                    {
                        "authorId": "2108118336",
                        "name": "Chen Liu"
                    },
                    {
                        "authorId": "8283163",
                        "name": "Xinwei Sun"
                    },
                    {
                        "authorId": "2187419298",
                        "name": "Siqian Yang"
                    },
                    {
                        "authorId": "2628601",
                        "name": "Yabiao Wang"
                    },
                    {
                        "authorId": "1978245",
                        "name": "Chengjie Wang"
                    },
                    {
                        "authorId": "35782003",
                        "name": "Yanwei Fu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The principle behind this approach is using limited supervision and fine-tuning in assessment [13, 51, 71]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a68c7b4002d5b2b74e00a91d79f05f0bec6e6711",
                "externalIds": {
                    "ArXiv": "2211.08672",
                    "CorpusId": 258715256
                },
                "corpusId": 258715256,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a68c7b4002d5b2b74e00a91d79f05f0bec6e6711",
                "title": "Fair contrastive pre-training for geographic image segmentation",
                "abstract": "Contrastive self-supervised learning is widely employed in visual recognition for geographic image data (remote or proximal sensing), but because of landscape heterogeneity, models can show disparate performance across spatial units. In this work, we consider fairness risks in such contrastive pre-training; we show learnt representations present large performance gaps across selected sensitive groups: urban and rural areas for satellite images and city GDP level for street view images on downstream semantic segmentation. We propose fair dense representations with contrastive learning (FairDCL) to address the issue, a multi-level latent space de-biasing objective, using a novel dense sensitive attribute encoding technique to constrain spurious local information disparately distributes across groups. The method achieves improved downstream task fairness and outperforms state-of-the-art methods for the absence of a fairness-accuracy trade-off. Image embedding evaluation and ablation studies further demonstrate effectiveness of FairDCL. As fairness in geographic imagery is a nascent topic without existing state-of-the-art data or results, our work motivates researchers to consider fairness metrics in such applications, especially reinforced by our results showing no accuracy degradation. Our code is available at: https://anonymous.4open.science/r/FairDCL-1283",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47474101",
                        "name": "Miao Zhang"
                    },
                    {
                        "authorId": "3144230",
                        "name": "R. Chunara"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The effectiveness of self-supervised learning for FSL has been demonstrated, such as contrastive learning in either unsupervised pre-training [30] or episodic training [6,25], and auxiliary rotation prediction task [13,47,41]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "464cbc77f536e11287980ce33cb6900bee1c3b2c",
                "externalIds": {
                    "DBLP": "conf/eccv/LaiYLZHWLGW22",
                    "ArXiv": "2211.00868",
                    "DOI": "10.1007/978-3-031-20044-1_1",
                    "CorpusId": 253099735
                },
                "corpusId": 253099735,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/464cbc77f536e11287980ce33cb6900bee1c3b2c",
                "title": "tSF: Transformer-Based Semantic Filter for Few-Shot Learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "15037661",
                        "name": "Jinxiang Lai"
                    },
                    {
                        "authorId": "2187419298",
                        "name": "Siqian Yang"
                    },
                    {
                        "authorId": "2184457262",
                        "name": "Wenlong Liu"
                    },
                    {
                        "authorId": "2111107023",
                        "name": "Yi Zeng"
                    },
                    {
                        "authorId": "1914918005",
                        "name": "Zhongyi Huang"
                    },
                    {
                        "authorId": "2184570180",
                        "name": "Wenlong Wu"
                    },
                    {
                        "authorId": "2184458202",
                        "name": "Jun Liu"
                    },
                    {
                        "authorId": "2226422",
                        "name": "Bin-Bin Gao"
                    },
                    {
                        "authorId": "1978245",
                        "name": "Chengjie Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3f3b1031fe1efc068f80f3b41262b96a3b8763a4",
                "externalIds": {
                    "DBLP": "journals/ijon/WuZL23",
                    "DOI": "10.1016/j.neucom.2022.11.073",
                    "CorpusId": 253866383
                },
                "corpusId": 253866383,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3f3b1031fe1efc068f80f3b41262b96a3b8763a4",
                "title": "Invariant and consistent: Unsupervised representation learning for few-shot visual recognition",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31957431",
                        "name": "Heng-Yang Wu"
                    },
                    {
                        "authorId": "151469503",
                        "name": "Yifan Zhao"
                    },
                    {
                        "authorId": "2119130343",
                        "name": "Jia Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "87b8cda4c152b8973b2271e1ab111d35ade4bd77",
                "externalIds": {
                    "DOI": "10.1117/1.JEI.31.6.063029",
                    "CorpusId": 253903008
                },
                "corpusId": 253903008,
                "publicationVenue": {
                    "id": "c677ab24-0c04-487d-83e2-c252af9479c8",
                    "name": "Journal of Electronic Imaging (JEI)",
                    "type": "journal",
                    "alternate_names": [
                        "J Electron Imaging (JEI",
                        "Journal of Electronic Imaging",
                        "J Electron Imaging"
                    ],
                    "issn": "1017-9909",
                    "url": "https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging",
                    "alternate_urls": [
                        "http://electronicimaging.spiedigitallibrary.org/journal.aspx"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/87b8cda4c152b8973b2271e1ab111d35ade4bd77",
                "title": "Dual spatial constraints-based few-shot image classification",
                "abstract": "Abstract. As one of the development directions of artificial intelligence in the future, few-shot learning has attracted more and more attention in recent years. How to make full use of the information of a small amount of samples is one of the main difficulties in the field of few-shot learning. Most of the research work utilizes the meta-learning mechanism to alleviate the negative impact of insufficient samples on model performance. However, the training between subtasks also makes it difficult for meta-learning models to obtain general feature representations between samples. Therefore, researchers are turning their research perspective to supervised learning, and they have drawn a conclusion that embedding models with good performance are simpler and more effective than complex meta-learning models. Recent research work has also proved the importance of feature representation. Based on the above view points and analysis, we propose a few-shot image classification method, which strengthens the difference of samples from different categories and the similarity of samples from the same category and realizes dual constraints in high-dimensional feature space and low-dimensional feature space. Experimental results on four public datasets demonstrate that the proposed method effectively improves the accuracy of image classification with few-shot learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2508428",
                        "name": "Songhao Zhu"
                    },
                    {
                        "authorId": "2192289273",
                        "name": "Xiong Bian"
                    },
                    {
                        "authorId": "2922281",
                        "name": "Zhiwei Liang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We split the dataset into 100 classes for training, 50 for validation, and 50 for testing following the prior standard works [19, 2].",
                "However, some recent works adopted standard supervision setting [20] along with various self-supervised approaches [15, 10, 19] to enhance the quality of the results."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f94993081bc9e6c6a10420758ef81aa05a0e8aa8",
                "externalIds": {
                    "ArXiv": "2210.11000",
                    "DBLP": "journals/corr/abs-2210-11000",
                    "DOI": "10.48550/arXiv.2210.11000",
                    "CorpusId": 253018950
                },
                "corpusId": 253018950,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f94993081bc9e6c6a10420758ef81aa05a0e8aa8",
                "title": "Visual-Semantic Contrastive Alignment for Few-Shot Image Classification",
                "abstract": "Few-Shot learning aims to train and optimize a model that can adapt to unseen visual classes with only a few labeled examples. The existing few-shot learning (FSL) methods, heavily rely only on visual data, thus fail to capture the semantic attributes to learn a more generalized version of the visual concept from very few examples. However, it is a known fact that human visual learning benefits immensely from inputs from multiple modalities such as vision, language, and audio. Inspired by the human learning nature of encapsulating the existing knowledge of a visual category which is in the form of language, we introduce a contrastive alignment mechanism for visual and semantic feature vectors to learn much more generalized visual concepts for few-shot learning. Our method simply adds an auxiliary contrastive learning objective which captures the contextual knowledge of a visual category from a strong textual encoder in addition to the existing training mechanism. Hence, the approach is more generalized and can be plugged into any existing FSL method. The pre-trained semantic feature extractor (learned from a large-scale text corpora) we use in our approach provides a strong contextual prior knowledge to assist FSL. The experimental results done in popular FSL datasets show that our approach is generic in nature and provides a strong boost to the existing FSL baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2084548724",
                        "name": "Mohamed Afham"
                    },
                    {
                        "authorId": "144952844",
                        "name": "R. Rodrigo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "58163d41c17e406c48fbed5a20437d8f3bdaf099",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-00890",
                    "ArXiv": "2211.00890",
                    "DOI": "10.1145/3503161.3547853",
                    "CorpusId": 252782052
                },
                "corpusId": 252782052,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/58163d41c17e406c48fbed5a20437d8f3bdaf099",
                "title": "Rethinking the Metric in Few-shot Learning: From an Adaptive Multi-Distance Perspective",
                "abstract": "Few-shot learning problem focuses on recognizing unseen classes given a few labeled images. In recent effort, more attention is paid to fine-grained feature embedding, ignoring the relationship among different distance metrics. In this paper, for the first time, we investigate the contributions of different distance metrics, and propose an adaptive fusion scheme, bringing significant improvements in few-shot classification. We start from a naive baseline of confidence summation and demonstrate the necessity of exploiting the complementary property of different distance metrics. By finding the competition problem among them, built upon the baseline, we propose an Adaptive Metrics Module (AMM) to decouple metrics fusion into metric-prediction fusion and metric-losses fusion. The former encourages mutual complementary, while the latter alleviates metric competition via multi-task collaborative learning. Based on AMM, we design a few-shot classification framework AMTNet, including the AMM and the Global Adaptive Loss (GAL), to jointly optimize the few-shot task and auxiliary self-supervised task, making the embedding features more robust. In the experiment, the proposed AMM achieves 2% higher performance than the naive metrics fusion module, and our AMTNet outperforms the state-of-the-arts on multiple benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "15037661",
                        "name": "Jinxiang Lai"
                    },
                    {
                        "authorId": "2187419298",
                        "name": "Siqian Yang"
                    },
                    {
                        "authorId": "2084534028",
                        "name": "Guannan Jiang"
                    },
                    {
                        "authorId": "102872728",
                        "name": "Xi Wang"
                    },
                    {
                        "authorId": "2110543875",
                        "name": "Yuxi Li"
                    },
                    {
                        "authorId": "1742462487",
                        "name": "Zihui Jia"
                    },
                    {
                        "authorId": "46772408",
                        "name": "Xiao Chen"
                    },
                    {
                        "authorId": "40478976",
                        "name": "J. Liu"
                    },
                    {
                        "authorId": "2226422",
                        "name": "Bin-Bin Gao"
                    },
                    {
                        "authorId": "2155467393",
                        "name": "Wei Zhang"
                    },
                    {
                        "authorId": "2154871188",
                        "name": "Yuan Xie"
                    },
                    {
                        "authorId": "1978245",
                        "name": "Chengjie Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Other works [10, 18, 20, 29] have shown that addition of self supervision losses in the pretraining stage provides more robust features, resulting in improved few shot performance."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "58807389e830704e04ca80d28c25e3c6e50f7dd7",
                "externalIds": {
                    "ArXiv": "2210.02476",
                    "DBLP": "conf/bmvc/ManiparambilMO22",
                    "DOI": "10.48550/arXiv.2210.02476",
                    "CorpusId": 252735224
                },
                "corpusId": 252735224,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/58807389e830704e04ca80d28c25e3c6e50f7dd7",
                "title": "BaseTransformers: Attention over base data-points for One Shot Learning",
                "abstract": "Few shot classification aims to learn to recognize novel categories using only limited samples per category. Most current few shot methods use a base dataset rich in labeled examples to train an encoder that is used for obtaining representations of support instances for novel classes. Since the test instances are from a distribution different to the base distribution, their feature representations are of poor quality, degrading performance. In this paper we propose to make use of the well-trained feature representations of the base dataset that are closest to each support instance to improve its representation during meta-test time. To this end, we propose BaseTransformers, that attends to the most relevant regions of the base dataset feature space and improves support instance representations. Experiments on three benchmark data sets show that our method works well for several backbones and achieves state-of-the-art results in the inductive one shot setting. Code is available at github.com/mayug/BaseTransformers",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46223705",
                        "name": "Mayug Maniparambil"
                    },
                    {
                        "authorId": "145470864",
                        "name": "Kevin McGuinness"
                    },
                    {
                        "authorId": "2137567915",
                        "name": "Noel E. O'Connor"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bc7854c699ca05e26b660a52ef8bb7c9d33773e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01600",
                    "ArXiv": "2210.01600",
                    "DOI": "10.48550/arXiv.2210.01600",
                    "CorpusId": 252693418
                },
                "corpusId": 252693418,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/bc7854c699ca05e26b660a52ef8bb7c9d33773e8",
                "title": "Positive Pair Distillation Considered Harmful: Continual Meta Metric Learning for Lifelong Object Re-Identification",
                "abstract": "Lifelong object re-identification incrementally learns from a stream of re-identification tasks. The objective is to learn a representation that can be applied to all tasks and that generalizes to previously unseen re-identification tasks. The main challenge is that at inference time the representation must generalize to previously unseen identities. To address this problem, we apply continual meta metric learning to lifelong object re-identification. To prevent forgetting of previous tasks, we use knowledge distillation and explore the roles of positive and negative pairs. Based on our observation that the distillation and metric losses are antagonistic, we propose to remove positive pairs from distillation to robustify model updates. Our method, called Distillation without Positive Pairs (DwoPP), is evaluated on extensive intra-domain experiments on person and vehicle re-identification datasets, as well as inter-domain experiments on the LReID benchmark. Our experiments demonstrate that DwoPP significantly outperforms the state-of-the-art. The code is here: https://github.com/wangkai930418/DwoPP_code",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2148896193",
                        "name": "Kai Wang"
                    },
                    {
                        "authorId": "46199723",
                        "name": "Chenshen Wu"
                    },
                    {
                        "authorId": "1749498",
                        "name": "Andrew D. Bagdanov"
                    },
                    {
                        "authorId": "2108855071",
                        "name": "Xialei Liu"
                    },
                    {
                        "authorId": "49080524",
                        "name": "Shiqi Yang"
                    },
                    {
                        "authorId": "114581743",
                        "name": "Shangling Jui"
                    },
                    {
                        "authorId": "2820687",
                        "name": "Joost van de Weijer"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3bf3c0b6b25994038bfb4e07b07af6c51ab960e4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-08224",
                    "ArXiv": "2209.08224",
                    "DOI": "10.48550/arXiv.2209.08224",
                    "CorpusId": 252367908
                },
                "corpusId": 252367908,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/3bf3c0b6b25994038bfb4e07b07af6c51ab960e4",
                "title": "Few-Shot Classification with Contrastive Learning",
                "abstract": "A two-stage training paradigm consisting of sequential pre-training and meta-training stages has been widely used in current few-shot learning (FSL) research. Many of these methods use self-supervised learning and contrastive learning to achieve new state-of-the-art results. However, the potential of contrastive learning in both stages of FSL training paradigm is still not fully exploited. In this paper, we propose a novel contrastive learning-based framework that seamlessly integrates contrastive learning into both stages to improve the performance of few-shot classification. In the pre-training stage, we propose a self-supervised contrastive loss in the forms of feature vector vs. feature map and feature map vs. feature map, which uses global and local information to learn good initial representations. In the meta-training stage, we propose a cross-view episodic training mechanism to perform the nearest centroid classification on two different views of the same episode and adopt a distance-scaled contrastive loss based on them. These two strategies force the model to overcome the bias between views and promote the transferability of representations. Extensive experiments on three benchmark datasets demonstrate that our method achieves competitive results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "66591757",
                        "name": "Zhanyuan Yang"
                    },
                    {
                        "authorId": "46585071",
                        "name": "Jinghua Wang"
                    },
                    {
                        "authorId": "40592392",
                        "name": "Ying J. Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f84c66d3f23e6c489c62e3ca92f479c20954bdd8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-06993",
                    "ArXiv": "2209.06993",
                    "DOI": "10.48550/arXiv.2209.06993",
                    "CorpusId": 252280506
                },
                "corpusId": 252280506,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f84c66d3f23e6c489c62e3ca92f479c20954bdd8",
                "title": "Learning from Future: A Novel Self-Training Framework for Semantic Segmentation",
                "abstract": "Self-training has shown great potential in semi-supervised learning. Its core idea is to use the model learned on labeled data to generate pseudo-labels for unlabeled samples, and in turn teach itself. To obtain valid supervision, active attempts typically employ a momentum teacher for pseudo-label prediction yet observe the confirmation bias issue, where the incorrect predictions may provide wrong supervision signals and get accumulated in the training process. The primary cause of such a drawback is that the prevailing self-training framework acts as guiding the current state with previous knowledge, because the teacher is updated with the past student only. To alleviate this problem, we propose a novel self-training strategy, which allows the model to learn from the future. Concretely, at each training step, we first virtually optimize the student (i.e., caching the gradients without applying them to the model weights), then update the teacher with the virtual future student, and finally ask the teacher to produce pseudo-labels for the current student as the guidance. In this way, we manage to improve the quality of pseudo-labels and thus boost the performance. We also develop two variants of our future-self-training (FST) framework through peeping at the future both deeply (FST-D) and widely (FST-W). Taking the tasks of unsupervised domain adaptive semantic segmentation and semi-supervised semantic segmentation as the instances, we experimentally demonstrate the effectiveness and superiority of our approach under a wide range of settings. Code will be made publicly available.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2132788144",
                        "name": "Ye Du"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "48017277",
                        "name": "Haochen Wang"
                    },
                    {
                        "authorId": "2544305",
                        "name": "Jingjing Fei"
                    },
                    {
                        "authorId": "48625175",
                        "name": "Wei Li"
                    },
                    {
                        "authorId": "49279300",
                        "name": "Liwei Wu"
                    },
                    {
                        "authorId": "2153291683",
                        "name": "Rui Zhao"
                    },
                    {
                        "authorId": "2566501",
                        "name": "Zehua Fu"
                    },
                    {
                        "authorId": "2333334",
                        "name": "Qingjie Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Gidaris et al. (2019) and Su et al. (2020) study the use of self-supervised tasks including rotation(Gidaris et al., 2018) and jigsaw(Noroozi & Favaro, 2016) to improve few-shot learning."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cfe6557596badb090bc14a459db8f3ab6500d68b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-02073",
                    "ArXiv": "2209.02073",
                    "DOI": "10.48550/arXiv.2209.02073",
                    "CorpusId": 252090284
                },
                "corpusId": 252090284,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cfe6557596badb090bc14a459db8f3ab6500d68b",
                "title": "A Study on Representation Transfer for Few-Shot Learning",
                "abstract": "Few-shot classification aims to learn to classify new object categories well using only a few labeled examples. Transferring feature representations from other models is a popular approach for solving few-shot classification problems. In this work we perform a systematic study of various feature representations for few-shot classification, including representations learned from MAML, supervised classification, and several common self-supervised tasks. We find that learning from more complex tasks tend to give better representations for few-shot classification, and thus we propose the use of representations learned from multiple tasks for few-shot classification. Coupled with new tricks on feature selection and voting to handle the issue of small sample size, our direct transfer learning method offers performance comparable to state-of-art on several benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40040135",
                        "name": "Chun-Nam Yu"
                    },
                    {
                        "authorId": "2149182696",
                        "name": "Yi Xie"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Self-supervision has been proven to improve few-shot learning in various recent works [20, 8] as it helps to overcome supervision-collapse [20], a phenomenon where training on the base classes force the network to discard information irrelevant for the discrimination of base classes, but crucial for the",
                "S2M2 [8] extends their work with self-supervision techniques [20].",
                "22 WACV20 Proto+Jig [20] ResNet18 - 89.",
                "In this work, we opt rotation prediction [20] mainly because of its simplicity and effectiveness [20, 8].",
                "For other four datasets, we follow the split of [20].",
                "\u2020 denotes the values are reported from the implementation in [20]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6a6190641a37ebd80232205bd641a66c01646a38",
                "externalIds": {
                    "ArXiv": "2209.10250",
                    "DBLP": "journals/corr/abs-2209-10250",
                    "DOI": "10.1016/j.patcog.2022.109049",
                    "CorpusId": 252407742
                },
                "corpusId": 252407742,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6a6190641a37ebd80232205bd641a66c01646a38",
                "title": "Query-Guided Networks for Few-shot Fine-grained Classification and Person Search",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "113898027",
                        "name": "Bharti Munjal"
                    },
                    {
                        "authorId": "2162586026",
                        "name": "Alessandro Flaborea"
                    },
                    {
                        "authorId": "40404576",
                        "name": "S. Amin"
                    },
                    {
                        "authorId": "50516802",
                        "name": "F. Tombari"
                    },
                    {
                        "authorId": "1787725",
                        "name": "Fabio Galasso"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We employ the setting in ProtoNet [5] and conduct a cross-domain experiment where our model is trained on miniImagenet and evaluated on the CUB dataset.",
                "Specifically, we use the ResNet-18 as the backbone, then the episodic training mechanism is used to classify each query sample into one of the N support classes, called N-way K-shot task as the ProtoNet [5] shows.",
                "ProtoNet [5] regards the mean value of each class\u2019s embedding as prototype and calculates Euclidean distance between support and query samples for classification.",
                "Method Stanford Cars Stanford Dogs\n5-way 1-shot 5-way 5-shot 5-way1-shot 5-way 5-shot\nMatchNet [4, 68] GNN [69, 68] DN4 [13] ProtoNet [5, 68] MAML [3, 70] RelationNet [12, 70] MML [71] ATL-Net [16] CovaMNet [68] PABN+cpt [69] LRPABN+cpt [69] MATANets [71]\n34.80 55.85 61.51 40.90 47.22 47.67 72.43 67.95 56.65 54.44 60.28 73.15\n44.70 71.25 89.60 52.93 61.21 60.59 91.05 89.16 71.33 67.36 73.29 91.89\n35.80 46.98 55.85 40.90 44.81 43.33 59.05 54.49 49.10 45.65 45.72 55.63\n47.50 62.27 63.51 48.19 58.68 55.23 75.59 73.20 63.04 61.24 60.94 70.29\nPMRN (ours) 87.91 94.38 80.11 89.18",
                "MatchNet [41, 4] MatchNet [49, 4, 15] RelationNet [41, 12] RelationNet [12, 41] ProtoNet [41, 5] MAML [3, 51] SCA+MAML++ [63] Baseline [41] Baseline++ [41] S2M2 [64] DeepEMD [15] DEML [65] ATL-Net [16] Cosine classifier [41] DSN [67] FRN [58] CPDE [24] CFA [66] MCL [17] ResNet-18 ResNet-12 ResNet-18 ResNet-34 ResNet-18 ResNet-18 DenseNet ResNet-18 ResNet-18 ResNet-18 ResNet-12 ResNet-50 ConvNet ResNet-18 ResNet-12 ResNet-12 ResNet-18 ResNet-18 ResNet-12 73.",
                "Recently, some work [41, 23, 42] introduce SSL methods such as predicting the rotation and the relative position to FSL.",
                "In addition, we follow the same setting as Su [41] to train some baselines from scratch on Oxford Flowers and FGVC Aircraft.",
                "Our model is implemented with the Pytorch [53] based on the codebase for few-shot learning denoted in [41]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "86d81c92340bda3ebdab8136186af00add52283f",
                "externalIds": {
                    "ArXiv": "2208.09717",
                    "CorpusId": 259165507
                },
                "corpusId": 259165507,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/86d81c92340bda3ebdab8136186af00add52283f",
                "title": "Learning Primitive-aware Discriminative Representations for Few-shot Learning",
                "abstract": "Few-shot learning (FSL) aims to learn a classifier that can be easily adapted to recognize novel classes with only a few labeled examples. Some recent work about FSL has yielded promising classification performance, where the image-level feature is used to calculate the similarity among samples for classification. However, the image-level feature ignores abundant fine-grained and structural in-formation of objects that may be transferable and consistent between seen and unseen classes. How can humans easily identify novel classes with several sam-ples? Some study from cognitive science argues that humans can recognize novel categories through primitives. Although base and novel categories are non-overlapping, they can share some primitives in common. Inspired by above re-search, we propose a Primitive Mining and Reasoning Network (PMRN) to learn primitive-aware representations based on metric-based FSL model. Concretely, we first add Self-supervision Jigsaw task (SSJ) for feature extractor parallelly, guiding the model to encode visual pattern corresponding to object parts into fea-ture channels. To further mine discriminative representations, an Adaptive Chan-nel Grouping (ACG) method is applied to cluster and weight spatially and se-mantically related visual patterns to generate a group of visual primitives. To fur-ther enhance the discriminability and transferability of primitives, we propose a visual primitive Correlation Reasoning Network (CRN) based on graph convolu-tional network to learn abundant structural information and internal correlation among primitives. Finally, a primitive-level metric is conducted for classification in a meta-task based on episodic training strategy. Extensive experiments show that our method achieves state-of-the-art results on six standard benchmarks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118801703",
                        "name": "Jian Yang"
                    },
                    {
                        "authorId": "2153662703",
                        "name": "Yuhang Niu"
                    },
                    {
                        "authorId": "2033687",
                        "name": "Xuemei Xie"
                    },
                    {
                        "authorId": "48745275",
                        "name": "G. Shi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "The choice for the auxiliary self-supervised objective was motivated by a series of works that successfully applied it to improve few-shot classification [8, 16, 46], robustness [23], pre-training for novel class discovery [19], image generation [10, 32], semi-supervised learning [54]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "06a4f5125ca5347f79aafea53f9bb05d1daa0a7e",
                "externalIds": {
                    "ArXiv": "2208.08217",
                    "DBLP": "journals/corr/abs-2208-08217",
                    "DOI": "10.48550/arXiv.2208.08217",
                    "CorpusId": 251623281
                },
                "corpusId": 251623281,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/06a4f5125ca5347f79aafea53f9bb05d1daa0a7e",
                "title": "How does the degree of novelty impacts semi-supervised representation learning for novel class retrieval?",
                "abstract": "Supervised representation learning with deep networks tends to overfit the training classes and the generalization to novel classes is a challenging question. It is common to evaluate a learned embedding on held-out images of the same training classes. In real applications however, data comes from new sources and novel classes are likely to arise. We hypothesize that incorporating unlabelled images of novel classes in the training set in a semi-supervised fashion would be beneficial for the efficient retrieval of novel-class images compared to a vanilla supervised representation. To verify this hypothesis in a comprehensive way, we propose an original evaluation methodology that varies the degree of novelty of novel classes by partitioning the dataset category-wise either randomly, or semantically, i.e. by minimizing the shared semantics between base and novel classes. This evaluation procedure allows to train a representation blindly to any novel-class labels and evaluate the frozen representation on the retrieval of base or novel classes. We find that a vanilla supervised representation falls short on the retrieval of novel classes even more so when the semantics gap is higher. Semi-supervised algorithms allow to partially bridge this performance gap but there is still much room for improvement.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50782614",
                        "name": "Q. Leroy"
                    },
                    {
                        "authorId": "2066146221",
                        "name": "Olivier Buisson"
                    },
                    {
                        "authorId": "144406930",
                        "name": "A. Joly"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3cba9690c2d70143e7ef62fa32eea2ed86d98bae",
                "externalIds": {
                    "DBLP": "journals/tits/KubinBASTS22",
                    "DOI": "10.1109/tits.2021.3114816",
                    "CorpusId": 243368834
                },
                "corpusId": 243368834,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3cba9690c2d70143e7ef62fa32eea2ed86d98bae",
                "title": "Deep Crash Detection From Vehicular Sensor Data With Multimodal Self-Supervision",
                "abstract": "The ability to detect vehicle accidents from on-board sensor data is of the utmost importance to provide prompt assistance to prevent injuries and fatalities. In this article, we present a novel deep learning method capable of analyzing time series recorded from Inertial Measurement Units (IMU) and GPS devices to recognize the presence of an accident along with its severity. We propose a neural architecture capable of exploiting the different sensor streams (i.e., acceleration, gyroscope, and GPS speed), a multimodal contrastive self-supervised training procedure, and an ad-hoc stack of data augmentation techniques, specifically designed to counteract the extreme class imbalance and to improve the generalization capabilities of the whole pipeline. The proposed method has been validated against several state-of-the-art methods on a large and highly imbalanced dataset, composed of more than 200 thousand time series collected from US vehicles, with different vehicle sizes and traveling on different types of road. Our method achieves an average-precision score (AP) of 0.9 in the detection of crashes and 0.76 in the detection of severe crashes, significantly outperforming all the other approaches, and has small footprint and latency, so that it can easily be deployed on embedded devices.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2042693230",
                        "name": "Luca Kubin"
                    },
                    {
                        "authorId": "2093102",
                        "name": "Tommaso Bianconcini"
                    },
                    {
                        "authorId": "2124744013",
                        "name": "Douglas Coimbra de Andrade"
                    },
                    {
                        "authorId": "2533756",
                        "name": "Matteo Simoncini"
                    },
                    {
                        "authorId": "2616063",
                        "name": "L. Taccari"
                    },
                    {
                        "authorId": "1443777687",
                        "name": "Francesco Sambo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "self-supervised task may interfere with the main task if both tasks are not properly aligned [41,53,67]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "911770d6614af8f46a352b7427ad47a76984ab2f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-11707",
                    "ArXiv": "2207.11707",
                    "DOI": "10.48550/arXiv.2207.11707",
                    "CorpusId": 251040952
                },
                "corpusId": 251040952,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/911770d6614af8f46a352b7427ad47a76984ab2f",
                "title": "Improving Test-Time Adaptation via Shift-agnostic Weight Regularization and Nearest Source Prototypes",
                "abstract": "This paper proposes a novel test-time adaptation strategy that adjusts the model pre-trained on the source domain using only unlabeled online data from the target domain to alleviate the performance degradation due to the distribution shift between the source and target domains. Adapting the entire model parameters using the unlabeled online data may be detrimental due to the erroneous signals from an unsupervised objective. To mitigate this problem, we propose a shift-agnostic weight regularization that encourages largely updating the model parameters sensitive to distribution shift while slightly updating those insensitive to the shift, during test-time adaptation. This regularization enables the model to quickly adapt to the target domain without performance degradation by utilizing the benefit of a high learning rate. In addition, we present an auxiliary task based on nearest source prototypes to align the source and target features, which helps reduce the distribution shift and leads to further performance improvement. We show that our method exhibits state-of-the-art performance on various standard benchmarks and even outperforms its supervised counterpart.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48919377",
                        "name": "Sungha Choi"
                    },
                    {
                        "authorId": "30779333",
                        "name": "Seunghan Yang"
                    },
                    {
                        "authorId": "31335213",
                        "name": "Seokeon Choi"
                    },
                    {
                        "authorId": "3057834",
                        "name": "Sungrack Yun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Most works [25, 49, 67] leverage the pretext task of SSL as an auxiliary loss to enhance the representation learning of supervised pre-training.",
                "Subsequently, many studies [13, 47, 49, 55, 67] focus on how to learn a good embedding instead of designing complex meta-learning strategies."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "32ff43704a04191cf0b90a1dac7cbbc8f5df12a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-09176",
                    "ArXiv": "2207.09176",
                    "DOI": "10.48550/arXiv.2207.09176",
                    "CorpusId": 250643930
                },
                "corpusId": 250643930,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/32ff43704a04191cf0b90a1dac7cbbc8f5df12a3",
                "title": "Self-Supervision Can Be a Good Few-Shot Learner",
                "abstract": "Existing few-shot learning (FSL) methods rely on training with a large labeled dataset, which prevents them from leveraging abundant unlabeled data. From an information-theoretic perspective, we propose an effective unsupervised FSL method, learning representations with self-supervision. Following the InfoMax principle, our method learns comprehensive representations by capturing the intrinsic structure of the data. Specifically, we maximize the mutual information (MI) of instances and their representations with a low-bias MI estimator to perform self-supervised pre-training. Rather than supervised pre-training focusing on the discriminable features of the seen classes, our self-supervised model has less bias toward the seen classes, resulting in better generalization for unseen classes. We explain that supervised pre-training and self-supervised pre-training are actually maximizing different MI objectives. Extensive experiments are further conducted to analyze their FSL performance with various training settings. Surprisingly, the results show that self-supervised pre-training can outperform supervised pre-training under the appropriate conditions. Compared with state-of-the-art FSL methods, our approach achieves comparable performance on widely used FSL benchmarks without any labels of the base classes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159316456",
                        "name": "Yuning Lu"
                    },
                    {
                        "authorId": "147383784",
                        "name": "Liangjiang Wen"
                    },
                    {
                        "authorId": "2144167531",
                        "name": "Jianzhuang Liu"
                    },
                    {
                        "authorId": "2144470300",
                        "name": "Yajing Liu"
                    },
                    {
                        "authorId": "40434674",
                        "name": "Xinmei Tian"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", data augmentation (DA) and self-supervised learning (SSL), following existing works [11,37,28,26,40]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "519175bab6f5080da936689828576c6dcb7173fa",
                "externalIds": {
                    "DBLP": "conf/eccv/ZhangHLW22",
                    "ArXiv": "2207.06989",
                    "DOI": "10.48550/arXiv.2207.06989",
                    "CorpusId": 250526176
                },
                "corpusId": 250526176,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/519175bab6f5080da936689828576c6dcb7173fa",
                "title": "Tree Structure-Aware Few-Shot Image Classification via Hierarchical Aggregation",
                "abstract": ". In this paper, we mainly focus on the problem of how to learn additional feature representations for few-shot image classification through pretext tasks ( e.g. , rotation or color permutation and so on). This additional knowledge generated by pretext tasks can further improve the performance of few-shot learning (FSL) as it differs from human-annotated supervision ( i.e. , class labels of FSL tasks). To solve this problem, we present a plug-in Hierarchical Tree Structure-aware (HTS) method, which not only learns the relationship of FSL and pretext tasks, but more importantly, can adaptively select and aggregate feature representations generated by pretext tasks to maximize the performance of FSL tasks. A hierarchical tree constructing component and a gated selection aggregating component is introduced to construct the tree structure and find richer transferable knowledge that can rapidly adapt to novel classes with a few labeled images. Extensive experiments show that our HTS can significantly enhance multiple few-shot methods to achieve new state-of-the-art performance on four benchmark datasets. The code is available at: https://github.com/remiMZ/HTS-ECCV22 .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39767557",
                        "name": "M. Zhang"
                    },
                    {
                        "authorId": "122132048",
                        "name": "Siteng Huang"
                    },
                    {
                        "authorId": "35660603",
                        "name": "Wenbin Li"
                    },
                    {
                        "authorId": "2111224425",
                        "name": "Donglin Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "contexts": [
                "(Zhang et al., 2020; Caccia et al., 2021; Mazumder et al., 2021; Su et al., 2020), relinquished pre-training altogether and employed auxiliary pretext task to boost task-agnostic learning.",
                "\u2026than supervised pre-training in CL. Owing to additional computational effort, some of the approaches, e.g. (Zhang et al., 2020; Caccia et al., 2021; Mazumder et al., 2021; Su et al., 2020), relinquished pre-training altogether and employed auxiliary pretext task to boost task-agnostic learning."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2308462ab329f5f83a4ccc1b52ff3903d3c2814b",
                "externalIds": {
                    "DBLP": "conf/collas/BhatZA22",
                    "ArXiv": "2207.06267",
                    "DOI": "10.48550/arXiv.2207.06267",
                    "CorpusId": 250491932
                },
                "corpusId": 250491932,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2308462ab329f5f83a4ccc1b52ff3903d3c2814b",
                "title": "Task Agnostic Representation Consolidation: a Self-supervised based Continual Learning Approach",
                "abstract": "Continual learning (CL) over non-stationary data streams remains one of the long-standing challenges in deep neural networks (DNNs) as they are prone to catastrophic forgetting. CL models can benefit from self-supervised pre-training as it enables learning more generalizable task-agnostic features. However, the effect of self-supervised pre-training diminishes as the length of task sequences increases. Furthermore, the domain shift between pre-training data distribution and the task distribution reduces the generalizability of the learned representations. To address these limitations, we propose Task Agnostic Representation Consolidation (TARC), a two-stage training paradigm for CL that intertwines task-agnostic and task-specific learning whereby self-supervised training is followed by supervised learning for each task. To further restrict the deviation from the learned representations in the self-supervised stage, we employ a task-agnostic auxiliary loss during the supervised stage. We show that our training paradigm can be easily added to memory- or regularization-based approaches and provides consistent performance gain across more challenging CL settings. We further show that it leads to more robust and well-calibrated models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48000147",
                        "name": "Prashant Bhat"
                    },
                    {
                        "authorId": "2107033749",
                        "name": "Bahram Zonooz"
                    },
                    {
                        "authorId": "51109237",
                        "name": "E. Arani"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d03dd793513e1bf599ff1d6b595056caa9802801",
                "externalIds": {
                    "DBLP": "journals/tcsv/HaoHCT22",
                    "DOI": "10.1109/TCSVT.2021.3132912",
                    "CorpusId": 244932874
                },
                "corpusId": 244932874,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d03dd793513e1bf599ff1d6b595056caa9802801",
                "title": "Global-Local Interplay in Semantic Alignment for Few-Shot Learning",
                "abstract": "Few-shot learning aims to recognize novel classes from only a few labeled training examples. Aligning semantically relevant local regions has shown promise in effectively comparing a query image with support images. However, global information is usually overlooked in the existing approaches, resulting in a higher possibility of learning semantics unrelated to the global information. To address this issue, we propose a Global-Local Interplay Metric Learning (GLIML) framework to employ the interplay between global features and local features to guide semantic alignment. We first design a Global-Local Information Concurrent Learning (GLICL) module to extract both global features and local features and perform global-local interplay. We then design a Global-Local Information Cross-Covariance Estimator (GLICCE) to learn the similarity on the global-local interplay, in contrast to the current practice where only local features are considered. Visualizations show that the global-local interplay decreases (1) the weights placed on the semantics that are irrelevant to the global information and (2) the variability of the learned features within every class in the feature space. Quantitative experiments on three benchmark datasets demonstrate that GLIML achieves state-of-the-art performance while maintaining high efficiency.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "41022398",
                        "name": "Fusheng Hao"
                    },
                    {
                        "authorId": "51209425",
                        "name": "Fengxiang He"
                    },
                    {
                        "authorId": "2116819244",
                        "name": "Jun Cheng"
                    },
                    {
                        "authorId": "143719920",
                        "name": "D. Tao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "(Chen et al. 2019b) that current FSL methods degraded significantly when encountering domain shifts.",
                "Taking gender classification (2-way) as an example, most FSL methods can only reach 55%@accuracy, seeming that there is no improvement to random guessing.",
                "Recently, these methods have been proposed for solving FSL (Chen et al. 2019a; Gidaris et al. 2019; Su et al. 2019) and cross-domain (Xu et al.",
                "Second, we design a generalized taskagnostic test, where we re-think the generalization ability of existing FSL methods.",
                "We can draw following conclusion from this figure: 1) Under traditional FSL scope, models enjoy the benefit of pre-training process.",
                "Furthermore, we follow the general FSL setting that ignores X t in the training phase and does not require any fine-tuning processes, enabling fast model deployment to the unseen task.",
                "Recently, these methods have been proposed for solving FSL (Chen et al. 2019a; Gidaris et al. 2019; Su et al. 2019) and cross-domain (Xu et al. 2019; Carlucci et al. 2019) tasks.",
                "However, as indicated by (Su et al. 2019), episode technique might discard semantic information that is irrelevant for base classes but critical for novel classes.",
                "As a significant advance, few-shot learning (FSL), which generalizes the metaknowledge in base classes (sufficient samples) to novel classes (few labeled data), has attracted considerable attention.",
                "Few-shot Learning (FSL): C(Ds)\u2229C(Dt) = \u2205, P (Ds) \u2248 P (Dt), and the support data is few.",
                "Homogeneous task (Motiian et al. 2017; Teshima et al. 2020) which weakens the FSL requirements is not within the scope of this work.\nexpected that these task consistency can largely stabilize the meta-learning process and hence improve the generalization ability of the trained model."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e22db1c9eb5ce1ab4226e26309878126c6fe3d1f",
                "externalIds": {
                    "DBLP": "conf/aaai/YuanZWS0M22",
                    "DOI": "10.1609/aaai.v36i3.20230",
                    "CorpusId": 250294952
                },
                "corpusId": 250294952,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e22db1c9eb5ce1ab4226e26309878126c6fe3d1f",
                "title": "Task-Level Self-Supervision for Cross-Domain Few-Shot Learning",
                "abstract": "Learning with limited labeled data is a long-standing problem. Among various solutions, episodic training progres-sively classifies a series of few-shot tasks and thereby is as-sumed to be beneficial for improving the model\u2019s generalization ability. However, recent studies show that it is eveninferior to the baseline model when facing domain shift between base and novel classes. To tackle this problem, we pro-pose a domain-independent task-level self-supervised (TL-SS) method for cross-domain few-shot learning.TL-SS strategy promotes the general idea of label-based instance-levelsupervision to task-level self-supervision by augmenting mul-tiple views of tasks. Two regularizations on task consistencyand correlation metric are introduced to remarkably stabi-lize the training process and endow the generalization ability into the prediction model. We also propose a high-order associated encoder (HAE) being adaptive to various tasks.By utilizing 3D convolution module, HAE is able to generate proper parameters and enables the encoder to flexibly toany unseen tasks. Two modules complement each other andshow great promotion against state-of-the-art methods experimentally. Finally, we design a generalized task-agnostic test,where our intriguing findings highlight the need to re-think the generalization ability of existing few-shot approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114109236",
                        "name": "Wang Yuan"
                    },
                    {
                        "authorId": "2108963095",
                        "name": "Zhizhong Zhang"
                    },
                    {
                        "authorId": "2171700638",
                        "name": "Cong Wang"
                    },
                    {
                        "authorId": "115918366",
                        "name": "Haichuan Song"
                    },
                    {
                        "authorId": "2154871672",
                        "name": "Yuan Xie"
                    },
                    {
                        "authorId": "2149343311",
                        "name": "Lizhuang Ma"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Furthermore, SSL has been shown to be a key component for few-shot applications [14, 20, 52], zero-shot generalization [62], and semi-supervised learning [68] among others."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b293467d6dd1595e8c32d0539dae5aa44d52a2b7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-08339",
                    "ArXiv": "2206.08339",
                    "DOI": "10.48550/arXiv.2206.08339",
                    "CorpusId": 249712416
                },
                "corpusId": 249712416,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b293467d6dd1595e8c32d0539dae5aa44d52a2b7",
                "title": "iBoot: Image-bootstrapped Self-Supervised Video Representation Learning",
                "abstract": "Learning visual representations through self-supervision is an extremely challenging task as the network needs to sieve relevant patterns from spurious distractors without the active guidance provided by supervision. This is achieved through heavy data augmentation, large-scale datasets and prohibitive amounts of compute. Video self-supervised learning (SSL) suffers from added challenges: video datasets are typically not as large as image datasets, compute is an order of magnitude larger, and the amount of spurious patterns the optimizer has to sieve through is multiplied several fold. Thus, directly learning self-supervised representations from video data might result in sub-optimal performance. To address this, we propose to utilize a strong image-based model, pre-trained with self- or language supervision, in a video representation learning framework, enabling the model to learn strong spatial and temporal information without relying on the video labeled data. To this end, we modify the typical video-based SSL design and objective to encourage the video encoder to \\textit{subsume} the semantic content of an image-based model trained on a general domain. The proposed algorithm is shown to learn much more efficiently (i.e. in less epochs and with a smaller batch) and results in a new state-of-the-art performance on standard downstream tasks among single-modality SSL methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9120887",
                        "name": "F. Saleh"
                    },
                    {
                        "authorId": "2055882010",
                        "name": "Fuwen Tan"
                    },
                    {
                        "authorId": "145245424",
                        "name": "Adrian Bulat"
                    },
                    {
                        "authorId": "2137359565",
                        "name": "Georgios Tzimiropoulos"
                    },
                    {
                        "authorId": "145944235",
                        "name": "Brais Mart\u00ednez"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Although self-supervised learning is underrepresented in the context of few-shot learning, some recent works [16, 34, 45] have shown that self-supervision via pretext tasks can be beneficial when integrated as auxiliary loss."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "27bfbb21230e90bb373e5c02fa01ae205b3e3f10",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-07267",
                    "ArXiv": "2206.07267",
                    "DOI": "10.48550/arXiv.2206.07267",
                    "CorpusId": 249674585
                },
                "corpusId": 249674585,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/27bfbb21230e90bb373e5c02fa01ae205b3e3f10",
                "title": "Rethinking Generalization in Few-Shot Classification",
                "abstract": "Single image-level annotations only correctly describe an often small subset of an image's content, particularly when complex real-world scenes are depicted. While this might be acceptable in many classification scenarios, it poses a significant challenge for applications where the set of classes differs significantly between training and test time. In this paper, we take a closer look at the implications in the context of $\\textit{few-shot learning}$. Splitting the input samples into patches and encoding these via the help of Vision Transformers allows us to establish semantic correspondences between local regions across images and independent of their respective class. The most informative patch embeddings for the task at hand are then determined as a function of the support set via online optimization at inference time, additionally providing visual interpretability of `$\\textit{what matters most}$' in the image. We build on recent advances in unsupervised training of networks via masked image modelling to overcome the lack of fine-grained labels and learn the more general statistical structure of the data while avoiding negative image-level annotation influence, $\\textit{aka}$ supervision collapse. Experimental results show the competitiveness of our approach, achieving new state-of-the-art results on four popular few-shot classification benchmarks for $5$-shot and $1$-shot scenarios.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47467782",
                        "name": "M. Hiller"
                    },
                    {
                        "authorId": "2146354647",
                        "name": "Rongkai Ma"
                    },
                    {
                        "authorId": "23911916",
                        "name": "Mehrtash Harandi"
                    },
                    {
                        "authorId": "144418842",
                        "name": "T. Drummond"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b82f01a87fe726d8356f3e44fb9c1ba47d349eac",
                "externalIds": {
                    "DBLP": "journals/patterns/WilsonB23",
                    "PubMedCentral": "10140600",
                    "DOI": "10.1016/j.patter.2023.100693",
                    "CorpusId": 249613201,
                    "PubMed": "37123442"
                },
                "corpusId": 249613201,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b82f01a87fe726d8356f3e44fb9c1ba47d349eac",
                "title": "SynapseCLR: Uncovering features of synapses in primary visual cortex through contrastive representation learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1434750166",
                        "name": "A. Wilson"
                    },
                    {
                        "authorId": "2862417",
                        "name": "M. Babadi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "We also seek to strengthen the baseline with orthogonal techniques such as self-supervised learning (SSL) [18, 50] and knowledge distillation (KD) [52].",
                "A popular group of methods in FSL focus on strengthening the backbone network by various techniques, from increasing model capacity [6, 11], self-supervised learning (SSL) [18, 50, 64], to knowledge distillation (KD) [52].",
                "For SSL, we implement the \"rotation baseline\u201d (Rot baseline).",
                "With SSL, we employ the \u201crot baseline\u201d, which is trained with the standard cross-entropy loss and an auxiliary loss to predict the rotation angles of the perturbed images.",
                "Moreover, our method is agnostic to the backbone network; thus, it does not have the need of a training phase to adopt as in SSL and KD, while incurring just a little overhead at inference (for fine-tuning prototypes with approximately 200 gradient updating steps).",
                "We also conduct experiments with additional loss functions including self-supervised loss (SSL) and knowledge distillation (KD) in Figure 2a.",
                "It can be observed that combining POODLE with other techniques such as SSL and KD achieves comparable or outperform state-of-the-art techniques for both inductive/transductive inference on two datasets.",
                "Accordingly, the SSL loss function Lssl is given by:\nLssl(\u03b8, \u03c6;Db,R) = 1\nNb|R| Nb\u2211 i=0 \u2211 j\u2208R |R|\u2211 k=0 I(k = j) log p\u03c6(k|f\u03b8(xji )) (7)\nHere, I(\u00b7) is the indicator function and p\u03c6(\u00b7|f\u03b8) denotes the (predicted) probability of rotation angle."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d9cfcc03374cd67141af93a143a65080869e9090",
                "externalIds": {
                    "ArXiv": "2206.04679",
                    "DBLP": "journals/corr/abs-2206-04679",
                    "DOI": "10.48550/arXiv.2206.04679",
                    "CorpusId": 248498306
                },
                "corpusId": 248498306,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d9cfcc03374cd67141af93a143a65080869e9090",
                "title": "POODLE: Improving Few-shot Learning via Penalizing Out-of-Distribution Samples",
                "abstract": "In this work, we propose to use out-of-distribution samples, i.e., unlabeled samples coming from outside the target classes, to improve few-shot learning. Specifically, we exploit the easily available out-of-distribution samples to drive the classifier to avoid irrelevant features by maximizing the distance from prototypes to out-of-distribution samples while minimizing that of in-distribution samples (i.e., support, query data). Our approach is simple to implement, agnostic to feature extractors, lightweight without any additional cost for pre-training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed method consistently improves the performance of pretrained networks with different architectures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2059393417",
                        "name": "Duong H. Le"
                    },
                    {
                        "authorId": "144295869",
                        "name": "Khoi Duc Minh Nguyen"
                    },
                    {
                        "authorId": "144295869",
                        "name": "Khoi Duc Minh Nguyen"
                    },
                    {
                        "authorId": "3090093",
                        "name": "Quoc-Huy Tran"
                    },
                    {
                        "authorId": "2789311",
                        "name": "R. Nguyen"
                    },
                    {
                        "authorId": "143807806",
                        "name": "Binh-Son Hua"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "[15,16] propose to integrate SSL into few-shot learning by adding an auxiliary SSL pretext task in a few-shot model."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "04bda0c0a61b5cd62a85a731aca1814d86b7f629",
                "externalIds": {
                    "DBLP": "journals/jcst/SunOSD22",
                    "DOI": "10.1007/s11390-022-2029-5",
                    "CorpusId": 249839700
                },
                "corpusId": 249839700,
                "publicationVenue": {
                    "id": "2d2707e1-5dbb-4002-8849-876154b870a8",
                    "name": "Journal of Computational Science and Technology",
                    "type": "journal",
                    "alternate_names": [
                        "Journal of Computer Science and Technology",
                        "J Comput Sci Technol"
                    ],
                    "issn": "1881-6894",
                    "alternate_issns": [
                        "1666-6038",
                        "1000-9000"
                    ],
                    "url": "https://www.jstage.jst.go.jp/browse/jcst",
                    "alternate_urls": [
                        "https://rd.springer.com/journal/11390",
                        "https://link.springer.com/journal/11390",
                        "http://journal.info.unlp.edu.ar/",
                        "https://www.jstage.jst.go.jp/browse/jcst/list/-char/en"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/04bda0c0a61b5cd62a85a731aca1814d86b7f629",
                "title": "Self-Supervised Task Augmentation for Few-Shot Intent Detection",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2075417110",
                        "name": "Peng Sun"
                    },
                    {
                        "authorId": null,
                        "name": "Ya-Wen Ouyang"
                    },
                    {
                        "authorId": "1610927763",
                        "name": "Dingjie Song"
                    },
                    {
                        "authorId": "2158176877",
                        "name": "Xinmiao Dai"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Similarly, The pre-tasks were also used to color the image [128] and predict the relative position of each patch [129, 130], local Fisher discriminant [131]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "74487ea7c7a27a1a45c5a4dc306aea47cef0238d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-06743",
                    "ArXiv": "2205.06743",
                    "DOI": "10.1145/3582688",
                    "CorpusId": 248798765
                },
                "corpusId": 248798765,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/74487ea7c7a27a1a45c5a4dc306aea47cef0238d",
                "title": "A Comprehensive Survey of Few-shot Learning: Evolution, Applications, Challenges, and Opportunities",
                "abstract": "Few-shot learning (FSL) has emerged as an effective learning method and shows great potential. Despite the recent creative works in tackling FSL tasks, learning valid information rapidly from just a few or even zero samples remains a serious challenge. In this context, we extensively investigated 200+ FSL papers published in top journals and conferences in the past three years, aiming to present a timely and comprehensive overview of the most recent advances in FSL with a fresh perspective and to provide an impartial comparison of the strengths and weaknesses of existing work. To avoid conceptual confusion, we first elaborate and contrast a set of relevant concepts including few-shot learning, transfer learning, and meta-learning. Then, we inventively extract prior knowledge related to few-shot learning in the form of a pyramid, which summarizes and classifies previous work in detail from the perspective of challenges. Furthermore, to enrich this survey, we present in-depth analysis and insightful discussions of recent advances in each subsection. What is more, taking computer vision as an example, we highlight the important application of FSL, covering various research hotspots. Finally, we conclude the survey with unique insights into technology trends and potential future research opportunities to guide FSL follow-up research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1678381",
                        "name": "Yisheng Song"
                    },
                    {
                        "authorId": "2155393230",
                        "name": "Ting-Yuan Wang"
                    },
                    {
                        "authorId": "2231689993",
                        "name": "Puyu Cai"
                    },
                    {
                        "authorId": "2203223",
                        "name": "S. Mondal"
                    },
                    {
                        "authorId": "2141370576",
                        "name": "J. P. Sahoo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "33f4b18a3b5592b64a014c36cbbbd6bc793d344c",
                "externalIds": {
                    "DBLP": "journals/ipm/LastillaAFKMS22",
                    "DOI": "10.1016/j.ipm.2022.102875",
                    "CorpusId": 246735691
                },
                "corpusId": 246735691,
                "publicationVenue": {
                    "id": "37f5b9b7-f828-4ae1-a174-45b538cbd4e4",
                    "name": "Information Processing & Management",
                    "type": "journal",
                    "alternate_names": [
                        "Inf Process Manag",
                        "Inf Process  Manag",
                        "Information Processing and Management"
                    ],
                    "issn": "0306-4573",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/244/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/information-processing-and-management/",
                        "http://www.sciencedirect.com/science/journal/03064573",
                        "http://www.journals.elsevier.com/information-processing-and-management/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/33f4b18a3b5592b64a014c36cbbbd6bc793d344c",
                "title": "Self-supervised learning for medieval handwriting identification: A case study from the Vatican Apostolic Library",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "73003120",
                        "name": "L. Lastilla"
                    },
                    {
                        "authorId": "2069372367",
                        "name": "Serena Ammirati"
                    },
                    {
                        "authorId": "1728335",
                        "name": "D. Firmani"
                    },
                    {
                        "authorId": "2505902",
                        "name": "N. Komodakis"
                    },
                    {
                        "authorId": "1796590",
                        "name": "P. Merialdo"
                    },
                    {
                        "authorId": "2154051363",
                        "name": "Simone Scardapane"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1a5cfd3fdcc7867c8e3d66f411af19a87d2840be",
                "externalIds": {
                    "ArXiv": "2204.05104",
                    "DBLP": "conf/mm/YuanHDSG0R22",
                    "DOI": "10.1145/3503161.3548121",
                    "CorpusId": 248085301
                },
                "corpusId": 248085301,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1a5cfd3fdcc7867c8e3d66f411af19a87d2840be",
                "title": "Self-Supervised Graph Neural Network for Multi-Source Domain Adaptation",
                "abstract": "Domain adaptation (DA) tries to tackle the scenarios when the test data does not fully follow the same distribution of the training data, and multi-source domain adaptation (MSDA) is very attractive for real world applications. By learning from large-scale unlabeled samples, self-supervised learning has now become a new trend in deep learning. It is worth noting that both self-supervised learning and multi-source domain adaptation share a similar goal: they both aim to leverage unlabeled data to learn more expressive representations. Unfortunately, traditional multi-task self-supervised learning faces two challenges: (1) the pretext task may not strongly relate to the downstream task, thus it could be difficult to learn useful knowledge being shared from the pretext task to the target task; (2) when the same feature extractor is shared between the pretext task and the downstream one and only different prediction heads are used, it is ineffective to enable inter-task information exchange and knowledge sharing. To address these issues, we propose a novel Self-Supervised Graph Neural Network (SSG), where a graph neural network is used as the bridge to enable more effective inter-task information exchange and knowledge sharing. More expressive representation is learned by adopting a mask token strategy to mask some domain information. Our extensive experiments have demonstrated that our proposed SSG method has achieved state-of-the-art results over four multi-source domain adaptation datasets, which have shown the effectiveness of our proposed SSG method from different aspects.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2150063810",
                        "name": "Jin Yuan"
                    },
                    {
                        "authorId": "49272105",
                        "name": "Feng Hou"
                    },
                    {
                        "authorId": "3165307",
                        "name": "Yangzhou Du"
                    },
                    {
                        "authorId": "2558130",
                        "name": "Zhongchao Shi"
                    },
                    {
                        "authorId": "1735299",
                        "name": "Xin Geng"
                    },
                    {
                        "authorId": "2152732801",
                        "name": "Jianping Fan"
                    },
                    {
                        "authorId": "2113828170",
                        "name": "Yong Rui"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The augmented data can be in the form of hallucination with a data generator function [25, 60], using unlabelled data under semi-supervised [44, 70] or selfsupervised [21,50] frameworks, or aligning the novel classes to the base data [1]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ae3031a792f91c747a9381b6b08537ae33d4adb3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-00949",
                    "ArXiv": "2204.00949",
                    "DOI": "10.1109/CVPR52688.2022.00881",
                    "CorpusId": 247939410
                },
                "corpusId": 247939410,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ae3031a792f91c747a9381b6b08537ae33d4adb3",
                "title": "Matching Feature Sets for Few-Shot Image Classification",
                "abstract": "In image classification, it is common practice to train deep networks to extract a single feature vector per input image. Few-shot classification methods also mostly follow this trend. In this work, we depart from this established direction and instead propose to extract sets of feature vectors for each image. We argue that a set-based representation intrinsically builds a richer representation of images from the base classes, which can subsequently better transfer to the few-shot classes. To do so, we propose to adapt existing feature extractors to instead produce sets of feature vectors from images. Our approach, dubbed SetFeat, embeds shallow self-attention mechanisms inside existing encoder architectures. The attention modules are lightweight, and as such our method results in encoders that have approximately the same number of parameters as their original versions. During training and inference, a set-to-set matching metric is used to perform image classification. The effectiveness of our proposed architecture and metrics is demonstrated via thorough experiments on standard few-shot datasets-namely miniImageNet, tieredImageNet, and CUB-in both the 1- and 5-shot scenarios. In all cases but one, our method outperforms the state-of-the-art.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9050817",
                        "name": "Arman Afrasiyabi"
                    },
                    {
                        "authorId": "1777528",
                        "name": "H. Larochelle"
                    },
                    {
                        "authorId": "144430305",
                        "name": "Jean-Fran\u00e7ois Lalonde"
                    },
                    {
                        "authorId": "11146706",
                        "name": "Christian Gagn'e"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Among them, the meta-learning [3, 4, 5, 6, 7, 8] and fine-tuning methods [9] achieve excellent performance."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a060372b807624b4a862e2ec668b00bebad9162e",
                "externalIds": {
                    "DOI": "10.1088/1742-6596/2253/1/012025",
                    "CorpusId": 248260276
                },
                "corpusId": 248260276,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a060372b807624b4a862e2ec668b00bebad9162e",
                "title": "Improving Few-Shot Learning with Vision Transformer",
                "abstract": "Vision Transformer (ViT) is emerging as an alternative to convolutional neural network (CNN) for visual recognition and has achieved impressive results, however, due to its data-hungry nature, ViT encoder in few-shot setting remains rarely explored. In this paper, we first propose a simple yet effective baseline that exploits the complementarity of self-supervised learning (SSL) and ViT, enabling the use of standard ViT model as the few-shot learner. Second, based on the baseline, we introduce a novel regularized fine-tuning framework, where the Parametric Instance discrimination (PID) and Base-Novel (BN) regularizations are proposed to reduce the intra-class variance and calibrate the biased distribution of novel classes to further enhance few-shot recognition. We conduct extensive experiments and show that our method can achieve new state-of-the-art performances on two widely used benchmarks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1380129958",
                        "name": "Di Qi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "A work (Su, Maji, and Hariharan 2020) show that attaching self-supervised tasks using data across domains can boost the performance for conventional few-shot learning."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "45ecf9c6c2a32b857a1600d001db6a47320a7211",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-09014",
                    "ArXiv": "2202.09014",
                    "CorpusId": 246996613
                },
                "corpusId": 246996613,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/45ecf9c6c2a32b857a1600d001db6a47320a7211",
                "title": "How Well Do Self-Supervised Methods Perform in Cross-Domain Few-Shot Learning?",
                "abstract": "Cross-domain few-shot learning (CDFSL) remains a largely unsolved problem in the area of computer vision, while self-supervised learning presents a promising solution. Both learning methods attempt to alleviate the dependency of deep networks on the requirement of large-scale labeled data. Although self-supervised methods have recently advanced dramatically, their utility on CDFSL is relatively unexplored. In this paper, we investigate the role of self-supervised representation learning in the context of CDFSL via a thorough evaluation of existing methods. It comes as a surprise that even with shallow architectures or small training datasets, self-supervised methods can perform favorably compared to the existing SOTA methods. Nevertheless, no single self-supervised approach dominates all datasets indicating that existing self-supervised methods are not universally applicable. In addition, we find that representations extracted from self-supervised methods exhibit stronger robustness than the supervised method. Intriguingly, whether self-supervised representations perform well on the source domain has little correlation with their applicability on the target domain. As part of our study, we conduct an objective measurement of the performance for six kinds of representative classifiers. The results suggest Prototypical Classifier as the standard evaluation recipe for CDFSL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108362202",
                        "name": "Yiyi Zhang"
                    },
                    {
                        "authorId": "2157975039",
                        "name": "Ying Zheng"
                    },
                    {
                        "authorId": "2132113092",
                        "name": "Xiaogang Xu"
                    },
                    {
                        "authorId": "2152812945",
                        "name": "Jun Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b5edd7f241c19e9ff729a320d8b9c0316ed23044",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08951",
                    "ArXiv": "2201.08951",
                    "DOI": "10.1109/icassp43922.2022.9747568",
                    "CorpusId": 246240530
                },
                "corpusId": 246240530,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/b5edd7f241c19e9ff729a320d8b9c0316ed23044",
                "title": "Visual Representation Learning with Self-Supervised Attention for Low-Label High-Data Regime",
                "abstract": "Self-supervision has shown outstanding results for natural language processing, and more recently, for image recognition. Simultaneously, vision transformers and its variants have emerged as a promising and scalable alternative to convolutions on various computer vision tasks. In this paper, we are the first to question if self-supervised vision transformers (SSL-ViTs) can be adapted to two important computer vision tasks in the low-label, high-data regime: few-shot image classification and zero-shot image retrieval. The motivation is to reduce the number of manual annotations required to train a visual embedder, and to produce generalizable and semantically meaningful embeddings. For few-shot image classification we train SSL-ViTs without any supervision, on external data, and use this trained embedder to adapt quickly to novel classes with limited number of labels. For zero-shot image retrieval, we use SSL-ViTs pre-trained on a large dataset without any labels and fine-tune them with several metric learning objectives. Our self-supervised attention representations outperforms the state-of-the-art on several public benchmarks for both tasks, namely miniImageNet and CUB200 for few-shot image classification by up-to 6%-10%, and Stanford Online Products, Cars196 and CUB200 for zero-shot image retrieval by up-to 4%-11%. Code is available at https://github.com/AutoVision-cloud/SSL-ViT-lowlabel-highdata.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50705060",
                        "name": "Prarthana Bhattacharyya"
                    },
                    {
                        "authorId": "4870207",
                        "name": "Chenge Li"
                    },
                    {
                        "authorId": "2154966118",
                        "name": "Xiaonan Zhao"
                    },
                    {
                        "authorId": "2065410341",
                        "name": "Istv'an Feh'erv'ari"
                    },
                    {
                        "authorId": "2110677274",
                        "name": "Jason Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Self-training has shown empirical success in diversified applications such as few-shot image classification (Su et al., 2020; Xie et al., 2020; Chen et al., 2020a; Yalniz et al., 2019; Zoph et al., 2020), objective detection (Rosenberg et al.",
                "Self-training has shown empirical success in diversified applications such as few-shot image classification (Su et al., 2020; Xie et al., 2020; Chen et al., 2020a; Yalniz et al., 2019; Zoph et al., 2020), objective detection (Rosenberg et al., 2005), robustness-aware model training against\u2026"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d208842ecfb0c356315a6393df471b2042e383fc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08514",
                    "ArXiv": "2201.08514",
                    "CorpusId": 246210284
                },
                "corpusId": 246210284,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d208842ecfb0c356315a6393df471b2042e383fc",
                "title": "How does unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis",
                "abstract": "Self-training, a semi-supervised learning algorithm, leverages a large amount of unlabeled data to improve learning when the labeled data are limited. Despite empirical successes, its theoretical characterization remains elusive. To the best of our knowledge, this work establishes the first theoretical analysis for the known iterative self-training paradigm and proves the benefits of unlabeled data in both training convergence and generalization ability. To make our theoretical analysis feasible, we focus on the case of one-hidden-layer neural networks. However, theoretical understanding of iterative self-training is non-trivial even for a shallow neural network. One of the key challenges is that existing neural network landscape analysis built upon supervised learning no longer holds in the (semi-supervised) self-training paradigm. We address this challenge and prove that iterative self-training converges linearly with both convergence rate and generalization accuracy improved in the order of $1/\\sqrt{M}$, where $M$ is the number of unlabeled samples. Experiments from shallow neural networks to deep neural networks are also provided to justify the correctness of our established theoretical insights on self-training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108432203",
                        "name": "Shuai Zhang"
                    },
                    {
                        "authorId": "39872583",
                        "name": "M. Wang"
                    },
                    {
                        "authorId": "143743061",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "153191489",
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "authorId": "145042856",
                        "name": "Jinjun Xiong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Self-supervision has also been studied in few-shot learning [14], [76], [94], [98] where transformation-based auxiliary self-supervised classifiers are employed to improve the robustness of few-shot learning models."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7c8c78cb4633383295eaf3ad1903f1f340f4d073",
                "externalIds": {
                    "DBLP": "journals/tmm/ZhangLK23",
                    "ArXiv": "2201.05916",
                    "DOI": "10.1109/TMM.2022.3142955",
                    "CorpusId": 246015407
                },
                "corpusId": 246015407,
                "publicationVenue": {
                    "id": "10e76a35-58d6-443c-9683-fc16f2dd0a92",
                    "name": "IEEE transactions on multimedia",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Transactions on Multimedia",
                        "IEEE Trans Multimedia",
                        "IEEE trans multimedia"
                    ],
                    "issn": "1520-9210",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046"
                },
                "url": "https://www.semanticscholar.org/paper/7c8c78cb4633383295eaf3ad1903f1f340f4d073",
                "title": "Multi-Level Second-Order Few-Shot Learning",
                "abstract": "We propose a Multi-level Second-order (MlSo) few-shot learning network for supervised or unsupervised few-shot image classification and few-shot action recognition. We leverage so-called power-normalized second-order base learner streams combined with features that express multiple levels of visual abstraction, and we use self-supervised discriminating mechanisms. As Second-order Pooling (SoP) is popular in image recognition, we employ its basic element-wise variant in our pipeline. The goal of multi-level feature design is to extract feature representations at different layer-wise levels of CNN, realizing several levels of visual abstraction to achieve robust few-shot learning. As SoP can handle convolutional feature maps of varying spatial sizes, we also introduce image inputs at multiple spatial scales into MlSo. To exploit the discriminative information from multi-level and multi-scale features, we develop a Feature Matching (FM) module that reweights their respective branches. We also introduce a self-supervised step, which is a discriminator of the spatial level and the scale of abstraction. Our pipeline is trained in an end-to-end manner. With a simple architecture, we demonstrate respectable results on standard datasets such as Omniglot, mini\u2013ImageNet, tiered\u2013ImageNet, Open MIC, fine-grained datasets such as CUB Birds, Stanford Dogs and Cars, and action recognition datasets such as HMDB51, UCF101, and mini\u2013MIT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108970904",
                        "name": "Hongguang Zhang"
                    },
                    {
                        "authorId": "46382489",
                        "name": "Hongdong Li"
                    },
                    {
                        "authorId": "2155775",
                        "name": "Piotr Koniusz"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "contexts": [
                "Distribution shifts can be generalized better (Sun et al., 2020), and few-shot learning is enhanced as long as the distribution of images used for meta-learning and self-supervised learning are not too different (Su et al., 2020).",
                "The increasing advantage self-supervision has when the amount of data available is reduced is in accordance with Su et al. (2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0f540d551257ec50e96313add87a8446c0904b8d",
                "externalIds": {
                    "DOI": "10.1002/sta4.455",
                    "CorpusId": 245957145
                },
                "corpusId": 245957145,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0f540d551257ec50e96313add87a8446c0904b8d",
                "title": "Improving image classification robustness using self\u2010supervision",
                "abstract": "Self\u2010supervised learning allows training of neural networks without immense, high\u2010quality or labelled data sets. We demonstrate that self\u2010supervision furthermore improves robustness of models using small, imbalanced or incomplete data sets which pose severe difficulties to supervised models. For small data sets, the accuracy of our approach is up to 12.5% higher using MNIST and 15.2% using Fashion\u2010MNIST compared to random initialization. Moreover, self\u2010supervision influences the way of learning itself, which means that in case of small or strongly imbalanced data sets, it can be prevented that classes are not or insufficiently learned. Even if input data are corrupted and large image regions are missing from the training set, self\u2010supervision significantly improves classification accuracy (up to 7.3% for MNIST and 2.2% for Fashion\u2010MNIST). In addition, we analyse combinations of data manipulations and seek to generate a better understanding of how pretext accuracy and downstream accuracy are related. This is not only important to ensure optimal pretraining but also for training with unlabelled data in order to find an appropriate evaluation measure. As such, we make an important contribution to learning with realistic data sets and making machine learning accessible to application areas that require expensive and difficult data collection.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "102346092",
                        "name": "Ladyna Wittscher"
                    },
                    {
                        "authorId": "2037075617",
                        "name": "Jan Diers"
                    },
                    {
                        "authorId": "1902141",
                        "name": "Christian Pigorsch"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "This makes annotation a laborious, biased, and ambiguous task, motivating the need for newer paradigms such as few-shot learning [54, 48, 35, 43] and self-supervised"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "08a027154b8926ca374ba7fd1786db407c8f915c",
                "externalIds": {
                    "ArXiv": "2204.02958",
                    "DBLP": "journals/corr/abs-2204-02958",
                    "DOI": "10.1109/WACV51458.2022.00310",
                    "CorpusId": 246870384
                },
                "corpusId": 246870384,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/08a027154b8926ca374ba7fd1786db407c8f915c",
                "title": "LEAD: Self-Supervised Landmark Estimation by Aligning Distributions of Feature Similarity",
                "abstract": "In this work, we introduce LEAD, an approach to dis-cover landmarks from an unannotated collection of category-specific images. Existing works in self-supervised landmark detection are based on learning dense (pixel-level) feature representations from an image, which are further used to learn landmarks in a semi-supervised manner. While there have been advances in self-supervised learning of image features for instance-level tasks like classification, these methods do not ensure dense equivariant representations. The property of equivariance is of interest for dense prediction tasks like landmark estimation. In this work, we introduce an approach to enhance the learning of dense equivariant representations in a self-supervised fashion. We follow a two-stage training approach: first, we train a network using the BYOL [13] objective which operates at an instance level. The correspondences obtained through this network are further used to train a dense and compact representation of the image using a lightweight network. We show that having such a prior in the feature extractor helps in landmark detection, even under drastically limited number of annotations while also improving generalization across scale variations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "115373370",
                        "name": "Tejan Karmali"
                    },
                    {
                        "authorId": "2154621042",
                        "name": "Abhinav Atrishi"
                    },
                    {
                        "authorId": "2154622177",
                        "name": "Sai Sree Harsha"
                    },
                    {
                        "authorId": "2008802066",
                        "name": "Susmit Agrawal"
                    },
                    {
                        "authorId": "2131639924",
                        "name": "Varun Jampani"
                    },
                    {
                        "authorId": "144682140",
                        "name": "R. Venkatesh Babu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", colorization [78, 79], rotations of inputs [22], modeling image similarity and dissimilarity among multiple views [4, 5, 8\u201310, 31, 76], to name a few), and transfer them to target domains, such as object detection [23, 24, 32, 55], action recognition [6, 56], or various fine-grained recognition tasks [11, 18, 58, 64]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "df5ce7ff629f8f6888fa2c6b9f8fbc5bc60852c4",
                "externalIds": {
                    "ArXiv": "2112.08459",
                    "DBLP": "journals/corr/abs-2112-08459",
                    "CorpusId": 245218536
                },
                "corpusId": 245218536,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/df5ce7ff629f8f6888fa2c6b9f8fbc5bc60852c4",
                "title": "Rethinking Nearest Neighbors for Visual Classification",
                "abstract": "Neural network classifiers have become the de-facto choice for current\"pre-train then fine-tune\"paradigms of visual classification. In this paper, we investigate k-Nearest-Neighbor (k-NN) classifiers, a classical model-free learning method from the pre-deep learning era, as an augmentation to modern neural network based approaches. As a lazy learning method, k-NN simply aggregates the distance between the test image and top-k neighbors in a training set. We adopt k-NN with pre-trained visual representations produced by either supervised or self-supervised methods in two steps: (1) Leverage k-NN predicted probabilities as indications for easy vs. hard examples during training. (2) Linearly interpolate the k-NN predicted distribution with that of the augmented classifier. Via extensive experiments on a wide range of classification tasks, our study reveals the generality and flexibility of k-NN integration with additional insights: (1) k-NN achieves competitive results, sometimes even outperforming a standard linear classifier. (2) Incorporating k-NN is especially beneficial for tasks where parametric classifiers perform poorly and / or in low-data regimes. We hope these discoveries will encourage people to rethink the role of pre-deep learning, classical methods in computer vision. Our code is available at: https://github.com/KMnP/nn-revisit.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51502783",
                        "name": "Menglin Jia"
                    },
                    {
                        "authorId": "33970300",
                        "name": "Bor-Chun Chen"
                    },
                    {
                        "authorId": "3099139",
                        "name": "Zuxuan Wu"
                    },
                    {
                        "authorId": "2064285348",
                        "name": "Claire Cardie"
                    },
                    {
                        "authorId": "2067789287",
                        "name": "S. Belongie"
                    },
                    {
                        "authorId": "38760573",
                        "name": "Ser-Nam Lim"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8fcf2c89df04c9cf55d2a8d50567f7716b08e006",
                "externalIds": {
                    "ArXiv": "2112.01719",
                    "DBLP": "conf/aaai/MaFDH22",
                    "DOI": "10.1609/aaai.v36i2.20087",
                    "CorpusId": 244923783
                },
                "corpusId": 244923783,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8fcf2c89df04c9cf55d2a8d50567f7716b08e006",
                "title": "Adaptive Poincar\u00e9 Point to Set Distance for Few-Shot Classification",
                "abstract": "Learning and generalizing from limited examples, i.e., few-shot learning, is of core importance to many real-world vision applications. A principal way of achieving few-shot learning is to realize an embedding where samples from different classes are distinctive. Recent studies suggest that embedding via hyperbolic geometry enjoys low distortion for hierarchical and structured data, making it suitable for few-shot learning. In this paper, we propose to learn a context-aware hyperbolic metric to characterize the distance between a point and a set associated with a learned set to set distance. To this end, we formulate the metric as a weighted sum on the tangent bundle of the hyperbolic space and develop a mechanism to obtain the weights adaptively, based on the constellation of the points. This not only makes the metric local but also dependent on the task in hand, meaning that the metric will adapt depending on the samples that it compares. We empirically show that such metric yields robustness in the presence of outliers and achieves a tangible improvement over baseline models. This includes the state-of-the-art results on five popular few-shot classification benchmarks, namely mini-ImageNet, tiered-ImageNet, Caltech-UCSD Birds-200-2011(CUB), CIFAR-FS, and FC100.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2146354647",
                        "name": "Rongkai Ma"
                    },
                    {
                        "authorId": "49208246",
                        "name": "Pengfei Fang"
                    },
                    {
                        "authorId": "144418842",
                        "name": "T. Drummond"
                    },
                    {
                        "authorId": "1686714",
                        "name": "M. Harandi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "This is perhaps not as surprising as previous works have noted that the self-supervised losses improve few-shot learning [14, 39, 50].",
                "Other methods use a combination of self-supervised and semi-supervised learning techniques [14, 39, 50], which is sometimes followed by an additional step where the model\u2019s predictions are used to train a \u201cstudent model\u201d using distillation [8, 47, 48, 53].",
                ", [39]) is less effective, but using the hierarchy to exclude the novel categories leads to a small improvement in some cases."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "09c3ca27403c2f73fbebf04729856a675f3fecaf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-11595",
                    "ArXiv": "2111.11595",
                    "CorpusId": 244488608
                },
                "corpusId": 244488608,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/09c3ca27403c2f73fbebf04729856a675f3fecaf",
                "title": "Semi-Supervised Learning with Taxonomic Labels",
                "abstract": "We propose techniques to incorporate coarse taxonomic labels to train image classifiers in fine-grained domains. Such labels can often be obtained with a smaller effort for fine-grained domains such as the natural world where categories are organized according to a biological taxonomy. On the Semi-iNat dataset consisting of 810 species across three Kingdoms, incorporating Phylum labels improves the Species level classification accuracy by 6% in a transfer learning setting using ImageNet pre-trained models. Incorporating the hierarchical label structure with a state-of-the-art semi-supervised learning algorithm called FixMatch improves the performance further by 1.3%. The relative gains are larger when detailed labels such as Class or Order are provided, or when models are trained from scratch. However, we find that most methods are not robust to the presence of out-of-domain data from novel classes. We propose a technique to select relevant data from a large collection of unlabeled images guided by the hierarchy which improves the robustness. Overall, our experiments show that semi-supervised learning with coarse taxonomic labels are practical for training classifiers in fine-grained domains.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148766061",
                        "name": "Jong-Chyi Su"
                    },
                    {
                        "authorId": "35208858",
                        "name": "Subhransu Maji"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "develop fine-grained few-shot learning (FGFS) methods [148], [198], [199], [200]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c56eb865d1c9427602eecd89869eb2faf700f4f6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-06119",
                    "ArXiv": "2111.06119",
                    "DOI": "10.1109/TPAMI.2021.3126648",
                    "CorpusId": 243940200,
                    "PubMed": "34752384"
                },
                "corpusId": 243940200,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c56eb865d1c9427602eecd89869eb2faf700f4f6",
                "title": "Fine-Grained Image Analysis With Deep Learning: A Survey",
                "abstract": "Fine-grained image analysis (FGIA) is a longstanding and fundamental problem in computer vision and pattern recognition, and underpins a diverse set of real-world applications. The task of FGIA targets analyzing visual objects from subordinate categories, e.g., species of birds or models of cars. The small inter-class and large intra-class variation inherent to fine-grained image analysis makes it a challenging problem. Capitalizing on advances in deep learning, in recent years we have witnessed remarkable progress in deep learning powered FGIA. In this paper we present a systematic survey of these advances, where we attempt to re-define and broaden the field of FGIA by consolidating two fundamental fine-grained research areas \u2013 fine-grained image recognition and fine-grained image retrieval. In addition, we also review other key issues of FGIA, such as publicly available benchmark datasets and related domain-specific applications. We conclude by highlighting several research directions and open problems which need further exploration from the community.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2126047",
                        "name": "Xiu-Shen Wei"
                    },
                    {
                        "authorId": "1705408",
                        "name": "Yi-Zhe Song"
                    },
                    {
                        "authorId": "2918822",
                        "name": "Oisin Mac Aodha"
                    },
                    {
                        "authorId": "1808816",
                        "name": "Jianxin Wu"
                    },
                    {
                        "authorId": "143753918",
                        "name": "Yuxin Peng"
                    },
                    {
                        "authorId": "8053308",
                        "name": "Jinhui Tang"
                    },
                    {
                        "authorId": "51460259",
                        "name": "Jian Yang"
                    },
                    {
                        "authorId": "2067789287",
                        "name": "S. Belongie"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a83a8ec238a17cc88315e19c7cf2b7b3926aa0e1",
                "externalIds": {
                    "DBLP": "conf/cvpr/WangLBHJ022",
                    "ArXiv": "2111.04993",
                    "DOI": "10.1109/CVPRW56347.2022.00417",
                    "CorpusId": 243860695
                },
                "corpusId": 243860695,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a83a8ec238a17cc88315e19c7cf2b7b3926aa0e1",
                "title": "Incremental Meta-Learning via Episodic Replay Distillation for Few-Shot Image Recognition",
                "abstract": "In this paper we consider the problem of incremental meta-learning in which classes are presented incrementally in discrete tasks. We propose Episodic Replay Distillation (ERD), that mixes classes from the current task with exemplars from previous tasks when sampling episodes for meta-learning. To allow the training to benefit from a large as possible variety of classes, which leads to more generalizable feature representations, we propose the cross-task meta loss. Furthermore, we propose episodic replay distillation that also exploits exemplars for improved knowledge distillation. Experiments on four datasets demonstrate that ERD surpasses the state-of-the-art. In particular, on the more challenging one-shot, long task sequence scenarios, we reduce the gap between Incremental Meta-Learning and the joint-training upper bound from 3.5% / 10.1% / 13.4% / 11.7% with the current state-of-the-art to 2.6% / 2.9% / 5.0% / 0.2% with our method on Tiered-ImageNet / Mini-ImageNet / CIFAR100 / CUB, respectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148896193",
                        "name": "Kai Wang"
                    },
                    {
                        "authorId": "2108855071",
                        "name": "Xialei Liu"
                    },
                    {
                        "authorId": "1749498",
                        "name": "Andrew D. Bagdanov"
                    },
                    {
                        "authorId": "143603281",
                        "name": "Luis Herranz"
                    },
                    {
                        "authorId": "2121582188",
                        "name": "Shang Rui"
                    },
                    {
                        "authorId": "2820687",
                        "name": "Joost van de Weijer"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "18573ce15d379f185b44c282215e887d7874b1bb",
                "externalIds": {
                    "DBLP": "journals/cmpb/ZoetmulderGCM22",
                    "DOI": "10.1016/j.cmpb.2021.106539",
                    "CorpusId": 244541624,
                    "PubMed": "34875512"
                },
                "corpusId": 244541624,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/18573ce15d379f185b44c282215e887d7874b1bb",
                "title": "Domain- and task-specific transfer learning for medical segmentation tasks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1411066828",
                        "name": "Riaan Zoetmulder"
                    },
                    {
                        "authorId": "2304222",
                        "name": "E. Gavves"
                    },
                    {
                        "authorId": "1435233335",
                        "name": "M. Caan"
                    },
                    {
                        "authorId": "2256501790",
                        "name": "Henk A. Marquering"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Still others improve performance by incorporating more general machine learning techniques, like self-supervised learning [8, 33] or knowledge distillation [36]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "55345706d97e75209bd4f5d4c4effc84e6280724",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-13386",
                    "ArXiv": "2110.13386",
                    "CorpusId": 239885714
                },
                "corpusId": 239885714,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/55345706d97e75209bd4f5d4c4effc84e6280724",
                "title": "Self-Denoising Neural Networks for Few Shot Learning",
                "abstract": "In this paper, we introduce a new architecture for few shot learning, the task of teaching a neural network from as few as one or five labeled examples. Inspired by the theoretical results of Alaine et al that Denoising Autoencoders refine features to lie closer to the true data manifold, we present a new training scheme that adds noise at multiple stages of an existing neural architecture while simultaneously learning to be robust to this added noise. This architecture, which we call a Self-Denoising Neural Network (SDNN), can be applied easily to most modern convolutional neural architectures, and can be used as a supplement to many existing few-shot learning techniques. We empirically show that SDNNs out-perform previous state-of-the-art methods for few shot image recognition using the Wide-ResNet architecture on the \\textit{mini}ImageNet, tiered-ImageNet, and CIFAR-FS few shot learning datasets. We also perform a series of ablation experiments to empirically justify the construction of the SDNN architecture. Finally, we show that SDNNs even improve few shot performance on the task of human action detection in video using experiments on the ActEV SDL Surprise Activities challenge.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "52023459",
                        "name": "S. Schwarcz"
                    },
                    {
                        "authorId": "3403576",
                        "name": "Sai Saketh Rambhatla"
                    },
                    {
                        "authorId": "69416958",
                        "name": "Ramalingam Chellappa"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For example, recent work [26, 9, 35] uses unlabeled data from the novel classes: it is, after all, the labels that are expensive; data is often cheap."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "43e256c49c5611e89b816839887a913b48a0aa37",
                "externalIds": {
                    "DBLP": "conf/iccv/PhooH21",
                    "DOI": "10.1109/ICCV48922.2021.00892",
                    "CorpusId": 244114794
                },
                "corpusId": 244114794,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/43e256c49c5611e89b816839887a913b48a0aa37",
                "title": "Coarsely-labeled Data for Better Few-shot Transfer",
                "abstract": "Few-shot learning is based on the premise that labels are expensive, especially when they are fine-grained and require expertise. But coarse labels might be easy to acquire and thus abundant. We present a representation learning approach - PAS that allows few-shot learners to leverage coarsely-labeled data available before evaluation. Inspired by self-training, we label the additional data using a teacher trained on the base dataset and filter the teacher\u2019s prediction based on the coarse labels; a new student representation is then trained on the base dataset and the pseudo-labeled dataset. PAS is able to produce a representation that consistently and significantly outperforms the baselines in 3 different datasets. Code is available at https://github.com/cpphoo/PAS",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51257044",
                        "name": "Cheng Perng Phoo"
                    },
                    {
                        "authorId": "1790580",
                        "name": "Bharath Hariharan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Prior works that use additional unlabelled data for few-shot classification include [4,14,35,51,61,72].",
                "Complementary to [4,14,61] that exploit unlabelled data via self-supervised objectives in the prior learning phase, we use unlabelled data specifically for task-specific finetuning."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cd80f90caa386003c221891620f6947940564585",
                "externalIds": {
                    "ArXiv": "2109.09883",
                    "DBLP": "journals/corr/abs-2109-09883",
                    "DOI": "10.1109/ICCV48922.2021.00890",
                    "CorpusId": 237581491
                },
                "corpusId": 237581491,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/cd80f90caa386003c221891620f6947940564585",
                "title": "On the Importance of Distractors for Few-Shot Classification",
                "abstract": "Few-shot classification aims at classifying categories of a novel task by learning from just a few (typically, 1 to 5) labelled examples. An effective approach to few-shot classification involves a prior model trained on a large-sample base domain, which is then finetuned over the novel few-shot task to yield generalizable representations. However, task-specific finetuning is prone to overfitting due to the lack of enough training examples. To alleviate this issue, we propose a new finetuning approach based on contrastive learning that reuses unlabelled examples from the base do-main in the form of distractors. Unlike the nature of unlabelled data used in prior works, distractors belong to classes that do not overlap with the novel categories. We demonstrate for the first time that inclusion of such distractors can significantly boost few-shot generalization. Our technical novelty includes a stochastic pairing of examples sharing the same category in the few-shot task and a weighting term that controls the relative influence of task-specific negatives and distractors. An important aspect of our finetuning objective is that it is agnostic to distractor labels and hence applicable to various base domain settings. More precisely, compared to state-of-the-art approaches, our method shows accuracy gains of up to 12% in cross-domain and up to 5% in unsupervised prior-learning settings. Our code is available at https://github.com/quantacode/Contrastive-Finetuning.git",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2052289398",
                        "name": "Rajshekhar Das"
                    },
                    {
                        "authorId": "2302062",
                        "name": "Yu-Xiong Wang"
                    },
                    {
                        "authorId": "144915495",
                        "name": "Jos\u00e9 M. F. Moura"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recent work [32] shows that self-supervised learning can also improve the few-shot learning performance in image classification."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "99d38e707c36ffa7502985bb0edb16428391e791",
                "externalIds": {
                    "DBLP": "conf/itsc/MaSLT21",
                    "DOI": "10.1109/itsc48978.2021.9564510",
                    "CorpusId": 239973690
                },
                "corpusId": 239973690,
                "publicationVenue": {
                    "id": "17b6641a-6ee7-414d-8de5-997ba376f619",
                    "name": "International Conference on Intelligent Transportation Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Intell Transp Syst",
                        "ITSC"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/99d38e707c36ffa7502985bb0edb16428391e791",
                "title": "Multi-Agent Driving Behavior Prediction across Different Scenarios with Self-Supervised Domain Knowledge",
                "abstract": "How to make precise multi-agent trajectory prediction is a crucial problem in the context of autonomous driving. It is significant to have the ability to predict surrounding road participants' behaviors in many different, seen or unseen scenarios for enhancing autonomous driving safety and efficiency. Extensive research has been conducted to improve the overall prediction performance based on one enormous dataset or pay attention to some specified scenarios. However, how to generalize the prediction to different scenarios is less investigated. In this paper, we introduce a graph-neural-network-based framework for multi-agent interaction-aware trajectory prediction. In contrast to recent works which use the Cartesian coordinate system and global context images directly as input, we propose to leverage human's prior knowledge such as the comprehension of pairwise relations between agents and pairwise context information extracted by self-supervised learning approaches to attain an effective Fren\u00e9t-based representation. We evaluate our method across different traffic scenarios with diverse layouts and compare it with state-of-the-art methods. We demonstrate that our approach achieves superior performance in terms of overall performance, zero-shot and few-shot transferability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9780100",
                        "name": "Hengbo Ma"
                    },
                    {
                        "authorId": "2130294539",
                        "name": "Yaofeng Sun"
                    },
                    {
                        "authorId": "2125031571",
                        "name": "Jiachen Li"
                    },
                    {
                        "authorId": "1680165",
                        "name": "M. Tomizuka"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "For example, some recent FSL works have introduced such \u201ctricks\u201d, such as knowledge distillation [30], [32], selfsupervision [58], [59] and Mixup [33], into the FSL problem."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c9943aec3dfb3c38b57aee430b9c000970327115",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-04898",
                    "ArXiv": "2109.04898",
                    "DOI": "10.1109/TPAMI.2023.3312125",
                    "CorpusId": 237485475,
                    "PubMed": "37669193"
                },
                "corpusId": 237485475,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c9943aec3dfb3c38b57aee430b9c000970327115",
                "title": "LibFewShot: A Comprehensive Library for Few-shot Learning",
                "abstract": "Few-shot learning, especially few-shot image classification, has received increasing attention and witnessed significant advances in recent years. Some recent studies implicitly show that many generic techniques or \"tricks\", such as data augmentation, pre-training, knowledge distillation, and self-supervision, may greatly boost the performance of a few-shot learning method. Moreover, different works may employ different software platforms, backbone architectures and input image sizes, making fair comparisons difficult and practitioners struggle with reproducibility. To address these situations, we propose a comprehensive library for few-shot learning (LibFewShot) by re-implementing eighteen state-of-the-art few-shot learning methods in a unified framework with the same single codebase in PyTorch. Furthermore, based on LibFewShot, we provide comprehensive evaluations on multiple benchmarks with various backbone architectures to evaluate common pitfalls and effects of different training tricks. In addition, with respect to the recent doubts on the necessity of meta- or episodic-training mechanism, our evaluation results confirm that such a mechanism is still necessary especially when combined with pre-training. We hope our work can not only lower the barriers for beginners to enter the area of few-shot learning but also elucidate the effects of nontrivial tricks to facilitate intrinsic research on few-shot learning. The source code is available from https://github.com/RL-VIG/LibFewShot.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108691450",
                        "name": "Wenbin Li"
                    },
                    {
                        "authorId": "28113294",
                        "name": "C. Dong"
                    },
                    {
                        "authorId": "51305326",
                        "name": "Pinzhuo Tian"
                    },
                    {
                        "authorId": "1379496298",
                        "name": "Tiexin Qin"
                    },
                    {
                        "authorId": "2145180721",
                        "name": "Xuesong Yang"
                    },
                    {
                        "authorId": "47196654",
                        "name": "Ziyi Wang"
                    },
                    {
                        "authorId": "2055851838",
                        "name": "Jing Huo"
                    },
                    {
                        "authorId": "2475959",
                        "name": "Yinghuan Shi"
                    },
                    {
                        "authorId": null,
                        "name": "Lei Wang"
                    },
                    {
                        "authorId": "145644819",
                        "name": "Yang Gao"
                    },
                    {
                        "authorId": "2116782926",
                        "name": "Jiebo Luo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "631c705fd545b83ba7d707a30e535e9837669bf3",
                "externalIds": {
                    "ArXiv": "2108.11072",
                    "DBLP": "journals/corr/abs-2108-11072",
                    "CorpusId": 237291524
                },
                "corpusId": 237291524,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/631c705fd545b83ba7d707a30e535e9837669bf3",
                "title": "Learning Class-level Prototypes for Few-shot Learning",
                "abstract": "Few-shot learning aims to recognize new categories using very few labeled samples. Although few-shot learning has witnessed promising development in recent years, most existing methods adopt an average operation to calculate prototypes, thus limited by the outlier samples. In this work, we propose a simple yet effective framework for few-shot classification, which can learn to generate preferable prototypes from few support data, with the help of an episodic prototype generator module. The generated prototype is meant to be close to a certain \\textit{\\targetproto{}} and is less influenced by outlier samples. Extensive experiments demonstrate the effectiveness of this module, and our approach gets a significant raise over baseline models, and get a competitive result compared to previous methods on \\textit{mini}ImageNet, \\textit{tiered}ImageNet, and cross-domain (\\textit{mini}ImageNet $\\rightarrow$ CUB-200-2011) datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1382297688",
                        "name": "Minglei Yuan"
                    },
                    {
                        "authorId": "71074736",
                        "name": "Wenhai Wang"
                    },
                    {
                        "authorId": "41154933",
                        "name": "Tao Wang"
                    },
                    {
                        "authorId": "2089156465",
                        "name": "Chunhao Cai"
                    },
                    {
                        "authorId": "2149106499",
                        "name": "Qian Xu"
                    },
                    {
                        "authorId": "2087069029",
                        "name": "Tong Lu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The majority of existing FSL works adopt the meta-learning paradigm [24] and are mostly focused on image classification [33, 11, 27, 2, 29, 12, 19, 28, 13]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2aed105fc8fdce011a7eb320cbc5ef2557c5b0a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-03032",
                    "ArXiv": "2108.03032",
                    "DOI": "10.1109/ICCV48922.2021.00862",
                    "CorpusId": 236950782
                },
                "corpusId": 236950782,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/2aed105fc8fdce011a7eb320cbc5ef2557c5b0a3",
                "title": "Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer",
                "abstract": "A few-shot semantic segmentation model is typically composed of a CNN encoder, a CNN decoder and a simple classifier (separating foreground and background pixels). Most existing methods meta-learn all three model components for fast adaptation to a new class. However, given that as few as a single support set image is available, effective model adaption of all three components to the new class is extremely challenging. In this work we propose to simplify the meta-learning task by focusing solely on the simplest component \u2013 the classifier, whilst leaving the en-coder and decoder to pre-training. We hypothesize that if we pretrain an off-the-shelf segmentation model over a set of diverse training classes with sufficient annotations, the encoder and decoder can capture rich discriminative features applicable for any unseen classes, rendering the sub-sequent meta-learning stage unnecessary. For the classifier meta-learning, we introduce a Classifier Weight Transformer (CWT) designed to dynamically adapt the support-set trained classifier\u2019s weights to each query image in an inductive way. Extensive experiments on two standard bench-marks show that despite its simplicity, our method outperforms the state-of-the-art alternatives, often by a large margin. Code is available on https://github.com/zhiheLu/CWT-for-FSS.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9702077",
                        "name": "Zhihe Lu"
                    },
                    {
                        "authorId": "47287647",
                        "name": "Sen He"
                    },
                    {
                        "authorId": "2116163653",
                        "name": "Xiatian Zhu"
                    },
                    {
                        "authorId": "2152827279",
                        "name": "Li Zhang"
                    },
                    {
                        "authorId": "2115712433",
                        "name": "Yi-Zhe Song"
                    },
                    {
                        "authorId": "145406421",
                        "name": "T. Xiang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4097946a128faafcec78d8084e6ab23607483fa7",
                "externalIds": {
                    "DBLP": "conf/ijcai/AnXZZ21",
                    "DOI": "10.24963/ijcai.2021/295",
                    "CorpusId": 237101033
                },
                "corpusId": 237101033,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4097946a128faafcec78d8084e6ab23607483fa7",
                "title": "Conditional Self-Supervised Learning for Few-Shot Classification",
                "abstract": "How to learn a transferable feature representation from limited examples is a key challenge for few-shot classification. Self-supervision as an auxiliary task to the main supervised few-shot task is considered to be a conceivable way to solve the problem since self-supervision can provide additional structural information easily ignored by the main task. However, learning a good representation by traditional self-supervised methods is usually dependent on large training samples. In few-shot scenarios, due to the lack of sufficient samples, these self-supervised methods might learn a biased representation, which more likely leads to the wrong guidance for the main tasks and finally causes the performance degradation. In this paper, we propose conditional self-supervised learning (CSS) to use auxiliary information to guide the representation learning of self-supervised tasks. Specifically, CSS leverages supervised information as prior knowledge to shape and improve the learning feature manifold of self-supervision without auxiliary unlabeled data, so as to reduce representation bias and mine more effective semantic information. Moreover, CSS exploits more meaningful information through supervised and the improved self-supervised learning respectively and integrates the information into a unified distribution, which can further enrich and broaden the original representation. Extensive experiments demonstrate that our proposed method without any fine-tuning can achieve a significant accuracy improvement on the few-shot classification scenarios compared to the state-of-the-art few-shot learning methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7686972",
                        "name": "Yuexuan An"
                    },
                    {
                        "authorId": "143962062",
                        "name": "H. Xue"
                    },
                    {
                        "authorId": "47039400",
                        "name": "Xingyu Zhao"
                    },
                    {
                        "authorId": "2156147512",
                        "name": "Lu Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2dfad8e67810bce70b8eaf0f8769c8fadcbb4b9c",
                "externalIds": {
                    "DBLP": "conf/sigir/Wang0GYL021",
                    "DOI": "10.1145/3404835.3462944",
                    "CorpusId": 235792498
                },
                "corpusId": 235792498,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/2dfad8e67810bce70b8eaf0f8769c8fadcbb4b9c",
                "title": "Decoupling Representation Learning and Classification for GNN-based Anomaly Detection",
                "abstract": "GNN-based anomaly detection has recently attracted considerable attention. Existing attempts have thus far focused on jointly learning the node representations and the classifier for detecting the anomalies. Inspired by the recent advances of self-supervised learning (SSL) on graphs, we explore another possibility of decoupling the node representation learning and the classification for anomaly detection. We conduct a preliminary study to show that decoupled training using existing graph SSL schemes to represent nodes can obtain performance gains over joint training, but it may deteriorate when the behavior patterns and the label semantics become highly inconsistent. To be less biased by the inconsistency, we propose a simple yet effective graph SSL scheme, called Deep Cluster Infomax (DCI) for node representation learning, which captures the intrinsic graph properties in more concentrated feature spaces by clustering the entire graph into multiple parts. We conduct extensive experiments on four real-world datasets for anomaly detection. The results demonstrate that decoupled training equipped with a proper SSL scheme can outperform joint training in AUC. Compared with existing graph SSL schemes, DCI can help decoupled training gain more improvements.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108975817",
                        "name": "Yanling Wang"
                    },
                    {
                        "authorId": "2155700347",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "2119113081",
                        "name": "Shasha Guo"
                    },
                    {
                        "authorId": "2416851",
                        "name": "Hongzhi Yin"
                    },
                    {
                        "authorId": "1625473962",
                        "name": "Cuiping Li"
                    },
                    {
                        "authorId": "92779309",
                        "name": "Hong Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Following Su et al. (2019), we use a ResNet-18 (He et al., 2016) backbone network to facilitate training with bigger batch sizes as it was reported to improve performance (Chen et al., 2020a,b)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eb7459f45e43386062ac8bafdcb1e74809820d99",
                "externalIds": {
                    "DBLP": "conf/preregister/DahiyaSH20",
                    "MAG": "3200787342",
                    "DOI": "10.3929/ETHZ-B-000484477",
                    "CorpusId": 236923129
                },
                "corpusId": 236923129,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/eb7459f45e43386062ac8bafdcb1e74809820d99",
                "title": "Exploring self-supervised learning techniques for hand pose estimation",
                "abstract": "3D hand pose estimation from monocular RGB is a challenging problem due to signi\ufb01cantly varying environmental conditions such as lighting or variation in subject appearances. One way to improve performance across-the-board is to introduce more data. However, acquiring 3D annotated data for hands is a laborious task, as it involves heavy multi-camera setups leading to lab-like training data which does not generalize well. Alternatively, one could make use of unsupervised pre-training in order to signi\ufb01cantly increase the training data size one can train on. More recently, contrastive learning has shown promising results on tasks such as image classi\ufb01cation. Yet, no study has been made on how it a\ufb00ects structured regression problems such as hand pose estimation. We hypothesize that the contrastive objective does not extend well to such downstream task due to its inherent invariance and instead propose a relation objective, promoting equivariance. Our goal is to perform extensive experiments to validate our hypothesis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110645914",
                        "name": "Aneesh Dahiya"
                    },
                    {
                        "authorId": "21195502",
                        "name": "Adrian Spurr"
                    },
                    {
                        "authorId": "1466533438",
                        "name": "Otmar Hilliges"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "However, with the continuous increase of K, the lack of supervision information is no longer its biggest limitation [41]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "46fc96f03630fce3da1d85aab722ed1712ac7faf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-15467",
                    "ArXiv": "2106.15467",
                    "CorpusId": 235670130
                },
                "corpusId": 235670130,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/46fc96f03630fce3da1d85aab722ed1712ac7faf",
                "title": "Few-Shot Electronic Health Record Coding through Graph Contrastive Learning",
                "abstract": "Electronic health record (EHR) coding is the task of assigning ICD codes to each EHR. Most previous studies either only focus on the frequent ICD codes or treat rare and frequent ICD codes in the same way. These methods perform well on frequent ICD codes but due to the extremely unbalanced distribution of ICD codes, the performance on rare ones is far from satisfactory. We seek to improve the performance for both frequent and rare ICD codes by using a contrastive graph-based EHR coding framework, CoGraph, which re-casts EHR coding as a few-shot learning task. First, we construct a heterogeneous EHR word-entity (HEWE) graph for each EHR, where the words and entities extracted from an EHR serve as nodes and the relations between them serve as edges. Then, CoGraph learns similarities and dissimilarities between HEWE graphs from different ICD codes so that information can be transferred among them. In a few-shot learning scenario, the model only has access to frequent ICD codes during training, which might force it to encode features that are useful for frequent ICD codes only. To mitigate this risk, CoGraph devises two graph contrastive learning schemes, GSCL and GECL, that exploit the HEWE graph structures so as to encode transferable features. GSCL utilizes the intra-correlation of different sub-graphs sampled from HEWE graphs while GECL exploits the inter-correlation among HEWE graphs at different clinical stages. Experiments on the MIMIC-III benchmark dataset show that CoGraph significantly outperforms state-of-the-art methods on EHR coding, not only on frequent ICD codes, but also on rare codes, in terms of several evaluation indicators. On frequent ICD codes, GSCL and GECL improve the classification accuracy and F1 by 1.31% and 0.61%, respectively, and on rare ICD codes CoGraph has more obvious improvements by 2.12% and 2.95%.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49184081",
                        "name": "Shanshan Wang"
                    },
                    {
                        "authorId": "1749477",
                        "name": "Pengjie Ren"
                    },
                    {
                        "authorId": "1721165",
                        "name": "Zhumin Chen"
                    },
                    {
                        "authorId": "2780667",
                        "name": "Z. Ren"
                    },
                    {
                        "authorId": "2108951824",
                        "name": "Huasheng Liang"
                    },
                    {
                        "authorId": "2072780012",
                        "name": "Qiang Yan"
                    },
                    {
                        "authorId": "1713134",
                        "name": "E. Kanoulas"
                    },
                    {
                        "authorId": "1696030",
                        "name": "M. de Rijke"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In fewshot classifcation, several works (Gidaris et al., 2019; Su et al., 2020) use additional self-supervision loss during the training of base datasets to learn more general features."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2cb4f724728a79d5b7778c29618c902c7516c0ed",
                "externalIds": {
                    "ArXiv": "2106.11486",
                    "DBLP": "conf/icml/LeeC21",
                    "CorpusId": 235592875
                },
                "corpusId": 235592875,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2cb4f724728a79d5b7778c29618c902c7516c0ed",
                "title": "Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction for Few-Shot Classification",
                "abstract": "We propose unsupervised embedding adaptation for the downstream few-shot classification task. Based on findings that deep neural networks learn to generalize before memorizing, we develop Early-Stage Feature Reconstruction (ESFR) -- a novel adaptation scheme with feature reconstruction and dimensionality-driven early stopping that finds generalizable features. Incorporating ESFR consistently improves the performance of baseline methods on all standard settings, including the recently proposed transductive method. ESFR used in conjunction with the transductive method further achieves state-of-the-art performance on mini-ImageNet, tiered-ImageNet, and CUB; especially with 1.2%~2.0% improvements in accuracy over the previous best performing method on 1-shot setting.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145100833",
                        "name": "Dong Lee"
                    },
                    {
                        "authorId": "2116020357",
                        "name": "Sae-Young Chung"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5f34f70a88af83726726a6f304650e950dfeb495",
                "externalIds": {
                    "DBLP": "journals/tnn/MaBLWYBLW22",
                    "DOI": "10.1109/TNNLS.2021.3082928",
                    "CorpusId": 235472592,
                    "PubMed": "34138714"
                },
                "corpusId": 235472592,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5f34f70a88af83726726a6f304650e950dfeb495",
                "title": "Transductive Relation-Propagation With Decoupling Training for Few-Shot Learning",
                "abstract": "Few-shot learning, aiming to learn novel concepts from one or a few labeled examples, is an interesting and very challenging problem with many practical advantages. Existing few-shot methods usually utilize data of the same classes to train the feature embedding module and in a row, which is unable to learn adapting to new tasks. Besides, traditional few-shot models fail to take advantage of the valuable relations of the support-query pairs, leading to performance degradation. In this article, we propose a transductive relation-propagation graph neural network (GNN) with a decoupling training strategy (TRPN-D) to explicitly model and propagate such relations across support-query pairs, and empower the few-shot module the ability of transferring past knowledge to new tasks via the decoupling training. Our few-shot module, namely TRPN, treats the relation of each support-query pair as a graph node, named relational node, and resorts to the known relations between support samples, including both intraclass commonality and interclass uniqueness. Through relation propagation, the model could generate the discriminative relation embeddings for support-query pairs. To the best of our knowledge, this is the first work that decouples the training of the embedding network and the few-shot graph module with different tasks, which might offer a new way to solve the few-shot learning problem. Extensive experiments conducted on several benchmark datasets demonstrate that our method can significantly outperform a variety of state-of-the-art few-shot learning methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47009264",
                        "name": "Yuqing Ma"
                    },
                    {
                        "authorId": "151475528",
                        "name": "Shihao Bai"
                    },
                    {
                        "authorId": "1654091065",
                        "name": "Wei Liu"
                    },
                    {
                        "authorId": "2146294850",
                        "name": "Shuo Wang"
                    },
                    {
                        "authorId": "2046912381",
                        "name": "Yue Yu"
                    },
                    {
                        "authorId": "145425648",
                        "name": "Xiao Bai"
                    },
                    {
                        "authorId": "6820648",
                        "name": "Xianglong Liu"
                    },
                    {
                        "authorId": "2146058472",
                        "name": "Meng Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "FSL approaches use different methods such as metalearners [9, 40, 42, 46], distance-based classifiers [45, 49], and embedding learning [2, 47]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2bd3395d16875fcf3b7e3bc22db4e9bf743171b0",
                "externalIds": {
                    "DBLP": "conf/cvpr/PrabhakarSABG21",
                    "DOI": "10.1109/CVPR46437.2021.00484",
                    "CorpusId": 235657200
                },
                "corpusId": 235657200,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2bd3395d16875fcf3b7e3bc22db4e9bf743171b0",
                "title": "Labeled from Unlabeled: Exploiting Unlabeled Data for Few-shot Deep HDR Deghosting",
                "abstract": "High Dynamic Range (HDR) deghosting is an indispensable tool in capturing wide dynamic range scenes without ghosting artifacts. Recently, convolutional neural networks (CNNs) have shown tremendous success in HDR deghosting. However, CNN-based HDR deghosting methods require collecting large datasets with ground truth, which is a tedious and time-consuming process. This paper proposes a pioneering work by introducing zero and few-shot learning strategies for data-efficient HDR deghosting. Our approach consists of two stages of training. In stage one, we train the model with few labeled (5 or less) dynamic samples and a pool of unlabeled samples with a self-supervised loss. We use the trained model to predict HDRs for the unlabeled samples. To derive data for the next stage of training, we propose a novel method for generating corresponding dynamic inputs from the predicted HDRs of unlabeled data. The generated artificial dynamic inputs and predicted HDRs are used as paired labeled data. In stage two, we finetune the model with the original few labeled data and artificially generated labeled data. Our few-shot approach outperforms many fully-supervised methods in two publicly available datasets, using as little as five labeled dynamic samples.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144051362",
                        "name": "K. Prabhakar"
                    },
                    {
                        "authorId": "2088813938",
                        "name": "G. Senthil"
                    },
                    {
                        "authorId": "2008802066",
                        "name": "Susmit Agrawal"
                    },
                    {
                        "authorId": "144682140",
                        "name": "R. Venkatesh Babu"
                    },
                    {
                        "authorId": "2115177973",
                        "name": "Rama Krishna"
                    },
                    {
                        "authorId": "3729107",
                        "name": "S. Gorthi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f23274d20128b7713473e391e801a8587cfdcbef",
                "externalIds": {
                    "DBLP": "conf/cvpr/PangHTWH21",
                    "DOI": "10.1109/CVPRW53098.2021.00298",
                    "CorpusId": 235702967
                },
                "corpusId": 235702967,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f23274d20128b7713473e391e801a8587cfdcbef",
                "title": "Unlocking the Full Potential of Small Data with Diverse Supervision",
                "abstract": "Virtually all of deep learning literature relies on the assumption of large amounts of available training data. Indeed, even the majority of few-shot learning methods rely on a large set of \"base classes\" for pre-training. This assumption, however, does not always hold. For some tasks, annotating a large number of classes can be infeasible, and even collecting the images themselves can be a challenge in some scenarios. In this paper, we study this problem and call it \"Small Data\" setting, in contrast to \"Big Data.\" To unlock the full potential of small data, we propose to augment the models with annotations for other related tasks, thus increasing their generalization abilities. In particular, we use the richly annotated scene parsing dataset ADE20K to construct our realistic Long-tail Recognition with Diverse Supervision (LRDS) benchmark, by splitting the object categories into head and tail based on their distribution. Following the standard few-shot learning protocol, we use the head classes for representation learning and the tail classes for evaluation. Moreover, we further subsample the head categories and images to generate two novel settings which we call \"Scarce-Class\" and \"Scarce-Image,\" respectively corresponding to the shortage of training classes and images. Finally, we analyze the effect of applying various additional supervision sources under the proposed settings. Our experiments demonstrate that densely labeling a small set of images can indeed largely remedy the small data constraints. Our code and benchmark are available at https://github.com/BinahHu/ADE-FewShot.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1438305846",
                        "name": "Ziqi Pang"
                    },
                    {
                        "authorId": null,
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "authorId": "2931554",
                        "name": "P. Tokmakov"
                    },
                    {
                        "authorId": "2302062",
                        "name": "Yu-Xiong Wang"
                    },
                    {
                        "authorId": "145670946",
                        "name": "M. Hebert"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To be in consistent with the previous works [3, 31], We sample 600 few-shot tasks from the set of novel classes.",
                "Following [31], we split the dataset into 51 for training, 26 for validation and 25 for test classes.",
                "To draw fair comparisons with the existing methods, we deploy the following CNN architectures as visual backbones: 4-layer convolutional architecture proposed in [30] for CUB, ResNet-12 for miniImageNet [33, 4, 18], and ResNet-18 [31] for VGG-Flowers.",
                "Following [31], we split the available classes in the dataset into 100 for training, 50 for validation and 50 for testing."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c09e7ac9824aa711daa107bc13142bab74bfe925",
                "externalIds": {
                    "DBLP": "conf/bmvc/Afham0KNK21",
                    "ArXiv": "2104.12709",
                    "CorpusId": 233393856
                },
                "corpusId": 233393856,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/c09e7ac9824aa711daa107bc13142bab74bfe925",
                "title": "Rich Semantics Improve Few-shot Learning",
                "abstract": "Human learning benefits from multi-modal inputs that often appear as rich semantics (e.g., description of an object's attributes while learning about it). This enables us to learn generalizable concepts from very limited visual examples. However, current few-shot learning (FSL) methods use numerical class labels to denote object classes which do not provide rich semantic meanings about the learned concepts. In this work, we show that by using 'class-level' language descriptions, that can be acquired with minimal annotation cost, we can improve the FSL performance. Given a support set and queries, our main idea is to create a bottleneck visual feature (hybrid prototype) which is then used to generate language descriptions of the classes as an auxiliary task during training. We develop a Transformer based forward and backward encoding mechanism to relate visual and semantic tokens that can encode intricate relationships between the two modalities. Forcing the prototypes to retain semantic information about class description acts as a regularizer on the visual features, improving their generalization to novel classes at inference. Furthermore, this strategy imposes a human prior on the learned representations, ensuring that the model is faithfully relating visual and semantic concepts, thereby improving model interpretability. Our experiments on four datasets and ablation studies show the benefit of effectively modeling rich semantics for FSL.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2084548724",
                        "name": "Mohamed Afham"
                    },
                    {
                        "authorId": "152973423",
                        "name": "Salman Hameed Khan"
                    },
                    {
                        "authorId": "2115774820",
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "authorId": "40894826",
                        "name": "Muzammal Naseer"
                    },
                    {
                        "authorId": "2358803",
                        "name": "F. Khan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "This is how this task is utilized for the classification problem [49, 28, 52, 48]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "773143612b39acfc692b3f73171d276becd2bf4d",
                "externalIds": {
                    "ArXiv": "2104.08689",
                    "DBLP": "journals/corr/abs-2104-08689",
                    "CorpusId": 233296114
                },
                "corpusId": 233296114,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/773143612b39acfc692b3f73171d276becd2bf4d",
                "title": "RPCL: A Framework for Improving Cross-Domain Detection with Auxiliary Tasks",
                "abstract": "Cross-Domain Detection (XDD) aims to train an object detector using labeled image from a source domain but have good performance in the target domain with only unlabeled images. Existing approaches achieve this either by aligning the feature maps or the region proposals from the two domains, or by transferring the style of source images to that of target image. Contrasted with prior work, this paper provides a complementary solution to align domains by learning the same auxiliary tasks in both domains simultaneously. These auxiliary tasks push image from both domains towards shared spaces, which bridges the domain gap. Specifically, this paper proposes Rotation Prediction and Consistency Learning (PRCL), a framework complementing existing XDD methods for domain alignment by leveraging the two auxiliary tasks. The first one encourages the model to extract region proposals from foreground regions by rotating an image and predicting the rotation angle from the extracted region proposals. The second task encourages the model to be robust to changes in the image space by optimizing the model to make consistent class predictions for region proposals regardless of image perturbations. Experiments show the detection performance can be consistently and significantly enhanced by applying the two proposed tasks to existing XDD methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2158257300",
                        "name": "Kai Li"
                    },
                    {
                        "authorId": "26360698",
                        "name": "Curtis Wigington"
                    },
                    {
                        "authorId": "67319819",
                        "name": "Chris Tensmeyer"
                    },
                    {
                        "authorId": "2852035",
                        "name": "Vlad I. Morariu"
                    },
                    {
                        "authorId": "7574699",
                        "name": "Handong Zhao"
                    },
                    {
                        "authorId": "1977256",
                        "name": "Varun Manjunatha"
                    },
                    {
                        "authorId": "1598478975",
                        "name": "Nikolaos Barmpalios"
                    },
                    {
                        "authorId": "46956675",
                        "name": "Y. Fu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In previous works [16, 39], these tradeoffs are usually set by experience in practical situations.",
                "Showing that the auxiliary loss without labels can extract discriminative features for few-shot learning, [16] considers rotation prediction and relative patch location as self-supervised tasks, and [39] uses image jigsaw puzzle.",
                "43 + jig + rot, (SSFSL [39]) ResNet-18 58.",
                "Moreover, these works [16, 39] attempt to find one single solution for all objectives, which is likely to sacrifice the performance of the main task and be inconsistent with the goal of few-shot auxiliary learning.",
                "As self-supervised learning can improve the generalization of the network under the limitation of labeled data, some recent few-shot auxiliary learning (FSAL) works [16, 39] take few-shot learning as learning main task with self-supervised auxiliary tasks."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "01263f15e0724ed145f7ebd421702c883b0949b9",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChenGZHW21",
                    "ArXiv": "2104.07841",
                    "DOI": "10.1109/CVPR46437.2021.01345",
                    "CorpusId": 233289906
                },
                "corpusId": 233289906,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/01263f15e0724ed145f7ebd421702c883b0949b9",
                "title": "Pareto Self-Supervised Training for Few-Shot Learning",
                "abstract": "While few-shot learning (FSL) aims for rapid generalization to new concepts with little supervision, self-supervised learning (SSL) constructs supervisory signals directly computed from unlabeled data. Exploiting the complementarity of these two manners, few-shot auxiliary learning has recently drawn much attention to deal with few labeled data. Previous works benefit from sharing inductive bias between the main task (FSL) and auxiliary tasks (SSL), where the shared parameters of tasks are optimized by minimizing a linear combination of task losses. However, it is challenging to select a proper weight to balance tasks and reduce task conflict. To handle the problem as a whole, we propose a novel approach named as Pareto self-supervised training (PSST) for FSL. PSST explicitly decomposes the few-shot auxiliary problem into multiple constrained multi-objective subproblems with different trade-off preferences, and here a preference region in which the main task achieves the best performance is identified. Then, an effective preferred Pareto exploration is proposed to find a set of optimal solutions in such a preference region. Extensive experiments on several public benchmark datasets validate the effectiveness of our approach by achieving state-of-the-art performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2117202703",
                        "name": "Zhengyu Chen"
                    },
                    {
                        "authorId": "2077594077",
                        "name": "Jixie Ge"
                    },
                    {
                        "authorId": "2077593788",
                        "name": "Heshen Zhan"
                    },
                    {
                        "authorId": "122132048",
                        "name": "Siteng Huang"
                    },
                    {
                        "authorId": "2111224425",
                        "name": "Donglin Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, contrastive learning methods have become popular self-supervised representation learning tools and gained big progress in few-shot learning due to its better discriminative ability [3, 4]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "77583edbeac7fb4f5056243a9fd978b08ccb87ab",
                "externalIds": {
                    "DBLP": "conf/emnlp/Du0WX0LJ21",
                    "ArXiv": "2104.05094",
                    "DOI": "10.18653/v1/2021.findings-emnlp.118",
                    "CorpusId": 233209960
                },
                "corpusId": 233209960,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/77583edbeac7fb4f5056243a9fd978b08ccb87ab",
                "title": "Constructing Contrastive samples via Summarization for Text Classification with limited annotations",
                "abstract": "Contrastive Learning has emerged as a powerful representation learning method and facilitates various downstream tasks especially when supervised data is limited. How to construct efficient contrastive samples through data augmentation is key to its success. Unlike vision tasks, the data augmentation method for contrastive learning has not been investigated sufficiently in language tasks. In this paper, we propose a novel approach to construct contrastive samples for language tasks using text summarization. We use these samples for supervised contrastive learning to gain better text representations which greatly benefit text classification tasks with limited annotations. To further improve the method, we mix up samples from different classes and add an extra regularization, named Mixsum, in addition to the cross-entropy-loss. Experiments on real-world text classification datasets (Amazon-5, Yelp-5, AG News, and IMDb) demonstrate the effectiveness of the proposed contrastive learning framework with summarization-based data augmentation and Mixsum regularization.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111936104",
                        "name": "Yangkai Du"
                    },
                    {
                        "authorId": "40411766",
                        "name": "Tengfei Ma"
                    },
                    {
                        "authorId": "3008832",
                        "name": "Lingfei Wu"
                    },
                    {
                        "authorId": "2392383",
                        "name": "Fangli Xu"
                    },
                    {
                        "authorId": "49469875",
                        "name": "Xuhong Zhang"
                    },
                    {
                        "authorId": "2081160",
                        "name": "S. Ji"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "99b7a308e82165890e0a14e577715f81e11e8f89",
                "externalIds": {
                    "DBLP": "conf/cvpr/HongFL0SHP21",
                    "ArXiv": "2104.04192",
                    "DOI": "10.1109/CVPR46437.2021.00097",
                    "CorpusId": 233204529
                },
                "corpusId": 233204529,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/99b7a308e82165890e0a14e577715f81e11e8f89",
                "title": "Reinforced Attention for Few-Shot Learning and Beyond",
                "abstract": "Few-shot learning aims to correctly recognize query samples from unseen classes given a limited number of support samples, often by relying on global embeddings of images. In this paper, we propose to equip the backbone network with an attention agent, which is trained by reinforcement learning. The policy gradient algorithm is employed to train the agent towards adaptively localizing the representative regions on feature maps over time. We further design a reward function based on the prediction of the held-out data, thus helping the attention mechanism to generalize better across the unseen classes. The extensive experiments show, with the help of the reinforced attention, that our embedding network has the capability to progressively generate a more discriminative representation in few-shot learning. Moreover, experiments on the task of image classification also show the effectiveness of the proposed design.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152093629",
                        "name": "Jie Hong"
                    },
                    {
                        "authorId": "49208246",
                        "name": "Pengfei Fang"
                    },
                    {
                        "authorId": "1944575455",
                        "name": "Weihao Li"
                    },
                    {
                        "authorId": "50728655",
                        "name": "Tong Zhang"
                    },
                    {
                        "authorId": "144616396",
                        "name": "Christian Simon"
                    },
                    {
                        "authorId": "23911916",
                        "name": "Mehrtash Harandi"
                    },
                    {
                        "authorId": "47773335",
                        "name": "L. Petersson"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "result"
            ],
            "contexts": [
                "These results are in a similar vein to prior work on the evaluation of SSL approaches that have analyzed the robustness of SSL techniques to the choice of hyper-parameters [30], network architectures [9, 51], and domain shifts [30,42,49], etc.",
                "These include incorporating pre-text tasks such as predicting image rotations [14], the order of patches (jigsaw puzzle task) [29] during semi-supervised learning [35, 42, 54]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7ce8f0dda13a434314562f92d56147c7970f1c62",
                "externalIds": {
                    "ArXiv": "2104.00679",
                    "DBLP": "conf/cvpr/SuCM21",
                    "DOI": "10.1109/CVPR46437.2021.01277",
                    "CorpusId": 232478718
                },
                "corpusId": 232478718,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7ce8f0dda13a434314562f92d56147c7970f1c62",
                "title": "A Realistic Evaluation of Semi-Supervised Learning for Fine-Grained Classification",
                "abstract": "We evaluate the effectiveness of semi-supervised learning (SSL) on a realistic benchmark where data exhibits considerable class imbalance and contains images from novel classes. Our benchmark consists of two fine-grained classification datasets obtained by sampling classes from the Aves and Fungi taxonomy. We find that recently proposed SSL methods provide significant benefits, and can effectively use out-of-class data to improve performance when deep networks are trained from scratch. Yet their performance pales in comparison to a transfer learning baseline, an alternative approach for learning from a few examples. Furthermore, in the transfer setting, while existing SSL methods provide improvements, the presence of out-of-class is often detrimental. In this setting, standard fine-tuning followed by distillation-based self-training is the most robust. Our work suggests that semi-supervised learning with experts on realistic datasets may require different strategies than those currently prevalent in the literature.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148766061",
                        "name": "Jong-Chyi Su"
                    },
                    {
                        "authorId": "3365553",
                        "name": "Zezhou Cheng"
                    },
                    {
                        "authorId": "35208858",
                        "name": "Subhransu Maji"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", 2020) or minimizing the sum of loss functions by meta-learning (Su et al., 2020), we propose to train the model such that it directly learns to adapt at test-time without supervision."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "51cb7ff41dc6e97a83151e2b7daff87a77d14490",
                "externalIds": {
                    "DBLP": "conf/aistats/BartlerBWD022",
                    "ArXiv": "2103.16201",
                    "CorpusId": 232417890
                },
                "corpusId": 232417890,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/51cb7ff41dc6e97a83151e2b7daff87a77d14490",
                "title": "MT3: Meta Test-Time Training for Self-Supervised Test-Time Adaption",
                "abstract": "An unresolved problem in Deep Learning is the ability of neural networks to cope with domain shifts during test-time, imposed by commonly fixing network parameters after training. Our proposed method Meta Test-Time Training (MT3), however, breaks this paradigm and enables adaption at test-time. We combine meta-learning, self-supervision and test-time training to learn to adapt to unseen test distributions. By minimizing the self-supervised loss, we learn task-specific model parameters for different tasks. A meta-model is optimized such that its adaption to the different task-specific models leads to higher performance on those tasks. During test-time a single unlabeled image is sufficient to adapt the meta-model parameters. This is achieved by minimizing only the self-supervised loss component resulting in a better prediction for that image. Our approach significantly improves the state-of-the-art results on the CIFAR-10-Corrupted image classification benchmark. Our implementation is available on GitHub.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "52197778",
                        "name": "Alexander Bartler"
                    },
                    {
                        "authorId": "2064721089",
                        "name": "Andreas B\u00fchler"
                    },
                    {
                        "authorId": "90100607",
                        "name": "Felix Wiewel"
                    },
                    {
                        "authorId": "2067355774",
                        "name": "Mario D\u00f6bler"
                    },
                    {
                        "authorId": "49188662",
                        "name": "Binh Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[56] studied the effectiveness of utilizing self-supervised learning (SSL) techniques in few-shot setting."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2f8324c7aa25787a51aa5a1b373670a4ce04f500",
                "externalIds": {
                    "DBLP": "journals/tomccap/HeHLXS22",
                    "ArXiv": "2103.16009",
                    "DOI": "10.1145/3511917",
                    "CorpusId": 246945679
                },
                "corpusId": 246945679,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2f8324c7aa25787a51aa5a1b373670a4ce04f500",
                "title": "Revisiting Local Descriptor for Improved Few-Shot Classification",
                "abstract": "Few-shot classification studies the problem of quickly adapting a deep learner to understanding novel classes based on few support images. In this context, recent research efforts have been aimed at designing more and more complex classifiers that measure similarities between query and support images but left the importance of feature embeddings seldom explored. We show that the reliance on sophisticated classifiers is not necessary, and a simple classifier applied directly to improved feature embeddings can instead outperform most of the leading methods in the literature. To this end, we present a new method, named DCAP, for few-shot classification, in which we investigate how one can improve the quality of embeddings by leveraging Dense Classification and Attentive Pooling (DCAP). Specifically, we propose to train a learner on base classes with abundant samples to solve dense classification problem first and then meta-train the learner on plenty of randomly sampled few-shot tasks to adapt it to few-shot scenario or the test time scenario. During meta-training, we suggest to pool feature maps by applying attentive pooling instead of the widely used global average pooling to prepare embeddings for few-shot classification. Attentive pooling learns to reweight local descriptors, explaining what the learner is looking for as evidence for decision making. Experiments on two benchmark datasets show the proposed method to be superior in multiple few-shot settings while being simpler and more explainable. Code is publicly available at https://github.com/Ukeyboard/dcap/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1434623735",
                        "name": "J. He"
                    },
                    {
                        "authorId": "48043335",
                        "name": "Richang Hong"
                    },
                    {
                        "authorId": "3076466",
                        "name": "Xueliang Liu"
                    },
                    {
                        "authorId": "2285442",
                        "name": "Mingliang Xu"
                    },
                    {
                        "authorId": "2138109020",
                        "name": "Qianru Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[53] combined self-supervised and meta learning and showed improved few-shot classification accuracy for finegrained categories."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "800928c91d451307befda8aec5a87d60b62e4ec6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-16483",
                    "ArXiv": "2103.16483",
                    "DOI": "10.1109/CVPR46437.2021.01269",
                    "CorpusId": 232417868
                },
                "corpusId": 232417868,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/800928c91d451307befda8aec5a87d60b62e4ec6",
                "title": "Benchmarking Representation Learning for Natural World Image Collections",
                "abstract": "Recent progress in self-supervised learning has resulted in models that are capable of extracting rich representations from image collections without requiring any explicit label supervision. However, to date the vast majority of these approaches have restricted themselves to training on standard benchmark datasets such as ImageNet. We argue that fine-grained visual categorization problems, such as plant and animal species classification, provide an informative testbed for self-supervised learning. In order to facilitate progress in this area we present two new natural world visual classification datasets, iNat2021 and NeWT. The former consists of 2.7M images from 10k different species up-loaded by users of the citizen science application iNaturalist. We designed the latter, NeWT, in collaboration with domain experts with the aim of benchmarking the performance of representation learning algorithms on a suite of challenging natural world binary classification tasks that go beyond standard species classification. These two new datasets allow us to explore questions related to large-scale representation and transfer learning in the context of fine-grained categories. We provide a comprehensive analysis of feature extractors trained with and without supervision on ImageNet and iNat2021, shedding light on the strengths and weaknesses of different learned features across a diverse set of tasks. We find that features produced by standard supervised methods still outperform those produced by self-supervised approaches such as SimCLR. However, improved self-supervised learning methods are constantly being released and the iNat2021 and NeWT datasets are a valuable resource for tracking their progress.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2996914",
                        "name": "Grant Van Horn"
                    },
                    {
                        "authorId": "1630502974",
                        "name": "Elijah Cole"
                    },
                    {
                        "authorId": "31937047",
                        "name": "Sara Beery"
                    },
                    {
                        "authorId": "2047941901",
                        "name": "Kimberly Wilber"
                    },
                    {
                        "authorId": "50172592",
                        "name": "Serge J. Belongie"
                    },
                    {
                        "authorId": "2918822",
                        "name": "Oisin Mac Aodha"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f6be8e34d507b3b9cf801855c0e01304f8376b95",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-13318",
                    "ArXiv": "2103.13318",
                    "DOI": "10.1109/TPAMI.2021.3129870",
                    "CorpusId": 232335329,
                    "PubMed": "34813469"
                },
                "corpusId": 232335329,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f6be8e34d507b3b9cf801855c0e01304f8376b95",
                "title": "Factors of Influence for Transfer Learning Across Diverse Appearance Domains and Task Types",
                "abstract": "Transfer learning enables to re-use knowledge learned on a source task to help learning a target task. A simple form of transfer learning is common in current state-of-the-art computer vision models, i.e., pre-training a model for image classification on the ILSVRC dataset, and then fine-tune on any target task. However, previous systematic studies of transfer learning have been limited and the circumstances in which it is expected to work are not fully understood. In this paper we carry out an extensive experimental exploration of transfer learning across vastly different image domains (consumer photos, autonomous driving, aerial imagery, underwater, indoor scenes, synthetic, close-ups) and task types (semantic segmentation, object detection, depth estimation, keypoint detection). Importantly, these are all complex, structured output tasks types relevant to modern computer vision applications. In total we carry out over 2000 transfer learning experiments, including many where the source and target come from different image domains, task types, or both. We systematically analyze these experiments to understand the impact of image domain, task type, and dataset size on transfer learning performance. Our study leads to several insights and concrete recommendations: (1) for most tasks there exists a source which significantly outperforms ILSVRC\u201912 pre-training; (2) the image domain is the most important factor for achieving positive transfer; (3) the source dataset should include the image domain of the target dataset to achieve best results; (4) at the same time, we observe only small negative effects when the image domain of the source task is much broader than that of the target; (5) transfer across task types can be beneficial, but its success is heavily dependent on both the source and target task types.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1722052",
                        "name": "Thomas Mensink"
                    },
                    {
                        "authorId": "1823362",
                        "name": "J. Uijlings"
                    },
                    {
                        "authorId": "33746152",
                        "name": "Alina Kuznetsova"
                    },
                    {
                        "authorId": "3037160",
                        "name": "Michael Gygli"
                    },
                    {
                        "authorId": "143865718",
                        "name": "V. Ferrari"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Inspired by the similarity of few-shot and self-supervised learning, some works [6, 7] have weaved self-supervision into the training process of few-shot learning."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1638bf5ae1de6caac51b064f5515e75eea10c6d9",
                "externalIds": {
                    "ArXiv": "2103.05985",
                    "DBLP": "journals/corr/abs-2103-05985",
                    "MAG": "3172122903",
                    "DOI": "10.1109/ICME51207.2021.9428447",
                    "CorpusId": 232170260
                },
                "corpusId": 232170260,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1638bf5ae1de6caac51b064f5515e75eea10c6d9",
                "title": "Multi-Pretext Attention Network For Few-Shot Learning With Self-Supervision",
                "abstract": "Few-shot learning is an interesting and challenging study, which enables machines to learn from few samples like humans. Existing studies rarely exploit auxiliary information from large amount of unlabeled data. Self-supervised learning is emerged as an efficient method to utilize unlabeled data. Existing self-supervised learning methods always rely on the combination of geometric transformations for the single sample by augmentation, while seriously neglect the endogenous correlation information among different samples that is the same important for the task. In this work, we propose a Graph-driven Clustering (GC), a novel augmentation-free method for self-supervised learning, which does not rely on any auxiliary sample and utilizes the endogenous correlation information among input samples. Besides, we propose Multi-pretext Attention Network (MAN), which exploits a specific attention mechanism to combine the traditional augmentation-relied methods and our GC, adaptively learning their optimized weights to improve the performance and enabling the feature extractor to obtain more universal representations. We evaluate our MAN extensively on miniImageNet and tieredImageNet datasets and the results demonstrate that the proposed method outperforms the state-of-the-art (SOTA) relevant methods. 1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109024995",
                        "name": "Hainan Li"
                    },
                    {
                        "authorId": "1380391852",
                        "name": "Renshuai Tao"
                    },
                    {
                        "authorId": "2152753762",
                        "name": "Jun Li"
                    },
                    {
                        "authorId": "1381853008",
                        "name": "Haotong Qin"
                    },
                    {
                        "authorId": "71056959",
                        "name": "Yifu Ding"
                    },
                    {
                        "authorId": "2146294850",
                        "name": "Shuo Wang"
                    },
                    {
                        "authorId": "6820648",
                        "name": "Xianglong Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[67] also used rotation and permutation of patches as auxiliary tasks and concluded that SSL is more effective in low-shot regimes and under significant domain shifts.",
                "Recently, the potential of SSL for FSL was explored in [23, 67]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "04733e633493d5adc31f5f507ebf54a5e509fae4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-01315",
                    "ArXiv": "2103.01315",
                    "DOI": "10.1109/CVPR46437.2021.01069",
                    "CorpusId": 232092147
                },
                "corpusId": 232092147,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/04733e633493d5adc31f5f507ebf54a5e509fae4",
                "title": "Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning",
                "abstract": "In many real-world problems, collecting a large number of labeled samples is infeasible. Few-shot learning (FSL) is the dominant approach to address this issue, where the objective is to quickly adapt to novel categories in presence of a limited number of samples. FSL tasks have been predominantly solved by leveraging the ideas from gradient-based meta-learning and metric learning approaches. However, recent works have demonstrated the significance of powerful feature representations with a simple embedding network that can outperform existing sophisticated FSL algorithms. In this work, we build on this insight and propose a novel training mechanism that simultaneously enforces equivariance and invariance to a general set of geometric transformations. Equivariance or invariance has been employed standalone in the previous works; however, to the best of our knowledge, they have not been used jointly. Simultaneous optimization for both of these contrasting objectives allows the model to jointly learn features that are not only independent of the input transformation but also the features that encode the structure of geometric transformations. These complementary sets of features help generalize well to novel classes with only a few data samples. We achieve additional improvements by incorporating a novel self-supervised distillation objective. Our extensive experimentation shows that even without knowledge distillation our proposed method can outperform current state-of-the-art FSL methods on five popular benchmark datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9247631",
                        "name": "Mamshad Nayeem Rizve"
                    },
                    {
                        "authorId": "152973423",
                        "name": "Salman Hameed Khan"
                    },
                    {
                        "authorId": "2358803",
                        "name": "F. Khan"
                    },
                    {
                        "authorId": "145103012",
                        "name": "M. Shah"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Self-Supervised Network Most of the prior works [61,62] in computer vision weave self-supervision into fewshot learning by adding pretext tasks loss."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dfafbf1ed78638c8c7e19286c39036c22e52d583",
                "externalIds": {
                    "DBLP": "journals/sensors/ZhangLWW21",
                    "PubMedCentral": "7956409",
                    "DOI": "10.3390/s21051566",
                    "CorpusId": 232129644,
                    "PubMed": "33668138"
                },
                "corpusId": 232129644,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dfafbf1ed78638c8c7e19286c39036c22e52d583",
                "title": "RS-SSKD: Self-Supervision Equipped with Knowledge Distillation for Few-Shot Remote Sensing Scene Classification",
                "abstract": "While growing instruments generate more and more airborne or satellite images, the bottleneck in remote sensing (RS) scene classification has shifted from data limits toward a lack of ground truth samples. There are still many challenges when we are facing unknown environments, especially those with insufficient training data. Few-shot classification offers a different picture under the umbrella of meta-learning: digging rich knowledge from a few data are possible. In this work, we propose a method named RS-SSKD for few-shot RS scene classification from a perspective of generating powerful representation for the downstream meta-learner. Firstly, we propose a novel two-branch network that takes three pairs of original-transformed images as inputs and incorporates Class Activation Maps (CAMs) to drive the network mining, the most relevant category-specific region. This strategy ensures that the network generates discriminative embeddings. Secondly, we set a round of self-knowledge distillation to prevent overfitting and boost the performance. Our experiments show that the proposed method surpasses current state-of-the-art approaches on two challenging RS scene datasets: NWPU-RESISC45 and RSD46-WHU. Finally, we conduct various ablation experiments to investigate the effect of each component of the proposed method and analyze the training time of state-of-the-art methods and ours.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1805951175",
                        "name": "Pei Zhang"
                    },
                    {
                        "authorId": "2155504510",
                        "name": "Ying Li"
                    },
                    {
                        "authorId": "2152690638",
                        "name": "Dong Wang"
                    },
                    {
                        "authorId": "2051995270",
                        "name": "Jiyue Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "Another line of research relevant to ours is efficient MAML, e.g., (Raghu et al., 2019; Song et al., 2019; Su et al., 2019), where the goal is to improve the computation efficiency and/or the generalization of MAML.",
                "In (Su et al., 2019), a self-supervised representation learning task was augmented to the meta-updating objective and resulted in a meta-model with improved generalization.",
                ", (Raghu et al., 2019; Song et al., 2019; Su et al., 2019), where the goal is to improve the computation efficiency and/or the generalization of MAML."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "118a605ad954c8f8e1ad65941429d0fd2c14c918",
                "externalIds": {
                    "DBLP": "conf/iclr/0008X0CWGW21",
                    "ArXiv": "2102.10454",
                    "CorpusId": 231985673
                },
                "corpusId": 231985673,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/118a605ad954c8f8e1ad65941429d0fd2c14c918",
                "title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning",
                "abstract": "Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a meta-initialization} of model parameters (that we call meta-model) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how adversarial robustness can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study WHEN a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate HOW robust regularization can efficiently be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153537425",
                        "name": "Ren Wang"
                    },
                    {
                        "authorId": "46321210",
                        "name": "Kaidi Xu"
                    },
                    {
                        "authorId": "143743061",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "153191489",
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "authorId": "27836724",
                        "name": "Tsui-Wei Weng"
                    },
                    {
                        "authorId": "144158271",
                        "name": "Chuang Gan"
                    },
                    {
                        "authorId": "2146059787",
                        "name": "Meng Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b742fdd29d627620db13fac244b5267edbb8ea92",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-03752",
                    "ArXiv": "2102.03752",
                    "DOI": "10.1109/TASLP.2021.3105013",
                    "CorpusId": 231847192
                },
                "corpusId": 231847192,
                "publicationVenue": {
                    "id": "309e00f7-4bbd-461f-ab37-a90cd14ef21d",
                    "name": "IEEE/ACM Transactions on Audio Speech and Language Processing",
                    "alternate_names": [
                        "IEEE/ACM Trans Audio Speech Lang Process"
                    ],
                    "issn": "2329-9290",
                    "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=6570655",
                    "alternate_urls": [
                        "https://signalprocessingsociety.org/publications-resources/ieeeacm-transactions-audio-speech-and-language-processing/ieeeacm"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b742fdd29d627620db13fac244b5267edbb8ea92",
                "title": "CSS-LM: A Contrastive Framework for Semi-Supervised Fine-Tuning of Pre-Trained Language Models",
                "abstract": "Fine-tuning pre-trained language models (PLMs) has demonstrated its effectiveness on various downstream NLP tasks recently. However, in many scenarios with limited supervised data, the conventional fine-tuning strategies cannot sufficiently capture the important semantic features for downstream tasks. To address this issue, we introduce a novel framework (named \u201cCSS-LM\u201d) to improve the fine-tuning phase of PLMs via contrastive semi-supervised learning. Specifically, given a specific task, we retrieve positive and negative instances from large-scale unlabeled corpora according to their domain-level and class-level semantic relatedness to the task. We then perform contrastive semi-supervised learning on both the retrieved unlabeled instances and original labeled instances to help PLMs capture crucial task-related semantic features. The experimental results show that CSS-LM achieves better results than the conventional fine-tuning strategy on a series of downstream tasks with few-shot settings by up to 7.8%, and outperforms the latest supervised contrastive fine-tuning strategy by up to 7.1%. Our datasets and source code will be available to provide more details.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48576745",
                        "name": "Yusheng Su"
                    },
                    {
                        "authorId": "145760425",
                        "name": "Xu Han"
                    },
                    {
                        "authorId": "2427350",
                        "name": "Yankai Lin"
                    },
                    {
                        "authorId": "2621696",
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "authorId": "2141313179",
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "authorId": "2209965245",
                        "name": "Peng Li"
                    },
                    {
                        "authorId": "1753344",
                        "name": "Maosong Sun"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Some methods [15, 43] use self-supervised losses (e."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "25e571a8923107bcf764445ac42d3646194841fe",
                "externalIds": {
                    "ArXiv": "2101.11058",
                    "CorpusId": 235593418
                },
                "corpusId": 235593418,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/25e571a8923107bcf764445ac42d3646194841fe",
                "title": "Supervised Momentum Contrastive Learning for Few-Shot Classification",
                "abstract": "Few-shot learning aims to transfer information from one task to enable generalization on novel tasks given a few examples. This information is present both in the domain and the class labels. In this work we investigate the complementary roles of these two sources of information by combining instance-discriminative contrastive learning and supervised learning in a single framework called Supervised Momentum Contrastive learning (SUPMOCO). Our approach avoids a problem observed in supervised learning where information in images not relevant to the task is discarded, which hampers their generalization to novel tasks. We show that (self-supervised) contrastive learning and supervised learning are mutually beneficial, leading to a new state-of-the-art on the META-DATASET - a recently introduced benchmark for few-shot learning. Our method is based on a simple modification of MOCO and scales better than prior work on combining supervised and self-supervised learning. This allows us to easily combine data from multiple domains leading to further improvements.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "134549850",
                        "name": "Orchid Majumder"
                    },
                    {
                        "authorId": "2529423",
                        "name": "Avinash Ravichandran"
                    },
                    {
                        "authorId": "35208858",
                        "name": "Subhransu Maji"
                    },
                    {
                        "authorId": "16163297",
                        "name": "A. Achille"
                    },
                    {
                        "authorId": "32235780",
                        "name": "M. Polito"
                    },
                    {
                        "authorId": "1715959",
                        "name": "Stefano Soatto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Several recent works [14, 15, 3, 37, 8, 49, 28] have utilized data augmentation for metalearning based FSL.",
                "It has also been considered for FSL [8, 49, 28].",
                "For supervised FSL, [8, 49, 28] take a multi-task learning framework where augmented data are used for auxiliary self-supervised pretext tasks (e.",
                "Our CPLAE is also a supervised FSL model, but the way data augmentation is used is very different from that in [8, 49, 28]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4b3709a718bf6d92e57d93723aff40c0266074c5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-09499",
                    "ArXiv": "2101.09499",
                    "CorpusId": 231699122
                },
                "corpusId": 231699122,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4b3709a718bf6d92e57d93723aff40c0266074c5",
                "title": "Contrastive Prototype Learning with Augmented Embeddings for Few-Shot Learning",
                "abstract": "Most recent few-shot learning (FSL) methods are based on meta-learning with episodic training. In each meta-training episode, a discriminative feature embedding and/or classifier are first constructed from a support set in an inner loop, and then evaluated in an outer loop using a query set for model updating. This query set sample centered learning objective is however intrinsically limited in addressing the lack of training data problem in the support set. In this paper, a novel contrastive prototype learning with augmented embeddings (CPLAE) model is proposed to overcome this limitation. First, data augmentations are introduced to both the support and query sets with each sample now being represented as an augmented embedding (AE) composed of concatenated embeddings of both the original and augmented versions. Second, a novel support set class prototype centered contrastive loss is proposed for contrastive prototype learning (CPL). With a class prototype as an anchor, CPL aims to pull the query samples of the same class closer and those of different classes further away. This support set sample centered loss is highly complementary to the existing query centered loss, fully exploiting the limited training data in each episode. Extensive experiments on several benchmarks demonstrate that our proposed CPLAE achieves new state-of-the-art.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1939358",
                        "name": "Yizhao Gao"
                    },
                    {
                        "authorId": "21313225",
                        "name": "Nanyi Fei"
                    },
                    {
                        "authorId": "151472765",
                        "name": "Guangzhen Liu"
                    },
                    {
                        "authorId": "1776220",
                        "name": "Zhiwu Lu"
                    },
                    {
                        "authorId": "145406421",
                        "name": "T. Xiang"
                    },
                    {
                        "authorId": "2410938",
                        "name": "Songfang Huang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Additionally, we compare the performances of our approach with other self-supervised auxiliary losses, i.e., rotation prediction (Gidaris et al., 2018) and jigsaw puzzle (Noroozi & Favaro, 2016), for which (Su et al., 2020) provided their integration into the ProtoNet framework.",
                "Such methods [11,38,6,9,23] integrates various types of self-supervised training objective into different few-shot learning frameworks in order to learn transferable features and improve the few-shot classification performance.",
                ", rotation prediction [12] and jigsaw puzzle [26], for which [38] provided their integration into the ProtoNet framework.",
                "Such methods (Gidaris et al., 2019; Medina et al., 2020; Su et al., 2020; Doersch et al., 2020; Gao et al., 2021) integrate various types of self-supervised training objectives into different few-shot learning frameworks in order to learn more transferable features and improve the few-shot\u2026"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1d4fe5b45a6450554fe1beea8e7daf5a6fc7ca05",
                "externalIds": {
                    "ArXiv": "2012.13831",
                    "DBLP": "journals/corr/abs-2012-13831",
                    "DOI": "10.1007/978-3-030-86486-6_41",
                    "CorpusId": 229677873
                },
                "corpusId": 229677873,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1d4fe5b45a6450554fe1beea8e7daf5a6fc7ca05",
                "title": "Spatial Contrastive Learning for Few-Shot Classification",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "67035902",
                        "name": "Yassine Ouali"
                    },
                    {
                        "authorId": "1931593",
                        "name": "C. Hudelot"
                    },
                    {
                        "authorId": "30784787",
                        "name": "Myriam Tami"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In [39, 29, 24, 11, 31] additional unlabeled data is used, [60, 47] leverage additional semantic information available for the classes, and [11, 21, 1, 50] examine the usage of unsupervised or self-supervised training in the context of a standard few-shot learning."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "370ff19f0d5983ec61d490a17da47095ee4a29cd",
                "externalIds": {
                    "ArXiv": "2012.03515",
                    "MAG": "3112156746",
                    "DBLP": "journals/corr/abs-2012-03515",
                    "DOI": "10.1109/CVPR46437.2021.00862",
                    "CorpusId": 227335406
                },
                "corpusId": 227335406,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/370ff19f0d5983ec61d490a17da47095ee4a29cd",
                "title": "Fine-grained Angular Contrastive Learning with Coarse Labels",
                "abstract": "Few-shot learning methods offer pre-training techniques optimized for easier later adaptation of the model to new classes (unseen during training) using one or a few examples. This adaptivity to unseen classes is especially important for many practical applications where the pre-trained label space cannot remain fixed for effective use and the model needs to be \"specialized\" to support new categories on the fly. One particularly interesting scenario, essentially overlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where the training classes (e.g. animals) are of much \u2018coarser granularity\u2019 than the target (test) classes (e.g. breeds). A very practical example of C2FS is when the target classes are sub-classes of the training classes. Intuitively, it is especially challenging as (both regular and few-shot) supervised pre-training tends to learn to ignore intra-class variability which is essential for separating sub-classes. In this paper, we introduce a novel \u2019Angular normalization\u2019 module that allows to effectively combine supervised and self-supervised contrastive pre-training to approach the proposed C2FS task, demonstrating significant gains in a broad study over multiple baselines and datasets. We hope that this work will help to pave the way for future research on this new, challenging, and very practical topic of C2FS classification.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2032221619",
                        "name": "Guy Bukchin"
                    },
                    {
                        "authorId": "1455047952",
                        "name": "Eli Schwartz"
                    },
                    {
                        "authorId": "2903226",
                        "name": "Kate Saenko"
                    },
                    {
                        "authorId": "38714260",
                        "name": "Ori Shahar"
                    },
                    {
                        "authorId": "1723233",
                        "name": "R. Feris"
                    },
                    {
                        "authorId": "2711839",
                        "name": "R. Giryes"
                    },
                    {
                        "authorId": "2428823",
                        "name": "Leonid Karlinsky"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "90cfc2e7cb55142e8e359e30543bc08056be33d8",
                "externalIds": {
                    "DBLP": "journals/pami/YeHZ23",
                    "ArXiv": "2011.14663",
                    "DOI": "10.1109/TPAMI.2022.3179368",
                    "CorpusId": 249277518,
                    "PubMed": "35648875"
                },
                "corpusId": 249277518,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/90cfc2e7cb55142e8e359e30543bc08056be33d8",
                "title": "Revisiting Unsupervised Meta-Learning via the Characteristics of Few-Shot Tasks",
                "abstract": "Meta-learning has become a practical approach towards few-shot image classification, where \u201ca strategy to learn a classifier\u201d is meta-learned on labeled base classes and can be applied to tasks with novel classes. We remove the requirement of base class labels and learn generalizable embeddings via Unsupervised Meta-Learning (UML). Specifically, episodes of tasks are constructed with data augmentations from unlabeled base classes during meta-training, and we apply embedding-based classifiers to novel tasks with labeled few-shot examples during meta-test. We observe two elements play important roles in UML, i.e., the way to sample tasks and measure similarities between instances. Thus we obtain a strong baseline with two simple modifications \u2014 a sufficient sampling strategy constructing multiple tasks per episode efficiently together with a semi-normalized similarity. We then take advantage of the characteristics of tasks from two directions to get further improvements. First, synthesized confusing instances are incorporated to help extract more discriminative embeddings. Second, we utilize an additional task-specific embedding transformation as an auxiliary component during meta-training to promote the generalization ability of the pre-adapted embeddings. Experiments on few-shot learning benchmarks verify that our approaches outperform previous UML methods and achieve comparable or even better performance than its supervised variants.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2151459740",
                        "name": "Han-Jia Ye"
                    },
                    {
                        "authorId": "2112708270",
                        "name": "Lu Han"
                    },
                    {
                        "authorId": "1721819",
                        "name": "De-chuan Zhan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Inspired by the similarity between the embedding-based meta-learning and the contrastive self-supervised learning methods [15, 4], several recent approaches apply self-supervised learning in both supervised [50] and unsupervised meta-learning [19, 33, 37]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2fa7a0aaa6de2c6297e1389b07d02a0c5fb08711",
                "externalIds": {
                    "MAG": "3107424713",
                    "DBLP": "journals/corr/abs-2011-14663",
                    "CorpusId": 227228090
                },
                "corpusId": 227228090,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2fa7a0aaa6de2c6297e1389b07d02a0c5fb08711",
                "title": "Revisiting Unsupervised Meta-Learning: Amplifying or Compensating for the Characteristics of Few-Shot Tasks",
                "abstract": "Meta-learning becomes a practical approach towards few-shot image classification, where a visual recognition system is constructed with limited annotated data. Inductive bias such as embedding is learned from a base class set with ample labeled examples and then generalizes to few-shot tasks with novel classes. Surprisingly, we find that the base class set labels are not necessary, and discriminative embeddings could be meta-learned in an unsupervised manner. Comprehensive analyses indicate two modifications -- the semi-normalized distance metric and the sufficient sampling -- improves unsupervised meta-learning (UML) significantly. Based on the modified baseline, we further amplify or compensate for the characteristic of tasks when training a UML model. First, mixed embeddings are incorporated to increase the difficulty of few-shot tasks. Next, we utilize a task-specific embedding transformation to deal with the specific properties among tasks, maintaining the generalization ability into the vanilla embeddings. Experiments on few-shot learning benchmarks verify that our approaches outperform previous UML methods by a 4-10% performance gap, and embeddings learned with our UML achieve comparable or even better performance than its supervised variants.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2151459740",
                        "name": "Han-Jia Ye"
                    },
                    {
                        "authorId": "2112708270",
                        "name": "Lu Han"
                    },
                    {
                        "authorId": "1721819",
                        "name": "De-chuan Zhan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Meta learning In meta learning [12, 18, 55, 59, 63, 64, 65, 72, 79, 83], approaches imitate the few-shot scenario by repeatedly sampling similar scenarios (episodes) from the base classes during the pre-training phase."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c38af920ef584336a9b3b454165f6b4116459fbf",
                "externalIds": {
                    "DBLP": "conf/iccv/AfrasiyabiLG21",
                    "ArXiv": "2011.11872",
                    "DOI": "10.1109/ICCV48922.2021.00891",
                    "CorpusId": 237154257
                },
                "corpusId": 237154257,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/c38af920ef584336a9b3b454165f6b4116459fbf",
                "title": "Mixture-based Feature Space Learning for Few-shot Image Classification",
                "abstract": "We introduce Mixture-based Feature Space Learning (MixtFSL) for obtaining a rich and robust feature representation in the context of few-shot image classification. Previous works have proposed to model each base class either with a single point or with a mixture model by relying on offline clustering algorithms. In contrast, we propose to model base classes with mixture models by simultaneously training the feature extractor and learning the mixture model parameters in an online manner. This results in a richer and more discriminative feature space which can be employed to classify novel examples from very few samples. Two main stages are proposed to train the MixtFSL model. First, the multimodal mixtures for each base class and the feature extractor parameters are learned using a combination of two loss functions. Second, the resulting network and mixture models are progressively refined through a leader-follower learning procedure, which uses the current estimate as a \"target\" network. This target network is used to make a consistent assignment of instances to mixture components, which increases performance and stabilizes training. The effectiveness of our end-to-end feature space learning approach is demonstrated with extensive experiments on four standard datasets and four backbones. Notably, we demon-strate that when we combine our robust representation with recent alignment-based approaches, we achieve new state-of-the-art results in the inductive setting, with an absolute accuracy for 5-shot classification of 82.45% on miniImageNet, 88.20% with tieredImageNet, and 60.70% in FC100 using the ResNet-12 backbone.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "9050817",
                        "name": "Arman Afrasiyabi"
                    },
                    {
                        "authorId": "144430305",
                        "name": "Jean-Fran\u00e7ois Lalonde"
                    },
                    {
                        "authorId": "11146706",
                        "name": "Christian Gagn'e"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b73b130084c20349d84f9fca5588a33688658d36",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-10082",
                    "ArXiv": "2011.10082",
                    "MAG": "3110241708",
                    "DOI": "10.1109/CVPRW56347.2022.00308",
                    "CorpusId": 227119006
                },
                "corpusId": 227119006,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b73b130084c20349d84f9fca5588a33688658d36",
                "title": "Hybrid Consistency Training with Prototype Adaptation for Few-Shot Learning",
                "abstract": "Few-Shot Learning (FSL) aims to improve a model\u2019s generalization capability in low data regimes. Recent FSL works have made steady progress via metric learning, meta learning, representation learning, etc. However, FSL remains challenging due to the following longstanding difficulties. 1) The seen and unseen classes are disjoint, resulting in a distribution shift between training and testing. 2) During testing, labeled data of previously unseen classes is sparse, making it difficult to reliably extrapolate from labeled support examples to unlabeled query examples. To tackle the first challenge, we introduce Hybrid Consistency Training to jointly leverage two types of consistency: 1) interpolation consistency, which interpolates hidden features to imposes linear behavior locally, and 2) data augmentation consistency, which learns robust embeddings against sample variations. As for the second challenge, we use unlabeled examples to iteratively normalize features and adapt prototypes, as opposed to commonly used one-time update, for more reliable prototype-based transductive inference. We show that our method generates a 2% to 5% improvement over the state-of-the-art methods with similar backbones on five FSL datasets and, more notably, a 7% to 8% improvement for more challenging cross-domain FSL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2054892429",
                        "name": "Meng Ye"
                    },
                    {
                        "authorId": "2117690187",
                        "name": "Xiaoyu Lin"
                    },
                    {
                        "authorId": "69919463",
                        "name": "Giedrius Burachas"
                    },
                    {
                        "authorId": "47977519",
                        "name": "Ajay Divakaran"
                    },
                    {
                        "authorId": "1400198856",
                        "name": "Yi Yao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "While self supervision has been shown to boost few-shot learning (Gidaris et al., 2019; Su et al., 2020), its utility in cases of large domain gaps between base and novel datasets have not been evaluated."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1d16d4cdc3fcce26e2c2097d13896ec09683eee3",
                "externalIds": {
                    "ArXiv": "2010.07734",
                    "DBLP": "journals/corr/abs-2010-07734",
                    "MAG": "3092800243",
                    "CorpusId": 222379753
                },
                "corpusId": 222379753,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1d16d4cdc3fcce26e2c2097d13896ec09683eee3",
                "title": "Self-training for Few-shot Transfer Across Extreme Task Differences",
                "abstract": "All few-shot learning techniques must be pre-trained on a large, labeled \"base dataset\". In problem domains where such large labeled datasets are not available for pre-training (e.g., X-ray images), one must resort to pre-training in a different \"source\" problem domain (e.g., ImageNet), which can be very different from the desired target task. Traditional few-shot and transfer learning techniques fail in the presence of such extreme differences between the source and target tasks. In this paper, we present a simple and effective solution to tackle this extreme domain gap: self-training a source domain representation on unlabeled data from the target domain. We show that this improves one-shot performance on the target domain by 2.9 points on average on a challenging benchmark with multiple domains.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51257044",
                        "name": "Cheng Perng Phoo"
                    },
                    {
                        "authorId": "73710317",
                        "name": "B. Hariharan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1cfe5cc309c060b6a73999c6cfd04db0f4604081",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChenML21",
                    "ArXiv": "2010.02430",
                    "MAG": "3092340867",
                    "DOI": "10.1109/CVPRW53098.2021.00300",
                    "CorpusId": 222141573
                },
                "corpusId": 222141573,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1cfe5cc309c060b6a73999c6cfd04db0f4604081",
                "title": "Shot in the Dark: Few-Shot Learning with No Base-Class Labels",
                "abstract": "Few-shot learning aims to build classifiers for new classes from a small number of labeled examples and is commonly facilitated by access to examples from a distinct set of \u2018base classes\u2019. The difference in data distribution between the test set (novel classes) and the base classes used to learn an inductive bias often results in poor generalization on the novel classes. To alleviate problems caused by the distribution shift, previous research has explored the use of unlabeled examples from the novel classes, in addition to labeled examples of the base classes, which is known as the transductive setting. In this work, we show that, surprisingly, off-the-shelf self-supervised learning outperforms transductive few-shot methods by 3.9% for 5-shot accuracy on miniImageNet without using any base class labels. This motivates us to examine more carefully the role of features learned through self-supervision in few-shot learning. Comprehensive experiments are conducted to compare the transferability, robustness, efficiency, and the complementarity of supervised and self-supervised features.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "13556061",
                        "name": "Z. Chen"
                    },
                    {
                        "authorId": "35208858",
                        "name": "Subhransu Maji"
                    },
                    {
                        "authorId": "1389846455",
                        "name": "E. Learned-Miller"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Varying the amount of data samples has led to interesting observations as well [42, 59]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "087982ef88b84b1e9e2c35333a7d97bd8df9451c",
                "externalIds": {
                    "DBLP": "journals/nca/StuhrB22",
                    "ArXiv": "2009.02383",
                    "MAG": "3083205773",
                    "DOI": "10.1007/s00521-022-07031-9",
                    "CorpusId": 221516899
                },
                "corpusId": 221516899,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/087982ef88b84b1e9e2c35333a7d97bd8df9451c",
                "title": "Don\u2019t miss the mismatch: investigating the objective function mismatch for unsupervised representation learning",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1490942327",
                        "name": "Bonifaz Stuhr"
                    },
                    {
                        "authorId": "32522952",
                        "name": "J\u00fcrgen Brauer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Studies have shown that self supervised learning can be used along with few-shot learning to boost the performance of the model towards novel categories [100], [101]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "619571a2be1f0fc03899ce4bff8d6dee40608540",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-15484",
                    "ArXiv": "2007.15484",
                    "MAG": "3046014872",
                    "CorpusId": 220870675
                },
                "corpusId": 220870675,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/619571a2be1f0fc03899ce4bff8d6dee40608540",
                "title": "Learning from Few Samples: A Survey",
                "abstract": "Deep neural networks have been able to outperform humans in some cases like image recognition and image classification. However, with the emergence of various novel categories, the ability to continuously widen the learning capability of such networks from limited samples, still remains a challenge. Techniques like Meta-Learning and/or few-shot learning showed promising results, where they can learn or generalize to a novel category/task based on prior knowledge. In this paper, we perform a study of the existing few-shot meta-learning techniques in the computer vision domain based on their method and evaluation metrics. We provide a taxonomy for the techniques and categorize them as data-augmentation, embedding, optimization and semantics based learning for few-shot, one-shot and zero-shot settings. We then describe the seminal work done in each category and discuss their approach towards solving the predicament of learning from few samples. Lastly we provide a comparison of these techniques on the commonly used benchmark datasets: Omniglot, and MiniImagenet, along with a discussion towards the future direction of improving the performance of these techniques towards the final goal of outperforming humans.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1491010174",
                        "name": "Nihar Bendre"
                    },
                    {
                        "authorId": "1399006250",
                        "name": "H. Terashima-Mar\u00edn"
                    },
                    {
                        "authorId": "71756373",
                        "name": "Peyman Najafirad"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "rch, which inherently deals with transfer from pretext tasks to semantic ones and must therefore represent more than their training data [2, 11, 13, 18, 21, 27, 33, 44, 56, 92, 93]. Some recent works [26, 73] demonstrate that this can improve few-shot learning, although these use self-supervised auxiliary losses rather than integrating self-supervision into episodic training. Also particularly relevant ar"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0fee1138854bd786697dcdb1f052b079d077b9e9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-11498",
                    "MAG": "3044220283",
                    "ArXiv": "2007.11498",
                    "CorpusId": 220686825
                },
                "corpusId": 220686825,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0fee1138854bd786697dcdb1f052b079d077b9e9",
                "title": "CrossTransformers: spatially-aware few-shot transfer",
                "abstract": "Given new tasks with very little data$-$such as new classes in a classification problem or a domain shift in the input$-$performance of modern vision systems degrades remarkably quickly. In this work, we illustrate how the neural network representations which underpin modern vision systems are subject to supervision collapse, whereby they lose any information that is not necessary for performing the training task, including information that may be necessary for transfer to new tasks or domains. We then propose two methods to mitigate this problem. First, we employ self-supervised learning to encourage general-purpose features that transfer better. Second, we propose a novel Transformer based neural network architecture called CrossTransformers, which can take a small number of labeled images and an unlabeled query, find coarse spatial correspondence between the query and the labeled images, and then infer class membership by computing distances between spatially-corresponding features. The result is a classifier that is more robust to task and domain shift, which we demonstrate via state-of-the-art performance on Meta-Dataset, a recent dataset for evaluating transfer from ImageNet to many other vision datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2786693",
                        "name": "Carl Doersch"
                    },
                    {
                        "authorId": "2110759501",
                        "name": "Ankush Gupta"
                    },
                    {
                        "authorId": "1688869",
                        "name": "Andrew Zisserman"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "\u2026aided by self-supervision: Several works have proposed to use a selfsupervised loss either alongside supervised meta-learning episodes (Gidaris et al., 2019; Liu et al., 2019) or to initialize a model prior to supervised meta-learning on the source domain (Chen et al., 2019; Su et al., 2019).",
                ", 2019) or to initialize a model prior to supervised meta-learning on the source domain (Chen et al., 2019; Su et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a8d75bf3fd4c94ea678bbbc603b980d71b84c649",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-11325",
                    "MAG": "3036984428",
                    "ArXiv": "2006.11325",
                    "CorpusId": 219966219
                },
                "corpusId": 219966219,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a8d75bf3fd4c94ea678bbbc603b980d71b84c649",
                "title": "Self-Supervised Prototypical Transfer Learning for Few-Shot Classification",
                "abstract": "Most approaches in few-shot learning rely on costly annotated data related to the goal task domain during (pre-)training. Recently, unsupervised meta-learning methods have exchanged the annotation requirement for a reduction in few-shot classification performance. Simultaneously, in settings with realistic domain shift, common transfer learning has been shown to outperform supervised meta-learning. Building on these insights and on advances in self-supervised learning, we propose a transfer learning approach which constructs a metric embedding that clusters unlabeled prototypical samples and their augmentations closely together. This pre-trained embedding is a starting point for few-shot classification by summarizing class clusters and fine-tuning. We demonstrate that our self-supervised prototypical transfer learning approach ProtoTransfer outperforms state-of-the-art unsupervised meta-learning methods on few-shot tasks from the mini-ImageNet dataset. In few-shot experiments with domain shift, our approach even has comparable performance to supervised methods, but requires orders of magnitude fewer labels.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2170718178",
                        "name": "Carlos Medina"
                    },
                    {
                        "authorId": "1664923028",
                        "name": "A. Devos"
                    },
                    {
                        "authorId": "2052174",
                        "name": "M. Grossglauser"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The pretext tasks shall be carefully designed in order to facilitate the network to learn downstream-related semantics features (Su et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ac2e6afbadc3428eb5dff167b52025e64525441e",
                "externalIds": {
                    "MAG": "3034792570",
                    "DBLP": "journals/corr/abs-2006-09136",
                    "ArXiv": "2006.09136",
                    "CorpusId": 219708273,
                    "PubMed": "33283198"
                },
                "corpusId": 219708273,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ac2e6afbadc3428eb5dff167b52025e64525441e",
                "title": "When Does Self-Supervision Help Graph Convolutional Networks?",
                "abstract": "Self-supervision as an emerging technique has been employed to train convolutional neural networks (CNNs) for more transferrable, generalizable, and robust representation learning of images. Its introduction to graph convolutional networks (GCNs) operating on graph data is however rarely explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into GCNs. We first elaborate three mechanisms to incorporate self-supervision into GCNs, analyze the limitations of pretraining & finetuning and self-training, and proceed to focus on multi-task learning. Moreover, we propose to investigate three novel self-supervised learning tasks for GCNs with theoretical rationales and numerical comparisons. Lastly, we further integrate multi-task self-supervision into graph adversarial training. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining more generalizability and robustness. Our codes are available at https://github.com/Shen-Lab/SS-GCNs.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "89197162",
                        "name": "Yuning You"
                    },
                    {
                        "authorId": "2648459",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "1705610299",
                        "name": "Yang Shen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "84de84ee56df7d71f81eb8ea982957c92e337680",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-11992",
                    "ArXiv": "2004.11992",
                    "MAG": "3018373304",
                    "DOI": "10.1007/978-3-030-58574-7_43",
                    "CorpusId": 216552999
                },
                "corpusId": 216552999,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/84de84ee56df7d71f81eb8ea982957c92e337680",
                "title": "Extending and Analyzing Self-Supervised Learning Across Domains",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "152823401",
                        "name": "Bram Wallace"
                    },
                    {
                        "authorId": "73710317",
                        "name": "B. Hariharan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "contexts": [
                "These approaches are most beneficial in data regimes where labeled data is sparse [26, 30], which is often the case for medical datasets.",
                "We opt for this more challenging task based on previous work that has shown that more difficult auxiliary tasks produce more useful feature representations [19, 26]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d4757d419161ef7a69902a4b6d07fad92b688bcc",
                "externalIds": {
                    "ArXiv": "2004.09629",
                    "MAG": "3018581516",
                    "DBLP": "journals/corr/abs-2004-09629",
                    "DOI": "10.1109/CVPRW50498.2020.00497",
                    "CorpusId": 216036143
                },
                "corpusId": 216036143,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d4757d419161ef7a69902a4b6d07fad92b688bcc",
                "title": "Self-Supervised Feature Extraction for 3D Axon Segmentation",
                "abstract": "Existing learning-based methods to automatically trace axons in 3D brain imagery often rely on manually annotated segmentation labels. Labeling is a labor-intensive process and is not scalable to whole-brain analysis, which is needed for improved understanding of brain function. We propose a self-supervised auxiliary task that utilizes the tube-like structure of axons to build a feature extractor from unlabeled data. The proposed auxiliary task constrains a 3D convolutional neural network (CNN) to predict the order of permuted slices in an input 3D volume. By solving this task, the 3D CNN is able to learn features without ground-truth labels that are useful for downstream segmentation with the 3D U-Net model. To the best of our knowledge, our model is the first to perform automated segmentation of axons imaged at subcellular resolution with the SHIELD technique. We demonstrate improved segmentation performance over the 3D U-Net model on both the SHIELD PVGPe dataset and the BigNeuron Project, single neuron Janelia dataset.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "103417134",
                        "name": "Tzofi Klinghoffer"
                    },
                    {
                        "authorId": "153785516",
                        "name": "Peter Morales"
                    },
                    {
                        "authorId": "46796816",
                        "name": "Young-Gyun Park"
                    },
                    {
                        "authorId": "2057387142",
                        "name": "Nicholas B. Evans"
                    },
                    {
                        "authorId": "2227861750",
                        "name": "Kwanghun Chung"
                    },
                    {
                        "authorId": "2865214",
                        "name": "L. Brattain"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "741efeedb616045961efa9ae03a173bbc24a0560",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-13525",
                    "ArXiv": "2003.13525",
                    "MAG": "3013795656",
                    "CorpusId": 214714010
                },
                "corpusId": 214714010,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/741efeedb616045961efa9ae03a173bbc24a0560",
                "title": "Improving out-of-distribution generalization via multi-task self-supervised pretraining",
                "abstract": "Self-supervised feature representations have been shown to be useful for supervised classification, few-shot learning, and adversarial robustness. We show that features obtained using self-supervised learning are comparable to, or better than, supervised learning for domain generalization in computer vision. We introduce a new self-supervised pretext task of predicting responses to Gabor filter banks and demonstrate that multi-task learning of compatible pretext tasks improves domain generalization performance as compared to training individual tasks alone. Features learnt through self-supervision obtain better generalization to unseen domains when compared to their supervised counterpart when there is a larger domain shift between training and test distributions and even show better localization ability for objects of interest. Self-supervised feature representations can also be combined with other domain generalization methods to further boost performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "153804922",
                        "name": "Isabela Albuquerque"
                    },
                    {
                        "authorId": "2047256670",
                        "name": "N. Naik"
                    },
                    {
                        "authorId": "49299019",
                        "name": "Junnan Li"
                    },
                    {
                        "authorId": "2844898",
                        "name": "N. Keskar"
                    },
                    {
                        "authorId": "2166511",
                        "name": "R. Socher"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "classification problem, such as works based on graph theories [66], [67], [68], [69], reinforcement learning [70], differentiable SVM [71], generative models [72], [73], [74], [75], [76], [77], [78], [79], [80], transductive learning [81], [82], [83], [84], [85], recurrent models [86], [87], self-supervised learning [88], [89], the recent capsule network [90], and temporal convolutions [91]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7eddaa5da37658a8c3bfe80e770079b38d6c96ca",
                "externalIds": {
                    "DBLP": "journals/pami/ZhangCLS23",
                    "MAG": "3022615309",
                    "ArXiv": "2003.06777",
                    "DOI": "10.1109/TPAMI.2022.3217373",
                    "CorpusId": 218571015,
                    "PubMed": "36288227"
                },
                "corpusId": 218571015,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7eddaa5da37658a8c3bfe80e770079b38d6c96ca",
                "title": "DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning",
                "abstract": "In this work, we develop methods for few-shot image classification from a new perspective of optimal matching between image regions. We employ the Earth Mover's Distance (EMD) as a metric to compute a structural distance between dense image representations to determine image relevance. The EMD generates the optimal matching flows between structural elements that have the minimum matching cost, which is used to calculate the image distance for classification. To generate the important weights of elements in the EMD formulation, we design a cross-reference mechanism, which can effectively alleviate the adverse impact caused by the cluttered background and large intra-class appearance variations. To implement <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"lin-ieq1-3217373.gif\"/></alternatives></inline-formula>-shot classification, we propose to learn a structured fully connected layer that can directly classify dense image representations with the EMD. Based on the implicit function theorem, the EMD can be inserted as a layer into the network for end-to-end training. Our extensive experiments validate the effectiveness of our algorithm which outperforms state-of-the-art methods by a significant margin on five widely used few-shot classification benchmarks, namely, miniImageNet, tieredImageNet, Fewshot-CIFAR100 (FC100), Caltech-UCSD Birds-200-2011 (CUB), and CIFAR-FewShot (CIFAR-FS). We also demonstrate the effectiveness of our method on the image retrieval task in our experiments.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144876211",
                        "name": "Chi Zhang"
                    },
                    {
                        "authorId": "1928716951",
                        "name": "Yujun Cai"
                    },
                    {
                        "authorId": "2604251",
                        "name": "Guosheng Lin"
                    },
                    {
                        "authorId": "12459603",
                        "name": "Chunhua Shen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "51b9a61c6673bf43144127a8a1ad98e27c643f94",
                "externalIds": {
                    "DBLP": "conf/eccv/GadelhaRSKCL0M20",
                    "ArXiv": "2003.13834",
                    "MAG": "3110047846",
                    "DOI": "10.1007/978-3-030-58607-2_28",
                    "CorpusId": 214727684
                },
                "corpusId": 214727684,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/51b9a61c6673bf43144127a8a1ad98e27c643f94",
                "title": "Label-Efficient Learning on Point Clouds using Approximate Convex Decompositions",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48644686",
                        "name": "Matheus Gadelha"
                    },
                    {
                        "authorId": "2895705",
                        "name": "Aruni RoyChowdhury"
                    },
                    {
                        "authorId": "153778266",
                        "name": "Gopal Sharma"
                    },
                    {
                        "authorId": "2808670",
                        "name": "E. Kalogerakis"
                    },
                    {
                        "authorId": "48749954",
                        "name": "Liangliang Cao"
                    },
                    {
                        "authorId": "1389846455",
                        "name": "E. Learned-Miller"
                    },
                    {
                        "authorId": "2151037031",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "35208858",
                        "name": "Subhransu Maji"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Moreover, recent work has shown that self-supervision can be beneficial to many other learning problems [10, 18, 28, 29, 58, 71], such as few-shot [18, 58] and semi-supervised [28, 71] learning, or training generative adversarial networks [10]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8a96d5d6da7d0dfb5c06a8d1c6b5d9036d7a9215",
                "externalIds": {
                    "DBLP": "conf/cvpr/GidarisBKPC20",
                    "MAG": "3008503415",
                    "ArXiv": "2002.12247",
                    "DOI": "10.1109/CVPR42600.2020.00696",
                    "CorpusId": 211532737
                },
                "corpusId": 211532737,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8a96d5d6da7d0dfb5c06a8d1c6b5d9036d7a9215",
                "title": "Learning Representations by Predicting Bags of Visual Words",
                "abstract": "Self-supervised representation learning targets to learn convnet-based image representations from unlabeled data. Inspired by the success of NLP methods in this area, in this work we propose a self-supervised approach based on spatially dense image descriptions that encode discrete visual concepts, here called visual words. To build such discrete representations, we quantize the feature maps of a first pre-trained self-supervised convnet, over a k-means based vocabulary. Then, as a self-supervised task, we train another convnet to predict the histogram of visual words of an image (i.e., its Bag-of-Words representation) given as input a perturbed version of that image. The proposed task forces the convnet to learn perturbation-invariant and context-aware image features, useful for downstream image understanding tasks. We extensively evaluate our method and demonstrate very strong empirical results, e.g., our pre-trained self-supervised representations transfer better on detection task and similarly on classification over classes ``unseen'' during pre-training, when compared to the supervised case. This also shows that the process of image discretization into visual words can provide the basis for very powerful self-supervised approaches in the image domain, thus allowing further connections to be made to related methods from the NLP domain that have been extremely successful so far.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2475428",
                        "name": "Spyros Gidaris"
                    },
                    {
                        "authorId": "3056236",
                        "name": "Andrei Bursuc"
                    },
                    {
                        "authorId": "2505902",
                        "name": "N. Komodakis"
                    },
                    {
                        "authorId": "2066449716",
                        "name": "P. P'erez"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "161ecde59203eaaa5347cdead5a1090f2a1669a2",
                "externalIds": {
                    "MAG": "3095374178",
                    "DBLP": "conf/eccv/ZhangZQLTK20",
                    "DOI": "10.1007/978-3-030-58558-7_31",
                    "CorpusId": 220647489
                },
                "corpusId": 220647489,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/161ecde59203eaaa5347cdead5a1090f2a1669a2",
                "title": "Few-Shot Action Recognition with Permutation-Invariant Attention",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2849892",
                        "name": "Hongguang Zhang"
                    },
                    {
                        "authorId": "48571183",
                        "name": "Li Zhang"
                    },
                    {
                        "authorId": "50844674",
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "authorId": "40124570",
                        "name": "Hongdong Li"
                    },
                    {
                        "authorId": "143635540",
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "authorId": "2155775",
                        "name": "Piotr Koniusz"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "19244df5dc082d736152c9d5db118ba27512123d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1912-04486",
                    "MAG": "2995450817",
                    "CorpusId": 209140286
                },
                "corpusId": 209140286,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/19244df5dc082d736152c9d5db118ba27512123d",
                "title": "To Balance or Not to Balance: An Embarrassingly Simple Approach for Learning with Long-Tailed Distributions",
                "abstract": "Real-world visual data often exhibits a long-tailed distribution, where some \u201chead\u201d classes have a large number of samples, yet only a few samples are available for the \u201ctail\u201d classes. Such imbalanced distribution causes a great challenge for learning a deep neural network, which can be boiled down into a dilemma: on the one hand, we prefer to increase the exposure of the tail class samples to avoid the excessive dominance of head classes in the classifier training. On the other hand, oversampling tail classes makes the network prone to over-fitting, since the head class samples are often consequently under-represented. To resolve this dilemma, in this paper, we propose an embarrassingly simple-yet-effective approach. The key idea is to split a network into a classifier part and a feature extractor part, and then employ different training strategies for each part. Specifically, to promote the awareness of tail-classes, a class-balanced sampling scheme is utilised for training both the classifier and the feature extractor. For the feature extractor, we also introduce an auxiliary training task, which is to train a classifier under the regular random sampling scheme. In this way, the feature extractor is jointly trained from both sampling strategies and thus can take advantage of all training data and avoid the over-fitting issue. Apart from this basic auxiliary task, we further explore the benefit of using self-supervised learning as the auxiliary task. Without using any bells and whistles, our model achieves superior performance over the state-of-the-art solutions.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2108073390",
                        "name": "Junjie Zhang"
                    },
                    {
                        "authorId": "2161037",
                        "name": "Lingqiao Liu"
                    },
                    {
                        "authorId": "2155300848",
                        "name": "Peng Wang"
                    },
                    {
                        "authorId": "12459603",
                        "name": "Chunhua Shen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "42dfba880b734b30910cdf689b4c43c2769d8ad8",
                "externalIds": {
                    "MAG": "3011401432",
                    "ArXiv": "1912.04486",
                    "CorpusId": 215886134
                },
                "corpusId": 215886134,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/42dfba880b734b30910cdf689b4c43c2769d8ad8",
                "title": "To Balance or Not to Balance: A Simple-yet-Effective Approach for Learning with Long-Tailed Distributions",
                "abstract": "Real-world visual data often exhibits a long-tailed distribution, where some ''head'' classes have a large number of samples, yet only a few samples are available for ''tail'' classes. Such imbalanced distribution causes a great challenge for learning a deep neural network, which can be boiled down into a dilemma: on the one hand, we prefer to increase the exposure of tail class samples to avoid the excessive dominance of head classes in the classifier training. On the other hand, oversampling tail classes makes the network prone to over-fitting, since head class samples are often consequently under-represented. To resolve this dilemma, in this paper, we propose a simple-yet-effective auxiliary learning approach. The key idea is to split a network into a classifier part and a feature extractor part, and then employ different training strategies for each part. Specifically, to promote the awareness of tail-classes, a class-balanced sampling scheme is utilised for training both the classifier and the feature extractor. For the feature extractor, we also introduce an auxiliary training task, which is to train a classifier under the regular random sampling scheme. In this way, the feature extractor is jointly trained from both sampling strategies and thus can take advantage of all training data and avoid the over-fitting issue. Apart from this basic auxiliary task, we further explore the benefit of using self-supervised learning as the auxiliary task. Without using any bells and whistles, our model achieves superior performance over the state-of-the-art solutions.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2108073390",
                        "name": "Junjie Zhang"
                    },
                    {
                        "authorId": "2161037",
                        "name": "Lingqiao Liu"
                    },
                    {
                        "authorId": "144282676",
                        "name": "Peng Wang"
                    },
                    {
                        "authorId": "12459603",
                        "name": "Chunhua Shen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Moreover, unlabeled examples [55], [56] or regularization techniques [57], [58] have also been utilized to improve the few-shot classification performance."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "aec864aa0c640a8c1641fef146ae58931fb2ed37",
                "externalIds": {
                    "DBLP": "journals/tmm/ChengHHLZ23",
                    "DOI": "10.1109/TMM.2021.3123813",
                    "CorpusId": 240269302
                },
                "corpusId": 240269302,
                "publicationVenue": {
                    "id": "10e76a35-58d6-443c-9683-fc16f2dd0a92",
                    "name": "IEEE transactions on multimedia",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Transactions on Multimedia",
                        "IEEE Trans Multimedia",
                        "IEEE trans multimedia"
                    ],
                    "issn": "1520-9210",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046"
                },
                "url": "https://www.semanticscholar.org/paper/aec864aa0c640a8c1641fef146ae58931fb2ed37",
                "title": "Mixer-Based Semantic Spread for Few-Shot Learning",
                "abstract": "Key semantics can come from everywhere on an image. Semantic alignment is a key part of few-shot learning but still remains challenging. In this paper, we design a Mixer-Based Semantic Spread (MBSS) algorithm that employs a mixer module to spread the key semantic on the whole image, so that one can directly compare the processed image pairs. We first adopt a convolutional neural network to extract features from both support and query images and separate each of them into multiple Local Descriptor-based Representations (LDRs). The LDRs are then fed into the mixer for semantic spread, where every LDR attracts complementary information from its peers. In this way, the objective semantic is made spread on the whole image in a data-driven manner. The overall pipeline is supervised by a voting-based loss, guaranteeing a good mixer. Visualization results validate the feasibility of our mixer. Comprehensive experiments on three benchmark datasets, miniImageNet, tieredImageNet, and CUB, show that our algorithm achieves the state-of-the-art performance in both 5-way 1-shot and 5-way 5-shot settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2157745206",
                        "name": "Jun Cheng"
                    },
                    {
                        "authorId": "41022398",
                        "name": "Fusheng Hao"
                    },
                    {
                        "authorId": "51209425",
                        "name": "Fengxiang He"
                    },
                    {
                        "authorId": "2109528221",
                        "name": "Liu Liu"
                    },
                    {
                        "authorId": "1783427",
                        "name": "Qieshi Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0719dd491e9a9d739270d977765515624b096034",
                "externalIds": {
                    "DBLP": "journals/tgrs/LiGLZZW23",
                    "DOI": "10.1109/TGRS.2023.3234252",
                    "CorpusId": 255738725
                },
                "corpusId": 255738725,
                "publicationVenue": {
                    "id": "70628d6a-97aa-4571-9701-bc0eb3989c32",
                    "name": "IEEE Transactions on Geoscience and Remote Sensing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Geosci Remote Sens"
                    ],
                    "issn": "0196-2892",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=36",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0719dd491e9a9d739270d977765515624b096034",
                "title": "Multiform Ensemble Self-Supervised Learning for Few-Shot Remote Sensing Scene Classification",
                "abstract": "Self-supervised learning is an effective way to solve model collapse for few-shot remote sensing scene classification (FSRSSC). However, most self-supervised contrastive learning auxiliary tasks perform poorly on the high interclass similarity problem in FSRSSC. Furthermore, it is time-consuming and computationally expensive to obtain the best combination among numerous self-supervised auxiliary tasks. In practical applications, we may encounter difficulties in remote sensing data acquisition and labeling, while most FSRSSC studies only focus on the former. To alleviate the above problems, we propose a multiform ensemble self-supervised learning (MES2L) framework for FSRSSC in this article. Based on the transfer learning-based few-shot scheme, we design a novel global\u2013local contrastive learning auxiliary task to solve the low interclass separability problem. The self-attention mechanism is designed in the local contrast features to investigate the intrinsic associations between different remote sensing scene objectives. We also present a multiform ensemble enhancement (MEE) training method. Ensemble enhancement involves the concatenation of features extracted from different backbones trained by a combination of multiform self-supervised auxiliary tasks. MEE can not only be regarded as a more straightforward alternative to knowledge distillation but also can achieve an effective compromise between expensive computational cost and classification accuracy. In addition, we provide two scene classification schemes of inductive and transductive settings, corresponding to solving the difficulties of remote sensing data acquisition and labeling. The proposed network achieves state-of-the-art results on three benchmark FSRSSC datasets. The potential of the MES2L framework is also demonstrated in combination with classical metalearning-based and metric learning-based few-shot algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2130167832",
                        "name": "Jianzhao Li"
                    },
                    {
                        "authorId": "2052304304",
                        "name": "Maoguo Gong"
                    },
                    {
                        "authorId": "2155487560",
                        "name": "Huilin Liu"
                    },
                    {
                        "authorId": "2108399250",
                        "name": "Yourun Zhang"
                    },
                    {
                        "authorId": "2108147319",
                        "name": "Mingyang Zhang"
                    },
                    {
                        "authorId": "46220633",
                        "name": "Yue Wu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "(ref.(146)) presented a systematic study by varying the degree of domain shift and analysing the performance of multiple metalearners on a variety of domains."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cba3fe0df3b3df1dc94e087644e7d0f2c090c4a2",
                "externalIds": {
                    "CorpusId": 259905633
                },
                "corpusId": 259905633,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cba3fe0df3b3df1dc94e087644e7d0f2c090c4a2",
                "title": "Small data problems in deep learning applications with remote sensing: A review",
                "abstract": "Researchers and engineers have increasingly used Deep Learning (DL) for a variety of Remote Sensing (RS) tasks. However, data from local observations or via ground truth is often quite limited for training DL models, especially when these models represent key socio-environmental problems, such as the monitoring of extreme, destructive climate events, biodiversity, and sudden changes in ecosystem states. Such cases, also known as small data problems, pose significant methodological challenges. This review summarises these challenges in the RS domain and the possibility of using emerging DL techniques to overcome them. We show that the small data problem is a common challenge across disciplines and scales that results in poor model generalisability and transferability, yet this has not been investigated in a structured way. We first introduce ten emerging DL techniques: transfer learning, self-supervised learning, semi-supervised learning, few-shot learning, zero-shot learning, active learning, weakly supervised learning, multitask learning, process-aware learning, and ensemble learning; we also include a validation technique known as spatial k-fold cross validation. These techniques have shown promising potential in other scientific disciplines, but have been rarely applied in the RS domain. We also provide guidance on which learning technique to use in various cases, which helps to create a more methodologically robust DL application (and a greater number of them) that can be used to tackle socially important problems with limited data. Introduction Over the last decade, Artificial Intelligence (AI) technologies, especially Machine Learning (ML) and Deep Learning (DL), have been increasingly used for understanding and predicting human-environment interactions. ML is a subset of AI that implements algorithms which use data to learn how to perform a specific task without being explicitly programmed. DL is a subset of ML that focuses on training deep neural networks capable of implicit feature extraction from unstructured data, such as images, text, and sound. Scientists have actively employed DL for image processing and data analysis, recently providing innovative solutions in the field of Remote Sensing (RS) to detect and classify objects on Earth. This study defines RS as the use of satellite and aircraft-based sensors. The expanding field of RS provides an abundance of data streams from numerous sources. This, combined with the growing array of available data products, delivers a wide range of data that is useful for addressing various problems. Among them, Landsat, has been operational since the early 1970s and provides a unique long-term record of satellite imagery with a 30-metre spatial resolution. The Copernicus programme\u2019s Sentinel-2 system generates data with a 10-metre spatial resolution, offering a balance between spatial detail and data continuity as well as radar imagery based on the Sentinel-1 mission. One recently launched hyperspectral mission, the Environmental Mapping and Analysis Program (EnMap), stands out with over 200 spectral bands and a 30-metre spatial resolution; this offers unique opportunities for researchers to map ecosystems and their changes in detail. In addition, commercial platforms, such as SkySat, provide extremely high-resolution data with a spatial resolution of less than one metre. Together, these diverse RS platforms contribute to a more comprehensive understanding of the Earth\u2019s surface across different scales and domains. For this reason, these RS products are widely used to study local and regional environmental problems, including agricultural productivity, the water quality of lakes and ponds, the ecological patterns of forests and grasslands, and damage to natural, cultivated, and inhabited land through extreme weather events. Since the spatial and temporal resolution of RS products is likely to continue increasing, DL applications are expected to become even more popular for solving fine-scale local issues where each local site has its own unique conditions and context. Since DL algorithms have fewer inductive biases but larger parameter counts than conventional ML algorithms, DL models normally require a large amount of data for training. DL methods usually learn from raw data and skip manual feature engineering steps; this means that human efforts are not needed to quantitatively measure some attributes from the data. For example, DL algorithms can learn from image data directly, instead of using the extracted shape and size of an object in an image. When sufficient data is available, DL methods can automatically extract the meaningful features from low to high levels for prediction. However, although raw data of common events is generally abundant, the lack of sufficient labelling information makes the collection and preparation of a large reference dataset a persistent challenge for many RS applications. Moreover, certain scenarios also lack available reference data. For instance, biodiversity monitoring needs a large number of human observers well trained in taxonomic classification, which often prevents observation campaigns from generating datasets large enough for sound DL applications. Furthermore, anomaly events such as climate extremes and disease outbreaks are too rare for researchers to acquire sufficient data coverage. Their sample size is often as small as n = 1\u2013300, which is usually insufficient for DL application . The gap between the large data availability of RS imagery and the small data availability of several important real-world environmental problems (referred to as the \u201csmall data problem\u201d) is a very common challenge. It is hard to acquire the ground-truth response labels associated with the input features. This makes sense, because the goal of most of these studies is to develop a model designed to predict a specific response variable from the various observed input features. However, traditional DL training methodologies require a large initial set of labelled data to train predictive models. It is increasingly clear that this is an emerging problem for AI, and researchers have proposed several novel DL techniques that require less labelled data (e.g., transfer learning and self-supervised learning). However, to the best of our knowledge, the small data problem has not been systematically tackled as an emerging scientific challenge in the RS domain. In this review, we summarise the current research on the small data problem in RS (particularly as it relates to DL) and suggest promising DL techniques to address this problem. First, we explore how the small data problem can be defined. Second, we describe a few common elements of the previous studies. Third, we present the advantages and disadvantages of using a small dataset. Finally, we offer a set of practical recommendations about how RS scientists can better implement DL techniques to fully make use of a small dataset. We believe that the small data problem is a common \u2013 but still unsolved \u2013 issue for recent RS applications, and therefore, this review should serve as a valuable resource for supporting RS and DL applications in scientists\u2019 and policymakers\u2019 attempts to address a wide range of environmental problems. What is the small data problem? We argue that a dataset can be considered large (not small) when the dataset consists of >100,000 annotated samples, or when it covers the entire probability distribution in a high-dimensional space. In this case, model generalisability and transferability are expected to be high. For example, there are several free large datasets that can be used for DL: the ImageNet dataset, containing over 14 million annotated images, the Common Objects in Context (COCO) dataset, containing 330K images, 1.5 million object instances, and 80 object categories, and the OpenImages dataset, containing over 9 million images. These datasets can be used for training a large DL model with thousands to millions of parameters. In the RS domain, land use / land cover classification would be a typical example. In contrast, data is more likely to be regarded as small (or not large enough) when the dataset consists of <1,000 annotated samples, the dataset covers the distribution poorly, or the number of samples is expected to be insufficient when using DL to find meaningful features. This is a frequently occurring situation, but it can be a significant challenge for training deep neural networks. A relatively small dataset can negatively affect the performance of a DL model due to overfitting, which is when a model performs well with the training data but poorly on new, independent testing data. This therefore results in low levels of model generalisability and transferability. A common case within the RS domain (but particularly relevant) is that the data can be \u201cextra-small\u201d, meaning that the dataset consists of just 1\u201310 annotated samples (e.g., historical natural disasters and disease outbreaks). The size would be sufficient for human beings to start guessing what features can uniquely describe the target, but it would not be sufficient for automated, implicit DL feature extraction. In the DL domain, the Tiny ImageNet Dataset (also known as MicroImageNet) contains 500 images for each class (of 200 classes), indicating that DL scientists regard this level of data size as small. According to the articles we reviewed in the following section, the majority of the studies targeted classification of a few types, and many of them collected less than 500 images for each class (e.g., refs.). There should not be a strict divide between \u201csmall\u201d and \u201clarge\u201d when training DL models, because the size of the dataset required may depend on various factors such as the complexity of the task and the number of features in the data. Typically, the challenges stemming from a limited amount of labelled data increase with system complexity, the rarity of observations (e.g., endangered biol",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "96305984",
                        "name": "A. Safonova"
                    },
                    {
                        "authorId": "9708281",
                        "name": "Gohar Ghazaryan"
                    },
                    {
                        "authorId": "2197399241",
                        "name": "Stefan Stiller"
                    },
                    {
                        "authorId": "1405401637",
                        "name": "M. Main-Knorn"
                    },
                    {
                        "authorId": "1706813",
                        "name": "C. Nendel"
                    },
                    {
                        "authorId": "38057242",
                        "name": "M. Ryo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "unexplored, we have come across some works on similar topics (Cao and Wu, 2021; Su et al., 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c46284952ad503541cfa8f972a6d27a7d7545e73",
                "externalIds": {
                    "CorpusId": 262051399
                },
                "corpusId": 262051399,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c46284952ad503541cfa8f972a6d27a7d7545e73",
                "title": "Ten deep learning techniques to address small data problems with remote sensing",
                "abstract": "Researchers and engineers have increasingly used Deep Learning (DL) for a variety of Remote Sensing (RS) tasks. However, data from local observations or via ground truth is often quite limited for training DL models, especially when these models represent key socio-environmental problems, such as the monitoring of extreme, destructive climate events, biodiversity, and sudden changes in ecosystem states. Such cases, also known as small data problems, pose significant methodological challenges. This review summarises these challenges in the RS domain and the possibility of using emerging DL techniques to overcome them. We show that the small data problem is a common challenge across disciplines and scales that results in poor model generalisability and transferability. We then introduce an overview of ten promising DL techniques: transfer learning, self-supervised learning, semi-supervised learning, few-shot learning, zeroshot learning, active learning, weakly supervised learning, multitask learning, process-aware learning, and ensemble learning; we also include a validation technique known as spatial k-fold cross validation. Our particular contribution was to develop a flowchart that helps DL users select which technique to use given by answering a few questions. We hope that our review article facilitate DL applications to tackle societally important environmental problems with limited reference data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243219320",
                        "name": "Anastasiia Safonova"
                    },
                    {
                        "authorId": "9708281",
                        "name": "Gohar Ghazaryan"
                    },
                    {
                        "authorId": "2197399241",
                        "name": "Stefan Stiller"
                    },
                    {
                        "authorId": "2243225380",
                        "name": "Magdalena Main-Knorn"
                    },
                    {
                        "authorId": "1706813",
                        "name": "C. Nendel"
                    },
                    {
                        "authorId": "2243220281",
                        "name": "Masahiro Ryo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1cd398b747ee777c470eed32fe40bd9bd7a4eb2e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-09717",
                    "DOI": "10.48550/arXiv.2208.09717",
                    "CorpusId": 251719260
                },
                "corpusId": 251719260,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1cd398b747ee777c470eed32fe40bd9bd7a4eb2e",
                "title": "Learning Primitive-aware Discriminative Representations for FSL",
                "abstract": "Few-shot learning (FSL)aims to learn a classifier that can be easily adapted to recognize novel classes,given only a few labeled examples per class.Limited data keep this task challenging for deep learning.Recent work has achieved promising classification performance,where the image-level feature from global average pooling operation is used to measure the similarity among samples.However,these global features ignore abundant local and structural information that is transferable and consistent between seen and unseen classes.How can humans easily recognize novel classes with only few samples?Some study in cognitive science argue that humans can recognize novel classes with the learned primitives .Although base and novel classes are non-overlapping, they can share some primitives in common.We expect to mine both transferable and discriminative representation from base classes and adopt them to recognize novel classes.Concretely, building on the episodic training mechanism, We propose a Primitive Mining and Reasoning Network(PMRN) to learn primitive-aware discriminative representation in an end-to-end manner for metric-based FSL model.We first add self-supervision auxiliary task in parallel,forcing model to learn visual pattern corresponding to primitives.To further mine and produce transferable primitive-aware representations,we design an Adaptive Channel Grouping(ACG) module to synthesize a set of visual primitive features from object embedding by enhancing informative channel maps while suppressing useless ones. Based on the learned primitive feature,a Semantic Correlation Reasoning(SCR) module is proposed to improve discriminative power of primitives by capturing internal relations among them.Finally,we learn the task-specific importance of primitives and conduct the primitive-level metric based on task-specific attention feature.Extensive experiments show that our method achieves state-of-the- art results on six standard benchmarks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118801703",
                        "name": "Jian Yang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5942115e923dcd5b3eae63865a59c05160ad1ad7",
                "externalIds": {
                    "DBLP": "conf/iclr/NiSSGG22",
                    "CorpusId": 251647290
                },
                "corpusId": 251647290,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5942115e923dcd5b3eae63865a59c05160ad1ad7",
                "title": "The Close Relationship Between Contrastive Learning and Meta-Learning",
                "abstract": "Contrastive learning has recently taken off as a paradigm for learning from unlabeled data. In this paper, we discuss the close relationship between contrastive learning and meta-learning under a certain task distribution. We complement this observation by showing that established meta-learning methods, such as Prototypical Networks, achieve comparable performance to SimCLR when paired with this task distribution. This relationship can be leveraged by taking established techniques from meta-learning, such as task-based data augmentation, and showing that they benefit contrastive learning as well. These tricks also benefit state-of-the-art self-supervised learners without using negative pairs such as BYOL, which achieves 94.6% accuracy on CIFAR-10 using a self-supervised ResNet-18 feature extractor trained with our meta-learning tricks. We conclude that existing advances designed for contrastive learning or metalearning can be exploited to benefit the other, and it is better for contrastive learning researchers to take lessons from the meta-learning literature (and viceversa) than to reinvent the wheel. Our Pytorch implementation can be found on: https://github.com/RenkunNi/MetaContrastive",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3434279",
                        "name": "Renkun Ni"
                    },
                    {
                        "authorId": "1643697854",
                        "name": "Manli Shu"
                    },
                    {
                        "authorId": "78859465",
                        "name": "Hossein Souri"
                    },
                    {
                        "authorId": "121592562",
                        "name": "Micah Goldblum"
                    },
                    {
                        "authorId": "1962083",
                        "name": "T. Goldstein"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recent works (Gidaris et al., 2019; Su et al., 2020; Chen et al., 2021) show that adding self-supervised loss functions for representation learning improves fewshot recognition performance."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a8fc34772ed42b17f49580cb7e8372fc96e307ad",
                "externalIds": {
                    "DBLP": "conf/iclr/DasYP22",
                    "CorpusId": 251648027
                },
                "corpusId": 251648027,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a8fc34772ed42b17f49580cb7e8372fc96e307ad",
                "title": "ConFeSS: A Framework for Single Source Cross-Domain Few-Shot Learning",
                "abstract": "Most current few-shot learning methods train a model from abundantly labeled base category data and then transfer and adapt the model to sparsely labeled novel category data. These methods mostly generalize well on novel categories from the same domain as the base categories but perform poorly for distant domain categories. In this paper, we propose a framework for few-shot learning coined as ConFeSS (Contrastive Learning and Feature Selection System) that tackles large domain shift between base and novel categories. The first step of our framework trains a feature extracting backbone with the contrastive loss on the base category data. Since the contrastive loss does not use supervision, the features can generalize better to distant target domains. For the second step, we train a masking module to select relevant features that are more suited to target domain classification. Finally, a classifier is fine-tuned along with the backbone such that the backbone produces features similar to the relevant ones. To evaluate our framework, we tested it on a recently introduced cross-domain few-shot learning benchmark. Experimental results demonstrate that our framework outperforms all meta-learning approaches and produces competitive results against recent cross-domain methods. Additional analyses are also performed to better understand our framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49950690",
                        "name": "Debasmit Das"
                    },
                    {
                        "authorId": "3057834",
                        "name": "Sungrack Yun"
                    },
                    {
                        "authorId": "29905643",
                        "name": "F. Porikli"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2c4dc7d89635a2bf1baacba3f6cc1eb4c6726719",
                "externalIds": {
                    "DBLP": "journals/tim/JiangWLLZ22",
                    "DOI": "10.1109/TIM.2022.3189739",
                    "CorpusId": 251481675
                },
                "corpusId": 251481675,
                "publicationVenue": {
                    "id": "3edbd5e0-8799-420a-9ca8-b35c646c354f",
                    "name": "IEEE Transactions on Instrumentation and Measurement",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Instrum Meas"
                    ],
                    "issn": "0018-9456",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=19",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=19"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2c4dc7d89635a2bf1baacba3f6cc1eb4c6726719",
                "title": "Multilevel Noise Contrastive Network for Few-Shot Image Denoising",
                "abstract": "In recent years, most denoising methods based on deep convolutional neural networks heavily rely on massive noisy\u2013clean image pairs. Collecting massive noisy\u2013clean image pairs is expensive and not practical in real scenes. Currently, few-shot learning has been applied to many areas to cope with the absence of data. The few-shot learning, however, in image denoising severely suffers from domain gap problems, including dataset domain gap and feature domain gap, especially for the real noisy images. Therefore, this article proposes a multilevel noise contrastive network (MNC-Net) performing few-shot image denoising. MNC-Net consists of two training stages: 1) using contrastive learning to self-supervise the training of multilevel noise contrastive learner (MNCL) on the pure synthetic noisy images with multiple Gaussian noise levels to ease the acute dataset domain gap and 2) features generated by the MNCL on limited data are fused to the second stage and alleviate the feature domain gap using our proposed denoising network. Specifically, the MNCL consists of a contrastive feature extractor (CFE) and a contrastive feature projector (CFP). MNCL learns the rich and complex content-invariant degradations and general multiple-level noise representations. The denoising network in the second stage is composed of guided feature encoder (GFE) and adaptive denoising decoder (ADD). The GFE uses contrast features from CFE to guide the produced representations on the specific input noisy images. Then, such output features are fed into the ADD to adaptively denoise the noisy images on the corresponding noise distribution. To the best of our knowledge, this work is the first attempt to jointly use the few-shot learning and contrastive learning in the deep denoising field. Extensive experiments on CBSD68, Kodak24, Set12, SIDD, and DND show that our method achieves promising denoising performances in the absence of data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153963636",
                        "name": "Bo Jiang"
                    },
                    {
                        "authorId": "2155328403",
                        "name": "Jiahuan Wang"
                    },
                    {
                        "authorId": "2143370518",
                        "name": "Yao Lu"
                    },
                    {
                        "authorId": "143911582",
                        "name": "Guangming Lu"
                    },
                    {
                        "authorId": "71108051",
                        "name": "Dafan Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recent works [12, 34] point out that contrastive learning helps to avoid few-shot learning from limitations like over-fitting [6, 21] or supervision collapse [8], which serves as auxiliary losses to learn the representation alignment."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f38f18b30095c014cf549c5ad1c985bc2a233a64",
                "externalIds": {
                    "DBLP": "conf/eccv/ZhengCJ22",
                    "DOI": "10.1007/978-3-031-19772-7_18",
                    "CorpusId": 253448607
                },
                "corpusId": 253448607,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/f38f18b30095c014cf549c5ad1c985bc2a233a64",
                "title": "Few-Shot Action Recognition with Hierarchical Matching and Contrastive Learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1391032149",
                        "name": "S. Zheng"
                    },
                    {
                        "authorId": "3009919",
                        "name": "Shizhe Chen"
                    },
                    {
                        "authorId": "1721329",
                        "name": "Qin Jin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2c3b7fc79c430b559260d900196df960de152f12",
                "externalIds": {
                    "DBLP": "conf/eccv/JiangCCCZYAW22",
                    "DOI": "10.1007/978-3-031-20044-1_14",
                    "CorpusId": 253099711
                },
                "corpusId": 253099711,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/2c3b7fc79c430b559260d900196df960de152f12",
                "title": "DnA: Improving Few-Shot Transfer Learning with Low-Rank Decomposition and Alignment",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152420547",
                        "name": "Ziyu Jiang"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "1753619492",
                        "name": "Xuxi Chen"
                    },
                    {
                        "authorId": "153655416",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "2116644664",
                        "name": "Luowei Zhou"
                    },
                    {
                        "authorId": "2150687325",
                        "name": "Lu Yuan"
                    },
                    {
                        "authorId": "2072795428",
                        "name": "A. Awadallah"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[51] Jong-Chyi Su, Subhransu Maji, and Bharath Hariharan.",
                "The principle behind this approach is using limited supervision and fine-tuning in assessment [13, 51, 71]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "218c0ec463224ae6135e0823c03c8c23a93d0316",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-08672",
                    "DOI": "10.48550/arXiv.2211.08672",
                    "CorpusId": 253553382
                },
                "corpusId": 253553382,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/218c0ec463224ae6135e0823c03c8c23a93d0316",
                "title": "Fair contrastive pre-training for geographic images",
                "abstract": "Contrastive representation learning is widely employed in visual recognition for geographic image data (remote-sensing such as satellite imagery or proximal sensing such as street-view imagery), but because of landscape heterogeneity, models can show disparate performance across spatial units. In this work, we consider fairness risks in land-cover semantic segmentation which uses pre-trained representation in contrastive self-supervised learning. We assess class distribution shifts and model prediction disparities across selected sensitive groups: urban and rural scenes for satellite image datasets and city GDP level for a street view image dataset. We propose a mutual information training objective for multi-level latent space. The objective improves feature identification by removing spurious representations of dense local features which are disparately distributed across groups. The method achieves improved fairness results and outperforms state-of-the-art methods in terms of precision-fairness trade-off. In addition, we validate that representations learnt with the proposed method include lowest sensitive information using a linear separation evaluation. This work highlights the need for specific fairness analyses in geographic images, and provides a so-lution that can be generalized to different self-supervised learning methods or image data. Our code is available at: https://anonymous.4open.science/r/FairDCL-1283",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150238772",
                        "name": "Miaohui Zhang"
                    },
                    {
                        "authorId": "3144230",
                        "name": "R. Chunara"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "(Zhang et al., 2020; Caccia et al., 2021; Mazumder et al., 2021; Su et al., 2020), relinquished pre-training altogether and employed auxiliary pretext task to boost task-agnostic learning.",
                "\u2026than supervised pre-training in CL. Owing to additional computational effort, some of the approaches, e.g. (Zhang et al., 2020; Caccia et al., 2021; Mazumder et al., 2021; Su et al., 2020), relinquished pre-training altogether and employed auxiliary pretext task to boost task-agnostic learning."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "259123993",
                "publicationVenue": null,
                "url": null,
                "title": "SELF-SUPERVISED BASED CONTINUAL LEARNING APPROACH",
                "abstract": null,
                "year": 2022,
                "authors": []
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Some recent work has showed that self-supervised learning could contribute to few-shot learning, and the loss of self-supervised learning was introduced in [46,47]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f17518b80441fe4497dc800a25d7fc7167207953",
                "externalIds": {
                    "DBLP": "journals/remotesensing/ZhouDL22",
                    "DOI": "10.3390/rs14133111",
                    "CorpusId": 251107546
                },
                "corpusId": 251107546,
                "publicationVenue": {
                    "id": "8e1bd4b5-d5b2-4e22-ba0a-01fe5568d472",
                    "name": "Remote Sensing",
                    "type": "journal",
                    "alternate_names": [
                        "Remote Sens"
                    ],
                    "issn": "2315-4675",
                    "alternate_issns": [
                        "2072-4292"
                    ],
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-169233",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/remotesensing",
                        "http://www.mdpi.com/journal/remotesensing",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-169233"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f17518b80441fe4497dc800a25d7fc7167207953",
                "title": "Self-Supervision and Self-Distillation with Multilayer Feature Contrast for Supervision Collapse in Few-Shot Remote Sensing Scene Classification",
                "abstract": ": Although the means of catching remote sensing images are becoming more effective and more abundant, the samples that can be collected in some speci\ufb01c environments can be quite scarce. When there are limited labeled samples, the methods for analyzing remote sensing images for scene classi\ufb01cation perform drastically worse. Methods that classify few-shot remote sensing image scenes are often based on meta-learning algorithms for the handling of sparse data. However, this research shows they will be affected by supervision collapse where features in remote sensing images that help with out-of-distribution classes are discarded, which is harmful for the generation of unseen classes and new tasks. In this work, we wish to remind readers of the existence of supervision collapse in scene classi\ufb01cation of few-shot remote sensing images and propose a method named SSMR based on multi-layer feature contrast to overcome supervision collapse. First of all, the method makes use of the label information contained in a \ufb01nite number of samples for supervision and guides self-supervised learning to train the embedding network with supervision generated by multilayer feature contrast. This can prevent features from losing intra-class variation. Intra-class variation is always useful in classifying unseen data. What is more, the multi-layer feature contrast is merged with self-distillation, and the modi\ufb01ed self-distillation is used to encourage the embedding network to extract suf\ufb01ciently general features that transfer better to unseen classes and new domains. We demonstrate that most of the existing few-shot scene classi\ufb01cation methods suffer from supervision collapse and that SSMR overcomes supervision collapse well in the experiments on the new dataset we specially designed for examining the problem, with a 2.4\u201317.2% increase compared to the available methods. Furthermore, we performed a series of ablation experiments to demonstrate how effective and necessary each structure of the proposed method is and to show how different choices in training impact \ufb01nal performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2166206531",
                        "name": "Haonan Zhou"
                    },
                    {
                        "authorId": "2179407169",
                        "name": "Xiaoping Du"
                    },
                    {
                        "authorId": "2129770955",
                        "name": "Sen Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Self-supervised learning has been used for few-shot classification but primarily as an auxiliary loss [14, 47] along with the standard cross-entropy loss."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "49a654a9284c4f30092fdde13847cb6727faa80e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-11058",
                    "CorpusId": 231719070
                },
                "corpusId": 231719070,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/49a654a9284c4f30092fdde13847cb6727faa80e",
                "title": "Revisiting Contrastive Learning for Few-Shot Classification",
                "abstract": "Instance discrimination based contrastive learning has emerged as a leading approach for self-supervised learning of visual representations. Yet, its generalization to novel tasks remains elusive when compared to representations learned with supervision, especially in the few-shot setting. We demonstrate how one can incorporate supervision in the instance discrimination based contrastive self-supervised learning framework to learn representations that generalize better to novel tasks. We call our approach CIDS ( C ontrastive I nstance D iscrimination with S upervision). CIDS performs favorably compared to existing algorithms on popular few-shot benchmarks like Mini-ImageNet or Tiered-ImageNet. We also propose a novel model selection algorithm that can be used in conjunction with a universal embedding trained using CIDS to outperform state-of-the-art algorithms on the challenging Meta-Dataset benchmark.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "134549850",
                        "name": "Orchid Majumder"
                    },
                    {
                        "authorId": "2529423",
                        "name": "Avinash Ravichandran"
                    },
                    {
                        "authorId": "35208858",
                        "name": "Subhransu Maji"
                    },
                    {
                        "authorId": "32235780",
                        "name": "M. Polito"
                    },
                    {
                        "authorId": "3243878",
                        "name": "Rahul Bhotika"
                    },
                    {
                        "authorId": "1715959",
                        "name": "Stefano Soatto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "39ca5df5a480178fe0308fb3fc6eed30838af71e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-16009",
                    "CorpusId": 232417809
                },
                "corpusId": 232417809,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/39ca5df5a480178fe0308fb3fc6eed30838af71e",
                "title": "Revisiting Deep Local Descriptor for Improved Few-Shot Classification",
                "abstract": "\u2014Few-shot classi\ufb01cation studies the problem of quickly adapting a deep learner to understanding novel classes based on few support images. In this context, recent research efforts have been aimed at designing more and more complex classi\ufb01ers that measure similarities between query and support images, but left the importance of feature embeddings seldom explored. We show that the reliance on sophisticated classi\ufb01er is not necessary and a simple classi\ufb01er applied directly to improved feature embeddings can outperform state-of-the-art methods. To this end, we present a new method named DCAP in which we investigate how one can improve the quality of embeddings by leveraging Dense Classi\ufb01cation and Attentive Pooling. Speci\ufb01-cally, we propose to pre-train a learner on base classes with abundant samples to solve dense classi\ufb01cation problem \ufb01rst and then \ufb01ne-tune the learner on a bunch of randomly sampled few-shot tasks to adapt it to few-shot scenerio or the test time scenerio. We suggest to pool feature maps by applying attentive pooling instead of the widely used global average pooling (GAP) to prepare embeddings for few-shot classi\ufb01cation during meta- \ufb01netuning. Attentive pooling learns to reweight local descriptors, explaining what the learner is looking for as evidence for decision making. Experiments on two benchmark datasets show the proposed method to be superior in multiple few-shot settings while being simpler and more explainable. Code is available at: https://github.com/Ukeyboard/dcap/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2155125868",
                        "name": "Jun He"
                    },
                    {
                        "authorId": "48043335",
                        "name": "Richang Hong"
                    },
                    {
                        "authorId": "3076466",
                        "name": "Xueliang Liu"
                    },
                    {
                        "authorId": "2285442",
                        "name": "Mingliang Xu"
                    },
                    {
                        "authorId": "2146058571",
                        "name": "Meng Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "DNNs only produce meaningful outputs for indistribution (ID) data (Su et al., 2020).",
                "We expect dataset shift to manifest in an unusually large self-supervision loss (Su et al., 2020) that compensates for the decreased ability to detect uncertain cases of uncertainty estimation methods."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2b749b0fceed04765c6e898e0d3fbbde1eff05a3",
                "externalIds": {
                    "DBLP": "conf/midl/Gonzalez021",
                    "CorpusId": 234797935
                },
                "corpusId": 234797935,
                "publicationVenue": {
                    "id": "2c1c3a62-7d6e-44b9-b225-a9ddb7ebdb8d",
                    "name": "International Conference on Medical Imaging with Deep Learning",
                    "type": "conference",
                    "alternate_names": [
                        "MIDL",
                        "Int Conf Med Imaging Deep Learn"
                    ],
                    "url": "https://www.midl.io/"
                },
                "url": "https://www.semanticscholar.org/paper/2b749b0fceed04765c6e898e0d3fbbde1eff05a3",
                "title": "Self-supervised Out-of-distribution Detection for Cardiac CMR Segmentation",
                "abstract": "The segmentation of cardiac structures in Cine Magnetic Resonance imaging (CMR) plays an important role in monitoring ventricular function, and many deep learning solutions have been introduced that successfully automate this task. Yet due to variabilities in the CMR acquisition process, images from different centers or acquisition protocols differ considerably. This causes deep learning models to fail silently. It is therefore crucial to identify out-of-distribution (OOD) samples for which the trained model is unsuitable. For models with a self-supervised proxy task, we propose a simple method to identify OOD samples that does not require adapting the model architecture or access to a separate OOD dataset during training. As the performance of self-supervised tasks can be assessed without ground truth information, it indicates during test time when a sample differs from the training distribution. The proposed method combines a voxel-wise uncertainty estimate with the self-supervision information. Our approach is validated across three CMR datasets and two different proxy tasks. We find that it is more effective at detecting OOD samples than state-of-the-art post-hoc OOD detection and uncertainty estimation approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2086768035",
                        "name": "Camila Gonz\u00e1lez"
                    },
                    {
                        "authorId": "2079272715",
                        "name": "A. Mukhopadhyay"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a8da7cb34868956f3213f3481e7f5c97ea6af5d3",
                "externalIds": {
                    "DBLP": "conf/pricai/NiZZXYL21",
                    "DOI": "10.1007/978-3-030-89370-5_9",
                    "CorpusId": 240461929
                },
                "corpusId": 240461929,
                "publicationVenue": {
                    "id": "a058ca2a-3dc3-485e-beab-559941c41cf1",
                    "name": "Pacific Rim International Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Pac Rim Int Conf Artif Intell",
                        "PRICAI"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a8da7cb34868956f3213f3481e7f5c97ea6af5d3",
                "title": "ANF: Attention-Based Noise Filtering Strategy for Unsupervised Few-Shot Classification",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "102363444",
                        "name": "Guang-ren Ni"
                    },
                    {
                        "authorId": "2108970904",
                        "name": "Hongguang Zhang"
                    },
                    {
                        "authorId": "46509258",
                        "name": "Jing Zhao"
                    },
                    {
                        "authorId": "2112147841",
                        "name": "Liyang Xu"
                    },
                    {
                        "authorId": "40552948",
                        "name": "Wenjing Yang"
                    },
                    {
                        "authorId": "2410125",
                        "name": "L. Lan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Finally, (Su et al., 2020) has shown that selfsupervision is very beneficial to few-shot learning, especially when the pretext task is very complex, and that using more unlabelled data for pretraining is useful only if they come from the same domain as the ones used for the few-shot task.",
                "Finally, (Su et al., 2020) has shown that selfsupervision is very beneficial to few-shot learning, es-"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ed8228d32fbe7ac55f680eab4f9843d49eeb6e7",
                "externalIds": {
                    "DBLP": "conf/robovis/RiandDC21",
                    "DOI": "10.5220/0010689500003061",
                    "CorpusId": 240257860
                },
                "corpusId": 240257860,
                "publicationVenue": {
                    "id": "6ae5751c-a613-4745-822e-1a136e34d2a7",
                    "name": "International Conference on Robotics, Computer Vision and Intelligent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "ROBOVIS",
                        "Int Conf Robot Comput Vis Intell Syst"
                    ],
                    "url": "http://www.robovis.org"
                },
                "url": "https://www.semanticscholar.org/paper/2ed8228d32fbe7ac55f680eab4f9843d49eeb6e7",
                "title": "Implicitly using Human Skeleton in Self-supervised Learning: Influence on Spatio-temporal Puzzle Solving and on Video Action Recognition",
                "abstract": ": In this paper we studied the in\ufb02uence of adding skeleton data on top of human actions videos when performing self-supervised learning and action recognition. We show that adding this information without additional constraints actually hurts the accuracy of the network; we argue that the added skeleton is not considered by the network and seen as a noise masking part of the natural image. We bring \ufb01rst results on puzzle solving and video action recognition to support this hypothesis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2136496899",
                        "name": "Mathieu Riand"
                    },
                    {
                        "authorId": "2091111876",
                        "name": "Laurent Doll\u00e9"
                    },
                    {
                        "authorId": "1776651",
                        "name": "P. Callet"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For jigsaw tasks, we use 35-permutations from Su et al. (2020).",
                "An additional module is inserted between the embedding network and classifier and we use hidden dimensions from Su et al. (2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6d0851a3003dba7ef635d84174dd0364203a65d9",
                "externalIds": {
                    "CorpusId": 257925460
                },
                "corpusId": 257925460,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6d0851a3003dba7ef635d84174dd0364203a65d9",
                "title": "Embedding Adaptation via Early-Stage Feature",
                "abstract": "In this section, we describe the preprocessing including equations in our paper. Assume we are given the embedding support set Sf and embedding query set Qf . We apply centering and l2-normalization to the embedding samples for reconstruction training as described in (A.2). Preprocessed embeddings z \u2208 Zpreprocess are used as an input to the reconstruction module g\u03c6. The same preprocessing (centering and l2-normalization) is applied at the output of reconstruction module to compute the reconstruction loss LFR as in (A.5).",
                "year": 2021,
                "authors": []
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e8fcf8f5c47bfbcffcb19bc67ffa6870d7a56b82",
                "externalIds": {
                    "CorpusId": 231401334
                },
                "corpusId": 231401334,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e8fcf8f5c47bfbcffcb19bc67ffa6870d7a56b82",
                "title": "A UTO-VIEW CONTRASTIVE LEARNING FOR FEW-SHOT IMAGE RECOGNITION",
                "abstract": "Few-shot learning aims to recognize new classes with few annotated instances within each category. Recently, metric-based meta-learning approaches have shown the superior performance in tackling few-shot learning problems. Despite their success, existing metric-based few-shot approaches often fail to push the fine-grained sub-categories apart in the embedding space given no fine-grained labels. This may result in poor generalization to fine-grained sub-categories, and thus affects model interpretation. To alleviate this problem, we introduce contrastive loss into few-shot classification for learning latent fine-grained structure in the embedding space. Furthermore, to overcome the drawbacks of random image transformation used in current contrastive learning in producing noisy and inaccurate image pairs (i.e., views), we develop a learning-to-learn algorithm to automatically generate different views of the same image. Extensive experiments on standard few-shot learning benchmarks and few-shot fine-grained image classification demonstrate the superiority of our method.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "However, such unlabelled images may have a negative impact if the domain shift between the unlabelled and labelled dataset is too big [27].",
                "[27] Jong-Chyi Su, Subhransu Maji, and Bharath Hariharan."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cdf45ca898ba4b120f82e518997a4149adc94d1f",
                "externalIds": {
                    "CorpusId": 254587587
                },
                "corpusId": 254587587,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cdf45ca898ba4b120f82e518997a4149adc94d1f",
                "title": "Classi\ufb01cation with a domain shift in medical imaging",
                "abstract": "In medical imaging, the amount of unlabelled data often exceeds that of labelled data [28]. When labelled data is lacking, we may still be able to obtain training sets that are big enough for training large-scale deep models, but that are from a different data distribution from the actual data to be encountered at test time. In particular, retinal images are obtained through devices from different vendors, which have diverse characteristics. Moreover, shifts in data distribution may be found even among datasets of images collected with devices from the same vendor due to patient cooperation during the examination or interoperator variability [20, 14]. In this study, we propose a method that is able to exploit additional unlabelled datasets, possibly with a domain shift, to improve predictions on our labelled data. In order to do so, we exploit a Convolutional Neural Network (CNN) architecture with three classifiers that share a common feature backbone: (1) classifies labelled samples, (2) determines the dataset of origin of each sample, and (3) solve a self-supervised task, in particular predicting image rotations and solving Jigsaw puzzles, on unlabelled data. Classifier (2) is trained to not be able to distinguish between different datasets, so that data is projected into a common feature space, while the self supervised learner (3) learns features from unlabelled data, that are shared through the common feature backbone. Previously, in [6], Carlucci et al. tested how solving Jigsaw puzzles on unlabelled samples, while classifying labelled images, helps in learning more general representations. However, there are two key differences with respect to our work. First, their architecture does not include a domain classifier. Second, they focus on the classification performance on the unlabelled dataset, while we focus on the performance on the labelled one. The main contributions of our work are the following: 1) we proposed a new architecture that is able to exploit unlabelled data with a domain shift to improve predictions on labelled data. It works by learning features through self-supervised learning (SSL) while projecting all the data onto the same space to achieve better transfer; 2) we run a series of experiments on Office-31 dataset, to test that our method can be successfully applied to natural images; 3) we tested this method on retinal images, in particular for age-related macular degeneration (AMD) and diabetic retinopathy (DR) grading, consistently improving the results of the baselines (from 78.83% to 83.53% average test accuracy on AMD grading and from 60.00% to 67.79% average test accuracy on DR grading). We qualitatively and quantitatively verified, through saliency maps, how the proposed method is able to focus more",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "36759523",
                        "name": "A. Fontanella"
                    }
                ]
            }
        }
    ]
}