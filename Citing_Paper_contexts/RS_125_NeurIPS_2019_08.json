{
    "offset": 0,
    "data": [
        {
            "contexts": [
                "Such an alternative training of the potential function and the policy is similar to model-based reinforcement learning algorithms (Luo et al., 2018; Sun et al., 2018; Janner et al., 2019) for monotonic improvement of policies."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "835cbfe49247443ce9481ca86050b4f46c0faec5",
                "externalIds": {
                    "ArXiv": "2310.03301",
                    "CorpusId": 263671883
                },
                "corpusId": 263671883,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/835cbfe49247443ce9481ca86050b4f46c0faec5",
                "title": "Learning Energy Decompositions for Partial Inference of GFlowNets",
                "abstract": "This paper studies generative flow networks (GFlowNets) to sample objects from the Boltzmann energy distribution via a sequence of actions. In particular, we focus on improving GFlowNet with partial inference: training flow functions with the evaluation of the intermediate states or transitions. To this end, the recently developed forward-looking GFlowNet reparameterizes the flow functions based on evaluating the energy of intermediate states. However, such an evaluation of intermediate energies may (i) be too expensive or impossible to evaluate and (ii) even provide misleading training signals under large energy fluctuations along the sequence of actions. To resolve this issue, we propose learning energy decompositions for GFlowNets (LED-GFN). Our main idea is to (i) decompose the energy of an object into learnable potential functions defined on state transitions and (ii) reparameterize the flow functions using the potential functions. In particular, to produce informative local credits, we propose to regularize the potential to change smoothly over the sequence of actions. It is also noteworthy that training GFlowNet with our learned potential can preserve the optimal policy. We empirically verify the superiority of LED-GFN in five problems including the generation of unstructured and maximum independent sets, molecular graphs, and RNA sequences.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2208976555",
                        "name": "Hyosoon Jang"
                    },
                    {
                        "authorId": "2254481377",
                        "name": "Minsu Kim"
                    },
                    {
                        "authorId": "2255852402",
                        "name": "Sungsoo Ahn"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4935ed52766af2ebca8f08ca027b87261719b69f",
                "externalIds": {
                    "ArXiv": "2310.01706",
                    "CorpusId": 263608900
                },
                "corpusId": 263608900,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4935ed52766af2ebca8f08ca027b87261719b69f",
                "title": "On Representation Complexity of Model-based and Model-free Reinforcement Learning",
                "abstract": "We study the representation complexity of model-based and model-free reinforcement learning (RL) in the context of circuit complexity. We prove theoretically that there exists a broad class of MDPs such that their underlying transition and reward functions can be represented by constant depth circuits with polynomial size, while the optimal $Q$-function suffers an exponential circuit complexity in constant-depth circuits. By drawing attention to the approximation errors and building connections to complexity theory, our theory provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective: in some cases, the ground-truth rule (model) of the environment is simple to represent, while other quantities, such as $Q$-function, appear complex. We empirically corroborate our theory by comparing the approximation error of the transition kernel, reward function, and optimal $Q$-function in various Mujoco environments, which demonstrates that the approximation errors of the transition kernel and reward function are consistently lower than those of the optimal $Q$-function. To the best of our knowledge, this work is the first to study the circuit complexity of RL, which also provides a rigorous framework for future research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2254086052",
                        "name": "Hanlin Zhu"
                    },
                    {
                        "authorId": "2253949316",
                        "name": "Baihe Huang"
                    },
                    {
                        "authorId": "2253435878",
                        "name": "Stuart Russell"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the pre-training objective, we use forward dynamics prediction, as it has been shown to be useful in model-based methods (Janner et al., 2019) and auxiliary loss literature (He et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "965d0dd0110bd39ad02baf4eaa86fbfb952db735",
                "externalIds": {
                    "ArXiv": "2310.00771",
                    "CorpusId": 263605596
                },
                "corpusId": 263605596,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/965d0dd0110bd39ad02baf4eaa86fbfb952db735",
                "title": "Pre-training with Synthetic Data Helps Offline Reinforcement Learning",
                "abstract": "Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets. The results of this paper not only illustrate the importance of pre-training for offline DRL but also show that the pre-training data can be synthetic and generated with remarkably simple mechanisms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2253430428",
                        "name": "Zecheng Wang"
                    },
                    {
                        "authorId": "2254007293",
                        "name": "Che Wang"
                    },
                    {
                        "authorId": "2256465605",
                        "name": "Zixuan Dong"
                    },
                    {
                        "authorId": "2253397883",
                        "name": "Keith Ross"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "383ba22db30ff612a6067daf2921eefd331274ac",
                "externalIds": {
                    "DOI": "10.1016/j.eswa.2023.121742",
                    "CorpusId": 263639814
                },
                "corpusId": 263639814,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/383ba22db30ff612a6067daf2921eefd331274ac",
                "title": "How predictors affect the RL-based search strategy in Neural Architecture Search?",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2253810457",
                        "name": "Wu Jia"
                    },
                    {
                        "authorId": "2253669114",
                        "name": "Tianjin Deng"
                    },
                    {
                        "authorId": "2254931799",
                        "name": "Qi Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "LatentDiffuser is incorporated into a research trajectory focused on model-based reinforcement learning (RL) (Sutton, 1990; Janner et al., 2019; Schrittwieser et al., 2020; Lu et al., 2021; Eysenbach et al., 2022; Suh et al., 2023), as it makes decisions by forecasting future outcomes."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "02aaccf045178a0ad70906941f21b1eabde34227",
                "externalIds": {
                    "ArXiv": "2310.00311",
                    "CorpusId": 263334587
                },
                "corpusId": 263334587,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/02aaccf045178a0ad70906941f21b1eabde34227",
                "title": "Efficient Planning with Latent Diffusion",
                "abstract": "Temporal abstraction and efficient planning pose significant challenges in offline reinforcement learning, mainly when dealing with domains that involve temporally extended tasks and delayed sparse rewards. Existing methods typically plan in the raw action space and can be inefficient and inflexible. Latent action spaces offer a more flexible paradigm, capturing only possible actions within the behavior policy support and decoupling the temporal structure between planning and modeling. However, current latent-action-based methods are limited to discrete spaces and require expensive planning. This paper presents a unified framework for continuous latent action space representation learning and planning by leveraging latent, score-based diffusion models. We establish the theoretical equivalence between planning in the latent action space and energy-guided sampling with a pretrained diffusion model and incorporate a novel sequence-level exact sampling method. Our proposed method, $\\texttt{LatentDiffuser}$, demonstrates competitive performance on low-dimensional locomotion control tasks and surpasses existing methods in higher-dimensional tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2249834508",
                        "name": "Wenhao Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "89e2413c3b5db4d48431e7d116c61ccff5e8d03e",
                "externalIds": {
                    "ArXiv": "2309.15421",
                    "CorpusId": 262940040
                },
                "corpusId": 262940040,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/89e2413c3b5db4d48431e7d116c61ccff5e8d03e",
                "title": "Deep Learning in Deterministic Computational Mechanics",
                "abstract": "The rapid growth of deep learning research, including within the field of computational mechanics, has resulted in an extensive and diverse body of literature. To help researchers identify key concepts and promising methodologies within this field, we provide an overview of deep learning in deterministic computational mechanics. Five main categories are identified and explored: simulation substitution, simulation enhancement, discretizations as neural networks, generative approaches, and deep reinforcement learning. This review focuses on deep learning methods rather than applications for computational mechanics, thereby enabling researchers to explore this field more effectively. As such, the review is not necessarily aimed at researchers with extensive knowledge of deep learning -- instead, the primary audience is researchers at the verge of entering this field or those who attempt to gain an overview of deep learning in computational mechanics. The discussed concepts are, therefore, explained as simple as possible.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35562955",
                        "name": "L. Herrmann"
                    },
                    {
                        "authorId": "2244431426",
                        "name": "Stefan Kollmannsberger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 Official codes distributed from the paper [24]: to build PMT-G.",
                "[24] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",
                "MBPO is the state-of-the-art model-based policy optimization [24].",
                "Existing methods for non-stationary environments can be grouped into three schools of thought: 1) shoehorning: directly using established frameworks for stationary MDPs, assuming no extra mechanisms are needed since non-stationarity already exists in standard RL due to policy updates; 2) model-based policy updates: updating models with new data, using short rollouts to prevent model exploitation [24, 29], online model updates, or through latent factor identification [4, 13\u201316]; and 3) anticipating future changes by forecasting policy gradients or value functions [7, 30, 20, 10, 31]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "185f6e5f0f89f5ad40a76928dd26e5aaff853020",
                "externalIds": {
                    "ArXiv": "2309.14989",
                    "CorpusId": 262825275
                },
                "corpusId": 262825275,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/185f6e5f0f89f5ad40a76928dd26e5aaff853020",
                "title": "Tempo Adaption in Non-stationary Reinforcement Learning",
                "abstract": "We first raise and tackle ``time synchronization'' issue between the agent and the environment in non-stationary reinforcement learning (RL), a crucial factor hindering its real-world applications. In reality, environmental changes occur over wall-clock time ($\\mathfrak{t}$) rather than episode progress ($k$), where wall-clock time signifies the actual elapsed time within the fixed duration $\\mathfrak{t} \\in [0, T]$. In existing works, at episode $k$, the agent rollouts a trajectory and trains a policy before transitioning to episode $k+1$. In the context of the time-desynchronized environment, however, the agent at time $\\mathfrak{t}_k$ allocates $\\Delta \\mathfrak{t}$ for trajectory generation and training, subsequently moves to the next episode at $\\mathfrak{t}_{k+1}=\\mathfrak{t}_{k}+\\Delta \\mathfrak{t}$. Despite a fixed total episode ($K$), the agent accumulates different trajectories influenced by the choice of \\textit{interaction times} ($\\mathfrak{t}_1,\\mathfrak{t}_2,...,\\mathfrak{t}_K$), significantly impacting the sub-optimality gap of policy. We propose a Proactively Synchronizing Tempo (ProST) framework that computes optimal $\\{ \\mathfrak{t}_1,\\mathfrak{t}_2,...,\\mathfrak{t}_K \\} (= \\{ \\mathfrak{t} \\}_{1:K})$. Our main contribution is that we show optimal $\\{ \\mathfrak{t} \\}_{1:K}$ trades-off between the policy training time (agent tempo) and how fast the environment changes (environment tempo). Theoretically, this work establishes an optimal $\\{ \\mathfrak{t} \\}_{1:K}$ as a function of the degree of the environment's non-stationarity while also achieving a sublinear dynamic regret. Our experimental evaluation on various high dimensional non-stationary environments shows that the ProST framework achieves a higher online return at optimal $\\{ \\mathfrak{t} \\}_{1:K}$ than the existing methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110007977",
                        "name": "Hyunin Lee"
                    },
                    {
                        "authorId": "2111069722",
                        "name": "Yuhao Ding"
                    },
                    {
                        "authorId": "2157126659",
                        "name": "Jongmin Lee"
                    },
                    {
                        "authorId": "1455155615",
                        "name": "Ming Jin"
                    },
                    {
                        "authorId": "1688041",
                        "name": "J. Lavaei"
                    },
                    {
                        "authorId": "1685407",
                        "name": "S. Sojoudi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Hence, a handful of Dyna-style methods proposed to simulate multi-step rollouts by using such ensemble models, such as SLBO [28] and MBPO [16].",
                "[16] has been systematically explained that inaccuracies in learned models make long rollouts unreliable due to the compounding error.",
                "A bootstrapped ensemble of dynamics models is constructed to approximate the true transition dynamics of environment: f(st+1|st, at), which has been demonstrated in several studies [5, 20, 16, 36, 43, 44]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cb04d4f99af770921c326486a911755ea8f182f6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-13508",
                    "ArXiv": "2309.13508",
                    "DOI": "10.48550/arXiv.2309.13508",
                    "CorpusId": 262459314
                },
                "corpusId": 262459314,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cb04d4f99af770921c326486a911755ea8f182f6",
                "title": "Guided Cooperation in Hierarchical Reinforcement Learning via Model-based Rollout",
                "abstract": "Goal-conditioned hierarchical reinforcement learning (HRL) presents a promising approach for enabling effective exploration in complex long-horizon reinforcement learning (RL) tasks via temporal abstraction. Yet, most goal-conditioned HRL algorithms focused on the subgoal discovery, regardless of inter-level coupling. In essence, for hierarchical systems, the increased inter-level communication and coordination can induce more stable and robust policy improvement. Here, we present a goal-conditioned HRL framework with Guided Cooperation via Model-based Rollout (GCMR), which estimates forward dynamics to promote inter-level cooperation. The GCMR alleviates the state-transition error within off-policy correction through a model-based rollout, further improving the sample efficiency. Meanwhile, to avoid being disrupted by these corrected but possibly unseen or faraway goals, lower-level Q-function gradients are constrained using a gradient penalty with a model-inferred upper bound, leading to a more stable behavioral policy. Besides, we propose a one-step rollout-based planning to further facilitate inter-level cooperation, where the higher-level Q-function is used to guide the lower-level policy by estimating the value of future states so that global task information is transmitted downwards to avoid local pitfalls. Experimental results demonstrate that incorporating the proposed GCMR framework with ACLG, a disentangled variant of HIGL, yields more stable and robust policy improvement than baselines and substantially outperforms previous state-of-the-art (SOTA) HRL algorithms in both hard-exploration problems and robotic control.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2256769639",
                        "name": "Haoran Wang"
                    },
                    {
                        "authorId": "48186778",
                        "name": "Yaoru Sun"
                    },
                    {
                        "authorId": "7572514",
                        "name": "Fang Wang"
                    },
                    {
                        "authorId": "2195014867",
                        "name": "Ye-Ting Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A key issue in model learning is model bias, which refers to the error between the model and the real environment [19].",
                "Hence, we are inclined to propose a generic algorithm similar to [23, 19, 20] that can be plugged into many SOTA MFRL algorithms [14, 27], rather than just proposing for a specific policy optimization algorithm.",
                "In MFRL, methods such as TRPO [39] and CPI [21] choose to optimize the performance difference bound, whilst most of the previous work in MBRL [29, 19, 50, 36, 23] choose to optimize the difference of expected return under the model and that of the real environment, which is termed return discrepancy.",
                "By minimizing this optimization objective after the model update via maximum likelihood estimation (MLE) [6, 19], we can tune the model to adaptively find appropriate updates to get a performance improvement guarantee.",
                "On the contrary, model-based RL (MBRL) algorithms, using a world model to generate the imaginary rollouts and then taking them for policy optimization [29, 19], have high sample efficiency while achieving similar asymptotic performance, thus becoming a compelling alternative in practical cases [37, 15, 49].",
                "Phase 1 uses traditional MLE loss to train the model, which may impair the performance by excessive model updates due to only considering the impacts of model bias.",
                "In phase 1, the ensemble models are trained on shared but differently shuffled data, where the optimization objective is MLE [6, 19].",
                "We follow the previous work [19] to use the combination of the ensemble model technique with short model rollouts to mitigate the compounding error.",
                "While having achieved comparable results, they only account for model bias in one iteration [19] but do not consider the impacts of model shift between two iterations [20], which can lead to performance deterioration due to excessive model updates.",
                "The performance difference bound can be decomposed into three terms, V \u03c02|M2 \u2212 V \u03c01|M1 = (V \u03c02|M2 \u2212 V \u03c02 M2)\u2212 (V \u03c01|M1 \u2212 V \u03c01 M1) + (V \u03c02 M2 \u2212 V \u03c01 M1) (5) Obviously, compared to directly optimizing the return discrepancy of each iteration [19], the performance difference bound chooses to optimize the return discrepancy of two adjacent iterations, namely V \u03c02|M2 \u2212 V \u03c02 M2 and V \u03c01|M1 \u2212 V \u03c01 M1 respectively, and the expected return variation between these two iterations, namely V \u03c02 M2 \u2212 V \u03c01 M1 , demonstrating better rigorousness.",
                "The uncertainty estimation techniques are used to adjust the rollout length [19, 28] or the transition weight [18, 36].",
                "Many prior methods [29, 19, 50, 36, 23] rely on return discrepancy to obtain model updates with a performance improvement guarantee."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "2d65c4094a3375585318bfb0178b587d7bffc0fd",
                "externalIds": {
                    "ArXiv": "2309.12671",
                    "DBLP": "journals/corr/abs-2309-12671",
                    "DOI": "10.48550/arXiv.2309.12671",
                    "CorpusId": 262216939
                },
                "corpusId": 262216939,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2d65c4094a3375585318bfb0178b587d7bffc0fd",
                "title": "How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization",
                "abstract": "Designing and deriving effective model-based reinforcement learning (MBRL) algorithms with a performance improvement guarantee is challenging, mainly attributed to the high coupling between model learning and policy optimization. Many prior methods that rely on return discrepancy to guide model learning ignore the impacts of model shift, which can lead to performance deterioration due to excessive model updates. Other methods use performance difference bound to explicitly consider model shift. However, these methods rely on a fixed threshold to constrain model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. In this paper, we theoretically derive an optimization objective that can unify model shift and model bias and then formulate a fine-tuning process. This process adaptively adjusts the model updates to get a performance improvement guarantee while avoiding model overfitting. Based on these, we develop a straightforward algorithm USB-PO (Unified model Shift and model Bias Policy Optimization). Empirical results show that USB-PO achieves state-of-the-art performance on several challenging benchmark tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220559577",
                        "name": "Hai Zhang"
                    },
                    {
                        "authorId": "2245270655",
                        "name": "Hang Yu"
                    },
                    {
                        "authorId": "2245189042",
                        "name": "Junqiao Zhao"
                    },
                    {
                        "authorId": "2220688800",
                        "name": "Di Zhang"
                    },
                    {
                        "authorId": "2244857519",
                        "name": "Chang Huang"
                    },
                    {
                        "authorId": "113866137",
                        "name": "Hongtu Zhou"
                    },
                    {
                        "authorId": "2245317632",
                        "name": "Xiao Zhang"
                    },
                    {
                        "authorId": "2064448848",
                        "name": "Chen Ye"
                    }
                ]
            }
        },
        {
            "contexts": [
                "An ensemble of probabilistic networks [97] was utilized by the MBPO method [98] to deal with the two sources of the dynamics model error."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "dc96ff6357ce0fa8b231b3bb7db1197577b91aba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-12560",
                    "ArXiv": "2309.12560",
                    "DOI": "10.48550/arXiv.2309.12560",
                    "CorpusId": 262217529
                },
                "corpusId": 262217529,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dc96ff6357ce0fa8b231b3bb7db1197577b91aba",
                "title": "Machine Learning Meets Advanced Robotic Manipulation",
                "abstract": "Automated industries lead to high quality production, lower manufacturing cost and better utilization of human resources. Robotic manipulator arms have major role in the automation process. However, for complex manipulation tasks, hard coding efficient and safe trajectories is challenging and time consuming. Machine learning methods have the potential to learn such controllers based on expert demonstrations. Despite promising advances, better approaches must be developed to improve safety, reliability, and efficiency of ML methods in both training and deployment phases. This survey aims to review cutting edge technologies and recent trends on ML methods applied to real-world manipulation tasks. After reviewing the related background on ML, the rest of the paper is devoted to ML applications in different domains such as industry, healthcare, agriculture, space, military, and search and rescue. The paper is closed with important research directions for future works.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2154153866",
                        "name": "Saeid Nahavandi"
                    },
                    {
                        "authorId": "150271813",
                        "name": "R. Alizadehsani"
                    },
                    {
                        "authorId": "9013739",
                        "name": "D. Nahavandi"
                    },
                    {
                        "authorId": "2244667310",
                        "name": "Chee Peng Lim"
                    },
                    {
                        "authorId": "2244624038",
                        "name": "Kevin Kelly"
                    },
                    {
                        "authorId": "2244623700",
                        "name": "Fernando Bello"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Overall, previous works [17], [19], [20] have suggested using deep neural networks to overcome the computational burden of probabilistic MBRL and left the following issues unaddressed: 1) the uncertainty propagation based on neural networks is unstable; 2) the fitting error of neural networks is less considered; 3) the aleatoric and epistemic uncertainties are not distinguished during propagation.",
                "We selected Deep Pilco [17], PETS [19], MBPO [20] as the MBRL baselines, and selected SAC [23], PPO [24], DDPG [25] as the model-free RL baselines2.",
                "Approach MPC DNN Fitting Error Distinguished Uncertainties PILCO [10] \u00d7 \u00d7 N/A \u00d7 GP-MPC [14] \u20dd \u00d7 N/A \u00d7 Deep Pilco [17] \u00d7 \u20dd \u00d7 \u00d7 PETS [19] \u20dd \u20dd \u00d7 \u00d7 MBPO [20] \u00d7 \u20dd \u00d7 \u00d7 DPETS (ours) \u20dd \u20dd \u20dd \u20dd",
                "propagation by generating enhanced data from the predictive model [20]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5eceb4d435f1eb86d9b211ffc1f76c87f79aa2c8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-11089",
                    "ArXiv": "2309.11089",
                    "DOI": "10.48550/arXiv.2309.11089",
                    "CorpusId": 262063657
                },
                "corpusId": 262063657,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5eceb4d435f1eb86d9b211ffc1f76c87f79aa2c8",
                "title": "Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling",
                "abstract": "This paper addresses the prediction stability, prediction accuracy and control capability of the current probabilistic model-based reinforcement learning (MBRL) built on neural networks. A novel approach dropout-based probabilistic ensembles with trajectory sampling (DPETS) is proposed where the system uncertainty is stably predicted by combining the Monte-Carlo dropout and trajectory sampling in one framework. Its loss function is designed to correct the fitting error of neural networks for more accurate prediction of probabilistic models. The state propagation in its policy is extended to filter the aleatoric uncertainty for superior control capability. Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency. The open source code of DPETS is available at https://github.com/mrjun123/DPETS.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243567771",
                        "name": "Wenjun Huang"
                    },
                    {
                        "authorId": "1824476",
                        "name": "Yunduan Cui"
                    },
                    {
                        "authorId": "2243031546",
                        "name": "Huiyun Li"
                    },
                    {
                        "authorId": "145739804",
                        "name": "Xin Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1435d929d76656ef2ef12164bd1091594a3c39f6",
                "externalIds": {
                    "DBLP": "conf/nips/ChenYLLQSY21",
                    "DOI": "10.1109/TPAMI.2023.3317131",
                    "CorpusId": 248225952,
                    "PubMed": "37725727"
                },
                "corpusId": 248225952,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1435d929d76656ef2ef12164bd1091594a3c39f6",
                "title": "Offline Model-based Adaptable Policy Learning",
                "abstract": "In reinforcement learning, a promising direction to avoid online trial-and-error costs is learning from an offline dataset. Current offline reinforcement learning methods commonly learn in the policy space constrained to in-support regions by the offline dataset, in order to ensure the robustness of the outcome policies. Such constraints, however, also limit the potential of the outcome policies. In this paper, to release the potential of offline policy learning, we investigate the decision-making problems in out-of-support regions directly and propose offline Model-based Adaptable Policy LEarning (MAPLE). By this approach, instead of learning in in-support regions, we learn an adaptable policy that can adapt its behavior in out-of-support regions when deployed. We give a practical implementation of MAPLE via meta-learning techniques and ensemble model learning techniques. We conduct experiments on MuJoCo locomotion tasks with offline datasets. The results show that the proposed method can make robust decisions in out-of-support regions and achieve better performance than SOTA algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108968873",
                        "name": "Xiong-Hui Chen"
                    },
                    {
                        "authorId": "144705629",
                        "name": "Yang Yu"
                    },
                    {
                        "authorId": "1895668",
                        "name": "Qingyang Li"
                    },
                    {
                        "authorId": "2072689683",
                        "name": "Fan Luo"
                    },
                    {
                        "authorId": "144559512",
                        "name": "Zhiwei Qin"
                    },
                    {
                        "authorId": "2066300496",
                        "name": "Wenjie Shang"
                    },
                    {
                        "authorId": "2778556",
                        "name": "Jieping Ye"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[23] employed a model ensemble approach to reduce biases introduced by probabilistic networks."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "481cda5c60cd9395ca291d868a1e18487ca593f3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-08925",
                    "ArXiv": "2309.08925",
                    "DOI": "10.48550/arXiv.2309.08925",
                    "CorpusId": 261884865
                },
                "corpusId": 261884865,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/481cda5c60cd9395ca291d868a1e18487ca593f3",
                "title": "DOMAIN: MilDly COnservative Model-BAsed OfflINe Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (RL), which learns environment model from offline dataset and generates more out-of-distribution model data, has become an effective approach to the problem of distribution shift in offline RL. Due to the gap between the learned and actual environment, conservatism should be incorporated into the algorithm to balance accurate offline data and imprecise model data. The conservatism of current algorithms mostly relies on model uncertainty estimation. However, uncertainty estimation is unreliable and leads to poor performance in certain scenarios, and the previous methods ignore differences between the model data, which brings great conservatism. Therefore, this paper proposes a milDly cOnservative Model-bAsed offlINe RL algorithm (DOMAIN) without estimating model uncertainty to address the above issues. DOMAIN introduces adaptive sampling distribution of model samples, which can adaptively adjust the model data penalty. In this paper, we theoretically demonstrate that the Q value learned by the DOMAIN outside the region is a lower bound of the true Q value, the DOMAIN is less conservative than previous model-based offline RL algorithms and has the guarantee of security policy improvement. The results of extensive experiments show that DOMAIN outperforms prior RL algorithms on the D4RL dataset benchmark, and achieves better performance than other RL algorithms on tasks that require generalization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2241377930",
                        "name": "Xiao-Yin Liu"
                    },
                    {
                        "authorId": "9302892",
                        "name": "Xiao-Hu Zhou"
                    },
                    {
                        "authorId": "7893337",
                        "name": "Xiaoliang Xie"
                    },
                    {
                        "authorId": "50857142",
                        "name": "Shiqi Liu"
                    },
                    {
                        "authorId": "2525427",
                        "name": "Zhen-Qiu Feng"
                    },
                    {
                        "authorId": "2156011080",
                        "name": "Hao Li"
                    },
                    {
                        "authorId": "1935720124",
                        "name": "Mei-Jiang Gui"
                    },
                    {
                        "authorId": "39575087",
                        "name": "Tian-Yu Xiang"
                    },
                    {
                        "authorId": "2202060443",
                        "name": "De-Xing Huang"
                    },
                    {
                        "authorId": "2241350912",
                        "name": "Zeng-Guang Hou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This has been extensively studied in both online and offline model-based RL literature [37, 38, 39, 40].",
                "For both algorithms, we solve the inner problem using Model-Based Policy Optimization (MBPO; [39]) which uses Soft Actor-Critic (SAC; [26]) in a dynamics model ensemble."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "615e827e9bfea05caa31b656f62af39b3c6e9a9d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-08571",
                    "ArXiv": "2309.08571",
                    "DOI": "10.48550/arXiv.2309.08571",
                    "CorpusId": 262013429
                },
                "corpusId": 262013429,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/615e827e9bfea05caa31b656f62af39b3c6e9a9d",
                "title": "A Bayesian Approach to Robust Inverse Reinforcement Learning",
                "abstract": "We consider a Bayesian approach to offline model-based inverse reinforcement learning (IRL). The proposed framework differs from existing offline model-based IRL approaches by performing simultaneous estimation of the expert's reward function and subjective model of environment dynamics. We make use of a class of prior distributions which parameterizes how accurate the expert's model of the environment is to develop efficient algorithms to estimate the expert's reward and subjective dynamics in high-dimensional settings. Our analysis reveals a novel insight that the estimated policy exhibits robust performance when the expert is believed (a priori) to have a highly accurate model of the environment. We verify this observation in the MuJoCo environments and show that our algorithms outperform state-of-the-art offline IRL algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145064542",
                        "name": "Ran Wei"
                    },
                    {
                        "authorId": "46975510",
                        "name": "Siliang Zeng"
                    },
                    {
                        "authorId": "143971529",
                        "name": "Chenliang Li"
                    },
                    {
                        "authorId": "2140833946",
                        "name": "Alfredo Garcia"
                    },
                    {
                        "authorId": "2143389347",
                        "name": "A. McDonald"
                    },
                    {
                        "authorId": "2243026280",
                        "name": "Mingyi Hong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The MBRL baselines are MBPO: [33] with STL accuracy as the reward, PETS: [34] with a hand-crafted reward, and CEM: Cross Entropy Method [35] with STL robustness reward."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f875fb3d3cebe72640e34ff95b0a35625b0dc539",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-05131",
                    "ArXiv": "2309.05131",
                    "DOI": "10.48550/arXiv.2309.05131",
                    "CorpusId": 261682174
                },
                "corpusId": 261682174,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f875fb3d3cebe72640e34ff95b0a35625b0dc539",
                "title": "Signal Temporal Logic Neural Predictive Control",
                "abstract": "Ensuring safety and meeting temporal specifications are critical challenges for long-term robotic tasks. Signal temporal logic (STL) has been widely used to systematically and rigorously specify these requirements. However, traditional methods of finding the control policy under those STL requirements are computationally complex and not scalable to high-dimensional or systems with complex nonlinear dynamics. Reinforcement learning (RL) methods can learn the policy to satisfy the STL specifications via hand-crafted or STL-inspired rewards, but might encounter unexpected behaviors due to ambiguity and sparsity in the reward. In this paper, we propose a method to directly learn a neural network controller to satisfy the requirements specified in STL. Our controller learns to roll out trajectories to maximize the STL robustness score in training. In testing, similar to Model Predictive Control (MPC), the learned controller predicts a trajectory within a planning horizon to ensure the satisfaction of the STL requirement in deployment. A backup policy is designed to ensure safety when our controller fails. Our approach can adapt to various initial conditions and environmental parameters. We conduct experiments on six tasks, where our method with the backup policy outperforms the classical methods (MPC, STL-solver), model-free and model-based RL methods in STL satisfaction rate, especially on tasks with complex STL specifications while being 10X-100X faster than the classical methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144098758",
                        "name": "Yue Meng"
                    },
                    {
                        "authorId": "2239052395",
                        "name": "Chuchu Fan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This reduces the interactions with the real environment and improves sample efficiency (Xu et al. 2018; Janner et al. 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "37e0a9126e15c9c77dbdc334762275020f0c3217",
                "externalIds": {
                    "ArXiv": "2309.04615",
                    "DBLP": "journals/corr/abs-2309-04615",
                    "DOI": "10.48550/arXiv.2309.04615",
                    "CorpusId": 261682604
                },
                "corpusId": 261682604,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/37e0a9126e15c9c77dbdc334762275020f0c3217",
                "title": "Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning",
                "abstract": "In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. We present experimental results in Easy, Hard, and Super-Hard StarCraft II micro-management challenges to demonstrate that our method achieves high sample efficiency and exhibits superior performance in defeating the enemy armies compared to other baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2239056283",
                        "name": "Zhizun Wang"
                    },
                    {
                        "authorId": "2462512",
                        "name": "D. Meger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Reinforcement learning provides an alternative way that allows robots to learn a robust policy through trial and error including either model-based method [24], modelfree method [25] or their combination [26], and the learned policy generalizes well under task uncertainties."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "87d47acf8c6213c8f4872797af88a9b9952654c6",
                "externalIds": {
                    "DBLP": "journals/ral/WangZS23",
                    "DOI": "10.1109/LRA.2023.3300238",
                    "CorpusId": 260410697
                },
                "corpusId": 260410697,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/87d47acf8c6213c8f4872797af88a9b9952654c6",
                "title": "Learning Robotic Insertion Tasks From Human Demonstration",
                "abstract": "Robotic insertion tasks often rely on delicate manual tuning due to the complexity of contact dynamics. In contrast, human is remarkably efficient in these tasks. In this context, Programming by Demonstration (PbD) has gained much traction since it shows the possibility for robots to learn new skills by observing human demonstration. However, existing PbD approaches suffer from the high cost of demonstration data collection, and low robustness to task uncertainties. In order to address these issues, we propose a new PbD-based learning framework for robotic insertion tasks. This framework includes a new demonstration data acquisition system, which replaces the expensive motion capture device with deep learning based hand pose tracking algorithm and a low-cost RGBD camera. A latent skill-guided reinforcement learning (RL) approach is also included for safe, efficient, and robust human-robot skill transfer, in which risky explorations are prevented by the reward function design and safety constraints in action space. A series of peg-hole-insertion experiments on a FANUC industrial robot are conducted to illustrate the effectiveness of the proposed approach.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "30321248",
                        "name": "Kaimeng Wang"
                    },
                    {
                        "authorId": "2226684742",
                        "name": "Yu Zhao"
                    },
                    {
                        "authorId": "1707826",
                        "name": "I. Sakuma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Though model-based RL began with low-dimensional, compact state spaces [26, 37, 27, 57], advances in visual model-based reinforcement learning [17, 19, 18, 44, 42, 21] learn"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a53fc030584797adaae398a95b7a38b57e8af2be",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-00082",
                    "ArXiv": "2309.00082",
                    "DOI": "10.48550/arXiv.2309.00082",
                    "CorpusId": 261493781
                },
                "corpusId": 261493781,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a53fc030584797adaae398a95b7a38b57e8af2be",
                "title": "RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability",
                "abstract": "Visual model-based RL methods typically encode image observations into low-dimensional representations in a manner that does not eliminate redundant information. This leaves them susceptible to spurious variations -- changes in task-irrelevant components such as background distractors or lighting conditions. In this paper, we propose a visual model-based RL method that learns a latent representation resilient to such spurious variations. Our training objective encourages the representation to be maximally predictive of dynamics and reward, while constraining the information flow from the observation to the latent representation. We demonstrate that this objective significantly bolsters the resilience of visual model-based RL methods to visual distractors, allowing them to operate in dynamic environments. We then show that while the learned encoder is resilient to spirious variations, it is not invariant under significant distribution shift. To address this, we propose a simple reward-free alignment procedure that enables test time adaptation of the encoder. This allows for quick adaptation to widely differing environments without having to relearn the dynamics and policy. Our effort is a step towards making model-based RL a practical and useful tool for dynamic, diverse domains. We show its effectiveness in simulation benchmarks with significant spurious variations as well as a real-world egocentric navigation task with noisy TVs in the background. Videos and code at https://zchuning.github.io/repo-website/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118160513",
                        "name": "Chuning Zhu"
                    },
                    {
                        "authorId": "2237422766",
                        "name": "Max Simchowitz"
                    },
                    {
                        "authorId": "1646669139",
                        "name": "Siri Gadipudi"
                    },
                    {
                        "authorId": "144150274",
                        "name": "Abhishek Gupta"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7f8c2978525bdeb10b60529fab51ef69f03dc3ed",
                "externalIds": {
                    "ArXiv": "2308.13654",
                    "DBLP": "journals/corr/abs-2308-13654",
                    "DOI": "10.1007/s11538-023-01198-5",
                    "CorpusId": 261243449,
                    "PubMed": "37665428"
                },
                "corpusId": 261243449,
                "publicationVenue": {
                    "id": "8a47ce78-e793-4315-8bf8-b09466ebf633",
                    "name": "Bulletin of Mathematical Biology",
                    "type": "journal",
                    "alternate_names": [
                        "Bull Math Biology"
                    ],
                    "issn": "0092-8240",
                    "url": "http://link.springer.com/journal/11538"
                },
                "url": "https://www.semanticscholar.org/paper/7f8c2978525bdeb10b60529fab51ef69f03dc3ed",
                "title": "Pretty Darn Good Control: When are Approximate Solutions Better than Approximate Models",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1422033452",
                        "name": "Felipe Montealegre-Mora"
                    },
                    {
                        "authorId": "2112209593",
                        "name": "Marcus Lapeyrolerie"
                    },
                    {
                        "authorId": "50774682",
                        "name": "Melissa S. Chapman"
                    },
                    {
                        "authorId": "2052073088",
                        "name": "Abigail G. Keller"
                    },
                    {
                        "authorId": "72997413",
                        "name": "C. Boettiger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We adopt the baseline architecture from MBPO (Janner et al., 2019) and the implementation from Pineda et al. (2021), where the posterior MDP, denoted \u0393\u03c8, is represented as an ensemble of n neural networks trained via supervised learning on the environment dataset D to predict the mean and variance\u2026",
                "We adopt the baseline architecture from MBPO (Janner et al., 2019) and the implementation from Pineda et al.",
                "MBPO with slight modifications from Janner et al. (2019): (1) it only uses Dmodel to update the actor and critic, rather than mixing in data from D; (2) it uses a fixed rollout length k, instead of an adaptive scheme."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "28aa4452c63da832a3e185475285044ba1496391",
                "externalIds": {
                    "ArXiv": "2308.06590",
                    "DBLP": "journals/corr/abs-2308-06590",
                    "DOI": "10.48550/arXiv.2308.06590",
                    "CorpusId": 260886933
                },
                "corpusId": 260886933,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/28aa4452c63da832a3e185475285044ba1496391",
                "title": "Value-Distributional Model-Based Reinforcement Learning",
                "abstract": "Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2470985",
                        "name": "Carlos E. Luis"
                    },
                    {
                        "authorId": "47846973",
                        "name": "A. Bottero"
                    },
                    {
                        "authorId": "3428828",
                        "name": "Julia Vinogradska"
                    },
                    {
                        "authorId": "2141578581",
                        "name": "Felix Berkenkamp"
                    },
                    {
                        "authorId": "2107720654",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Next, we use MOReL [12] as a representative of a general MBPO approach that covers both a classical (na\u00efve) MBRL and a Pessimistic MDP-based MBRL.",
                "3.2 MDP-based Learning A contrastive approach to offline RL is to derive an MDP from the data and solve it either optimally or approximately with a model-based policy optimization (MBPO) [10].",
                "A contrastive approach to offline RL is to derive an MDP from the data and solve it either optimally or approximately with a model-based policy optimization (MBPO) [10]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4874a7e4673f27e3eceb8afa34339c364fe7cc8f",
                "externalIds": {
                    "DBLP": "conf/kdd/KunjirCCJR23",
                    "DOI": "10.1145/3580305.3599459",
                    "CorpusId": 260499439
                },
                "corpusId": 260499439,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/4874a7e4673f27e3eceb8afa34339c364fe7cc8f",
                "title": "Optimizing Traffic Control with Model-Based Learning: A Pessimistic Approach to Data-Efficient Policy Inference",
                "abstract": "Traffic signal control is an important problem in urban mobility with a significant potential for economic and environmental impact. While there is a growing interest in Reinforcement Learning (RL) for traffic signal control, the work so far has focussed on learning through simulations which could lead to inaccuracies due to simplifying assumptions. Instead, real experience data on traffic is available and could be exploited at minimal costs. Recent progress in offline or batch RL has enabled just that. Model-based offline RL methods, in particular, have been shown to generalize from the experience data much better than others. We build a model-based learning framework that infers a Markov Decision Process (MDP) from a dataset collected using a cyclic traffic signal control policy that is both commonplace and easy to gather. The MDP is built with pessimistic costs to manage out-of-distribution scenarios using an adaptive shaping of rewards which is shown to provide better regularization compared to the prior related work in addition to being PAC-optimal. Our model is evaluated on a complex signalized roundabout and a large multi-intersection environment, demonstrating that highly performant traffic control policies can be built in a data-efficient manner.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3094459",
                        "name": "Mayuresh Kunjir"
                    },
                    {
                        "authorId": "2180253530",
                        "name": "Sanjay Chawla"
                    },
                    {
                        "authorId": "2221323707",
                        "name": "Siddarth Chandrasekar"
                    },
                    {
                        "authorId": "38310189",
                        "name": "Devika Jay"
                    },
                    {
                        "authorId": "1723632",
                        "name": "Balaraman Ravindran"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5f4d9c40ad5acbfbdf134ca3fcb0aa538695681c",
                "externalIds": {
                    "DBLP": "journals/eaai/Li0D23",
                    "DOI": "10.1016/j.engappai.2023.106300",
                    "CorpusId": 258359937
                },
                "corpusId": 258359937,
                "publicationVenue": {
                    "id": "1a24ea21-4c37-41d8-9e76-ab802d4afb3e",
                    "name": "Engineering applications of artificial intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Eng appl artif intell",
                        "Eng Appl Artif Intell",
                        "Engineering Applications of Artificial Intelligence"
                    ],
                    "issn": "0952-1976",
                    "url": "http://www.sciencedirect.com/science/journal/09521976"
                },
                "url": "https://www.semanticscholar.org/paper/5f4d9c40ad5acbfbdf134ca3fcb0aa538695681c",
                "title": "Meta-GNAS: Meta-reinforcement learning for graph neural architecture search",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": null,
                        "name": "YuFei Li"
                    },
                    {
                        "authorId": "153171583",
                        "name": "Jia Wu"
                    },
                    {
                        "authorId": "2187019852",
                        "name": "TianJin Deng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026Kumar et al., 2019b; Zhang et al., 2021; Kostrikov et al., 2021a); ii) enforcing conservative estimates of future rewards (Kumar et al., 2020; Yu et al., 2021; Cheng et al., 2022); and iii) model-based methods that estimate the uncertainty via ensembles (Janner et al., 2019; Kidambi et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d91591feb96936237167c9e569b8f74e0b2bfcc3",
                "externalIds": {
                    "DBLP": "conf/iclr/GurtlerBKWWBSM23",
                    "ArXiv": "2307.15690",
                    "DOI": "10.48550/arXiv.2307.15690",
                    "CorpusId": 259298732
                },
                "corpusId": 259298732,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d91591feb96936237167c9e569b8f74e0b2bfcc3",
                "title": "Benchmarking Offline Reinforcement Learning on Real-Robot Hardware",
                "abstract": "Learning policies from previously recorded data is a promising direction for real-world robotics tasks, as online learning is often infeasible. Dexterous manipulation in particular remains an open problem in its general form. The combination of offline reinforcement learning with large diverse datasets, however, has the potential to lead to a breakthrough in this challenging domain analogously to the rapid progress made in supervised learning in recent years. To coordinate the efforts of the research community toward tackling this problem, we propose a benchmark including: i) a large collection of data for offline learning from a dexterous manipulation platform on two tasks, obtained with capable RL agents trained in simulation; ii) the option to execute learned policies on a real-world robotic system and a simulation for efficient debugging. We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets and provide a reproducible experimental setup for offline reinforcement learning on real systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50057572",
                        "name": "Nico G\u00fcrtler"
                    },
                    {
                        "authorId": "46773173",
                        "name": "Sebastian Blaes"
                    },
                    {
                        "authorId": "2662064",
                        "name": "Pavel Kolev"
                    },
                    {
                        "authorId": "47804478",
                        "name": "F. Widmaier"
                    },
                    {
                        "authorId": "36661824",
                        "name": "Manuel W\u00fcthrich"
                    },
                    {
                        "authorId": "153125952",
                        "name": "Stefan Bauer"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    },
                    {
                        "authorId": "144247521",
                        "name": "G. Martius"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based policy optimisation (MBPO) [27] is a sample-efficient neural architecture for optimising policies in learned dynamics model.",
                "Furthermore, model-based approaches to safe RL have gained increasing traction, in part due to recent significant developments in model-based RL (MBRL) [18, 21, 22] and the superior sample complexity of model-based approaches [19, 27].",
                "Model-based RL as a paradigm for learning complex policies has become increasingly popular in recent years due to its superior sample efficiency [19, 27]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a03e5a49c0485f66b51997763075be20ad174e84",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-00707",
                    "ArXiv": "2308.00707",
                    "DOI": "10.48550/arXiv.2308.00707",
                    "CorpusId": 260379003
                },
                "corpusId": 260379003,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a03e5a49c0485f66b51997763075be20ad174e84",
                "title": "Approximate Model-Based Shielding for Safe Reinforcement Learning",
                "abstract": "Reinforcement learning (RL) has shown great potential for solving complex tasks in a variety of domains. However, applying RL to safety-critical systems in the real-world is not easy as many algorithms are sample-inefficient and maximising the standard RL objective comes with no guarantees on worst-case performance. In this paper we propose approximate model-based shielding (AMBS), a principled look-ahead shielding algorithm for verifying the performance of learned RL policies w.r.t. a set of given safety constraints. Our algorithm differs from other shielding approaches in that it does not require prior knowledge of the safety-relevant dynamics of the system. We provide a strong theoretical justification for AMBS and demonstrate superior performance to other safety-aware approaches on a set of Atari games with state-dependent safety-labels.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2206167880",
                        "name": "Alexander W. Goodall"
                    },
                    {
                        "authorId": "3142000",
                        "name": "F. Belardinelli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, training a policy using an inaccurately estimated model can be harmful (Janner et al., 2019).",
                "However, an inaccurately estimated model for unobserved state-action pairs in D can lead to poor performance of the learned policy (Janner et al., 2019).",
                "Following previous works (Janner et al., 2019; Yu et al., 2020; 2021b; Rigter et al., 2022), we train an ensemble of 7 such models that each contain the dynamics model and autoencoder and pick the best 5 models based on the validation prediction error on a held-out test set of 1000 transitions from\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "1431a7ac7be90c585b7dc84be328802f85b194a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-11352",
                    "ArXiv": "2307.11352",
                    "DOI": "10.48550/arXiv.2307.11352",
                    "CorpusId": 260091653
                },
                "corpusId": 260091653,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1431a7ac7be90c585b7dc84be328802f85b194a3",
                "title": "Model-based Offline Reinforcement Learning with Count-based Conservatism",
                "abstract": "In this paper, we propose a model-based offline reinforcement learning method that integrates count-based conservatism, named $\\texttt{Count-MORL}$. Our method utilizes the count estimates of state-action pairs to quantify model estimation error, marking the first algorithm of demonstrating the efficacy of count-based conservatism in model-based offline deep RL to the best of our knowledge. For our proposed method, we first show that the estimation error is inversely proportional to the frequency of state-action pairs. Secondly, we demonstrate that the learned policy under the count-based conservative model offers near-optimality performance guarantees. Through extensive numerical experiments, we validate that $\\texttt{Count-MORL}$ with hash code implementation significantly outperforms existing offline RL algorithms on the D4RL benchmark datasets. The code is accessible at $\\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3231991",
                        "name": "Byeongchang Kim"
                    },
                    {
                        "authorId": "2683817",
                        "name": "Min-hwan Oh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, model-based RL involves learning a model of the world (Racani\u00e8re et al., 2017; Hafner et al., 2019; Janner et al., 2019; Schrittwieser et al., 2020) while most model-free policy gradient methods train a value or Q-network to control the variance of the gradient update (Mnih et al.,\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5185a88711818f11633bae71f15902278885e57d",
                "externalIds": {
                    "ArXiv": "2307.10936",
                    "DBLP": "journals/corr/abs-2307-10936",
                    "DOI": "10.48550/arXiv.2307.10936",
                    "CorpusId": 259991747
                },
                "corpusId": 259991747,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5185a88711818f11633bae71f15902278885e57d",
                "title": "PASTA: Pretrained Action-State Transformer Agents",
                "abstract": "Self-supervised learning has brought about a revolutionary paradigm shift in various computing domains, including NLP, vision, and biology. Recent approaches involve pre-training transformer models on vast amounts of unlabeled data, serving as a starting point for efficiently solving downstream tasks. In the realm of reinforcement learning, researchers have recently adapted these approaches by developing models pre-trained on expert trajectories, enabling them to address a wide range of tasks, from robotics to recommendation systems. However, existing methods mostly rely on intricate pre-training objectives tailored to specific downstream applications. This paper presents a comprehensive investigation of models we refer to as Pretrained Action-State Transformer Agents (PASTA). Our study uses a unified methodology and covers an extensive set of general downstream tasks including behavioral cloning, offline RL, sensor failure robustness, and dynamics change adaptation. Our goal is to systematically compare various design choices and provide valuable insights to practitioners for building robust models. Key highlights of our study include tokenization at the action and state component level, using fundamental pre-training objectives like next token prediction, training models across diverse domains simultaneously, and using parameter efficient fine-tuning (PEFT). The developed models in our study contain fewer than 10 million parameters and the application of PEFT enables fine-tuning of fewer than 10,000 parameters during downstream adaptation, allowing a broad community to use these models and reproduce our experiments. We hope that this study will encourage further research into the use of transformers with first-principles design choices to represent RL trajectories and contribute to robust policy learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2187208083",
                        "name": "Raphael Boige"
                    },
                    {
                        "authorId": "1412725415",
                        "name": "Yannis Flet-Berliac"
                    },
                    {
                        "authorId": "2914172",
                        "name": "Arthur Flajolet"
                    },
                    {
                        "authorId": "2060044611",
                        "name": "Guillaume Richard"
                    },
                    {
                        "authorId": "84011266",
                        "name": "Thomas Pierrot"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2012) Hopper implementation using the Model-Based Policy Optimization (MBPO) algorithm (Janner et al., 2019)10 that learns an ensemble of dynamics models, each predicting the parameters of a multivariate Gaussian distribution over the next-step observation and reward given the current state and performed action.",
                "We trained an agent in the MuJoCo (Todorov et al., 2012) Hopper implementation using the Model-Based Policy Optimization (MBPO) algorithm (Janner et al., 2019)10 that learns an ensemble of dynamics models, each predicting the parameters of a multivariate Gaussian distribution over the next-step\u2026",
                "This is the statistical uncertainty representative of the inherent system stochasticity, i.e., the unknowns that differ each time the same experiment is run (Chua et al., 2018; Janner et al., 2019).",
                "We follow approaches in the model-based RL literature (Chua et al., 2018; Janner et al., 2019; Shyam et al., 2019; Pathak et al., 2019) where epistemic uncertainty is measured by estimating the level of disagreement between different predictive models, forming an ensemble, that are trained independently, usually by random sub-sampling of a common replay buffer.",
                "We follow approaches in the model-based RL literature (Chua et al., 2018; Janner et al., 2019; Shyam et al., 2019; Pathak et al., 2019) where epistemic uncertainty is measured by estimating the level of disagreement between different predictive models, forming an ensemble, that are trained\u2026",
                ", the unknowns that differ each time the same experiment is run (Chua et al., 2018; Janner et al., 2019).",
                "\u2026or an action-value function, Q(s, a), asserting the value of executing some action given a state;\nModel-based approaches (e.g., Chua et al., 2018; Janner et al., 2019) that learn a model of the environment\u2019s dynamics, P(s\u2032, r|s, a), i.e., a function mapping from observations and actions to\u2026",
                "Model-based approaches (e.g., Chua et al., 2018; Janner et al., 2019) that learn a model of the environment\u2019s dynamics, P(s\u2032, r|s, a), i."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "66a3998f98b1880010b73cc51a006392ca030d40",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-08933",
                    "ArXiv": "2307.08933",
                    "DOI": "10.48550/arXiv.2307.08933",
                    "CorpusId": 259950869
                },
                "corpusId": 259950869,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/66a3998f98b1880010b73cc51a006392ca030d40",
                "title": "IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness",
                "abstract": "In recent years, advances in deep learning have resulted in a plethora of successes in the use of reinforcement learning (RL) to solve complex sequential decision tasks with high-dimensional inputs. However, existing systems lack the necessary mechanisms to provide humans with a holistic view of their competence, presenting an impediment to their adoption, particularly in critical applications where the decisions an agent makes can have significant consequences. Yet, existing RL-based systems are essentially competency-unaware in that they lack the necessary interpretation mechanisms to allow human operators to have an insightful, holistic view of their competency. Towards more explainable Deep RL (xDRL), we propose a new framework based on analyses of interestingness. Our tool provides various measures of RL agent competence stemming from interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit. We showcase the use of our framework by applying the proposed pipeline in a set of scenarios of varying complexity. We empirically assess the capability of the approach in identifying agent behavior patterns and competency-controlling conditions, and the task elements mostly responsible for an agent's competence, based on global and local analyses of interestingness. Overall, we show that our framework can provide agent designers with insights about RL agent competence, both their capabilities and limitations, enabling more informed decisions about interventions, additional training, and other interactions in collaborative human-machine settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50196279",
                        "name": "Pedro Sequeira"
                    },
                    {
                        "authorId": "2720925",
                        "name": "M. Gervasio"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The experimental results show that the proposed method can achieve higher sample ef\ufb01ciency than previous works such as vanilla HER [14], MBPO [11] and MHER [18].",
                "Based on the [10], the work in [11], [12] avoided the compounding error by generating short branched roll outs from real states, which is also used in our method.",
                "To answer the \ufb01rst question, the following methods are compared with the GMRL. 1) MBRL, a standard model-based reinforcement learning which is similar to the MBPO [11] but uses a single model.",
                "1) MBRL, a standard modelbased reinforcement learning which is similar to the MBPO [11] but uses a single model.",
                "The experimental results show that the proposed method can achieve higher sample efficiency than previous works such as vanilla HER [14], MBPO [11] and MHER [18]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "74a2b64af133c30609aae1c64ba31ee2f0f02418",
                "externalIds": {
                    "DBLP": "conf/rcar/DongLWFLCSZS23",
                    "DOI": "10.1109/RCAR58764.2023.10250060",
                    "CorpusId": 262075907
                },
                "corpusId": 262075907,
                "publicationVenue": {
                    "id": "646f127e-53b1-40c6-8ed6-7a251df5de0b",
                    "name": "International Conference on Real-time Computing and Robotics",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Real-time Comput Robot",
                        "RCAR",
                        "IEEE International Conference on Real-time Computing and Robotics",
                        "Int Conf Real-time Comput Robot"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/74a2b64af133c30609aae1c64ba31ee2f0f02418",
                "title": "Reinforcement Learning with Goal Relabeling and Dynamic Model for Robotic Tasks",
                "abstract": "Improving sample efficiency is crucial for reinforcement learning, especially for the robot manipulation tasks. The model-based method could improve the sample efficiency by introducing a dynamic model to generate a large number of samples, so that the requirement of interaction with the environment can be reduced. However, as the dynamic model is constructed by the deep network, the model error is inevitable. This error will increase the errors of the data generated by the model, which may damage the policy training process to a certain extent. To overcome the limitations of the above data augmentation methods, this paper proposes a new reinforcement learning method based on goal relabeling and dynamic (GMRL) model. In the GMRL, the quality of the explored data will be improved by the goal relabeling at first, followed by introducing a dynamic model to further increase the data quantity. The proposed method has been tested in a reinforcement learning benchmark environment, and the results show that the performance of the proposed method is significantly better than that both of the goal relabeling and standard model-based methods. At the same time, the proposed method has a higher sample efficiency than other existing combined schema of goal relabeling and dynamic model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057564824",
                        "name": "Kun Dong"
                    },
                    {
                        "authorId": "97685390",
                        "name": "Yongle Luo"
                    },
                    {
                        "authorId": "2180352936",
                        "name": "Yuxin Wang"
                    },
                    {
                        "authorId": "2242949258",
                        "name": "Shan Fang"
                    },
                    {
                        "authorId": "2256806848",
                        "name": "Yu Liu"
                    },
                    {
                        "authorId": "2065660974",
                        "name": "Erkang Cheng"
                    },
                    {
                        "authorId": "2243031884",
                        "name": "Zhiyong Sun"
                    },
                    {
                        "authorId": "19219781",
                        "name": "Q. Zhang"
                    },
                    {
                        "authorId": "2105722685",
                        "name": "Bo Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In particular, we compare to the model-based approach MBPO [16] and the model-free approaches SAC [8] and PPO [9].",
                "The first source of error can arise if the model is used to simulate or \u2018hallucinate\u2019 trajectories for the system which are then added to the data set [16, 17, 18, 19]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "364ead837d4c287377fdc068672fb7aea7ae9121",
                "externalIds": {
                    "ArXiv": "2307.08168",
                    "DBLP": "journals/corr/abs-2307-08168",
                    "DOI": "10.48550/arXiv.2307.08168",
                    "CorpusId": 259937515
                },
                "corpusId": 259937515,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/364ead837d4c287377fdc068672fb7aea7ae9121",
                "title": "Feedback is All You Need: Real-World Reinforcement Learning with Approximate Physics-Based Models",
                "abstract": "We focus on developing efficient and reliable policy optimization strategies for robot learning with real-world data. In recent years, policy gradient methods have emerged as a promising paradigm for training control policies in simulation. However, these approaches often remain too data inefficient or unreliable to train on real robotic hardware. In this paper we introduce a novel policy gradient-based policy optimization framework which systematically leverages a (possibly highly simplified) first-principles model and enables learning precise control policies with limited amounts of real-world data. Our approach $1)$ uses the derivatives of the model to produce sample-efficient estimates of the policy gradient and $2)$ uses the model to design a low-level tracking controller, which is embedded in the policy class. Theoretical analysis provides insight into how the presence of this feedback controller addresses overcomes key limitations of stand-alone policy gradient methods, while hardware experiments with a small car and quadruped demonstrate that our approach can learn precise control strategies reliably and with only minutes of real-world data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3408174",
                        "name": "T. Westenbroek"
                    },
                    {
                        "authorId": "2223583947",
                        "name": "Jacob Levy"
                    },
                    {
                        "authorId": "1390046746",
                        "name": "David Fridovich-Keil"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We will introduce a loss which learns an approximate model m\u0303, which can then be combined with the replay buffer D to use both experienced transitions and modelled transitions to learn \u03c0, as was done in e.g. Sutton (1991) or Janner et al. (2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2361fa49ee175d6dafd91366667a3a211352ab4d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-01708",
                    "ArXiv": "2307.01708",
                    "DOI": "10.48550/arXiv.2307.01708",
                    "CorpusId": 259341963
                },
                "corpusId": 259341963,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2361fa49ee175d6dafd91366667a3a211352ab4d",
                "title": "Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning",
                "abstract": "We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112211486",
                        "name": "Tyler Kastner"
                    },
                    {
                        "authorId": "2090630",
                        "name": "Murat A. Erdogdu"
                    },
                    {
                        "authorId": "5689899",
                        "name": "Amir-massoud Farahmand"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, we use model-based RL strategies [43], [44], [61] to obtain the",
                "In the case of model-based RL, the stability issue with a large \u03b3 would be more obvious since there exist compounding model errors when a long rollout is sampled from the model [44], [60].",
                "In addition, various planning strategies can be used to derive a policy from the model [42], such as MPC [43] and policy optimization [44]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ee03bdb42392b61dc2236e02a228332f22b21b03",
                "externalIds": {
                    "DBLP": "journals/tsusc/ChenMZ23",
                    "DOI": "10.1109/TSUSC.2023.3251302",
                    "CorpusId": 257300410
                },
                "corpusId": 257300410,
                "publicationVenue": {
                    "id": "8972ab32-b3a8-4d2e-bfa0-494c8801bcce",
                    "name": "IEEE Transactions on Sustainable Computing",
                    "alternate_names": [
                        "IEEE Trans Sustain Comput"
                    ],
                    "issn": "2377-3790",
                    "alternate_issns": [
                        "2377-3782"
                    ],
                    "url": "https://www.computer.org/web/tsusc",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274860"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ee03bdb42392b61dc2236e02a228332f22b21b03",
                "title": "Fast Human-in-the-Loop Control for HVAC Systems via Meta-Learning and Model-Based Offline Reinforcement Learning",
                "abstract": "Reinforcement learning (RL) methods can be used to develop a controller for the heating, ventilation, and air conditioning (HVAC) systems that both saves energy and ensures high occupants\u2019 thermal comfort levels. However, the existing works typically require on-policy data to train an RL agent, and the occupants\u2019 personalized thermal preferences are not considered, which is limited in the real-world scenarios. This paper designs a high-performance model-based offline RL algorithm for personalized HVAC systems. The proposed algorithm can quickly adapt to different occupants\u2019 thermal preferences with a few thermal feedbacks, guaranteeing the high occupants\u2019 personalized thermal comfort levels efficiently. First, we use a meta-supervised learning algorithm to train an occupant's thermal preference model. Then, we train an ensemble neural network to predict the thermal states of the considered zone. In addition, the obtained ensemble networks can indicate the regions in the state and action spaces covered by the offline dataset. With the personalized thermal preference model updated via meta-testing, model-based RL is used to derive the optimal HVAC controller. Since the proposed algorithm only requires offline datasets and a few online thermal feedbacks for training, it contributes to a more practical deployment of the RL algorithm to HVAC systems. We use the ASHRAE database II to verify the effectiveness and advantage of the meta-learning algorithm for modeling different occupants\u2019 thermal preferences. Numerical simulations on the EnergyPlus environment demonstrate that the proposed algorithm can guarantee personalized thermal preferences with a slight increase of power consumption of 1.91% compared with the model-based RL algorithm with on-policy data aggregation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51456165",
                        "name": "Liangliang Chen"
                    },
                    {
                        "authorId": "2086783908",
                        "name": "Fei Meng"
                    },
                    {
                        "authorId": "2153391672",
                        "name": "Ying Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1 Learned models are used to augment the reinforcement learning process in several ways [63, 9, 32, 28, 5], and depending on the way they are used and trained the behavior and guarantees of the RL agent change significantly.",
                "Note that this is a unique advantage of value-aware models, which is not true for observation-space or latent self-prediction losses such as those used in Dreamer [28] or MBPO [32].",
                "The resulting model errors can impact the learned policy [58, 35, 57, 65, 44, 32, 38] leading to worse performance.",
                "Among these, several focus on correcting models using information obtained during exploration [33, 65, 47, 56], or limiting interaction with wrong models [9, 32, 3]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "edccb296307f1ea187c403072159fc00b96cb888",
                "externalIds": {
                    "ArXiv": "2306.17366",
                    "DBLP": "journals/corr/abs-2306-17366",
                    "DOI": "10.48550/arXiv.2306.17366",
                    "CorpusId": 259308999
                },
                "corpusId": 259308999,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/edccb296307f1ea187c403072159fc00b96cb888",
                "title": "\u03bb-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces",
                "abstract": "The idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decision-Aware Actor-Critic framework ($\\lambda$-AC) for decision-aware model-based reinforcement learning in continuous state-spaces and highlight important design choices in different environments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1387979639",
                        "name": "C. Voelcker"
                    },
                    {
                        "authorId": "2186822213",
                        "name": "Arash Ahmadian"
                    },
                    {
                        "authorId": "1515553184",
                        "name": "Romina Abachi"
                    },
                    {
                        "authorId": "2072248",
                        "name": "Igor Gilitschenski"
                    },
                    {
                        "authorId": "5689899",
                        "name": "Amir-massoud Farahmand"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is as expected given that previous works have proved that the policy performance degradation is bounded by the difference in transition distributions between two systems [29].",
                "When the simulation dynamics are very close to the real-world dynamics, one can expect the trajectory rollouts in the simulator to be close to that in the real world as well; here, an optimal agent trained in the simulator would expect near-optimal performance in the real world [29]."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "0cf40a645175d65a07f3358ed5549a8a9db02490",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-15864",
                    "ArXiv": "2306.15864",
                    "DOI": "10.48550/arXiv.2306.15864",
                    "CorpusId": 259274881
                },
                "corpusId": 259274881,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0cf40a645175d65a07f3358ed5549a8a9db02490",
                "title": "What Went Wrong? Closing the Sim-to-Real Gap via Differentiable Causal Discovery",
                "abstract": "Training control policies in simulation is more appealing than on real robots directly, as it allows for exploring diverse states in a safe and efficient manner. Yet, robot simulators inevitably exhibit disparities from the real world, yielding inaccuracies that manifest as the simulation-to-real gap. Existing literature has proposed to close this gap by actively modifying specific simulator parameters to align the simulated data with real-world observations. However, the set of tunable parameters is usually manually selected to reduce the search space in a case-by-case manner, which is hard to scale up for complex systems and requires extensive domain knowledge. To address the scalability issue and automate the parameter-tuning process, we introduce an approach that aligns the simulator with the real world by discovering the causal relationship between the environment parameters and the sim-to-real gap. Concretely, our method learns a differentiable mapping from the environment parameters to the differences between simulated and real-world robot-object trajectories. This mapping is governed by a simultaneously-learned causal graph to help prune the search space of parameters, provide better interpretability, and improve generalization. We perform experiments to achieve both sim-to-sim and sim-to-real transfer, and show that our method has significant improvements in trajectory alignment and task success rate over strong baselines in a challenging manipulation task.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119407102",
                        "name": "Peide Huang"
                    },
                    {
                        "authorId": "2158345669",
                        "name": "Xilun Zhang"
                    },
                    {
                        "authorId": "2113998517",
                        "name": "Ziang Cao"
                    },
                    {
                        "authorId": "2131159790",
                        "name": "Shiqi Liu"
                    },
                    {
                        "authorId": "1768153749",
                        "name": "Mengdi Xu"
                    },
                    {
                        "authorId": "152425748",
                        "name": "Wenhao Ding"
                    },
                    {
                        "authorId": "26253744",
                        "name": "Jonathan M Francis"
                    },
                    {
                        "authorId": "12515120",
                        "name": "Bingqing Chen"
                    },
                    {
                        "authorId": "47783130",
                        "name": "Ding Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, such a model is not always available, and learning them is prone to errors that compound for longer horizons (Ross et al., 2011; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6b8e98792e4af57687939156c07b99cd12187f89",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-14808",
                    "ArXiv": "2306.14808",
                    "DOI": "10.48550/arXiv.2306.14808",
                    "CorpusId": 259261970
                },
                "corpusId": 259261970,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6b8e98792e4af57687939156c07b99cd12187f89",
                "title": "Maximum State Entropy Exploration using Predecessor and Successor Representations",
                "abstract": "Animals have a developed ability to explore that aids them in important tasks such as locating food, exploring for shelter, and finding misplaced items. These exploration skills necessarily track where they have been so that they can plan for finding items with relative efficiency. Contemporary exploration algorithms often learn a less efficient exploration strategy because they either condition only on the current state or simply rely on making random open-loop exploratory moves. In this work, we propose $\\eta\\psi$-Learning, a method to learn efficient exploratory policies by conditioning on past episodic experience to make the next exploratory move. Specifically, $\\eta\\psi$-Learning learns an exploration policy that maximizes the entropy of the state visitation distribution of a single trajectory. Furthermore, we demonstrate how variants of the predecessor representation and successor representations can be combined to predict the state visitation entropy. Our experiments demonstrate the efficacy of $\\eta\\psi$-Learning to strategically explore the environment and maximize the state coverage with limited samples.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "7284555",
                        "name": "A. Jain"
                    },
                    {
                        "authorId": "39251318",
                        "name": "Lucas Lehnert"
                    },
                    {
                        "authorId": "2109771",
                        "name": "I. Rish"
                    },
                    {
                        "authorId": "2994035",
                        "name": "G. Berseth"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[22] also proposes \u201cirrecoverable state\u201d with a similar meaning to dead-ends state, preventing dangerous situations from occurring through reward shaping and model-based rollout [23]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9eed36fb9b9bbb97579953d5e303dc0cf6c70a58",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-13944",
                    "ArXiv": "2306.13944",
                    "DOI": "10.48550/arXiv.2306.13944",
                    "CorpusId": 259251801
                },
                "corpusId": 259251801,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9eed36fb9b9bbb97579953d5e303dc0cf6c70a58",
                "title": "Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery",
                "abstract": "Safety is one of the main challenges in applying reinforcement learning to realistic environmental tasks. To ensure safety during and after training process, existing methods tend to adopt overly conservative policy to avoid unsafe situations. However, overly conservative policy severely hinders the exploration, and makes the algorithms substantially less rewarding. In this paper, we propose a method to construct a boundary that discriminates safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pretrained on an offline dataset, in which the safety critic evaluates upper bound of safety in each state as awareness of environmental safety for the agent. During online training, a behavior correction mechanism is adopted, ensuring the agent to interact with the environment using safe actions only. Finally, experiments of continuous control tasks demonstrate that our approach has better task performance with less safety violations than state-of-the-art algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2128029656",
                        "name": "Xiao Zhang"
                    },
                    {
                        "authorId": "2220559577",
                        "name": "Hai Zhang"
                    },
                    {
                        "authorId": "113866137",
                        "name": "Hongtu Zhou"
                    },
                    {
                        "authorId": "2110926580",
                        "name": "Chang Huang"
                    },
                    {
                        "authorId": "2220688800",
                        "name": "Di Zhang"
                    },
                    {
                        "authorId": "2064448848",
                        "name": "Chen Ye"
                    },
                    {
                        "authorId": "2406328",
                        "name": "Junqiao Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d00a0d3f80a8d60d2a9d5f8f574e0ffd6b044428",
                "externalIds": {
                    "ArXiv": "2306.11941",
                    "DBLP": "journals/corr/abs-2306-11941",
                    "DOI": "10.48550/arXiv.2306.11941",
                    "CorpusId": 259212224
                },
                "corpusId": 259212224,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d00a0d3f80a8d60d2a9d5f8f574e0ffd6b044428",
                "title": "Efficient Dynamics Modeling in Interactive Environments with Koopman Theory",
                "abstract": "The accurate modeling of dynamics in interactive environments is critical for successful long-range prediction. Such a capability could advance Reinforcement Learning (RL) and Planning algorithms, but achieving it is challenging. Inaccuracies in model estimates can compound, resulting in increased errors over long horizons. We approach this problem from the lens of Koopman theory, where the nonlinear dynamics of the environment can be linearized in a high-dimensional latent space. This allows us to efficiently parallelize the sequential problem of long-range prediction using convolution while accounting for the agent's action at every time step. Our approach also enables stability analysis and better control over gradients through time. Taken together, these advantages result in significant improvement over the existing approaches, both in the efficiency and the accuracy of modeling dynamics over extended horizons. We also show that this model can be easily incorporated into dynamics modeling for model-based planning and model-free RL and report promising experimental results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2163155722",
                        "name": "Arnab Kumar Mondal"
                    },
                    {
                        "authorId": "2090453508",
                        "name": "S. Panigrahi"
                    },
                    {
                        "authorId": "1818842",
                        "name": "Sai Rajeswar"
                    },
                    {
                        "authorId": "1786870",
                        "name": "K. Siddiqi"
                    },
                    {
                        "authorId": "2111187",
                        "name": "Siamak Ravanbakhsh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We calculate the estimation error based on the difference between the Monte Carlo return value and the Q-estimates as in [33, 6, 14].",
                "We evaluate the policy return after each epoch by calculating the undiscounted sum of rewards when running the current learnt policy [6, 14]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4b4ec8a7994f466b305244ac550526f704748898",
                "externalIds": {
                    "ArXiv": "2306.11918",
                    "DBLP": "conf/nips/WangLZ21",
                    "DOI": "10.48550/arXiv.2306.11918",
                    "CorpusId": 247471774
                },
                "corpusId": 247471774,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4b4ec8a7994f466b305244ac550526f704748898",
                "title": "Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback",
                "abstract": "The ensemble method is a promising way to mitigate the overestimation issue in Q-learning, where multiple function approximators are used to estimate the action values. It is known that the estimation bias hinges heavily on the ensemble size (i.e., the number of Q-function approximators used in the target), and that determining the `right' ensemble size is highly nontrivial, because of the time-varying nature of the function approximation errors during the learning process. To tackle this challenge, we first derive an upper bound and a lower bound on the estimation bias, based on which the ensemble size is adapted to drive the bias to be nearly zero, thereby coping with the impact of the time-varying approximation errors accordingly. Motivated by the theoretic findings, we advocate that the ensemble method can be combined with Model Identification Adaptive Control (MIAC) for effective ensemble size adaptation. Specifically, we devise Adaptive Ensemble Q-learning (AdaEQ), a generalized ensemble method with two key steps: (a) approximation error characterization which serves as the feedback for flexibly controlling the ensemble size, and (b) ensemble size adaptation tailored towards minimizing the estimation bias. Extensive experiments are carried out to show that AdaEQ can improve the learning performance than the existing methods for the MuJoCo benchmark.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155266301",
                        "name": "Hang Wang"
                    },
                    {
                        "authorId": "1641386905",
                        "name": "Sen Lin"
                    },
                    {
                        "authorId": "47540395",
                        "name": "Junshan Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 Uncertainty Estimation [13], [40], [41]: This method is allowed to switch between conservative and naive offpolicy RL methods and conduct a proper estimation and usage of uncertainty."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a0d87da6f822789da3b7c2c832594d1a542848ee",
                "externalIds": {
                    "DBLP": "conf/ijcnn/SuKW23",
                    "DOI": "10.1109/IJCNN54540.2023.10191211",
                    "CorpusId": 260386805
                },
                "corpusId": 260386805,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/a0d87da6f822789da3b7c2c832594d1a542848ee",
                "title": "Uncertainty-Aware Data Augmentation for Offline Reinforcement Learning",
                "abstract": "One of the key challenges in Offline Reinforcement Learning is that it cannot conduct further environment exploration and performs poorly in terms of out-of-distribution generalizations. Data augmentation is commonly used to solve the issue of limited coverage of the full state-action space in static offline dataset. However, the existing data augmentation methods for proprioceptive observation suffer from the dilemma where the data coverage is often limited by tight constraints, while aggressive methods may exacerbate the performance. At the heart of this phenomenon are the diverged action distribution and the high uncertainty of the value function. In this paper, we propose to extend the static offline datasets during training by adding gradient-based perturbation to the state and utilizing the estimated uncertainty of the value function to constrain the range of the gradient. The estimated uncertainty of the value function works as a guidance to adjust the range of augmentation automatically, ensuring the adaptability and reliability of the state perturbation. The proposed algorithm Uncertainty-Aware Data Augmentation(UADA), is plugged into various standard offline RL algorithms and evaluated on several offline rein-forcement learning tasks. The empirical results confirm that UADA substantially improves the performance and achieves better model stability compared with the original algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226694001",
                        "name": "Yunjie Su"
                    },
                    {
                        "authorId": "2147218845",
                        "name": "Yilun Kong"
                    },
                    {
                        "authorId": "2155638834",
                        "name": "Xueqian Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The used model is an ensemble of 10 blackbox neural networks as [15] that jointly predicts the temperature transition and reward with a rollout length of 1.",
                "\u25a0 BL4 (Model-based policy optimization) refers to a state-of-theart model-based method that uses short rollouts from the model to update the agent [15].",
                "In this example, the black-box model is an ensemble of black-box MLPs used in [15] trained with historical data."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b33649fafdbbe08b96d376ea74b149920b413a6a",
                "externalIds": {
                    "DBLP": "conf/eenergy/WangC00T23",
                    "DOI": "10.1145/3575813.3595189",
                    "CorpusId": 259177768
                },
                "corpusId": 259177768,
                "publicationVenue": {
                    "id": "0b391b14-8353-4d09-be78-077ce8a257f7",
                    "name": "Energy-Efficient Computing and Networking",
                    "type": "conference",
                    "alternate_names": [
                        "Energy-efficient Comput Netw",
                        "E-Energy",
                        "e-Energy"
                    ],
                    "url": "http://www.energyware.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b33649fafdbbe08b96d376ea74b149920b413a6a",
                "title": "Phyllis: Physics-Informed Lifelong Reinforcement Learning for Data Center Cooling Control",
                "abstract": "Deep reinforcement learning (DRL) has shown good performance in data center cooling control for improving energy efficiency. The main challenge in deploying the DRL agent to real-world data centers is how to quickly adapt the agent to the ever-changing system with thermal safety compliance. Existing approaches rely on DRL\u2019s native fine-tuning or a learned data-driven dynamics model to assist the adaptation. However, they require long-term unsafe exploration before the agent or the model can capture a new environment. This paper proposes Phyllis, a physics-informed reinforcement learning approach to assist the DRL agent\u2019s lifelong learning under evolving data center environment. Phyllis first identifies a transition model to capture the data hall thermodynamics in the offline stage. When the environment changes in the online stage, Phyllis assists the adaptation by i) supervising safe data collection with the identified transition model, ii) fitting power usage and residual thermal models, iii) pretraining the agent by interacting with these models, and iv) deploying the agent for further fine-tuning. Phyllis uses known physical laws to inform the transition and power models for improving the extrapolation ability to unseen states. Extensive evaluation for two simulated data centers with different system changes shows that Phyllis saves 5.7% to 13.8% energy usage compared with feedback cooling control and adapts to new environments 8x to 10x faster than fine-tuning with at most 0.74\u00b0C temperature overshoot.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108755336",
                        "name": "Ruihang Wang"
                    },
                    {
                        "authorId": "2392789",
                        "name": "Zhi-Ying Cao"
                    },
                    {
                        "authorId": "48666864",
                        "name": "Xiaoxia Zhou"
                    },
                    {
                        "authorId": "2117855118",
                        "name": "Yonggang Wen"
                    },
                    {
                        "authorId": "2100318587",
                        "name": "Rui Tan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Though model-based methods have been applied to the humanoid task, prior works tend to keep the horizon intentionally short to prevent the accumulation of model errors (Janner et al., 2019; Amos et al., 2021).",
                "\u2026been much work on learned latent spaces, with the usual spectrum ranging from those trained with reconstructive objectives (Hafner et al., 2021a; Janner et al., 2019) to those that contain only value-relevant information (Grimm et al., 2020) and options that interpolate between these two\u2026",
                ", 2017), MBPO (Janner et al., 2019), and MVE (Feinberg et al.",
                "There has been much work on learned latent spaces, with the usual spectrum ranging from those trained with reconstructive objectives (Hafner et al., 2021a; Janner et al., 2019) to those that contain only value-relevant information (Grimm et al.",
                "There is a natural tradeoff with \u03b3-models: the higher \u03b3 is, the fewer model steps are needed to make long-horizon predictions, reducing model-based compounding prediction errors (Asadi et al., 2019; Janner et al., 2019).",
                "We compare this approach to the state-of-the-art in model-based and model-free methods, with representative algorithms consisting of SAC, PPO (Schulman et al., 2017), MBPO (Janner et al., 2019), and MVE (Feinberg et al., 2018).",
                "Empirically, dynamics models are found to be easier to train than value functions, allowing for better sample efficiency and generalization of learned models (Janner et al., 2019); this can be viewed as a consequence of either differences between the types of algorithms used to train value functions versus dynamics models (Kumar et al.",
                "The rightmost plot depicts the value map produced by value iteration on a discretization of the same environment for reference. . . . . . . . . . . . . . . . . . . . . . . . 21 3.5 (\u03b3-MVE control performance) Comparative performance of \u03b3-MVE and four prior reinforcement learning algorithms on continuous control benchmark tasks. \u03b3MVE retains the asymptotic performance of SAC with sample-efficiency matching that of MBPO.",
                "\u2026dynamics models are found to be easier to train than value functions, allowing for better sample efficiency and generalization of learned models (Janner et al., 2019); this can be viewed as a consequence of either differences between the types of algorithms used to train value functions versus\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b109a1b6254ca0f9467c98285a1d9a1f710f6b2a",
                "externalIds": {
                    "ArXiv": "2306.08810",
                    "DBLP": "journals/corr/abs-2306-08810",
                    "DOI": "10.48550/arXiv.2306.08810",
                    "CorpusId": 259164647
                },
                "corpusId": 259164647,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b109a1b6254ca0f9467c98285a1d9a1f710f6b2a",
                "title": "Deep Generative Models for Decision-Making and Control",
                "abstract": "Deep model-based reinforcement learning methods offer a conceptually simple approach to the decision-making and control problem: use learning for the purpose of estimating an approximate dynamics model, and offload the rest of the work to classical trajectory optimization. However, this combination has a number of empirical shortcomings, limiting the usefulness of model-based methods in practice. The dual purpose of this thesis is to study the reasons for these shortcomings and to propose solutions for the uncovered problems. Along the way, we highlight how inference techniques from the contemporary generative modeling toolbox, including beam search, classifier-guided sampling, and image inpainting, can be reinterpreted as viable planning strategies for reinforcement learning problems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35163402",
                        "name": "Michael Janner"
                    }
                ]
            }
        },
        {
            "contexts": [
                "World Models Model-based RL algorithms use the experience gathered by an agent to learn a model of the environment [68, 10, 25].",
                "Our work focuses on training more robust world-models [18, 20, 21, 22, 25, 54, 62, 76] in the reward-free setting."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "620a09d817407926e4390258bb1080912a96320e",
                "externalIds": {
                    "ArXiv": "2306.09205",
                    "DBLP": "journals/corr/abs-2306-09205",
                    "DOI": "10.48550/arXiv.2306.09205",
                    "CorpusId": 259164621
                },
                "corpusId": 259164621,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/620a09d817407926e4390258bb1080912a96320e",
                "title": "Reward-Free Curricula for Training Robust World Models",
                "abstract": "There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. Achieving a general agent requires robustness across different environments. However, different environments may require different amounts of data to learn a suitable world model. In this work, we address the problem of efficiently learning robust world models in the reward-free setting. As a measure of robustness, we consider the minimax regret objective. We show that the minimax regret objective can be connected to minimising the maximum error in the world model across environments. This informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WAKER outperforms naive domain randomisation, resulting in improved robustness, efficiency, and generalisation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51300315",
                        "name": "Marc Rigter"
                    },
                    {
                        "authorId": "2152154941",
                        "name": "Minqi Jiang"
                    },
                    {
                        "authorId": "1834086",
                        "name": "I. Posner"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many recent MBRL methods (Janner et al., 2019; Clavera et al., 2020; Yu et al., 2020; Kidambi et al., 2020) employ the ensemble model used in PETS by default."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e9ad2c8a4f57ba4296dcd31a6e7d3c4b44725787",
                "externalIds": {
                    "ArXiv": "2306.09466",
                    "DBLP": "journals/corr/abs-2306-09466",
                    "DOI": "10.48550/arXiv.2306.09466",
                    "CorpusId": 259188106
                },
                "corpusId": 259188106,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e9ad2c8a4f57ba4296dcd31a6e7d3c4b44725787",
                "title": "Simplified Temporal Consistency Reinforcement Learning",
                "abstract": "Reinforcement learning is able to solve complex sequential decision-making tasks but is currently limited by sample efficiency and required computation. To improve sample efficiency, recent work focuses on model-based RL which interleaves model learning with planning. Recent methods further utilize policy learning, value estimation, and, self-supervised learning as auxiliary objectives. In this paper we show that, surprisingly, a simple representation learning approach relying only on a latent dynamics model trained by latent temporal consistency is sufficient for high-performance RL. This applies when using pure planning with a dynamics model conditioned on the representation, but, also when utilizing the representation as policy and value function features in model-free RL. In experiments, our approach learns an accurate dynamics model to solve challenging high-dimensional locomotion tasks with online planners while being 4.1 times faster to train compared to ensemble-based methods. With model-free RL without planning, especially on high-dimensional tasks, such as the DeepMind Control Suite Humanoid and Dog tasks, our approach outperforms model-free methods by a large margin and matches model-based methods' sample efficiency while training 2.4 times faster.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109917181",
                        "name": "Yi Zhao"
                    },
                    {
                        "authorId": "51044974",
                        "name": "Wenshuai Zhao"
                    },
                    {
                        "authorId": "22169323",
                        "name": "Rinu Boney"
                    },
                    {
                        "authorId": "1776374",
                        "name": "Juho Kannala"
                    },
                    {
                        "authorId": "34906504",
                        "name": "J. Pajarinen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also compare offline MBPO with HIPODE since it can also be seen as a method directly using dynamics-model-generated data as augmented data.",
                "The MB+\u03b1TD3BC results, together with those from MBPO, suggest that using dynamics-model-generated data as augmentation can harm the offline agent.",
                "Additionally, HIPODE is outperformed by vanilla model-based ORL methods (e.g., MBPO) on -random datasets because the value penalty is excessively strict on those datasets.",
                "The difference between model based TD3BC and MBPO is that TD3BC has a behaviour cloning restrict on it\u2019s critic [5] while MBPO [11] dose not."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5bfc037bae6c3d056c1181d27aa509d8d86e6ebd",
                "externalIds": {
                    "ArXiv": "2306.06329",
                    "DBLP": "journals/corr/abs-2306-06329",
                    "DOI": "10.48550/arXiv.2306.06329",
                    "CorpusId": 259138582
                },
                "corpusId": 259138582,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5bfc037bae6c3d056c1181d27aa509d8d86e6ebd",
                "title": "HIPODE: Enhancing Offline Reinforcement Learning with High-Quality Synthetic Data from a Policy-Decoupled Approach",
                "abstract": "Offline reinforcement learning (ORL) has gained attention as a means of training reinforcement learning models using pre-collected static data. To address the issue of limited data and improve downstream ORL performance, recent work has attempted to expand the dataset's coverage through data augmentation. However, most of these methods are tied to a specific policy (policy-dependent), where the generated data can only guarantee to support the current downstream ORL policy, limiting its usage scope on other downstream policies. Moreover, the quality of synthetic data is often not well-controlled, which limits the potential for further improving the downstream policy. To tackle these issues, we propose \\textbf{HI}gh-quality \\textbf{PO}licy-\\textbf{DE}coupled~(HIPODE), a novel data augmentation method for ORL. On the one hand, HIPODE generates high-quality synthetic data by selecting states near the dataset distribution with potentially high value among candidate states using the negative sampling technique. On the other hand, HIPODE is policy-decoupled, thus can be used as a common plug-in method for any downstream ORL process. We conduct experiments on the widely studied TD3BC and CQL algorithms, and the results show that HIPODE outperforms the state-of-the-art policy-decoupled data augmentation method and most prevalent model-based ORL methods on D4RL benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219927001",
                        "name": "Shixi Lian"
                    },
                    {
                        "authorId": "2146275908",
                        "name": "Yi Ma"
                    },
                    {
                        "authorId": "2124810107",
                        "name": "Jinyi Liu"
                    },
                    {
                        "authorId": "2111092315",
                        "name": "Yan Zheng"
                    },
                    {
                        "authorId": "1889014",
                        "name": "Zhaopeng Meng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Indeed, many model-based RL algorithms explicitly rely on measures of model uncertainty, or alternatively on some measure of the distance to the previously observed training data, during policy synthesis [37, 38, 3, 39, 4, 40, 41, 42, 43, 44]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "8b5e502384da02b730b6693a0a623bf41f4b91ce",
                "externalIds": {
                    "ArXiv": "2306.06335",
                    "DBLP": "journals/corr/abs-2306-06335",
                    "DOI": "10.48550/arXiv.2306.06335",
                    "CorpusId": 259138787
                },
                "corpusId": 259138787,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8b5e502384da02b730b6693a0a623bf41f4b91ce",
                "title": "How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations",
                "abstract": "We present a framework and algorithms to learn controlled dynamics models using neural stochastic differential equations (SDEs) -- SDEs whose drift and diffusion terms are both parametrized by neural networks. We construct the drift term to leverage a priori physics knowledge as inductive bias, and we design the diffusion term to represent a distance-aware estimate of the uncertainty in the learned model's predictions -- it matches the system's underlying stochasticity when evaluated on states near those from the training dataset, and it predicts highly stochastic dynamics when evaluated on states beyond the training regime. The proposed neural SDEs can be evaluated quickly enough for use in model predictive control algorithms, or they can be used as simulators for model-based reinforcement learning. Furthermore, they make accurate predictions over long time horizons, even when trained on small datasets that cover limited regions of the state space. We demonstrate these capabilities through experiments on simulated robotic systems, as well as by using them to model and control a hexacopter's flight dynamics: A neural SDE trained using only three minutes of manually collected flight data results in a model-based control policy that accurately tracks aggressive trajectories that push the hexacopter's velocity and Euler angles to nearly double the maximum values observed in the training dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152050811",
                        "name": "Franck Djeumou"
                    },
                    {
                        "authorId": "1796254983",
                        "name": "Cyrus Neary"
                    },
                    {
                        "authorId": "3199888",
                        "name": "U. Topcu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that the proposed TDM is very different from the conventional dynamics models used in model-based RL (MBRL) methods [21, 50, 23, 42, 51]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b6cb5763e064f070162e24ffb63e51de53ea368d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-04220",
                    "ArXiv": "2306.04220",
                    "DOI": "10.48550/arXiv.2306.04220",
                    "CorpusId": 259095661
                },
                "corpusId": 259095661,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b6cb5763e064f070162e24ffb63e51de53ea368d",
                "title": "Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL",
                "abstract": "Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. These can be readily used to construct a new offline RL algorithm (TSRL) with less conservative policy constraints and a reliable latent space data augmentation procedure. Based on extensive experiments, we find TSRL achieves great performance on small benchmark datasets with as few as 1% of the original samples, which significantly outperforms the recent offline RL algorithms in terms of data efficiency and generalizability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219274266",
                        "name": "Peng Cheng"
                    },
                    {
                        "authorId": "3415564",
                        "name": "Xianyuan Zhan"
                    },
                    {
                        "authorId": "2146254134",
                        "name": "Zhihao Wu"
                    },
                    {
                        "authorId": "2047518186",
                        "name": "Wenjia Zhang"
                    },
                    {
                        "authorId": "2219667539",
                        "name": "Shoucheng Song"
                    },
                    {
                        "authorId": "2144396405",
                        "name": "Han Wang"
                    },
                    {
                        "authorId": "2624174",
                        "name": "Youfang Lin"
                    },
                    {
                        "authorId": "2148655709",
                        "name": "Li Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While these methods enables one-step forward prediction [38, 64, 103] and auto-regressive imaginary rollouts [18, 27, 68, 94, 113] that is a subset of the more general video prediction problem [14, 55, 63], model outputs usually degrade rapidly into the future [48, 115]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d11ae7f22045a2217fb2ef169037fba216153c63",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-03346",
                    "ArXiv": "2306.03346",
                    "DOI": "10.48550/arXiv.2306.03346",
                    "CorpusId": 259088679
                },
                "corpusId": 259088679,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d11ae7f22045a2217fb2ef169037fba216153c63",
                "title": "Stabilizing Contrastive RL: Techniques for Offline Goal Reaching",
                "abstract": "In the same way that the computer vision (CV) and natural language processing (NLP) communities have developed self-supervised methods, reinforcement learning (RL) can be cast as a self-supervised problem: learning to reach any goal, without requiring human-specified rewards or labels. However, actually building a self-supervised foundation for RL faces some important challenges. Building on prior contrastive approaches to this RL problem, we conduct careful ablation experiments and discover that a shallow and wide architecture, combined with careful weight initialization and data augmentation, can significantly boost the performance of these contrastive RL approaches on challenging simulated benchmarks. Additionally, we demonstrate that, with these design decisions, contrastive approaches can solve real-world robotic manipulation tasks, with tasks being specified by a single goal image provided after training.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1382713388",
                        "name": "Chongyi Zheng"
                    },
                    {
                        "authorId": "8140754",
                        "name": "Benjamin Eysenbach"
                    },
                    {
                        "authorId": "2029241116",
                        "name": "Homer Walke"
                    },
                    {
                        "authorId": "2163582683",
                        "name": "Patrick Yin"
                    },
                    {
                        "authorId": "145213709",
                        "name": "Kuan Fang"
                    },
                    {
                        "authorId": "145124475",
                        "name": "R. Salakhutdinov"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "918ddc8773cdee006a93e51a085556792e3b1284",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-03552",
                    "ArXiv": "2306.03552",
                    "DOI": "10.48550/arXiv.2306.03552",
                    "CorpusId": 259088927
                },
                "corpusId": 259088927,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/918ddc8773cdee006a93e51a085556792e3b1284",
                "title": "State Regularized Policy Optimization on Data with Dynamics Shift",
                "abstract": "In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \\textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\\textbf{S}tate \\textbf{R}egularized \\textbf{P}olicy \\textbf{O}ptimization) algorithm. To conduct theoretical analyses, the intuition of similar environment structures is characterized by the notion of homomorphous MDPs. We then demonstrate a lower-bound performance guarantee on policies regularized by the stationary state distribution. In practice, SRPO can be an add-on module to context-based algorithms in both online and offline RL settings. Experimental results show that SRPO can make several context-based algorithms far more data efficient and significantly improve their overall performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2093481204",
                        "name": "Zhenghai Xue"
                    },
                    {
                        "authorId": "144994208",
                        "name": "Qingpeng Cai"
                    },
                    {
                        "authorId": "50152132",
                        "name": "Shuchang Liu"
                    },
                    {
                        "authorId": "2153430224",
                        "name": "Dong Zheng"
                    },
                    {
                        "authorId": "2061280682",
                        "name": "Peng Jiang"
                    },
                    {
                        "authorId": "20029557",
                        "name": "Kun Gai"
                    },
                    {
                        "authorId": "143706345",
                        "name": "Bo An"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Future work can improve sample efficiency through offline datasets [35], model-based reinforcement learning [36, 37], or better representation learning methods [38, 39]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8143fd755457261cc7bade5ba3b1eb1720bf5035",
                "externalIds": {
                    "ArXiv": "2306.04026",
                    "DBLP": "journals/corr/abs-2306-04026",
                    "DOI": "10.48550/arXiv.2306.04026",
                    "CorpusId": 259129913
                },
                "corpusId": 259129913,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8143fd755457261cc7bade5ba3b1eb1720bf5035",
                "title": "Value Functions are Control Barrier Functions: Verification of Safe Policies using Control Theory",
                "abstract": "Guaranteeing safe behaviour of reinforcement learning (RL) policies poses significant challenges for safety-critical applications, despite RL's generality and scalability. To address this, we propose a new approach to apply verification methods from control theory to learned value functions. By analyzing task structures for safety preservation, we formalize original theorems that establish links between value functions and control barrier functions. Further, we propose novel metrics for verifying value functions in safe control tasks and practical implementation details to improve learning. Our work presents a novel method for certificate learning, which unlocks a diversity of verification techniques from control theory for RL policies, and marks a significant step towards a formal framework for the general, scalable, and verifiable design of RL-based control systems. Code and videos are available at this https url: https://rl-cbf.github.io/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219272080",
                        "name": "Daniel C.H. Tan"
                    },
                    {
                        "authorId": "2126051157",
                        "name": "Fernando Acero"
                    },
                    {
                        "authorId": "144722083",
                        "name": "Robert McCarthy"
                    },
                    {
                        "authorId": "1704541",
                        "name": "D. Kanoulas"
                    },
                    {
                        "authorId": "2216390361",
                        "name": "Zhibin Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In contrast to previous methods that utilize SAC as policy optimization backbones [38, 46, 62, 39], our MB-BAC algorithm treats real and model-generated data differently.",
                "We evaluate the performance of MB-BAC, which integrates the BEE operator into the MBPO algorithm, against several model-based and model-free baselines.",
                "To facilitate a fair comparison, MB-BAC and MBPO are run with identical network architectures and training configurations as specified by MBRL-LIB.",
                "To ensure a fair comparison, we follow the same settings as our model-based baselines (MBPO [38], AutoMBPO [46], CMLO [39]), in which observations are truncated.",
                "As for model-based methods, we compare with four state-of-the-art model-based algorithms, MBPO [38], SLBO [52], CMLO [39], AutoMBPO [46].",
                "The implementation of SLBO is taken from an open-source MBRL benchmark [84], while MBPO is implemented based on the MBRL-LIB toolbox [65].",
                "Among the Dyna-style counterparts, MBPO [38], CMLO [39], and AutoMBPO [46] use SAC as the policy optimizer, while SLBO [52] employs TRPO [70].",
                "The practical implementation builds upon MBPO [38] by integrating the BAC as policy optimizer, with the pseudocode in Appendix B."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "result"
            ],
            "citingPaper": {
                "paperId": "4f8c6a8a38bcea04343143358dca8a6f9c8e2fed",
                "externalIds": {
                    "ArXiv": "2306.02865",
                    "DBLP": "journals/corr/abs-2306-02865",
                    "DOI": "10.48550/arXiv.2306.02865",
                    "CorpusId": 259076434
                },
                "corpusId": 259076434,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4f8c6a8a38bcea04343143358dca8a6f9c8e2fed",
                "title": "Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic",
                "abstract": "Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and the current policy. The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2072498620",
                        "name": "Tianying Ji"
                    },
                    {
                        "authorId": "1491625903",
                        "name": "Yuping Luo"
                    },
                    {
                        "authorId": "2323566",
                        "name": "Fuchun Sun"
                    },
                    {
                        "authorId": "3415564",
                        "name": "Xianyuan Zhan"
                    },
                    {
                        "authorId": "1739414",
                        "name": "Jianwei Zhang"
                    },
                    {
                        "authorId": "3286703",
                        "name": "Huazhe Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides, in model-based RL, a series of works (Chua et al., 2018; Janner et al., 2019) adopt ensemble dynamic models for robust dynamics modeling."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "13bc7c2672e662103aaf38995cddaa8d2c84ed7c",
                "externalIds": {
                    "DBLP": "journals/jair/XuLYYH23",
                    "DOI": "10.1613/jair.1.14398",
                    "CorpusId": 259190415
                },
                "corpusId": 259190415,
                "publicationVenue": {
                    "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
                    "name": "Journal of Artificial Intelligence Research",
                    "type": "journal",
                    "alternate_names": [
                        "JAIR",
                        "J Artif Intell Res",
                        "The Journal of Artificial Intelligence Research"
                    ],
                    "issn": "1076-9757",
                    "url": "http://www.jair.org/"
                },
                "url": "https://www.semanticscholar.org/paper/13bc7c2672e662103aaf38995cddaa8d2c84ed7c",
                "title": "Efficient Multi-Goal Reinforcement Learning via Value Consistency Prioritization",
                "abstract": "Goal-conditioned reinforcement learning (RL) with sparse rewards remains a challenging problem in deep RL. Hindsight Experience Replay (HER) has been demonstrated to be an effective solution, where HER replaces desired goals in failed experiences with practically achieved states. Existing approaches mainly focus on either exploration or exploitation to improve the performance of HER. From a joint perspective, exploiting specific past experiences can also implicitly drive exploration. Therefore, we concentrate on prioritizing both original and relabeled samples for efficient goal-conditioned RL. To achieve this, we propose a novel value consistency prioritization (VCP) method, where the priority of samples is determined by the consistency of ensemble Q-values. This distinguishes the VCP method with most existing prioritization approaches which prioritizes samples based on the uncertainty of ensemble Q-values. Through extensive experiments, we demonstrate that VCP achieves significantly higher sample efficiency than existing algorithms on a range of challenging goal-conditioned manipulation tasks. We also visualize how VCP prioritizes good experiences to enhance policy learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144295575",
                        "name": "Jiawei Xu"
                    },
                    {
                        "authorId": "2115951736",
                        "name": "Shuxing Li"
                    },
                    {
                        "authorId": "145094495",
                        "name": "Rui Yang"
                    },
                    {
                        "authorId": "2151575531",
                        "name": "Chun Yuan"
                    },
                    {
                        "authorId": "2112661118",
                        "name": "Lei Han"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Before presenting the proof of Lemma A.3, we first introduce the assumption of concentration properties from (Auer et al., 2008; Kumar et al., 2020) and a modified lemma from (Janner et al., 2019).",
                "However, using single-step dynamics to autoregressively generate trajectories may suffer from the compounding rollout errors of long-term predictions (Janner et al., 2019), which would further lead to tremendous estimation error of value functions.",
                "Then we provide a lemma modified from Lemma B.3 in (Janner et al., 2019).",
                ", 2020) and a modified lemma from (Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "2b8efe38e1d95cacbe1f45390632ab07c17a5fc8",
                "externalIds": {
                    "DBLP": "conf/icml/LinTWYMXWW23",
                    "ArXiv": "2306.00603",
                    "DOI": "10.48550/arXiv.2306.00603",
                    "CorpusId": 258999173
                },
                "corpusId": 258999173,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2b8efe38e1d95cacbe1f45390632ab07c17a5fc8",
                "title": "Safe Offline Reinforcement Learning with Real-Time Budget Constraints",
                "abstract": "Aiming at promoting the safe real-world deployment of Reinforcement Learning (RL), research on safe RL has made significant progress in recent years. However, most existing works in the literature still focus on the online setting where risky violations of the safety budget are likely to be incurred during training. Besides, in many real-world applications, the learned policy is required to respond to dynamically determined safety budgets (i.e., constraint threshold) in real time. In this paper, we target at the above real-time budget constraint problem under the offline setting, and propose Trajectory-based REal-time Budget Inference (TREBI) as a novel solution that approaches this problem from the perspective of trajectory distribution. Theoretically, we prove an error bound of the estimation on the episodic reward and cost under the offline setting and thus provide a performance guarantee for TREBI. Empirical results on a wide range of simulation tasks and a real-world large-scale advertising application demonstrate the capability of TREBI in solving real-time budget constraint problems under offline settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2087070731",
                        "name": "Qian Lin"
                    },
                    {
                        "authorId": "2065096762",
                        "name": "Bo Tang"
                    },
                    {
                        "authorId": "2109687122",
                        "name": "Zifan Wu"
                    },
                    {
                        "authorId": "2155747980",
                        "name": "Chao Yu"
                    },
                    {
                        "authorId": "2218736604",
                        "name": "Shangqin Mao"
                    },
                    {
                        "authorId": "2409611",
                        "name": "Qianlong Xie"
                    },
                    {
                        "authorId": "2144803534",
                        "name": "Xingxing Wang"
                    },
                    {
                        "authorId": "49370704",
                        "name": "Dong Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "45672900200e6bbbf746a7cb6e7776302cc44a78",
                "externalIds": {
                    "DBLP": "journals/tac/GreeneDND23",
                    "DOI": "10.1109/TAC.2022.3194040",
                    "CorpusId": 251144097
                },
                "corpusId": 251144097,
                "publicationVenue": {
                    "id": "1283a59c-0d1f-48c3-81d7-02172f597e70",
                    "name": "IEEE Transactions on Automatic Control",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Autom Control"
                    ],
                    "issn": "0018-9286",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=9"
                },
                "url": "https://www.semanticscholar.org/paper/45672900200e6bbbf746a7cb6e7776302cc44a78",
                "title": "Approximate Optimal Trajectory Tracking With Sparse Bellman Error Extrapolation",
                "abstract": "This article provides an approximate online adaptive solution to the infinite-horizon optimal tracking problem for control-affine continuous-time nonlinear systems with uncertain drift dynamics. A model-based approximate dynamic programming (ADP) approach, which is facilitated using a concurrent learning-based system identifier, approximates the optimal value function. To reduce the computational complexity of model-based ADP, the state space is segmented into user-defined segments (i.e., regions). Off-policy trajectories are selected within each segment to facilitate learning of the value function weight estimates; this process is called Bellman error (BE) extrapolation. Within certain segments of the state space, sparse neural networks are used to reduce the computational expense of BE extrapolation. Discontinuities occur in the weight update laws since different groupings of extrapolated BE trajectories are active in certain regions of the state space. A Lyapunov-like stability analysis is presented to prove boundedness of the overall system in the presence of discontinuities. Simulation results are included to demonstrate the performance and validity of the developed method. The simulation results demonstrate that using the sparse, switched BE extrapolation method developed in this article reduces the computation time by 85.6% when compared to the traditional BE extrapolation method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1564561074",
                        "name": "Max L. Greene"
                    },
                    {
                        "authorId": "40896029",
                        "name": "Patryk Deptula"
                    },
                    {
                        "authorId": "19203200",
                        "name": "Scott A. Nivison"
                    },
                    {
                        "authorId": "2054161655",
                        "name": "W. E. Dixon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, model-based methods encounter with model bias caused by the difference between the trained model and real environment [25], especially when the environment has high-dimensional states and complex dynamics.",
                "However, model-based methods are limited by model bias, as previous work [25] emphasized."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "a5aa2d83b8aa3d28b9dc4d62cd9e6850025e4f45",
                "externalIds": {
                    "DOI": "10.1109/FRSE58934.2023.00028",
                    "CorpusId": 261713445
                },
                "corpusId": 261713445,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a5aa2d83b8aa3d28b9dc4d62cd9e6850025e4f45",
                "title": "Multi-Step Hindsight Experience Replay with Bias Reduction for Efficient Multi-Goal Reinforcement Learning",
                "abstract": "Multi-goal reinforcement learning has emerged as a powerful approach for planning and robot manipulation tasks, but it faces challenges such as sparse rewards and sample inefficiency. Hindsight Experience Replay (HER) has been proposed as a solution to these challenges by relabeling goals, but it still requires a large number of samples and significant computation. To address these issues, we propose Multi-step Hindsight Experience Replay (MHER), which incorporates multi-step relabeling to improve sample efficiency. Despite the advantages of $n -$step relabeling, we theoretically and experimentally prove the off-policy $n -$step bias introduced by $n -$step relabeling may lead to poor performance in many environments. To address this issue, two bias-reduced MHER algorithms, MHER $( \\lambda )$ and Model-based MHER (MMHER) are presented. MHER $( \\lambda )$ exploits the $\\lambda$ return while MMHER benefits from model-based value expansions. Experimental results on numerous multi-goal robotic tasks show that our solutions can successfully alleviate the off-policy $n -$step bias and achieve significantly higher sample efficiency than previous multi-goal RL baselines with little additional computation beyond HER.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2116465145",
                        "name": "Yu Yang"
                    },
                    {
                        "authorId": "145094495",
                        "name": "Rui Yang"
                    },
                    {
                        "authorId": "2008151131",
                        "name": "Jiafei Lyu"
                    },
                    {
                        "authorId": "30411824",
                        "name": "Jiangpeng Yan"
                    },
                    {
                        "authorId": "2072689111",
                        "name": "Feng Luo"
                    },
                    {
                        "authorId": "2061549073",
                        "name": "Dijun Luo"
                    },
                    {
                        "authorId": "2127382771",
                        "name": "Xiu Li"
                    },
                    {
                        "authorId": "2117007545",
                        "name": "Lanqing Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The hyper-parameters are kept the same with the MBPO baseline Janner et al. (2019) across all domains and are summarized as in Table 2.",
                "We compare MEX-MB with MBPO (Janner et al., 2019), where our method differs from MBPO only in the inclusion of the value gradient in (7.3) during model updates.",
                ", 2018) and MBPO (Janner et al., 2019) to design practical versions of MEX in model-free and model-based fashion, respectively.",
                "We compare MEX-MB with MBPO (Janner et al., 2019), where our method differs from MBPO only in the inclusion of the value gradient in (7.",
                "Moving beyond theory and into practice, we adapt famous RL baselines TD3 (Fujimoto et al., 2018) and MBPO (Janner et al., 2019) to design practical versions of MEX in model-free and model-based fashion, respectively.",
                "Notably, in the sparse reward settings, MEX-MB excels at achieving the goal velocity and outperforms MBPO by a stable margin."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4d1e15f03bc4bd07533ec1b20741134d2efb0608",
                "externalIds": {
                    "ArXiv": "2305.18258",
                    "DBLP": "journals/corr/abs-2305-18258",
                    "DOI": "10.48550/arXiv.2305.18258",
                    "CorpusId": 258960380
                },
                "corpusId": 258960380,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4d1e15f03bc4bd07533ec1b20741134d2efb0608",
                "title": "One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration",
                "abstract": "In online reinforcement learning (online RL), balancing exploration and exploitation is crucial for finding an optimal policy in a sample-efficient way. To achieve this, existing sample-efficient online RL algorithms typically consist of three components: estimation, planning, and exploration. However, in order to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as optimization within data-dependent level-sets or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called \\textit{Maximize to Explore} (\\texttt{MEX}), which only needs to optimize \\emph{unconstrainedly} a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that \\texttt{MEX} achieves a sublinear regret with general function approximations for Markov decision processes (MDP) and is further extendable to two-player zero-sum Markov games (MG). Meanwhile, we adapt deep RL baselines to design practical versions of \\texttt{MEX}, in both model-free and model-based manners, which can outperform baselines by a stable margin in various MuJoCo environments with sparse rewards. Compared with existing sample-efficient online RL algorithms with general function approximations, \\texttt{MEX} achieves similar sample efficiency while enjoying a lower computational cost and is more compatible with modern deep RL methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2280742",
                        "name": "Zhihan Liu"
                    },
                    {
                        "authorId": "2166932409",
                        "name": "Miao Lu"
                    },
                    {
                        "authorId": "1380008340",
                        "name": "Wei Xiong"
                    },
                    {
                        "authorId": "2064919742",
                        "name": "Han Zhong"
                    },
                    {
                        "authorId": "1752767710",
                        "name": "Haotian Hu"
                    },
                    {
                        "authorId": "2145522248",
                        "name": "Shenao Zhang"
                    },
                    {
                        "authorId": "2148276424",
                        "name": "Sirui Zheng"
                    },
                    {
                        "authorId": "150358650",
                        "name": "Zhuoran Yang"
                    },
                    {
                        "authorId": "50218397",
                        "name": "Zhaoran Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another line of research aiming at improving the sample efficiency in continuous control tasks sets their focus on learning a dynamics model of the environment [86, 9, 12, 44, 19, 57, 36, 97].",
                "This is similar in spirit to model-based methods (e.g., MBPO [44]) and REDQ [11] as they\nusually employ a large update-to-data (UTD) ratio, i.e., update the critic multiple times by sampling with bootstrapping (the sampled batch is different each time).",
                "We note that 300K is a typical interaction step adopted widely in prior work [11, 44, 36] for examining sample efficiency.",
                "They achieve this by alleviating the overestimation bias in value estimate [29, 56, 50, 64], using high update-to-data (UTD) ratio [11, 41], adopting model-based methods [44, 51, 71, 102], etc.",
                "Though 300K or 500K (or even fewer) online interactions are widely adopted for examining sample efficiency in model-based methods [44, 71, 51, 101] and REDQ [11], one may wonder whether our method can consistently improve sample efficiency with longer online interactions.",
                "The results even match the performance of MBPO [44] (around 73K)."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "8a84ee6294bc7f88d0343c8615a12f1c763209bd",
                "externalIds": {
                    "ArXiv": "2305.18443",
                    "DBLP": "journals/corr/abs-2305-18443",
                    "DOI": "10.48550/arXiv.2305.18443",
                    "CorpusId": 258967871
                },
                "corpusId": 258967871,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8a84ee6294bc7f88d0343c8615a12f1c763209bd",
                "title": "Off-Policy RL Algorithms Can be Sample-Efficient for Continuous Control via Sample Multiple Reuse",
                "abstract": "Sample efficiency is one of the most critical issues for online reinforcement learning (RL). Existing methods achieve higher sample efficiency by adopting model-based methods, Q-ensemble, or better exploration mechanisms. We, instead, propose to train an off-policy RL agent via updating on a fixed sampled batch multiple times, thus reusing these samples and better exploiting them within a single optimization loop. We name our method sample multiple reuse (SMR). We theoretically show the properties of Q-learning with SMR, e.g., convergence. Furthermore, we incorporate SMR with off-the-shelf off-policy RL algorithms and conduct experiments on a variety of continuous control benchmarks. Empirical results show that SMR significantly boosts the sample efficiency of the base methods across most of the evaluated tasks without any hyperparameter tuning or additional tricks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2008151131",
                        "name": "Jiafei Lyu"
                    },
                    {
                        "authorId": "2187301367",
                        "name": "Le Wan"
                    },
                    {
                        "authorId": "2265693",
                        "name": "Zongqing Lu"
                    },
                    {
                        "authorId": "2180539270",
                        "name": "Xiu Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired by [20], [10] and [25], we present an MBRL algorithm that uses an ensemble of probabilistic networks to learn a predictive model.",
                "Our proposal shares multiple aspects already mentioned in [20], [10]; we simplify these proposals in some aspects.",
                "For instance, the authors in [20] implemented branched rollouts with k-steps predictions.",
                "This could be a bottleneck deteriorating the performance of the policy, creating a limitation on MBRL methods to perform worse or converge to less optimal solutions than their model-free counterparts [48], [20]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "6281489f4a99144336afd53b3378934fbc76a36e",
                "externalIds": {
                    "DBLP": "conf/icra/ValenciaJLHLTGLMW23",
                    "DOI": "10.1109/ICRA48891.2023.10160983",
                    "CorpusId": 259338691
                },
                "corpusId": 259338691,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6281489f4a99144336afd53b3378934fbc76a36e",
                "title": "Comparison of Model-Based and Model-Free Reinforcement Learning for Real-World Dexterous Robotic Manipulation Tasks",
                "abstract": "Model Free Reinforcement Learning (MFRL) has shown significant promise for learning dexterous robotic manipulation tasks, at least in simulation. However, the high number of samples, as well as the long training times, prevent MFRL from scaling to complex real-world tasks. Model- Based Reinforcement Learning (MBRL) emerges as a potential solution that, in theory, can improve the data efficiency of MFRL approaches. This could drastically reduce the training time of MFRL, and increase the application of RL for real- world robotic tasks. This article presents a study on the feasibility of using the state-of-the-art MBRL to improve the training time for two real-world dexterous manipulation tasks. The evaluation is conducted on a real low-cost robot gripper where the predictive model and the control policy are learned from scratch. The results indicate that MBRL is capable of learning accurate models of the world, but does not show clear improvements in learning the control policy in the real world as prior literature suggests should be expected.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152760508",
                        "name": "David Valencia"
                    },
                    {
                        "authorId": "2221117182",
                        "name": "John Jia"
                    },
                    {
                        "authorId": "2221150032",
                        "name": "Raymond Li"
                    },
                    {
                        "authorId": "2221120221",
                        "name": "Alex Hayashi"
                    },
                    {
                        "authorId": "2221119090",
                        "name": "Megan Lecchi"
                    },
                    {
                        "authorId": "2221119788",
                        "name": "Reuel Terezakis"
                    },
                    {
                        "authorId": "47268959",
                        "name": "Trevor Gee"
                    },
                    {
                        "authorId": "2646612",
                        "name": "Minas Liarokapis"
                    },
                    {
                        "authorId": "2152761106",
                        "name": "Bruce A. MacDonald"
                    },
                    {
                        "authorId": "87380174",
                        "name": "Henry Williams"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2c5ce661e1a3f0cc47fc19183eb60bbd787c0658",
                "externalIds": {
                    "DBLP": "conf/icra/UdathaLD23",
                    "DOI": "10.1109/ICRA48891.2023.10161418",
                    "CorpusId": 259338561
                },
                "corpusId": 259338561,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2c5ce661e1a3f0cc47fc19183eb60bbd787c0658",
                "title": "Reinforcement Learning with Probabilistically Safe Control Barrier Functions for Ramp Merging",
                "abstract": "Prior work has looked at applying reinforcement learning (RL) approaches to autonomous driving scenarios, but the safety of the algorithm is often compromised due to instability or the presence of ill-defined reward functions. With the use of control barrier functions embedded into the RL policy, we arrive at safe policies to optimize the performance of the autonomous driving vehicle through the advantage of a safety layer over the RL methods to ease the design of reward functions. However, control barrier functions need a good approximation of the model of the system. We use probabilistic control barrier functions [4] to account for model uncertainty. Our Safety-Assured Policy Optimization - Ramp Merging (SAPO-RM) algorithm is implemented online in the CARLA [1] Simulator and offline on the US I-80 dataset extracted from the NGSIM Database provided by NHTSA [2]. We further test the algorithm and perform ablation studies of it on the US-101 and exi-D datasets to compare the approaches. The proposed algorithm can also be applied to other driving scenarios by changing the reward and safety constraints.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2003032642",
                        "name": "Soumith Udatha"
                    },
                    {
                        "authorId": null,
                        "name": "Yiwei Lyu"
                    },
                    {
                        "authorId": "151620519",
                        "name": "J. Dolan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[27] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",
                "The performance guarantee of a policy trained with imaginary transitions from an inaccurate dynamics model has been analyzed in prior Dynastyle [61, 62, 64] model-based RL algorithms [41, 27, 58].",
                "VGDF: We use a five-layer MLP with 200 units as the dynamics model using Swish activation following prior works [9, 27].",
                "However, the learned model can be inaccurate, which results in model exploitation and performance degradation [27, 28].",
                "The training of the dynamics model ensemble follows prior works [9, 27] with the MLE loss.",
                "Unlike prior works in model-based RL [27, 58] that utilize",
                "To provide rigorous interpretations for the results, we derive a performance guarantee for the dynamicsguided methods, which mainly build on the theories proposed in prior methods [27, 14]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "bcf4b63c32f20aab48355276fa90782db3b6321a",
                "externalIds": {
                    "ArXiv": "2305.17625",
                    "DBLP": "journals/corr/abs-2305-17625",
                    "DOI": "10.48550/arXiv.2305.17625",
                    "CorpusId": 258960613
                },
                "corpusId": 258960613,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bcf4b63c32f20aab48355276fa90782db3b6321a",
                "title": "Cross-Domain Policy Adaptation via Value-Guided Data Filtering",
                "abstract": "Generalizing policies across different domains with dynamics mismatch poses a significant challenge in reinforcement learning. For example, a robot learns the policy in a simulator, but when it is deployed in the real world, the dynamics of the environment may be different. Given the source and target domain with dynamics mismatch, we consider the online dynamics adaptation problem, in which case the agent can access sufficient source domain data while online interactions with the target domain are limited. Existing research has attempted to solve the problem from the dynamics discrepancy perspective. In this work, we reveal the limitations of these methods and explore the problem from the value difference perspective via a novel insight on the value consistency across domains. Specifically, we present the Value-Guided Data Filtering (VGDF) algorithm, which selectively shares transitions from the source domain based on the proximity of paired value targets across the two domains. Empirical results on various environments with kinematic and morphology shifts demonstrate that our method achieves superior performance compared to prior approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153294049",
                        "name": "Kang Xu"
                    },
                    {
                        "authorId": "150944133",
                        "name": "Chenjia Bai"
                    },
                    {
                        "authorId": "2125106047",
                        "name": "Xiaoteng Ma"
                    },
                    {
                        "authorId": "49370704",
                        "name": "Dong Wang"
                    },
                    {
                        "authorId": "2112632513",
                        "name": "Bingyan Zhao"
                    },
                    {
                        "authorId": "2118454138",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2192821449",
                        "name": "Xuelong Li"
                    },
                    {
                        "authorId": "153021098",
                        "name": "Wei Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1f56856748285c3fdbd0e8ee97d678123b9f17d5",
                "externalIds": {
                    "ArXiv": "2305.18427",
                    "CorpusId": 258967542
                },
                "corpusId": 258967542,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1f56856748285c3fdbd0e8ee97d678123b9f17d5",
                "title": "Interpretable Reward Redistribution in Reinforcement Learning: A Causal Approach",
                "abstract": "A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Reward redistribution serves as a solution to re-assign credits for each time step from observed sequences. While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable reward redistribution and preserving policy invariance. In this paper, we start by studying the role of causal generative models in reward redistribution by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called Generative Return Decomposition (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then, GRD makes use of the identified causal generative model to form a compact representation to train policy over the most favorable subspace of the state space of the agent. Theoretically, we show that the unobservable Markovian reward function is identifiable, as well as the underlying causal structure and causal models. Experimental results show that our method outperforms state-of-the-art methods and the provided visualization further demonstrates the interpretability of our method. The source code will be released at \\href{https://github.com/ReedZyd/GRD_NeurIPS2023}{https://github.com/ReedZyd/GRD\\_NeurIPS2023}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2213717624",
                        "name": "Yudi Zhang"
                    },
                    {
                        "authorId": "1390662136",
                        "name": "Yali Du"
                    },
                    {
                        "authorId": "1938684",
                        "name": "Biwei Huang"
                    },
                    {
                        "authorId": "2142663126",
                        "name": "Ziyan Wang"
                    },
                    {
                        "authorId": "48093888",
                        "name": "Jun Wang"
                    },
                    {
                        "authorId": "2055723958",
                        "name": "Meng Fang"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Reinforcement learning [Sutton and Barto, 2018] approaches are classified as model-free or model-based [Janner et al., 2019, Ha and Schmidhuber, 2018, Osband and Van Roy, 2014], dependent on if they attempt to explicitly try to learn the underlying transition dynamics an agent is subject to.",
                "[2013], including introductions of model-based variants [Janner et al., 2019].",
                "This was later extended to more tractable formulations and structured uncertainty sets in Tessler et al. [2019], Mankowitz et al. [2019], Pinto et al. [2017], Zhang et al. [2021], Tamar et al. [2013], including introductions of model-based variants [Janner et al., 2019]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6c22ad8a27d9ceaca7fa89983614e341bf367966",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-17552",
                    "ArXiv": "2305.17552",
                    "DOI": "10.48550/arXiv.2305.17552",
                    "CorpusId": 258960468
                },
                "corpusId": 258960468,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6c22ad8a27d9ceaca7fa89983614e341bf367966",
                "title": "Online Nonstochastic Model-Free Reinforcement Learning",
                "abstract": "In this work, we explore robust model-free reinforcement learning algorithms for environments that may be dynamic or even adversarial. Conventional state-based policies fail to accommodate the challenge imposed by the presence of unmodeled disturbances in such settings. Additionally, optimizing linear state-based policies pose obstacle for efficient optimization, leading to nonconvex objectives even in benign environments like linear dynamical systems. Drawing inspiration from recent advancements in model-based control, we introduce a novel class of policies centered on disturbance signals. We define several categories of these signals, referred to as pseudo-disturbances, and corresponding policy classes based on them. We provide efficient and practical algorithms for optimizing these policies. Next, we examine the task of online adaptation of reinforcement learning agents to adversarial disturbances. Our methods can be integrated with any black-box model-free approach, resulting in provable regret guarantees if the underlying dynamics is linear. We evaluate our method over different standard RL benchmarks and demonstrate improved robustness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "68976882",
                        "name": "Udaya Ghai"
                    },
                    {
                        "authorId": "2110047443",
                        "name": "Arushi Gupta"
                    },
                    {
                        "authorId": "3413958",
                        "name": "Wenhan Xia"
                    },
                    {
                        "authorId": "2109143637",
                        "name": "Karan Singh"
                    },
                    {
                        "authorId": "34840427",
                        "name": "Elad Hazan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(1) MBPO [19] is a modelbased RL method that learns a standard one-step dynamics model and uses actor-critic methods to plan in the model.",
                "Similar to MBPO, we pre-train dreamer\u2019s dynamics branch with offline data before the online phase.",
                "We note that SF is performing reasonably well, likely because the method also reduces the compounding error compared to MBPO, and it has privileged information.",
                "Model-based RL arises as a natural fit for disentangling dynamics and rewards [33, 10, 17, 19, 20].",
                "PETS adapts faster than MBPO but performs worse than our method as it suffers from compounding error.",
                "MBPO slowly catches up with our performance with more samples, since it still needs to learn the Q function from scratch even with the dynamics branch trained.",
                "11, when we curate the labeling process of the privileged dataset to satisfy the in-distribution assumption, CQL and SF receive a significant performance boost, while the performance\nof our method and MBPO are unaffected as neither algorithm depends on the offline objectives.",
                "MBPO adapts at a slower rate since higher dimensional observation and longer horizons increase the compounding error of model-based methods.",
                "We pre-train the dynamics model for MBPO on the offline dataset."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "e325dd94c54ce753534fd2571cf89cff129e32f1",
                "externalIds": {
                    "ArXiv": "2305.17250",
                    "DBLP": "journals/corr/abs-2305-17250",
                    "DOI": "10.48550/arXiv.2305.17250",
                    "CorpusId": 258959224
                },
                "corpusId": 258959224,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e325dd94c54ce753534fd2571cf89cff129e32f1",
                "title": "Self-Supervised Reinforcement Learning that Transfers using Random Features",
                "abstract": "Model-free reinforcement learning algorithms have exhibited great potential in solving single-task sequential decision-making problems with high-dimensional observations and long horizons, but are known to be hard to generalize across tasks. Model-based RL, on the other hand, learns task-agnostic models of the world that naturally enables transfer across different reward functions, but struggles to scale to complex environments due to the compounding error. To get the best of both worlds, we propose a self-supervised reinforcement learning method that enables the transfer of behaviors across tasks with different rewards, while circumventing the challenges of model-based RL. In particular, we show self-supervised pre-training of model-free reinforcement learning with a number of random features as rewards allows implicit modeling of long-horizon environment dynamics. Then, planning techniques like model-predictive control using these implicit models enable fast adaptation to problems with new reward functions. Our method is self-supervised in that it can be trained on offline datasets without reward labels, but can then be quickly deployed on new tasks. We validate that our proposed method enables transfer across tasks on a variety of manipulation and locomotion domains in simulation, opening the door to generalist decision-making agents.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "8786274",
                        "name": "Boyuan Chen"
                    },
                    {
                        "authorId": "2118160513",
                        "name": "Chuning Zhu"
                    },
                    {
                        "authorId": "33932184",
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "authorId": "1776230",
                        "name": "K. Zhang"
                    },
                    {
                        "authorId": "144150274",
                        "name": "Abhishek Gupta"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also would like to emphasize that the training time of our approach is much less than that of MBPO (4 hours v.s. 3 days).",
                ", MBPO (Janner et al., 2019) on the phases criss-cross network environment (Fig.",
                "Dyna-type approaches have been successfully used in model-based online reinforcement learning for policy optimization, including several state-of-the-art algorithms, including ME-TRPO (Kurutach et al., 2018), SLBO (Luo et al., 2019), MB-MPO (Clavera et al., 2018), MBPO (Janner et al., 2019), MOPO (Yu et al., 2020).",
                "\u2026have been successfully used in model-based online reinforcement learning for policy optimization, including several state-of-the-art algorithms, including ME-TRPO (Kurutach et al., 2018), SLBO (Luo et al., 2019), MB-MPO (Clavera et al., 2018), MBPO (Janner et al., 2019), MOPO (Yu et al., 2020).",
                "We also compare our algorithm SAC-ASG with state-of-art Dyna-type model-based approaches, i.e., MBPO (Janner et al., 2019) on the phases criss-cross network environment (Fig."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "cc563dcbacdb5474de52535b1de384aebb30b597",
                "externalIds": {
                    "ArXiv": "2305.16483",
                    "DBLP": "journals/corr/abs-2305-16483",
                    "DOI": "10.48550/arXiv.2305.16483",
                    "CorpusId": 258947309
                },
                "corpusId": 258947309,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cc563dcbacdb5474de52535b1de384aebb30b597",
                "title": "Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks",
                "abstract": "This paper considers a class of reinforcement learning problems, which involve systems with two types of states: stochastic and pseudo-stochastic. In such systems, stochastic states follow a stochastic transition kernel while the transitions of pseudo-stochastic states are deterministic given the stochastic states/transitions. We refer to such systems as mixed systems, which are widely used in various applications, including manufacturing systems, communication networks, and queueing networks. We propose a sample efficient RL method that accelerates learning by generating augmented data samples. The proposed algorithm is data-driven and learns the policy from data samples from both real and augmented samples. This method significantly improves learning by reducing the sample complexity such that the dataset only needs to have sufficient coverage of the stochastic states. We analyze the sample complexity of the proposed method under Fitted Q Iteration (FQI) and demonstrate that the optimality gap decreases as $\\tilde{\\mathcal{O}}(\\sqrt{{1}/{n}}+\\sqrt{{1}/{m}}),$ where $n$ is the number of real samples and $m$ is the number of augmented samples per real sample. It is important to note that without augmented samples, the optimality gap is $\\tilde{\\mathcal{O}}(1)$ due to insufficient data coverage of the pseudo-stochastic states. Our experimental results on multiple queueing network applications confirm that the proposed method indeed significantly accelerates learning in both deep Q-learning and deep policy gradient.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "66022476",
                        "name": "Honghao Wei"
                    },
                    {
                        "authorId": "2146074389",
                        "name": "Xin Liu"
                    },
                    {
                        "authorId": "47825052",
                        "name": "Weina Wang"
                    },
                    {
                        "authorId": "2142478228",
                        "name": "Lei Ying"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare TOM within the widely used MBPO framework (Janner et al., 2019) to standard MLE model learning, and two representative recent approaches that target the MBRL objective mismatch problem."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5811d7789de6f667b10095ed45dc37b5931ff532",
                "externalIds": {
                    "DBLP": "conf/l4dc/MaSYBJ23",
                    "ArXiv": "2305.12663",
                    "DOI": "10.48550/arXiv.2305.12663",
                    "CorpusId": 258832902
                },
                "corpusId": 258832902,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5811d7789de6f667b10095ed45dc37b5931ff532",
                "title": "TOM: Learning Policy-Aware Models for Model-Based Reinforcement Learning via Transition Occupancy Matching",
                "abstract": "Standard model-based reinforcement learning (MBRL) approaches fit a transition model of the environment to all past experience, but this wastes model capacity on data that is irrelevant for policy improvement. We instead propose a new\"transition occupancy matching\"(TOM) objective for MBRL model learning: a model is good to the extent that the current policy experiences the same distribution of transitions inside the model as in the real environment. We derive TOM directly from a novel lower bound on the standard reinforcement learning objective. To optimize TOM, we show how to reduce it to a form of importance weighted maximum-likelihood estimation, where the automatically computed importance weights identify policy-relevant past experiences from a replay buffer, enabling stable optimization. TOM thus offers a plug-and-play model learning sub-routine that is compatible with any backbone MBRL algorithm. On various Mujoco continuous robotic control tasks, we show that TOM successfully focuses model learning on policy-relevant experience and drives policies faster to higher task rewards than alternative model learning approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2130215451",
                        "name": "Yecheng Jason Ma"
                    },
                    {
                        "authorId": "2218335793",
                        "name": "K. Sivakumar"
                    },
                    {
                        "authorId": "2168557442",
                        "name": "Jason Yan"
                    },
                    {
                        "authorId": "1697444",
                        "name": "O. Bastani"
                    },
                    {
                        "authorId": "144348441",
                        "name": "Dinesh Jayaraman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, in model-based RL [37,38], the state transition probability (and reward function) should be learned for planning the optimal action.",
                "Here, we consider the problem with the explicit definition of \u03c0 as probability distribution, instead of Q-learning [36], which indirectly designs \u03c0 fromQ(s, a), or model-based RL [37,38], which learns pe and obtains the optimal a through planning."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f3e2d9a178a353f3596b45e7360a71648ce1bbf8",
                "externalIds": {
                    "DBLP": "journals/ar/KobayashiA23",
                    "DOI": "10.1080/01691864.2023.2208634",
                    "CorpusId": 258681123
                },
                "corpusId": 258681123,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f3e2d9a178a353f3596b45e7360a71648ce1bbf8",
                "title": "Design of restricted normalizing flow towards arbitrary stochastic policy with computational efficiency",
                "abstract": "This paper proposes a new design method for a stochastic control policy using a normalizing flow (NF). In reinforcement learning (RL), the policy is usually modeled as a distribution model with trainable parameters. When this parameterization has less expressiveness, it would fail to acquiring the optimal policy. A mixture model has capability of a universal approximation, but it with too much redundancy increases the computational cost, which can become a bottleneck when considering the use of real-time robot control. As another approach, NF, which is with additional parameters for invertible transformation from a simple stochastic model as a base, is expected to exert high expressiveness and lower computational cost. However, NF cannot compute its mean analytically due to complexity of the invertible transformation, and it lacks reliability because it retains stochastic behaviors after deployment for robot controller. This paper therefore designs a restricted NF (RNF) that achieves an analytic mean by appropriately restricting the invertible transformation. In addition, the expressiveness impaired by this restriction is regained using bimodal student-t distribution as its base, so-called Bit-RNF. In RL benchmarks, Bit-RNF policy outperformed the previous models. Finally, a real robot experiment demonstrated the applicability of Bit-RNF policy to real world. GRAPHICAL ABSTRACT",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111084752",
                        "name": "Taisuke Kobayashi"
                    },
                    {
                        "authorId": "66696177",
                        "name": "Takumi Aotani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This concept can be related to the recent strand of model-based policy optimization literature that simulates short trajectories in an estimated model of the considered domain to update the parameters of the policy (Janner et al., 2019; Nguyen et al., 2018; Bhatia et al., 2022)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3a003de5c21f1a2f028dcff14a0231d3cf67db77",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-04361",
                    "ArXiv": "2305.04361",
                    "DOI": "10.48550/arXiv.2305.04361",
                    "CorpusId": 258557253
                },
                "corpusId": 258557253,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3a003de5c21f1a2f028dcff14a0231d3cf67db77",
                "title": "Truncating Trajectories in Monte Carlo Reinforcement Learning",
                "abstract": "In Reinforcement Learning (RL), an agent acts in an unknown environment to maximize the expected cumulative discounted sum of an external reward signal, i.e., the expected return. In practice, in many tasks of interest, such as policy optimization, the agent usually spends its interaction budget by collecting episodes of fixed length within a simulator (i.e., Monte Carlo simulation). However, given the discounted nature of the RL objective, this data collection strategy might not be the best option. Indeed, the rewards taken in early simulation steps weigh exponentially more than future rewards. Taking a cue from this intuition, in this paper, we design an a-priori budget allocation strategy that leads to the collection of trajectories of different lengths, i.e., truncated. The proposed approach provably minimizes the width of the confidence intervals around the empirical estimates of the expected return of a policy. After discussing the theoretical properties of our method, we make use of our trajectory truncation mechanism to extend Policy Optimization via Importance Sampling (POIS, Metelli et al., 2018) algorithm. Finally, we conduct a numerical comparison between our algorithm and POIS: the results are consistent with our theory and show that an appropriate truncation of the trajectories can succeed in improving performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1785396322",
                        "name": "Riccardo Poiani"
                    },
                    {
                        "authorId": "2141401054",
                        "name": "A. Metelli"
                    },
                    {
                        "authorId": "1792167",
                        "name": "Marcello Restelli"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c6773fc39a83dd1512016571f486e7012baaf9ea",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-02749",
                    "ArXiv": "2305.02749",
                    "DOI": "10.48550/arXiv.2305.02749",
                    "CorpusId": 258479970
                },
                "corpusId": 258479970,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c6773fc39a83dd1512016571f486e7012baaf9ea",
                "title": "Explainable Reinforcement Learning via a Causal World Model",
                "abstract": "Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2116678812",
                        "name": "Zhongwei Yu"
                    },
                    {
                        "authorId": "2135060971",
                        "name": "Jingqing Ruan"
                    },
                    {
                        "authorId": "144185398",
                        "name": "Dengpeng Xing"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Forward dynamics models are an integral component of several model-based RL algorithms (Janner et al., 2019; Rajeswaran et al., 2020; Hafner et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7572bf46bd1d895c92f367b2b46c205cfeb2e967",
                "externalIds": {
                    "ArXiv": "2305.02968",
                    "DBLP": "journals/corr/abs-2305-02968",
                    "DOI": "10.48550/arXiv.2305.02968",
                    "CorpusId": 258480255
                },
                "corpusId": 258480255,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7572bf46bd1d895c92f367b2b46c205cfeb2e967",
                "title": "Masked Trajectory Models for Prediction, Representation, and Control",
                "abstract": "We introduce Masked Trajectory Models (MTM) as a generic abstraction for sequential decision making. MTM takes a trajectory, such as a state-action sequence, and aims to reconstruct the trajectory conditioned on random subsets of the same trajectory. By training with a highly randomized masking pattern, MTM learns versatile networks that can take on different roles or capabilities, by simply choosing appropriate masks at inference time. For example, the same MTM network can be used as a forward dynamics model, inverse dynamics model, or even an offline RL agent. Through extensive experiments in several continuous control tasks, we show that the same MTM network -- i.e. same weights -- can match or outperform specialized networks trained for the aforementioned capabilities. Additionally, we find that state representations learned by MTM can significantly accelerate the learning speed of traditional RL algorithms. Finally, in offline RL benchmarks, we find that MTM is competitive with specialized offline RL algorithms, despite MTM being a generic self-supervised learning method without any explicit RL components. Code is available at https://github.com/facebookresearch/mtm",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108864104",
                        "name": "Philipp Wu"
                    },
                    {
                        "authorId": "2905057",
                        "name": "Arjun Majumdar"
                    },
                    {
                        "authorId": "2059203883",
                        "name": "Kevin Stone"
                    },
                    {
                        "authorId": "1491144944",
                        "name": "Yixin Lin"
                    },
                    {
                        "authorId": "2080746",
                        "name": "Igor Mordatch"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    },
                    {
                        "authorId": "19275599",
                        "name": "A. Rajeswaran"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b1a115d55c8ceced3b003e2cd384e70680e291bb",
                "externalIds": {
                    "DOI": "10.1016/j.jobe.2023.106852",
                    "CorpusId": 258815720
                },
                "corpusId": 258815720,
                "publicationVenue": {
                    "id": "edc86f1a-0bc0-4f32-8b0d-9134204610bf",
                    "name": "Journal of Building Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "J Build Eng",
                        "J build eng",
                        "Journal of building engineering"
                    ],
                    "issn": "2352-7102",
                    "url": "https://www.journals.elsevier.com/leukemia-research-reports/",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/23527102"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b1a115d55c8ceced3b003e2cd384e70680e291bb",
                "title": "Comparative study of model-based and model-free reinforcement learning control performance in HVAC systems",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2114086529",
                        "name": "Cheng Gao"
                    },
                    {
                        "authorId": "2155681607",
                        "name": "Dan Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "19437fb353dc13f85f9d2ba577a302d7c8b95f79",
                "externalIds": {
                    "DBLP": "journals/eswa/KimKA23",
                    "DOI": "10.1016/j.eswa.2023.120493",
                    "CorpusId": 258932447
                },
                "corpusId": 258932447,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/19437fb353dc13f85f9d2ba577a302d7c8b95f79",
                "title": "Evolving population method for real-time reinforcement learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3396273",
                        "name": "Man-Je Kim"
                    },
                    {
                        "authorId": "2109319303",
                        "name": "Jun Suk Kim"
                    },
                    {
                        "authorId": "2152406648",
                        "name": "Chang Wook Ahn"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Because such a model can be used for planning (searching for a good policy without interacting with the environment), model-based methods have the potential to be substantially more sample efficient than model-free algorithms (Kaiser et al., 2019; Janner et al., 2019), which attempt to find good policies without building a model.",
                "\u2026for planning (searching for a good policy without interacting with the environment), model-based methods have the potential to be substantially more sample efficient than model-free algorithms (Kaiser et al., 2019; Janner et al., 2019), which attempt to find good policies without building a model.",
                "This is an important development since planning has the potential to make model-based methods highly sample efficient (Kaiser et al., 2019; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1fd20eb2cc756165660facd7c5ae528e6431a524",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00477",
                    "ArXiv": "2305.00477",
                    "DOI": "10.48550/arXiv.2305.00477",
                    "CorpusId": 258426894
                },
                "corpusId": 258426894,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1fd20eb2cc756165660facd7c5ae528e6431a524",
                "title": "Posterior Sampling for Deep Reinforcement Learning",
                "abstract": "Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. PSDRL combines efficient uncertainty quantification over latent state space models with a specially tailored continual planning algorithm based on value-function approximation. Extensive experiments on the Atari benchmark show that PSDRL significantly outperforms previous state-of-the-art attempts at scaling up posterior sampling while being competitive with a state-of-the-art (model-based) reinforcement learning method, both in sample efficiency and computational efficiency.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1643872086",
                        "name": "Remo Sasso"
                    },
                    {
                        "authorId": "2105606361",
                        "name": "Michelangelo Conserva"
                    },
                    {
                        "authorId": "2066986443",
                        "name": "Paulo E. Rauber"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are some typical methods such as Dyna [26], Model-based policy optimization (MBPO) [27], Model-based value expansion (MVE) [28], etc\nSome efforts have been made to seek transition pathways via reinforcement learning in real-world applications.",
                "There are some typical methods such as Dyna [26], Model-based policy optimization (MBPO) [27], Model-based value expansion (MVE) [28], etc Some efforts have been made to seek transition pathways via reinforcement learning in real-world applications."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "069699f66744bd6df64d195dbca9f7960d459790",
                "externalIds": {
                    "ArXiv": "2304.12994",
                    "CorpusId": 258309803
                },
                "corpusId": 258309803,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/069699f66744bd6df64d195dbca9f7960d459790",
                "title": "Deep Reinforcement Learning in Finite-Horizon to Explore the Most Probable Transition Pathway",
                "abstract": "In many scientific and engineering problems, noise and nonlinearity are unavoidable, which could induce interesting mathematical problem such as transition phenomena. This paper focuses on efficiently discovering the most probable transition pathway for stochastic dynamical systems employing reinforcement learning. With the Onsager-Machlup action functional theory to quantify rare events in stochastic dynamical systems, finding the most probable pathway is equivalent to solving a variational problem on the action functional. When the action function cannot be explicitly expressed by paths near the reference orbit, the variational problem needs to be converted into an optimal control problem. First, by integrating terminal prediction into the reinforcement learning framework, we develop a Terminal Prediction Deep Deterministic Policy Gradient (TP-DDPG) algorithm to deal with the finite-horizon optimal control issue in a forward way. Next, we present the convergence analysis of our algorithm for the value function in terms of the neural network's approximation error and estimation error. Finally, we conduct various experiments in different dimensions for the transition problems in applications to illustrate the effectiveness of our algorithm.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2157959267",
                        "name": "Jinqiu Guo"
                    },
                    {
                        "authorId": "2072687047",
                        "name": "Ting Gao"
                    },
                    {
                        "authorId": "40075735",
                        "name": "P. Zhang"
                    },
                    {
                        "authorId": "2211479696",
                        "name": "Jinqiao Duan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, most existing work either focuses on 2D environments [1, 4,12,19\u201324,28,30,32,43,48,57,59,63,64,66] or has to make strong assumptions about the accessible information of the underlying environment [2, 7, 26, 34, 35, 42, 46, 47, 53, 69] (e."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f3f1592ff282fbf58864c243510eb4a425f3fc70",
                "externalIds": {
                    "ArXiv": "2304.11470",
                    "DBLP": "conf/cvpr/XueTTYLT23",
                    "DOI": "10.1109/CVPRW59228.2023.00370",
                    "CorpusId": 258298858
                },
                "corpusId": 258298858,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f3f1592ff282fbf58864c243510eb4a425f3fc70",
                "title": "3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes",
                "abstract": "Given a visual scene, humans have strong intuitions about how a scene can evolve over time under given actions. The intuition, often termed visual intuitive physics, is a critical ability that allows us to make effective plans to manipulate the scene to achieve desired outcomes without relying on extensive trial and error. In this paper, we present a framework capable of learning 3D-grounded visual intuitive physics models from videos of complex scenes with fluids. Our method is composed of a conditional Neural Radiance Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction backend, using which we can impose strong relational and structural inductive bias to capture the structure of the underlying environment. Unlike existing intuitive point-based dynamics works that rely on the supervision of dense point trajectory from simulators, we relax the requirements and only assume access to multi-view RGB images and (imperfect) instance masks acquired using color prior. This enables the proposed model to handle scenarios where accurate point estimation and tracking are hard or impossible. We generate datasets including three challenging scenarios involving fluid, granular materials, and rigid objects in the simulation. The datasets do not include any dense particle information so most previous 3D-based intuitive physics pipelines can barely deal with that. We show our model can make long-horizon future predictions by learning from raw images and significantly outperforms models that do not employ an explicit 3D representation space. We also show that once trained, our model can achieve strong generalization in complex scenarios under extrapolate settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "30944025",
                        "name": "Haotian Xue"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    },
                    {
                        "authorId": "1763295",
                        "name": "J. Tenenbaum"
                    },
                    {
                        "authorId": "40657572",
                        "name": "Daniel L. K. Yamins"
                    },
                    {
                        "authorId": "2110483798",
                        "name": "Yunzhu Li"
                    },
                    {
                        "authorId": "1693704",
                        "name": "H. Tung"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This increase in interest can be attributed in part to exciting developments in MBRL [30, 31] and the superior sample complexity of modelbased approaches [28, 34]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5802f94ba38dd06f2893fa027d5f18469a425a8c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-11104",
                    "ArXiv": "2304.11104",
                    "DOI": "10.48550/arXiv.2304.11104",
                    "CorpusId": 258291944
                },
                "corpusId": 258291944,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5802f94ba38dd06f2893fa027d5f18469a425a8c",
                "title": "Approximate Shielding of Atari Agents for Safe Exploration",
                "abstract": "Balancing exploration and conservatism in the constrained setting is an important problem if we are to use reinforcement learning for meaningful tasks in the real world. In this paper, we propose a principled algorithm for safe exploration based on the concept of shielding. Previous approaches to shielding assume access to a safety-relevant abstraction of the environment or a high-fidelity simulator. Instead, our work is based on latent shielding - another approach that leverages world models to verify policy roll-outs in the latent space of a learned dynamics model. Our novel algorithm builds on this previous work, using safety critics and other additional features to improve the stability and farsightedness of the algorithm. We demonstrate the effectiveness of our approach by running experiments on a small set of Atari games with state dependent safety labels. We present preliminary results that show our approximate shielding algorithm effectively reduces the rate of safety violations, and in some cases improves the speed of convergence and quality of the final agent.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2206167880",
                        "name": "Alexander W. Goodall"
                    },
                    {
                        "authorId": "3142000",
                        "name": "F. Belardinelli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Building on these hypotheses, several mitigation strategies, such as model-based data augmentation (Janner et al., 2019), the use of ensembles (Chen et al., 2021), network regularizations (Hiraoka et al., 2021), and periodically reseting the RL agent from scratch while keeping the replay buffer\u2026",
                "Devising such efficient RL algorithm has been an important thread of research in recent years (Janner et al., 2019; Chen et al., 2021; Hiraoka et al., 2021).",
                "However, when done na\u0131\u0308vely, this can lead to worse performance (e.g., on DMC (Nikishin et al., 2022) and on MuJoCo gym (Janner et al., 2019)).",
                "Building on these hypotheses, several mitigation strategies, such as model-based data augmentation (Janner et al., 2019), the use of ensembles (Chen et al."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f04fe5f3f47f5b25e5295c29cdc8b109887f959c",
                "externalIds": {
                    "ArXiv": "2304.10466",
                    "DBLP": "conf/iclr/LiKKL23",
                    "DOI": "10.48550/arXiv.2304.10466",
                    "CorpusId": 258236460
                },
                "corpusId": 258236460,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f04fe5f3f47f5b25e5295c29cdc8b109887f959c",
                "title": "Efficient Deep Reinforcement Learning Requires Regulating Overfitting",
                "abstract": "Deep reinforcement learning algorithms that learn policies by trial-and-error must learn from limited amounts of data collected by actively interacting with the environment. While many prior works have shown that proper regularization techniques are crucial for enabling data-efficient RL, a general understanding of the bottlenecks in data-efficient RL has remained unclear. Consequently, it has been difficult to devise a universal technique that works well across all domains. In this paper, we attempt to understand the primary bottleneck in sample-efficient deep RL by examining several potential hypotheses such as non-stationarity, excessive action distribution shift, and overfitting. We perform thorough empirical analysis on state-based DeepMind control suite (DMC) tasks in a controlled and systematic way to show that high temporal-difference (TD) error on the validation set of transitions is the main culprit that severely affects the performance of deep RL algorithms, and prior methods that lead to good performance do in fact, control the validation TD error to be low. This observation gives us a robust principle for making deep RL efficient: we can hill-climb on the validation TD error by utilizing any form of regularization techniques from supervised learning. We show that a simple online model selection method that targets the validation TD error is effective across state-based DMC and Gym tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "8194287",
                        "name": "Qiyang Li"
                    },
                    {
                        "authorId": "1488785534",
                        "name": "Aviral Kumar"
                    },
                    {
                        "authorId": "2064497515",
                        "name": "Ilya Kostrikov"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2137755655e1889b1341de2fc726fb961cb2fa01",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-09718",
                    "ArXiv": "2304.09718",
                    "DOI": "10.48550/arXiv.2304.09718",
                    "CorpusId": 258212667
                },
                "corpusId": 258212667,
                "publicationVenue": {
                    "id": "349f119f-f4ee-48cf-aedb-89bcb56ab8e3",
                    "name": "Physical Review Research",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Rev Res"
                    ],
                    "issn": "2643-1564",
                    "url": "https://journals.aps.org/prresearch"
                },
                "url": "https://www.semanticscholar.org/paper/2137755655e1889b1341de2fc726fb961cb2fa01",
                "title": "Sample-efficient Model-based Reinforcement Learning for Quantum Control",
                "abstract": "We propose a model-based reinforcement learning (RL) approach for noisy time-dependent gate optimization with improved sample complexity over model-free RL. Sample complexity is the number of controller interactions with the physical system. Leveraging an inductive bias, inspired by recent advances in neural ordinary differential equations (ODEs), we use an auto-differentiable ODE parametrised by a learnable Hamiltonian ansatz to represent the model approximating the environment whose time-dependent part, including the control, is fully known. Control alongside Hamiltonian learning of continuous time-independent parameters is addressed through interactions with the system. We demonstrate an order of magnitude advantage in the sample complexity of our method over standard model-free RL in preparing some standard unitary gates with closed and open system dynamics, in realistic numerical experiments incorporating single shot measurements, arbitrary Hilbert space truncations and uncertainty in Hamiltonian parameters. Also, the learned Hamiltonian can be leveraged by existing control methods like GRAPE for further gradient-based optimization with the controllers found by RL as initializations. Our algorithm that we apply on nitrogen vacancy (NV) centers and transmons in this paper is well suited for controlling partially characterised one and two qubit systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2176774823",
                        "name": "Irtaza Khalid"
                    },
                    {
                        "authorId": "37302097",
                        "name": "C. Weidner"
                    },
                    {
                        "authorId": "12672965",
                        "name": "E. Jonckheere"
                    },
                    {
                        "authorId": "2176777287",
                        "name": "Sophie G. Shermer"
                    },
                    {
                        "authorId": "2300848",
                        "name": "F. Langbein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MBRL algorithms (Sutton, 1991; Janner et al., 2019; Lee et al., 2020; Moerland et al., 2023) employ an explicit model trained to estimate the environment dynamics (i.e., state transition and reward functions) using self-supervised learning.",
                "MBRL algorithms (Sutton, 1991; Janner et al., 2019; Lee et al., 2020; Moerland et al., 2023) employ an explicit model trained to estimate the environment dynamics (i.",
                "Overall, we also observe that in easy tasks, it may be easier for MARL algorithms to learn from raw inputs rather than latent states generated by the model, which are subject to epistemic uncertainty (Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4bbf9d3f19a80f5dec3a80d8d92ce78fcd6776c9",
                "externalIds": {
                    "ArXiv": "2304.06011",
                    "DBLP": "journals/corr/abs-2304-06011",
                    "DOI": "10.48550/arXiv.2304.06011",
                    "CorpusId": 258078790
                },
                "corpusId": 258078790,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4bbf9d3f19a80f5dec3a80d8d92ce78fcd6776c9",
                "title": "Bi-level Latent Variable Model for Sample-Efficient Multi-Agent Reinforcement Learning",
                "abstract": "Despite their potential in real-world applications, multi-agent reinforcement learning (MARL) algorithms often suffer from high sample complexity. To address this issue, we present a novel model-based MARL algorithm, BiLL (Bi-Level Latent Variable Model-based Learning), that learns a bi-level latent variable model from high-dimensional inputs. At the top level, the model learns latent representations of the global state, which encode global information relevant to behavior learning. At the bottom level, it learns latent representations for each agent, given the global latent representations from the top level. The model generates latent trajectories to use for policy learning. We evaluate our algorithm on complex multi-agent tasks in the challenging SMAC and Flatland environments. Our algorithm outperforms state-of-the-art model-free and model-based baselines in sample efficiency, including on two extremely challenging Super Hard SMAC maps.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Aravind Venugopal"
                    },
                    {
                        "authorId": "144177520",
                        "name": "Stephanie Milani"
                    },
                    {
                        "authorId": "47324743",
                        "name": "Fei Fang"
                    },
                    {
                        "authorId": "1723632",
                        "name": "Balaraman Ravindran"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It has been revealed that directly applying model-based online RL methods like MBPO [15] fails on offline datasets [44].",
                "However, MBPO fails to resolve the issue of extrapolation error in the offline setting.",
                "The practical implementation of TATU can be generally divided into three steps: Step 1: Training Dynamics Models: Following prior work [15], we train the dynamics model P\u0302 (\u00b7|s, a) with a neural network p\u0302\u03c8(s|s, a) parameterized by \u03c8 that produces a Gaussian distribution over the next state, i.",
                "MBPO improves the sample efficiency for online model-based RL by introducing the branch rollout method that queries the environmental dynamics model for short rollouts."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bfd9d1b5d0b4af68d393ee4865aaa2eeebd97ae6",
                "externalIds": {
                    "ArXiv": "2304.04660",
                    "CorpusId": 260164479
                },
                "corpusId": 260164479,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bfd9d1b5d0b4af68d393ee4865aaa2eeebd97ae6",
                "title": "Uncertainty-driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning",
                "abstract": "Equipped with the trained environmental dynamics, model-based offline reinforcement learning (RL) algorithms can often successfully learn good policies from fixed-sized datasets, even some datasets with poor quality. Unfortunately, however, it can not be guaranteed that the generated samples from the trained dynamics model are reliable (e.g., some synthetic samples may lie outside of the support region of the static dataset). To address this issue, we propose Trajectory Truncation with Uncertainty (TATU), which adaptively truncates the synthetic trajectory if the accumulated uncertainty along the trajectory is too large. We theoretically show the performance bound of TATU to justify its benefits. To empirically show the advantages of TATU, we first combine it with two classical model-based offline RL algorithms, MOPO and COMBO. Furthermore, we integrate TATU with several off-the-shelf model-free offline RL algorithms, e.g., BCQ. Experimental results on the D4RL benchmark show that TATU significantly improves their performance, often by a large margin. Code is available here.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155570419",
                        "name": "Junjie Zhang"
                    },
                    {
                        "authorId": "2008151131",
                        "name": "Jiafei Lyu"
                    },
                    {
                        "authorId": "2125106047",
                        "name": "Xiaoteng Ma"
                    },
                    {
                        "authorId": "30411824",
                        "name": "Jiangpeng Yan"
                    },
                    {
                        "authorId": "2146157882",
                        "name": "Jun Yang"
                    },
                    {
                        "authorId": "2187301367",
                        "name": "Le Wan"
                    },
                    {
                        "authorId": "2116523082",
                        "name": "Xiu Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bdbecd8054c7f9095ec1e76cbcb6116603b47823",
                "externalIds": {
                    "DBLP": "journals/eswa/JangHK23",
                    "DOI": "10.1016/j.eswa.2023.120136",
                    "CorpusId": 258160326
                },
                "corpusId": 258160326,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bdbecd8054c7f9095ec1e76cbcb6116603b47823",
                "title": "K-mixup: Data augmentation for offline reinforcement learning using mixup in a Koopman invariant subspace",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "153050483",
                        "name": "Junwoo Jang"
                    },
                    {
                        "authorId": "2890065",
                        "name": "Jungwook Han"
                    },
                    {
                        "authorId": "1866900",
                        "name": "Jinwhan Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2013) and MBPO (Janner et al. 2019) in the MuJoCo (Todorov, Erez, and Tassa 2012) benchmark, the overall improvement",
                "Nevertheless, compared to the performance of Dreamer V2 in Atari games (Bellemare et al. 2013) and MBPO (Janner et al. 2019) in the MuJoCo (Todorov, Erez, and Tassa 2012) benchmark, the overall improvement\nof sample efficiency, as well as the asymptotic performances in some difficult tasks achieved by MAMBA are still relatively limited, which may be due to the high complexity of the dynamics of multi-agent systems.",
                "It is worth noting that Theorem 1 is not simply a multiagent version of the results that have been derived in the single-agent setting (Luo et al. 2019; Janner et al. 2019).",
                "This work focuses on model learning and adopts the most common model usage, that is, generating pseudo samples to enrich the data buffer, so as to reduce the interaction with the environment and accelerate policy learning (Sutton 1990, 1991; Deisenroth et al. 2013; Kalweit and Boedecker 2017; Luo et al. 2019; Janner et al. 2019; Pan et al. 2020).",
                "Most of previous works in MBRL train the model simply by minimizing each one-step prediction error for transitions available in the environment dataset (Kurutach et al. 2018; Chua et al. 2018; Janner et al. 2019).",
                ", generating pseudo samples to enrich the dataset, so as to accelerate policy learning and reduce interactions with the true environment (Sutton 1991; Chua et al. 2018; Luo et al. 2019; Janner et al. 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "a8e839087b28dc469a47a184cbd197b21f32d9e6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-17984",
                    "ArXiv": "2303.17984",
                    "DOI": "10.48550/arXiv.2303.17984",
                    "CorpusId": 257901108
                },
                "corpusId": 257901108,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a8e839087b28dc469a47a184cbd197b21f32d9e6",
                "title": "Models as Agents: Optimizing Multi-Step Predictions of Interactive Local Models in Model-Based Multi-Agent Reinforcement Learning",
                "abstract": "Research in model-based reinforcement learning has made significant progress in recent years. Compared to single-agent settings, the exponential dimension growth of the joint state-action space in multi-agent systems dramatically increases the complexity of the environment dynamics, which makes it infeasible to learn an accurate global model and thus necessitates the use of agent-wise local models. However, during multi-step model rollouts, the prediction of one local model can affect the predictions of other local models in the next step. As a result, local prediction errors can be propagated to other localities and eventually give rise to considerably large global errors. Furthermore, since the models are generally used to predict for multiple steps, simply minimizing one-step prediction errors regardless of their long-term effect on other models may further aggravate the propagation of local errors. To this end, we propose Models as AGents (MAG), a multi-agent model optimization framework that reversely treats the local models as multi-step decision making agents and the current policies as the dynamics during the model rollout process. In this way, the local models are able to consider the multi-step mutual affect between each other before making predictions. Theoretically, we show that the objective of MAG is approximately equivalent to maximizing a lower bound of the true environment return. Experiments on the challenging StarCraft II benchmark demonstrate the effectiveness of MAG.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109687122",
                        "name": "Zifan Wu"
                    },
                    {
                        "authorId": "2155747980",
                        "name": "Chao Yu"
                    },
                    {
                        "authorId": "40590308",
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "40513470",
                        "name": "Jianye Hao"
                    },
                    {
                        "authorId": "74076606",
                        "name": "H. Zhuo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[36] suffers from compounding errors as other learned single-step models do [20], but our non-parametric approach of composing previously seen transitions enables us to plan for longer horizons.",
                "This can be expected, as learned models suffer from compounding errors when rolled out (Janner et al., 2019) and prior methods that use MPC for object-centric methods only roll out for very short horizons (Veerapaneni et al., 2020).",
                "But planning with object-centric methods that do infer entities (Veerapaneni et al., 2020) is also not easy because the difficulties of long-horizon planning with learned parametric models (Janner et al., 2019) are exacerbated in combinatorial spaces.",
                "\u2026observe that it is indeed difficult to perform shooting-based planning with an entity-centric world model trained to predict a single step forward (Janner et al., 2019): the MPC baseline performs poorly because its rollouts are\npoor, and it is significantly more computationally expensive to run\u2026",
                "Veerapaneni et al. (2020) also considers control tasks, but their shooting-based planning method suffers from compounding errors as other learned single-step models do (Janner et al., 2019), while our hierarchical non-parametric approach enables us to plan for longer horizons."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3dc25449a9db4deecbcb50c8eaf6d4a59c245d70",
                "externalIds": {
                    "DBLP": "conf/iclr/0003DM0L023",
                    "ArXiv": "2303.11373",
                    "DOI": "10.48550/arXiv.2303.11373",
                    "CorpusId": 253899413
                },
                "corpusId": 253899413,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3dc25449a9db4deecbcb50c8eaf6d4a59c245d70",
                "title": "Neural Constraint Satisfaction: Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement",
                "abstract": "Object rearrangement is a challenge for embodied agents because solving these tasks requires generalizing across a combinatorially large set of configurations of entities and their locations. Worse, the representations of these entities are unknown and must be inferred from sensory percepts. We present a hierarchical abstraction approach to uncover these underlying entities and achieve combinatorial generalization from unstructured visual inputs. By constructing a factorized transition graph over clusters of entity representations inferred from pixels, we show how to learn a correspondence between intervening on states of entities in the agent's model and acting on objects in the environment. We use this correspondence to develop a method for control that generalizes to different numbers and configurations of objects, which outperforms current offline deep RL methods when evaluated on simulated rearrangement tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47235561",
                        "name": "Michael Chang"
                    },
                    {
                        "authorId": "2072295667",
                        "name": "Alyssa Dayan"
                    },
                    {
                        "authorId": "153145615",
                        "name": "Franziska Meier"
                    },
                    {
                        "authorId": "1799860",
                        "name": "T. Griffiths"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "2111672235",
                        "name": "Amy Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based learning methods from standard RL can be used to learn from demonstrations [150, 23].",
                "IRL [67] 2008 Sensors GPS Data IRL N/A Matching Path Following Mobile Robot MBPO [150] 2019 N/A N/A Policy Learning (OPE) Regression Acc.",
                "A theoretical analysis is present in [150], where they formulate the bounds on the error between the learned policy and the policy in the data set, due to distributional shifts in the policy and model."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "e9abbbf1e64cd972fb2e8bbc1ffe983c8cdc640e",
                "externalIds": {
                    "ArXiv": "2303.11191",
                    "DBLP": "journals/corr/abs-2303-11191",
                    "DOI": "10.2139/ssrn.4390650",
                    "CorpusId": 257601734
                },
                "corpusId": 257601734,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e9abbbf1e64cd972fb2e8bbc1ffe983c8cdc640e",
                "title": "A Survey of Demonstration Learning",
                "abstract": "With the fast improvement of machine learning, reinforcement learning (RL) has been used to automate human tasks in different areas. However, training such agents is difficult and restricted to expert users. Moreover, it is mostly limited to simulation environments due to the high cost and safety concerns of interactions in the real world. Demonstration Learning is a paradigm in which an agent learns to perform a task by imitating the behavior of an expert shown in demonstrations. It is a relatively recent area in machine learning, but it is gaining significant traction due to having tremendous potential for learning complex behaviors from demonstrations. Learning from demonstration accelerates the learning process by improving sample efficiency, while also reducing the effort of the programmer. Due to learning without interacting with the environment, demonstration learning would allow the automation of a wide range of real world applications such as robotics and healthcare. This paper provides a survey of demonstration learning, where we formally introduce the demonstration problem along with its main challenges and provide a comprehensive overview of the process of learning from demonstrations from the creation of the demonstration data set, to learning methods from demonstrations, and optimization by combining demonstration learning with different machine learning methods. We also review the existing benchmarks and identify their strengths and limitations. Additionally, we discuss the advantages and disadvantages of the paradigm as well as its main applications. Lastly, we discuss our perspective on open problems and research directions for this rapidly growing field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2054951003",
                        "name": "Andr\u00e9 Rosa de Sousa Porf\u00edrio Correia"
                    },
                    {
                        "authorId": "2212027274",
                        "name": "Lu\u00eds Alexandre"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Baselines: For all cases we compare with: model-base RL (MBPO (Janner et al., 2019)), model-free RL(SAC (Haarnoja et al.",
                "Baselines: For all cases we compare with: model-base RL (MBPO (Janner et al., 2019)), model-free RL(SAC (Haarnoja et al., 2018), PPO (Schulman et al., 2017)\nand DDPG (Lillicrap et al., 2015)) and model predictive control (MPC)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "61c120df6ae4f9c17508b0041e48577f2c702d39",
                "externalIds": {
                    "ArXiv": "2303.10327",
                    "DBLP": "conf/l4dc/MengF23",
                    "DOI": "10.48550/arXiv.2303.10327",
                    "CorpusId": 257632260
                },
                "corpusId": 257632260,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/61c120df6ae4f9c17508b0041e48577f2c702d39",
                "title": "Hybrid Systems Neural Control with Region-of-Attraction Planner",
                "abstract": "Hybrid systems are prevalent in robotics. However, ensuring the stability of hybrid systems is challenging due to sophisticated continuous and discrete dynamics. A system with all its system modes stable can still be unstable. Hence special treatments are required at mode switchings to stabilize the system. In this work, we propose a hierarchical, neural network (NN)-based method to control general hybrid systems. For each system mode, we first learn an NN Lyapunov function and an NN controller to ensure the states within the region of attraction (RoA) can be stabilized. Then an RoA NN estimator is learned across different modes. Upon mode switching, we propose a differentiable planner to ensure the states after switching can land in next mode's RoA, hence stabilizing the hybrid system. We provide novel theoretical stability guarantees and conduct experiments in car tracking control, pogobot navigation, and bipedal walker locomotion. Our method only requires 0.25X of the training time as needed by other learning-based methods. With low running time (10-50X faster than model predictive control (MPC)), our controller achieves a higher stability/success rate over other baselines such as MPC, reinforcement learning (RL), common Lyapunov methods (CLF), linear quadratic regulator (LQR), quadratic programming (QP) and Hamilton-Jacobian-based methods (HJB). The project page is on https://mit-realm.github.io/hybrid-clf.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Yue Meng"
                    },
                    {
                        "authorId": "2344739",
                        "name": "Chuchu Fan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Modelbased methods, such as MBPO (Janner et al. 2019), are most suitable for such adaptations."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1bb2d9b6b333afbfc53e1434d8b6c038c4ca5d90",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-08271",
                    "ArXiv": "2303.08271",
                    "DOI": "10.48550/arXiv.2303.08271",
                    "CorpusId": 257532489
                },
                "corpusId": 257532489,
                "publicationVenue": {
                    "id": "267934f4-c986-4571-bef8-d0eebc5e0e54",
                    "name": "International Conference on Automated Planning and Scheduling",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Autom Plan Sched",
                        "ICAPS"
                    ],
                    "url": "http://www.icaps-conference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1bb2d9b6b333afbfc53e1434d8b6c038c4ca5d90",
                "title": "Act-Then-Measure: Reinforcement Learning for Partially Observable Environments with Active Measuring",
                "abstract": "We study Markov decision processes (MDPs), where agents control when and how they gather information, as formalized by action-contingent noiselessly observable MDPs (ACNO-MPDs). In these models, actions have two components: a control action that influences how the environment changes and a measurement action that affects the agent's observation. To solve ACNO-MDPs, we introduce the act-then-measure (ATM) heuristic, which assumes that we can ignore future state uncertainty when choosing control actions. To decide whether or not to measure, we introduce the concept of measuring value. We show how following this heuristic may lead to shorter policy computation times and prove a bound on the performance loss it incurs. We develop a reinforcement learning algorithm based on the ATM heuristic, using a Dyna-Q variant adapted for partially observable domains, and showcase its superior performance compared to prior methods on a number of partially-observable environments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2211601985",
                        "name": "Merlijn Krale"
                    },
                    {
                        "authorId": "51893920",
                        "name": "T. D. Sim\u00e3o"
                    },
                    {
                        "authorId": "35252898",
                        "name": "N. Jansen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We note the parallels between synthetic data generation and model-based reinforcement learning [34, 47, 75]; methods that generate synthetic samples by rolling out from observed states."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b6b9056274ed37f49f990dbb6f0c761be3e2a7fe",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-06614",
                    "ArXiv": "2303.06614",
                    "DOI": "10.48550/arXiv.2303.06614",
                    "CorpusId": 257495808
                },
                "corpusId": 257495808,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b6b9056274ed37f49f990dbb6f0c761be3e2a7fe",
                "title": "Synthetic Experience Replay",
                "abstract": "A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements when upsampling small offline datasets and see that additional synthetic data also allows us to effectively train larger networks. Furthermore, SynthER enables online agents to train with a much higher update-to-data ratio than before, leading to a significant increase in sample efficiency, without any algorithmic changes. We believe that synthetic training data could open the door to realizing the full potential of deep learning for replay-based RL algorithms from limited data. Finally, we open-source our code at https://github.com/conglu1997/SynthER.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110752472",
                        "name": "Cong Lu"
                    },
                    {
                        "authorId": "2053179501",
                        "name": "Philip J. Ball"
                    },
                    {
                        "authorId": "1410302742",
                        "name": "Jack Parker-Holder"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our algorithm builds on MBPO (Janner et al., 2019), which is a Dyna-style approach that learns policy with real data and simulated data.",
                "We choose MBPO, a widely-used MBRL algorithm with asymptotic performance rivaling the best modelfree algorithms, as the baseline.",
                "One widely used approach to modeling the transition is to directly minimize the distance between the predictions and the ground truth data, where the distance can be a mean square error or likelihood probability with various re-weighting methods (Janner et al., 2019; Yu et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e6abeba9da5390f272dfc209db5c6fd612992bbb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-05458",
                    "ArXiv": "2303.05458",
                    "DOI": "10.48550/arXiv.2303.05458",
                    "CorpusId": 257427341
                },
                "corpusId": 257427341,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e6abeba9da5390f272dfc209db5c6fd612992bbb",
                "title": "Beware of Instantaneous Dependence in Reinforcement Learning",
                "abstract": "Playing an important role in Model-Based Reinforcement Learning (MBRL), environment models aim to predict future states based on the past. Existing works usually ignore instantaneous dependence in the state, that is, assuming that the future state variables are conditionally independent given the past states. However, instantaneous dependence is prevalent in many RL environments. For instance, in the stock market, instantaneous dependence can exist between two stocks because the fluctuation of one stock can quickly affect the other and the resolution of price change is lower than that of the effect. In this paper, we prove that with few exceptions, ignoring instantaneous dependence can result in suboptimal policy learning in MBRL. To address the suboptimality problem, we propose a simple plug-and-play method to enable existing MBRL algorithms to take instantaneous dependence into account. Through experiments on two benchmarks, we (1) confirm the existence of instantaneous dependence with visualization; (2) validate our theoretical findings that ignoring instantaneous dependence leads to suboptimal policy; (3) verify that our method effectively enables reinforcement learning with instantaneous dependence and improves policy performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2211182470",
                        "name": "Zhengmao Zhu"
                    },
                    {
                        "authorId": "40685903",
                        "name": "Yu-Ren Liu"
                    },
                    {
                        "authorId": "2153440065",
                        "name": "Hong Tian"
                    },
                    {
                        "authorId": "144705629",
                        "name": "Yang Yu"
                    },
                    {
                        "authorId": "2119017332",
                        "name": "Kun Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This model is then used for data generation (Sutton, 1990; Janner et al., 2019; Cowen-Rivers et al., 2022), planning (Chua et al., 2018; Hafner et al., 2019; Lutter et al., 2021a;b; Schneider et al., 2022) or stochastic optimization (Deisenroth & Rasmussen, 2011; Heess et al., 2015; Clavera et al.,\u2026",
                "This model is then used for data generation (Sutton, 1990; Janner et al., 2019; Cowen-Rivers et al., 2022), planning (Chua et al.",
                "In practice, rollout horizons are often kept short to avoid significant compounding model error build-up (Janner et al., 2019).",
                "Recently, various improvements have been proposed to the original DynaQ algorithm, such as using ensemble neural network models and short rollout horizons (Janner et al., 2019; Lai et al., 2020), and improving the synthetic data generation with model predictive control (Morgan et al.",
                "Recently, various improvements have been proposed to the original DynaQ algorithm, such as using ensemble neural network models and short rollout horizons (Janner et al., 2019; Lai et al., 2020), and improving the synthetic data generation with model predictive control (Morgan et al., 2021).",
                "The learned dynamics model is a reimplementation of the one introduced in Janner et al. (2019).",
                "In model-based RL, a model of the system dynamics is usually learned from data, which is subsequently used for planning (Chua et al., 2018; Hafner et al., 2019) or for policy learning (Sutton, 1990; Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "1484cc1fc1d7ed1b9e77b230aaee01f4b29e9327",
                "externalIds": {
                    "ArXiv": "2303.03955",
                    "DBLP": "conf/iclr/PalenicekLC023",
                    "DOI": "10.48550/arXiv.2303.03955",
                    "CorpusId": 259373058
                },
                "corpusId": 259373058,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1484cc1fc1d7ed1b9e77b230aaee01f4b29e9327",
                "title": "Diminishing Return of Value Expansion Methods in Model-Based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning is one approach to increase sample efficiency. However, the accuracy of the dynamics model and the resulting compounding error over modelled trajectories are commonly regarded as key limitations. A natural question to ask is: How much more sample efficiency can be gained by improving the learned dynamics models? Our paper empirically answers this question for the class of model-based value expansion methods in continuous control problems. Value expansion methods should benefit from increased model accuracy by enabling longer rollout horizons and better value function approximations. Our empirical study, which leverages oracle dynamics models to avoid compounding model errors, shows that (1) longer horizons increase sample efficiency, but the gain in improvement decreases with each additional expansion step, and (2) the increased model accuracy only marginally increases the sample efficiency compared to learned models with identical horizons. Therefore, longer horizons and increased model accuracy yield diminishing returns in terms of sample efficiency. These improvements in sample efficiency are particularly disappointing when compared to model-free value expansion methods. Even though they introduce no computational overhead, we find their performance to be on-par with model-based value expansion methods. Therefore, we conclude that the limitation of model-based value expansion methods is not the model accuracy of the learned models. While higher model accuracy is beneficial, our experiments show that even a perfect model will not provide an un-rivalled sample efficiency but that the bottleneck lies elsewhere.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1751630928",
                        "name": "Daniel Palenicek"
                    },
                    {
                        "authorId": "49199170",
                        "name": "M. Lutter"
                    },
                    {
                        "authorId": "2067739440",
                        "name": "Jo\u00e3o Carvalho"
                    },
                    {
                        "authorId": "2107720654",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[20, 21] use bootstrap ensembles of predictive models, which are able to capture aleatoric uncertainty and epistemic uncertainty."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6106b5eb3ac3e9c7cd3c5fd515d66549ab9a9a1f",
                "externalIds": {
                    "ArXiv": "2303.03811",
                    "DBLP": "journals/corr/abs-2303-03811",
                    "DOI": "10.48550/arXiv.2303.03811",
                    "CorpusId": 257378452
                },
                "corpusId": 257378452,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6106b5eb3ac3e9c7cd3c5fd515d66549ab9a9a1f",
                "title": "ENTROPY: Environment Transformer and Offline Policy Optimization",
                "abstract": "Model-based methods provide an effective approach to offline reinforcement learning (RL). They learn an environmental dynamics model from interaction experiences and then perform policy optimization based on the learned model. However, previous model-based offline RL methods lack long-term prediction capability, resulting in large errors when generating multi-step trajectories. We address this issue by developing a sequence modeling architecture, Environment Transformer, which can generate reliable long-horizon trajectories based on offline datasets. We then propose a novel model-based offline RL algorithm, ENTROPY, that learns the dynamics model and reward function by ENvironment TRansformer and performs Offline PolicY optimization. We evaluate the proposed method on MuJoCo continuous control RL environments. Results show that ENTROPY performs comparably or better than the state-of-the-art model-based and model-free offline RL methods and demonstrates more powerful long-term trajectory prediction capability compared to existing model-based offline methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210883545",
                        "name": "Pengqin Wang"
                    },
                    {
                        "authorId": "3452528",
                        "name": "Meixin Zhu"
                    },
                    {
                        "authorId": "3225993",
                        "name": "S. Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is in accordance with many previous works (Thm. 1 in (Xu et al., 2019), Thm. 4.1 in (Janner et al., 2019) and Thm. 1 in (Schulman et al., 2015)), which include (1 \u2212 \u03b3)2 in the denominator when it comes to differences of the cumulative return, given the difference in the action distribution."
            ],
            "isInfluential": true,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "c28c30a846d97f7e9c0722fbc35bb88aba3c6b04",
                "externalIds": {
                    "ArXiv": "2303.01728",
                    "DBLP": "journals/corr/abs-2303-01728",
                    "DOI": "10.48550/arXiv.2303.01728",
                    "CorpusId": 257353270
                },
                "corpusId": 257353270,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c28c30a846d97f7e9c0722fbc35bb88aba3c6b04",
                "title": "Guarded Policy Optimization with Imperfect Online Demonstrations",
                "abstract": "The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher's own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2093481204",
                        "name": "Zhenghai Xue"
                    },
                    {
                        "authorId": "46216016",
                        "name": "Zhenghao Peng"
                    },
                    {
                        "authorId": "2108644837",
                        "name": "Quanyi Li"
                    },
                    {
                        "authorId": "2280742",
                        "name": "Zhihan Liu"
                    },
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d7e36bcaabc37abb8af0fe5ef59d9df58f219c0e",
                "externalIds": {
                    "DBLP": "conf/atal/LiuXZLJ0Z023",
                    "ArXiv": "2303.02073",
                    "DOI": "10.48550/arXiv.2303.02073",
                    "CorpusId": 257353358
                },
                "corpusId": 257353358,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d7e36bcaabc37abb8af0fe5ef59d9df58f219c0e",
                "title": "How To Guide Your Learner: Imitation Learning with Active Adaptive Expert Involvement",
                "abstract": "Imitation learning aims to mimic the behavior of experts without explicit reward signals. Passive imitation learning methods which use static expert datasets typically suffer from compounding error, low sample efficiency, and high hyper-parameter sensitivity. In contrast, active imitation learning methods solicit expert interventions to address the limitations. However, recent active imitation learning methods are designed based on human intuitions or empirical experience without theoretical guarantee. In this paper, we propose a novel active imitation learning framework based on a teacher-student interaction model, in which the teacher's goal is to identify the best teaching behavior and actively affect the student's learning process. By solving the optimization objective of this framework, we propose a practical implementation, naming it AdapMen. Theoretical analysis shows that AdapMen can improve the error bound and avoid compounding error under mild conditions. Experiments on the MetaDrive benchmark and Atari 2600 games validate our theoretical analysis and show that our method achieves near-expert performance with much less expert involvement and total sampling steps than previous methods. The code is available at https://github.com/liuxhym/AdapMen.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108707316",
                        "name": "Xu-Hui Liu"
                    },
                    {
                        "authorId": "2152479603",
                        "name": "Feng Xu"
                    },
                    {
                        "authorId": null,
                        "name": "Xinyu Zhang"
                    },
                    {
                        "authorId": "2210776853",
                        "name": "Tianyuan Liu"
                    },
                    {
                        "authorId": "2119326931",
                        "name": "Shengyi Jiang"
                    },
                    {
                        "authorId": "2118230208",
                        "name": "Rui Chen"
                    },
                    {
                        "authorId": "2079174",
                        "name": "Zongzhang Zhang"
                    },
                    {
                        "authorId": "2152850415",
                        "name": "Yang Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is a well-studied phenomenon in model-based RL [e.g. see Janner et al., 2019].",
                "For instance, this is discussed extensively in the context of model-based RL in Janner et al. [2019]. Often, a discount factor is used when computing returns to alleviate these issues."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d3cbd5fe7e3a7b4ce91a60064ade7131b3a04146",
                "externalIds": {
                    "ArXiv": "2303.01076",
                    "DBLP": "journals/corr/abs-2303-01076",
                    "DOI": "10.48550/arXiv.2303.01076",
                    "CorpusId": 257280158
                },
                "corpusId": 257280158,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d3cbd5fe7e3a7b4ce91a60064ade7131b3a04146",
                "title": "Hallucinated Adversarial Control for Conservative Offline Policy Evaluation",
                "abstract": "We study the problem of conservative off-policy evaluation (COPE) where given an offline dataset of environment interactions, collected by other agents, we seek to obtain a (tight) lower bound on a policy's performance. This is crucial when deciding whether a given policy satisfies certain minimal performance/safety criteria before it can be deployed in the real world. To this end, we introduce HAMBO, which builds on an uncertainty-aware learned model of the transition dynamics. To form a conservative estimate of the policy's performance, HAMBO hallucinates worst-case trajectories that the policy may take, within the margin of the models' epistemic confidence regions. We prove that the resulting COPE estimates are valid lower bounds, and, under regularity conditions, show their convergence to the true expected return. Finally, we discuss scalable variants of our approach based on Bayesian Neural Networks and empirically demonstrate that they yield reliable and tight lower bounds in various continuous control environments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35309584",
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "authorId": "2151000167",
                        "name": "Bhavya Sukhija"
                    },
                    {
                        "authorId": "2210283652",
                        "name": "Tobias Birchler"
                    },
                    {
                        "authorId": "40796674",
                        "name": "Parnian Kassraie"
                    },
                    {
                        "authorId": "153243248",
                        "name": "A. Krause"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "dc744d27a13da837f332b542434a4bc877b0fd17",
                "externalIds": {
                    "DBLP": "conf/icml/Vemula0SBC23",
                    "ArXiv": "2303.00694",
                    "DOI": "10.48550/arXiv.2303.00694",
                    "CorpusId": 257255086
                },
                "corpusId": 257255086,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/dc744d27a13da837f332b542434a4bc877b0fd17",
                "title": "The Virtues of Laziness in Model-based RL: A Unified Objective and Algorithms",
                "abstract": "We propose a novel approach to addressing two fundamental challenges in Model-based Reinforcement Learning (MBRL): the computational expense of repeatedly finding a good policy in the learned model, and the objective mismatch between model fitting and policy computation. Our\"lazy\"method leverages a novel unified objective, Performance Difference via Advantage in Model, to capture the performance difference between the learned policy and expert policy under the true dynamics. This objective demonstrates that optimizing the expected policy advantage in the learned model under an exploration distribution is sufficient for policy computation, resulting in a significant boost in computational efficiency compared to traditional planning methods. Additionally, the unified objective uses a value moment matching term for model fitting, which is aligned with the model's usage during policy computation. We present two no-regret algorithms to optimize the proposed objective, and demonstrate their statistical and computational gains compared to existing MBRL methods through simulated benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2387189",
                        "name": "Anirudh Vemula"
                    },
                    {
                        "authorId": "2152602077",
                        "name": "Yuda Song"
                    },
                    {
                        "authorId": "2109423866",
                        "name": "Aarti Singh"
                    },
                    {
                        "authorId": "1756566",
                        "name": "J. Bagnell"
                    },
                    {
                        "authorId": "2487768",
                        "name": "Sanjiban Choudhury"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Among the various model-based RL approaches [24], [25], [26], Dyna stands as a fundamental architecture which combines the model-free and model-based algorithms flexibly [27].",
                "training stability with a smaller gradient update number, which could attribute to that the model-generated data can effectively reduce the overfitting risk [26]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "311f9ddd78286cbc2245c17132e5be065a6fab72",
                "externalIds": {
                    "DBLP": "journals/tiv/HuFW23",
                    "DOI": "10.1109/TIV.2022.3233592",
                    "CorpusId": 255665118
                },
                "corpusId": 255665118,
                "publicationVenue": {
                    "id": "c2eeb1be-38e9-4a0a-a89d-957f4fd71ea1",
                    "name": "IEEE Transactions on Intelligent Vehicles",
                    "alternate_names": [
                        "IEEE Trans Intell Veh"
                    ],
                    "issn": "2379-8858",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274857"
                },
                "url": "https://www.semanticscholar.org/paper/311f9ddd78286cbc2245c17132e5be065a6fab72",
                "title": "Safe Reinforcement Learning for Model-Reference Trajectory Tracking of Uncertain Autonomous Vehicles With Model-Based Acceleration",
                "abstract": "Applying reinforcement learning (RL) algorithms to control systems design remains a challenging task due to the potential unsafe exploration and the low sample efficiency. In this paper, we propose a novel safe model-based RL algorithm to solve the collision-free model-reference trajectory tracking problem of uncertain autonomous vehicles (AVs). Firstly, a new type of robust control barrier function (CBF) condition for collision-avoidance is derived for the uncertain AVs by incorporating the estimation of the system uncertainty with Gaussian process (GP) regression. Then, a robust CBF-based RL control structure is proposed, where the nominal control input is composed of the RL policy and a model-based reference control policy. The actual control input obtained from the quadratic programming problem can satisfy the constraints of collision-avoidance, input saturation and velocity boundedness simultaneously with a relatively high probability. Finally, within this control structure, a Dyna-style safe model-based RL algorithm is proposed, where the safe exploration is achieved through executing the robust CBF-based actions and the sample efficiency is improved by leveraging the GP models. The superior learning performance of the proposed RL control structure is demonstrated through simulation experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149298671",
                        "name": "Yifan Hu"
                    },
                    {
                        "authorId": "3261370",
                        "name": "Junjie Fu"
                    },
                    {
                        "authorId": "145193290",
                        "name": "G. Wen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These imaginary transitions can be used as extra training samples for TD methods (e.g. Sutton, 1990; Gu et al., 2016; Feinberg et al., 2018; Janner et al., 2019; D\u2019Oro & Jas\u0301kowski, 2020; Buckman et al., 2018).",
                "(2, top row) shows TaTD3 performs at least as well, if not batter, than the baseline algorithms in all four benchmark tasks: note the much poorer performance of MAGE on\nWalker2d-v2, of MBPO on Humanoid-v2 relative to TaTD3.",
                "Dyna-TD3 is conceptually similar to MBPO, with the main difference of MBPO relying on SAC instead of TD3.",
                "Plotted performance of MBPO was directly taken from the official algorithm repository on GitHub.",
                "The first model-based algorithm is Model-based Policy Optimization (MBPO) (Janner et al., 2019), which employs the soft actor-critic algorithm (SAC) (Haarnoja et al.",
                "\u2026transitions can be used to provide better TD targets for existing data points (e.g. Feinberg et al., 2018) or to train the actor and/or critic by generating short-horizon trajectories starting at existing state-action pairs (e.g. Janner et al., 2019; Clavera et al., 2020; Buckman et al., 2018).",
                "The first model-based algorithm is Model-based Policy Optimization (MBPO) (Janner et al., 2019), which employs the soft actor-critic algorithm (SAC) (Haarnoja et al., 2018) within a model-based Dyna setting.",
                "These imaginary transitions can be used as extra training samples for TD methods (e.g. Sutton, 1990; Gu et al., 2016; Feinberg et al., 2018; Janner et al., 2019; D\u2019Oro & Ja\u015bkowski, 2020; Buckman et al., 2018)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ddc332f991c6b7580555789a4515f1b5762355f0",
                "externalIds": {
                    "ArXiv": "2302.14182",
                    "DBLP": "journals/corr/abs-2302-14182",
                    "DOI": "10.48550/arXiv.2302.14182",
                    "CorpusId": 257232418
                },
                "corpusId": 257232418,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ddc332f991c6b7580555789a4515f1b5762355f0",
                "title": "Taylor TD-learning",
                "abstract": "Many reinforcement learning approaches rely on temporal-difference (TD) learning to learn a critic. However, TD-learning updates can be high variance due to their sole reliance on Monte Carlo estimates of the updates. Here, we introduce a model-based RL framework, Taylor TD, which reduces this variance. Taylor TD uses a first-order Taylor series expansion of TD updates. This expansion allows to analytically integrate over stochasticity in the action-choice, and some stochasticity in the state distribution for the initial state and action of each TD update. We include theoretical and empirical evidence of Taylor TD updates being lower variance than (standard) TD updates. Additionally, we show that Taylor TD has the same stable learning guarantees as (standard) TD-learning under linear function approximation. Next, we combine Taylor TD with the TD3 algorithm (Fujimoto et al., 2018), into TaTD3. We show TaTD3 performs as well, if not better, than several state-of-the art model-free and model-based baseline algorithms on a set of standard benchmark tasks. Finally, we include further analysis of the settings in which Taylor TD may be most beneficial to performance relative to standard TD-learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1400599328",
                        "name": "Michele Garibbo"
                    },
                    {
                        "authorId": "2155503163",
                        "name": "Maxime Robeyns"
                    },
                    {
                        "authorId": "2724259",
                        "name": "L. Aitchison"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Lastly, we optimize \u03c0\u03c6 as in MBPO via SGD on the SAC policy loss, but also adding the uncertainty term from (9).",
                "The performance of deep MBRL algorithms was historically lower than that of model-free methods, but the gap has been closing in recent years (Janner et al., 2019).",
                "We adopt as a baseline architecture MBPO by Janner et al. (2019) and the implementation from Pineda et al. (2021).",
                "The optimistic approach on top of MBPO (Janner et al., 2019) is presented in Algorithm 2.",
                "The original MBPO only executes the former to fill up Dmodel.",
                "D.1 Implementation Details\nThe optimistic approach on top of MBPO (Janner et al., 2019) is presented in Algorithm 2.",
                "The original MBPO trains Q-functions represented as neural networks via TD-learning on data generated via modelrandomized k-step rollouts from initial states that are sampled from Dt.",
                "We propose a new UBE and integrate it within a model-based soft actor-critic (Haarnoja et al., 2018) architecture similar to Janner et al. (2019); Froehlich et al. (2022).",
                "Algorithm 2 MBPO-style optimistic learning\n1: Initialize policy \u03c0\u03c6, predictive model p\u03b8, critic ensemble {Qi}Ni=1, uncertainty net U\u03c8 (optional), environment dataset Dt, model datasets Dmodel and { Dimodel }N i=1\n.",
                "MBPO maximizes the minimum of the twin critics (as in SAC).",
                "Algorithm 1 requires a few modifications from the MBPO methodology.",
                "MBPO trains twin critics (as in SAC) on mini-batches from Dmodel."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2e03d9d745c1b8634201e0beff40512a7f66cf29",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-12526",
                    "ArXiv": "2302.12526",
                    "DOI": "10.48550/arXiv.2302.12526",
                    "CorpusId": 257206125
                },
                "corpusId": 257206125,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2e03d9d745c1b8634201e0beff40512a7f66cf29",
                "title": "Model-Based Uncertainty in Value Functions",
                "abstract": "We consider the problem of quantifying uncertainty over expected cumulative rewards in model-based reinforcement learning. In particular, we focus on characterizing the variance over values induced by a distribution over MDPs. Previous work upper bounds the posterior variance over values by solving a so-called uncertainty Bellman equation, but the over-approximation may result in inefficient exploration. We propose a new uncertainty Bellman equation whose solution converges to the true posterior variance over values and explicitly characterizes the gap in previous work. Moreover, our uncertainty quantification technique is easily integrated into common exploration strategies and scales naturally beyond the tabular setting by using standard deep reinforcement learning architectures. Experiments in difficult exploration tasks, both in tabular and continuous control settings, show that our sharper uncertainty estimates improve sample-efficiency.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2470985",
                        "name": "Carlos E. Luis"
                    },
                    {
                        "authorId": "47846973",
                        "name": "A. Bottero"
                    },
                    {
                        "authorId": "3428828",
                        "name": "Julia Vinogradska"
                    },
                    {
                        "authorId": "2141578581",
                        "name": "Felix Berkenkamp"
                    },
                    {
                        "authorId": "2107720654",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026Lillicrap et al., 2015) and improves learning efficiency in high-dimensional continuous control tasks (Fujimoto et al., 2018; Haarnoja et al., 2018), it was later shown in Ha & Schmidhuber (2018); Janner et al. (2019) that model-based methods have much higher sample efficiency once properly tuned."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7fcc7c21023b3c8f95773e14db7cc8007c4c8d24",
                "externalIds": {
                    "DBLP": "conf/aistats/HoltHQSS23",
                    "ArXiv": "2302.12604",
                    "DOI": "10.48550/arXiv.2302.12604",
                    "CorpusId": 257206082
                },
                "corpusId": 257206082,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7fcc7c21023b3c8f95773e14db7cc8007c4c8d24",
                "title": "Neural Laplace Control for Continuous-time Delayed Systems",
                "abstract": "Many real-world offline reinforcement learning (RL) problems involve continuous-time environments with delays. Such environments are characterized by two distinctive features: firstly, the state x(t) is observed at irregular time intervals, and secondly, the current action a(t) only affects the future state x(t + g) with an unknown delay g>0. A prime example of such an environment is satellite control where the communication link between earth and a satellite causes irregular observations and delays. Existing offline RL algorithms have achieved success in environments with irregularly observed states in time or known delays. However, environments involving both irregular observations in time and unknown delays remains an open and challenging problem. To this end, we propose Neural Laplace Control, a continuous-time model-based offline RL method that combines a Neural Laplace dynamics model with a model predictive control (MPC) planner--and is able to learn from an offline dataset sampled with irregular time intervals from an environment that has a inherent unknown constant delay. We show experimentally on continuous-time delayed environments it is able to achieve near expert policy performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2069144895",
                        "name": "Samuel Holt"
                    },
                    {
                        "authorId": "83246796",
                        "name": "Alihan H\u00fcy\u00fck"
                    },
                    {
                        "authorId": "8797071",
                        "name": "Z. Qian"
                    },
                    {
                        "authorId": "2118180980",
                        "name": "Hao Sun"
                    },
                    {
                        "authorId": "1729969",
                        "name": "M. Schaar"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020), or data augmentation (Fan et al., 2021; Janner et al., 2019; Hansen et al., 2021) have been adopted to account for overfitting."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6c282567e9c452f81416214933b9b1e45ab3add4",
                "externalIds": {
                    "ArXiv": "2302.12902",
                    "DBLP": "conf/icml/SokarACE23",
                    "DOI": "10.48550/arXiv.2302.12902",
                    "CorpusId": 257219318
                },
                "corpusId": 257219318,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6c282567e9c452f81416214933b9b1e45ab3add4",
                "title": "The Dormant Neuron Phenomenon in Deep Reinforcement Learning",
                "abstract": "In this work we identify the dormant neuron phenomenon in deep reinforcement learning, where an agent's network suffers from an increasing number of inactive neurons, thereby affecting network expressivity. We demonstrate the presence of this phenomenon across a variety of algorithms and environments, and highlight its effect on learning. To address this issue, we propose a simple and effective method (ReDo) that Recycles Dormant neurons throughout training. Our experiments demonstrate that ReDo maintains the expressive power of networks by reducing the number of dormant neurons and results in improved performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "67102129",
                        "name": "Ghada Sokar"
                    },
                    {
                        "authorId": "29767024",
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "authorId": "39163115",
                        "name": "P. S. Castro"
                    },
                    {
                        "authorId": "3399348",
                        "name": "Utku Evci"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several bounds have been introduced in MBPO (Janner et al., 2019) for the return bound analysis, which however are not sufficient in decentralized learning.",
                "Dyna-style methods (Sutton, 1990; Feinberg et al., 2018; Janner et al., 2019) use both data collected in the real environment and data generated by the learned model to update the policy.",
                "Moreover, when using the learned latent variable model to train an agent, we adopt k-step branched model rollout in MBPO (Janner et al., 2019) to avoid compounding model error due to long-horizon rollout."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "1472cf8c45d686d665b2656a510c71c0bb7f2306",
                "externalIds": {
                    "ArXiv": "2302.08139",
                    "DBLP": "journals/corr/abs-2302-08139",
                    "DOI": "10.48550/arXiv.2302.08139",
                    "CorpusId": 256900652
                },
                "corpusId": 256900652,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1472cf8c45d686d665b2656a510c71c0bb7f2306",
                "title": "Model-Based Decentralized Policy Optimization",
                "abstract": "Decentralized policy optimization has been commonly used in cooperative multi-agent tasks. However, since all agents are updating their policies simultaneously, from the perspective of individual agents, the environment is non-stationary, resulting in it being hard to guarantee monotonic policy improvement. To help the policy improvement be stable and monotonic, we propose model-based decentralized policy optimization (MDPO), which incorporates a latent variable function to help construct the transition and reward function from an individual perspective. We theoretically analyze that the policy optimization of MDPO is more stable than model-free decentralized policy optimization. Moreover, due to non-stationarity, the latent variable function is varying and hard to be modeled. We further propose a latent variable prediction method to reduce the error of the latent variable function, which theoretically contributes to the monotonic policy improvement. Empirically, MDPO can indeed obtain superior performance than model-free decentralized policy optimization in a variety of cooperative multi-agent tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2199828096",
                        "name": "Hao Luo"
                    },
                    {
                        "authorId": "46179766",
                        "name": "Jiechuan Jiang"
                    },
                    {
                        "authorId": "2265693",
                        "name": "Zongqing Lu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Popular off-policy temporal difference algorithms spanning both imitation learning [39, 59] and RL [27, 20, 28, 69, 34] exemplify this class."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "672ec9fa4ddb5b6bfc46c61c5b2f4bdfa1aa8ed9",
                "externalIds": {
                    "ArXiv": "2302.08560",
                    "CorpusId": 259244100
                },
                "corpusId": 259244100,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/672ec9fa4ddb5b6bfc46c61c5b2f4bdfa1aa8ed9",
                "title": "Dual RL: Unification and New Methods for Reinforcement and Imitation Learning",
                "abstract": "The goal of reinforcement learning (RL) is to maximize the expected cumulative return. It has been shown that this objective can be represented by an optimization problem of the state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as dual RL, is unconstrained and easier to optimize. We show that several state-of-the-art off-policy deep reinforcement learning (RL) algorithms, under both online and offline, RL and imitation learning (IL) settings, can be viewed as dual RL approaches in a unified framework. This unification provides a common ground to study and identify the components that contribute to the success of these methods and also reveals the common shortcomings across methods with new insights for improvement. Our analysis shows that prior off-policy imitation learning methods are based on an unrealistic coverage assumption and are minimizing a particular f-divergence between the visitation distributions of the learned policy and the expert policy. We propose a new method using a simple modification to the dual RL framework that allows for performant imitation learning with arbitrary off-policy data to obtain near-expert performance, without learning a discriminator. Further, by framing a recent SOTA offline RL method XQL in the dual RL framework, we propose alternative choices to replace the Gumbel regression loss, which achieve improved performance and resolve the training instability issue of XQL. Project code and details can be found at this https://hari-sikchi.github.io/dual-rl.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51521430",
                        "name": "Harshit S. Sikchi"
                    },
                    {
                        "authorId": "2166847",
                        "name": "Qinqing Zheng"
                    },
                    {
                        "authorId": "2111672235",
                        "name": "Amy Zhang"
                    },
                    {
                        "authorId": "2791038",
                        "name": "S. Niekum"
                    }
                ]
            }
        },
        {
            "contexts": [
                "of the agent even after updating the behavior more than once, we collect a large number of artificial samples Nmodel in each iteration (in [24], for example, 400 model rollouts are performed for each sample of the environment).",
                "Adopting the soft-actor critic [37] as our off-policy method (as in MBPO [24]) the samples our model synthesizes in each iteration can be kept in dataset Dmodel for a number of repetitions.",
                ", [24]), we make use of the elite mechanism for the ensemble.",
                ", [24]) are performed per environment interaction.",
                "what extent capable function approximators can overcome modeling bias and how to avoid for errors to propagate into control strategies [21]\u2013[24].",
                "today in state-of-the-art algorithms for model-based control [24], [29].",
                "We use short model-based rollouts of policy \u03c0\u03c9 for artificial data collection early on, and increase their prediction horizon once more data is available [24]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9eb7759766023a51450ce84410b1f993a5e21e3b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-07160",
                    "ArXiv": "2302.07160",
                    "DOI": "10.48550/arXiv.2302.07160",
                    "CorpusId": 256846832
                },
                "corpusId": 256846832,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9eb7759766023a51450ce84410b1f993a5e21e3b",
                "title": "Learning a model is paramount for sample efficiency in reinforcement learning control of PDEs",
                "abstract": "The goal of this paper is to make a strong point for the usage of dynamical models when using reinforcement learning (RL) for feedback control of dynamical systems governed by partial differential equations (PDEs). To breach the gap between the immense promises we see in RL and the applicability in complex engineering systems, the main challenges are the massive requirements in terms of the training data, as well as the lack of performance guarantees. We present a solution for the first issue using a data-driven surrogate model in the form of a convolutional LSTM with actuation. We demonstrate that learning an actuated model in parallel to training the RL agent significantly reduces the total amount of required data sampled from the real system. Furthermore, we show that iteratively updating the model is of major importance to avoid biases in the RL training. Detailed ablation studies reveal the most important ingredients of the modeling process. We use the chaotic Kuramoto-Sivashinsky equation do demonstarte our findings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2169681628",
                        "name": "Stefan Werner"
                    },
                    {
                        "authorId": "46244768",
                        "name": "Sebastian Peitz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In offline RL, the model is often used to augment data (Yu et al., 2020; 2021) or act as a surrogate of real environment to interact with agent (Kidambi et al., 2020), which would easily introduce bootstrapped errors along the long horizon (Janner et al., 2019).",
                "Following previous work (Janner et al., 2019; Yu et al., 2020; 2021), we implement the probabilistic dynamics model using an ensemble of deep neural networks {p\u03b81, . . . , p\u03b8B}.",
                "Following previous work (Janner et al., 2019; Yu et al., 2020; 2021), we implement the probabilistic dynamics model using an ensemble of deep neural networks {p\u03b8(1), .",
                ", 2020), which would easily introduce bootstrapped errors along the long horizon (Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "8c4a2558851522b0344c7d605aac7d2a36b740aa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-06884",
                    "ArXiv": "2302.06884",
                    "DOI": "10.48550/arXiv.2302.06884",
                    "CorpusId": 256846729
                },
                "corpusId": 256846729,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8c4a2558851522b0344c7d605aac7d2a36b740aa",
                "title": "Conservative State Value Estimation for Offline Reinforcement Learning",
                "abstract": "Offline reinforcement learning faces a significant challenge of value over-estimation due to the distributional drift between the dataset and the current learned policy, leading to learning failure in practice. The common approach is to incorporate a penalty term to reward or value estimation in the Bellman iterations. Meanwhile, to avoid extrapolation on out-of-distribution (OOD) states and actions, existing methods focus on conservative Q-function estimation. In this paper, we propose Conservative State Value Estimation (CSVE), a new approach that learns conservative V-function via directly imposing penalty on OOD states. Compared to prior work, CSVE allows more effective in-data policy optimization with conservative value guarantees. Further, we apply CSVE and develop a practical actor-critic algorithm in which the critic does the conservative value estimation by additionally sampling and penalizing the states \\emph{around} the dataset, and the actor applies advantage weighted updates extended with state exploration to improve the policy. We evaluate in classic continual control tasks of D4RL, showing that our method performs better than the conservative Q-function learning methods and is strongly competitive among recent SOTA methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108438312",
                        "name": "Liting Chen"
                    },
                    {
                        "authorId": "2107302346",
                        "name": "Jie Yan"
                    },
                    {
                        "authorId": "2205658574",
                        "name": "Zhengdao Shao"
                    },
                    {
                        "authorId": "2163383329",
                        "name": "Lu Wang"
                    },
                    {
                        "authorId": "2793487",
                        "name": "Qingwei Lin"
                    },
                    {
                        "authorId": "2109581369",
                        "name": "Dongmei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thereafter it updates the policy with augmented data in the ensemble model, using TRPO model-free algorithm [64]; Model-Based Policy Optimization (MBPO) [81] samples the branched rollouts with the policy and the learned model, and utilizes SAC [70] to further learn the optimal policy with augmented data.",
                "Thereafter it updates the policy with augmented data in the ensemble model, using TRPO model-free algorithm [64]; Model-Based Policy Optimization (MBPO) [81] samples the branched rollouts with the policy and the learned model, and utilizes SAC [70] to further learn the optimal policy with"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6ae2240d21c96f8c703409fc0e94ed0e0a9f61c7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-05209",
                    "ArXiv": "2302.05209",
                    "DOI": "10.48550/arXiv.2302.05209",
                    "CorpusId": 256808678
                },
                "corpusId": 256808678,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6ae2240d21c96f8c703409fc0e94ed0e0a9f61c7",
                "title": "A Survey on Causal Reinforcement Learning",
                "abstract": "While Reinforcement Learning (RL) achieves tremendous success in sequential decision-making problems of many domains, it still faces key challenges of data inefficiency and the lack of interpretability. Interestingly, many researchers have leveraged insights from the causality literature recently, bringing forth flourishing works to unify the merits of causality and address well the challenges from RL. As such, it is of great necessity and significance to collate these Causal Reinforcement Learning (CRL) works, offer a review of CRL methods, and investigate the potential functionality from causality toward RL. In particular, we divide existing CRL approaches into two categories according to whether their causality-based information is given in advance or not. We further analyze each category in terms of the formalization of different models, ranging from the Markov Decision Process (MDP), Partially Observed Markov Decision Process (POMDP), Multi-Arm Bandits (MAB), and Dynamic Treatment Regime (DTR). Moreover, we summarize the evaluation matrices and open sources while we discuss emerging applications, along with promising prospects for the future development of CRL.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50233704",
                        "name": "Yan Zeng"
                    },
                    {
                        "authorId": "39913331",
                        "name": "Ruichu Cai"
                    },
                    {
                        "authorId": "2323566",
                        "name": "Fuchun Sun"
                    },
                    {
                        "authorId": "2125555462",
                        "name": "Libo Huang"
                    },
                    {
                        "authorId": "145586380",
                        "name": "Z. Hao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous works generally try to resolve this by restricting model usage (Buckman et al., 2018; Janner et al., 2019) or better estimating uncertainty (Chua et al., 2018).",
                "Previous works generally try to resolve this by restricting model usage (Buckman et al., 2018; Janner et al., 2019) or better estimating uncertainty (Chua et al.",
                ", 2018) or only using the model for short rollouts (Buckman et al., 2018; Janner et al., 2019).",
                "Erroneous predictive models can yield deleterious effects on policy learning, which is known as the model exploitation problem (Ross & Bagnell, 2012; Janner et al., 2019; Kidambi et al., 2020; Kang et al., 2022).",
                "MBRL Performance Bound\nWe first present the performance bound of a policy \u03c0 in the original MDPM = (S,A, \u00b5, p, r) and its model-based MDP (Janner et al., 2019).",
                "MBPO Training Procedure\nMBPO (Janner et al., 2019) is a Dyna-style (Sutton, 1991) model-based RL algorithm, which trains a model-free RL method on top of truncated model-based rollouts starting from intermediate environment states.",
                "\u2026et al., 2018; Hafner et al., 2019; Nagabandi et al., 2019), reinforcement learning (Heess et al., 2015; Feinberg et al., 2018; Buckman et al., 2018; Janner et al., 2019; Hafner et al., 2020; Nguyen et al., 2021), or both (Argenson & Dulac-Arnold, 2021; Sikchi et al., 2022; Hansen et al., 2022).",
                "We first present the performance bound of classic MBRL (Janner et al., 2019): Theorem 6.",
                "To avoid making suboptimal decisions based on incorrect models, prior works either restrict the horizon length of model rollouts (Janner et al., 2019) or employ various uncertainty estimation techniques, such as Gaussian processes (Rasmussen & Kuss, 2003; Deisenroth & Rasmussen, 2011) or model\u2026",
                ", 2019), reinforcement learning (Heess et al., 2015; Feinberg et al., 2018; Buckman et al., 2018; Janner et al., 2019; Hafner et al., 2020; Nguyen et al., 2021), or both (Argenson & Dulac-Arnold, 2021; Sikchi et al.",
                "Previous approaches often try to address model exploitation by estimating model uncertainty (Chua et al., 2018) or only using the model for short rollouts (Buckman et al., 2018; Janner et al., 2019).",
                "The first is based on MBPO (Janner et al., 2019), which we describe in Section 5.1.",
                "MBPO (Janner et al., 2019) is a Dyna-style model-based RL algorithm that trains a model-free RL method on top of truncated model-based rollouts starting from intermediate environment states.",
                "We first present the performance bound of classic MBRL (Janner et al., 2019):\nTheorem 6.2.",
                "(38)\nEquation (23) is the same bound as Lemma B.3 in Janner et al. (2019), but we use a milder assumption in Equation (22), which only assumes that the expectation (not the maximum) of the total variation distance between the policies is bounded.",
                "The first is based on MBPO (Janner et al., 2019), which we describe in Section 5.",
                "To avoid making suboptimal decisions based on incorrect models, prior works either restrict the horizon length of model rollouts (Janner et al., 2019) or employ various uncertainty estimation techniques, such as Gaussian processes (Rasmussen & Kuss, 2003; Deisenroth & Rasmussen, 2011) or model ensembles (Rajeswaran et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "aeaddc24fe0c4179105e483e83cbe1387515914b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-03921",
                    "ArXiv": "2302.03921",
                    "DOI": "10.48550/arXiv.2302.03921",
                    "CorpusId": 256662404
                },
                "corpusId": 256662404,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/aeaddc24fe0c4179105e483e83cbe1387515914b",
                "title": "Predictable MDP Abstraction for Unsupervised Model-Based RL",
                "abstract": "A key component of model-based reinforcement learning (RL) is a dynamics model that predicts the outcomes of actions. Errors in this predictive model can degrade the performance of model-based controllers, and complex Markov decision processes (MDPs) can present exceptionally difficult prediction problems. To mitigate this issue, we propose predictable MDP abstraction (PMA): instead of training a predictive model on the original MDP, we train a model on a transformed MDP with a learned action space that only permits predictable, easy-to-model actions, while covering the original state-action space as much as possible. As a result, model learning becomes easier and more accurate, which allows robust, stable model-based planning or model-based RL. This transformation is learned in an unsupervised manner, before any task is specified by the user. Downstream tasks can then be solved with model-based control in a zero-shot fashion, without additional environment interactions. We theoretically analyze PMA and empirically demonstrate that PMA leads to significant improvements over prior unsupervised model-based RL approaches in a range of benchmark environments. Our code and videos are available at https://seohong.me/projects/pma/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118885320",
                        "name": "Seohong Park"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The model is trained using negative log likelihood loss (Janner et al.,\n2019): L(\u03b8k) = \u2211N t=1[\u00b5\u03b8k(st, at) \u2212 st+1]\n\u22a4\u03a3\u22121\u03b8k (st, at)[\u00b5\u03b8k(st, at) \u2212 st+1] + log det\u03a3\u03b8k(st, at) During model rollouts, the probabilistic dynamics model ensemble first randomly selects a network from the ensemble and then\u2026",
                "For the probabilistic dynamics model ensemble, we set the ensemble size to 7 which is the setting used in the original paper of MBPO (Janner et al., 2019).",
                "We first conduct an experiment on the MuJoCo Humanoid environment with MBPO (Janner et al., 2019), the SOTA model-based algorithm using Soft Actor-Critic (SAC) (Haarnoja et al.",
                "We first conduct an experiment on the MuJoCo Humanoid environment with MBPO (Janner et al., 2019), the SOTA model-based algorithm using Soft Actor-Critic (SAC) (Haarnoja et al., 2018) as the backbone algorithm for policy and value optimization.",
                "Among these approaches, the most popular and common approach is to use an ensemble of probabilistic dynamics models (Buckman et al., 2018; Janner et al., 2019; Lai et al., 2020; Clavera et al., 2020; Froehlich et al., 2022; Li et al., 2022).",
                "The model is trained using negative log likelihood loss (Janner et al., 2019): L(\u03b8k) = \u2211N t=1[\u03bc\u03b8k(st, at) \u2212 st+1] \u22a4\u03a3\u22121 \u03b8k (st, at)[\u03bc\u03b8k(st, at) \u2212 st+1] + log det\u03a3\u03b8k(st, at) During model rollouts, the probabilistic dynamics model ensemble first randomly selects a network from the ensemble and then samples the next state from the predicted Gaussian distribution.",
                "We compare the two training mechanisms with MBPO (Janner et al., 2019) using an ensemble of probabilistic transition models."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "53a058a02c16d31acd2176c1104ac95e544fdcfa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-01244",
                    "ArXiv": "2302.01244",
                    "DOI": "10.48550/arXiv.2302.01244",
                    "CorpusId": 256503666
                },
                "corpusId": 256503666,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/53a058a02c16d31acd2176c1104ac95e544fdcfa",
                "title": "Is Model Ensemble Necessary? Model-based RL via a Single Model with Lipschitz Regularized Value Function",
                "abstract": "Probabilistic dynamics model ensemble is widely used in existing model-based reinforcement learning methods as it outperforms a single dynamics model in both asymptotic performance and sample efficiency. In this paper, we provide both practical and theoretical insights on the empirical success of the probabilistic dynamics model ensemble through the lens of Lipschitz continuity. We find that, for a value function, the stronger the Lipschitz condition is, the smaller the gap between the true dynamics- and learned dynamics-induced Bellman operators is, thus enabling the converged value function to be closer to the optimal value function. Hence, we hypothesize that the key functionality of the probabilistic dynamics model ensemble is to regularize the Lipschitz condition of the value function using generated samples. To test this hypothesis, we devise two practical robust training mechanisms through computing the adversarial noise and regularizing the value network's spectral norm to directly regularize the Lipschitz condition of the value functions. Empirical results show that combined with our mechanisms, model-based RL algorithms with a single dynamics model outperform those with an ensemble of probabilistic dynamics models. These findings not only support the theoretical insight, but also provide a practical solution for developing computationally efficient model-based RL algorithms.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108815599",
                        "name": "Ruijie Zheng"
                    },
                    {
                        "authorId": "48630770",
                        "name": "Xiyao Wang"
                    },
                    {
                        "authorId": "3286703",
                        "name": "Huazhe Xu"
                    },
                    {
                        "authorId": "40070055",
                        "name": "Furong Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[23] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",
                "[18] \u2225\u03c1 \u2212 \u03c1\u22251 \u2264 2tD TV (\u03c0||\u03b2) \u2264 2t \u221a Dmax KL (\u03c0||\u03b2) (79) which can be adopted from [34] [24] [35] [23]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "41c94687c88c8e98bf62b58fad4ede2312adfdfd",
                "externalIds": {
                    "ArXiv": "2302.00533",
                    "DBLP": "journals/corr/abs-2302-00533",
                    "DOI": "10.48550/arXiv.2302.00533",
                    "CorpusId": 256459415
                },
                "corpusId": 256459415,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/41c94687c88c8e98bf62b58fad4ede2312adfdfd",
                "title": "Distillation Policy Optimization",
                "abstract": "While on-policy algorithms are known for their stability, they often demand a substantial number of samples. In contrast, off-policy algorithms, which leverage past experiences, are considered sample-efficient but tend to exhibit instability. Can we develop an algorithm that harnesses the benefits of off-policy data while maintaining stable learning? In this paper, we introduce an actor-critic learning framework that harmonizes two data sources for both evaluation and control, facilitating rapid learning and adaptable integration with on-policy algorithms. This framework incorporates variance reduction mechanisms, including a unified advantage estimator (UAE) and a residual baseline, improving the efficacy of both on- and off-policy learning. Our empirical results showcase substantial enhancements in sample efficiency for on-policy algorithms, effectively bridging the gap to the off-policy approaches. It demonstrates the promise of our approach as a novel learning paradigm.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2005408902",
                        "name": "Jianfei Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar to previous work (Janner et al., 2019), we modelled state transition dynamics as a multivariate normal distribution with a diagonal covariance matrix, where the vector of means and log-standard deviations were outputted from a single feed-forward neural network with two hidden layers of 200 units each.",
                "While guarantees of many OPE methods require similar assumptions (Janner et al., 2019; Le et al., 2019; Xie et al., 2019), extending the MSBE to have better guarantees in the face of partial coverage (Uehara et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5fe4712d9abbb60f070028b89355966c1e2bc91a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-00141",
                    "ArXiv": "2302.00141",
                    "DOI": "10.48550/arXiv.2302.00141",
                    "CorpusId": 253187438
                },
                "corpusId": 253187438,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5fe4712d9abbb60f070028b89355966c1e2bc91a",
                "title": "Revisiting Bellman Errors for Offline Model Selection",
                "abstract": "Offline model selection (OMS), that is, choosing the best policy from a set of many policies given only logged data, is crucial for applying offline RL in real-world settings. One idea that has been extensively explored is to select policies based on the mean squared Bellman error (MSBE) of the associated Q-functions. However, previous work has struggled to obtain adequate OMS performance with Bellman errors, leading many researchers to abandon the idea. To this end, we elucidate why previous work has seen pessimistic results with Bellman errors and identify conditions under which OMS algorithms based on Bellman errors will perform well. Moreover, we develop a new estimator of the MSBE that is more accurate than prior methods. Our estimator obtains impressive OMS performance on diverse discrete control tasks, including Atari games.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35491194",
                        "name": "Joshua P. Zitovsky"
                    },
                    {
                        "authorId": "9249162",
                        "name": "D. Marchi"
                    },
                    {
                        "authorId": "29767024",
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "authorId": "2203976371",
                        "name": "Michael R. Kosorok University of North Carolina at Chapel Hill"
                    },
                    {
                        "authorId": "2203937165",
                        "name": "Google Research Brain Team"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2021) and learned transition models (Chua et al., 2018; Kurutach et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020; As et al., 2022), and has been applied to promote both exploration and safety."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0ae0a029f91fcb43d5b4bdecf0b891c14267faa2",
                "externalIds": {
                    "ArXiv": "2301.12593",
                    "DBLP": "journals/corr/abs-2301-12593",
                    "DOI": "10.48550/arXiv.2301.12593",
                    "CorpusId": 256390545
                },
                "corpusId": 256390545,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0ae0a029f91fcb43d5b4bdecf0b891c14267faa2",
                "title": "Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning",
                "abstract": "Many real-world domains require safe decision making in the presence of uncertainty. In this work, we propose a deep reinforcement learning framework for approaching this important problem. We consider a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures, and we show that our formulation is equivalent to a distributionally robust safe reinforcement learning problem with robustness guarantees on performance and safety. We propose an ef\ufb01cient implementation that only requires access to a single training environment, and we demonstrate that our framework produces robust, safe performance on a variety of continuous control tasks with safety constraints in the Real-World Reinforcement Learning Suite.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2039906784",
                        "name": "James Queeney"
                    },
                    {
                        "authorId": "1730046",
                        "name": "M. Benosman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "dc65508248e7e43ff2d2287ca27a93080b87a24a",
                "externalIds": {
                    "DBLP": "conf/icml/ChakrabortyBKWH23",
                    "ArXiv": "2301.12038",
                    "DOI": "10.48550/arXiv.2301.12038",
                    "CorpusId": 256389712
                },
                "corpusId": 256389712,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/dc65508248e7e43ff2d2287ca27a93080b87a24a",
                "title": "STEERING: Stein Information Directed Exploration for Model-Based Reinforcement Learning",
                "abstract": "Directed Exploration is a crucial challenge in reinforcement learning (RL), especially when rewards are sparse. Information-directed sampling (IDS), which optimizes the information ratio, seeks to do so by augmenting regret with information gain. However, estimating information gain is computationally intractable or relies on restrictive assumptions which prohibit its use in many practical instances. In this work, we posit an alternative exploration incentive in terms of the integral probability metric (IPM) between a current estimate of the transition model and the unknown optimal, which under suitable conditions, can be computed in closed form with the kernelized Stein discrepancy (KSD). Based on KSD, we develop a novel algorithm \\algo: \\textbf{STE}in information dir\\textbf{E}cted exploration for model-based \\textbf{R}einforcement Learn\\textbf{ING}. To enable its derivation, we develop fundamentally new variants of KSD for discrete conditional distributions. {We further establish that {\\algo} archives sublinear Bayesian regret, improving upon prior learning rates of information-augmented MBRL.} Experimentally, we show that the proposed algorithm is computationally affordable and outperforms several prior approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "49081354",
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "authorId": "3387859",
                        "name": "A. S. Bedi"
                    },
                    {
                        "authorId": "2063871",
                        "name": "Alec Koppel"
                    },
                    {
                        "authorId": "145731462",
                        "name": "Mengdi Wang"
                    },
                    {
                        "authorId": "40070055",
                        "name": "Furong Huang"
                    },
                    {
                        "authorId": "2172597446",
                        "name": "Dinesh Manocha"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare CAROL with the following methods: (1) MBPO [17], our base RL algorithm.",
                "We implement CAROL on top of the MBPO [17] model-based RL algorithm using the implementation from [27].",
                "During exploration, our algorithm learns a model of the environment using an existing model-based reinforcement learning algorithm [17].",
                "3: for N epochs do 4: Train model E\u03b8 on Denv via maximum likelihood 5: Unroll M trajectories int he model under \u03c0\u03c8; add to Dmodel 6: Take action in environment according to \u03c0\u03c8; add to Denv 7: for G gradient updates do 8: Calculate normal policy loss L(\u03c0\u03c8,Dmodel) as in MBPO [17] 9: Sample \u27e8st, at, st+1, rt\u27e9 uniformly from Dmodel 10: Rollout \u03c0 starting from st under E\u03b8 for Ttrain steps and compute the total reward R 11: Compute the worst-case reward Rmin using Algorithm 2 over horizon Ttrain."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "107d45aa934b990114f4a5a93a3917c8736f8ad0",
                "externalIds": {
                    "ArXiv": "2301.11374",
                    "CorpusId": 258960107
                },
                "corpusId": 258960107,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/107d45aa934b990114f4a5a93a3917c8736f8ad0",
                "title": "Certifiably Robust Reinforcement Learning through Model-Based Abstract Interpretation",
                "abstract": "We present a reinforcement learning (RL) framework in which the learned policy comes with a machine-checkable certificate of provable adversarial robustness. Our approach, called CAROL, learns a model of the environment. In each learning iteration, it uses the current version of this model and an external abstract interpreter to construct a differentiable signal for provable robustness. This signal is used to guide learning, and the abstract interpretation used to construct it directly leads to the robustness certificate returned at convergence. We give a theoretical analysis that bounds the worst-case accumulative reward of CAROL. We also experimentally evaluate CAROL on four MuJoCo environments with continuous state and action spaces. On these tasks, CAROL learns policies that, when contrasted with policies from the state-of-the-art robust RL algorithms, exhibit: (i) markedly enhanced certified performance lower bounds; and (ii) comparable performance under empirical adversarial attacks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46962421",
                        "name": "Chenxi Yang"
                    },
                    {
                        "authorId": "2064997780",
                        "name": "Greg Anderson"
                    },
                    {
                        "authorId": "35865989",
                        "name": "Swarat Chaudhuri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2019) [41] proposed model-based Policy Optimization (MBPO) algorithm to boost PETS through monotonic improvement at each step to get the best performance."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "13363a90ddaa448f3778eacfbd60d5f2bd6e7359",
                "externalIds": {
                    "ArXiv": "2301.09297",
                    "DOI": "10.1063/5.0155574",
                    "CorpusId": 257426966,
                    "PubMed": "37561122"
                },
                "corpusId": 257426966,
                "publicationVenue": {
                    "id": "30c0ded7-c8b4-473c-bbc0-f237234ac1a6",
                    "name": "Chaos",
                    "type": "journal",
                    "issn": "1054-1500",
                    "url": "http://chaos.aip.org/",
                    "alternate_urls": [
                        "https://aip.scitation.org/journal/cha"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/13363a90ddaa448f3778eacfbd60d5f2bd6e7359",
                "title": "Model-based reinforcement learning with non-Gaussian environment dynamics and its application to portfolio optimization.",
                "abstract": "The rapid development of quantitative portfolio optimization in financial engineering has produced promising results in AI-based algorithmic trading strategies. However, the complexity of financial markets poses challenges for comprehensive simulation due to various factors, such as abrupt transitions, unpredictable hidden causal factors, and heavy tail properties. This paper aims to address these challenges by employing heavy-tailed preserving normalizing flows to simulate the high-dimensional joint probability of the complex trading environment under a model-based reinforcement learning framework. Through experiments with various stocks from three financial markets (Dow, NASDAQ, and S&P), we demonstrate that Dow outperforms the other two based on multiple evaluation metrics in our testing system. Notably, our proposed method mitigates the impact of unpredictable financial market crises during the COVID-19 pandemic, resulting in a lower maximum drawdown. Additionally, we explore the explanation of our reinforcement learning algorithm, employing the pattern causality method to study interactive relationships among stocks, analyzing dynamics of training for loss functions to ensure convergence, visualizing high-dimensional state transition data with t-SNE to uncover effective patterns for portfolio optimization, and utilizing eigenvalue analysis to study convergence properties of the environment's model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2151675056",
                        "name": "Huifang Huang"
                    },
                    {
                        "authorId": "2072687794",
                        "name": "Ting Gao"
                    },
                    {
                        "authorId": "2118111202",
                        "name": "Pengbo Li"
                    },
                    {
                        "authorId": "2157959267",
                        "name": "Jinqiu Guo"
                    },
                    {
                        "authorId": "40075735",
                        "name": "P. Zhang"
                    },
                    {
                        "authorId": "2140321947",
                        "name": "Nan Du"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This may be explained by the relatively small scale of policy shift with respect to model error, as observed in [Janner et al., 2019].",
                "The model-based baselines include MBPO [Janner et al., 2019], which uses short-horizon rollouts branched from the state distribution of previous policies, SLBO [Luo et al.",
                "In Walker2d, the best rollout length is one step in our implementation and MBPO, thus resulting in similar performance between these two methods.",
                "In our experiments, we plug the model learning process of P2P into MBPO since it is a widely accepted strong baseline in MBRL.",
                "The model-based baselines include MBPO [Janner et al., 2019], which uses short-horizon rollouts branched from the state distribution of previous policies, SLBO [Luo et al., 2019], which enjoys theoretical performance guarantee and uses model rollouts from the initial state distribution, and STEVE [Buckman et al., 2018], which also generates short rollouts but uses model data for estimating target values instead of policy learning."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "abd08e482d830ad942620c3ec8b3170518866788",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-08502",
                    "ArXiv": "2301.08502",
                    "DOI": "10.48550/arXiv.2301.08502",
                    "CorpusId": 256080404
                },
                "corpusId": 256080404,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abd08e482d830ad942620c3ec8b3170518866788",
                "title": "Plan To Predict: Learning an Uncertainty-Foreseeing Model for Model-Based Reinforcement Learning",
                "abstract": "In Model-based Reinforcement Learning (MBRL), model learning is critical since an inaccurate model can bias policy learning via generating misleading samples. However, learning an accurate model can be difficult since the policy is continually updated and the induced distribution over visited states used for model learning shifts accordingly. Prior methods alleviate this issue by quantifying the uncertainty of model-generated samples. However, these methods only quantify the uncertainty passively after the samples were generated, rather than foreseeing the uncertainty before model trajectories fall into those highly uncertain regions. The resulting low-quality samples can induce unstable learning targets and hinder the optimization of the policy. Moreover, while being learned to minimize one-step prediction errors, the model is generally used to predict for multiple steps, leading to a mismatch between the objectives of model learning and model usage. To this end, we propose \\emph{Plan To Predict} (P2P), an MBRL framework that treats the model rollout process as a sequential decision making problem by reversely considering the model as a decision maker and the current policy as the dynamics. In this way, the model can quickly adapt to the current policy and foresee the multi-step future uncertainty when generating trajectories. Theoretically, we show that the performance of P2P can be guaranteed by approximately optimizing a lower bound of the true environment return. Empirical results demonstrate that P2P achieves state-of-the-art performance on several challenging benchmark tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109687122",
                        "name": "Zifan Wu"
                    },
                    {
                        "authorId": "2155747980",
                        "name": "Chao Yu"
                    },
                    {
                        "authorId": "40590308",
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "40513470",
                        "name": "Jianye Hao"
                    },
                    {
                        "authorId": "74076606",
                        "name": "H. Zhuo"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", on Prioritized Experience Replay buffers [19, 26, 39], which are a form of (limited) non-parametric models [45]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c19cb0567d46e45c9a5c9ab546a56d81e58230c8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-07784",
                    "ArXiv": "2301.07784",
                    "DOI": "10.48550/arXiv.2301.07784",
                    "CorpusId": 256000043
                },
                "corpusId": 256000043,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c19cb0567d46e45c9a5c9ab546a56d81e58230c8",
                "title": "Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization",
                "abstract": "Multi-objective reinforcement learning (MORL) algorithms tackle sequential decision problems where agents may have different preferences over (possibly conflicting) reward functions. Such algorithms often learn a set of policies (each optimized for a particular agent preference) that can later be used to solve problems with novel preferences. We introduce a novel algorithm that uses Generalized Policy Improvement (GPI) to define principled, formally-derived prioritization schemes that improve sample-efficient learning. They implement active-learning strategies by which the agent can (i) identify the most promising preferences/objectives to train on at each moment, to more rapidly solve a given MORL problem; and (ii) identify which previous experiences are most relevant when learning a policy for a particular agent preference, via a novel Dyna-style MORL method. We prove our algorithm is guaranteed to always converge to an optimal solution in a finite number of steps, or an $\\epsilon$-optimal solution (for a bounded $\\epsilon$) if the agent is limited and can only identify possibly sub-optimal policies. We also prove that our method monotonically improves the quality of its partial solutions while learning. Finally, we introduce a bound that characterizes the maximum utility loss (with respect to the optimal solution) incurred by the partial solutions computed by our method throughout learning. We empirically show that our method outperforms state-of-the-art MORL algorithms in challenging multi-objective tasks, both with discrete and continuous state and action spaces.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1411722140",
                        "name": "L. N. Alegre"
                    },
                    {
                        "authorId": "1707374",
                        "name": "A. Bazzan"
                    },
                    {
                        "authorId": "1917202",
                        "name": "Diederik M. Roijers"
                    },
                    {
                        "authorId": "2175934496",
                        "name": "Ann Now'e"
                    },
                    {
                        "authorId": "145471664",
                        "name": "Bruno C. da Silva"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model based reinforcement learning (Clavera et al., 2020, 2018; Janner et al., 2019; Wang et al., 2019) builds an additional internal model of the environment to predict next states given the current state and action being taken."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6b61df20462587fad5d7e0540dc9d41eb8d535ee",
                "externalIds": {
                    "ArXiv": "2301.03933",
                    "DBLP": "journals/corr/abs-2301-03933",
                    "DOI": "10.48550/arXiv.2301.03933",
                    "CorpusId": 255569733
                },
                "corpusId": 255569733,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6b61df20462587fad5d7e0540dc9d41eb8d535ee",
                "title": "Hint assisted reinforcement learning: an application in radio astronomy",
                "abstract": ",",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2174490",
                        "name": "S. Yatawatta"
                    }
                ]
            }
        },
        {
            "contexts": [
                "RAC-SAC RAC-TD3 REDQ MBPO TQC20 TD3 SAC TQC\nHumanoid 11,107 \u00b1 475 9,321 \u00b1 1,126 5,504 \u00b1 120 5,162 \u00b1 350 7,053 \u00b1 857 7,014 \u00b1 643 7,681 \u00b1 1,118 10,731 \u00b1 1,296\nAnt 6,283 \u00b1 549 6,470 \u00b1 165 5,475 \u00b1 890 5,281 \u00b1 699 4,722 \u00b1 567 6,796 \u00b1 277 6,433\u00b1 332 6,402\u00b1 1,371\nWalker 5,860 \u00b1 440 5,114 \u00b1 489 5,034 \u00b1 711 4,864 \u00b1 488 5,109 \u00b1 696 4,419 \u00b1 1,682 5,249 \u00b1 554 5,821 \u00b1 457\nHopper 3,421 \u00b1 483 3,495 \u00b1 672 3,563 \u00b1 94 3,280 \u00b1 455 3,208 \u00b1 538 3,433 \u00b1 321 2,815 \u00b1 585 3,011 \u00b1 866\nHalfCheetah 15,717 \u00b1 1,063 15,083 \u00b1 1,113 10,802 \u00b1 1,179 13,477 \u00b1 443 12,123 \u00b1 2,600 14,462 \u00b1 1,982 16,330 \u00b1 323 17,245 \u00b1 293\nSwimmer 143 \u00b1 6.8 71 \u00b1 83 98 \u00b1 31 - 143 \u00b1 9.6 53 \u00b1 8.8 51 \u00b1 4.2 65 \u00b1 5.8\nThemaximum value for each task is bolded.",
                ", 2021), MBPO (Janner et al., 2019), SAC (Haarnoja et al.",
                "For MBPO (https://github.com/JannerM/ mbpo), REDQ (https://github.com/watchernyu/REDQ), TD3 (https://github.com/sfujim/TD3), and TQC (https://github. com/SamsungLabs/tqc_pytorch), we use the authors\u2019 code.",
                "RAC outperforms the current state-of-the-art algorithms (MBPO Janner et al., 2019, REDQ Chen et al., 2021, and TQC Kuznetsov et al., 2020), achieving state-of-the-art sample efficiency on the Humanoid benchmark.",
                "The baseline algorithms are REDQ (Chen et al., 2021), MBPO (Janner et al., 2019), SAC (Haarnoja et al., 2018), TD3 (Fujimoto et al., 2018), and TQC (Kuznetsov et al., 2020).",
                "RACSAC\nREDQ MBPO TQC TQC20 REDQ/RACSAC MBPO/RACSAC TQC/RACSAC TQC20/RACSAC\nHumanoid at 2,000 63 K 109 K 154 K 145 K 147 K 1.73 2.44 2.30 2.33\nHumanoid at 5,000 134 K 250 K 295 K 445 K 258 K 1.87 2.20 3.32 1.93\nHumanoid at 10,000 552 K - - 3,260 K - - - 5.91 -\nAnt at 1,000 21 K 28 K 62 K 185 K 42 K 1.33 2.95 8.81 2.00\nAnt at 3,000 56 K 56 K 152 K 940 K 79K 1.00 2.71 16.79 1.41\nAnt at 6,000 248 K - - 3,055 K - - - 12.31 -\nWalker at 1,000 27 K 42 K 54 K 110 K 50 K 1.56 2.00 4.07 1.85\nWalker at 3,000 53 K 79 K 86 K 270 K 89K 1.49 1.62 10.75 1.68\nWalker at 5,000 147 K 272 K - 960 K 270 K 1.85 - 6.53 1.84\nSample efficiency (Chen et al., 2021; Dorner, 2021) is measured by the ratio of the number of samples collected when RAC and some algorithms reach the specified performance.",
                "Results of MBPO are obtained at 3 \u00d7 105 time steps for Ant, Humanoid, and Walker2d, 4 \u00d7 105 for HalfCheetah and 1.25\u00d7 105 for Hopper."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b1cdd73c39fbe7b4e9b758389554efc51d121447",
                "externalIds": {
                    "DBLP": "journals/finr/LiTPMW22",
                    "PubMedCentral": "9868235",
                    "DOI": "10.3389/fnbot.2022.1081242",
                    "CorpusId": 249642286,
                    "PubMed": "36699950"
                },
                "corpusId": 249642286,
                "publicationVenue": {
                    "id": "de454aec-8c73-4737-bb1f-5231453ca8fa",
                    "name": "Frontiers in Neurorobotics",
                    "type": "journal",
                    "alternate_names": [
                        "Front Neurorobotics"
                    ],
                    "issn": "1662-5218",
                    "url": "https://www.frontiersin.org/journals/neurorobotics#articles",
                    "alternate_urls": [
                        "http://www.frontiersin.org/neurorobotics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b1cdd73c39fbe7b4e9b758389554efc51d121447",
                "title": "Realistic Actor-Critic: A framework for balance between value overestimation and underestimation",
                "abstract": "Introduction The value approximation bias is known to lead to suboptimal policies or catastrophic overestimation bias accumulation that prevent the agent from making the right decisions between exploration and exploitation. Algorithms have been proposed to mitigate the above contradiction. However, we still lack an understanding of how the value bias impact performance and a method for efficient exploration while keeping stable updates. This study aims to clarify the effect of the value bias and improve the reinforcement learning algorithms to enhance sample efficiency. Methods This study designs a simple episodic tabular MDP to research value underestimation and overestimation in actor-critic methods. This study proposes a unified framework called Realistic Actor-Critic (RAC), which employs Universal Value Function Approximators (UVFA) to simultaneously learn policies with different value confidence-bound with the same neural network, each with a different under overestimation trade-off. Results This study highlights that agents could over-explore low-value states due to inflexible under-overestimation trade-off in the fixed hyperparameters setting, which is a particular form of the exploration-exploitation dilemma. And RAC performs directed exploration without over-exploration using the upper bounds while still avoiding overestimation using the lower bounds. Through carefully designed experiments, this study empirically verifies that RAC achieves 10x sample efficiency and 25% performance improvement compared to Soft Actor-Critic in the most challenging Humanoid environment. All the source codes are available at https://github.com/ihuhuhu/RAC. Discussion This research not only provides valuable insights for research on the exploration-exploitation trade-off by studying the frequency of policies access to low-value states under different value confidence-bounds guidance, but also proposes a new unified framework that can be combined with current actor-critic methods to improve sample efficiency in the continuous control domain.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143204744",
                        "name": "Sicen Li"
                    },
                    {
                        "authorId": "2151875551",
                        "name": "Qinyun Tang"
                    },
                    {
                        "authorId": "46767237",
                        "name": "Yiming Pang"
                    },
                    {
                        "authorId": "2170236920",
                        "name": "Xinmeng Ma"
                    },
                    {
                        "authorId": "2096527",
                        "name": "G. Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In fact, although model-based methods are data-efficient, they suffer from the compounding prediction error increasing with model rollout length, which greatly affects the performance and limits model rollout length [Janner et al., 2019]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "638b5c76d96e32f54475a8327a9c68e0167156a9",
                "externalIds": {
                    "ArXiv": "2301.03044",
                    "DBLP": "journals/corr/abs-2301-03044",
                    "DOI": "10.48550/arXiv.2301.03044",
                    "CorpusId": 255546304
                },
                "corpusId": 255546304,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/638b5c76d96e32f54475a8327a9c68e0167156a9",
                "title": "A Survey on Transformers in Reinforcement Learning",
                "abstract": "Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159213989",
                        "name": "Wenzhe Li"
                    },
                    {
                        "authorId": "2199828096",
                        "name": "Hao Luo"
                    },
                    {
                        "authorId": "41123614",
                        "name": "Zichuan Lin"
                    },
                    {
                        "authorId": "2111387140",
                        "name": "Chongjie Zhang"
                    },
                    {
                        "authorId": "2265693",
                        "name": "Zongqing Lu"
                    },
                    {
                        "authorId": "2055648566",
                        "name": "Deheng Ye"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Popular model-based RL methods include Guestrin et al. (2002); Janner et al. (2019); Lai et al. (2020); Li et al. (2020), to name a few."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6e97544840a37042c55c53f2808614d39a066f93",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-00927",
                    "ArXiv": "2301.00927",
                    "DOI": "10.48550/arXiv.2301.00927",
                    "CorpusId": 255393661
                },
                "corpusId": 255393661,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6e97544840a37042c55c53f2808614d39a066f93",
                "title": "Deep Spectral Q-learning with Application to Mobile Health",
                "abstract": "Dynamic treatment regimes assign personalized treatments to patients sequentially over time based on their baseline information and time-varying covariates. In mobile health applications, these covariates are typically collected at different frequencies over a long time horizon. In this paper, we propose a deep spectral Q-learning algorithm, which integrates principal component analysis (PCA) with deep Q-learning to handle the mixed frequency data. In theory, we prove that the mean return under the estimated optimal policy converges to that under the optimal one and establish its rate of convergence. The usefulness of our proposal is further illustrated via simulations and an application to a diabetes dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2199245566",
                        "name": "Yuhe Gao"
                    },
                    {
                        "authorId": "6086301",
                        "name": "C. Shi"
                    },
                    {
                        "authorId": "145401368",
                        "name": "R. Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the field of deep reinforcement learning, new algorithms using various optimization theory, information theory, and control theory are being actively developed [41], [42], [43], [44], [45], [46], [47], [48], [49]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "85d4cf8e1bacdc77aa45a901415a843d40b25192",
                "externalIds": {
                    "DBLP": "journals/tvt/LeeJL23",
                    "DOI": "10.1109/TVT.2022.3207510",
                    "CorpusId": 252390736
                },
                "corpusId": 252390736,
                "publicationVenue": {
                    "id": "983b0731-eddf-4f05-9c9b-81059a9f9c51",
                    "name": "IEEE Transactions on Vehicular Technology",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Veh Technol"
                    ],
                    "issn": "0018-9545",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=25",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=25"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/85d4cf8e1bacdc77aa45a901415a843d40b25192",
                "title": "Deep Reinforcement Learning of Semi-Active Suspension Controller for Vehicle Ride Comfort",
                "abstract": "Among the controllable suspension systems, the control of the semi-active suspension is mostly based on optimal control. Recently, deep reinforcement learning is widely used as a method to solve the optimal control problem. Control strategies developed using reinforcement learning have shown performance beyond conventional control algorithms in some fields. In the current study, we have proposed a near optimal semi-active suspension ride comfort controller using deep reinforcement learning. An algorithm suitable for a semi-active suspension control environment was selected based on deep reinforcement learning theory to increase convergence in training. Furthermore, a state normalization filter was designed to improve the generalization performance. When compared with the ride comfort oriented classical control algorithms, our trained controller showed the best performance in terms of ride comfort. Policy map comparison with mixed SH-ADD (Skyhook-Acceleration Driven Damping) algorithm suggested the direction to the design of the semi-active suspension control algorithm.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2115418657",
                        "name": "Daekyun Lee"
                    },
                    {
                        "authorId": "2148370155",
                        "name": "S. Jin"
                    },
                    {
                        "authorId": "50521378",
                        "name": "Chibum Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Taking the cumulative reward subjected to a policy in the actual environment as \u03b7 and its counterpart in the constructed virtual environment model as \u03b7M, we can achieve the relationship between \u03b7 and \u03b7M within k iteration steps as [37]:"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9410e6edfbf6a245e76ae51bff75fcfd74661bd1",
                "externalIds": {
                    "DBLP": "journals/tiv/WuHL23",
                    "DOI": "10.1109/TIV.2022.3185159",
                    "CorpusId": 249942277
                },
                "corpusId": 249942277,
                "publicationVenue": {
                    "id": "c2eeb1be-38e9-4a0a-a89d-957f4fd71ea1",
                    "name": "IEEE Transactions on Intelligent Vehicles",
                    "alternate_names": [
                        "IEEE Trans Intell Veh"
                    ],
                    "issn": "2379-8858",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274857"
                },
                "url": "https://www.semanticscholar.org/paper/9410e6edfbf6a245e76ae51bff75fcfd74661bd1",
                "title": "Uncertainty-Aware Model-Based Reinforcement Learning: Methodology and Application in Autonomous Driving",
                "abstract": "To further improve learning efficiency and performance of reinforcement learning (RL), a novel uncertainty-aware model-based RL method is proposed and validated in autonomous driving scenarios in this paper. First, an action-conditioned ensemble model with the capability of uncertainty assessment is established as the environment model. Then, a novel uncertainty-aware model-based RL method is developed based on the adaptive truncation approach, providing virtual interactions between the agent and environment model, and improving RL\u2019s learning efficiency and performance. The proposed method is then implemented in end-to-end autonomous vehicle control tasks, validated and compared with state-of-the-art methods under various driving scenarios. Validation results suggest that the proposed method outperforms the model-free RL approach with respect to learning efficiency, and model-based approach with respect to both efficiency and performance, demonstrating its feasibility and effectiveness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47875689",
                        "name": "Jingda Wu"
                    },
                    {
                        "authorId": "47272000",
                        "name": "Zhiyu Huang"
                    },
                    {
                        "authorId": "144818584",
                        "name": "Chen Lv"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Function approximation: Stochastic policies Focusing on the most recent works on model-based RL (Janner et al. 2019; Yu et al. 2020), stochastic Gaussian networks can be used as the function approximator to learn the policy f \u2248 T , such that:",
                "Function approximation: Stochastic policies Focusing on the most recent works on model-based RL (Janner et al. 2019; Yu et al. 2020), stochastic Gaussian networks can be used as the function approximator to learn the policy f \u2248 T , such that:\nf = \u2206s\u0302\ndt \u223c N\n( \u00b5s,a, \u03c3 2 s,a ) (9)\nThis allows to\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f369b4f2eeb7ca8d82e8a2e4ac6626ab12b76014",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-07123",
                    "ArXiv": "2212.07123",
                    "DOI": "10.48550/arXiv.2212.07123",
                    "CorpusId": 254636341
                },
                "corpusId": 254636341,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f369b4f2eeb7ca8d82e8a2e4ac6626ab12b76014",
                "title": "Reinforcement Learning in System Identification",
                "abstract": "System identi\ufb01cation, also known as learning forward models, transfer functions, system dynamics, etc., has a long tradition both in science and engineering in different \ufb01elds. Particularly, it is a recurring theme in Reinforcement Learning research, where forward models approximate the state transition function of a Markov Decision Process by learning a mapping function from current state and action to the next state. This problem is commonly de\ufb01ned as a Supervised Learning problem in a direct way. This common approach faces several dif\ufb01culties due to the inherent complexities of the dynamics to learn, for example, delayed effects, high non-linearity, non-stationarity, partial observability and, more important, error accumulation when using bootstrapped predictions (predictions based on past predictions), over large time horizons. Here we explore the use of Reinforcement Learning in this problem. We elaborate on why and how this problem \ufb01ts naturally and sound as a Reinforcement Learning problem, and present some experimental results that demonstrate RL is a promising technique to solve these kind of problems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2057764597",
                        "name": "J. Antonio"
                    },
                    {
                        "authorId": "2231974257",
                        "name": "Martin H Oscar Fern\u00e1ndez"
                    },
                    {
                        "authorId": "2185507147",
                        "name": "Sergio P\u00e9rez"
                    },
                    {
                        "authorId": "2196944467",
                        "name": "Anas Belfadil"
                    },
                    {
                        "authorId": "1402943547",
                        "name": "C. Ib\u00e1\u00f1ez-Llano"
                    },
                    {
                        "authorId": "2230643012",
                        "name": "Freddy Jos\u00e9 Perozo"
                    },
                    {
                        "authorId": "2228404118",
                        "name": "Rond\u00f3n Jose"
                    },
                    {
                        "authorId": "2228033029",
                        "name": "Javier Valle"
                    },
                    {
                        "authorId": "2196938269",
                        "name": "Javier Arechalde Pelaz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, MBPO [35], MOPO [41] and MBOP [24] use a neural network that outputs the parameters of a Gaussian distribution, to predict the next state and reward.",
                "This modelling assumption is fairly common in applications involving continuous state spaces [22, 35, 40, 41].",
                "2 Model-based methods Model-based algorithms rely on an approximation of the environment\u2019s dynamics [34, 35], that is probability distributions where the next state and reward are predicted from a current state and action.",
                "Model-based RL approaches typically use such dynamics\u2019 models conditioned on the action as well as the state to make predictions [24, 35, 40, 41]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "782c3235b6e9a3f9711274eedffa114792657f93",
                "externalIds": {
                    "ArXiv": "2212.04280",
                    "DBLP": "journals/corr/abs-2212-04280",
                    "DOI": "10.48550/arXiv.2212.04280",
                    "CorpusId": 254408896
                },
                "corpusId": 254408896,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/782c3235b6e9a3f9711274eedffa114792657f93",
                "title": "Model-based trajectory stitching for improved behavioural cloning and its applications",
                "abstract": "Behavioural cloning (BC) is a commonly used imitation learning method to infer a sequential decision-making policy from expert demonstrations. However, when the quality of the data is not optimal, the resulting behavioural policy also performs sub-optimally once deployed. Recently, there has been a surge in offline reinforcement learning methods that hold the promise to extract high-quality policies from sub-optimal historical data. A common approach is to perform regularisation during training, encouraging updates during policy evaluation and/or policy improvement to stay close to the underlying data. In this work, we investigate whether an offline approach to improving the quality of the existing data can lead to improved behavioural policies without any changes in the BC algorithm. The proposed data improvement approach - Model-Based Trajectory Stitching (MBTS) - generates new trajectories (sequences of states and actions) by \u2018stitching\u2019 pairs of states that were disconnected in the original data and generating their connecting new action. By construction, these new transitions are guaranteed to be highly plausible according to probabilistic models of the environment, and to improve a state-value function. We demonstrate that the iterative process of replacing old trajectories with new ones incrementally improves the underlying behavioural policy. Extensive experimental results show that significant performance gains can be achieved using MBTS over BC policies extracted from the original data. Furthermore, using the D4RL benchmarking suite, we demonstrate that state-of-the-art results are obtained by combining MBTS with two existing offline learning methodologies reliant on BC, model-based offline planning (MBOP) and policy constraint (TD3+BC).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2191626656",
                        "name": "Charles A. Hepburn"
                    },
                    {
                        "authorId": "1699650",
                        "name": "G. Montana"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some model-based RL algorithms use the model just to generate additional data and update the policy using a model-free algorithm (Sutton, 1991; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4c62574c2900a8201b5201aab062fe5ee9625297",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-02179",
                    "ArXiv": "2212.02179",
                    "DOI": "10.48550/arXiv.2212.02179",
                    "CorpusId": 254246239
                },
                "corpusId": 254246239,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4c62574c2900a8201b5201aab062fe5ee9625297",
                "title": "Physics-Informed Model-Based Reinforcement Learning",
                "abstract": "We apply reinforcement learning (RL) to robotics tasks. One of the drawbacks of traditional RL algorithms has been their poor sample efficiency. One approach to improve the sample efficiency is model-based RL. In our model-based RL algorithm, we learn a model of the environment, essentially its transition dynamics and reward function, use it to generate imaginary trajectories and backpropagate through them to update the policy, exploiting the differentiability of the model. Intuitively, learning more accurate models should lead to better model-based RL performance. Recently, there has been growing interest in developing better deep neural network based dynamics models for physical systems, by utilizing the structure of the underlying physics. We focus on robotic systems undergoing rigid body motion without contacts. We compare two versions of our model-based RL algorithm, one which uses a standard deep neural network based dynamics model and the other which uses a much more accurate, physics-informed neural network based dynamics model. We show that, in model-based RL, model accuracy mainly matters in environments that are sensitive to initial conditions, where numerical errors accumulate fast. In these environments, the physics-informed version of our algorithm achieves significantly better average-return and sample efficiency. In environments that are not sensitive to initial conditions, both versions of our algorithm achieve similar average-return, while the physics-informed version achieves better sample efficiency. We also show that, in challenging environments, physics-informed model-based RL achieves better average-return than state-of-the-art model-free RL algorithms such as Soft Actor-Critic, as it computes the policy-gradient analytically, while the latter estimates it through sampling.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153519852",
                        "name": "Adithya Ramesh"
                    },
                    {
                        "authorId": "1723632",
                        "name": "Balaraman Ravindran"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[9] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",
                "Model-based offline RL methods [9, 11, 26, 27] train a model of the environment using state-action transitions from the logged data."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "41f0d8676c6c801e0ac423776652bece1e52f38d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-02620",
                    "ArXiv": "2212.02620",
                    "DOI": "10.48550/arXiv.2212.02620",
                    "CorpusId": 254146115
                },
                "corpusId": 254146115,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/41f0d8676c6c801e0ac423776652bece1e52f38d",
                "title": "Benchmarking Offline Reinforcement Learning Algorithms for E-Commerce Order Fraud Evaluation",
                "abstract": "Amazon and other e-commerce sites must employ mechanisms to protect their millions of customers from fraud, such as unauthorized use of credit cards. One such mechanism is order fraud evaluation, where systems evaluate orders for fraud risk, and either\"pass\"the order, or take an action to mitigate high risk. Order fraud evaluation systems typically use binary classification models that distinguish fraudulent and legitimate orders, to assess risk and take action. We seek to devise a system that considers both financial losses of fraud and long-term customer satisfaction, which may be impaired when incorrect actions are applied to legitimate customers. We propose that taking actions to optimize long-term impact can be formulated as a Reinforcement Learning (RL) problem. Standard RL methods require online interaction with an environment to learn, but this is not desirable in high-stakes applications like order fraud evaluation. Offline RL algorithms learn from logged data collected from the environment, without the need for online interaction, making them suitable for our use case. We show that offline RL methods outperform traditional binary classification solutions in SimStore, a simplified e-commerce simulation that incorporates order fraud risk. We also propose a novel approach to training offline RL policies that adds a new loss term during training, to better align policy exploration with taking correct actions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2516072",
                        "name": "Soysal Degirmenci"
                    },
                    {
                        "authorId": "2115601070",
                        "name": "Chris Jones"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A recent comprehensive review of the model-based RL is presented by Janner et al. (2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d2d56b7720b55b0a1b4aa54af57143f0d8ae1e52",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-01498",
                    "ArXiv": "2212.01498",
                    "DOI": "10.48550/arXiv.2212.01498",
                    "CorpusId": 254246331
                },
                "corpusId": 254246331,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d2d56b7720b55b0a1b4aa54af57143f0d8ae1e52",
                "title": "Policy Learning for Active Target Tracking over Continuous SE(3) Trajectories",
                "abstract": "This paper proposes a novel model-based policy gradient algorithm for tracking dynamic targets using a mobile robot, equipped with an onboard sensor with limited field of view. The task is to obtain a continuous control policy for the mobile robot to collect sensor measurements that reduce uncertainty in the target states, measured by the target distribution entropy. We design a neural network control policy with the robot $SE(3)$ pose and the mean vector and information matrix of the joint target distribution as inputs and attention layers to handle variable numbers of targets. We also derive the gradient of the target entropy with respect to the network parameters explicitly, allowing efficient model-based policy gradient optimization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49731662",
                        "name": "Pengzhi Yang"
                    },
                    {
                        "authorId": "8411734",
                        "name": "Shumon Koga"
                    },
                    {
                        "authorId": "2045172623",
                        "name": "Arash Asgharivaskasi"
                    },
                    {
                        "authorId": "50365495",
                        "name": "Nikolay A. Atanasov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, t ose methods reflect a higher sample efficiency, s r flected through empirical [62,63] and theoretical [64] studies."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "96433de43674b7297422db6c21c5490501f08b68",
                "externalIds": {
                    "DOI": "10.3390/app122312377",
                    "CorpusId": 254338700
                },
                "corpusId": 254338700,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/96433de43674b7297422db6c21c5490501f08b68",
                "title": "A Review of Deep Reinforcement Learning Approaches for Smart Manufacturing in Industry 4.0 and 5.0 Framework",
                "abstract": "In this review, the industry\u2019s current issues regarding intelligent manufacture are presented. This work presents the status and the potential for the I4.0 and I5.0\u2019s revolutionary technologies. AI and, in particular, the DRL algorithms, which are a perfect response to the unpredictability and volatility of modern demand, are studied in detail. Through the introduction of RL concepts and the development of those with ANNs towards DRL, the potential and variety of these kinds of algorithms are highlighted. Moreover, because these algorithms are data based, their modification to meet the requirements of industry operations is also included. In addition, this review covers the inclusion of new concepts, such as digital twins, in response to an absent environment model and how it can improve the performance and application of DRL algorithms even more. This work highlights that DRL applicability is demonstrated across all manufacturing industry operations, outperforming conventional methodologies and, most notably, enhancing the manufacturing process\u2019s resilience and adaptability. It is stated that there is still considerable work to be carried out in both academia and industry to fully leverage the promise of these disruptive tools, begin their deployment in industry, and take a step closer to the I5.0 industrial revolution.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2123901187",
                        "name": "Alejandro del Real Torres"
                    },
                    {
                        "authorId": "2156210189",
                        "name": "Doru Stefan Andreiana"
                    },
                    {
                        "authorId": "2156206537",
                        "name": "\u00c1lvaro Ojeda Rold\u00e1n"
                    },
                    {
                        "authorId": "2194052686",
                        "name": "Alfonso Hern\u00e1ndez Bustos"
                    },
                    {
                        "authorId": "2156208116",
                        "name": "Luis Enrique Acevedo Galicia"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a53d9c02aa1e58dd0968076f1e554ed278dcda8b",
                "externalIds": {
                    "DBLP": "journals/ras/TianFYYLW23",
                    "DOI": "10.1016/j.robot.2022.104351",
                    "CorpusId": 255259063
                },
                "corpusId": 255259063,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a53d9c02aa1e58dd0968076f1e554ed278dcda8b",
                "title": "Reinforcement learning under temporal logic constraints as a sequence modeling problem",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1380803486",
                        "name": "Daiying Tian"
                    },
                    {
                        "authorId": "2113485859",
                        "name": "Hao Fang"
                    },
                    {
                        "authorId": "50514034",
                        "name": "Qingkai Yang"
                    },
                    {
                        "authorId": "2118495495",
                        "name": "Haoyong Yu"
                    },
                    {
                        "authorId": "2113676050",
                        "name": "Wenyu Liang"
                    },
                    {
                        "authorId": "2145072603",
                        "name": "Yan Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3e467350b85f557255c67673fe971cb35bcedbe0",
                "externalIds": {
                    "ArXiv": "2212.00618",
                    "DBLP": "journals/corr/abs-2212-00618",
                    "DOI": "10.48550/arXiv.2212.00618",
                    "CorpusId": 254125734
                },
                "corpusId": 254125734,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3e467350b85f557255c67673fe971cb35bcedbe0",
                "title": "Safe Reinforcement Learning with Probabilistic Control Barrier Functions for Ramp Merging",
                "abstract": "Prior work has looked at applying reinforcement learning and imitation learning approaches to autonomous driving scenarios, but either the safety or the efficiency of the algorithm is compromised. With the use of control barrier functions embedded into the reinforcement learning policy, we arrive at safe policies to optimize the performance of the autonomous driving vehicle. However, control barrier functions need a good approximation of the model of the car. We use probabilistic control barrier functions as an estimate of the model uncertainty. The algorithm is implemented as an online version in the CARLA (Dosovitskiy et al., 2017) Simulator and as an offline version on a dataset extracted from the NGSIM Database. The proposed algorithm is not just a safe ramp merging algorithm but a safe autonomous driving algorithm applied to address ramp merging on highways.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2003032642",
                        "name": "Soumith Udatha"
                    },
                    {
                        "authorId": null,
                        "name": "Yiwei Lyu"
                    },
                    {
                        "authorId": "151620519",
                        "name": "J. Dolan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ed93314df87a8b9cb3892d6202e5f0b0021720e9",
                "externalIds": {
                    "DOI": "10.1016/j.scs.2022.104351",
                    "CorpusId": 254709372
                },
                "corpusId": 254709372,
                "publicationVenue": {
                    "id": "96999166-7d47-4d87-b4f8-e11d11c7c45a",
                    "name": "Sustainable cities and society",
                    "type": "journal",
                    "alternate_names": [
                        "Sustain city soc",
                        "Sustain City Soc",
                        "Sustainable Cities and Society"
                    ],
                    "issn": "2210-6707",
                    "url": "https://www.journals.elsevier.com/sustainable-cities-and-society",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/22106707"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ed93314df87a8b9cb3892d6202e5f0b0021720e9",
                "title": "A review of reinforcement learning for controlling Building Energy Systems from a computer science perspective",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2195908154",
                        "name": "David Weinberg"
                    },
                    {
                        "authorId": "2183688302",
                        "name": "Qian Wang"
                    },
                    {
                        "authorId": "102400559",
                        "name": "Thomas Ohlson Timoudas"
                    },
                    {
                        "authorId": "1709815",
                        "name": "C. Fischione"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many recent works focus on improving MBRL in line with the advancements in deep-RL [13], [14], [8], [15], [9].",
                "In [15], a theoretical analysis is formulated to guarantee a monotonic policy improvement in MBRL and demonstrates that a simple procedure of using short model-generated rollouts branched from real data could improve the performance of MBRL."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c344820387c97adba25dfbad5851e0283ddf5c77",
                "externalIds": {
                    "DBLP": "conf/icmla/AnandKAGG22",
                    "DOI": "10.1109/ICMLA55696.2022.00009",
                    "CorpusId": 257719949
                },
                "corpusId": 257719949,
                "publicationVenue": {
                    "id": "f6752838-f268-4a1b-87e7-c5f30a36713c",
                    "name": "International Conference on Machine Learning and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Mach Learn Appl",
                        "ICMLA"
                    ],
                    "url": "http://www.icmla-conference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c344820387c97adba25dfbad5851e0283ddf5c77",
                "title": "Addressing Sample Efficiency and Model-bias in Model-based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning promises to be an effective way to bring reinforcement learning to real-world robotic systems by offering a sample efficient learning approach compared to model-free reinforcement learning. However, model-based reinforcement learning approaches at present struggle to match the performance of model-free ones. This work attempts to fill this gap by improving the performance of model-based reinforcement learning while further improving its sample efficiency. To improve the sample efficiency, an exploration strategy is formulated which maximizes the information gain. The asymptotic performance is improved by compensating for the model-bias using a model-free critic. We have evaluated our proposed approach on four reinforcement learning benchmarking tasks in the openAI gym framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1571204621",
                        "name": "Akhil S. Anand"
                    },
                    {
                        "authorId": "2212613523",
                        "name": "Jens Erik Kveen"
                    },
                    {
                        "authorId": "1405378395",
                        "name": "Fares J. Abu-Dakka"
                    },
                    {
                        "authorId": "30580449",
                        "name": "E. Gr\u00f8tli"
                    },
                    {
                        "authorId": "2212614515",
                        "name": "Jan Tommy Gravdahl"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Dashed red line indicates the standard Q-values from running MBPO [32].",
                "MBPO performs standard off-policy RL using an augmented dataset D \u222a D\u0302, where D\u0302 is synthetic data generated by simulating short rollouts in the learnt model.",
                "Following previous works [34, 74, 75], our approach utilises model-based policy optimisation (MBPO) [32].",
                "However, sampling full length trajectories is not desirable in model-based methods due to compounding modelling error [32].",
                "Thus, naive policy optimisation on a learnt model in the offline setting can result in model exploitation [32, 40, 54], i."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "a7f1be908de231bb72928fa984289501c0796823",
                "externalIds": {
                    "ArXiv": "2212.00124",
                    "CorpusId": 259064328
                },
                "corpusId": 259064328,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a7f1be908de231bb72928fa984289501c0796823",
                "title": "One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion",
                "abstract": "Offline reinforcement learning (RL) is suitable for safety-critical domains where online exploration is not feasible. In such domains, decision-making should take into consideration the risk of catastrophic outcomes. In other words, decision-making should be risk-averse. An additional challenge of offline RL is avoiding distributional shift, i.e. ensuring that state-action pairs visited by the policy remain near those in the dataset. Previous works on risk in offline RL combine offline RL techniques (to avoid distributional shift), with risk-sensitive RL algorithms (to achieve risk-aversion). In this work, we propose risk-aversion as a mechanism to jointly address both of these issues. We propose a model-based approach, and use an ensemble of models to estimate epistemic uncertainty, in addition to aleatoric uncertainty. We train a policy that is risk-averse, and avoids high uncertainty actions. Risk-aversion to epistemic uncertainty prevents distributional shift, as areas not covered by the dataset have high epistemic uncertainty. Risk-aversion to aleatoric uncertainty discourages actions that are inherently risky due to environment stochasticity. Thus, by only introducing risk-aversion, we avoid distributional shift in addition to achieving risk-aversion to aleatoric risk. Our algorithm, 1R2R, achieves strong performance on deterministic benchmarks, and outperforms existing approaches for risk-sensitive objectives in stochastic domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51300315",
                        "name": "Marc Rigter"
                    },
                    {
                        "authorId": "145350537",
                        "name": "Bruno Lacerda"
                    },
                    {
                        "authorId": "2072387078",
                        "name": "N. Hawes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MOPO extends MBPO [13], which utilizes learned environment models to generate short roll-outs of length h that are employed to update the learning policy using the soft-actor critic (SAC) policy gradient algorithm [11, 12].",
                "Errors made by the learned environment models, such as those arising from poor generalization performance under distributional shifts, can result in model exploitation: the policy being trained learns to take advantage of model errors when optimizing the reward, leading to poor performance in the real environment [5, 20, 13, 7, 17].",
                "MOPO extends MBPO [12], which combines the soft-actor critic (SAC) policy gradient67 algorithm [10, 11] with using learned dynamics models to generate short roll-outs for training.68\n3 Methods69 Our method stems from the observation that different demonstrators in an offline RL setting cor-70 respond to different domains in a domain generalization setting."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "630afad39e7d523cee62f9a1f8254e4e3d6ac0be",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-14827",
                    "ArXiv": "2211.14827",
                    "DOI": "10.48550/arXiv.2211.14827",
                    "CorpusId": 253180790
                },
                "corpusId": 253180790,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/630afad39e7d523cee62f9a1f8254e4e3d6ac0be",
                "title": "Domain Generalization for Robust Model-Based Offline Reinforcement Learning",
                "abstract": "Existing offline reinforcement learning (RL) algorithms typically assume that training data is either: 1) generated by a known policy, or 2) of entirely unknown origin. We consider multi-demonstrator offline RL, a middle ground where we know which demonstrators generated each dataset, but make no assumptions about the underlying policies of the demonstrators. This is the most natural setting when collecting data from multiple human operators, yet remains unexplored. Since different demonstrators induce different data distributions, we show that this can be naturally framed as a domain generalization problem, with each demonstrator corresponding to a different domain. Specifically, we propose Domain-Invariant Model-based Offline RL (DIMORL), where we apply Risk Extrapolation (REx) (Krueger et al., 2020) to the process of learning dynamics and rewards models. Our results show that models trained with REx exhibit improved domain generalization performance when compared with the natural baseline of pooling all demonstrators' data. We observe that the resulting models frequently enable the learning of superior policies in the offline model-based RL setting, can improve the stability of the policy learning process, and potentially enable increased exploration.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192607763",
                        "name": "Alan Clark"
                    },
                    {
                        "authorId": "29005173",
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "authorId": "2066422293",
                        "name": "Robert Kirk"
                    },
                    {
                        "authorId": "2066185365",
                        "name": "Usman Anwar"
                    },
                    {
                        "authorId": "2112491703",
                        "name": "Stephen Chung"
                    },
                    {
                        "authorId": "145055042",
                        "name": "David Krueger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For instance, learning to predict the continuous control parameters in dynamic models has been built on a Multivariate Gaussian Distribution (MGD) where the covariance matrix is diagonal [3]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e9346ec89254ca85e22c070dbe9fe764250f19cd",
                "externalIds": {
                    "ArXiv": "2211.12627",
                    "DBLP": "journals/corr/abs-2211-12627",
                    "DOI": "10.48550/arXiv.2211.12627",
                    "CorpusId": 253801575
                },
                "corpusId": 253801575,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e9346ec89254ca85e22c070dbe9fe764250f19cd",
                "title": "\u03b2-Multivariational Autoencoder for Entangled Representation Learning in Video Frames",
                "abstract": "It is crucial to choose actions from an appropriate distribution while learning a sequential decision-making process in which a set of actions is expected given the states and previous reward. Yet, if there are more than two latent variables and every two variables have a covariance value, learning a known prior from data becomes challenging. Because when the data are big and diverse, many posterior estimate methods experience posterior collapse. In this paper, we propose the $\\beta$-Multivariational Autoencoder ($\\beta$MVAE) to learn a Multivariate Gaussian prior from video frames for use as part of a single object-tracking in form of a decision-making process. We present a novel formulation for object motion in videos with a set of dependent parameters to address a single object-tracking task. The true values of the motion parameters are obtained through data analysis on the training set. The parameters population is then assumed to have a Multivariate Gaussian distribution. The $\\beta$MVAE is developed to learn this entangled prior $p = N(\\mu, \\Sigma)$ directly from frame patches where the output is the object masks of the frame patches. We devise a bottleneck to estimate the posterior's parameters, i.e. $\\mu', \\Sigma'$. Via a new reparameterization trick, we learn the likelihood $p(\\hat{x}|z)$ as the object mask of the input. Furthermore, we alter the neural network of $\\beta$MVAE with the U-Net architecture and name the new network $\\beta$Multivariational U-Net ($\\beta$MVUnet). Our networks are trained from scratch via over 85k video frames for 24 ($\\beta$MVUnet) and 78 ($\\beta$MVAE) million steps. We show that $\\beta$MVUnet enhances both posterior estimation and segmentation functioning over the test set. Our code and the trained networks are publicly released.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2072935906",
                        "name": "F. Nouri"
                    },
                    {
                        "authorId": "2145950",
                        "name": "R. Bergevin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based RL approaches typically use such dynamics\u2019 models conditioned on the action as well as the state to make predictions [30, 63, 36, 2].",
                "In the online setting, they tend to improve sample efficiency [33, 30, 15, 7, 12].",
                "Modelling the environment dynamics as a Gaussian distribution is common for continuous state-space applications [30, 63, 36, 62].",
                "Model-based algorithms rely on an approximation of the environment\u2019s dynamics [58, 30].",
                "[30] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b9ecf9fb3b2d5a70451fa30911142e55848fde2d",
                "externalIds": {
                    "ArXiv": "2211.11603",
                    "DBLP": "journals/corr/abs-2211-11603",
                    "DOI": "10.48550/arXiv.2211.11603",
                    "CorpusId": 253735126
                },
                "corpusId": 253735126,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b9ecf9fb3b2d5a70451fa30911142e55848fde2d",
                "title": "Model-based Trajectory Stitching for Improved Offline Reinforcement Learning",
                "abstract": "In many real-world applications, collecting large and high-quality datasets may be too costly or impractical. Offline reinforcement learning (RL) aims to infer an optimal decision-making policy from a fixed set of data. Getting the most information from historical data is then vital for good performance once the policy is deployed. We propose a model-based data augmentation strategy, Trajectory Stitching (TS), to improve the quality of sub-optimal historical trajectories. TS introduces unseen actions joining previously disconnected states: using a probabilistic notion of state reachability, it effectively `stitches' together parts of the historical demonstrations to generate new, higher quality ones. A stitching event consists of a transition between a pair of observed states through a synthetic and highly probable action. New actions are introduced only when they are expected to be beneficial, according to an estimated state-value function. We show that using this data augmentation strategy jointly with behavioural cloning (BC) leads to improvements over the behaviour-cloned policy from the original dataset. Improving over the BC policy could then be used as a launchpad for online RL through planning and demonstration-guided RL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2191626656",
                        "name": "Charles A. Hepburn"
                    },
                    {
                        "authorId": "1699650",
                        "name": "G. Montana"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026in the model-based approaches to combat compounding error and model exploitation (Kurutach et al., 2018; Chua et al., 2018; Lai et al., 2020; Janner et al., 2019), in model-free to greatly increase sample efficiency (Chen et al., 2021; Hiraoka et al., 2021; Liang et al., 2022) and in\u2026",
                "They are employed in the model-based approaches to combat compounding error and model exploitation (Kurutach et al., 2018; Chua et al., 2018; Lai et al., 2020; Janner et al., 2019), in model-free to greatly increase sample efficiency (Chen et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "e201436082ee00f2c8a73da4d19809c6f1772102",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-11092",
                    "ArXiv": "2211.11092",
                    "DOI": "10.48550/arXiv.2211.11092",
                    "CorpusId": 253734617
                },
                "corpusId": 253734617,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e201436082ee00f2c8a73da4d19809c6f1772102",
                "title": "Q-Ensemble for Offline RL: Don't Scale the Ensemble, Scale the Batch Size",
                "abstract": "Training large neural networks is known to be time-consuming, with the learning duration taking days or even weeks. To address this problem, large-batch optimization was introduced. This approach demonstrated that scaling mini-batch sizes with appropriate learning rate adjustments can speed up the training process by orders of magnitude. While long training time was not typically a major issue for model-free deep offline RL algorithms, recently introduced Q-ensemble methods achieving state-of-the-art performance made this issue more relevant, notably extending the training duration. In this work, we demonstrate how this class of methods can benefit from large-batch optimization, which is commonly overlooked by the deep offline RL community. We show that scaling the mini-batch size and naively adjusting the learning rate allows for (1) a reduced size of the Q-ensemble, (2) stronger penalization of out-of-distribution actions, and (3) improved convergence time, effectively shortening training duration by 3-4x times on average.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155646949",
                        "name": "Alexander Nikulin"
                    },
                    {
                        "authorId": "1390186081",
                        "name": "Vladislav Kurenkov"
                    },
                    {
                        "authorId": "2064312685",
                        "name": "Denis Tarasov"
                    },
                    {
                        "authorId": "2069005173",
                        "name": "Dmitry Akimov"
                    },
                    {
                        "authorId": "145374794",
                        "name": "Sergey Kolesnikov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, the value-function effect of replacing P with P\u0303 , is an instance of sensitivity analysis for MDPs; see e.g. Mastin and Jaillet (2012); Ross et al. (2009) and, in model-based reinforcement learning, Janner et al. (2019); Sun et al. (2018)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e3ffc62378d7dacc354f473458f072c527b498e7",
                "externalIds": {
                    "DOI": "10.1287/opre.2022.2392",
                    "CorpusId": 235415006
                },
                "corpusId": 235415006,
                "publicationVenue": {
                    "id": "cd20e681-2796-4e88-8fc8-cc4a58bea933",
                    "name": "Operational Research",
                    "type": "journal",
                    "alternate_names": [
                        "Operations Research",
                        "Oper Res"
                    ],
                    "issn": "1109-2858",
                    "alternate_issns": [
                        "0030-364X"
                    ],
                    "url": "http://www.springer.com/business/operations+research/journal/12351",
                    "alternate_urls": [
                        "http://or.journal.informs.org/",
                        "http://www.jstor.org/journals/0030364x.html",
                        "https://link.springer.com/journal/12351",
                        "https://www.jstor.org/journal/operrese"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e3ffc62378d7dacc354f473458f072c527b498e7",
                "title": "A Low-Rank Approximation for MDPs via Moment Coupling",
                "abstract": "Markov Decision Process Tayloring for Approximation Design Optimal control problems are difficult to solve for problems on large state spaces, calling for the development of approximate solution methods. In \u201cA Low-rank Approximation for MDPs via Moment Coupling,\u201d Zhang and Gurvich introduce a novel framework to approximate Markov decision processes (MDPs) that stands on two pillars: (i) state aggregation, as the algorithmic infrastructure, and (ii) central-limit-theorem-type approximations, as the mathematical underpinning. The theoretical guarantees are grounded in the approximation of the Bellman equation by a partial differential equation (PDE) where, in the spirit of the central limit theorem, the transition matrix of the controlled Markov chain is reduced to its local first and second moments. Instead of solving the PDE, the algorithm introduced in the paper constructs a \u201csister\u201d' (controlled) Markov chain whose two local transition moments are approximately identical with those of the focal chain. Because of this moment matching, the original chain and its sister are coupled through the PDE, facilitating optimality guarantees. Embedded into standard soft aggregation, moment matching provides a disciplined mechanism to tune the aggregation and disaggregation probabilities.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111672235",
                        "name": "Amy Zhang"
                    },
                    {
                        "authorId": "81066242",
                        "name": "Itai Gurvich"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "623218665fd6ccb8749da5da2094ec1e58e3b5ad",
                "externalIds": {
                    "ArXiv": "2211.08796",
                    "DBLP": "journals/corr/abs-2211-08796",
                    "DOI": "10.48550/arXiv.2211.08796",
                    "CorpusId": 253553383
                },
                "corpusId": 253553383,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/623218665fd6ccb8749da5da2094ec1e58e3b5ad",
                "title": "Model Based Residual Policy Learning with Applications to Antenna Control",
                "abstract": "Non-differentiable controllers and rule-based policies are widely used for controlling real systems such as telecommunication networks and robots. Specifically, parameters of mobile network base station antennas can be dynamically configured by these policies to improve users coverage and quality of service. Motivated by the antenna tilt control problem, we introduce Model-Based Residual Policy Learning (MBRPL), a practical reinforcement learning (RL) method. MBRPL enhances existing policies through a model-based approach, leading to improved sample efficiency and a decreased number of interactions with the actual environment when compared to off-the-shelf RL methods.To the best of our knowledge, this is the first paper that examines a model-based approach for antenna control. Experimental results reveal that our method delivers strong initial performance while improving sample efficiency over previous RL methods, which is one step towards deploying these algorithms in real networks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2190957650",
                        "name": "Viktor Eriksson Mollerstedt"
                    },
                    {
                        "authorId": "49570638",
                        "name": "Alessio Russo"
                    },
                    {
                        "authorId": "40577385",
                        "name": "Maxime Bouton"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There is a large body of empirical research showing the potential of learned models to improve sample efficiency (Deisenroth & Rasmussen, 2011; Buckman et al., 2018; Kaiser et al., 2019; Janner et al., 2019; Curi et al., 2020; Hafner et al., 2021).",
                "There is a large body of empirical research showing the potential of learned models to improve sample efficiency (Deisenroth & Rasmussen, 2011; Buckman et al., 2018; Kaiser et al., 2019; Janner et al., 2019; Curi et al., 2020; Hafner et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "54c2eab18cc0f083a5f67b0a7cc5785eb458fe4b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-02222",
                    "ArXiv": "2211.02222",
                    "DOI": "10.48550/arXiv.2211.02222",
                    "CorpusId": 253370616
                },
                "corpusId": 253370616,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/54c2eab18cc0f083a5f67b0a7cc5785eb458fe4b",
                "title": "The Benefits of Model-Based Generalization in Reinforcement Learning",
                "abstract": "Model-Based Reinforcement Learning (RL) is widely believed to have the potential to improve sample efficiency by allowing an agent to synthesize large amounts of imagined experience. Experience Replay (ER) can be considered a simple kind of model, which has proved effective at improving the stability and efficiency of deep RL. In principle, a learned parametric model could improve on ER by generalizing from real experience to augment the dataset with additional plausible experience. However, given that learned value functions can also generalize, it is not immediately obvious why model generalization should be better. Here, we provide theoretical and empirical insight into when, and how, we can expect data generated by a learned model to be useful. First, we provide a simple theorem motivating how learning a model as an intermediate step can narrow down the set of possible value functions more than learning a value function directly from data using the Bellman equation. Second, we provide an illustrative example showing empirically how a similar effect occurs in a more concrete setting with neural network function approximation. Finally, we provide extensive experiments showing the benefit of model-based learning for online RL in environments with combinatorial complexity, but factored structure that allows a learned model to generalize. In these experiments, we take care to control for other factors in order to isolate, insofar as possible, the benefit of using experience generated by a learned model relative to ER alone.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145991379",
                        "name": "K. Young"
                    },
                    {
                        "authorId": "1992922591",
                        "name": "A. Ramesh"
                    },
                    {
                        "authorId": "3031520",
                        "name": "Louis Kirsch"
                    },
                    {
                        "authorId": "145341374",
                        "name": "J. Schmidhuber"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Comparisons to other methods such as MFRL, for instance adapting the methods of [6] to include demonstrations, could help establish how well our policies perform relative to policies trained with other methods.",
                "We choose this class of policy optimizer over others possible, such as model-free reinforcement learning (MFRL), primarily because MBRL has been shown by past studies to be more data-efficient than MFRL [30], [32]\u2013[34].",
                "We are not the first to use RL [5], [16], [29], [30] nor RL+IL [1]\u2013[3] for robot locomotion, and as such our methods use concepts from these prior methods.",
                "Our algorithm is inspired in particular by the algorithms of [5] and [30], and similarly iterate between collecting data, training the model, and training the policy."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0a7446207ae24a7ff81a5f31ecd32838e3d74e1c",
                "externalIds": {
                    "ArXiv": "2210.17491",
                    "DBLP": "journals/corr/abs-2210-17491",
                    "DOI": "10.48550/arXiv.2210.17491",
                    "CorpusId": 253237876
                },
                "corpusId": 253237876,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0a7446207ae24a7ff81a5f31ecd32838e3d74e1c",
                "title": "Learning Modular Robot Locomotion from Demonstrations",
                "abstract": "\u2014Modular robots can be recon\ufb01gured to create a variety of designs from a small set of components. But constructing a robot\u2019s hardware on its own is not enough\u2013 each robot needs a controller. One could create controllers for some designs individually, but developing policies for additional designs can be time consuming. This work presents a method that uses demonstrations from one set of designs to accelerate policy learning for additional designs. We leverage a learning framework in which a graph neural network is made up of modular components, each component corresponds to a type of module (e.g., a leg, wheel, or body) and these components can be recombined to learn from multiple designs at once. In this paper we develop a combined reinforcement and imitation learning algorithm. Our method is novel because the policy is optimized to both maximize a reward for one design, and simul- taneously imitate demonstrations from different designs, within one objective function. We show that when the modular policy is optimized with this combined objective, demonstrations from one set of designs in\ufb02uence how the policy behaves on a different design, decreasing the number of training iterations needed.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143754470",
                        "name": "Julian Whitman"
                    },
                    {
                        "authorId": "1742948",
                        "name": "H. Choset"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Ensemble methods are widely used for better performance in RL [37]\u2013[40].",
                "In model-based RL, PETS [39] and MBPO [40] use probabilistic ensembles to effectively model the dynamics of the environment."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "811eca8c2e6b25c99f488413dd6993ab3771292e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-13846",
                    "ArXiv": "2210.13846",
                    "DOI": "10.14428/esann/2022.es2022-110",
                    "CorpusId": 252340048
                },
                "corpusId": 252340048,
                "publicationVenue": {
                    "id": "93d6c444-c90a-48ee-a3ad-ef1f015bc28a",
                    "name": "The European Symposium on Artificial Neural Networks",
                    "type": "conference",
                    "alternate_names": [
                        "Eur Symp Artif Neural Netw",
                        "ESANN"
                    ],
                    "url": "https://www.esann.org/"
                },
                "url": "https://www.semanticscholar.org/paper/811eca8c2e6b25c99f488413dd6993ab3771292e",
                "title": "Adaptive Behavior Cloning Regularization for Stable Offline-to-Online Reinforcement Learning",
                "abstract": "Offline reinforcement learning, by learning from a fixed dataset, makes it possible to learn agent behaviors without interacting with the environment. However, depending on the quality of the offline dataset, such pre-trained agents may have limited performance and would further need to be fine-tuned online by interacting with the environment. During online fine-tuning, the performance of the pre-trained agent may collapse quickly due to the sudden distribution shift from offline to online data. While constraints enforced by offline RL methods such as a behaviour cloning loss prevent this to an extent, these constraints also significantly slow down online fine-tuning by forcing the agent to stay close to the behavior policy. We propose to adaptively weigh the behavior cloning loss during online fine-tuning based on the agent's performance and training stability. Moreover, we use a randomized ensemble of Q functions to further increase the sample efficiency of online fine-tuning by performing a large number of learning updates. Experiments show that the proposed method yields state-of-the-art offline-to-online reinforcement learning performance on the popular D4RL benchmark. Code is available: \\url{https://github.com/zhaoyi11/adaptive_bc}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109917181",
                        "name": "Yi Zhao"
                    },
                    {
                        "authorId": "22169323",
                        "name": "Rinu Boney"
                    },
                    {
                        "authorId": "2130303892",
                        "name": "A. Ilin"
                    },
                    {
                        "authorId": "1776374",
                        "name": "Juho Kannala"
                    },
                    {
                        "authorId": "34906504",
                        "name": "J. Pajarinen"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019; Ciosek & Whiteson, 2020); and achieving better quality gradients through simulating additional transitions via dynamics model in model-based SPG (MB-SPG) (Janner et al., 2019).",
                "\u2026SPG algorithms: achieving better quality gradients through MA via Q-network (QMA) (Asadi et al., 2017; Petit et al., 2019; Ciosek & Whiteson, 2020); and achieving better quality gradients through simulating additional transitions via dynamics model in model-based SPG (MB-SPG) (Janner et al., 2019).",
                "From the perspective of model-based on-policy SPG, MBMA builds upon on-policy Model-Based Policy Optimization (MPBO) (Janner et al., 2019) but introduces the distinction between two roles for simulated transitions: whereas MBPO calculates gradient at simulated states, we propose to use information from the dynamics model by backpropagating from real states with simulated actions (i.",
                "By comparing MBPO-PPO and MBMA-PPO we compare variance reduction of many-actions (MBMA) as opposed to extending the trajectory length (MBPO) in the MB-SPG context and validate our theoretical contribution\n2.",
                "Similarly, neither MBPO nor MBMA uses an ensemble of dynamics models (Buckman et al., 2018; Kurutach et al., 2018; Janner et al., 2019).",
                "MBPO PPO that leverages dynamics model to perform finite horizon rollouts branching from the on-policy data (Janner et al., 2019).",
                "From the perspective of model-based on-policy SPG, MBMA builds upon on-policy Model-Based Policy Optimization (MPBO) (Janner et al., 2019) but introduces the distinction between two roles for simulated transitions: whereas MBPO calculates gradient at simulated states, we propose to use information from the dynamics model by backpropagating from real states with simulated actions (i.e. simulating Q-values of those actions).",
                "From the perspective of model-based on-policy SPG, MBMA builds upon on-policy Model-Based Policy Optimization (MPBO) (Janner et al., 2019) but introduces the distinction between two roles for simulated transitions: whereas MBPO calculates gradient at simulated states, we propose to use information\u2026",
                "MBMA yields a favorable bias/variance structure as compared to learning from states simulated in the dynamics model rollout (Janner et al., 2019; Kaiser et al., 2019; Hafner et al., 2019) in the context of onpolicy SPG.",
                "Given a fixed amount of interactions with the environment, our theoretical analysis is related to two notions in on-policy SPG algorithms: achieving better quality gradients through MA via Q-network (QMA) (Asadi et al., 2017; Petit et al., 2019; Ciosek & Whiteson, 2020); and achieving better quality gradients through simulating additional transitions via dynamics model in model-based SPG (MB-SPG) (Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "581d81e03225cf504a34d452bb38b173f3ee1702",
                "externalIds": {
                    "ArXiv": "2210.13011",
                    "DBLP": "conf/icml/NaumanC23",
                    "CorpusId": 258436958
                },
                "corpusId": 258436958,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/581d81e03225cf504a34d452bb38b173f3ee1702",
                "title": "On Many-Actions Policy Gradient",
                "abstract": "We study the variance of stochastic policy gradients (SPGs) with many action samples per state. We derive a many-actions optimality condition, which determines when many-actions SPG yields lower variance as compared to a single-action agent with proportionally extended trajectory. We propose Model-Based Many-Actions (MBMA), an approach leveraging dynamics models for many-actions sampling in the context of SPG. MBMA addresses issues associated with existing implementations of many-actions SPG and yields lower bias and comparable variance to SPG estimated from states in model-simulated rollouts. We find that MBMA bias and variance structure matches that predicted by theory. As a result, MBMA achieves improved sample efficiency and higher returns on a range of continuous action environments as compared to model-free, many-actions, and model-based on-policy SPG baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2004147618",
                        "name": "Michal Nauman"
                    },
                    {
                        "authorId": "144560625",
                        "name": "Marek Cygan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In both simulated experiments, we compare against three baselines: PETS [20], SAC [39], and MBPO [10].",
                "We chose these three methods, because they are popular examples of each of the three main categories that most modern RL algorithms fall into: SAC is completely model-free, PETS is fully model-based and MBPO is a hybrid approach where the model is used to generate additional data for an underlying model-free agent.",
                "4: Cumulative reward in three different experiments for both versions of our agent (MI and LI) in comparison to three baselines: PETS [20], SAC [39], and MBPO [10].",
                "MoPAC [11] improved over MBPO by employing modelpredictive rollouts in the approximate MDP that is learned through the model, incentivizing the agent to explore areas of the state-space where model predictions are inefficient.",
                "Note that SAC and MBPO with their maximum entropy exploration strategy did not manage to learn the tasks, revealing that hard exploration problems require directed information seeking strategies, as our method does.",
                "Following methods attempt to couple model-free exploration with model learning, as in model-based policy optimization (MBPO) [10], but the purpose is to accelerate policy learning by utilizing model-based approximate samples.",
                "However, advantages of these methods compared to MBPO, MoPAC and other model-based methods without explicit exploration bonus [12] are not wellestablished.",
                "Following methods attempt to couple model-free exploration with model learning, as in model-based policy optimization (MBPO) [10], but"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "6183b0803d49ed43cd6ded5ca41255a9646be9b7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-12806",
                    "ArXiv": "2210.12806",
                    "DOI": "10.1109/IROS47612.2022.9982061",
                    "CorpusId": 253097983
                },
                "corpusId": 253097983,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6183b0803d49ed43cd6ded5ca41255a9646be9b7",
                "title": "Active Exploration for Robotic Manipulation",
                "abstract": "Robotic manipulation stands as a largely unsolved problem despite significant advances in robotics and machine learning in recent years. One of the key challenges in manipulation is the exploration of the dynamics of the environment when there is continuous contact between the objects being manipulated. This paper proposes a model-based active exploration approach that enables efficient learning in sparse-reward robotic manipulation tasks. The proposed method estimates an information gain objective using an ensemble of probabilistic models and deploys model predictive control (MPC) to plan actions online that maximize the expected reward while also performing directed exploration. We evaluate our proposed algorithm in simulation and on a real robot, trained from scratch with our method, on a challenging ball pushing task on tilted tables, where the target ball position is not known to the agent a-priori. Our real-world robot experiment serves as a fundamental application of active exploration in model-based reinforcement learning of complex robotic manipulation tasks. Project page https://sites.google.com/view/aerm.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2061582061",
                        "name": "Tim Schneider"
                    },
                    {
                        "authorId": "29505409",
                        "name": "B. Belousov"
                    },
                    {
                        "authorId": "1989757",
                        "name": "G. Chalvatzaki"
                    },
                    {
                        "authorId": "2777349",
                        "name": "D. Romeres"
                    },
                    {
                        "authorId": "2743474",
                        "name": "Devesh K. Jha"
                    },
                    {
                        "authorId": "145197867",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Training our dynamics models on existing state transitions is efficient and circumvents the challenges associated with learning dynamics in the context of a long-horizon task [53].",
                "Our work is also related to modelbased RL methods which jointly learn dynamics and reward models to guide planning [50\u201352], policy search [53, 54], or combine both [55, 56]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b75359b5b22024ac0aec8b942bbd86bde81f8e70",
                "externalIds": {
                    "ArXiv": "2210.12250",
                    "DBLP": "conf/icra/AgiaMWB23",
                    "DOI": "10.1109/ICRA48891.2023.10160220",
                    "CorpusId": 257482480
                },
                "corpusId": 257482480,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b75359b5b22024ac0aec8b942bbd86bde81f8e70",
                "title": "STAP: Sequencing Task-Agnostic Policies",
                "abstract": "Advances in robotic skill acquisition have made it possible to build general-purpose libraries of learned skills for downstream manipulation tasks. However, naively executing these skills one after the other is unlikely to succeed without accounting for dependencies between actions prevalent in longhorizon plans. We present Sequencing Task-Agnostic Policies (STAP), a scalable framework for training manipulation skills and coordinating their geometric dependencies at planning time to solve long-horizon tasks never seen by any skill during training. Given that Q-functions encode a measure of skill feasibility, we formulate an optimization problem to maximize the joint success of all skills sequenced in a plan, which we estimate by the product of their Q-values. Our experiments indicate that this objective function approximates ground truth plan feasibility and, when used as a planning objective, reduces myopic behavior and thereby promotes long-horizon task success. We further demonstrate how STAP can be used for task and motion planning by estimating the geometric feasibility of skill sequences provided by a task planner. We evaluate our approach in simulation and on a real robot. Qualitative results and code are made available at sites.google.com/stanford.edu/stap.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1742185866",
                        "name": "Christopher Agia"
                    },
                    {
                        "authorId": "2178889",
                        "name": "Toki Migimatsu"
                    },
                    {
                        "authorId": "2128659134",
                        "name": "Jiajun Wu"
                    },
                    {
                        "authorId": "1775407",
                        "name": "J. Bohg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Alongside a similar vein of research would be to explore the application of model-based RL approaches [56], [57] on real cells directly and infer the model \u201con-the-go\u201d to apply RL on it."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ffc967543a28d043432d2c32dfefefbab944aef0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-12229",
                    "ArXiv": "2210.12229",
                    "DOI": "10.1109/TCNS.2022.3232527",
                    "CorpusId": 253098051
                },
                "corpusId": 253098051,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/ffc967543a28d043432d2c32dfefefbab944aef0",
                "title": "Deep Reinforcement Learning for Stabilization of Large-scale Probabilistic Boolean Networks",
                "abstract": "The ability to direct a Probabilistic Boolean Network (PBN) to a desired state is important to applications such as targeted therapeutics in cancer biology. Reinforcement Learning (RL) has been proposed as a framework that solves a discrete-time optimal control problem cast as a Markov Decision Process. We focus on an integrative framework powered by a model-free deep RL method that can address different flavours of the control problem (e.g., with or without control inputs; attractor state or a subset of the state space as the target domain). The method is agnostic to the distribution of probabilities for the next state, hence it does not use the probability transition matrix. The time complexity is only linear on the time steps, or interactions between the agent (deep RL) and the environment (PBN), during training. Indeed, we explore the scalability of the deep RL approach to (set) stabilization of large-scale PBNs and demonstrate successful control on large networks, including a metastatic melanoma PBN with 200 nodes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31949472",
                        "name": "S. Moschoyiannis"
                    },
                    {
                        "authorId": "2165587068",
                        "name": "Evangelos Chatzaroulas"
                    },
                    {
                        "authorId": "11729817",
                        "name": "V. Sliogeris"
                    },
                    {
                        "authorId": "2108675424",
                        "name": "Yuhu Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More recent works showed that it is possible to exploit expressive neural-networks models to learn complex dynamics in robotics systems [41], and use them for planning [17] or policy learning [30]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4d157fda990f939e7e7df8334b692b4bcf8195c6",
                "externalIds": {
                    "DBLP": "conf/isola/BerducciG22",
                    "ArXiv": "2210.11259",
                    "DOI": "10.1007/978-3-031-19849-6_21",
                    "CorpusId": 253018947
                },
                "corpusId": 253018947,
                "publicationVenue": {
                    "id": "dc7e70f3-5b10-4b26-909f-548cab617ada",
                    "name": "Leveraging Applications of Formal Methods",
                    "type": "conference",
                    "alternate_names": [
                        "Leveraging Appl Form Method",
                        "ISoLA"
                    ],
                    "url": "http://isola-conference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4d157fda990f939e7e7df8334b692b4bcf8195c6",
                "title": "Safe Policy Improvement in Constrained Markov Decision Processes",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2052104408",
                        "name": "Luigi Berducci"
                    },
                    {
                        "authorId": "1787208",
                        "name": "R. Grosu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[20] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",
                "The model is \u201crolled out\u201d to generate \u201cimagined\u201d trajectories, which are used either for direct planning [11, 8], or as training data for the agent\u2019s policy and value functions [56, 20].",
                "If we restrict ourselves to states and actions in the empirical dataset (EMP) or short-horizon rollouts that start in the empirical state-action distribution (DYNA), as is typical in Dyna-style approaches [57, 20], we limit ourselves to a small neighborhood of the empirical stateaction distribution."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "8fde42018717d282e51ed5528aa129cb38a15028",
                "externalIds": {
                    "DBLP": "conf/nips/PitisCMG22",
                    "ArXiv": "2210.11287",
                    "DOI": "10.48550/arXiv.2210.11287",
                    "CorpusId": 253018840
                },
                "corpusId": 253018840,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8fde42018717d282e51ed5528aa129cb38a15028",
                "title": "MoCoDA: Model-based Counterfactual Data Augmentation",
                "abstract": "The number of states in a dynamic process is exponential in the number of objects, making reinforcement learning (RL) difficult in complex, multi-object domains. For agents to scale to the real world, they will need to react to and reason about unseen combinations of objects. We argue that the ability to recognize and use local factorization in transition dynamics is a key element in unlocking the power of multi-object reasoning. To this end, we show that (1) known local structure in the environment transitions is sufficient for an exponential reduction in the sample complexity of training a dynamics model, and (2) a locally factored dynamics model provably generalizes out-of-distribution to unseen states and actions. Knowing the local structure also allows us to predict which unseen states and actions this dynamics model will generalize to. We propose to leverage these observations in a novel Model-based Counterfactual Data Augmentation (MoCoDA) framework. MoCoDA applies a learned locally factored dynamics model to an augmented distribution of states and actions to generate counterfactual transitions for RL. MoCoDA works with a broader set of local structures than prior work and allows for direct control over the augmented training distribution. We show that MoCoDA enables RL agents to learn policies that generalize to unseen states and actions. We use MoCoDA to train an offline RL agent to solve an out-of-distribution robotics manipulation task on which standard offline RL algorithms fail.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32305445",
                        "name": "Silviu Pitis"
                    },
                    {
                        "authorId": "3422145",
                        "name": "Elliot Creager"
                    },
                    {
                        "authorId": "49686756",
                        "name": "Ajay Mandlekar"
                    },
                    {
                        "authorId": "2054554660",
                        "name": "Animesh Garg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "By contrast, model-driven agent introduces an analytic environment model, which is then used for efficient simulation [24]\u2013[26] or gradient computing by backpropogation through time [27]\u2013[29]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ccd9147369a55826159078ded994026df94a20c0",
                "externalIds": {
                    "ArXiv": "2210.10613",
                    "DBLP": "journals/corr/abs-2210-10613",
                    "DOI": "10.48550/arXiv.2210.10613",
                    "CorpusId": 252992944
                },
                "corpusId": 252992944,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccd9147369a55826159078ded994026df94a20c0",
                "title": "Integrated Decision and Control for High-Level Automated Vehicles by Mixed Policy Gradient and Its Experiment Verification",
                "abstract": "Self-evolution is indispensable to realize full autonomous driving. This paper presents a self-evolving decision-making system based on the Integrated Decision and Control (IDC), an advanced framework built on reinforcement learning (RL). First, an RL algorithm called constrained mixed policy gradient (CMPG) is proposed to consistently upgrade the driving policy of the IDC. It adapts the MPG under the penalty method so that it can solve constrained optimization problems using both the data and model. Second, an attention-based encoding (ABE) method is designed to tackle the state representation issue. It introduces an embedding network for feature extraction and a weighting network for feature fusion, fulfilling order-insensitive encoding and importance distinguishing of road users. Finally, by fusing CMPG and ABE, we develop the first data-driven decision and control system under the IDC architecture, and deploy the system on a fully-functional self-driving vehicle running in daily operation. Experiment results show that boosting by data, the system can achieve better driving ability over model-based methods. It also demonstrates safe, efficient and smart driving behavior in various complex scenes at a signalized intersection with real mixed traffic flow.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50463545",
                        "name": "Yang Guan"
                    },
                    {
                        "authorId": "2162087968",
                        "name": "Liye Tang"
                    },
                    {
                        "authorId": "2145255367",
                        "name": "Chuanxiao Li"
                    },
                    {
                        "authorId": "2023891",
                        "name": "S. Li"
                    },
                    {
                        "authorId": "3649406",
                        "name": "Yangang Ren"
                    },
                    {
                        "authorId": "3264197",
                        "name": "Junqing Wei"
                    },
                    {
                        "authorId": "2166288707",
                        "name": "Bo Zhang"
                    },
                    {
                        "authorId": "2149141969",
                        "name": "Ke Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another line of work focuses on RL with a learned model, which is promising for sample efficient learning [8, 10, 11, 12, 18, 27, 29, 48, 14]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4c09d6969f451d288d8a188aa7e48a2af38d1911",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09598",
                    "ArXiv": "2210.09598",
                    "DOI": "10.48550/arXiv.2210.09598",
                    "CorpusId": 252968311
                },
                "corpusId": 252968311,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4c09d6969f451d288d8a188aa7e48a2af38d1911",
                "title": "Planning for Sample Efficient Imitation Learning",
                "abstract": "Imitation learning is a class of promising policy learning algorithms that is free from many practical issues with reinforcement learning, such as the reward design issue and the exploration hardness. However, the current imitation algorithm struggles to achieve both high performance and high in-environment sample efficiency simultaneously. Behavioral Cloning (BC) does not need in-environment interactions, but it suffers from the covariate shift problem which harms its performance. Adversarial Imitation Learning (AIL) turns imitation learning into a distribution matching problem. It can achieve better performance on some tasks but it requires a large number of in-environment interactions. Inspired by the recent success of EfficientZero in RL, we propose EfficientImitate (EI), a planning-based imitation learning method that can achieve high in-environment sample efficiency and performance simultaneously. Our algorithmic contribution in this paper is two-fold. First, we extend AIL into the MCTS-based RL. Second, we show the seemingly incompatible two classes of imitation algorithms (BC and AIL) can be naturally unified under our framework, enjoying the benefits of both. We benchmark our method not only on the state-based DeepMind Control Suite, but also on the image version which many previous works find highly challenging. Experimental results show that EI achieves state-of-the-art results in performance and sample efficiency. EI shows over 4x gain in performance in the limited sample setting on state-based and image-based tasks and can solve challenging problems like Humanoid, where previous methods fail with small amount of interactions. Our code is available at https://github.com/zhaohengyin/EfficientImitate.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1693997019",
                        "name": "Zhao-Heng Yin"
                    },
                    {
                        "authorId": "83546634",
                        "name": "Weirui Ye"
                    },
                    {
                        "authorId": "2109347324",
                        "name": "Qifeng Chen"
                    },
                    {
                        "authorId": null,
                        "name": "Yang Gao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While many approaches rely on explicit epistemic uncertainty to tackle this issue (Chua et al., 2018; Janner et al., 2019), RSSMs succeed without capturing epistemic uncertainty.",
                "\u2026needed for model-based RL.\nEpistemic Uncertainty for Model-Based RL. Ample work emphasises the importance of modeling epistemic uncertainty for model-based RL (Deisenroth & Rasmussen, 2011; Chua et al., 2018; Janner et al., 2019) and several authors equipped RSSMs with epistemic uncertainty.",
                "Many modelbased RL approaches (Chua et al., 2018; Janner et al., 2019) handle this issue by explicitly modeling the epistemic uncertainty of the model, which is not required by the RSSM.",
                "Ample work emphasises the importance of modeling epistemic uncertainty for model-based RL (Deisenroth & Rasmussen, 2011; Chua et al., 2018; Janner et al., 2019) and several authors equipped RSSMs with epistemic uncertainty."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5a617480cd981a6d01b87ae167d36afc2ec9c24a",
                "externalIds": {
                    "DBLP": "journals/tmlr/BeckerN22",
                    "ArXiv": "2210.09256",
                    "DOI": "10.48550/arXiv.2210.09256",
                    "CorpusId": 252918781
                },
                "corpusId": 252918781,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5a617480cd981a6d01b87ae167d36afc2ec9c24a",
                "title": "On Uncertainty in Deep State Space Models for Model-Based Reinforcement Learning",
                "abstract": "Improved state space models, such as Recurrent State Space Models (RSSMs), are a key factor behind recent advances in model-based reinforcement learning (RL). Yet, despite their empirical success, many of the underlying design choices are not well understood. We show that RSSMs use a suboptimal inference scheme and that models trained using this inference overestimate the aleatoric uncertainty of the ground truth system. We find this overestimation implicitly regularizes RSSMs and allows them to succeed in model-based RL. We postulate that this implicit regularization fulfills the same functionality as explicitly modeling epistemic uncertainty, which is crucial for many other model-based RL approaches. Yet, overestimating aleatoric uncertainty can also impair performance in cases where accurately estimating it matters, e.g., when we have to deal with occlusions, missing observations, or fusing sensor modalities at different frequencies. Moreover, the implicit regularization is a side-effect of the inference scheme and not the result of a rigorous, principled formulation, which renders analyzing or improving RSSMs difficult. Thus, we propose an alternative approach building on well-understood components for modeling aleatoric and epistemic uncertainty, dubbed Variational Recurrent Kalman Network (VRKN). This approach uses Kalman updates for exact smoothing inference in a latent space and Monte Carlo Dropout to model epistemic uncertainty. Due to the Kalman updates, the VRKN can naturally handle missing observations or sensor fusion problems with varying numbers of observations per time step. Our experiments show that using the VRKN instead of the RSSM improves performance in tasks where appropriately capturing aleatoric uncertainty is crucial while matching it in the deterministic standard benchmarks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2070414791",
                        "name": "P. Becker"
                    },
                    {
                        "authorId": "26599977",
                        "name": "G. Neumann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[12]), with the exception of no environment interactions.",
                "For the model rollouts, we use the variance scaling action selection strategy for the first action only and use increasing rollout lengths for all domains, similar to [12]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4532aa9a2d51e4568f7fe9f2b54e2d4867622c5a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09337",
                    "ArXiv": "2210.09337",
                    "DOI": "10.48550/arXiv.2210.09337",
                    "CorpusId": 252967978
                },
                "corpusId": 252967978,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4532aa9a2d51e4568f7fe9f2b54e2d4867622c5a",
                "title": "Robust Imitation of a Few Demonstrations with a Backwards Model",
                "abstract": "Behavior cloning of expert demonstrations can speed up learning optimal policies in a more sample-efficient way over reinforcement learning. However, the policy cannot extrapolate well to unseen states outside of the demonstration data, creating covariate shift (agent drifting away from demonstrations) and compounding errors. In this work, we tackle this issue by extending the region of attraction around the demonstrations so that the agent can learn how to get back onto the demonstrated trajectories if it veers off-course. We train a generative backwards dynamics model and generate short imagined trajectories from states in the demonstrations. By imitating both demonstrations and these model rollouts, the agent learns the demonstrated paths and how to get back onto these paths. With optimal or near-optimal demonstrations, the learned policy will be both optimal and robust to deviations, with a wider region of attraction. On continuous control domains, we evaluate the robustness when starting from different initial states unseen in the demonstration data. While both our method and other imitation learning baselines can successfully solve the tasks for initial states in the training distribution, our method exhibits considerably more robustness to different initial states.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118946132",
                        "name": "Jung Yeon Park"
                    },
                    {
                        "authorId": "145307121",
                        "name": "Lawson L. S. Wong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Consistently, we find that our policy enjoys higher performance, with an average return lead of about 1855.29 over MBPO in the first 300k steps.",
                "For practical implementation, the probabilistic models { \u02c6f\u03c61 , \u02c6f\u03c62 , . . . , \u02c6f\u03c6K} are fitted on shared but differently shuffled replay buffer De, and the target is to optimize the Negative Log Likelihood (NLL).",
                "(6) AutoMBPO [28], a variant of MBPO, that uses an automatic hyperparameter controller to tune the model-training frequency but suffers from high pre-training cost and lacks theoretical analysis on parameters rationality.",
                "To ensure a fair comparison, we run CMLO and MBPO with the same network architectures and training configurations based on MBRLLIB.",
                "Various attempts have been proposed to improve model accuracy by investigating high-capacity models (the model ensemble technique [27, 8] and better function approximators [15, 38]) or amending the policy optimization stage based on model bias [24, 39, 28, 20, 7, 57].",
                "Among then, we truncate some redundant observations for Hopper, Ant and Humanoid as our model-based baselines (MBPO[20], AutoMBPO[28]) do.",
                "The probabilistic models are fitted on shared but differently shuffled replay buffer De, and the target is to optimize the Negative Log Likelihood (NLL).",
                "We find that CMLO achieves a more accurate model than the state-of-the-art baseline MBPO.",
                "Our method adopts an ensemble of probabilistic networks similarly as in [8, 20].",
                "The main difference from the general rollouts mechanism is that we restrict our rollouts to be generated from fresh models, rather than using outdated models to generate rollout data as MBPO [20] and AutoMBPO [28] do in their implementations.",
                "As for model-based methods, we compare with several algorithms including PETS [8], SLBO [34], MBPO [20] and AutoMBPO [28].",
                "In HalfCheetah, we find that our policy achieves higher coverage especially in first 4 stages than MBPO.",
                "While constructing such a bound for performance gap is straightforward, it has not been explored in previous MBRL theorectical analyses, instead they [49, 34, 20, 12, 28] turn to bound the discrepancy between returns under a model and those in the real environment.",
                "(5) MBPO [20], that employs a similar design of model ensemble technique (ensemble of probabilistic dynamics networks) and policy optimization oracle (SAC) as we do.",
                "Although there has been interest in non-decreasing performance guarantee, previous works [34, 20] commonly derive under a \"discrepancy bound\" scheme disregarding the model shifts (i.",
                "To reduce model bias, we chose to use NLL as a loss function in our implementation, which has been shown an effective way to learn model dynamics.",
                "And training objectives vary from Mean Square Error (MSE) [38, 34], Negative Log Likelihood (NLL) [8, 20], etc.",
                "Here, we present the numerical comparison to MBPO in Table 5.",
                "Besides, we notice that the performance is comparable to other MBRL baselines (MBPO etc.) when fixing our model training interval at 250.",
                "Although there has been recent interest in related subjects, most of the theoretical works seek to characterize the monotonicity in terms of a fixed model of interest [49, 34, 20, 12, 28], which does not naturally fit our case when the model is dynamically shifted."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "c02539e165cf327d3b8331597ab722300fb2981e",
                "externalIds": {
                    "ArXiv": "2210.08349",
                    "DBLP": "journals/corr/abs-2210-08349",
                    "DOI": "10.48550/arXiv.2210.08349",
                    "CorpusId": 252917812
                },
                "corpusId": 252917812,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c02539e165cf327d3b8331597ab722300fb2981e",
                "title": "When to Update Your Model: Constrained Model-based Reinforcement Learning",
                "abstract": "Designing and analyzing model-based RL (MBRL) algorithms with guaranteed monotonic improvement has been challenging, mainly due to the interdependence between policy optimization and model learning. Existing discrepancy bounds generally ignore the impacts of model shifts, and their corresponding algorithms are prone to degrade performance by drastic model updating. In this work, we first propose a novel and general theoretical scheme for a non-decreasing performance guarantee of MBRL. Our follow-up derived bounds reveal the relationship between model shifts and performance improvement. These discoveries encourage us to formulate a constrained lower-bound optimization problem to permit the monotonicity of MBRL. A further example demonstrates that learning models from a dynamically-varying number of explorations benefit the eventual returns. Motivated by these analyses, we design a simple but effective algorithm CMLO (Constrained Model-shift Lower-bound Optimization), by introducing an event-triggered mechanism that flexibly determines when to update the model. Experiments show that CMLO surpasses other state-of-the-art methods and produces a boost when various policy optimization methods are employed.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2072498620",
                        "name": "Tianying Ji"
                    },
                    {
                        "authorId": "65966135",
                        "name": "Yu-Juan Luo"
                    },
                    {
                        "authorId": "2323566",
                        "name": "Fuchun Sun"
                    },
                    {
                        "authorId": "40800641",
                        "name": "Mingxuan Jing"
                    },
                    {
                        "authorId": "51209425",
                        "name": "Fengxiang He"
                    },
                    {
                        "authorId": "2978255",
                        "name": "Wen-bing Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-Based Policy Optimization (MBPO) [17] uses short model-generated rollouts branched from real data to update the policy, and does\nnot consider safety constraints.",
                "Among all tasks, model-based algorithms (DRPO, MBPO and SMBPO) have fewer CTVs due to their high sample efficiency.",
                "Due to the probabilistic model (either posterior sampling in [19] or gaussian ensembles in [17]), the state trajectories vary a lot, leading to a distribution of Qh(s, a).",
                "Although [15], [17] proposed probabilistic model ensembles and clipped rollouts to cover the true dynamics within the support of the ensembles and mitigate the error, the stochasticity of the subsequent states s\u2032 generated by the uncertain model P\u0302 will still lead to a deviated or even wrong estimation about the cost value or reachability certificate in safe RL if not addressed properly.",
                "Model-Based Policy Optimization (MBPO) [17] uses short model-generated rollouts branched from real data to update the policy, and does",
                "Our DRPO relies on MBPO as well but leverages DRC and the shield policy to reduce violations.",
                "Model-based RL [1], [15], [17], [24] replaces the unknown transition dynamics with a learned model P\u0302 which is trained by minimizing E(s,a,s\u2032)\u223cB[D(P, P\u0302 )], where D is a certain distance metric and B is either an offline dataset of stateaction pairs or a replay buffer storing historical interactions.",
                "1) Model Learning and Usage: Same as prior MBRL work [15], [17], [18], we adopt an ensemble of diagonal Gaussian dynamics model parameterized by \u03c6 as the world model approximators, denoted as {P\u0302\u03c6i}i=1, where P\u0302\u03c6i = N (\u03bc\u03c6i(s, a), \u03c3(2) \u03c6i(s, a)).",
                "This type of truncated rollout method leads to a smaller error in terms of the value function [17].",
                ", performing policy updates with model-generated virtual data [1], [15]\u2013[17].",
                "Safe MBPO (SMBPO) [18] builds on MBPO and heavily penalizes unsafe trajectories to avoid safety violations."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "69bdc99655204190697067c3da5296e544e6865d",
                "externalIds": {
                    "ArXiv": "2210.07553",
                    "DBLP": "journals/corr/abs-2210-07553",
                    "DOI": "10.48550/arXiv.2210.07553",
                    "CorpusId": 252907858
                },
                "corpusId": 252907858,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/69bdc99655204190697067c3da5296e544e6865d",
                "title": "Safe Model-Based Reinforcement Learning with an Uncertainty-Aware Reachability Certificate",
                "abstract": "Safe reinforcement learning (RL) that solves constraint-satisfactory policies provides a promising way to the broader safety-critical applications of RL in real-world problems such as robotics. Among all safe RL approaches, model-based methods reduce training time violations further due to their high sample efficiency. However, lacking safety robustness against the model uncertainties remains an issue in safe model-based RL, especially in training time safety. In this paper, we propose a distributional reachability certificate (DRC) and its Bellman equation to address model uncertainties and characterize robust persistently safe states. Furthermore, we build a safe RL framework to resolve constraints required by the DRC and its corresponding shield policy. We also devise a line search method to maintain safety and reach higher returns simultaneously while leveraging the shield policy. Comprehensive experiments on classical benchmarks such as constrained tracking and navigation indicate that the proposed algorithm achieves comparable returns with much fewer constraint violations during training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48217817",
                        "name": "Dongjie Yu"
                    },
                    {
                        "authorId": "2051181955",
                        "name": "Wenjun Zou"
                    },
                    {
                        "authorId": "2108585577",
                        "name": "Yujie Yang"
                    },
                    {
                        "authorId": "2048987083",
                        "name": "Haitong Ma"
                    },
                    {
                        "authorId": "2153701697",
                        "name": "Sheng Li"
                    },
                    {
                        "authorId": "23637596",
                        "name": "Jingliang Duan"
                    },
                    {
                        "authorId": "1391201846",
                        "name": "Jianyu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "3 and Figure 4 of Kurutach et al. [2018]), but increase in number of models also leads to increase in space complexity.",
                "In order to tackle this problem, most of the model-based RL approaches [Deisenroth and Rasmussen, 2011, Kurutach et al., 2018, Janner et al., 2019] use shorter (or truncated) horizon during the policy optimization phase and achieve similar performance as Model-Free RL approaches."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "e79d9351477e48ca8e20cd97d6fb14973f26f004",
                "externalIds": {
                    "ArXiv": "2210.07573",
                    "DBLP": "journals/corr/abs-2210-07573",
                    "DOI": "10.48550/arXiv.2210.07573",
                    "CorpusId": 252907384
                },
                "corpusId": 252907384,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e79d9351477e48ca8e20cd97d6fb14973f26f004",
                "title": "Model-based Safe Deep Reinforcement Learning via a Constrained Proximal Policy Optimization Algorithm",
                "abstract": "During initial iterations of training in most Reinforcement Learning (RL) algorithms, agents perform a significant number of random exploratory steps. In the real world, this can limit the practicality of these algorithms as it can lead to potentially dangerous behavior. Hence safe exploration is a critical issue in applying RL algorithms in the real world. This problem has been recently well studied under the Constrained Markov Decision Process (CMDP) Framework, where in addition to single-stage rewards, an agent receives single-stage costs or penalties as well depending on the state transitions. The prescribed cost functions are responsible for mapping undesirable behavior at any given time-step to a scalar value. The goal then is to find a feasible policy that maximizes reward returns while constraining the cost returns to be below a prescribed threshold during training as well as deployment. We propose an On-policy Model-based Safe Deep RL algorithm in which we learn the transition dynamics of the environment in an online manner as well as find a feasible optimal policy using the Lagrangian Relaxation-based Proximal Policy Optimization. We use an ensemble of neural networks with different initializations to tackle epistemic and aleatoric uncertainty issues faced during environment model learning. We compare our approach with relevant model-free and model-based approaches in Constrained RL using the challenging Safe Reinforcement Learning benchmark - the Open AI Safety Gym. We demonstrate that our algorithm is more sample efficient and results in lower cumulative hazard violations as compared to constrained model-free approaches. Further, our approach shows better reward performance than other constrained model-based approaches in the literature.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2187873478",
                        "name": "Ashish Kumar Jayant"
                    },
                    {
                        "authorId": "143683893",
                        "name": "S. Bhatnagar"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4cf9dfe7ba8a2c011e8cf7d0188504bb46aa493f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-07241",
                    "ArXiv": "2210.07241",
                    "DOI": "10.1109/LRA.2023.3259681",
                    "CorpusId": 252873614
                },
                "corpusId": 252873614,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4cf9dfe7ba8a2c011e8cf7d0188504bb46aa493f",
                "title": "Visual Reinforcement Learning With Self-Supervised 3D Representations",
                "abstract": "A prominent approach to visual Reinforcement Learning (RL) is to learn an internal state representation using self-supervised methods, which has the potential benefit of improved sample-efficiency and generalization through additional learning signal and inductive biases. However, while the real world is inherently 3D, prior efforts have largely been focused on leveraging 2D computer vision techniques as auxiliary self-supervision. In this work, we present a unified framework for self-supervised learning of 3D representations for motor control. Our proposed framework consists of two phases: a pretraining phase where a deep voxel-based 3D autoencoder is pretrained on a large object-centric dataset, and a finetuning phase where the representation is jointly finetuned together with RL on in-domain data. We empirically show that our method enjoys improved sample efficiency compared to 2D representation learning methods. Additionally, our learned policies transfer zero-shot to a real robot setup with only approximate geometric correspondence, and successfully solve motor control tasks that involve grasping and lifting from a single, uncalibrated RGB camera.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151089356",
                        "name": "Yanjie Ze"
                    },
                    {
                        "authorId": "1491707104",
                        "name": "Nicklas Hansen"
                    },
                    {
                        "authorId": "47559228",
                        "name": "Yinbo Chen"
                    },
                    {
                        "authorId": "120480799",
                        "name": "Mohit Jain"
                    },
                    {
                        "authorId": "122024152",
                        "name": "Xiaolong Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For both experience thresholds, SSPG obtains the best average performance in 5/6 tasks, and still lags very close the model-based MBPO [61] algorithm for the remaining task (HalfCheetah-v2).",
                "We consider REDQ [60] and MBPO [61] for state-of-the-art algorithms based on the traditional model-free and model-based RL frameworks.",
                "Following REDQ [60] and MBPO [61], we employ a critic ensemble of 10 models and use the suggested task-specific target entropy values for automatic tuning of the MaxEnt coefficient, \u03b1 [39].",
                "SSPG converges much earlier than other algorithms, even while performing many less optimization steps (REDQ, REDQ-FLOW, MBPO, and SAC-20 all employ a UTD of 20, while we train SSPG with a UTD of 10, see Section 4)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3e5b3258df48dd41f53ffbd76ea391775b1d9dd3",
                "externalIds": {
                    "DBLP": "conf/nips/CetinC22",
                    "ArXiv": "2210.06766",
                    "DOI": "10.48550/arXiv.2210.06766",
                    "CorpusId": 252873371
                },
                "corpusId": 252873371,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3e5b3258df48dd41f53ffbd76ea391775b1d9dd3",
                "title": "Policy Gradient With Serial Markov Chain Reasoning",
                "abstract": "We introduce a new framework that performs decision-making in reinforcement learning (RL) as an iterative reasoning process. We model agent behavior as the steady-state distribution of a parameterized reasoning Markov chain (RMC), optimized with a new tractable estimate of the policy gradient. We perform action selection by simulating the RMC for enough reasoning steps to approach its steady-state distribution. We show our framework has several useful properties that are inherently missing from traditional RL. For instance, it allows agent behavior to approximate any continuous distribution over actions by parameterizing the RMC with a simple Gaussian transition function. Moreover, the number of reasoning steps to reach convergence can scale adaptively with the difficulty of each action selection decision and can be accelerated by re-using past solutions. Our resulting algorithm achieves state-of-the-art performance in popular Mujoco and DeepMind Control benchmarks, both for proprioceptive and pixel-based tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2052306608",
                        "name": "Edoardo Cetin"
                    },
                    {
                        "authorId": "1386958722",
                        "name": "O. \u00c7eliktutan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[43] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",
                "However, directly optimizing policy based on an offline learned model is vulnerable to model exploitation [22, 43]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c97943178542a6191087f317b0d51448d666ee2b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-06692",
                    "ArXiv": "2210.06692",
                    "DOI": "10.48550/arXiv.2210.06692",
                    "CorpusId": 252873146
                },
                "corpusId": 252873146,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c97943178542a6191087f317b0d51448d666ee2b",
                "title": "Model-Based Offline Reinforcement Learning with Pessimism-Modulated Dynamics Belief",
                "abstract": "Model-based offline reinforcement learning (RL) aims to find highly rewarding policy, by leveraging a previously collected static dataset and a dynamics model. While the dynamics model learned through reuse of the static dataset, its generalization ability hopefully promotes policy learning if properly utilized. To that end, several works propose to quantify the uncertainty of predicted dynamics, and explicitly apply it to penalize reward. However, as the dynamics and the reward are intrinsically different factors in context of MDP, characterizing the impact of dynamics uncertainty through reward penalty may incur unexpected tradeoff between model utilization and risk avoidance. In this work, we instead maintain a belief distribution over dynamics, and evaluate/optimize policy through biased sampling from the belief. The sampling procedure, biased towards pessimism, is derived based on an alternating Markov game formulation of offline RL. We formally show that the biased sampling naturally induces an updated dynamics belief with policy-dependent reweighting factor, termed Pessimism-Modulated Dynamics Belief. To improve policy, we devise an iterative regularized policy optimization algorithm for the game, with guarantee of monotonous improvement under certain condition. To make practical, we further devise an offline RL algorithm to approximately find the solution. Empirical results show that the proposed approach achieves state-of-the-art performance on a wide range of benchmark tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "10683161",
                        "name": "Kaiyang Guo"
                    },
                    {
                        "authorId": "49713700",
                        "name": "Yunfeng Shao"
                    },
                    {
                        "authorId": "2061441",
                        "name": "Yanhui Geng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(c) To alleviate the compounding errors (Janner et al., 2019), MuZero Unplugged unrolls the dynamics for multiple steps (5) and learns the policy, value, and reward predictions on the recurrently imagined latent state to match the real trajectory\u2019s improvement targets."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1aa039247885c446a05884ce2a3d079f94722b27",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-05980",
                    "ArXiv": "2210.05980",
                    "DOI": "10.48550/arXiv.2210.05980",
                    "CorpusId": 252846076
                },
                "corpusId": 252846076,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1aa039247885c446a05884ce2a3d079f94722b27",
                "title": "Efficient Offline Policy Optimization with a Learned Model",
                "abstract": "MuZero Unplugged presents a promising approach for offline policy learning from logged data. It conducts Monte-Carlo Tree Search (MCTS) with a learned model and leverages Reanalyze algorithm to learn purely from offline data. For good performance, MCTS requires accurate learned models and a large number of simulations, thus costing huge computing time. This paper investigates a few hypotheses where MuZero Unplugged may not work well under the offline RL settings, including 1) learning with limited data coverage; 2) learning from offline data of stochastic environments; 3) improperly parameterized models given the offline data; 4) with a low compute budget. We propose to use a regularized one-step look-ahead approach to tackle the above issues. Instead of planning with the expensive MCTS, we use the learned model to construct an advantage estimation based on a one-step rollout. Policy improvements are towards the direction that maximizes the estimated advantage with regularization of the dataset. We conduct extensive empirical studies with BSuite environments to verify the hypotheses and then run our algorithm on the RL Unplugged Atari benchmark. Experimental results show that our proposed approach achieves stable performance even with an inaccurate learned model. On the large-scale Atari benchmark, the proposed method outperforms MuZero Unplugged by 43%. Most significantly, it uses only 5.6% wall-clock time (i.e., 1 hour) compared to MuZero Unplugged (i.e., 17.8 hours) to achieve a 150% IQM normalized score with the same hardware and software stacks. Our implementation is open-sourced at https://github.com/sail-sg/rosmo.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117943295",
                        "name": "Zi-Yan Liu"
                    },
                    {
                        "authorId": "2118155396",
                        "name": "Siyi Li"
                    },
                    {
                        "authorId": "46605464",
                        "name": "W. Lee"
                    },
                    {
                        "authorId": "2186749683",
                        "name": "Shuicheng Yan"
                    },
                    {
                        "authorId": "2351434",
                        "name": "Zhongwen Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c3355f238b34edfb4ad5d375801d3b61f631d5c5",
                "externalIds": {
                    "DBLP": "conf/icra/ZhouKSGRK23",
                    "ArXiv": "2210.06479",
                    "DOI": "10.1109/ICRA48891.2023.10161474",
                    "CorpusId": 252873579
                },
                "corpusId": 252873579,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c3355f238b34edfb4ad5d375801d3b61f631d5c5",
                "title": "Real World Offline Reinforcement Learning with Realistic Data Source",
                "abstract": "Offline reinforcement learning (ORL) holds great promise for robot learning due to its ability to learn from arbitrary pre-generated experience. However, current ORL benchmarks are almost entirely in simulation and utilize contrived datasets like replay buffers of online RL agents or sub-optimal trajectories, and thus hold limited relevance for real-world robotics. In this work (Real-ORL), we posit that data collected from safe operations of closely related tasks are more practical data sources for real-world robot learning. Under these settings, we perform an extensive (6500+ trajectories collected over 800+ robot hours and 270+ human labor hour) empirical study evaluating generalization and transfer capabilities of representative ORL methods on four real-world tabletop manipulation tasks. Our study finds that ORL and imitation learning prefer different action spaces, and that ORL algorithms can generalize from leveraging offline heterogeneous data sources and outperform imitation learning. We release our dataset and implementations at URL: https://sites.google.com/view/real-orl.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2107532625",
                        "name": "G. Zhou"
                    },
                    {
                        "authorId": "3383717",
                        "name": "Liyiming Ke"
                    },
                    {
                        "authorId": "1752197",
                        "name": "S. Srinivasa"
                    },
                    {
                        "authorId": "2117767136",
                        "name": "Abhi Gupta"
                    },
                    {
                        "authorId": "19275599",
                        "name": "A. Rajeswaran"
                    },
                    {
                        "authorId": "2109446216",
                        "name": "Vikash Kumar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most of the existing offline MBRL works focus on policy learning under a given model trained by MLE.",
                "(11); train P\u0302 and r\u0302 by weighted MLE (Eq.",
                "In online (off-policy) MBRL, Lambert et al. [23] identify the mismatched objectives between the MLE model-training and the model\u2019s usage of improving the control performance.",
                "Most of the prior offline model-based RL (MBRL) methods [e.g., 16, 18\u201321], however, first pretrain a one-step forward dynamic model via maximum likelihood estimation (MLE) on the offline dataset, and then use the learned model to train the policy, without further improving the dynamic model during the policy learning process.",
                "With the offline dataset Denv, P\u0302 is trained via the MLE [15, 16, 18] as\narg maxP\u0302\u2208P E(s,a,s\u2032)\u223cDenv [ log P\u0302 (s\u2032 | s, a) ] .",
                "The reward function r\u0302 is still estimated by the weighted-MLE objective.",
                "This lower bound, leading to a weighted MLE objective for the dynamic-model training, is relaxed to a tractable regularized objective for the policy learning.",
                "As in prior work using Gaussian probabilistic ensemble on model-based RL [83, 15, 16, 21, 18], we use a double-head architecture for our dynamic model, where the two output heads represent the mean and log-standard-deviation of the normal distribution of the predicted output, respectively.",
                "We initialize the dynamic model by standard MLE training, and periodically update the model by minimizing Eq.",
                "To verify the effectiveness of our MIW-weighted model (re)training scheme, we compare our AMPL with its variant of training the model only at the beginning using MLE, i.e., No Weights (dubbed as NW).",
                "Thus, given the MIW \u03c9, we can optimize P\u0302 by minimizing the following loss\n`(P\u0302 ) , \u2212E(s,a,s\u2032)\u223cdP\u2217\u03c0b,\u03b3 [ \u03c9(s, a) log P\u0302 (s\u2032 | s, a) ] , (6)\nwhich is an MLE objective weighted by \u03c9(s, a).",
                "[15] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",
                "Most similar to our work, Hishinuma and Senda [47] also use a MIW-weighted MLE objective for model training.",
                "As a result, the objective function used for model training (e.g., MLE) and the objective of model utilization are unrelated with each other.",
                "Initialize: Dynamic model P\u0302 and r\u0302, policy \u03c0\u03c6, critics Q\u03b81 and Q\u03b82 , discriminator D\u03c8 , MIW \u03c9. Initialize P\u0302 and r\u0302 via the MLE (Eq.",
                "Rather than using a fixed MLE-trained model, we derive an objective that trains both the policy and the dynamic model toward maximizing a lower bound of true expected return (simultaneously minimizing the policy evaluation error |J(\u03c0, P \u2217)\u2212 J(\u03c0, P\u0302 )|).",
                "We follow the literature [83, 15, 16, 47] to assume no prior knowledge about the reward function and thus use neural network to approximate transition dynamic and the reward function.",
                "The model is initialized by the MLE loss.",
                "With the offline dataset Denv, P\u0302 is trained via the MLE [15, 16, 18] as arg maxP\u0302\u2208P E(s,a,s\u2032)\u223cDenv [ log P\u0302 (s\u2032 | s, a) ] ."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "2471a3584f8567d82379fd8cfa9764b5df5197c7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-05922",
                    "ArXiv": "2210.05922",
                    "DOI": "10.48550/arXiv.2210.05922",
                    "CorpusId": 252846640
                },
                "corpusId": 252846640,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2471a3584f8567d82379fd8cfa9764b5df5197c7",
                "title": "A Unified Framework for Alternating Offline Model Training and Policy Learning",
                "abstract": "In offline model-based reinforcement learning (offline MBRL), we learn a dynamic model from historically collected data, and subsequently utilize the learned model and fixed datasets for policy learning, without further interacting with the environment. Offline MBRL algorithms can improve the efficiency and stability of policy learning over the model-free algorithms. However, in most of the existing offline MBRL algorithms, the learning objectives for the dynamic models and the policies are isolated from each other. Such an objective mismatch may lead to inferior performance of the learned agents. In this paper, we address this issue by developing an iterative offline MBRL framework, where we maximize a lower bound of the true expected return, by alternating between dynamic-model training and policy learning. With the proposed unified model-policy learning framework, we achieve competitive performance on a wide range of continuous-control offline reinforcement learning datasets. Source code is publicly released.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155974419",
                        "name": "Shentao Yang"
                    },
                    {
                        "authorId": "2107944048",
                        "name": "Shujian Zhang"
                    },
                    {
                        "authorId": "22758695",
                        "name": "Yihao Feng"
                    },
                    {
                        "authorId": "2152176954",
                        "name": "Mi Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The differentiable world model bridges the gap between the simulation and control policy, allowing the objectives defined in the motion domain to supervise the policy directly, thus achieving efficient and stable training [Deisenroth and Rasmussen 2011; Janner et al. 2021].",
                "\u2026to pass through the barrier of the simulation [Chiappa et al. 2017; Heess et al. 2015; Schmidhuber 1990], thus enabling policy optimization to be solved efficiently using gradient-based techniques [Deisenroth and Rasmussen 2011; Heess et al. 2015; Janner et al. 2021; Nagabandi et al. 2018].",
                "Approximate models, or the World Models [Ha and Schmidhuber 2018], are typically formulated as Gaussian Process [Deisenroth and Rasmussen 2011] or neural networks [Janner et al. 2021; Nagabandi et al. 2018]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "a5978dc8e57c27966d218a06e4ccc8aa3a507507",
                "externalIds": {
                    "ArXiv": "2210.06063",
                    "DBLP": "journals/corr/abs-2210-06063",
                    "DOI": "10.1145/3550454.3555434",
                    "CorpusId": 252846386
                },
                "corpusId": 252846386,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a5978dc8e57c27966d218a06e4ccc8aa3a507507",
                "title": "ControlVAE",
                "abstract": "In this paper, we introduce ControlVAE, a novel model-based framework for learning generative motion control policies based on variational autoencoders (VAE). Our framework can learn a rich and flexible latent representation of skills and a skill-conditioned generative control policy from a diverse set of unorganized motion sequences, which enables the generation of realistic human behaviors by sampling in the latent space and allows high-level control policies to reuse the learned skills to accomplish a variety of downstream tasks. In the training of ControlVAE, we employ a learnable world model to realize direct supervision of the latent space and the control policy. This world model effectively captures the unknown dynamics of the simulation system, enabling efficient model-based learning of high-level downstream tasks. We also learn a state-conditional prior distribution in the VAE-based generative control policy, which generates a skill embedding that outperforms the non-conditional priors in downstream tasks. We demonstrate the effectiveness of ControlVAE using a diverse set of tasks, which allows realistic and interactive control of the simulated characters.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1492178134",
                        "name": "Heyuan Yao"
                    },
                    {
                        "authorId": "4525094",
                        "name": "Zhenhua Song"
                    },
                    {
                        "authorId": "26348170",
                        "name": "B. Chen"
                    },
                    {
                        "authorId": "1409896471",
                        "name": "Libin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Typically, MB algorithms \u2014 e.g., MOPO (Yu et al., 2020), MOReL (Kidambi et al., 2020), and COMBO (Yu et al., 2021) \u2014 adopt the Dyna-style policy optimization approach developed in online RL (Janner et al., 2019; Sutton, 1990).",
                "Assume we have a bootstrapped dynamics ensemble model f\u0302 consisting of K different models (f\u03021, . . . , f\u0302K) trained with different sequences of mini-batches of D (Chua et al., 2018; Janner et al., 2019).",
                "Model-based value expansion Unlike Dyna-style methods that augment the dataset with modelgenerated rollouts (Sutton, 1990; Janner et al., 2019), MVE (Feinberg et al., 2018) uses them for better estimating TD targets during policy evaluation.",
                "The approach for training the dynamics ensemble closely follows previous work on Bayesian ensemble estimation (Chua et al., 2018; Janner et al., 2019).",
                ", f\u0302K) trained with different sequences of mini-batches of D (Chua et al., 2018; Janner et al., 2019).",
                "We follow the common configurations used in the literature, e.g., MBPO (Janner et al., 2019) and MOPO (Yu et al., 2020).",
                "Model-based value expansion Unlike Dyna-style methods that augment the dataset with modelgenerated rollouts (Sutton, 1990; Janner et al., 2019), MVE (Feinberg et al.",
                "These methods follow the Dyna-style policy learning where model rollouts are used to augment the offline dataset (Sutton, 1990; Janner et al., 2019).",
                ", 2021) \u2014 adopt the Dyna-style policy optimization approach developed in online RL (Janner et al., 2019; Sutton, 1990)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b70f24e1c189d3caa11792a759bc763af93125f2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-03802",
                    "ArXiv": "2210.03802",
                    "DOI": "10.48550/arXiv.2210.03802",
                    "CorpusId": 252781188
                },
                "corpusId": 252781188,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b70f24e1c189d3caa11792a759bc763af93125f2",
                "title": "Conservative Bayesian Model-Based Value Expansion for Offline Policy Optimization",
                "abstract": "Offline reinforcement learning (RL) addresses the problem of learning a performant policy from a fixed batch of data collected by following some behavior policy. Model-based approaches are particularly appealing in the offline setting since they can extract more learning signals from the logged dataset by learning a model of the environment. However, the performance of existing model-based approaches falls short of model-free counterparts, due to the compounding of estimation errors in the learned model. Driven by this observation, we argue that it is critical for a model-based method to understand when to trust the model and when to rely on model-free estimates, and how to act conservatively w.r.t. both. To this end, we derive an elegant and simple methodology called conservative Bayesian model-based value expansion for offline policy optimization (CBOP), that trades off model-free and model-based estimates during the policy evaluation step according to their epistemic uncertainties, and facilitates conservatism by taking a lower bound on the Bayesian posterior value estimate. On the standard D4RL continuous control tasks, we find that our method significantly outperforms previous model-based approaches: e.g., MOPO by $116.4$%, MOReL by $23.2$% and COMBO by $23.7$%. Further, CBOP achieves state-of-the-art performance on $11$ out of $18$ benchmark datasets while doing on par on the remaining datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2633197",
                        "name": "Jihwan Jeong"
                    },
                    {
                        "authorId": "2155610840",
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "authorId": "52225987",
                        "name": "Michael Gimelfarb"
                    },
                    {
                        "authorId": "2154858261",
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "authorId": "1770243",
                        "name": "B. Abdulhai"
                    },
                    {
                        "authorId": "1732536",
                        "name": "S. Sanner"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[32] use an actor-critic method trained via rollouts in the model alongside the data collected to find a policy, and Chua et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3aa3b2aa47f4eeb9373aef0e6b2b1a545d0e94b8",
                "externalIds": {
                    "ArXiv": "2210.04642",
                    "DBLP": "journals/corr/abs-2210-04642",
                    "DOI": "10.48550/arXiv.2210.04642",
                    "CorpusId": 252780176
                },
                "corpusId": 252780176,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3aa3b2aa47f4eeb9373aef0e6b2b1a545d0e94b8",
                "title": "Exploration via Planning for Information about the Optimal Trajectory",
                "abstract": "Many potential applications of reinforcement learning (RL) are stymied by the large numbers of samples required to learn an effective policy. This is especially true when applying RL to real-world control tasks, e.g. in the sciences or robotics, where executing a policy in the environment is costly. In popular RL algorithms, agents typically explore either by adding stochasticity to a reward-maximizing policy or by attempting to gather maximal information about environment dynamics without taking the given task into account. In this work, we develop a method that allows us to plan for exploration while taking both the task and the current knowledge about the dynamics into account. The key insight to our approach is to plan an action sequence that maximizes the expected information gain about the optimal trajectory for the task at hand. We demonstrate that our method learns strong policies with 2x fewer samples than strong exploration baselines and 200x fewer samples than model free methods on a diverse set of low-to-medium dimensional control tasks in both the open-loop and closed-loop control settings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49061544",
                        "name": "Viraj Mehta"
                    },
                    {
                        "authorId": "2306889",
                        "name": "I. Char"
                    },
                    {
                        "authorId": "94396899",
                        "name": "J. Abbate"
                    },
                    {
                        "authorId": "1581200399",
                        "name": "R. Conlin"
                    },
                    {
                        "authorId": "39062485",
                        "name": "M. Boyer"
                    },
                    {
                        "authorId": "2490652",
                        "name": "Stefano Ermon"
                    },
                    {
                        "authorId": "1753432",
                        "name": "J. Schneider"
                    },
                    {
                        "authorId": "2934259",
                        "name": "W. Neiswanger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent work indicates that state-of-the-art models suffer from sever policy drift after a few predictions [2, 8, 10], and CostNet is no exception.",
                "In [10], the authors analyze previous methods and their capability to generalize well for longer time horizons."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c876bb6c6724502dc34b530eb97d933b43e296e0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01805",
                    "ArXiv": "2210.01805",
                    "DOI": "10.1007/978-3-030-63799-6_7",
                    "CorpusId": 228077289
                },
                "corpusId": 228077289,
                "publicationVenue": {
                    "id": "f3bfa077-18b6-4ca9-a559-a809b86a837f",
                    "name": "SGAI Conferences",
                    "type": "conference",
                    "alternate_names": [
                        "SGAI",
                        "SGAI Conf"
                    ],
                    "url": "http://www.bcs-sgai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c876bb6c6724502dc34b530eb97d933b43e296e0",
                "title": "CostNet: An End-to-End Framework for Goal-Directed Reinforcement Learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "33999307",
                        "name": "Per-Arne Andersen"
                    },
                    {
                        "authorId": "1833672",
                        "name": "M. G. Olsen"
                    },
                    {
                        "authorId": "2493161",
                        "name": "Ole-Christoffer Granmo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based reinforcement learning (MBRL) leverages a learned dynamic model of the environment to plan a sequence of actions in advance which augment the data (Sutton, 1991; Janner et al., 2019; Pan et al., 2020; Mu et al., 2020; Peng et al., 2021) or obtain the desired behavior through planning (Chua et al.",
                "\u2026learning (MBRL) leverages a learned dynamic model of the environment to plan a sequence of actions in advance which augment the data (Sutton, 1991; Janner et al., 2019; Pan et al., 2020; Mu et al., 2020; Peng et al., 2021) or obtain the desired behavior through planning (Chua et al., 2018; Hafner\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8cb359a03b499319c05eb7d36726341579dbe56f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-00498",
                    "ArXiv": "2210.00498",
                    "DOI": "10.48550/arXiv.2210.00498",
                    "CorpusId": 252683782
                },
                "corpusId": 252683782,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8cb359a03b499319c05eb7d36726341579dbe56f",
                "title": "EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model",
                "abstract": "Unsupervised reinforcement learning (URL) poses a promising paradigm to learn useful behaviors in a task-agnostic environment without the guidance of extrinsic rewards to facilitate the fast adaptation of various downstream tasks. Previous works focused on the pre-training in a model-free manner while lacking the study of transition dynamics modeling that leaves a large space for the improvement of sample efficiency in downstream tasks. To this end, we propose an Efficient Unsupervised Reinforcement Learning Framework with Multi-choice Dynamics model (EUCLID), which introduces a novel model-fused paradigm to jointly pre-train the dynamics model and unsupervised exploration policy in the pre-training phase, thus better leveraging the environmental samples and improving the downstream task sampling efficiency. However, constructing a generalizable model which captures the local dynamics under different behaviors remains a challenging problem. We introduce the multi-choice dynamics model that covers different local dynamics under different behaviors concurrently, which uses different heads to learn the state transition under different behaviors during unsupervised pre-training and selects the most appropriate head for prediction in the downstream task. Experimental results in the manipulation and locomotion domains demonstrate that EUCLID achieves state-of-the-art performance with high sample efficiency, basically solving the state-based URLB benchmark and reaching a mean normalized score of 104.0$\\pm$1.2$\\%$ in downstream tasks with 100k fine-tuning steps, which is equivalent to DDPG's performance at 2M interactive steps with 20x more data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112499487",
                        "name": "Yifu Yuan"
                    },
                    {
                        "authorId": "40513470",
                        "name": "Jianye Hao"
                    },
                    {
                        "authorId": "2060411245",
                        "name": "Fei Ni"
                    },
                    {
                        "authorId": "1675357512",
                        "name": "Yao Mu"
                    },
                    {
                        "authorId": "1752775197",
                        "name": "Yan Zheng"
                    },
                    {
                        "authorId": "1776850",
                        "name": "Yujing Hu"
                    },
                    {
                        "authorId": "2124810107",
                        "name": "Jinyi Liu"
                    },
                    {
                        "authorId": "152829349",
                        "name": "Yingfeng Chen"
                    },
                    {
                        "authorId": "3120655",
                        "name": "Changjie Fan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In state-based RL, model-based algorithms [2, 1, 28, 16] which learn a dynamics model from the pre-recorded dataset and augment the dataset with generated state transitions have emerged as a promising paradigm."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d3bc75b2fb8bebef156d425ea46cab5a30904b7c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-15256",
                    "ArXiv": "2209.15256",
                    "DOI": "10.48550/arXiv.2209.15256",
                    "CorpusId": 252668538
                },
                "corpusId": 252668538,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d3bc75b2fb8bebef156d425ea46cab5a30904b7c",
                "title": "S2P: State-conditioned Image Synthesis for Data Augmentation in Offline Reinforcement Learning",
                "abstract": "Offline reinforcement learning (Offline RL) suffers from the innate distributional shift as it cannot interact with the physical environment during training. To alleviate such limitation, state-based offline RL leverages a learned dynamics model from the logged experience and augments the predicted state transition to extend the data distribution. For exploiting such benefit also on the image-based RL, we firstly propose a generative model, S2P (State2Pixel), which synthesizes the raw pixel of the agent from its corresponding state. It enables bridging the gap between the state and the image domain in RL algorithms, and virtually exploring unseen image distribution via model-based transition in the state space. Through experiments, we confirm that our S2P-based image synthesis not only improves the image-based offline RL performance but also shows powerful generalization capability on unseen tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2161465903",
                        "name": "Daesol Cho"
                    },
                    {
                        "authorId": "6981689",
                        "name": "D. Shim"
                    },
                    {
                        "authorId": "2161495857",
                        "name": "H. J. Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our learning algorithm is based on MBPO (Janner et al., 2019) using Soft Actor-Critic (Haarnoja et al., 2018a) as the underlying model-free learning algorithm.",
                "To do this, we build on top of model-based policy optimization (MBPO) (Janner et al., 2019).",
                "As mentioned in Section 6, our tool is built on top of MBPO (Janner et al., 2019) using SAC (Haarnoja et al., 2018a) as the underlying learning algorithm.",
                "Our learning algorithm is based on MBPO (Janner et al., 2019) using Soft Actor-Critic (Haarnoja et al.",
                "(Janner et al. (2019), Lemma B.3) Let the expected KL-divergence between two transition distributions be bounded by maxt Ex\u223cpt1(x)DKL(p1(x\n\u2032u | x)\u2016p2(x\u2032,u | x)) \u2264 m and maxxDTV (\u03c01(u | x)\u2016\u03c02(u | x)) < \u03c0 ."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f552ea23e70adaa0b160dd02efb2c46d5224a3b0",
                "externalIds": {
                    "ArXiv": "2209.14148",
                    "DBLP": "journals/corr/abs-2209-14148",
                    "DOI": "10.48550/arXiv.2209.14148",
                    "CorpusId": 252568019
                },
                "corpusId": 252568019,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f552ea23e70adaa0b160dd02efb2c46d5224a3b0",
                "title": "Guiding Safe Exploration with Weakest Preconditions",
                "abstract": "In reinforcement learning for safety-critical settings, it is often desirable for the agent to obey safety constraints at all points in time, including during training. We present a novel neurosymbolic approach called SPICE to solve this safe exploration problem. SPICE uses an online shielding layer based on symbolic weakest preconditions to achieve a more precise safety analysis than existing tools without unduly impacting the training process. We evaluate the approach on a suite of continuous control benchmarks and show that it can achieve comparable performance to existing safe learning techniques while incurring fewer safety violations. Additionally, we present theoretical results showing that SPICE converges to the optimal safe policy under reasonable assumptions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064997780",
                        "name": "Greg Anderson"
                    },
                    {
                        "authorId": "35865989",
                        "name": "Swarat Chaudhuri"
                    },
                    {
                        "authorId": "1714075",
                        "name": "I\u015f\u0131l Dillig"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since the method proposed in this article can be used in conjunction with the system identification methods, our method in principle can be used as a model-based policy optimization method, which is similar in spirit to the modelbased RL approaches [7]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0e1ef6b2dada2018b79cb05bbe63e38c88f0c2d1",
                "externalIds": {
                    "DBLP": "conf/amcc/ShinPCA23",
                    "ArXiv": "2209.13050",
                    "DOI": "10.23919/ACC55779.2023.10156553",
                    "CorpusId": 252544954
                },
                "corpusId": 252544954,
                "publicationVenue": {
                    "id": "fe4d09f8-d278-4bfb-b73a-1a6a0e22f6a3",
                    "name": "American Control Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Adv Comput Control",
                        "ACC",
                        "Advances in Computing and Communications",
                        "Adv Comput Commun",
                        "Am Control Conf",
                        "Advances in Computer and Communication",
                        "International Conference on Advanced Computer Control"
                    ],
                    "issn": "2767-2875",
                    "url": "http://a2c2.org/conferences/american-control-conferences",
                    "alternate_urls": [
                        "http://www.acc-rajagiri.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0e1ef6b2dada2018b79cb05bbe63e38c88f0c2d1",
                "title": "Constrained Policy Optimization for Stochastic Optimal Control under Nonstationary Uncertainties*",
                "abstract": "This article presents a constrained policy optimization approach for the optimal control of systems under nonstationary uncertainties. We introduce an assumption that we call Markov embeddability that allows us to cast the stochastic optimal control problem as a policy optimization problem over the augmented state space. Then, the infinite-dimensional policy optimization problem is approximated as a finite-dimensional nonlinear program by applying function approximation, deterministic sampling, and temporal truncation. The approximated problem is solved by using automatic differentiation and condensed-space interior-point methods. We formulate several conceptual and practical open questions regarding the asymptotic exactness of the approximation and the solution strategies for the approximated problem. As proof of concept, we present numerical examples demonstrating the performance of the proposed method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2107758619",
                        "name": "S. Shin"
                    },
                    {
                        "authorId": "95074307",
                        "name": "F. Pacaud"
                    },
                    {
                        "authorId": "2186182875",
                        "name": "Emil Contantinescu"
                    },
                    {
                        "authorId": "1760979",
                        "name": "M. Anitescu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We then analyze the monotonic improvement for the joint policy under the world model based on Janner et al. (2019).",
                "See (Janner et al., 2019) (Lemma B.2).",
                "See (Janner et al., 2019) (Lemma B.1)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "71f53032132f0d8a7366b58d7ee9a82e9f78128e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-12713",
                    "ArXiv": "2209.12713",
                    "DOI": "10.48550/arXiv.2209.12713",
                    "CorpusId": 252531267
                },
                "corpusId": 252531267,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/71f53032132f0d8a7366b58d7ee9a82e9f78128e",
                "title": "Multi-Agent Sequential Decision-Making via Communication",
                "abstract": "Communication helps agents to obtain information about others so that better coordinated behavior can be learned. Some existing work communicates predicted future trajectory with others, hoping to get clues about what others would do for better coordination. However, circular dependencies sometimes can occur when agents are treated synchronously so it is hard to coordinate decision-making. In this paper, we propose a novel communication scheme, Sequential Communication (SeqComm). SeqComm treats agents asynchronously (the upper-level agents make decisions before the lower-level ones) and has two communication phases. In negotiation phase, agents determine the priority of decision-making by communicating hidden states of observations and comparing the value of intention, which is obtained by modeling the environment dynamics. In launching phase, the upper-level agents take the lead in making decisions and communicate their actions with the lower-level agents. Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, we show that SeqComm outperforms existing methods in various multi-agent cooperative tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1742314398",
                        "name": "Ziluo Ding"
                    },
                    {
                        "authorId": "2130705195",
                        "name": "Kefan Su"
                    },
                    {
                        "authorId": "153634924",
                        "name": "Wei-xing Hong"
                    },
                    {
                        "authorId": "2112262518",
                        "name": "Liwen Zhu"
                    },
                    {
                        "authorId": "2137455903",
                        "name": "Tiejun Huang"
                    },
                    {
                        "authorId": "2265693",
                        "name": "Zongqing Lu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Dynamics models combined with powerful search methods have led to impressive results on a wide variety of tasks such as Atari (Schrittwieser et al., 2020) and continuous control (Hafner et al., 2019a; Janner et al., 2019; Sikchi et al., 2021; Lowrey et al., 2018)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "57901846a7aed271e90775dbbc1411ac0c077928",
                "externalIds": {
                    "DBLP": "conf/icml/RajeswarMVPDCL23",
                    "ArXiv": "2209.12016",
                    "CorpusId": 258887708
                },
                "corpusId": 258887708,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/57901846a7aed271e90775dbbc1411ac0c077928",
                "title": "Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels",
                "abstract": "Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, as shown in the Unsupervised RL Benchmark (URLB; Laskin et al. 2021), whether current unsupervised strategies can improve generalization capabilities is still unclear, especially in visual control settings. In this work, we study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains 93.59% overall normalized performance, surpassing previous baselines by a staggering margin. The approach is empirically evaluated through a large-scale empirical study, which we use to validate our design choices and analyze our models. We also show robust performance on the Real-Word RL benchmark, hinting at resiliency to environment perturbations during adaptation. Project website: https://masteringurlb.github.io/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1818842",
                        "name": "Sai Rajeswar"
                    },
                    {
                        "authorId": "2098445287",
                        "name": "Pietro Mazzaglia"
                    },
                    {
                        "authorId": "2413244",
                        "name": "Tim Verbelen"
                    },
                    {
                        "authorId": "2064234172",
                        "name": "Alexandre Pich'e"
                    },
                    {
                        "authorId": "1733741",
                        "name": "B. Dhoedt"
                    },
                    {
                        "authorId": "2058336670",
                        "name": "Aaron C. Courville"
                    },
                    {
                        "authorId": "8651990",
                        "name": "Alexandre Lacoste"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There has been recent work focused on unifying modelfree and model-based approaches (Janner et al., 2019; Du and Narasimhan, 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e710fe668771e1cea4b0d172e34f706fa377f231",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-11123",
                    "ArXiv": "2209.11123",
                    "MAG": "3154766860",
                    "DOI": "10.1016/j.ifacol.2020.12.126",
                    "CorpusId": 219161658
                },
                "corpusId": 219161658,
                "publicationVenue": {
                    "id": "af98f1eb-affb-4b55-b8ff-1964b29cf894",
                    "name": "IFAC-PapersOnLine",
                    "type": "journal",
                    "issn": "2405-8963",
                    "url": "https://www.journals.elsevier.com/ifac-papersonline/",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/24058963",
                        "https://www.journals.elsevier.com/ifac-papersonline"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e710fe668771e1cea4b0d172e34f706fa377f231",
                "title": "Modern Machine Learning Tools for Monitoring and Control of Industrial Processes: A Survey",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1766686",
                        "name": "R. B. Gopaluni"
                    },
                    {
                        "authorId": "7854890",
                        "name": "Aditya Tulsyan"
                    },
                    {
                        "authorId": "48962186",
                        "name": "B. Chachuat"
                    },
                    {
                        "authorId": "144466701",
                        "name": "Biao Huang"
                    },
                    {
                        "authorId": "2108613431",
                        "name": "J. M. Lee"
                    },
                    {
                        "authorId": "2064582162",
                        "name": "Faraz Amjad"
                    },
                    {
                        "authorId": "2662707",
                        "name": "S. Damarla"
                    },
                    {
                        "authorId": "2110932805",
                        "name": "Jong Woo Kim"
                    },
                    {
                        "authorId": "36835885",
                        "name": "Nathan P. Lawrence"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0d9786c223866337b80e126f3a145c5a8a378473",
                "externalIds": {
                    "DBLP": "conf/iclr/GhugareBELS23",
                    "ArXiv": "2209.08466",
                    "DOI": "10.48550/arXiv.2209.08466",
                    "CorpusId": 252367338
                },
                "corpusId": 252367338,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0d9786c223866337b80e126f3a145c5a8a378473",
                "title": "Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective",
                "abstract": "While reinforcement learning (RL) methods that learn an internal model of the environment have the potential to be more sample efficient than their model-free counterparts, learning to model raw observations from high dimensional sensors can be challenging. Prior work has addressed this challenge by learning low-dimensional representation of observations through auxiliary objectives, such as reconstruction or value prediction. However, the alignment between these auxiliary objectives and the RL objective is often unclear. In this work, we propose a single objective which jointly optimizes a latent-space model and policy to achieve high returns while remaining self-consistent. This objective is a lower bound on expected returns. Unlike prior bounds for model-based RL on policy exploration or model guarantees, our bound is directly on the overall RL objective. We demonstrate that the resulting algorithm matches or improves the sample-efficiency of the best prior model-based and model-free RL methods. While sample efficient methods typically are computationally demanding, our method attains the performance of SAC in about 50% less wall-clock time.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2185406398",
                        "name": "Raj Ghugare"
                    },
                    {
                        "authorId": "51113848",
                        "name": "Homanga Bharadhwaj"
                    },
                    {
                        "authorId": "8140754",
                        "name": "Benjamin Eysenbach"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "145124475",
                        "name": "R. Salakhutdinov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "4.2 Practical Algorithm\nAlgorithm 1 Practical CDPO Algorithm Input: Prior \u03d5, model-based policy optimization solver MBPO(\u03c0, f,J ).",
                "For instance, Dyna agents [61, 20, 17] optimize policies using model-free learners with model-generated data.",
                "1: for iteration t = 1, ..., T do 2: qt \u2190 MBPO(\u00b7, f\u0302LSt , (4.1)) 3: Sample N models {ft,n}Nn=1 4: \u03c0t \u2190 MBPO(qt, {ft,n}Nn=1, (4.2)) 5: Execute \u03c0t in the real MDP 6: UpdateHt+1 = Ht \u222a {sh,t, ah,t, sh+1,t}h 7: Update f\u0302LSt+1 and \u03d5 8: end for 9: return policy \u03c0T\nThe pseudocode of CDPO is in Alg.",
                "It will also be interesting to explore different choices of the MBPO solvers, which we would like to leave as future work.",
                "We also examine a broader range of MBRL algorithms, including MBPO [20], SLBO [35], and ME-TRPO [30].",
                "The model-based solver MBPO(\u03c0, f,J ) outputs the policy (qt or \u03c0t) that optimizes the objective J with access to model f .",
                "Ablation on different choices of MBPO solver (Dyna and POPLIN-P [63]) shows the generalizability of CDPO."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "00d4aea3e13b7d4ad043d7081be31ffb2859ad99",
                "externalIds": {
                    "ArXiv": "2209.07676",
                    "DBLP": "journals/corr/abs-2209-07676",
                    "DOI": "10.48550/arXiv.2209.07676",
                    "CorpusId": 252355132
                },
                "corpusId": 252355132,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/00d4aea3e13b7d4ad043d7081be31ffb2859ad99",
                "title": "Conservative Dual Policy Optimization for Efficient Model-Based Reinforcement Learning",
                "abstract": "Provably efficient Model-Based Reinforcement Learning (MBRL) based on optimism or posterior sampling (PSRL) is ensured to attain the global optimality asymptotically by introducing the complexity measure of the model. However, the complexity might grow exponentially for the simplest nonlinear models, where global convergence is impossible within finite iterations. When the model suffers a large generalization error, which is quantitatively measured by the model complexity, the uncertainty can be large. The sampled model that current policy is greedily optimized upon will thus be unsettled, resulting in aggressive policy updates and over-exploration. In this work, we propose Conservative Dual Policy Optimization (CDPO) that involves a Referential Update and a Conservative Update. The policy is first optimized under a reference model, which imitates the mechanism of PSRL while offering more stability. A conservative range of randomness is guaranteed by maximizing the expectation of model value. Without harmful sampling procedures, CDPO can still achieve the same regret as PSRL. More importantly, CDPO enjoys monotonic policy improvement and global optimality simultaneously. Empirical results also validate the exploration efficiency of CDPO.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155924041",
                        "name": "Shen Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "*Equal contribution model for the MuJoCo Humanoid task [11]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d69ef3f2bbb3077db630ec148db18c0866bd7b4a",
                "externalIds": {
                    "ArXiv": "2209.07682",
                    "DBLP": "journals/corr/abs-2209-07682",
                    "DOI": "10.48550/arXiv.2209.07682",
                    "CorpusId": 252355020
                },
                "corpusId": 252355020,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d69ef3f2bbb3077db630ec148db18c0866bd7b4a",
                "title": "Masked Imitation Learning: Discovering Environment-Invariant Modalities in Multimodal Demonstrations",
                "abstract": "Multimodal demonstrations provide robots with an abundance of information to make sense of the world. However, such abundance may not always lead to good performance when it comes to learning sensorimotor control policies from human demonstrations. Extraneous data modalities can lead to state over-specification, where the state contains modalities that are not only useless for decision-making but also can change data distribution across environments. State over-specification leads to issues such as the learned policy not generalizing outside of the training data distribution. In this work, we propose Masked Imitation Learning (MIL) to address state over-specification by selectively using informative modalities. Specifically, we design a masked policy network with a binary mask to block certain modalities. We develop a bi-level optimization algorithm that learns this mask to accurately filter over-specified modalities. We demonstrate empirically that MIL outperforms baseline algorithms in simulated domains including MuJoCo and a robot arm environment using the Robomimic dataset, and effectively recovers the environment-invariant modalities on a multimodal dataset collected on a real robot. Our project website presents supplemental details and videos of our results at: https://tinyurl.com/masked-il",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2049032558",
                        "name": "Yilun Hao"
                    },
                    {
                        "authorId": "2144714403",
                        "name": "Ruinan Wang"
                    },
                    {
                        "authorId": "3451430",
                        "name": "Zhangjie Cao"
                    },
                    {
                        "authorId": null,
                        "name": "Zihan Wang"
                    },
                    {
                        "authorId": "7332443",
                        "name": "Yuchen Cui"
                    },
                    {
                        "authorId": "1779671",
                        "name": "Dorsa Sadigh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, MBPO [12] proposes the branched rollouts scheme with a gradually growing branch length to truncate imaginary model rollouts, avoiding the participation of unreliable fake samples in policy optimization.",
                "1 Model Learning Like MBPO [12], our dynamics model is an ensemble neural network that takes state-action pair as input and outputs Gaussian distribution of the next state and reward.",
                "For model-based methods, we compare MPPVE with MBPO [12], the most representative model-based method so far, and BMPO [15], which proposes a bidirectional dynamics model to generate model data with less compounding error than MBPO.",
                "We also utilize model rollouts to generate fake transitions, as proposed by MBPO [12].",
                "Moreover, MBPO [12] builds on SAC [11], which is an off-policy RL algorithm, and updates the policy with a mixture of the data from the real environment and imaginary branched rollouts."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c56bc32f12c991f0e1b7d2999a1ec4a842c25709",
                "externalIds": {
                    "ArXiv": "2209.05530",
                    "DBLP": "journals/corr/abs-2209-05530",
                    "DOI": "10.48550/arXiv.2209.05530",
                    "CorpusId": 252212149
                },
                "corpusId": 252212149,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c56bc32f12c991f0e1b7d2999a1ec4a842c25709",
                "title": "Model-based Reinforcement Learning with Multi-step Plan Value Estimation",
                "abstract": "A promising way to improve the sample efficiency of reinforcement learning is model-based methods, in which many explorations and evaluations can happen in the learned models to save real-world samples. However, when the learned model has a non-negligible model error, sequential steps in the model are hard to be accurately evaluated, limiting the model's utilization. This paper proposes to alleviate this issue by introducing multi-step plans to replace multi-step actions for model-based RL. We employ the multi-step plan value estimation, which evaluates the expected discounted return after executing a sequence of action plans at a given state, and updates the policy by directly computing the multi-step policy gradient via plan value estimation. The new model-based reinforcement learning algorithm MPPVE (Model-based Planning Policy Learning with Multi-step Plan Value Estimation) shows a better utilization of the learned model and achieves a better sample efficiency than state-of-the-art model-based RL approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2175115642",
                        "name": "Hao-Chu Lin"
                    },
                    {
                        "authorId": "120738487",
                        "name": "Yihao Sun"
                    },
                    {
                        "authorId": "2000940184",
                        "name": "Jiajin Zhang"
                    },
                    {
                        "authorId": "144705629",
                        "name": "Yang Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some model-based methods estimate the model\u2019s uncertainty and penalize the actions whose consequences are highly uncertain (Janner et al., 2019; Kidambi et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fbb15aa7303586d25dc73f84c23f9b5447b0c06b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-03993",
                    "ArXiv": "2209.03993",
                    "DOI": "10.48550/arXiv.2209.03993",
                    "CorpusId": 252185261
                },
                "corpusId": 252185261,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fbb15aa7303586d25dc73f84c23f9b5447b0c06b",
                "title": "Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline RL",
                "abstract": "Recent works have shown that tackling offline reinforcement learning (RL) with a conditional policy produces promising results. The Decision Transformer (DT) combines the conditional policy approach and a transformer architecture, showing competitive performance against several benchmarks. However, DT lacks stitching ability -- one of the critical abilities for offline RL to learn the optimal policy from sub-optimal trajectories. This issue becomes particularly significant when the offline dataset only contains sub-optimal trajectories. On the other hand, the conventional RL approaches based on Dynamic Programming (such as Q-learning) do not have the same limitation; however, they suffer from unstable learning behaviours, especially when they rely on function approximation in an off-policy learning setting. In this paper, we propose the Q-learning Decision Transformer (QDT) to address the shortcomings of DT by leveraging the benefits of Dynamic Programming (Q-learning). It utilises the Dynamic Programming results to relabel the return-to-go in the training data to then train the DT with the relabelled data. Our approach efficiently exploits the benefits of these two approaches and compensates for each other's shortcomings to achieve better performance. We empirically show these in both simple toy environments and the more complex D4RL benchmark, showing competitive performance gains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2067637897",
                        "name": "Taku Yamagata"
                    },
                    {
                        "authorId": "2100092361",
                        "name": "Ahmed Khalil"
                    },
                    {
                        "authorId": "2126710957",
                        "name": "Ra\u00fal Santos-Rodr\u00edguez"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "10fee102a1d0c0ed95bfc4a9efa169e2cecf4a7c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-02010",
                    "ArXiv": "2209.02010",
                    "DOI": "10.48550/arXiv.2209.02010",
                    "CorpusId": 252090062
                },
                "corpusId": 252090062,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/10fee102a1d0c0ed95bfc4a9efa169e2cecf4a7c",
                "title": "On the Origins of Self-Modeling",
                "abstract": "Self-Modeling is the process by which an agent, such as an animal or machine, learns to create a predictive model of its own dynamics. Once captured, this self-model can then allow the agent to plan and evaluate various potential behaviors internally using the self-model, rather than using costly physical experimentation. Here, we quantify the benefits of such self-modeling against the complexity of the robot. We find a R2 =0.90 correlation between the number of degrees of freedom a robot has, and the added value of self-modeling as compared to a direct learning baseline. This result may help motivate self modeling in increasingly complex robotic systems, as well as shed light on the origins of self-modeling, and ultimately self-awareness, in animals and humans.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1420209643",
                        "name": "Robert Kwiatkowski"
                    },
                    {
                        "authorId": "2109379640",
                        "name": "Yuhang Hu"
                    },
                    {
                        "authorId": "8786274",
                        "name": "Boyuan Chen"
                    },
                    {
                        "authorId": "1747909",
                        "name": "Hod Lipson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One class trains \u201con-policy\u201d model-free algorithms virtually inside the environment model [Kurutach et al., 2018, Luo et al., 2019] while the other trains \u201coff-policy\u201d model-free algorithms virtually [Janner et al., 2019].",
                ", 2019] while the other trains \u201coff-policy\u201d model-free algorithms virtually [Janner et al., 2019]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0b0c3ba6d814bc7a15dbc1e37b0fd46148f7eeeb",
                "externalIds": {
                    "ArXiv": "2209.01693",
                    "DBLP": "journals/corr/abs-2209-01693",
                    "DOI": "10.48550/arXiv.2209.01693",
                    "CorpusId": 252089474
                },
                "corpusId": 252089474,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0b0c3ba6d814bc7a15dbc1e37b0fd46148f7eeeb",
                "title": "Variational Inference for Model-Free and Model-Based Reinforcement Learning",
                "abstract": "Variational inference (VI) is a specific type of approximate Bayesian inference that approximates an intractable posterior distribution with a tractable one. VI casts the inference problem as an optimization problem, more specifically, the goal is to maximize a lower bound of the logarithm of the marginal likelihood with respect to the parameters of the approximate posterior. Reinforcement learning (RL) on the other hand deals with autonomous agents and how to make them act optimally such as to maximize some notion of expected future cumulative reward. In the non-sequential setting where agents' actions do not have an impact on future states of the environment, RL is covered by contextual bandits and Bayesian optimization. In a proper sequential scenario, however, where agents' actions affect future states, instantaneous rewards need to be carefully traded off against potential long-term rewards. This manuscript shows how the apparently different subjects of VI and RL are linked in two fundamental ways. First, the optimization objective of RL to maximize future cumulative rewards can be recovered via a VI objective under a soft policy constraint in both the non-sequential and the sequential setting. This policy constraint is not just merely artificial but has proven as a useful regularizer in many RL tasks yielding significant improvements in agent performance. And second, in model-based RL where agents aim to learn about the environment they are operating in, the model-learning part can be naturally phrased as an inference problem over the process that governs environment dynamics. We are going to distinguish between two scenarios for the latter: VI when environment states are fully observable by the agent and VI when they are only partially observable through an observation distribution.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2505365",
                        "name": "Felix Leibfried"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fdde70ccf8e0bf9efa257e3fffb83282e471d5ae",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-14501",
                    "ArXiv": "2208.14501",
                    "DOI": "10.48550/arXiv.2208.14501",
                    "CorpusId": 250361874
                },
                "corpusId": 250361874,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fdde70ccf8e0bf9efa257e3fffb83282e471d5ae",
                "title": "Model-Based Reinforcement Learning with SINDy",
                "abstract": "We draw on the latest advancements in the physics community to propose a novel method for discovering the governing non-linear dynamics of physical systems in reinforcement learning (RL). We establish that this method is capable of discovering the underlying dynamics using significantly fewer trajectories (as little as one rollout with $\\leq 30$ time steps) than state of the art model learning algorithms. Further, the technique learns a model that is accurate enough to induce near-optimal policies given significantly fewer trajectories than those required by model-free algorithms. It brings the benefits of model-based RL without requiring a model to be developed in advance, for systems that have physics-based dynamics. To establish the validity and applicability of this algorithm, we conduct experiments on four classic control tasks. We found that an optimal policy trained on the discovered dynamics of the underlying system can generalize well. Further, the learned policy performs well when deployed on the actual physical system, thus bridging the model to real system gap. We further compare our method to state-of-the-art model-based and model-free approaches, and show that our method requires fewer trajectories sampled on the true physical system compared other methods. Additionally, we explored approximate dynamics models and found that they also can perform well.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2175361122",
                        "name": "Rushiv Arora"
                    },
                    {
                        "authorId": "145471664",
                        "name": "Bruno C. da Silva"
                    },
                    {
                        "authorId": "2070704525",
                        "name": "E. Moss"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "caafa684d204fde798f919f575ee00cc91a2a438",
                "externalIds": {
                    "DOI": "10.3390/math10173059",
                    "CorpusId": 251861139
                },
                "corpusId": 251861139,
                "publicationVenue": {
                    "id": "6175efe8-6f8e-4cbe-8cee-d154f4e78627",
                    "name": "Mathematics",
                    "issn": "2227-7390",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-283014",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-283014",
                        "https://www.mdpi.com/journal/mathematics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/caafa684d204fde798f919f575ee00cc91a2a438",
                "title": "MIRA: Model-Based Imagined Rollouts Augmentation for Non-Stationarity in Multi-Agent Systems",
                "abstract": "One of the challenges in multi-agent systems comes from the environmental non-stationarity that policies of all agents are evolving individually over time. Many existing multi-agent reinforcement learning (MARL) methods have been proposed to address this problem. However, these methods rely on a large amount of training data and some of them require agents to intensely communicate, which is often impractical in real-world applications. To better tackle the non-stationarity problem, this article combines model-based reinforcement learning (MBRL) and meta-learning and proposes a method called Model-based Imagined Rollouts Augmentation (MIRA). Based on an environment dynamics model, distributed agents can independently perform multi-agent rollouts with opponent models during exploitation and learn to infer the environmental non-stationarity as a latent variable using the rollouts. Based on the world model and latent-variable inference module, we perform multi-agent soft actor-critic implementation for centralized training and decentralized decision making. Empirical results on the Multi-agent Particle Environment (MPE) have proved that the algorithm has a very considerable improvement in sample efficiency as well as better convergent rewards than state-of-the-art MARL methods, including COMA, MAAC, MADDPG, and VDN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2141375220",
                        "name": "Haotian Xu"
                    },
                    {
                        "authorId": "2170409542",
                        "name": "Qinhe Fang"
                    },
                    {
                        "authorId": "2110129886",
                        "name": "Cong Hu"
                    },
                    {
                        "authorId": "1581498565",
                        "name": "Yue Hu"
                    },
                    {
                        "authorId": "39660976",
                        "name": "Quanjun Yin"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", model-based policy optimization (MBPO) [18] and masked model-based actor-critic (M2AC) [19], where the agents are trained with samples drawn from the approximated environment model.",
                "CRPM can directly serve as approximated environment model as in modern model-based reinforcement learning algorithms, e.g., model-based policy optimization (MBPO) [18] and masked model-based actor-critic (M2AC) [19], where the agents are trained with samples drawn from the approximated environment model.",
                "For example, model-based RL algorithms generate extra experiences by interacting with a learned model [18], [19]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "402f0232ed92ae6c3384767064352306db471db8",
                "externalIds": {
                    "DBLP": "conf/case/LiJ22",
                    "DOI": "10.1109/CASE49997.2022.9926635",
                    "CorpusId": 253186472
                },
                "corpusId": 253186472,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/402f0232ed92ae6c3384767064352306db471db8",
                "title": "Bridging Scenarios in Reinforcement Learning with Continuously Generated Relaying Predictive Models",
                "abstract": "Transfer learning is an effective way to reduce expensive interactions with the physical environment in reinforcement learning (RL). Based on the correlation between scenarios, both the prior policy and historical experiences collected in the source domain may help to accelerate policy optimization in the target domain. However, without setting proper relaying scenarios, the discrepancy between domains may lead to sub-optimal policies or even negative transfer. In this paper, we firstly propose a continuously generated relaying predictive model (CRPM), which autonomously bridges the source domain and target domain with a series of gradually modi ed relaying scenarios. Then, we experimentally show that CRPM effectively reduces interactions required for policy optimization in the target domain. Besides, we combine CRPM with model-based RL, which further improves the performance. The CRPM also helps to improve the classical model-free algorithm by considering it as a particular case of transfer learning in the same domain. Experimental results show that CRPM helps to avoid sub-optimal policies and outperforms other algorithms in both the source and target scenarios.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115469157",
                        "name": "Kuo Li"
                    },
                    {
                        "authorId": "46355830",
                        "name": "Qing-Shan Jia"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MBPO [13] generates truncated model rollouts branched from real states to cripple the influence of model error and provides the condition for the return improvement in the true dynamics.",
                "According to the definition of the return in terms of the occupancy measure and the total variation distance, the return\ndiscrepancy bound can be derived as:\n|\u03b7[\u03c0]\u2212 \u03b7[\u03c0e]| = | \u2211 s,a kb\u2211 t=0 \u03b3tr(s, a)(pt(s, a)\u2212 pet (s, a))|\n\u2264 2rmax \u2211 s,a kb\u2211 t=0 \u03b3t 1 2 |pt(s, a)\u2212 pet (s, a)|\n\u2264 2rmax kb\u2211 t=0 \u03b3tDTV (pt(s, a)||pet (s, a)),\n(18) Next, according to Lemma B.1 in MBPO [13], we convert\njoint distribution to marginal distribution, thus we have:\nDTV (pt(s, a)||pet (s, a)) \u2264 DTV (pt(s)||pet (s))+ max t Es\u223cpt(s)[DTV (\u03c0t(a|s)||\u03c0et (a|s))]\n(19)\nThen, let \u03bet = DTV (pt(s)||pet (s)), and inspired by Lemma B.1 in BMPO [6], we have:\n\u03bet \u2264E(s\u2032,a)\u223cpt+1(s\u2032,a)[DTV (p(s|s\u2032, a)||pe(s|s\u2032, a))] +DTV (pt+1(s \u2032, a)||pet+1(s\u2032, a)) (20)\nHere, we make the assumption without loss of generality:\nm \u2265 max t E(s\u2032,a)\u223cpt(s\u2032,a)[DTV (p(s|s\u2032, a)||pe(s|s\u2032, a))]\n(21) After that, we can iteratively do the above decomposition to obtain:\n\u03bet \u2264 ( m + \u03c0) + \u03bet+1 \u2264 ( m + \u03c0)(kb \u2212 t) + \u03bekb \u2264 kb( m + \u03c0) (22)\nwhere \u03bekb = 0, because the real states sampled from D b hs are used as the starting states for the backward rollouts.",
                "Other hyperparameters not listed here are the same as those in MBPO [6].",
                "To evaluate the importance of BI, we compare the performance among three models: 1)\nMBPO that optimizes the policy on forward rollout samples via RL algorithm (Baseline); 2) BMPO that adds the backward rollout samples to the baseline model (Baseline+BR); 3) the model that employs backward imitation (Baseline+BI).",
                "The strategy of increasing rollout length linearly has shown the effectiveness in MBPO and BMPO.",
                "On the one hand, prior works [13], [6] have studied the discrepancy between the expected return in the actual environment and that in the branched rollouts, which is applicable for the analysis on the forward rollout return in BIFRL.",
                "To be specific, for model-based methods, we compare against MBPO [13] that is the backbone model of our method, and BMPO [6] that treats the samples from backward rollouts in the same way as those from forward rollouts.",
                "Besides, BIFRL is built on the Model-based Policy Optimization (MBPO) [13] algorithm that is one of stateof-the-art Dyna-style model-based methods and investigates forward truncated model rollouts.",
                "More qualitative results are shown in our supplementary materials, which provides the qualitative comparisons among BIFRL, BMPO, MBPO and SAC.",
                "1 in MBPO [13], we convert joint distribution to marginal distribution, thus we have:",
                "We observe that BMPO [6] ignores the variation distance between the forward policy and the backward policy (corresponding to \u03c0 in BIFRL), and consequently set kb = kf to obtain the tighter return discrepancy bound, compared to MBPO [13].",
                "In practice, we use kb = 23kf in most domains except Walker2D and Walker2D-NT where kb is set to the same as kf due to kf = 1 in the original MBPO paper."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "1d7ad567a22603cb5fb1130b9e8c155b4534e3e3",
                "externalIds": {
                    "ArXiv": "2208.02434",
                    "DBLP": "journals/corr/abs-2208-02434",
                    "DOI": "10.1109/IROS47612.2022.9981869",
                    "CorpusId": 251320296
                },
                "corpusId": 251320296,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1d7ad567a22603cb5fb1130b9e8c155b4534e3e3",
                "title": "Backward Imitation and Forward Reinforcement Learning via Bi-directional Model Rollouts",
                "abstract": "Traditional model-based reinforcement learning (RL) methods generate forward rollout traces using the learnt dynamics model to reduce interactions with the real environment. The recent model-based RL method considers the way to learn a backward model that specifies the conditional probability of the previous state given the previous action and the current state to additionally generate backward rollout trajectories. However, in this type of model-based method, the samples derived from backward rollouts and those from forward rollouts are simply aggregated together to optimize the policy via the model-free RL algorithm, which may decrease both the sample efficiency and the convergence rate. This is because such an approach ignores the fact that backward rollout traces are often generated starting from some high-value states and are certainly more instructive for the agent to improve the behavior. In this paper, we propose the backward imitation and forward reinforcement learning (BIFRL) framework where the agent treats backward rollout traces as expert demonstrations for the imitation of excellent behaviors, and then collects forward rollout transitions for policy reinforcement. Consequently, BIFRL empowers the agent to both reach to and explore from high-value states in a more efficient manner, and further reduces the real interactions, making it potentially more suitable for real-robot learning. Moreover, a value-regularized generative adversarial network is introduced to augment the valuable states which are infrequently received by the agent. Theoretically, we provide the condition where BIFRL is superior to the baseline methods. Experimentally, we demonstrate that BIFRL acquires the better sample efficiency and produces the competitive asymptotic performance on various MuJoCo locomotion tasks compared against state-of-the-art model-based methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2180617999",
                        "name": "Yuxin Pan"
                    },
                    {
                        "authorId": "144125597",
                        "name": "Fangzhen Lin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, Effective MBRL may be cumbersome because the ease of data generation must be weighed against the bias of model-generated data [40]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d42de385a611bc612a1a827d5832e47b8f6456c2",
                "externalIds": {
                    "PubMedCentral": "9416718",
                    "DBLP": "journals/sensors/OrdonezRVRF22",
                    "DOI": "10.3390/s22166301",
                    "CorpusId": 251781486,
                    "PubMed": "36016062"
                },
                "corpusId": 251781486,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d42de385a611bc612a1a827d5832e47b8f6456c2",
                "title": "Model-Based Reinforcement Learning with Automated Planning for Network Management",
                "abstract": "Reinforcement Learning (RL) comes with the promise of automating network management. However, due to its trial-and-error learning approach, model-based RL (MBRL) is not applicable in some network management scenarios. This paper explores the potential of using Automated Planning (AP) to achieve this MBRL in the functional areas of network management. In addition, a comparison of several integration strategies of AP and RL is depicted. We also describe an architecture that realizes a cognitive management control loop by combining AP and RL. Our experiments evaluate on a simulated environment evidence that the combination proposed improves model-free RL but demonstrates lower performance than Deep RL regarding the reward and convergence time metrics. Nonetheless, AP-based MBRL is useful when the prediction model needs to be understood and when the high computational complexity of Deep RL can not be used.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153804282",
                        "name": "Armando Ord\u00f3\u00f1ez"
                    },
                    {
                        "authorId": "2832428",
                        "name": "O. Rend\u00f3n"
                    },
                    {
                        "authorId": "1413967094",
                        "name": "W. F. Villota-Jacome"
                    },
                    {
                        "authorId": "1417528007",
                        "name": "Angela Rodriguez-Vivas"
                    },
                    {
                        "authorId": "2054661366",
                        "name": "N. L. D. Fonseca"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For modelbased methods, we choose MBPO (Janner et al., 2019), AMPO (Shen et al., 2020), and VaGraM (Voelcker et al., 2022).",
                "For Humanoid, we use the modified version introduced by MBPO (Janner et al., 2019).",
                "We conduct an experiment using a state-of-the-art modelbased RL method called MBPO (Janner et al., 2019) on four MuJoCo (Todorov et al.",
                "For better model usage, Janner et al. (2019) proved that short model rollouts could avoid the model error and improve the quality of model samples.",
                "Most of the model-based RL algorithms first use supervised learning techniques to learn a dynamics model based on the samples obtained from the real environment, and then use this learned dynamics model to generate massive samples to derive a policy (Luo et al., 2018; Janner et al., 2019).",
                "As introduced in MBPO (Janner et al., 2019), the rollout horizon should start at a short horizon and increase linearly with the interaction epoch.",
                "We conduct an experiment using a state-of-the-art modelbased RL method called MBPO (Janner et al., 2019) on four MuJoCo (Todorov et al., 2012) environments HalfCheetah, Hopper, Walker2d, and Ant.",
                "For modelbased methods, we choose MBPO (Janner et al., 2019), AMPO (Shen et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "23460bac616208c7ddea5664ee08f7adbc46855e",
                "externalIds": {
                    "ArXiv": "2207.12141",
                    "DBLP": "conf/icml/WangWJH23",
                    "DOI": "10.48550/arXiv.2207.12141",
                    "CorpusId": 251040796
                },
                "corpusId": 251040796,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/23460bac616208c7ddea5664ee08f7adbc46855e",
                "title": "Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy",
                "abstract": "Model-based reinforcement learning (RL) often achieves higher sample efficiency in practice than model-free RL by learning a dynamics model to generate samples for policy learning. Previous works learn a dynamics model that fits under the empirical state-action visitation distribution for all historical policies, i.e., the sample replay buffer. However, in this paper, we observe that fitting the dynamics model under the distribution for \\emph{all historical policies} does not necessarily benefit model prediction for the \\emph{current policy} since the policy in use is constantly evolving over time. The evolving policy during training will cause state-action visitation distribution shifts. We theoretically analyze how this distribution shift over historical policies affects the model learning and model rollouts. We then propose a novel dynamics model learning method, named \\textit{Policy-adapted Dynamics Model Learning (PDML)}. PDML dynamically adjusts the historical policy mixture distribution to ensure the learned model can continually adapt to the state-action visitation distribution of the evolving policy. Experiments on a range of continuous control environments in MuJoCo show that PDML achieves significant improvement in sample efficiency and higher asymptotic performance combined with the state-of-the-art model-based RL methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48630770",
                        "name": "Xiyao Wang"
                    },
                    {
                        "authorId": "2179109975",
                        "name": "Wichayaporn Wongkamjan"
                    },
                    {
                        "authorId": "40070055",
                        "name": "Furong Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", model-based RL [9, 10], which generates extra data through interacting with a learned model."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "daa15550dc6c6a327a39b83c42a10ac67c2ea8ef",
                "externalIds": {
                    "DOI": "10.23919/CCC55666.2022.9902595",
                    "CorpusId": 252850884
                },
                "corpusId": 252850884,
                "publicationVenue": {
                    "id": "23f8fe4c-6537-4027-a334-6a5863115984",
                    "name": "Cybersecurity and Cyberforensics Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Chin Control Conf",
                        "Computational Complexity Conference",
                        "CCC",
                        "Comput Complex Conf",
                        "Cybersecur Cyberforensics Conf",
                        "Conference on Computational Complexity",
                        "Computing Colombian Conference",
                        "Conf Comput Complex",
                        "Comput Colomb Conf",
                        "Chinese Control Conference"
                    ],
                    "url": "http://computationalcomplexity.org/"
                },
                "url": "https://www.semanticscholar.org/paper/daa15550dc6c6a327a39b83c42a10ac67c2ea8ef",
                "title": "Quantum Reinforcement Learning for Multi-Armed Bandits",
                "abstract": "This work focuses on the multi-armed bandits (MAB) problem and proposes a quantum reinforcement learning (RL) algorithm for action selection. Existing quantum RL algorithms generally assume that some prior information about the optimal action is known, and initial probability is set unequally. Our algorithm can be executed with equal initial probability on each action, and can greatly accelerate the learning process.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2185020629",
                        "name": "Yi-Pei Liu"
                    },
                    {
                        "authorId": "2115469157",
                        "name": "Kuo Li"
                    },
                    {
                        "authorId": "2187801247",
                        "name": "Xi Cao"
                    },
                    {
                        "authorId": "46355830",
                        "name": "Qing-Shan Jia"
                    },
                    {
                        "authorId": "2187673079",
                        "name": "Xu Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Dyna-style algorithms learn world models by interacting with the real environment, which is the most investigated category of model-based algorithms, and its typical algorithms include Deep Dyna-Q, SLBO, MBPO, LatCo and Dreamer,etc [3] [24] [25] [26] [27]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "948f51f46b3313d8de258915b96378d694a29f4d",
                "externalIds": {
                    "DBLP": "conf/ijcnn/MaXYL22",
                    "DOI": "10.1109/IJCNN55064.2022.9892381",
                    "CorpusId": 252625055
                },
                "corpusId": 252625055,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/948f51f46b3313d8de258915b96378d694a29f4d",
                "title": "MaxEnt Dreamer: Maximum Entropy Reinforcement Learning with World Model",
                "abstract": "Model-based reinforcement learning algorithms can alleviate the low sample efficiency problem compared with modelfree methods for control tasks. However, the learned policy's performance often lags behind the best model-free algorithms since its weak exploration ability. Existing model-based reinforcement learning algorithms learn policy by interacting with the learned world model and then use the learned policy to guide a new round of world model learning. Due to weak policy exploration ability, the learned world model has a large bias. As a result, it fails to learn the globally optimal policy on such a world model. This paper improves the learned world model by maximizing both the reward and the corresponding policy entropy in the framework of maximum entropy reinforcement learning. The effectiveness of applying the maximum entropy approach to model-based reinforcement learning is supported by the better performance of our algorithm on several complex mujoco and deepmind control suite tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2217673176",
                        "name": "Hongying Ma"
                    },
                    {
                        "authorId": "120552601",
                        "name": "Wuyang Xue"
                    },
                    {
                        "authorId": "1803937",
                        "name": "R. Ying"
                    },
                    {
                        "authorId": "1752791",
                        "name": "Peilin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "By learning a forward dynamics model to approximate the state transition dynamics of the environment, model-based reinforcement learning (MBRL) methods can achieve better sampling efficiency than model-free reinforcement learning\n(MFRL) methods.",
                "The learned dynamics model can be used as a simulator of MFRL [12], [13], provide priors for the algorithm [14], [15], or be utilized to predict future trajectory in an MPC fashion [16], [17]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "070881894f9bb65315fa4ccf4491f1cf63bd6226",
                "externalIds": {
                    "DBLP": "conf/rcar/ChenWYXLWX22",
                    "DOI": "10.1109/RCAR54675.2022.9872230",
                    "CorpusId": 252105562
                },
                "corpusId": 252105562,
                "publicationVenue": {
                    "id": "646f127e-53b1-40c6-8ed6-7a251df5de0b",
                    "name": "International Conference on Real-time Computing and Robotics",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Real-time Comput Robot",
                        "RCAR",
                        "IEEE International Conference on Real-time Computing and Robotics",
                        "Int Conf Real-time Comput Robot"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/070881894f9bb65315fa4ccf4491f1cf63bd6226",
                "title": "Fast Adaptation Dynamics Model for Robot\u2019s Damage Recovery",
                "abstract": "In the process of operating, robots will inevitably encounter damage due to external or internal factors, such as motors blockage. For the legged robot, when the motors of joints are failing, if other motors still act according to the original instructions, it will cause the robot to deviate from the predetermined trajectory, which is unacceptable for legged robots. Inspired by the fact that the model trained by supervised learning on the training set can be generalized to the testing set, our goal is to obtain a dynamic model that can be generalized to all kinds of motor damage situations. It can predict what state will be reached in the next step when an action is applied in the current state. With this dynamics model, we use the Monte Carlo particles to optimize the feasible actions in a model predictive control (MPC) fashion and achieve the expected goal (such as making the robot walk in a straight line). The comparison experiment adopt two meta-learning model and vanilla dynamics model approaches, the results show that the proposed method is superior to the three baselines, which proves the effectiveness of the proposed method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145370811",
                        "name": "Ci Chen"
                    },
                    {
                        "authorId": "153168083",
                        "name": "Dongqi Wang"
                    },
                    {
                        "authorId": "2062585",
                        "name": "Jiyu Yu"
                    },
                    {
                        "authorId": "2145640018",
                        "name": "Pingyu Xiang"
                    },
                    {
                        "authorId": "2149891860",
                        "name": "Haojian Lu"
                    },
                    {
                        "authorId": "2118461398",
                        "name": "Yue Wang"
                    },
                    {
                        "authorId": "5738033",
                        "name": "R. Xiong"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", deterministic models [1], [6], stochastic models [7], and ensemble models [8]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6f82db7d18eb1a0373494d631842e6092fb0b2b8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-07920",
                    "ArXiv": "2207.07920",
                    "DOI": "10.1109/IROS47612.2022.9981303",
                    "CorpusId": 250626777
                },
                "corpusId": 250626777,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6f82db7d18eb1a0373494d631842e6092fb0b2b8",
                "title": "Physics Embedded Neural Network Vehicle Model and Applications in Risk-Aware Autonomous Driving Using Latent Features",
                "abstract": "Non-holonomic vehicle motion has been studied extensively using physics-based models. Common approaches when using these models interpret the wheel/ground interactions using a linear tire model and thus may not fully capture the nonlinear and complex dynamics under various environments. On the other hand, neural network models have been widely employed in this domain, demonstrating powerful function approximation capabilities. However, these black-box learning strategies completely abandon the existing knowledge of well-known physics. In this paper, we seamlessly combine deep learning with a fully differentiable physics model to endow the neural network with available prior knowledge. The proposed model shows better generalization performance than the vanilla neural network model by a large margin. We also show that the latent features of our model can accurately represent lateral tire forces without the need for any additional training. Lastly, We develop a risk-aware model predictive controller using proprioceptive information derived from the latent features. We validate our idea in two autonomous driving tasks under unknown friction, outperforming the baseline control framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1675257694",
                        "name": "Taekyung Kim"
                    },
                    {
                        "authorId": "66406739",
                        "name": "Ho-Woon Lee"
                    },
                    {
                        "authorId": "2148959310",
                        "name": "Wonsuk Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, due to the accumulation of prediction error at each 70 step [5] and the large search space for long-horizon planning, the search for an optimal plan on 71 long-horizon tasks using model-based RL is inaccurate and computationally expensive [6, 7, 8].",
                "However, such model-based RL approaches have 32 shown only limited success in long-horizon tasks due to low long-horizon prediction accuracy [5] 33 and computationally expensive search [6, 7, 8]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "8e9d84a7b2db57adda8d639c6d54c8977ef10761",
                "externalIds": {
                    "DBLP": "conf/corl/ShiLL22",
                    "ArXiv": "2207.07560",
                    "DOI": "10.48550/arXiv.2207.07560",
                    "CorpusId": 250607756
                },
                "corpusId": 250607756,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8e9d84a7b2db57adda8d639c6d54c8977ef10761",
                "title": "Skill-based Model-based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (RL) is a sample-efficient way of learning complex behaviors by leveraging a learned single-step dynamics model to plan actions in imagination. However, planning every action for long-horizon tasks is not practical, akin to a human planning out every muscle movement. Instead, humans efficiently plan with high-level skills to solve complex tasks. From this intuition, we propose a Skill-based Model-based RL framework (SkiMo) that enables planning in the skill space using a skill dynamics model, which directly predicts the skill outcomes, rather than predicting all small details in the intermediate states, step by step. For accurate and efficient long-term planning, we jointly learn the skill dynamics model and a skill repertoire from prior experience. We then harness the learned skill dynamics model to accurately simulate and plan over long horizons in the skill space, which enables efficient downstream learning of long-horizon, sparse reward tasks. Experimental results in navigation and manipulation domains show that SkiMo extends the temporal horizon of model-based approaches and improves the sample efficiency for both model-based RL and skill-based RL. Code and videos are available at https://clvrai.com/skimo",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117039498",
                        "name": "Lu Shi"
                    },
                    {
                        "authorId": "35198686",
                        "name": "Joseph J. Lim"
                    },
                    {
                        "authorId": "46358230",
                        "name": "Youngwoon Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While the success of model-based RL (MB-RL) has been witnessed in many single agent RL tasks [10], [11], [12], [13], the understanding of its MARL counterpart is still limited.",
                "To reduce the negative effect of model error, we adopt a branched rollout scheme proposed in [12], [18].",
                "Due to low data efficiency, model-based methods are widely studied as a promising approach for improving sample efficiency [24], [8], [11], [12].",
                "To alleviate the issue of compounding model error, we adopt a branching strategy [18], [12] by replacing few long-horizon rollouts with many short-horizon rollouts to reduce compounding error in model-generated rollouts."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "05cd4badb91cdb9d042742fe6093e58e9065dc82",
                "externalIds": {
                    "ArXiv": "2207.06559",
                    "DBLP": "conf/iros/DuMLLDW022",
                    "DOI": "10.1109/IROS47612.2022.9982253",
                    "CorpusId": 252070977
                },
                "corpusId": 252070977,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/05cd4badb91cdb9d042742fe6093e58e9065dc82",
                "title": "Scalable Model-based Policy Optimization for Decentralized Networked Systems",
                "abstract": "Reinforcement learning algorithms require a large amount of samples; this often limits their real-world applications on even simple tasks. Such a challenge is more outstanding in multi-agent tasks, as each step of operation is more costly, requiring communications or shifting or resources. This work aims to improve data efficiency of multi-agent control by model-based learning. We consider networked systems where agents are cooperative and communicate only locally with their neighbors, and propose the decentralized model-based policy optimization framework (DMPO). In our method, each agent learns a dynamic model to predict future states and broadcast their predictions by communication, and then the policies are trained under the model rollouts. To alleviate the bias of model-generated data, we restrain the model usage for generating myopic rollouts, thus reducing the compounding error of model generation. To pertain the independence of policy update, we introduce extended value function and theoretically prove that the resulting policy gradient is a close approximation to true policy gradients. We evaluate our algorithm on several benchmarks for intelligent transportation systems, which are connected autonomous vehicle control tasks (Flow and CACC) and adaptive traffic signal control (ATSC). Empirical results show that our method achieves superior data efficiency and matches the performance of model-free methods using true models. The source code of our algorithm and baselines can be found at https://github.com/PKU-MARL/Model-Based-MARL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390662136",
                        "name": "Yali Du"
                    },
                    {
                        "authorId": "2121455365",
                        "name": "Chengdong Ma"
                    },
                    {
                        "authorId": "2116487134",
                        "name": "Yuchen Liu"
                    },
                    {
                        "authorId": "2167032295",
                        "name": "Runji Lin"
                    },
                    {
                        "authorId": "2113412435",
                        "name": "Hao Dong"
                    },
                    {
                        "authorId": "48094081",
                        "name": "Jun Wang"
                    },
                    {
                        "authorId": "47796324",
                        "name": "Yaodong Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides, the computational cost is a critical factor when applying Adv USR to more complex dynamics with millions of parameters (common in current model-based RL research (Janner et al., 2021))."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7da9c1badb960b40168792889d53b9e943f74e01",
                "externalIds": {
                    "ArXiv": "2207.02016",
                    "DBLP": "journals/corr/abs-2207-02016",
                    "DOI": "10.48550/arXiv.2207.02016",
                    "CorpusId": 250280211
                },
                "corpusId": 250280211,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7da9c1badb960b40168792889d53b9e943f74e01",
                "title": "Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization",
                "abstract": "Reinforcement learning (RL) is recognized as lacking generalization and robustness under environmental perturbations, which excessively restricts its application for real-world robotics. Prior work claimed that adding regularization to the value function is equivalent to learning a robust policy with uncertain transitions. Although the regularization-robustness transformation is appealing for its simplicity and efficiency, it is still lacking in continuous control tasks. In this paper, we propose a new regularizer named $\\textbf{U}$ncertainty $\\textbf{S}$et $\\textbf{R}$egularizer (USR), by formulating the uncertainty set on the parameter space of the transition function. In particular, USR is flexible enough to be plugged into any existing RL framework. To deal with unknown uncertainty sets, we further propose a novel adversarial approach to generate them based on the value function. We evaluate USR on the Real-world Reinforcement Learning (RWRL) benchmark, demonstrating improvements in the robust performance for perturbed testing environments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156008691",
                        "name": "Yuan Zhang"
                    },
                    {
                        "authorId": "2144538488",
                        "name": "Jianhong Wang"
                    },
                    {
                        "authorId": "145581493",
                        "name": "J. Boedecker"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We predict the difference of states rather than the next states as it has been shown in past studies [21, 22] to yield better dynamics predictions.",
                "The sample efficiency requirement for offline IL methods reminds us of the success of model-based approaches in the online and offline RL domains [21, 22, 23, 24, 25]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "result"
            ],
            "citingPaper": {
                "paperId": "81b17aea4278a31cbf35c3f19b42ba856e22e934",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-00244",
                    "ArXiv": "2207.00244",
                    "DOI": "10.48550/arXiv.2207.00244",
                    "CorpusId": 250243691
                },
                "corpusId": 250243691,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/81b17aea4278a31cbf35c3f19b42ba856e22e934",
                "title": "Discriminator-Guided Model-Based Offline Imitation Learning",
                "abstract": "Offline imitation learning (IL) is a powerful method to solve decision-making problems from expert demonstrations without reward labels. Existing offline IL methods suffer from severe performance degeneration under limited expert data. Including a learned dynamics model can potentially improve the state-action space coverage of expert data, however, it also faces challenging issues like model approximation/generalization errors and suboptimality of rollout data. In this paper, we propose the Discriminator-guided Model-based offline Imitation Learning (DMIL) framework, which introduces a discriminator to simultaneously distinguish the dynamics correctness and suboptimality of model rollout data against real expert demonstrations. DMIL adopts a novel cooperative-yet-adversarial learning strategy, which uses the discriminator to guide and couple the learning process of the policy and dynamics model, resulting in improved model performance and robustness. Our framework can also be extended to the case when demonstrations contain a large proportion of suboptimal data. Experimental results show that DMIL and its extension achieve superior performance and robustness compared to state-of-the-art offline IL methods under small datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Wenjia Zhang"
                    },
                    {
                        "authorId": "49507262",
                        "name": "Haoran Xu"
                    },
                    {
                        "authorId": "122919426",
                        "name": "Haoyi Niu"
                    },
                    {
                        "authorId": "2167325163",
                        "name": "Peng Cheng"
                    },
                    {
                        "authorId": "35834541",
                        "name": "Ming Li"
                    },
                    {
                        "authorId": "2153525920",
                        "name": "Heming Zhang"
                    },
                    {
                        "authorId": "2153104630",
                        "name": "Guyue Zhou"
                    },
                    {
                        "authorId": "3415564",
                        "name": "Xianyuan Zhan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based offline RL, in part, has shown to better generalize to out-ofdistribution states because the agent\u2019s internal world model allows for offline exploration, branched from real data (Janner et al. 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c22b18ce729295eaeb1c6080441aeec0d2bddf66",
                "externalIds": {
                    "DBLP": "conf/aaai/Jenkins0JL22",
                    "DOI": "10.1609/aaai.v36i11.21523",
                    "CorpusId": 249536152
                },
                "corpusId": 249536152,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c22b18ce729295eaeb1c6080441aeec0d2bddf66",
                "title": "Bayesian Model-Based Offline Reinforcement Learning for Product Allocation",
                "abstract": "Product allocation in retail is the process of placing products throughout a store to connect consumers with relevant products. Discovering a good allocation strategy is challenging due to the scarcity of data and the high cost of experimentation in the physical world. Some work explores Reinforcement learning (RL) as a solution, but these approaches are often limited because of the sim2real problem. Learning policies from logged trajectories of a system is a key step forward for RL in physical systems. Recent work has shown that model-based offline RL can improve the effectiveness of offline policy estimation through uncertainty-penalized exploration. However, existing work assumes a continuous state space and access to a covariance matrix of the environment dynamics, which is not possible in the discrete case. To solve this problem, we propose a Bayesian model-based technique that naturally produces probabilistic estimates of the environment dynamics via the posterior predictive distribution, which we use for uncertainty-penalized exploration. We call our approach Posterior Penalized Offline Policy Optimization (PPOPO). We show that our world model better fits historical data due to informative priors, and that PPOPO outperforms other offline techniques in simulation and against real-world data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "83816065",
                        "name": "P. Jenkins"
                    },
                    {
                        "authorId": "1474226770",
                        "name": "Hua Wei"
                    },
                    {
                        "authorId": "1420081386",
                        "name": "J. S. Jenkins"
                    },
                    {
                        "authorId": "2109640666",
                        "name": "Z. Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d78e611208de3b80982df0ce7b1957d1477374ab",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-13714",
                    "ArXiv": "2206.13714",
                    "DOI": "10.48550/arXiv.2206.13714",
                    "CorpusId": 250088978
                },
                "corpusId": 250088978,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d78e611208de3b80982df0ce7b1957d1477374ab",
                "title": "Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse",
                "abstract": "Data-driven, learning-based control methods offer the potential to improve operations in complex systems, and model-free deep reinforcement learning represents a popular approach to data-driven control. However, existing classes of algorithms present a trade-off between two important deployment requirements for real-world control: (i) practical performance guarantees and (ii) data efficiency. Off-policy algorithms make efficient use of data through sample reuse but lack theoretical guarantees, while on-policy algorithms guarantee approximate policy improvement throughout training but suffer from high sample complexity. In order to balance these competing goals, we develop a class of Generalized Policy Improvement algorithms that combines the policy improvement guarantees of on-policy methods with the efficiency of sample reuse. We demonstrate the benefits of this new class of algorithms through extensive experimental analysis on a variety of continuous control tasks from the DeepMind Control Suite.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2039906784",
                        "name": "James Queeney"
                    },
                    {
                        "authorId": "1691402",
                        "name": "I. Paschalidis"
                    },
                    {
                        "authorId": "1721319",
                        "name": "C. Cassandras"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based reinforcement learning (RL) holds the promise of sample-efficient robot learning by learning a world model and leveraging it for planning [1, 2, 3] or generating imaginary states for behavior learning [4, 5]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "31d629bb161d8199e18b6f2ed7e4ecbda10b6797",
                "externalIds": {
                    "DBLP": "conf/corl/SeoHLLJLA22",
                    "ArXiv": "2206.14244",
                    "DOI": "10.48550/arXiv.2206.14244",
                    "CorpusId": 250113367
                },
                "corpusId": 250113367,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31d629bb161d8199e18b6f2ed7e4ecbda10b6797",
                "title": "Masked World Models for Visual Control",
                "abstract": "Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling approach achieves state-of-the-art performance on a variety of visual robotic tasks from Meta-world and RLBench, e.g., we achieve 81.7% success rate on 50 visual robotic manipulation tasks from Meta-world, while the baseline achieves 67.9%. Code is available on the project website: https://sites.google.com/view/mwm-rl.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2067714176",
                        "name": "Younggyo Seo"
                    },
                    {
                        "authorId": "35006479",
                        "name": "Danijar Hafner"
                    },
                    {
                        "authorId": "2143855835",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "32324034",
                        "name": "Fangchen Liu"
                    },
                    {
                        "authorId": "2055291154",
                        "name": "Stephen James"
                    },
                    {
                        "authorId": "3436470",
                        "name": "Kimin Lee"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018), providing synthetic data (Kurutach et al., 2018; Janner et al., 2019), or improving Q-value estimates (Feinberg et al.",
                "Then the dynamics model and reward predictor are used for planning (Williams et al., 2017; Chua et al., 2018; Nagabandi et al., 2018), providing synthetic data (Kurutach et al., 2018; Janner et al., 2019), or improving Q-value estimates (Feinberg et al., 2018; Amos et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "97575233a5501d03566af33d795015dca455d179",
                "externalIds": {
                    "DBLP": "conf/icml/WangXXZS22",
                    "ArXiv": "2206.13452",
                    "DOI": "10.48550/arXiv.2206.13452",
                    "CorpusId": 250073227
                },
                "corpusId": 250073227,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/97575233a5501d03566af33d795015dca455d179",
                "title": "Causal Dynamics Learning for Task-Independent State Abstraction",
                "abstract": "Learning dynamics models accurately is an important goal for Model-Based Reinforcement Learning (MBRL), but most MBRL methods learn a dense dynamics model which is vulnerable to spurious correlations and therefore generalizes poorly to unseen states. In this paper, we introduce Causal Dynamics Learning for Task-Independent State Abstraction (CDL), which first learns a theoretically proved causal dynamics model that removes unnecessary dependencies between state variables and the action, thus generalizing well to unseen states. A state abstraction can then be derived from the learned dynamics, which not only improves sample efficiency but also applies to a wider range of tasks than existing state abstraction methods. Evaluated on two simulated environments and downstream tasks, both the dynamics model and policies learned by the proposed method generalize well to unseen states and the derived state abstraction improves sample efficiency compared to learning without it.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2881858",
                        "name": "Zizhao Wang"
                    },
                    {
                        "authorId": "2118724825",
                        "name": "Xuesu Xiao"
                    },
                    {
                        "authorId": "1935103531",
                        "name": "Zifan Xu"
                    },
                    {
                        "authorId": "2117748",
                        "name": "Yuke Zhu"
                    },
                    {
                        "authorId": "144848112",
                        "name": "P. Stone"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Second, in contrast with model-based dynamics (Janner et al., 2019) and curiosity (Pathak et al.",
                "Second, in contrast with model-based dynamics (Janner et al., 2019) and curiosity (Pathak et al., 2017) approaches, which require either the next state or the next action to compute the intrinsic signal, we need only the current state-action pair to obtain a plausible action direction."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "dfde1b276e3552038a384de286094685c96d222e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-12674",
                    "ArXiv": "2206.12674",
                    "DOI": "10.48550/arXiv.2206.12674",
                    "CorpusId": 250072360
                },
                "corpusId": 250072360,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dfde1b276e3552038a384de286094685c96d222e",
                "title": "Guided Exploration in Reinforcement Learning via Monte Carlo Critic Optimization",
                "abstract": "The class of deep deterministic off-policy algorithms is effectively applied to solve challenging continuous control problems. However, current approaches use random noise as a common exploration method that has several weaknesses, such as a need for manual adjusting on a given task and the absence of exploratory calibration during the training process. We address these challenges by proposing a novel guided exploration method that uses a differential directional controller to incorporate scalable exploratory action correction. An ensemble of Monte Carlo Critics that provides exploratory direction is presented as a controller. The proposed method improves the traditional exploration scheme by changing exploration dynamically. We then present a novel algorithm exploiting the proposed directional controller for both policy and critic modification. The presented algorithm outperforms modern reinforcement learning algorithms across a variety of problems from DMControl suite.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064784623",
                        "name": "Igor Kuznetsov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The learned model could also be used to extract a task policy later using model-based policy optimization [50] or to perform offline RL on the data generated during free play, which we demonstrate in Sec."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "cbf56ae8cd6f4641d6af592f85971282a38b459b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-11403",
                    "ArXiv": "2206.11403",
                    "DOI": "10.48550/arXiv.2206.11403",
                    "CorpusId": 249953667
                },
                "corpusId": 249953667,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/cbf56ae8cd6f4641d6af592f85971282a38b459b",
                "title": "Curious Exploration via Structured World Models Yields Zero-Shot Object Manipulation",
                "abstract": "It has been a long-standing dream to design artificial agents that explore their environment efficiently via intrinsic motivation, similar to how children perform curious free play. Despite recent advances in intrinsically motivated reinforcement learning (RL), sample-efficient exploration in object manipulation scenarios remains a significant challenge as most of the relevant information lies in the sparse agent-object and object-object interactions. In this paper, we propose to use structured world models to incorporate relational inductive biases in the control loop to achieve sample-efficient and interaction-rich exploration in compositional multi-object environments. By planning for future novelty inside structured world models, our method generates free-play behavior that starts to interact with objects early on and develops more complex behavior over time. Instead of using models only to compute intrinsic rewards, as commonly done, our method showcases that the self-reinforcing cycle between good models and good exploration also opens up another avenue: zero-shot generalization to downstream tasks via model-based planning. After the entirely intrinsic task-agnostic exploration phase, our method solves challenging downstream tasks such as stacking, flipping, pick&place, and throwing that generalizes to unseen numbers and arrangements of objects without any additional training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1486465732",
                        "name": "Cansu Sancaktar"
                    },
                    {
                        "authorId": "46773173",
                        "name": "Sebastian Blaes"
                    },
                    {
                        "authorId": "144247521",
                        "name": "G. Martius"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To solve this problem some researchers use RNN to encode the interactions for constructing the state in the model-based RL method [2, 10, 19, 54, 60]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b17ab85a17fcfcfeb2ffe1aa90528039a3572095",
                "externalIds": {
                    "DBLP": "journals/mlc/HuangLZ22",
                    "DOI": "10.1007/s13042-022-01594-8",
                    "CorpusId": 249973393
                },
                "corpusId": 249973393,
                "publicationVenue": {
                    "id": "a0c45882-7c78-4f0c-8886-d3481ba02586",
                    "name": "International Journal of Machine Learning and Cybernetics",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Mach Learn Cybern"
                    ],
                    "issn": "1868-8071",
                    "url": "http://www.springer.com/engineering/mathematical/journal/13042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/13042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b17ab85a17fcfcfeb2ffe1aa90528039a3572095",
                "title": "ACP based reinforcement learning for long-term recommender system",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109025908",
                        "name": "Tianyi Huang"
                    },
                    {
                        "authorId": "2145627224",
                        "name": "Min Li"
                    },
                    {
                        "authorId": "2162704092",
                        "name": "William Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Theorem 7 (Theorem 4.3 in [Janner et al., 2019]).",
                "The coefficient means a quadratic compounding error of learning in the model, which is the reason that studies such as [Janner et al., 2019] only adopt short rollouts, say, less than 10 steps, in the learned model.",
                "MBPO also gives a monotonic improvement theorem with model bias and k-branch rollouts.",
                "With a sufficiently accurate model, it is intuitive that MBRL yields higher sample efficiency than MFRL, as shown in recent studies from both theoretical [Sun et al., 2019] and empirical [Janner et al., 2019, Wang et al., 2019] perspectives.",
                "A straightforward approach to model learning fits one-step transitions, which has been widely employed [Kurutach et al., 2018a, Feinberg et al., 2018, Luo et al., 2018, Janner et al., 2019, Rajeswaran et al., 2020].",
                "On the contrary, the performance of MBPO drops rapidly as the rollouts become longer.",
                "Motivated by such an analysis, they proposed the AutoMBPO framework to automatically schedule the key hyperparameters of the MBPO [Janner et al., 2019] algorithm.",
                "(5)\nThe probabilistic transition model is often instantiated as a Gaussian distribution [Chua et al., 2018, Luo et al., 2018, Janner et al., 2019], i.e., M\u03b8(\u00b7|s, a) = N (\u00b5\u03b8(s, a),\u03a3\u03b8(s, a)) with parameterized models of \u00b5\u03b8 and \u03a3\u03b8.",
                "In many cases, the reward function is also explicitly defined, thus the major task of the model learning is to learn the state transition dynamics [Luo et al., 2018, Janner et al., 2019].",
                "Note that the bound of MBPO with the same rollout length to BMPO is C( m, \u03c0, m\u2032 , k1 + k2).",
                "MBPO begins a rollout from a state sampled in the real environment and runs k steps according to policy \u03c0 and the learned model p\u03b8.",
                "Typically, MBPO uses a short rollout length to avoid large compounding errors, which may limit the model usage.",
                "Model-based policy optimization (MBPO) [Janner et al., 2019], on the other hand, samples the branched rollout in the model.",
                "Moreover, MBPO also adopts Soft Actor-Critic [Haarnoja et al., 2018], which is an off-policy RL algorithm, to update the policy with the mixed data from the real environment and learned model.",
                "In the recent analysis [Luo et al., 2018, Janner et al., 2019, Xu et al., 2020], the error of the learned transition model M\u03b8 is measured over the data distribution, and thus directly connects with the learning loss Eq."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b6b6bc529e665ebf97326d084a71159634ae10a7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-09328",
                    "ArXiv": "2206.09328",
                    "DOI": "10.48550/arXiv.2206.09328",
                    "CorpusId": 249889734
                },
                "corpusId": 249889734,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b6b6bc529e665ebf97326d084a71159634ae10a7",
                "title": "A Survey on Model-based Reinforcement Learning",
                "abstract": "Reinforcement learning (RL) solves sequential decision-making problems via a trial-and-error process interacting with the environment. While RL achieves outstanding success in playing complex video games that allow huge trial-and-error, making errors is always undesired in the real world. To improve the sample efficiency and thus reduce the errors, model-based reinforcement learning (MBRL) is believed to be a promising direction, which builds environment models in which the trial-and-errors can take place without real costs. In this survey, we take a review of MBRL with a focus on the recent progress in deep RL. For non-tabular environments, there is always a generalization error between the learned environment model and the real environment. As such, it is of great importance to analyze the discrepancy between policy training in the environment model and that in the real environment, which in turn guides the algorithm design for better model learning, model usage, and policy training. Besides, we also discuss the recent advances of model-based techniques in other forms of RL, including offline RL, goal-conditioned RL, multi-agent RL, and meta-RL. Moreover, we discuss the applicability and advantages of MBRL in real-world tasks. Finally, we end this survey by discussing the promising prospects for the future development of MBRL. We think that MBRL has great potential and advantages in real-world applications that were overlooked, and we hope this survey could attract more research on MBRL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2072689683",
                        "name": "Fan Luo"
                    },
                    {
                        "authorId": "40084973",
                        "name": "Tian Xu"
                    },
                    {
                        "authorId": "2054235888",
                        "name": "Hang Lai"
                    },
                    {
                        "authorId": "2108968873",
                        "name": "Xiong-Hui Chen"
                    },
                    {
                        "authorId": "2155042473",
                        "name": "Weinan Zhang"
                    },
                    {
                        "authorId": "144705629",
                        "name": "Yang Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another way is to train an additional model learning environment dynamics [18, 20, 30, 53]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b7d27c5af2d314f6ec45b6d88984fb45220eb379",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-08569",
                    "ArXiv": "2206.08569",
                    "DOI": "10.48550/arXiv.2206.08569",
                    "CorpusId": 249848028
                },
                "corpusId": 249848028,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b7d27c5af2d314f6ec45b6d88984fb45220eb379",
                "title": "Bootstrapped Transformer for Offline Reinforcement Learning",
                "abstract": "Offline reinforcement learning (RL) aims at learning policies from previously collected static trajectory data without interacting with the real environment. Recent works provide a novel perspective by viewing offline RL as a generic sequence generation problem, adopting sequence models such as Transformer architecture to model distributions over trajectories, and repurposing beam search as a planning algorithm. However, the training datasets utilized in general offline RL tasks are quite limited and often suffer from insufficient distribution coverage, which could be harmful to training sequence generation models yet has not drawn enough attention in the previous works. In this paper, we propose a novel algorithm named Bootstrapped Transformer, which incorporates the idea of bootstrapping and leverages the learned model to self-generate more offline data to further boost the sequence model training. We conduct extensive experiments on two offline RL benchmarks and demonstrate that our model can largely remedy the existing offline RL training limitations and beat other strong baseline methods. We also analyze the generated pseudo data and the revealed characteristics may shed some light on offline RL training. The codes are available at https://seqml.github.io/bootorl.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2124632040",
                        "name": "Kerong Wang"
                    },
                    {
                        "authorId": "1490744375",
                        "name": "Hanye Zhao"
                    },
                    {
                        "authorId": "13289447",
                        "name": "Xufang Luo"
                    },
                    {
                        "authorId": "144931569",
                        "name": "Kan Ren"
                    },
                    {
                        "authorId": "2155042473",
                        "name": "Weinan Zhang"
                    },
                    {
                        "authorId": "2108481496",
                        "name": "Dongsheng Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following prior works [23, 73, 26], we train an ensemble of seven such probability neural networks for both the forward and backward model.",
                "Model-based online RL methods achieve superior sample efficiency [59, 25, 4, 23] by learning a dynamics model of the environment and planning with the model [60, 66, 69].",
                "Model-based reinforcement learning (RL) learns either forward dynamics or reverse dynamics of the environment [60, 20], and can produce imaginary transitions for training, which has been widely demonstrated to be effective in improving the sample efficiency of RL in the online setting [4, 23].",
                "Following prior work [73, 26], we train an ensemble of bootstrapped probabilistic dynamics models, which has been widely demonstrated to be effective in model-based RL [8, 23]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3f9f6f221df9c92bf267bc32415dbc3ce55d2d52",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-07989",
                    "ArXiv": "2206.07989",
                    "DOI": "10.48550/arXiv.2206.07989",
                    "CorpusId": 249712151
                },
                "corpusId": 249712151,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3f9f6f221df9c92bf267bc32415dbc3ce55d2d52",
                "title": "Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination",
                "abstract": "The learned policy of model-free offline reinforcement learning (RL) methods is often constrained to stay within the support of datasets to avoid possible dangerous out-of-distribution actions or states, making it challenging to handle out-of-support region. Model-based RL methods offer a richer dataset and benefit generalization by generating imaginary trajectories with either trained forward or reverse dynamics model. However, the imagined transitions may be inaccurate, thus downgrading the performance of the underlying offline RL method. In this paper, we propose to augment the offline dataset by using trained bidirectional dynamics models and rollout policies with double check. We introduce conservatism by trusting samples that the forward model and backward model agree on. Our method, confidence-aware bidirectional offline model-based imagination, generates reliable samples and can be combined with any model-free offline RL method. Experimental results on the D4RL benchmarks demonstrate that our method significantly boosts the performance of existing model-free offline RL algorithms and achieves competitive or better scores against baseline methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2008151131",
                        "name": "Jiafei Lyu"
                    },
                    {
                        "authorId": "2180539270",
                        "name": "Xiu Li"
                    },
                    {
                        "authorId": "2265693",
                        "name": "Zongqing Lu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c3a2e7d52d975b50c9e98e6cca4a7449b3068c2f",
                "externalIds": {
                    "DBLP": "journals/jair/MaMXZ22",
                    "ArXiv": "2206.07376",
                    "DOI": "10.1613/jair.1.13833",
                    "CorpusId": 249674908
                },
                "corpusId": 249674908,
                "publicationVenue": {
                    "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
                    "name": "Journal of Artificial Intelligence Research",
                    "type": "journal",
                    "alternate_names": [
                        "JAIR",
                        "J Artif Intell Res",
                        "The Journal of Artificial Intelligence Research"
                    ],
                    "issn": "1076-9757",
                    "url": "http://www.jair.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c3a2e7d52d975b50c9e98e6cca4a7449b3068c2f",
                "title": "Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement Learning",
                "abstract": "Keeping risk under control is often more crucial than maximizing expected reward in real-world decision-making situations, such as finance, robotics, autonomous driving, etc. The most natural choice of risk measures is variance, while it penalizes the upside volatility as much as the downside part. Instead, the (downside) semivariance, which captures the negative deviation of a random variable under its mean, is more suitable for risk-averse proposes. This paper aims at optimizing the mean-semivariance (MSV) criterion in reinforcement learning w.r.t. steady rewards. Since semivariance is time-inconsistent and does not satisfy the standard Bellman equation, the traditional dynamic programming methods are inapplicable to MSV problems directly. To tackle this challenge, we resort to the Perturbation Analysis (PA) theory and establish the performance difference formula for MSV. We reveal that the MSV problem can be solved by iteratively solving a sequence of RL problems with a policy-dependent reward function. Further, we propose two on-policy algorithms based on the policy gradient theory and the trust region method. Finally, we conduct diverse experiments from simple bandit problems to continuous control tasks in MuJoCo, which demonstrate the effectiveness of our proposed methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32704046",
                        "name": "Xiaoteng Ma"
                    },
                    {
                        "authorId": "2118868195",
                        "name": "Shuai Ma"
                    },
                    {
                        "authorId": "2107063909",
                        "name": "Li Xia"
                    },
                    {
                        "authorId": "36281262",
                        "name": "Qianchuan Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Perform h-step rollouts using the learned model P\u0302 and the current policy \u03c0\u03c6 by branching from the offline datasetDenv, and adding the generated data to a separate replay bufferDmodel, as in Janner et al. (2019) and Yu et al. (2020).",
                "With the offline dataset Denv, P\u0302 is typically trained using MLE (Janner et al., 2019; Yu et al., 2020; 2021) as\narg maxP\u0302\u2208P E(s,a,s\u2032)\u223cDenv [ log P\u0302 (s\u2032 | s, a) ] .",
                "\u2026and subsequently using that for policy learning, MBRL can provide a sample-efficient solution to answer the counterfactual question pertaining to action-value estimation in the online setting (Janner et al., 2019; Rajeswaran et al., 2020), and to provide an augmentation to the\nar X\niv :2\n20 6.",
                "We follow prior work (Chua et al., 2018; Janner et al., 2019; Yu et al., 2020) to construct our model as an ensemble of Gaussian probabilistic networks.",
                "(5) can be minimized via the MLE for P\u0302 , i.e., Eq.",
                "Combine Theorems 3.1 and 3.2 and Proposition 3.3, and suppose the dynamic model P\u0302 is trained by the MLE objective (Eq.",
                "With the learned model P\u0302 , we follow prior model-based RL work (Janner et al., 2019; Yu et al., 2020) to augment Denv with the replay buffer Dmodel consisting of h-horizon rollouts of the current policy \u03c0\u03c6 on the learned model P\u0302 , by branching from states in the offline dataset Denv.",
                "As in prior model-based RL using Gaussian probabilistic ensemble (Chua et al., 2018; Janner et al., 2019; Yu et al., 2020; Cang et al., 2021; Yu et al., 2021), the output of our model is double-headed, respectively outputting the mean and log-standard-deviation of the normal distribution of the\u2026",
                "To implement this bound, we (1) train a dynamic model in a sufficiently rich function class via the maximum likelihood estimation (MLE); and (2) add a tractable regularizer into the policy optimization objective.",
                "Combining Theorem 3.1, 3.2 and Proposition 3.3, and suppose the dynamics model P\u0302 is trained by the MLE objective (Eq.",
                "Different from the goal of estimating the performance of some policies using offline data, our goal here is to construct a tractable regularization for policy optimization, which involves a learned transition model trained by MLE.",
                "Informally, this proposition coincides with the intuition that estimating the environmental dynamic using MLE can be helpful for matching dP\u0302\u03c0 with d P\u2217\n\u03c0 .",
                "Similar to Janner et al. (2019) and Yu et al. (2020) we consider the rollout horizon h \u2208 {1, 3, 5}.",
                "If the function class P for the estimated dynamic P\u0302 is rich enough and we have sufficiently many empirical samples from dP \u2217\n\u03c0b , under the classical statistical regularity condi-\ntion, we can achieve a small model error Emodel by MLE (Ferguson, 1996; Casella & Berger, 2001).",
                "The first step is to minimize the model error Emodel by pre-training the dynamic model under a sufficiently rich function class via MLE."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ebc87ed7e0586f5b31fd7351f8b5b5db32a74505",
                "externalIds": {
                    "ArXiv": "2206.07166",
                    "DBLP": "journals/corr/abs-2206-07166",
                    "DOI": "10.48550/arXiv.2206.07166",
                    "CorpusId": 249674486
                },
                "corpusId": 249674486,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ebc87ed7e0586f5b31fd7351f8b5b5db32a74505",
                "title": "Regularizing a Model-based Policy Stationary Distribution to Stabilize Offline Reinforcement Learning",
                "abstract": "Offline reinforcement learning (RL) extends the paradigm of classical RL algorithms to purely learning from static datasets, without interacting with the underlying environment during the learning process. A key challenge of offline RL is the instability of policy training, caused by the mismatch between the distribution of the offline data and the undiscounted stationary state-action distribution of the learned policy. To avoid the detrimental impact of distribution mismatch, we regularize the undiscounted stationary distribution of the current policy towards the offline data during the policy optimization process. Further, we train a dynamics model to both implement this regularization and better estimate the stationary distribution of the current policy, reducing the error induced by distribution mismatch. On a wide range of continuous-control offline RL datasets, our method indicates competitive performance, which validates our algorithm. The code is publicly available.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155974419",
                        "name": "Shentao Yang"
                    },
                    {
                        "authorId": "22758695",
                        "name": "Yihao Feng"
                    },
                    {
                        "authorId": "2107944048",
                        "name": "Shujian Zhang"
                    },
                    {
                        "authorId": "2152176954",
                        "name": "Mi Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(6) contains a constraint on the policy update size, and directly solving 1The above terms and assumptions have been widely used in RL in the literature such as [9, 15, 11, 7].",
                "For the latter, we use an ensemble of dynamics models of size 7, following [7].",
                "It has been proved that the true return can be improved by interacting with the learned dynamics model when the model error is bounded [7].",
                "Compared to model-free methods, model-based RL [1, 14, 8, 7, 13] learns a dynamics model that mimics the transitions in the true environment, and then the policy can feel free to interact with the learned dynamics instead of the true environment.",
                "We also involve a state-of-the-art model-based method MBPO [7] as one of the baseline methods.",
                "This is possible when P (the dynamics in source environment) is trainable, as considered in physical dynamics modeling [6, 4, 23] and model-based RL methods [7].",
                "[7] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "478c1c3b3b344c22bc7bc6cd41f21ecd4ea6f261",
                "externalIds": {
                    "ArXiv": "2206.06009",
                    "DBLP": "journals/corr/abs-2206-06009",
                    "DOI": "10.48550/arXiv.2206.06009",
                    "CorpusId": 249626165
                },
                "corpusId": 249626165,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/478c1c3b3b344c22bc7bc6cd41f21ecd4ea6f261",
                "title": "Relative Policy-Transition Optimization for Fast Policy Transfer",
                "abstract": "We consider the problem of policy transfer between two Markov Decision Processes (MDPs). We introduce a lemma based on existing theoretical results in reinforcement learning (RL) to measure the relativity between two arbitrary MDPs, that is the difference between any two cumulative expected returns defined on different policies and environment dynamics. Based on this lemma, we propose two new algorithms referred to as Relative Policy Optimization (RPO) and Relative Transition Optimization (RTO), which can offer fast policy transfer and dynamics modeling, respectively. RPO updates the policy using the relative policy gradient to transfer the policy evaluated in one environment to maximize the return in another, while RTO updates the parameterized dynamics model (if there exists) using the relative transition gradient to reduce the gap between the dynamics of the two environments. Then, integrating the two algorithms offers the complete algorithm Relative Policy-Transition Optimization (RPTO), in which the policy interacts with the two environments simultaneously, such that data collections from two environments, policy and transition updates are completed in one closed loop to form a principled learning framework for policy transfer. We demonstrate the effectiveness of RPTO in OpenAI gym's classic control tasks by creating policy transfer problems via variant dynamics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112661118",
                        "name": "Lei Han"
                    },
                    {
                        "authorId": "2110476942",
                        "name": "Jiawei Xu"
                    },
                    {
                        "authorId": "2111168222",
                        "name": "Cheng Zhou"
                    },
                    {
                        "authorId": "1591122785",
                        "name": "Yizheng Zhang"
                    },
                    {
                        "authorId": "2148905709",
                        "name": "Zhengyou Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model We train the model as similar to [18, 24], where we train an ensemble of 7 neural networks and predict with a random sample from the ensemble predictions.",
                "In particular, [18, 17] builds on the work of [24] that uses an ensemble of model to estimate epistemic uncertainty.",
                "Indeed, offline estimation and usage of a model comes with its own set of difficulties[23, 18, 24], as the model only has access to data with limited support and cannot improve its accuracy using additional experience.",
                "[24] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",
                "It is also well known that a policy optimizing in such a model could learn to exploit where it is inaccurate, which degrades the evaluated performance[24, 23].",
                "As opposed to standard model based RL[24], in the imitation learning setting we provide some additional justifications as to why a longer H (up to a certain point) might be desirable in section B.",
                "Meanwhile too short of a rollout from Tl such as in[18, 24] increases the sampling bias from not sampling from d\u03c0\u0302Tl , and the resulting algorithm strays too much from our analysis.",
                "However, with infinite state action spaces, naively rolling out the entire trajectory for 1 1\u2212\u03b3 steps in Tl might not be desirable[24] as the model\u2019s error compounds as shown in section 4[34].",
                "Following works in the model based offline reinforcement learning literature[24], we train Tl as an ensemble of K probabilistic neural networks that each outputs a mean and a covariance matrix \u03a3 to estimate the transition from the behavior dataset, and use the discrepancies in their predictions to estimate uncertainty."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "eb6ed638b272e3cf2df3db8f43a3ff7f5942fce0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-05521",
                    "ArXiv": "2206.05521",
                    "DOI": "10.48550/arXiv.2206.05521",
                    "CorpusId": 249625867
                },
                "corpusId": 249625867,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eb6ed638b272e3cf2df3db8f43a3ff7f5942fce0",
                "title": "Model-based Offline Imitation Learning with Non-expert Data",
                "abstract": "Although Behavioral Cloning (BC) in theory suffers compounding errors, its scalability and simplicity still makes it an attractive imitation learning algorithm. In contrast, imitation approaches with adversarial training typically does not share the same problem, but necessitates interactions with the environment. Meanwhile, most imitation learning methods only utilises optimal datasets, which could be significantly more expensive to obtain than its suboptimal counterpart. A question that arises is, can we utilise the suboptimal dataset in a principled manner, which otherwise would have been idle? We propose a scalable model-based offline imitation learning algorithmic framework that leverages datasets collected by both suboptimal and optimal policies, and show that its worst case suboptimality becomes linear in the time horizon with respect to the expert samples. We empirically validate our theoretical results and show that the proposed method \\textit{always} outperforms BC in the low data regime on simulated continuous control domains",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116770797",
                        "name": "Jeongwon Park"
                    },
                    {
                        "authorId": "2155557118",
                        "name": "Lin Yang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "066426da96c29e20f07b7100bda974181eb1844e",
                "externalIds": {
                    "ArXiv": "2206.05165",
                    "DBLP": "journals/corr/abs-2206-05165",
                    "DOI": "10.48550/arXiv.2206.05165",
                    "CorpusId": 249605438
                },
                "corpusId": 249605438,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/066426da96c29e20f07b7100bda974181eb1844e",
                "title": "Multifidelity Reinforcement Learning with Control Variates",
                "abstract": "In many computational science and engineering applications, the output of a system of interest corresponding to a given input can be queried at different levels of fidelity with different costs. Typically, low-fidelity data is cheap and abundant, while high-fidelity data is expensive and scarce. In this work we study the reinforcement learning (RL) problem in the presence of multiple environments with different levels of fidelity for a given control task. We focus on improving the RL agent's performance with multifidelity data. Specifically, a multifidelity estimator that exploits the cross-correlations between the low- and high-fidelity returns is proposed to reduce the variance in the estimation of the state-action value function. The proposed estimator, which is based on the method of control variates, is used to design a multifidelity Monte Carlo RL (MFMCRL) algorithm that improves the learning of the agent in the high-fidelity environment. The impacts of variance reduction on policy evaluation and policy improvement are theoretically analyzed by using probability bounds. Our theoretical analysis and numerical experiments demonstrate that for a finite budget of high-fidelity data samples, our proposed MFMCRL agent attains superior performance compared with that of a standard RL agent that uses only the high-fidelity environment data for learning the optimal policy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143979834",
                        "name": "Sami Khairy"
                    },
                    {
                        "authorId": "2138151793",
                        "name": "P. Balaprakash"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based Reinforcement Learning Recent research in model-based reinforcement learning (MBRL) has shown a significant advantage [12, 16, 13, 35, 40] in sample efficiency over model-free reinforcement learning.",
                "Due to the accumulation of the prediction error at each step, these future estimations can quickly diverge from reality [16, 43].",
                "World models help agents understand environment dynamics and improve policies in sample efficiency [16] and performance [35].",
                "For example, Dynastyle methods like [24, 16, 28, 42] use world models to augment the training data."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "67b950f8d31e1b5dfed49993ef4d809e3bf0cc2c",
                "externalIds": {
                    "ArXiv": "2206.04384",
                    "DBLP": "journals/corr/abs-2206-04384",
                    "DOI": "10.48550/arXiv.2206.04384",
                    "CorpusId": 249538584
                },
                "corpusId": 249538584,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/67b950f8d31e1b5dfed49993ef4d809e3bf0cc2c",
                "title": "Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning",
                "abstract": "Reinforcement Learning (RL) methods are typically applied directly in environments to learn policies. In some complex environments with continuous state-action spaces, sparse rewards, and/or long temporal horizons, learning a good policy in the original environments can be difficult. Focusing on the offline RL setting, we aim to build a simple and discrete world model that abstracts the original environment. RL methods are applied to our world model instead of the environment data for simplified policy learning. Our world model, dubbed Value Memory Graph (VMG), is designed as a directed-graph-based Markov decision process (MDP) of which vertices and directed edges represent graph states and graph actions, separately. As state-action spaces of VMG are finite and relatively small compared to the original environment, we can directly apply the value iteration algorithm on VMG to estimate graph state values and figure out the best graph actions. VMG is trained from and built on the offline RL dataset. Together with an action translator that converts the abstract graph actions in VMG to real actions in the original environment, VMG controls agents to maximize episode returns. Our experiments on the D4RL benchmark show that VMG can outperform state-of-the-art offline RL methods in several goal-oriented tasks, especially when environments have sparse rewards and long temporal horizons. Code is available at https://github.com/TsuTikgiau/ValueMemoryGraph",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1388731230",
                        "name": "Deyao Zhu"
                    },
                    {
                        "authorId": "144180616",
                        "name": "Erran L. Li"
                    },
                    {
                        "authorId": "1712479",
                        "name": "Mohamed Elhoseiny"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Dynamics Generalization in MBRL Several meta-learning-based MBRL methods are proposed Nagabandi et al. (2018a;b); S\u00e6mundsson et al. (2018); Huang et al. (2021) to adapt the MBRL into environments with unseen dynamics by updating model parameters via a small number of gradient updates Finn et al. (2017) or hidden representations of a recurrent model Doshi-Velez & Konidaris (2016), and then Wang & van Hoof (2021) proposes a graph-structured model to improve dynamics forecasting.",
                "\u2026model f\u0302 to improve its generalization ability on different dynamics by optimizing the objective function following (Lee et al., 2020; Seo et al., 2020; Janner et al., 2019):\nLpred\u03b8,\u03c6 = \u2212 1\nN N\u2211 i=1 log f\u0302(sit+1|sit, ait, g(\u03c4 it\u2212k:t\u22121;\u03c6); \u03b8), (1)\nwhere k is the length of transition segments, t is\u2026",
                "Therefore, a slight change of environmental dynamics may cause a significant performance decline of MBRL methods (Lee et al., 2020; Nagabandi et al., 2018a; Seo et al., 2020).",
                "This problem is called the dynamics generalization problem in MBRL, where the training environments and test environments share the same state S and action space A but the transition dynamics between states p(st+1|st, at) varies across different environments.",
                "The vulnerability of MBRL to the change of environmental dynamics makes them unreliable in real world applications.",
                "Therefore, model-based reinforcement learning (MBRL) (Janner et al., 2019; Kaiser et al., 2019; Schrittwieser et al., 2020; Zhang et al., 2019b; van Hasselt et al., 2019; Hafner et al., 2019b;a; Lenz et al., 2015), which explicitly builds a predictive model to generate samples for learning RL\u2026",
                "After obtaining environment-specified \u1e91t\u2212k:t\u22121 at timestep t, we incorporate it into the dynamics prediction model f\u0302 to improve its generalization ability on different dynamics by optimizing the objective function following (Lee et al., 2020; Seo et al., 2020; Janner et al., 2019):",
                "However, the performance of MBRL methods highly relies on the prediction accuracy of the learned environmental model (Janner et al., 2019).",
                "Therefore, model-based reinforcement learning (MBRL) (Janner et al., 2019; Kaiser et al., 2019; Schrittwieser et al., 2020; Zhang et al., 2019b; van Hasselt et al., 2019; Hafner et al., 2019b;a; Lenz et al., 2015), which explicitly builds a predictive model to generate samples for learning RL policy, has been widely applied to a variety of limited data sequential decision-making problems.",
                "Taking the robotic control as an example (Nagabandi et al., 2018a; Yang et al., 2020; Rakelly et al., 2019; Gu et al., 2017; Bousmalis et al., 2018; Raileanu et al., 2020; Yang et al., 2019), dynamics change caused by parts damages could easily lead to the failure of MBRL algorithms."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "955536024c5db4166e63d41406c290fcf7ade696",
                "externalIds": {
                    "ArXiv": "2206.04551",
                    "DBLP": "conf/iclr/GuoGT22",
                    "DOI": "10.48550/arXiv.2206.04551",
                    "CorpusId": 249538446
                },
                "corpusId": 249538446,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/955536024c5db4166e63d41406c290fcf7ade696",
                "title": "A Relational Intervention Approach for Unsupervised Dynamics Generalization in Model-Based Reinforcement Learning",
                "abstract": "The generalization of model-based reinforcement learning (MBRL) methods to environments with unseen transition dynamics is an important yet challenging problem. Existing methods try to extract environment-specified information $Z$ from past transition segments to make the dynamics prediction model generalizable to different dynamics. However, because environments are not labelled, the extracted information inevitably contains redundant information unrelated to the dynamics in transition segments and thus fails to maintain a crucial property of $Z$: $Z$ should be similar in the same environment and dissimilar in different ones. As a result, the learned dynamics prediction function will deviate from the true one, which undermines the generalization ability. To tackle this problem, we introduce an interventional prediction module to estimate the probability of two estimated $\\hat{z}_i, \\hat{z}_j$ belonging to the same environment. Furthermore, by utilizing the $Z$'s invariance within a single environment, a relational head is proposed to enforce the similarity between $\\hat{{Z}}$ from the same environment. As a result, the redundant information will be reduced in $\\hat{Z}$. We empirically show that $\\hat{{Z}}$ estimated by our method enjoy less redundant information than previous methods, and such $\\hat{{Z}}$ can significantly reduce dynamics prediction errors and improve the performance of model-based RL methods on zero-shot new environments with unseen dynamics. The codes of this method are available at \\url{https://github.com/CR-Gjx/RIA}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2217924891",
                        "name": "J. Guo"
                    },
                    {
                        "authorId": "29393235",
                        "name": "Mingming Gong"
                    },
                    {
                        "authorId": "2082545536",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", poor transitions can be generated, especially in complex high-dimensional environments [22]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "64a7310c3983d998456b39c4a61eb8be6649609c",
                "externalIds": {
                    "DBLP": "conf/nips/LyuMLL22",
                    "ArXiv": "2206.04745",
                    "DOI": "10.48550/arXiv.2206.04745",
                    "CorpusId": 249605389
                },
                "corpusId": 249605389,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/64a7310c3983d998456b39c4a61eb8be6649609c",
                "title": "Mildly Conservative Q-Learning for Offline Reinforcement Learning",
                "abstract": "Offline reinforcement learning (RL) defines the task of learning from a static logged dataset without continually interacting with the environment. The distribution shift between the learned policy and the behavior policy makes it necessary for the value function to stay conservative such that out-of-distribution (OOD) actions will not be severely overestimated. However, existing approaches, penalizing the unseen actions or regularizing with the behavior policy, are too pessimistic, which suppresses the generalization of the value function and hinders the performance improvement. This paper explores mild but enough conservatism for offline learning while not harming generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD actions are actively trained by assigning them proper pseudo Q values. We theoretically show that MCQ induces a policy that behaves at least as well as the behavior policy and no erroneous overestimation will occur for OOD actions. Experimental results on the D4RL benchmarks demonstrate that MCQ achieves remarkable performance compared with prior work. Furthermore, MCQ shows superior generalization ability when transferring from offline to online, and significantly outperforms baselines. Our code is publicly available at https://github.com/dmksjfl/MCQ.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2008151131",
                        "name": "Jiafei Lyu"
                    },
                    {
                        "authorId": "2125106047",
                        "name": "Xiaoteng Ma"
                    },
                    {
                        "authorId": "2116523082",
                        "name": "Xiu Li"
                    },
                    {
                        "authorId": "2265693",
                        "name": "Zongqing Lu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "427f73ac565dc56bf5476a8388aa71c6b681d428",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-03567",
                    "ArXiv": "2206.03567",
                    "DOI": "10.48550/arXiv.2206.03567",
                    "CorpusId": 249461906
                },
                "corpusId": 249461906,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/427f73ac565dc56bf5476a8388aa71c6b681d428",
                "title": "A Model-Based Reinforcement Learning Approach for PID Design",
                "abstract": "\u2014Proportional-integral-derivative (PID) controller is widely used across various industrial process control applications because of its straightforward implementation. However, it can be challenging to \ufb01ne-tune the PID parameters in practice to achieve robust performance. The paper proposes a model- based reinforcement learning (RL) framework to design PID controllers leveraging the probabilistic inference for learning control (PILCO) method and Kullback-Leibler divergence (KLD). Since PID controllers have a much more interpretable control structure than a network basis function, an optimal policy given by PILCO is transformed into a set of robust PID tuning parameters for underactuated mechanical systems. The presented method is general and can blend with several model-based and model-free algorithms. The performance of the devised PID controllers is demonstrated with simulation studies for a benchmark cart-pole system under disturbances and system parameter uncertainties.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2168867926",
                        "name": "Hozefa Jesawada"
                    },
                    {
                        "authorId": "150928786",
                        "name": "A. Yerudkar"
                    },
                    {
                        "authorId": "144581886",
                        "name": "C. D. Vecchio"
                    },
                    {
                        "authorId": "144333502",
                        "name": "Navdeep M. Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent state-of-the-art approaches in model-based deep reinforcement learning have extended this architecture to perform multi-step rollouts where each transition in the synthesized trajectory is treated as an experience for model-free reinforcement learning (Holland et al., 2018; Janner et al., 2019; Kaiser et al., 2020).",
                "Our MBRL architecture is outlined in algorithm 1, which is similar to the off-policy Dyna-style architecture presented by Holland et al. (2018), Janner et al. (2019) and Kaiser et al. (2020).",
                "Janner et al. (2019) derive bounds on policy improvement using the model, based on the choice of the rollout length and the model\u2019s ability to generalize beyond its training distribution.",
                "\u2026in model-based deep reinforcement learning have extended this architecture to perform multi-step rollouts where each transition in the synthesized trajectory is treated as an experience for model-free reinforcement learning (Holland et al., 2018; Janner et al., 2019; Kaiser et al., 2020).",
                "Following Janner et al. (2019), the transition subnetwork first predicts the difference between the next state and the current state, and then reconstructs the next state using the difference."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f27be557b3258abb48681d3af5442b6344d4c107",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-02380",
                    "ArXiv": "2206.02380",
                    "DOI": "10.48550/arXiv.2206.02380",
                    "CorpusId": 249395358
                },
                "corpusId": 249395358,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f27be557b3258abb48681d3af5442b6344d4c107",
                "title": "Adaptive Rollout Length for Model-Based RL Using Model-Free Deep RL",
                "abstract": "Model-based reinforcement learning promises to learn an optimal policy from fewer interactions with the environment compared to model-free reinforcement learning by learning an intermediate model of the environment in order to predict future interactions. When predicting a sequence of interactions, the rollout length, which limits the prediction horizon, is a critical hyperparameter as accuracy of the predictions diminishes in the regions that are further away from real experience. As a result, with a longer rollout length, an overall worse policy is learned in the long run. Thus, the hyperparameter provides a trade-off between quality and efficiency. In this work, we frame the problem of tuning the rollout length as a meta-level sequential decision-making problem that optimizes the final policy learned by model-based reinforcement learning given a fixed budget of environment interactions by adapting the hyperparameter dynamically based on feedback from the learning process, such as accuracy of the model and the remaining budget of interactions. We use model-free deep reinforcement learning to solve the meta-level decision problem and demonstrate that our approach outperforms common heuristic baselines on two well-known reinforcement learning environments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50841820",
                        "name": "Abhinav Bhatia"
                    },
                    {
                        "authorId": "143640165",
                        "name": "P. Thomas"
                    },
                    {
                        "authorId": "1707550",
                        "name": "S. Zilberstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2) Model-Based (MB): MB trains dynamics and reward models by maximizing the log likelihood of the next state and reward from the offline dataset as recent successful model-based RL algorithms [48, 17].",
                "The model learning process is similar to MBPO [17], which learns an ensemble of probabilistic dynamic models and uses them for short rollouts.",
                "Such errors will accumulate along the trajectory and lead to the so-called compounding error [17, 18], which significantly degrades the performance of model-based value estimation.",
                "Recent model-based RL algorithms [17, 20, 21] use a bootstrap ensemble of dynamic models."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "46f34e6c47cd700b6f7762c221c201d0997d6fc4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-02000",
                    "ArXiv": "2206.02000",
                    "DOI": "10.48550/arXiv.2206.02000",
                    "CorpusId": 249394898
                },
                "corpusId": 249394898,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/46f34e6c47cd700b6f7762c221c201d0997d6fc4",
                "title": "Hybrid Value Estimation for Off-policy Evaluation and Offline Reinforcement Learning",
                "abstract": "Value function estimation is an indispensable subroutine in reinforcement learning, which becomes more challenging in the offline setting. In this paper, we propose Hybrid Value Estimation (HVE) to reduce value estimation error, which trades off bias and variance by balancing between the value estimation from offline data and the learned model. Theoretical analysis discloses that HVE enjoys a better error bound than the direct methods. HVE can be leveraged in both off-policy evaluation and offline reinforcement learning settings. We, therefore, provide two concrete algorithms Off-policy HVE (OPHVE) and Model-based Offline HVE (MOHVE), respectively. Empirical evaluations on MuJoCo tasks corroborate the theoretical claim. OPHVE outperforms other off-policy evaluation methods in all three metrics measuring the estimation effectiveness, while MOHVE achieves better or comparable performance with state-of-the-art offline reinforcement learning algorithms. We hope that HVE could shed some light on further research on reinforcement learning from fixed data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115888294",
                        "name": "Xuefeng Jin"
                    },
                    {
                        "authorId": "2108707316",
                        "name": "Xu-Hui Liu"
                    },
                    {
                        "authorId": "2119326931",
                        "name": "Shengyi Jiang"
                    },
                    {
                        "authorId": "144705629",
                        "name": "Yang Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are other popular model based methods in literature such as MBPO (Janner et al. 2019) and PETS (Chua et al. 2018) but MPC-PSRL is already shown to outperform them in (Fan and Ming 2021, Fig.",
                "There are other popular model based methods in literature such as MBPO (Janner et al. 2019) and PETS (Chua et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "98241cf085a52c5761278ea8eb6c51cffa3a9d38",
                "externalIds": {
                    "DBLP": "conf/aaai/ChakrabortyBTKS23",
                    "ArXiv": "2206.01162",
                    "DOI": "10.48550/arXiv.2206.01162",
                    "CorpusId": 249282244
                },
                "corpusId": 249282244,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/98241cf085a52c5761278ea8eb6c51cffa3a9d38",
                "title": "Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning",
                "abstract": "Model-based approaches to reinforcement learning (MBRL) exhibit favorable performance in practice, but their theoretical guarantees in large spaces are mostly restricted to the setting when transition model is Gaussian or Lipschitz, and demands a posterior estimate whose representational complexity grows unbounded with time. In this work, we develop a novel MBRL method (i) which relaxes the assumptions on the target transition model to belong to a generic family of mixture models; (ii) is applicable to large-scale training by incorporating a compression step such that the posterior estimate consists of a Bayesian coreset of only statistically significant past state-action pairs; and (iii) exhibits a sublinear Bayesian regret.\nTo achieve these results, we adopt an approach based upon Stein's method, which, under a smoothness condition on the constructed posterior and target, allows distributional distance to be evaluated in closed form as the kernelized Stein discrepancy (KSD). The aforementioned compression step is then computed in terms of greedily retaining only those samples which are more than a certain KSD away from the previous model estimate.\nExperimentally, we observe that this approach is competitive with several state-of-the-art RL methodologies, and can achieve up-to 50 percent reduction in wall clock time in some continuous control environments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49081354",
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "authorId": "3387859",
                        "name": "A. S. Bedi"
                    },
                    {
                        "authorId": "2063871",
                        "name": "Alec Koppel"
                    },
                    {
                        "authorId": "1709722",
                        "name": "Brian M. Sadler"
                    },
                    {
                        "authorId": "40070055",
                        "name": "Furong Huang"
                    },
                    {
                        "authorId": "2390456",
                        "name": "Pratap Tokekar"
                    },
                    {
                        "authorId": "1699159",
                        "name": "Dinesh Manocha"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b23911588a1751754738fa9a9d91d6dddf06bb48",
                "externalIds": {
                    "DBLP": "journals/ijrr/PinoskyABAM23",
                    "DOI": "10.1177/02783649221083331",
                    "CorpusId": 249326601
                },
                "corpusId": 249326601,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b23911588a1751754738fa9a9d91d6dddf06bb48",
                "title": "Hybrid control for combining model-based and model-free reinforcement learning",
                "abstract": "We develop an approach to improve the learning capabilities of robotic systems by combining learned predictive models with experience-based state-action policy mappings. Predictive models provide an understanding of the task and the dynamics, while experience-based (model-free) policy mappings encode favorable actions that override planned actions. We refer to our approach of systematically combining model-based and model-free learning methods as hybrid learning. Our approach efficiently learns motor skills and improves the performance of predictive models and experience-based policies. Moreover, our approach enables policies (both model-based and model-free) to be updated using any off-policy reinforcement learning method. We derive a deterministic method of hybrid learning by optimally switching between learning modalities. We adapt our method to a stochastic variation that relaxes some of the key assumptions in the original derivation. Our deterministic and stochastic variations are tested on a variety of robot control benchmark tasks in simulation as well as a hardware manipulation task. We extend our approach for use with imitation learning methods, where experience is provided through demonstrations, and we test the expanded capability with a real-world pick-and-place task. The results show that our method is capable of improving the performance and sample efficiency of learning motor skills in a variety of experimental domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "71065872",
                        "name": "Allison Pinosky"
                    },
                    {
                        "authorId": "144452150",
                        "name": "Ian Abraham"
                    },
                    {
                        "authorId": "40429319",
                        "name": "Alexander Broad"
                    },
                    {
                        "authorId": "1836885",
                        "name": "B. Argall"
                    },
                    {
                        "authorId": "2750574",
                        "name": "T. Murphey"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The proposed method is model-free (see [67]), as compared with model-based methods (see [73])."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7843919e2d5aa639638be44002bb43a298dd9cee",
                "externalIds": {
                    "DOI": "10.1016/j.apenergy.2022.118982",
                    "CorpusId": 216051499
                },
                "corpusId": 216051499,
                "publicationVenue": {
                    "id": "00a6fb8b-7f32-4ae0-a417-8033f5f369f9",
                    "name": "Applied Energy",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Energy"
                    ],
                    "issn": "0306-2619",
                    "url": "https://www.journals.elsevier.com/applied-energy",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/03062619"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7843919e2d5aa639638be44002bb43a298dd9cee",
                "title": "Joint control of manufacturing and onsite microgrid system via novel neural-network integrated reinforcement learning algorithms",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144498346",
                        "name": "Jiaojiao Yang"
                    },
                    {
                        "authorId": "2878626",
                        "name": "Zeyi Sun"
                    },
                    {
                        "authorId": "145428581",
                        "name": "Wenqing Hu"
                    },
                    {
                        "authorId": "1994554585",
                        "name": "Louis Steinmeister"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Janner et al. (2019) [Janner et al.(2019)] propose model-based Policy Optimization (MBPO) algorithm to boost PETS through monotonic improvement at each step and get state-of-the-art performance.",
                "Due to a certain deviation between the established environmental model and the real model, Janner and Fu et al. (2019) [Janner et al.(2019)] proposed a novel modelbased algorithm MBPO to improve PETs.",
                "DynaQ [Peng et al.(2018)], PETS [Chua et al.(2018)], MBPO [Janner et al.(2019)] are powerful model-based reinforcement algorithms in Gym environment.",
                "On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.(2016)]) to solve the portfolio management problem and design a standalone deep RL model. Moreover, Li et al. [Li et al.(2019a)] propose an adaptive deep deterministic RL method (Adaptive DDPG) for some portfolio allocation task, which incorporates optimistic or pessimistic deep RL algorithm based on the influence of prediction error. Through analyzing data from Dow Jones 30 component stocks, the trading strategy outperforms traditional DDPG method [Lillicrap et al.(2015)]. Sutta [Sornmayura(2019)] compares the performance of an AI agent to the results of the buy-and-hold strategy and the expert trader by testing 15 years\u2019 forex data market with a paired t-Test. The findings show that AI can beat the buy & hold strategy and commodity trading advisor in FOREX for both EURUSD and USDJPY currency pairs. However, there are still complex and challenging issues with reinforcement learning for financial environment. First of all, financial marketing data, in most scenarios appear to have complex non-linearity, even complex chaotic behaviors and uncertainties including non-Gaussian noise, which causes finance time series data has distribution shift [Cai and Wei(2020)] over time.",
                "On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.(2016)]) to solve the portfolio management problem and design a standalone deep RL model. Moreover, Li et al. [Li et al.(2019a)] propose an adaptive deep deterministic RL method (Adaptive DDPG) for some portfolio allocation task, which incorporates optimistic or pessimistic deep RL algorithm based on the influence of prediction error. Through analyzing data from Dow Jones 30 component stocks, the trading strategy outperforms traditional DDPG method [Lillicrap et al.(2015)]. Sutta [Sornmayura(2019)] compares the performance of an AI agent to the results of the buy-and-hold strategy and the expert trader by testing 15 years\u2019 forex data market with a paired t-Test.",
                "On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.(2016)]) to solve the portfolio management problem and design a standalone deep RL model.",
                "On the policy gradient side, Kang et al. (2018) [Kang et al.",
                "On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.",
                "On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.(2016)]) to solve the portfolio management problem and design a standalone deep RL model. Moreover, Li et al. [Li et al.(2019a)] propose an adaptive deep deterministic RL method (Adaptive DDPG) for some portfolio allocation task, which incorporates optimistic or pessimistic deep RL algorithm based on the influence of prediction error.",
                "On the policy gradient side, Kang et al. (2018) [Kang et al.(2018)] utilize the state-of-art Asynchronous Advantage Actor-Critic algorithm (A3C [Mnih et al.(2016)]) to solve the portfolio management problem and design a standalone deep RL model. Moreover, Li et al. [Li et al.(2019a)] propose an adaptive deep deterministic RL method (Adaptive DDPG) for some portfolio allocation task, which incorporates optimistic or pessimistic deep RL algorithm based on the influence of prediction error. Through analyzing data from Dow Jones 30 component stocks, the trading strategy outperforms traditional DDPG method [Lillicrap et al.(2015)]. Sutta [Sornmayura(2019)] compares the performance of an AI agent to the results of the buy-and-hold strategy and the expert trader by testing 15 years\u2019 forex data market with a paired t-Test. The findings show that AI can beat the buy & hold strategy and commodity trading advisor in FOREX for both EURUSD and USDJPY currency pairs. However, there are still complex and challenging issues with reinforcement learning for financial environment. First of all, financial marketing data, in most scenarios appear to have complex non-linearity, even complex chaotic behaviors and uncertainties including non-Gaussian noise, which causes finance time series data has distribution shift [Cai and Wei(2020)] over time. Besides, hidden interactions among different agencies can be unpredictably difficult to understand. As it is shown from lots of stock marketing analysis, agencies with large capital investment can sometimes cause dramatic price fluctuation, leading to marketing panic and instability among the public. Therefore, scholars are actively seeking intrinsic mechanics to solve the above problems. On one hand, various data denoise methods are utilized to preprocess the financial time series data. For example, Bao W, et al. (2017) [Bao et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "da5eee7bd6399142748397aa2503ea107db5593c",
                "externalIds": {
                    "ArXiv": "2205.15056",
                    "CorpusId": 249194349
                },
                "corpusId": 249194349,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/da5eee7bd6399142748397aa2503ea107db5593c",
                "title": "Stock Trading Optimization through Model-based Reinforcement Learning with Resistance Support Relative Strength (preprint)",
                "abstract": "Reinforcement learning (RL) is gaining attention by more and more researchers in quantitative finance as the agent-environment interaction framework is aligned with decision making process in many business problems. Most of the current financial applications using RL algorithms are based on model-free method, which still faces stability and adaptivity challenges. As lots of cutting-edge model-based reinforcement learning (MBRL) algorithms mature in applications such as video games or robotics, we design a new approach that leverages resistance and support (RS) level as regularization terms for action in MBRL, to improve the algorithm's efficiency and stability. From the experiment results, we can see RS level, as a market timing technique, enhances the performance of pure MBRL models in terms of various measurements and obtains better profit gain with less riskiness. Besides, our proposed method even resists big drop (less maximum drawdown) during COVID-19 pandemic period when the financial market got unpredictable crisis. Explanations on why control of resistance and support level can boost MBRL is also investigated through numerical experiments, such as loss of actor-critic network and prediction error of the transition dynamical model. It shows that RS indicators indeed help the MBRL algorithms to converge faster at early stage and obtain smaller critic loss as training episodes increase.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151675056",
                        "name": "Huifang Huang"
                    },
                    {
                        "authorId": "2072687794",
                        "name": "Ting Gao"
                    },
                    {
                        "authorId": "2135648424",
                        "name": "Lu Yang"
                    },
                    {
                        "authorId": "2150484860",
                        "name": "Yi Gui"
                    },
                    {
                        "authorId": "2157959267",
                        "name": "Jinqiu Guo"
                    },
                    {
                        "authorId": "40075735",
                        "name": "P. Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is common practice to use the disagreement of the predictions over an ensemble of neural networks as the epistemic uncertainty to guide exploration [27, 15, 36, 19]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "027526624220bc90efb648cdbb90f0a4829f66a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-15064",
                    "ArXiv": "2205.15064",
                    "DOI": "10.48550/arXiv.2205.15064",
                    "CorpusId": 249192417
                },
                "corpusId": 249192417,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/027526624220bc90efb648cdbb90f0a4829f66a3",
                "title": "SEREN: Knowing When to Explore and When to Exploit",
                "abstract": "Ef\ufb01cient reinforcement learning (RL) involves a trade-off between \u201cexploitative\" actions that maximise expected reward and \u201cexplorative\u2019\" ones that sample un-visited states. To encourage exploration, recent approaches proposed adding stochasticity to actions, separating exploration and exploitation phases, or equating reduction in uncertainty with reward. However, these techniques do not neces-sarily offer entirely systematic approaches making this trade-off. Here we in-troduce SE lective R einforcement E xploration N etwork (SEREN) that poses the exploration-exploitation trade-off as a game between an RL agent\u2014 Exploiter , which purely exploits known rewards, and another RL agent\u2014 Switcher , which chooses at which states to activate a pure exploration policy that is trained to minimise system uncertainty and override Exploiter . Using a form of policies known as impulse control , Switcher is able to determine the best set of states to switch to the exploration policy while Exploiter is free to execute its actions everywhere else. We prove that SEREN converges quickly and induces a natural schedule towards pure exploitation. Through extensive empirical studies in both discrete (MiniGrid) and continuous (MuJoCo) control benchmarks, we show that SEREN can be readily combined with existing RL algorithms to yield signi\ufb01cant improvement in performance relative to state-of-the-art algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110962578",
                        "name": "Changmin Yu"
                    },
                    {
                        "authorId": "41127915",
                        "name": "D. Mguni"
                    },
                    {
                        "authorId": "2108821139",
                        "name": "Dong Li"
                    },
                    {
                        "authorId": "144424020",
                        "name": "Aivar Sootla"
                    },
                    {
                        "authorId": "66063792",
                        "name": "J. Wang"
                    },
                    {
                        "authorId": "2167052542",
                        "name": "Neil Burgess"
                    }
                ]
            }
        },
        {
            "contexts": [
                "rollouts to H = 15 to account for increased complexity of the multiagent environments and possible compounding error [31] of the world model."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5d15c5a5bde2ae457038a0432dd6aa1a61227977",
                "externalIds": {
                    "ArXiv": "2205.15023",
                    "DBLP": "journals/corr/abs-2205-15023",
                    "DOI": "10.5555/3535850.3535894",
                    "CorpusId": 248700190
                },
                "corpusId": 248700190,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5d15c5a5bde2ae457038a0432dd6aa1a61227977",
                "title": "Scalable Multi-Agent Model-Based Reinforcement Learning",
                "abstract": "Recent Multi-Agent Reinforcement Learning (MARL) literature has been largely focused on Centralized Training with Decentralized Execution (CTDE) paradigm. CTDE has been a dominant approach for both cooperative and mixed environments due to its capability to efficiently train decentralized policies. While in mixed environments full autonomy of the agents can be a desirable outcome, cooperative environments allow agents to share information to facilitate coordination. Approaches that leverage this technique are usually referred as communication methods, as full autonomy of agents is compromised for better performance. Although communication approaches have shown impressive results, they do not fully leverage this additional information during training phase. In this paper, we propose a new method called MAMBA which utilizes Model-Based Reinforcement Learning (MBRL) to further leverage centralized training in cooperative environments. We argue that communication between agents is enough to sustain a world model for each agent during execution phase while imaginary rollouts can be used for training, removing the necessity to interact with the environment. These properties yield sample efficient algorithm that can scale gracefully with the number of agents. We empirically confirm that MAMBA achieves good performance while reducing the number of interactions with the environment up to an orders of magnitude compared to Model-Free state-of-the-art approaches in challenging domains of SMAC and Flatland.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071321854",
                        "name": "Vladimir Egorov"
                    },
                    {
                        "authorId": "51111583",
                        "name": "A. Shpilman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Value function estimation tends to be accurate when predicting at points in the neighborhood of the present observation [25]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f593dc96b20ce8427182e773e3b2192d707706a8",
                "externalIds": {
                    "DBLP": "journals/ral/LiTTZ22",
                    "ArXiv": "2205.11790",
                    "DOI": "10.48550/arXiv.2205.11790",
                    "CorpusId": 249017535
                },
                "corpusId": 249017535,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f593dc96b20ce8427182e773e3b2192d707706a8",
                "title": "Hierarchical Planning Through Goal-Conditioned Offline Reinforcement Learning",
                "abstract": "Offline Reinforcement learning (RL) has shown potent in many safe-critical tasks in robotics where exploration is risky and expensive. However, it still struggles to acquire skills in temporally extended tasks. In this paper, we study the problem of offline RL for temporally extended tasks. We propose a hierarchical planning framework, consisting of a low-level goal-conditioned RL policy and a high-level goal planner. The low-level policy is trained via offline RL. We improve the offline training to deal with out-of-distribution goals by a perturbed goal sampling process. The high-level planner selects intermediate sub-goals by taking advantages of model-based planning methods. It plans over future sub-goal sequences based on the learned value function of the low-level policy. We adopt a Conditional Variational Autoencoder to sample meaningful high-dimensional sub-goal candidates and to solve the high-level long-term strategy optimization problem. We evaluate our proposed method in long-horizon driving and robot navigation tasks. Experiments show that our method outperforms baselines with different hierarchical designs and other regular planners without hierarchy in these complex tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109377974",
                        "name": "Jinning Li"
                    },
                    {
                        "authorId": "1491105028",
                        "name": "Chen Tang"
                    },
                    {
                        "authorId": "1680165",
                        "name": "M. Tomizuka"
                    },
                    {
                        "authorId": "144267500",
                        "name": "W. Zhan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are also uncertainty-based and model-based methods that regularize the value function or policy with epistemic uncertainty estimated from model or value function (Janner et al., 2019; Yu et al., 2020; Uehara & Sun, 2021; Wu et al., 2021; Zhan et al., 2022; Bai et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "08f8fdb2ac880bb8cc4f4e529f36772e2067dbbd",
                "externalIds": {
                    "DBLP": "conf/iclr/LiZXZLZ23",
                    "ArXiv": "2205.11027",
                    "CorpusId": 256597815
                },
                "corpusId": 256597815,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/08f8fdb2ac880bb8cc4f4e529f36772e2067dbbd",
                "title": "When Data Geometry Meets Deep Function: Generalizing Offline Reinforcement Learning",
                "abstract": "In offline reinforcement learning (RL), one detrimental issue to policy learning is the error accumulation of deep Q function in out-of-distribution (OOD) areas. Unfortunately, existing offline RL methods are often over-conservative, inevitably hurting generalization performance outside data distribution. In our study, one interesting observation is that deep Q functions approximate well inside the convex hull of training data. Inspired by this, we propose a new method, DOGE (Distance-sensitive Offline RL with better GEneralization). DOGE marries dataset geometry with deep function approximators in offline RL, and enables exploitation in generalizable OOD areas rather than strictly constraining policy within data distribution. Specifically, DOGE trains a state-conditioned distance function that can be readily plugged into standard actor-critic methods as a policy constraint. Simple yet elegant, our algorithm enjoys better generalization compared to state-of-the-art methods on D4RL benchmarks. Theoretical analysis demonstrates the superiority of our approach to existing methods that are solely based on data distribution or support constraints.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2136086524",
                        "name": "Jianxiong Li"
                    },
                    {
                        "authorId": "3415564",
                        "name": "Xianyuan Zhan"
                    },
                    {
                        "authorId": "49507262",
                        "name": "Haoran Xu"
                    },
                    {
                        "authorId": "2144104090",
                        "name": "Xiangyu Zhu"
                    },
                    {
                        "authorId": "2108435258",
                        "name": "Jingjing Liu"
                    },
                    {
                        "authorId": "2142985939",
                        "name": "Ya-Qin Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Examples from online RL include [7, 38], which learn one-step observation-based dynamics along with extensions to ensembles [10, 6, 29, 50].",
                "via measures of ensemble-member disagreement [29].",
                "In this work, we use the same FF base-model architecture and training details as MBPO [29]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7db4e3bc4f1cd1a9cd5b2a8381d72d706f07eca2",
                "externalIds": {
                    "ArXiv": "2205.10739",
                    "DBLP": "journals/corr/abs-2205-10739",
                    "DOI": "10.48550/arXiv.2205.10739",
                    "CorpusId": 248987105
                },
                "corpusId": 248987105,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7db4e3bc4f1cd1a9cd5b2a8381d72d706f07eca2",
                "title": "Offline Policy Comparison with Confidence: Benchmarks and Baselines",
                "abstract": "Decision makers often wish to use offline historical data to compare sequential-action policies at various world states. Importantly, computational tools should produce confidence values for such offline policy comparison (OPC) to account for statistical variance and limited data coverage. Nevertheless, there is little work that directly evaluates the quality of confidence values for OPC. In this work, we address this issue by creating benchmarks for OPC with Confidence (OPCC), derived by adding sets of policy comparison queries to datasets from offline reinforcement learning. In addition, we present an empirical evaluation of the risk versus coverage trade-off for a class of model-based baselines. In particular, the baselines learn ensembles of dynamics models, which are used in various ways to produce simulations for answering queries with confidence values. While our results suggest advantages for certain baseline variations, there appears to be significant room for improvement in future work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40135938",
                        "name": "Anurag Koul"
                    },
                    {
                        "authorId": "2482400",
                        "name": "Mariano Phielipp"
                    },
                    {
                        "authorId": "145841336",
                        "name": "Alan Fern"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a4d80430f2c2f1e4f7afba5909f7aa4ae3c87f56",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-09683",
                    "ArXiv": "2205.09683",
                    "DOI": "10.1111/exsy.13205",
                    "CorpusId": 248887620
                },
                "corpusId": 248887620,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a4d80430f2c2f1e4f7afba5909f7aa4ae3c87f56",
                "title": "Dexterous robotic manipulation using deep reinforcement learning and knowledge transfer for complex sparse reward\u2010based tasks",
                "abstract": "This paper describes a deep reinforcement learning (DRL) approach that won Phase 1 of the Real Robot Challenge (RRC) 2021, and then extends this method to a more difficult manipulation task. The RRC consisted of using a TriFinger robot to manipulate a cube along a specified positional trajectory, but with no requirement for the cube to have any specific orientation. We used a relatively simple reward function, a combination of a goal\u2010based sparse reward and a distance reward, in conjunction with Hindsight Experience Replay (HER) to guide the learning of the DRL agent (Deep Deterministic Policy Gradient [DDPG]). Our approach allowed our agents to acquire dexterous robotic manipulation strategies in simulation. These strategies were then deployed on the real robot and outperformed all other competition submissions, including those using more traditional robotic control techniques, in the final evaluation stage of the RRC. Here we extend this method, by modifying the task of Phase 1 of the RRC to require the robot to maintain the cube in a particular orientation, while the cube is moved along the required positional trajectory. The requirement to also orient the cube makes the agent less able to learn the task through blind exploration due to increased problem complexity. To circumvent this issue, we make novel use of a Knowledge Transfer (KT) technique that allows the strategies learned by the agent in the original task (which was agnostic to cube orientation) to be transferred to this task (where orientation matters). KT allowed the agent to learn and perform the extended task in the simulator, which improved the average positional deviation from 0.134 to 0.02\u2009m, and average orientation deviation from 142\u00b0 to 76\u00b0 during evaluation. This KT concept shows good generalization properties and could be applied to any actor\u2010critic learning algorithm.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2183630851",
                        "name": "Qiang Wang"
                    },
                    {
                        "authorId": "2065325865",
                        "name": "Francisco Rold\u00e1n S\u00e1nchez"
                    },
                    {
                        "authorId": "2057594533",
                        "name": "R. McCarthy"
                    },
                    {
                        "authorId": "8399970",
                        "name": "David C\u00f3rdova Bulens"
                    },
                    {
                        "authorId": "145470864",
                        "name": "Kevin McGuinness"
                    },
                    {
                        "authorId": "2137567915",
                        "name": "Noel E. O'Connor"
                    },
                    {
                        "authorId": "1387895034",
                        "name": "M. Wuthrich"
                    },
                    {
                        "authorId": "47804478",
                        "name": "F. Widmaier"
                    },
                    {
                        "authorId": "153125952",
                        "name": "Stefan Bauer"
                    },
                    {
                        "authorId": "144869332",
                        "name": "S. Redmond"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e8a5b8a5dc76de0350a62b7570e47fa6ac4cab15",
                "externalIds": {
                    "ArXiv": "2205.09106",
                    "DBLP": "journals/corr/abs-2205-09106",
                    "DOI": "10.48550/arXiv.2205.09106",
                    "CorpusId": 248862873
                },
                "corpusId": 248862873,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e8a5b8a5dc76de0350a62b7570e47fa6ac4cab15",
                "title": "Reinforcement Learning Based Robust Policy Design for Relay and Power Optimization in DF Relaying Networks",
                "abstract": "In this paper, we study the outage minimization problem in a decode-and-forward cooperative network with relay uncertainty. To reduce the outage probability and improve the quality of service, existing researches usually rely on the assumption of both exact instantaneous channel state information (CSI) and environmental uncertainty. However, it is difficult to obtain perfect instantaneous CSI immediately under practical situations where channel states change rapidly, and the uncertainty in communication environments may not be observed, which makes traditional methods not applicable. Therefore, we turn to reinforcement learning (RL) methods for solutions, which do not need any prior knowledge of underlying channel or assumptions of environmental uncertainty. RL method is to learn from the interaction with communication environment, optimize its action policy, and then propose relay selection and power allocation schemes. We first analyse the robustness of RL action policy by giving the lower bound of the worst-case performance, when RL methods are applied to communication scenarios with environment uncertainty. Then, we propose a robust algorithm for outage probability minimization based on RL. Simulation results reveal that compared with traditional RL methods, our approach has better generalization ability and can improve the worst-case performance by about 6% when evaluated in unseen environments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51118368",
                        "name": "Yuanzhe Geng"
                    },
                    {
                        "authorId": "1754751",
                        "name": "Erwu Liu"
                    },
                    {
                        "authorId": "2151038194",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "2165567230",
                        "name": "Pengcheng Sun"
                    },
                    {
                        "authorId": "15125939",
                        "name": "Binyu Lu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e19e18e7269598130f42d4a3424e035935b59d56",
                "externalIds": {
                    "DOI": "10.1080/17460441.2022.2072288",
                    "CorpusId": 248528114,
                    "PubMed": "35510835"
                },
                "corpusId": 248528114,
                "publicationVenue": {
                    "id": "46314324-f0a7-46d3-8df0-30663e42d679",
                    "name": "Expert Opinion on Drug Discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Expert Opin Drug Discov"
                    ],
                    "issn": "1746-0441",
                    "url": "http://www.expertopin.com/loi/edc",
                    "alternate_urls": [
                        "http://www.tandfonline.com/iedc",
                        "http://www.tandfonline.com/openurl?genre=journal&stitle=iedc20"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e19e18e7269598130f42d4a3424e035935b59d56",
                "title": "Reinforcement learning for systems pharmacology-oriented and personalized drug design",
                "abstract": "ABSTRACT Introduction Many multi-genic systemic diseases such as neurological disorders, inflammatory diseases, and the majority of cancers do not have effective treatments yet. Reinforcement learning powered systems pharmacology is a potentially effective approach to designing personalized therapies for untreatable complex diseases. Areas covered In this survey, state-of-the-art reinforcement learning methods and their latest applications to drug design are reviewed. The challenges on harnessing reinforcement learning for systems pharmacology and personalized medicine are discussed. Potential solutions to overcome the challenges are proposed. Expert opinion In spite of successful application of advanced reinforcement learning techniques to target-based drug discovery, new reinforcement learning strategies are needed to address systems pharmacology-oriented personalized de novo drug design.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151006165",
                        "name": "Ryan K. Tan"
                    },
                    {
                        "authorId": "145595037",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "36523530",
                        "name": "Lei Xie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also tried MBPO [35], but we found that this method takes too much memory and could not finish any test."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b41dce2aa3f526437695a61026d389ba3a15cfd2",
                "externalIds": {
                    "ArXiv": "2205.01758",
                    "DBLP": "journals/corr/abs-2205-01758",
                    "DOI": "10.48550/arXiv.2205.01758",
                    "CorpusId": 244037049
                },
                "corpusId": 244037049,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b41dce2aa3f526437695a61026d389ba3a15cfd2",
                "title": "Differentiable Simulation of Soft Multi-body Systems",
                "abstract": "We present a method for differentiable simulation of soft articulated bodies. Our work enables the integration of differentiable physical dynamics into gradient-based pipelines. We develop a top-down matrix assembly algorithm within Projective Dynamics and derive a generalized dry friction model for soft continuum using a new matrix splitting strategy. We derive a differentiable control framework for soft articulated bodies driven by muscles, joint torques, or pneumatic tubes. The experiments demonstrate that our designs make soft body simulation more stable and realistic compared to other frameworks. Our method accelerates the solution of system identification problems by more than an order of magnitude, and enables efficient gradient-based learning of motion control with soft robots.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51300425",
                        "name": "Yi-Ling Qiao"
                    },
                    {
                        "authorId": "32417403",
                        "name": "Junbang Liang"
                    },
                    {
                        "authorId": "145231047",
                        "name": "V. Koltun"
                    },
                    {
                        "authorId": "2115913075",
                        "name": "Ming C. Lin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As such we need consider the aleatoric uncertainty and the epistemic uncertainty [43]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a0662e9eefc61b53ac61c18448c03293b9f7c0cf",
                "externalIds": {
                    "DOI": "10.1016/j.apenergy.2022.118762",
                    "CorpusId": 247061619
                },
                "corpusId": 247061619,
                "publicationVenue": {
                    "id": "00a6fb8b-7f32-4ae0-a417-8033f5f369f9",
                    "name": "Applied Energy",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Energy"
                    ],
                    "issn": "0306-2619",
                    "url": "https://www.journals.elsevier.com/applied-energy",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/03062619"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a0662e9eefc61b53ac61c18448c03293b9f7c0cf",
                "title": "Model-augmented safe reinforcement learning for Volt-VAR control in power distribution networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "6959558",
                        "name": "Yuanqi Gao"
                    },
                    {
                        "authorId": "2159070",
                        "name": "N. Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To generate the synthetic data, MBPO performs k-step rollouts in M\u0302 starting from states s \u2208 D, and adds this data to D\nM\u0302 .",
                "We train the policy with an actor-critic algorithm using synthetic data generated from the model in addition to data sampled from the dataset, similar to Dyna [55] and a number of recent methods [18, 68, 21, 67].",
                "Following previous approaches [68, 67, 18] we store data in D T\u0302\u03c6 in a first in, first out manner so that only data generated from recent iterations is stored in D T\u0302\u03c6 .",
                "MBPO utilises a standard actor-critic RL algorithm.",
                "In line with existing works [68, 67, 5], we use the model-based policy optimisation (MBPO) [18]\napproach to learn the optimal policy for M\u0302 .",
                "Thus, naive policy optimisation on a learned model in the offline setting can result in model exploitation [18, 27, 48].",
                "In line with existing works [68, 67, 5], we use the model-based policy optimisation (MBPO) [18] approach to learn the optimal policy for M\u0302 ."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "447ad78823fbf3369e72e8d1c36467f76f8dc427",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-12581",
                    "ArXiv": "2204.12581",
                    "DOI": "10.48550/arXiv.2204.12581",
                    "CorpusId": 248405675
                },
                "corpusId": 248405675,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/447ad78823fbf3369e72e8d1c36467f76f8dc427",
                "title": "RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning",
                "abstract": "Offline reinforcement learning (RL) aims to find performant policies from logged data without further environment interaction. Model-based algorithms, which learn a model of the environment from the dataset and perform conservative policy optimisation within that model, have emerged as a promising approach to this problem. In this work, we present Robust Adversarial Model-Based Offline RL (RAMBO), a novel approach to model-based offline RL. We formulate the problem as a two-player zero sum game against an adversarial environment model. The model is trained to minimise the value function while still accurately predicting the transitions in the dataset, forcing the policy to act conservatively in areas not covered by the dataset. To approximately solve the two-player game, we alternate between optimising the policy and adversarially optimising the model. The problem formulation that we address is theoretically grounded, resulting in a probably approximately correct (PAC) performance guarantee and a pessimistic value function which lower bounds the value function in the true environment. We evaluate our approach on widely studied offline RL benchmarks, and demonstrate that it outperforms existing state-of-the-art baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51300315",
                        "name": "Marc Rigter"
                    },
                    {
                        "authorId": "145350537",
                        "name": "Bruno Lacerda"
                    },
                    {
                        "authorId": "2072387078",
                        "name": "N. Hawes"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "56f77ccf6f57928ee790d2b66f713541c15ae794",
                "externalIds": {
                    "ArXiv": "2204.12026",
                    "DBLP": "journals/corr/abs-2204-12026",
                    "DOI": "10.48550/arXiv.2204.12026",
                    "CorpusId": 248391799
                },
                "corpusId": 248391799,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/56f77ccf6f57928ee790d2b66f713541c15ae794",
                "title": "BATS: Best Action Trajectory Stitching",
                "abstract": "The problem of offline reinforcement learning focuses on learning a good policy from a log of environment interactions. Past efforts for developing algorithms in this area have revolved around introducing constraints to online reinforcement learning algorithms to ensure the actions of the learned policy are constrained to the logged data. In this work, we explore an alternative approach by planning on the fixed dataset directly. Specifically, we introduce an algorithm which forms a tabular Markov Decision Process (MDP) over the logged data by adding new transitions to the dataset. We do this by using learned dynamics models to plan short trajectories between states. Since exact value iteration can be performed on this constructed MDP, it becomes easy to identify which trajectories are advantageous to add to the MDP. Crucially, since most transitions in this MDP come from the logged data, trajectories from the MDP can be rolled out for long periods with confidence. We prove that this property allows one to make upper and lower bounds on the value function up to appropriate distance metrics. Finally, we demonstrate empirically how algorithms that uniformly constrain the learned policy to the entire dataset can result in unwanted behavior, and we show an example in which simply behavior cloning the optimal policy of the MDP created by our algorithm avoids this problem.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2306889",
                        "name": "I. Char"
                    },
                    {
                        "authorId": "49061544",
                        "name": "Viraj Mehta"
                    },
                    {
                        "authorId": "9542787",
                        "name": "A. Villaflor"
                    },
                    {
                        "authorId": "31717323",
                        "name": "J. Dolan"
                    },
                    {
                        "authorId": "1753432",
                        "name": "J. Schneider"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1ee71262c4525bca44c6da5b8e1321a67a484953",
                "externalIds": {
                    "DBLP": "journals/finr/YuW22",
                    "PubMedCentral": "9083362",
                    "DOI": "10.3389/fnbot.2022.861825",
                    "CorpusId": 248398508,
                    "PubMed": "35548780"
                },
                "corpusId": 248398508,
                "publicationVenue": {
                    "id": "de454aec-8c73-4737-bb1f-5231453ca8fa",
                    "name": "Frontiers in Neurorobotics",
                    "type": "journal",
                    "alternate_names": [
                        "Front Neurorobotics"
                    ],
                    "issn": "1662-5218",
                    "url": "https://www.frontiersin.org/journals/neurorobotics#articles",
                    "alternate_urls": [
                        "http://www.frontiersin.org/neurorobotics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1ee71262c4525bca44c6da5b8e1321a67a484953",
                "title": "Dexterous Manipulation for Multi-Fingered Robotic Hands With Reinforcement Learning: A Review",
                "abstract": "With the increasing demand for the dexterity of robotic operation, dexterous manipulation of multi-fingered robotic hands with reinforcement learning is an interesting subject in the field of robotics research. Our purpose is to present a comprehensive review of the techniques for dexterous manipulation with multi-fingered robotic hands, such as the model-based approach without learning in early years, and the latest research and methodologies focused on the method based on reinforcement learning and its variations. This work attempts to summarize the evolution and the state of the art in this field and provide a summary of the current challenges and future directions in a way that allows future researchers to understand this field.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117822360",
                        "name": "Chunmiao Yu"
                    },
                    {
                        "authorId": "2155304170",
                        "name": "Peng Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9dac6b8ed81d536f41465eabe525c3351fe4ccdd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-09418",
                    "ArXiv": "2204.09418",
                    "DOI": "10.48550/arXiv.2204.09418",
                    "CorpusId": 248266550
                },
                "corpusId": 248266550,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9dac6b8ed81d536f41465eabe525c3351fe4ccdd",
                "title": "Mingling Foresight with Imagination: Model-Based Cooperative Multi-Agent Reinforcement Learning",
                "abstract": "Recently, model-based agents have achieved better performance than model-free ones using the same computational budget and training time in single-agent environments. However, due to the complexity of multi-agent systems, it is tough to learn the model of the environment. The significant compounding error may hinder the learning process when model-based methods are applied to multi-agent tasks. This paper proposes an implicit model-based multi-agent reinforcement learning method based on value decomposition methods. Under this method, agents can interact with the learned virtual environment and evaluate the current state value according to imagined future states in the latent space, making agents have the foresight. Our approach can be applied to any multi-agent value decomposition method. The experimental results show that our method improves the sample efficiency in different partially observable Markov decision process domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153008118",
                        "name": "Zhiwei Xu"
                    },
                    {
                        "authorId": "2115499783",
                        "name": "Dapeng Li"
                    },
                    {
                        "authorId": "2119453124",
                        "name": "Bin Zhang"
                    },
                    {
                        "authorId": "2162961253",
                        "name": "Yuan Zhan"
                    },
                    {
                        "authorId": "153802746",
                        "name": "Yunru Bai"
                    },
                    {
                        "authorId": "2067827377",
                        "name": "Guoliang Fan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2021) propose to improve a model-based RL method MBPO (Janner et al., 2019) by augmenting the rollout samples using data augmentation that relies on the Jacobian from the differentiable simulator.",
                "The core idea in DYNA was extended by STEVE (Buckman et al., 2018) and MBPO (Janner et al., 2019), which use interpolation between different horizon predictions compared to a single step roll-out in DYNA.",
                "Notably, as noted by Janner et al. (2019), empirically the one-step rollout is a very strong baseline to beat in part because error in model can undermine the advantage from model-based data-augmentation.",
                "On the other side, model-based RL methods (Kurutach et al., 2018; Janner et al., 2019) have been proposed to learn an approximated dynamics model from little experience and then fully exploit the learned dynamics model during policy learning.",
                "SE-MBPO: Qiao et al. (2021) propose to improve a model-based RL method MBPO (Janner et al., 2019) by augmenting the rollout samples using data augmentation that relies on the Jacobian from the differentiable simulator.",
                "To further illustrate the advantage of our method over model-based RL, we conduct the experiments to compare to Model-Based Policy Optimization (MBPO) (Janner et al., 2019).",
                ", 2018) and MBPO (Janner et al., 2019), which use interpolation between different horizon predictions compared to a single step roll-out in DYNA."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "efbc2c6306ff1f3bfa282fc62f8467764fd41c25",
                "externalIds": {
                    "DBLP": "conf/iclr/0028MNRMGM22",
                    "ArXiv": "2204.07137",
                    "DOI": "10.48550/arXiv.2204.07137",
                    "CorpusId": 248178090
                },
                "corpusId": 248178090,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/efbc2c6306ff1f3bfa282fc62f8467764fd41c25",
                "title": "Accelerated Policy Learning with Parallel Differentiable Simulation",
                "abstract": "Deep reinforcement learning can generate complex control policies, but requires large amounts of training data to work effectively. Recent work has attempted to address this issue by leveraging differentiable simulators. However, inherent problems such as local minima and exploding/vanishing numerical gradients prevent these methods from being generally applied to control tasks with complex contact-rich dynamics, such as humanoid locomotion in classical RL benchmarks. In this work we present a high-performance differentiable simulator and a new policy learning algorithm (SHAC) that can effectively leverage simulation gradients, even in the presence of non-smoothness. Our learning algorithm alleviates problems with local minima through a smooth critic function, avoids vanishing/exploding gradients through a truncated learning window, and allows many physical environments to be run in parallel. We evaluate our method on classical RL control tasks, and show substantial improvements in sample efficiency and wall-clock time over state-of-the-art RL and differentiable simulation-based algorithms. In addition, we demonstrate the scalability of our method by applying it to the challenging high-dimensional problem of muscle-actuated locomotion with a large action space, achieving a greater than 17x reduction in training time over the best-performing established RL algorithm.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145754514",
                        "name": "Jie Xu"
                    },
                    {
                        "authorId": "79875630",
                        "name": "Viktor Makoviychuk"
                    },
                    {
                        "authorId": "5046361",
                        "name": "Yashraj S. Narang"
                    },
                    {
                        "authorId": "153279354",
                        "name": "Fabio Ramos"
                    },
                    {
                        "authorId": "1752521",
                        "name": "W. Matusik"
                    },
                    {
                        "authorId": "2054554660",
                        "name": "Animesh Garg"
                    },
                    {
                        "authorId": "46637939",
                        "name": "M. Macklin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For instance, the exploitation of model inaccuracies by policy optimizers has been investigated in the literature [25], [50], [51].",
                "Suggested strategies are the use of probabilistic ensembles [25], [27], [52]\u2013[54], shorter task horizons [51] and denoising autoencoders [37]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "af622a44617bcc8253887d2e955ec104ae718ddd",
                "externalIds": {
                    "DBLP": "conf/icra/SukhijaKZZCKC23",
                    "ArXiv": "2204.04558",
                    "DOI": "10.1109/ICRA48891.2023.10161574",
                    "CorpusId": 252439104
                },
                "corpusId": 252439104,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/af622a44617bcc8253887d2e955ec104ae718ddd",
                "title": "Gradient-Based Trajectory Optimization With Learned Dynamics",
                "abstract": "Trajectory optimization methods have achieved an exceptional level of performance on real-world robots in recent years. These methods heavily rely on accurate analytical models of the dynamics, yet some aspects of the physical world can only be captured to a limited extent. An alternative approach is to leverage machine learning techniques to learn a differentiable dynamics model of the system from data. In this work, we use trajectory optimization and model learning for performing highly dynamic and complex tasks with robotic systems in absence of accurate analytical models of the dynamics. We show that a neural network can model highly nonlinear behaviors accurately for large time horizons, from data collected in only 25 minutes of interactions on two distinct robots: (i) the Boston Dynamics Spot and an (ii) RC car. Furthermore, we use the gradients of the neural network to perform gradient-based trajectory optimization. In our hardware experiments, we demonstrate that our learned model can represent complex dynamics for both the Spot and Radio-controlled (RC) car, and gives good performance in combination with trajectory optimization methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151000167",
                        "name": "Bhavya Sukhija"
                    },
                    {
                        "authorId": "2162047105",
                        "name": "Nathanael Kohler"
                    },
                    {
                        "authorId": "144022996",
                        "name": "Miguel Zamora"
                    },
                    {
                        "authorId": "49785124",
                        "name": "Simon Zimmermann"
                    },
                    {
                        "authorId": "12646136",
                        "name": "Sebastian Curi"
                    },
                    {
                        "authorId": "153243248",
                        "name": "A. Krause"
                    },
                    {
                        "authorId": "1783776",
                        "name": "Stelian Coros"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Janner et al. (2019) use fully connected neural networks with four hidden layers and 200 neurons per layer.",
                "\u2026models and losses for future work but highlight that significant performance might be gained by finding better tradeoffs than those discussed in Janner et al. (2019), especially when comparing deterministic and probabilistic models (compare Appendix F) as well as value-aware and value-agnostic\u2026",
                "On the Ant-v2, we see small performance improvements above the results reported by Janner et al. (2019) and Pineda et al. (2021).",
                "To assure a fair comparison we used the hyperparameters provided by Janner et al. (2019) for all experiments with our approach and the NLL loss function used for the baseline.",
                "As a deep learning based Dyna algorithm, we chose Model-based Policy Optimization (MBPO) (Janner et al., 2019) and ran all of our experiments using the implementation provided by Pineda et al.",
                "As a deep learning based Dyna algorithm, we chose Model-based Policy Optimization (MBPO) (Janner et al., 2019) and ran all of our experiments using the implementation provided by Pineda et al. (2021).",
                "One of the core problems of model-based policy learning methods however is that the accuracy of the model directly influences the quality of the learned policy or plan (Schneider, 1997; Kearns & Singh, 2002; Ross & Bagnell, 2012; Talvitie, 2017; Luo et al., 2019; Janner et al., 2019).",
                "F.1 ABLATION EXPERIMENT WITH DETERMINISTIC MODELS\nIn our experiments in the Hopper domain, we used probabilistic models following Janner et al. (2019)."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f0e91a012f447a3ba6692632abbc272cee41d1a0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-01464",
                    "ArXiv": "2204.01464",
                    "DOI": "10.48550/arXiv.2204.01464",
                    "CorpusId": 247939701
                },
                "corpusId": 247939701,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f0e91a012f447a3ba6692632abbc272cee41d1a0",
                "title": "Value Gradient weighted Model-Based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the performance of MBRL in challenging settings, such as small model capacity and the presence of distracting state dimensions. We analyze both MLE and value-aware approaches and demonstrate how they fail to account for exploration and the behavior of function approximation when learning value-aware models and highlight the additional goals that must be met to stabilize optimization in the deep learning setting. We verify our analysis by showing that our loss function is able to achieve high returns on the Mujoco benchmark suite while being more robust than maximum likelihood based approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1387979639",
                        "name": "C. Voelcker"
                    },
                    {
                        "authorId": "2161339569",
                        "name": "Victor Liao"
                    },
                    {
                        "authorId": "2054554660",
                        "name": "Animesh Garg"
                    },
                    {
                        "authorId": "5689899",
                        "name": "Amir-massoud Farahmand"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "18c295cc2552e269201d774e71a5cf91a2ec557a",
                "externalIds": {
                    "DBLP": "journals/ral/BimboMD22",
                    "DOI": "10.1109/LRA.2022.3152244",
                    "CorpusId": 246941084
                },
                "corpusId": 246941084,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/18c295cc2552e269201d774e71a5cf91a2ec557a",
                "title": "Force-Based Simultaneous Mapping and Object Reconstruction for Robotic Manipulation",
                "abstract": "This letter presents an algorithm to simultaneously reconstruct the geometry of an unknown object and its environment via physical interactions. Applications involving highly cluttered or occluded workspaces prevent the effective use of vision. To address some of the challenges that arise, we propose an approach that instead utilizes force and torque measurements at the robot end-effector to solve for possible contact locations and probabilistically determine the occupancy likelihood on a 3D map. Our procedure constructs two occupancy maps: one fixed that represents the environment and another map that moves with the robot end-effector and reconstructs the grasped object shape, where each map informs the probability updates on the other. The algorithm is applied and tested on two scenarios: retrieving a tangled object from a scene and reconstructing the geometry of an object. We compare the results against a configuration space planner and a reinforcement learning algorithm, with our method requiring fewer collisions with the environment to extract the object.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50880326",
                        "name": "Jo\u00e3o Bimbo"
                    },
                    {
                        "authorId": "144687315",
                        "name": "A. S. Morgan"
                    },
                    {
                        "authorId": "1797110",
                        "name": "A. Dollar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[8] is to use shorter rollout horizons with learned models.",
                "The learned models have been used for data augmentation [8, 9, 11], improving the value targets [2, 4, 12, 13], improving the policy gradient [7] or any combination thereof."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a07e30a42ed114d353aed2eac62e2b23fb749bc9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-14660",
                    "ArXiv": "2203.14660",
                    "DOI": "10.48550/arXiv.2203.14660",
                    "CorpusId": 247762265
                },
                "corpusId": 247762265,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a07e30a42ed114d353aed2eac62e2b23fb749bc9",
                "title": "Revisiting Model-based Value Expansion",
                "abstract": "Model-based value expansion methods promise to improve the quality of value function targets and, thereby, the effectiveness of value function learning. However, to date, these methods are being outperformed by Dyna-style algorithms with conceptually simpler 1-step value function targets. This shows that in practice, the theoretical justification of value expansion does not seem to hold. We provide a thorough empirical study to shed light on the causes of failure of value expansion methods in practice which is believed to be the compounding model error. By leveraging GPU based physics simulators, we are able to efficiently use the true dynamics for analysis inside the model-based reinforcement learning loop. Performing extensive comparisons between true and learned dynamics sheds light into this black box. This paper provides a better understanding of the actual problems in value expansion. We provide future directions of research by empirically testing the maximum theoretical performance of current approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1751630928",
                        "name": "Daniel Palenicek"
                    },
                    {
                        "authorId": "49199170",
                        "name": "M. Lutter"
                    },
                    {
                        "authorId": "2107720654",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More recent Model-Based RL algorithms such as MBPO [46] or PETS [47] may offer a better solution to solving our control problem, with their CMA-ES ensemble maximum likelihood [48] based model learning procedure."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "79f40dde059f244b83aed37f1712ac6f2b87295b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-10910",
                    "ArXiv": "2203.10910",
                    "DOI": "10.48550/arXiv.2203.10910",
                    "CorpusId": 247595098
                },
                "corpusId": 247595098,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/79f40dde059f244b83aed37f1712ac6f2b87295b",
                "title": "Quad2Plane: An Intermediate Training Procedure for Online Exploration in Aerial Robotics via Receding Horizon Control",
                "abstract": "Data driven robotics relies upon accurate real-world representations to learn useful policies. Despite our best-efforts, zero-shot sim-to-real transfer is still an unsolved problem, and we often need to allow our agents to explore online to learn useful policies for a given task. For many applications of field robotics online exploration is prohibitively expensive and dangerous, this is especially true in fixed-wing aerial robotics. To address these challenges we offer an intermediary solution for learning in field robotics. We investigate the use of dissimilar platform vehicle for learning and offer a procedure to mimic the behavior of one vehicle with another. We specifically consider the problem of training fixed-wing aircraft, an expensive and dangerous vehicle type, using a multi-rotor host platform. Using a Model Predictive Control approach, we design a controller capable of mimicking another vehicles behavior in both simulation and the real-world.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2148745482",
                        "name": "Alexander D. Quessy"
                    },
                    {
                        "authorId": "2056772487",
                        "name": "T. Richardson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MAMBPO follows the principle that only the generated data which is close to the real data can be leveraged for policy improvement.",
                "MAMBPO [Willemsen et al., 2021] investigates model-based methods in Centralized Training Decentralized Execution (CTDE) paradigm where agents only observe local observations when making decisions, and global information is accessible when agents improving their policies.",
                "MAMBPO can indeed improve the sample efficiency empirically, and alleviate the partial observability and non-stationarity problems by adopting the CTDE paradigm.",
                "Dyna-style methods in single-agent scenarios show their sample efficiency both practically and theoretically [Janner et al., 2019].",
                "Similar to MBPO [Janner et al., 2019], the model rollouts begin from the experienced states, and the data generated from the model within k steps is used to improve the policies."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "24ed28d44c33de145b05217814d678e3486acbc5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-10603",
                    "ArXiv": "2203.10603",
                    "DOI": "10.48550/arXiv.2203.10603",
                    "CorpusId": 247594411
                },
                "corpusId": 247594411,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/24ed28d44c33de145b05217814d678e3486acbc5",
                "title": "Model-based Multi-agent Reinforcement Learning: Recent Progress and Prospects",
                "abstract": "Significant advances have recently been achieved in Multi-Agent Reinforcement Learning (MARL) which tackles sequential decision-making problems involving multiple participants. However, MARL requires a tremendous number of samples for effective training. On the other hand, model-based methods have been shown to achieve provable advantages of sample efficiency. However, the attempts of model-based methods to MARL have just started very recently. This paper presents a review of the existing research on model-based MARL, including theoretical analyses, algorithms, and applications, and analyzes the advantages and potential of model-based MARL. Specifically, we provide a detailed taxonomy of the algorithms and point out the pros and cons for each algorithm according to the challenges inherent to multi-agent scenarios. We also outline promising directions for future development of this field.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2178008",
                        "name": "Xihuai Wang"
                    },
                    {
                        "authorId": "2116706397",
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "authorId": "2155042473",
                        "name": "Weinan Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, the Model-based Policy Optimization (MBPO) algorithm uses short horizons to avoid the compounding error problem (Janner et al., 2019), but there-in loses out on a lot of the potential for having a learned model and anticipating the future.",
                "The field of model-based reinforcement learning (MBRL) showcases this process and has been used to solve many robotic tasks by iteratively learning a black-box model (Deisenroth and Rasmussen, 2011; Chua et al., 2018; Nagabandi et al., 2019; Janner et al., 2019).",
                "\u2026successes in model-based reinforcement learning are one-step dynamics models; where they have been used for online model predictive control (MPC) (Chua et al., 2018; Nagabandi et al., 2019; Lambert et al., 2019) or value-estimation and offline rollouts of imagined policies (Janner et al., 2019).",
                "Different model parametrizations, particularly ensembles and probabilistic loss functions, have been shown to improve the peak performance of MBRL algorithms (Chua et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0ea5518a745a3c1f6fc53d2e7ec5b0bd016cb8d5",
                "externalIds": {
                    "ArXiv": "2203.09637",
                    "DBLP": "journals/corr/abs-2203-09637",
                    "DOI": "10.48550/arXiv.2203.09637",
                    "CorpusId": 247594271
                },
                "corpusId": 247594271,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0ea5518a745a3c1f6fc53d2e7ec5b0bd016cb8d5",
                "title": "Investigating Compounding Prediction Errors in Learned Dynamics Models",
                "abstract": "Accurately predicting the consequences of agents' actions is a key prerequisite for planning in robotic control. Model-based reinforcement learning (MBRL) is one paradigm which relies on the iterative learning and prediction of state-action transitions to solve a task. Deep MBRL has become a popular candidate, using a neural network to learn a dynamics model that predicts with each pass from high-dimensional states to actions. These\"one-step\"predictions are known to become inaccurate over longer horizons of composed prediction - called the compounding error problem. Given the prevalence of the compounding error problem in MBRL and related fields of data-driven control, we set out to understand the properties of and conditions causing these long-horizon errors. In this paper, we explore the effects of subcomponents of a control problem on long term prediction error: including choosing a system, collecting data, and training a model. These detailed quantitative studies on simulated and real-world data show that the underlying dynamics of a system are the strongest factor determining the shape and magnitude of prediction error. Given a clearer understanding of compounding prediction error, researchers can implement new types of models beyond\"one-step\"that are more useful for control.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2052363815",
                        "name": "Nathan Lambert"
                    },
                    {
                        "authorId": "143648343",
                        "name": "K. Pister"
                    },
                    {
                        "authorId": "35159852",
                        "name": "R. Calandra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "LOOP has been shown to outperform a number of model-based methods, e.g., MBPO (Janner et al., 2019) and POLO (Lowrey et al., 2019)) on select MuJoCo tasks.",
                "A common paradigm is to learn a model of the environment that can be used for planning (Ebert et al., 2018; Zhang et al., 2018; Janner et al., 2019; Hafner et al., 2019; Lowrey et al., 2019; Kaiser et al., 2020; Bhardwaj et al., 2020; Yu et al., 2020; Schrittwieser et al., 2020; Nguyen et al.,\u2026",
                "A common paradigm is to learn a model of the environment that can be used for planning (Ebert et al., 2018; Zhang et al., 2018; Janner et al., 2019; Hafner et al., 2019; Lowrey et al., 2019; Kaiser et al., 2020; Bhardwaj et al., 2020; Yu et al., 2020; Schrittwieser et al., 2020; Nguyen et al., 2021) or for training a model-free algorithm with generated data (Pong et al.",
                "Unlike prior work (Ha & Schmidhuber, 2018; Janner et al., 2019; Hafner et al., 2019; 2020; Sikchi et al., 2020), we find it sufficient to implement all components of TOLD as purely deterministic MLPs, i.e., without RNN gating mechanisms nor probabilistic models.",
                "To provide a rich learning signal for model learning, prior work on model-based RL commonly learn to directly predict future states or pixels (Ha & Schmidhuber, 2018; Janner et al., 2019; Lowrey et al., 2019; Kaiser et al., 2020; Sikchi et al., 2020).",
                "Planning is a powerful approach to such sequential decision making problems, and has achieved tremendous success in application areas such as game-playing (Kaiser et al., 2020; Schrittwieser et al., 2020) and continuous control (Tassa et al., 2012; Chua et al., 2018; Janner et al., 2019).",
                "This ablation makes our method more similar to prior work on model-based RL from states (Janner et al., 2019; Lowrey et al., 2019; Sikchi et al., 2020; Argenson & DulacArnold, 2021).",
                "Concretely, prior work on model-based methods can largely be subdivided into two directions, each exploiting key advantages of model-based learning: (i) planning, which is advantageous over a learned policy, but it can be prohibitively expensive to plan over long horizons (Janner et al., 2019; Lowrey et al., 2019; Hafner et al., 2019; Argenson & DulacArnold, 2021); and (ii) using a learned model to improve \u2020Equal advising.",
                "\u2026model-based learning: (i) planning, which is advantageous over a learned policy, but it can be prohibitively expensive to plan over long horizons (Janner et al., 2019; Lowrey et al., 2019; Hafner et al., 2019; Argenson & DulacArnold, 2021); and (ii) using a learned model to improve\n\u2020Equal\u2026",
                "Unlike prior work (Ha & Schmidhuber, 2018; Janner et al., 2019; Hafner et al., 2019; 2020; Sikchi et al., 2020), we find it sufficient to implement all components of TOLD as purely deterministic MLPs, i."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d46eef8b0d46cfdebefe9941a0f60aeeed31ded0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-04955",
                    "ArXiv": "2203.04955",
                    "DOI": "10.48550/arXiv.2203.04955",
                    "CorpusId": 247318709
                },
                "corpusId": 247318709,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d46eef8b0d46cfdebefe9941a0f60aeeed31ded0",
                "title": "Temporal Difference Learning for Model Predictive Control",
                "abstract": "Data-driven model predictive control has two key advantages over model-free methods: a potential for improved sample efficiency through model learning, and better performance as computational budget for planning increases. However, it is both costly to plan over long horizons and challenging to obtain an accurate model of the environment. In this work, we combine the strengths of model-free and model-based methods. We use a learned task-oriented latent dynamics model for local trajectory optimization over a short horizon, and use a learned terminal value function to estimate long-term return, both of which are learned jointly by temporal difference learning. Our method, TD-MPC, achieves superior sample efficiency and asymptotic performance over prior work on both state and image-based continuous control tasks from DMControl and Meta-World. Code and video results are available at https://nicklashansen.github.io/td-mpc.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1491707104",
                        "name": "Nicklas Hansen"
                    },
                    {
                        "authorId": "122024152",
                        "name": "Xiaolong Wang"
                    },
                    {
                        "authorId": "2093560213",
                        "name": "H. Su"
                    }
                ]
            }
        },
        {
            "contexts": [
                "and favor decisions that are consistent across the models [19],",
                "With these modified reward functions in hand, we can then use classic model-based approaches to tackle offline RL problems, such as Dyna-based methods that sample transitions from the model to train a model-free algorithm [19], [34]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "61e7a3d5606043594a8ce377870479f77a6b58c2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-01387",
                    "ArXiv": "2203.01387",
                    "DOI": "10.1109/TNNLS.2023.3250269",
                    "CorpusId": 247222916,
                    "PubMed": "37030754"
                },
                "corpusId": 247222916,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/61e7a3d5606043594a8ce377870479f77a6b58c2",
                "title": "A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems",
                "abstract": "With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field using a unified notation as well as a review of existing benchmarks' properties and shortcomings. Additionally, we provide a figure that summarizes the performance of each method and class of methods on different dataset properties, equipping researchers with the tools to decide which type of algorithm is best suited for the problem at hand and identify which classes of algorithms look the most promising. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157044946",
                        "name": "Rafael Figueiredo Prudencio"
                    },
                    {
                        "authorId": "31720494",
                        "name": "M. Maximo"
                    },
                    {
                        "authorId": "2210164",
                        "name": "E. Colombini"
                    }
                ]
            }
        },
        {
            "contexts": [
                "offline [4], [14] and repeat the data collection and learning loop until the task is solved [9], [11]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ff7754f45172edaf5c5a773ab3fc2a1438c7cf09",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-13638",
                    "ArXiv": "2202.13638",
                    "DOI": "10.1109/icra46639.2022.9811876",
                    "CorpusId": 247158230
                },
                "corpusId": 247158230,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ff7754f45172edaf5c5a773ab3fc2a1438c7cf09",
                "title": "GPU-Accelerated Policy Optimization via Batch Automatic Differentiation of Gaussian Processes for Real-World Control",
                "abstract": "The ability of Gaussian processes (GPs) to predict the behavior of dynamical systems as a more sample-efficient alternative to parametric models seems promising for real-world robotics research. However, the computational complexity of GPs has made policy search a highly time and memory consuming process that has not been able to scale to larger problems. In this work, we develop a policy optimization method by leveraging fast predictive sampling methods to process batches of trajectories in every forward pass, and compute gradient updates over policy parameters by automatic differentiation of Monte Carlo evaluations, all on GPU. We demonstrate the effectiveness of our approach in training policies on a set of reference-tracking control experiments with a heavy-duty machine. Benchmark results show a significant speedup over exact methods and showcase the scalability of our method to larger policy networks, longer horizons, and up to thousands of trajectories with a sublinear drop in speed.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1576543811",
                        "name": "Abdolreza Taheri"
                    },
                    {
                        "authorId": "34906504",
                        "name": "J. Pajarinen"
                    },
                    {
                        "authorId": "2099297",
                        "name": "R. Ghabcheloo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, such model-based methods may suffer from additional computation costs and may perform suboptimally in complex environments (Chua et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "23abe79046d7f7b430d5d21b6a93598d0aa1b9c2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-11566",
                    "ArXiv": "2202.11566",
                    "CorpusId": 247058767
                },
                "corpusId": 247058767,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/23abe79046d7f7b430d5d21b6a93598d0aa1b9c2",
                "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning",
                "abstract": "Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy algorithms to offline RL usually fails due to the extrapolation error caused by the out-of-distribution (OOD) actions. Previous methods tackle such problem by penalizing the Q-values of OOD actions or constraining the trained policy to be close to the behavior policy. Nevertheless, such methods typically prevent the generalization of value functions beyond the offline data and also lack precise characterization of OOD data. In this paper, we propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without explicit policy constraints. Specifically, PBRL conducts uncertainty quantification via the disagreement of bootstrapped Q-functions, and performs pessimistic updates by penalizing the value function based on the estimated uncertainty. To tackle the extrapolating error, we further propose a novel OOD sampling method. We show that such OOD sampling and pessimistic bootstrapping yields provable uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning for PBRL. Extensive experiments on D4RL benchmark show that PBRL has better performance compared to the state-of-the-art algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150944133",
                        "name": "Chenjia Bai"
                    },
                    {
                        "authorId": "2151976110",
                        "name": "Lingxiao Wang"
                    },
                    {
                        "authorId": "150358650",
                        "name": "Zhuoran Yang"
                    },
                    {
                        "authorId": "2075351197",
                        "name": "Zhihong Deng"
                    },
                    {
                        "authorId": "2054554660",
                        "name": "Animesh Garg"
                    },
                    {
                        "authorId": "2155812814",
                        "name": "Peng Liu"
                    },
                    {
                        "authorId": "50218397",
                        "name": "Zhaoran Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based RL for fully observable MDPs [5] is better understood and include \u201cworld models\u201d [17, 18] and Dyna-Q based methods [23, 50]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c38494d65afa882163a3c6f8bd43864024f48a8d",
                "externalIds": {
                    "DBLP": "conf/atal/KattNOA22",
                    "ArXiv": "2202.08884",
                    "DOI": "10.5555/3535850.3535932",
                    "CorpusId": 246996785
                },
                "corpusId": 246996785,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c38494d65afa882163a3c6f8bd43864024f48a8d",
                "title": "BADDr: Bayes-Adaptive Deep Dropout RL for POMDPs",
                "abstract": "While reinforcement learning (RL) has made great advances in scalability, exploration and partial observability are still active research topics. In contrast, Bayesian RL (BRL) provides a principled answer to both state estimation and the exploration-exploitation trade-off, but struggles to scale. To tackle this challenge, BRL frameworks with various prior assumptions have been proposed, with varied success. This work presents a representation-agnostic formulation of BRL under partially observability, unifying the previous models under one theoretical umbrella. To demonstrate its practical significance we also propose a novel derivation, Bayes-Adaptive Deep Dropout rl (BADDr), based on dropout networks. Under this parameterization, in contrast to previous work, the belief over the state and dynamics is a more scalable inference problem. We choose actions through Monte-Carlo tree search and empirically show that our method is competitive with state-of-the-art BRL methods on small domains while being able to solve much larger ones.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3442798",
                        "name": "S. Katt"
                    },
                    {
                        "authorId": "2110522917",
                        "name": "Hai V. Nguyen"
                    },
                    {
                        "authorId": "1799949",
                        "name": "F. Oliehoek"
                    },
                    {
                        "authorId": "34903901",
                        "name": "Chris Amato"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[29] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",
                "Model-based methods (to list a few, [29, 5, 42, 100, 40, 10, 67, 13, 38]) can be used together with VRL3."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7ed1566a286068f39effa67ae5c7489dbea06414",
                "externalIds": {
                    "ArXiv": "2202.10324",
                    "DBLP": "journals/corr/abs-2202-10324",
                    "CorpusId": 247011862
                },
                "corpusId": 247011862,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7ed1566a286068f39effa67ae5c7489dbea06414",
                "title": "VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning",
                "abstract": "We propose VRL3, a powerful data-driven framework with a simple design for solving challenging visual deep reinforcement learning (DRL) tasks. We analyze a number of major obstacles in taking a data-driven approach, and present a suite of design principles, novel findings, and critical insights about data-driven visual DRL. Our framework has three stages: in stage 1, we leverage non-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations; in stage 2, we use offline RL data (e.g. a limited number of expert demonstrations) to convert the task-agnostic representations into more powerful task-specific representations; in stage 3, we fine-tune the agent with online RL. On a set of challenging hand manipulation tasks with sparse reward and realistic visual inputs, compared to the previous SOTA, VRL3 achieves an average of 780% better sample efficiency. And on the hardest task, VRL3 is 1220% more sample efficient (2440% when using a wider encoder) and solves the task with only 10% of the computation. These significant results clearly demonstrate the great potential of data-driven deep reinforcement learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50879688",
                        "name": "Che Wang"
                    },
                    {
                        "authorId": "13289447",
                        "name": "Xufang Luo"
                    },
                    {
                        "authorId": "1829862",
                        "name": "K. Ross"
                    },
                    {
                        "authorId": "2108481496",
                        "name": "Dongsheng Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Experiments indicate that the practical instantiation of our algorithm, Safe Model-Based Policy Optimization (SMBPO), effectively reduces the number of safety violations on several continuous control tasks, achieving a comparable performance with far fewer safety violations compared to several model-free safe RL algorithms.",
                "We build on practices established in previous deep model-based algorithms, particularly MBPO [Janner et al., 2019] a state-of-the-art model-based algorithm (which does not emphasize safety).",
                "The algorithm, dubbed Safe Model-Based Policy Optimization (SMBPO), is described in Algorithm 1.",
                "3https://github.com/abalakrishna123/recovery-rl\nMBPO is competitive in terms of sample efficiency but incurs more safety violations because it isn\u2019t designed explicitly to avoid them.",
                "In the experimental evaluation, we compare our algorithm to several model-free safe RL algorithms, as well as MBPO, on various continuous control tasks based on the MuJoCo simulator [Todorov et al., 2012].",
                "All of the above algorithms except for MBPO are as implemented in the Recovery RL paper [Thananjeyan et al., 2020] and its publicly available codebase3.",
                "In MBPO, this takes the form of short model-based rollouts, starting from states in D, to reduce the risk of compounding error.",
                "With a larger C, SMBPO incurs substantially fewer safety violations, although the total rewards are learned slower.",
                "Here are some additional details regarding the (S)MBPO implementation:\n\u2022 All neural networks are implemented in PyTorch [Paszke et al., 2019] and optimized using the Adam optimizer [Kingma and Ba, 2014] and batch size 256.",
                "Algorithm 1 Safe Model-Based Policy Optimization (SMBPO) Require: Horizon H 1: Initialize empty buffers D and D\u0302, an ensemble of probabilistic dynamics {T\u0302\u03b8i} N i=1, policy \u03c0\u03c6, critic Q\u03c8 .",
                "A single run of (S)MBPO takes as long as 72 hours on a single GPU.",
                "We compare against the following algorithms:\n\u2022 MBPO: Corresponds to SMBPO with C = 0.",
                "MBPO is based on the soft actor-critic (SAC) algorithm, a widely used off-policy maximum-entropy actor-critic algorithm [Haarnoja et al., 2018a].",
                "Following prior work [Chua et al., 2018, Janner et al., 2019], we employ an ensemble of (diagonal) Gaussian dynamics models {T\u0302\u03b8i}Ni=1, where T\u0302i(s, a) = N (\u00b5\u03b8i(s, a),diag(\u03c32\u03b8i(s, a))), in an attempt to capture both aleatoric and epistemic uncertainties.",
                "Code is made available at https://github.com/gwthomas/Safe-MBPO.",
                "\u2022 MBPO+bonus: The same as MBPO, except adding back in the alive bonus which was subtracted\nout of the reward."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d8da5d28807fe5f8af2e6942c67738bd9f12a1ec",
                "externalIds": {
                    "DBLP": "conf/nips/ThomasLM21",
                    "ArXiv": "2202.07789",
                    "CorpusId": 245019637
                },
                "corpusId": 245019637,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d8da5d28807fe5f8af2e6942c67738bd9f12a1ec",
                "title": "Safe Reinforcement Learning by Imagining the Near Future",
                "abstract": "Safe reinforcement learning is a promising path toward applying reinforcement learning algorithms to real-world problems, where suboptimal behaviors may lead to actual negative consequences. In this work, we focus on the setting where unsafe states can be avoided by planning ahead a short time into the future. In this setting, a model-based agent with a sufficiently accurate model can avoid unsafe states. We devise a model-based algorithm that heavily penalizes unsafe trajectories, and derive guarantees that our algorithm can avoid unsafe states under certain assumptions. Experiments demonstrate that our algorithm can achieve competitive rewards with fewer safety violations in several continuous control tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8234443",
                        "name": "G. Thomas"
                    },
                    {
                        "authorId": "1491625903",
                        "name": "Yuping Luo"
                    },
                    {
                        "authorId": "2114186424",
                        "name": "Tengyu Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1The term \u2018model-based\u2019 does not mean \u2018model-based reinforcement learning\u2019 [1]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e48557b4311386db3c0f8bdb224c179664a1b71b",
                "externalIds": {
                    "DOI": "10.1109/TNNLS.2022.3147221",
                    "CorpusId": 246827874,
                    "PubMed": "35157599"
                },
                "corpusId": 246827874,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e48557b4311386db3c0f8bdb224c179664a1b71b",
                "title": "Model-Based Self-Advising for Multi-Agent Learning",
                "abstract": "In multiagent learning, one of the main ways to improve learning performance is to ask for advice from another agent. Contemporary advising methods share a common limitation that a teacher agent can only advise a student agent if the teacher has experience with an identical state. However, in highly complex learning scenarios, such as autonomous driving, it is rare for two agents to experience exactly the same state, which makes the advice less of a learning aid and more of a one-time instruction. In these scenarios, with contemporary methods, agents do not really help each other learn, and the main outcome of their back and forth requests for advice is an exorbitant communications\u2019 overhead. In human interactions, teachers are often asked for advice on what to do in situations that students are personally unfamiliar with. In these, we generally draw from similar experiences to formulate advice. This inspired us to provide agents with the same ability when asked for advice on an unfamiliar state. Hence, we propose a model-based self-advising method that allows agents to train a model based on states similar to the state in question to inform its response. As a result, the advice given can not only be used to resolve the current dilemma but also many other similar situations that the student may come across in the future via self-advising. Compared with contemporary methods, our method brings a significant improvement in learning performance with much lower communication overheads.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2617208",
                        "name": "Dayong Ye"
                    },
                    {
                        "authorId": "32620196",
                        "name": "Tianqing Zhu"
                    },
                    {
                        "authorId": "2154730074",
                        "name": "Congcong Zhu"
                    },
                    {
                        "authorId": "1745566",
                        "name": "Wanlei Zhou"
                    },
                    {
                        "authorId": "152297693",
                        "name": "Philip S. Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, we can used sauteed environments in the model-based RL setting (MBPO and PETS) as well.",
                ", 2021) as the core library, which has PyTorch implementation of MBPO (Janner et al., 2019), and PETS (Chua et al.",
                "While we mostly test with model-free approaches (PPO, TRPO, SAC), the model-based methods are also \u201csauteable\u201d, which we illustrate on MBPO and PETS.",
                "For our model-based implementations we used (Pineda et al., 2021) as the core library, which has PyTorch implementation of MBPO (Janner et al., 2019), and PETS (Chua et al., 2018).",
                "We proceed by \u201csauteing\u201d MBRL methods: MBPO and PETS.",
                ", 2018), model-based policy optimization (MBPO) (Janner et al., 2019), probabilistic ensembles with trajectory sampling (PETS) (Chua et al.",
                "We discuss in detail these state augmentation methods in Appendix B, but note that (Calvo-Fullana et al., 2021), (Chow et al., 2017) have not extended their methods to modern model-free and model-based RL methods such as trust region policy optimization (TRPO) (Schulman et al., 2015), proximal policy optimization (PPO) (Schulman et al., 2017), soft actor critic (SAC) (Haarnoja et al., 2018), model-based policy optimization (MBPO) (Janner et al., 2019), probabilistic ensembles with trajectory sampling (PETS) (Chua et al., 2018).",
                "\u2026policy optimization (TRPO) (Schulman et al., 2015), proximal policy optimization (PPO) (Schulman et al., 2017), soft actor critic (SAC) (Haarnoja et al., 2018), model-based policy optimization (MBPO) (Janner et al., 2019), probabilistic ensembles with trajectory sampling (PETS) (Chua et al., 2018)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "fda6c22149d269cb2730e919ed4254a5a5f84ae0",
                "externalIds": {
                    "DBLP": "conf/icml/SootlaCJWMWA22",
                    "ArXiv": "2202.06558",
                    "CorpusId": 246822687
                },
                "corpusId": 246822687,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fda6c22149d269cb2730e919ed4254a5a5f84ae0",
                "title": "SAUTE RL: Almost Surely Safe Reinforcement Learning Using State Augmentation",
                "abstract": "Satisfying safety constraints almost surely (or with probability one) can be critical for the deployment of Reinforcement Learning (RL) in real-life applications. For example, plane landing and take-off should ideally occur with probability one. We address the problem by introducing Safety Augmented (Saute) Markov Decision Processes (MDPs), where the safety constraints are eliminated by augmenting them into the state-space and reshaping the objective. We show that Saute MDP satisfies the Bellman equation and moves us closer to solving Safe RL with constraints satisfied almost surely. We argue that Saute MDP allows viewing the Safe RL problem from a different perspective enabling new features. For instance, our approach has a plug-and-play nature, i.e., any RL algorithm can be\"Sauteed\". Additionally, state augmentation allows for policy generalization across safety constraints. We finally show that Saute RL algorithms can outperform their state-of-the-art counterparts when constraint satisfaction is of high importance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144424020",
                        "name": "Aivar Sootla"
                    },
                    {
                        "authorId": "1413805313",
                        "name": "A. Cowen-Rivers"
                    },
                    {
                        "authorId": "1419479157",
                        "name": "Taher Jafferjee"
                    },
                    {
                        "authorId": "2142663126",
                        "name": "Ziyan Wang"
                    },
                    {
                        "authorId": "41127915",
                        "name": "D. Mguni"
                    },
                    {
                        "authorId": "48094081",
                        "name": "Jun Wang"
                    },
                    {
                        "authorId": "1398842047",
                        "name": "Haitham Bou-Ammar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our model-based algorithm can be further enhanced by using synthetic one-step transitions similarly to Janner et al. (2019), which would improve sample efficiency."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "69d042aaaf2fd7b15888aaf98009e184b54c32f8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-06557",
                    "ArXiv": "2202.06557",
                    "CorpusId": 246823263
                },
                "corpusId": 246823263,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/69d042aaaf2fd7b15888aaf98009e184b54c32f8",
                "title": "Reinforcement Learning in Presence of Discrete Markovian Context Evolution",
                "abstract": "We consider a context-dependent Reinforcement Learning (RL) setting, which is characterized by: a) an unknown finite number of not directly observable contexts; b) abrupt (discontinuous) context changes occurring during an episode; and c) Markovian context evolution. We argue that this challenging case is often met in applications and we tackle it using a Bayesian approach and variational inference. We adapt a sticky Hierarchical Dirichlet Process (HDP) prior for model learning, which is arguably best-suited for Markov process modeling. We then derive a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. We argue that the combination of these two components allows to infer the number of contexts from data thus dealing with the context cardinality assumption. We then find the representation of the optimal policy enabling efficient policy learning using off-the-shelf RL algorithms. Finally, we demonstrate empirically (using gym environments cart-pole swing-up, drone, intersection) that our approach succeeds where state-of-the-art methods of other frameworks fail and elaborate on the reasons for such failures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114292784",
                        "name": "Hang Ren"
                    },
                    {
                        "authorId": "144424020",
                        "name": "Aivar Sootla"
                    },
                    {
                        "authorId": "1419479157",
                        "name": "Taher Jafferjee"
                    },
                    {
                        "authorId": "2143669056",
                        "name": "Junxiao Shen"
                    },
                    {
                        "authorId": "48094081",
                        "name": "Jun Wang"
                    },
                    {
                        "authorId": "1398842047",
                        "name": "Haitham Bou-Ammar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026et al., 2016), we would expect analogous improvements to be possible in MBRL. Algorithms for MBRL are infamously sensitive to choice of prediction horizon, and one possible explanation is poor generalization caused by weak inductive biases (Janner et al., 2019; Pan et al., 2020; Amos et al., 2021).",
                "Algorithms for MBRL are infamously sensitive to choice of prediction horizon, and one possible explanation is poor generalization caused by weak inductive biases (Janner et al., 2019; Pan et al., 2020; Amos et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3f8dae850dfc1163990f9b513164b42908515a08",
                "externalIds": {
                    "ArXiv": "2202.04836",
                    "DBLP": "journals/corr/abs-2202-04836",
                    "CorpusId": 246706127
                },
                "corpusId": 246706127,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3f8dae850dfc1163990f9b513164b42908515a08",
                "title": "Deconstructing the Inductive Biases of Hamiltonian Neural Networks",
                "abstract": "Physics-inspired neural networks (NNs), such as Hamiltonian or Lagrangian NNs, dramatically outperform other learned dynamics models by leveraging strong inductive biases. These models, however, are challenging to apply to many real world systems, such as those that don't conserve energy or contain contacts, a common setting for robotics and reinforcement learning. In this paper, we examine the inductive biases that make physics-inspired models successful in practice. We show that, contrary to conventional wisdom, the improved generalization of HNNs is the result of modeling acceleration directly and avoiding artificial complexity from the coordinate system, rather than symplectic structure or energy conservation. We show that by relaxing the inductive biases of these models, we can match or exceed performance on energy-conserving systems while dramatically improving performance on practical, non-conservative systems. We extend this approach to constructing transition models for common Mujoco environments, showing that our model can appropriately balance inductive biases with the flexibility required for model-based control.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "13297813",
                        "name": "Nate Gruver"
                    },
                    {
                        "authorId": "51007156",
                        "name": "Marc Finzi"
                    },
                    {
                        "authorId": "2067201658",
                        "name": "S. Stanton"
                    },
                    {
                        "authorId": "145771261",
                        "name": "A. Wilson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following the same line as in (Janner et al., 2019; Yu et al., 2020; 2021b), the dynamics model for each task is represented as a probabilistic neural network that takes the current state-action as input and outputs a Gaussian distribution over the next state and reward:\nT\u0302\u03b8(st+1, r|s, a) = N\u2026",
                "2 in (Janner et al., 2019), we can obtain that Dtv(\u03c1 \u03c0o t (s, a)||\u03c1 \u03c0c t (s, a)) \u2264Dtv(\u03c1 t (s)||\u03c1 \u03c0c t (s)) + max s Dtv(\u03c0o(a|s)||\u03c0c(a|s)) \u2264tmax s Dtv(\u03c0o(a|s)||\u03c0c(a|s)) + max s Dtv(\u03c0o(a|s)||\u03c0c(a|s))",
                ", 2021b), have demonstrated promising performance on a single offline RL task by combining model-based policy optimization (Janner et al., 2019) and conservative policy evaluation (CQL (Kumar et al.",
                "To remove the need of uncertainty quantification, COMBO (Yu et al., 2021b) is proposed by combining model-based policy optimization (Janner et al., 2019) and conservative policy evaluation (Kumar et al., 2020).",
                "Following the same line as in (Janner et al., 2019; Yu et al., 2020; 2021b), the dynamics model for each task is represented as a probabilistic neural network that takes the current state-action as input and outputs a Gaussian distribution over the next state and reward: T\u0302\u03b8(st+1, r|s, a) = N (\u03bc\u03b8(st, at),\u03a3\u03b8(st, at)).",
                "Since both state-action marginals here correspond to rolling out \u03c0o and \u03c0c in the same MDP M\u0302, based on Lemma B.1 and B.2 in (Janner et al., 2019), we can obtain that\nDtv(\u03c1 \u03c0o t (s, a)||\u03c1 \u03c0c t (s, a))\n\u2264Dtv(\u03c1\u03c0ot (s)||\u03c1 \u03c0c t (s)) + max s Dtv(\u03c0o(a|s)||\u03c0c(a|s))\n\u2264tmax s Dtv(\u03c0o(a|s)||\u03c0c(a|s)) + max s\u2026",
                "Recent model-based offline RL algorithms, e.g., COMBO (Yu et al., 2021b), have demonstrated promising performance on a single offline RL task by combining model-based policy optimization (Janner et al., 2019) and conservative policy evaluation (CQL (Kumar et al., 2020)).",
                ", 2021b) is proposed by combining model-based policy optimization (Janner et al., 2019) and conservative policy evaluation (Kumar et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "02ad21eea9ec32783ba529487e74a76e85499a53",
                "externalIds": {
                    "DBLP": "conf/iclr/LinWXLZ22",
                    "ArXiv": "2202.02929",
                    "CorpusId": 246633939
                },
                "corpusId": 246633939,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/02ad21eea9ec32783ba529487e74a76e85499a53",
                "title": "Model-Based Offline Meta-Reinforcement Learning with Regularization",
                "abstract": "Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, offline Meta-RL could be outperformed by offline single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated between\"exploring\"the out-of-distribution state-actions by following the meta-policy and\"exploiting\"the offline dataset by staying close to the behavior policy. Motivated by such empirical analysis, we explore model-based offline Meta-RL with regularized Policy Optimization (MerPO), which learns a meta-model for efficient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block of MerPO, using conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via offline Meta-RL. Experiments corroborate the superior performance of MerPO over existing offline Meta-RL methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1641386905",
                        "name": "Sen Lin"
                    },
                    {
                        "authorId": "2153470794",
                        "name": "Jialin Wan"
                    },
                    {
                        "authorId": "51020953",
                        "name": "Tengyu Xu"
                    },
                    {
                        "authorId": "50014661",
                        "name": "Yingbin Liang"
                    },
                    {
                        "authorId": "47540395",
                        "name": "Junshan Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "92b23dbfb78c0e14508fb3029acb9d6196e4d870",
                "externalIds": {
                    "MAG": "3199814407",
                    "DBLP": "journals/is/LiWLZL22",
                    "DOI": "10.1016/j.is.2021.101878",
                    "CorpusId": 240572588
                },
                "corpusId": 240572588,
                "publicationVenue": {
                    "id": "bc2801b8-b059-4ee4-be9a-cd1943f45bf3",
                    "name": "Information Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Inf Syst"
                    ],
                    "issn": "0306-4379",
                    "alternate_issns": [
                        "2156-0749"
                    ],
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/236/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/03064379",
                        "https://www.journals.elsevier.com/information-systems"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/92b23dbfb78c0e14508fb3029acb9d6196e4d870",
                "title": "Electronic health records based reinforcement learning for treatment optimizing",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118910995",
                        "name": "Tianhao Li"
                    },
                    {
                        "authorId": "2140043947",
                        "name": "Zhishun Wang"
                    },
                    {
                        "authorId": "2153424553",
                        "name": "Wei Lu"
                    },
                    {
                        "authorId": "2145946441",
                        "name": "Qian Zhang"
                    },
                    {
                        "authorId": "87269812",
                        "name": "Dengfeng Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026reinforcement learning, simulators are often learned, but accumulate errors over long time horizons and often struggle to generalize beyond their training data (Janner et al., 2019; Talvitie, 2014; Venkatraman et al., 2015), making them unsuitable for design optimization without further finetuning.",
                "In robotics and reinforcement learning, simulators are often learned, but accumulate errors over long time horizons and often struggle to generalize beyond their training data (Janner et al., 2019; Talvitie, 2014; Venkatraman et al., 2015), making them unsuitable for design optimization without further finetuning."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "90cc86274f947b15ec3cc8c1dcfe1fc8db608e03",
                "externalIds": {
                    "ArXiv": "2202.00728",
                    "DBLP": "journals/corr/abs-2202-00728",
                    "CorpusId": 246473077
                },
                "corpusId": 246473077,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/90cc86274f947b15ec3cc8c1dcfe1fc8db608e03",
                "title": "Physical Design using Differentiable Learned Simulators",
                "abstract": "Designing physical artifacts that serve a purpose - such as tools and other functional structures - is central to engineering as well as everyday human behavior. Though automating design has tremendous promise, general-purpose methods do not yet exist. Here we explore a simple, fast, and robust approach to inverse design which combines learned forward simulators based on graph neural networks with gradient-based design optimization. Our approach solves high-dimensional problems with complex physical dynamics, including designing surfaces and tools to manipulate fluid flows and optimizing the shape of an airfoil to minimize drag. This framework produces high-quality designs by propagating gradients through trajectories of hundreds of steps, even when using models that were pre-trained for single-step predictions on data substantially different from the design tasks. In our fluid manipulation tasks, the resulting designs outperformed those found by sampling-based optimization techniques. In airfoil design, they matched the quality of those obtained with a specialized solver. Our results suggest that despite some remaining challenges, machine learning-based simulators are maturing to the point where they can support general-purpose design optimization across a variety of domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145254624",
                        "name": "Kelsey R. Allen"
                    },
                    {
                        "authorId": "1414739741",
                        "name": "Tatiana Lopez-Guevara"
                    },
                    {
                        "authorId": "2238306103",
                        "name": "Kimberly L. Stachenfeld"
                    },
                    {
                        "authorId": "1398105826",
                        "name": "Alvaro Sanchez-Gonzalez"
                    },
                    {
                        "authorId": "2019153",
                        "name": "P. Battaglia"
                    },
                    {
                        "authorId": "2158860",
                        "name": "Jessica B. Hamrick"
                    },
                    {
                        "authorId": "2054956",
                        "name": "T. Pfaff"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b75dec239ef07b70c9ac72a9d65b29976b4842d9",
                "externalIds": {
                    "DBLP": "journals/ras/QianXLBL22",
                    "DOI": "10.1016/j.robot.2022.104046",
                    "CorpusId": 246502511
                },
                "corpusId": 246502511,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b75dec239ef07b70c9ac72a9d65b29976b4842d9",
                "title": "Environment-adaptive learning from demonstration for proactive assistance in human-robot collaborative tasks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Kun Qian"
                    },
                    {
                        "authorId": "2152776881",
                        "name": "Xin Xu"
                    },
                    {
                        "authorId": "38746648",
                        "name": "Huan Liu"
                    },
                    {
                        "authorId": "2151699602",
                        "name": "Jishen Bai"
                    },
                    {
                        "authorId": "50240783",
                        "name": "Shan Luo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0dcc538c8d2e96e21af8ca2267df344cbca08446",
                "externalIds": {
                    "DBLP": "journals/ijon/GaoSWWZLS22",
                    "DOI": "10.1016/j.neucom.2022.02.022",
                    "CorpusId": 246698883
                },
                "corpusId": 246698883,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0dcc538c8d2e96e21af8ca2267df344cbca08446",
                "title": "Deterministic policy optimization with clipped value expansion and long-horizon planning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3224059",
                        "name": "Shiqing Gao"
                    },
                    {
                        "authorId": "2112497692",
                        "name": "Haibo Shi"
                    },
                    {
                        "authorId": "7572514",
                        "name": "Fang Wang"
                    },
                    {
                        "authorId": "2117423895",
                        "name": "Zijian Wang"
                    },
                    {
                        "authorId": "2145354037",
                        "name": "Siyu Zhang"
                    },
                    {
                        "authorId": "2155851586",
                        "name": "Yunxia Li"
                    },
                    {
                        "authorId": "48186778",
                        "name": "Yaoru Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To verify the superiority of the system model designed in this paper, the model is compared with the MLP model [37] and CNN\u2010LSTM model [38], which are commonly used in data\u2010driven system modeling.",
                "To verify the superiority of the system mo el desig ed in this paper, t e o el is co pared with the MLP model [37] and CNN-LSTM model [38], which are co only used in data-driven syste modeling."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1be65d44c6459afce71c5df4fbefab9995e3dafc",
                "externalIds": {
                    "DBLP": "journals/sensors/HouWZZ22",
                    "PubMedCentral": "8839166",
                    "DOI": "10.3390/s22031292",
                    "CorpusId": 246743438,
                    "PubMed": "35162036"
                },
                "corpusId": 246743438,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1be65d44c6459afce71c5df4fbefab9995e3dafc",
                "title": "Robotic Manipulation Planning for Automatic Peeling of Glass Substrate Based on Online Learning Model Predictive Path Integral",
                "abstract": "Autonomous planning robotic contact-rich manipulation has long been a challenging problem. Automatic peeling of glass substrates of LCD flat panel displays is a typical contact-rich manipulation task, which requires extremely high safe handling through the manipulation process. To this end of peeling glass substrates automatically, the system model is established from data and is used for the online planning of the robot motion in this paper. A simulation environment is designed to pretrain the process model with deep learning-based neural network structure to avoid expensive and time-consuming collection of real-time data. Then, an online learning algorithm is introduced to tune the pretrained model according to the real-time data from the peeling process experiments to cover the uncertainties of the real process. Finally, an Online Learning Model Predictive Path Integral (OL-MPPI) algorithm is proposed for the optimal trajectory planning of the robot. The performance of our algorithm was validated through glass substrate peeling tasks of experiments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "38866782",
                        "name": "Liwei Hou"
                    },
                    {
                        "authorId": "7643665",
                        "name": "Hengsheng Wang"
                    },
                    {
                        "authorId": "2089856379",
                        "name": "Haoran Zou"
                    },
                    {
                        "authorId": "2150666434",
                        "name": "Yalin Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare our algorithm with multiple state-of-the-art deep RL methods including model-free algorithms such as Tpprl (Upadhyay, De, and Gomez-Rodriguez 2018), SAC (Haarnoja et al. 2018), TD3 (Fujimoto, Hoof, and Meger 2018), DDQN (Van Hasselt, Guez, and Silver 2016) and model-based ones such as Dreamer (Hafner et al. 2019a) and MBPO (Janner et al. 2019).",
                "\u2026RL methods including model-free algorithms such as Tpprl (Upadhyay, De, and Gomez-Rodriguez 2018), SAC (Haarnoja et al. 2018), TD3 (Fujimoto, Hoof, and Meger 2018), DDQN (Van Hasselt, Guez, and Silver 2016) and model-based ones such as Dreamer (Hafner et al. 2019a) and MBPO (Janner et al. 2019).",
                "We replace the feed-forward neural network in MBPO by LSTM. 7) LSTM+SAC: The sequence encoding is obtained by the LSTM.",
                "6) MBPO: MBPO applies the model ensemble in the dynamic model learning to enhance the performance.",
                "In (a), our method SEDRL has the best result followed by Tpprl, Transformer+SAC, LSTM+ SAC, MBPO, Transformer+TD3, Dreamer and then Transformer+DDQN (Trans+Q)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7cf779d889dbf155e089289bab1495be2b186b11",
                "externalIds": {
                    "ArXiv": "2201.12569",
                    "DBLP": "journals/corr/abs-2201-12569",
                    "DOI": "10.1609/aaai.v37i8.26142",
                    "CorpusId": 246431011
                },
                "corpusId": 246431011,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7cf779d889dbf155e089289bab1495be2b186b11",
                "title": "Bellman Meets Hawkes: Model-Based Reinforcement Learning via Temporal Point Processes",
                "abstract": "We consider a sequential decision making problem where the agent faces the environment characterized by the stochastic discrete events and seeks an optimal intervention policy such that its long-term reward is maximized. This problem exists ubiquitously in social media, finance and health informatics but is rarely investigated by the conventional research in reinforcement learning. To this end, we present a novel framework of the model-based reinforcement learning where the agent's actions and observations are asynchronous stochastic discrete events occurring in continuous-time. We model the dynamics of the environment by Hawkes process with external intervention control term and develop an algorithm to embed such process in the Bellman equation which guides the direction of the value gradient. We demonstrate the superiority of our method in both synthetic simulator and real-data experiments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064702529",
                        "name": "C. Qu"
                    },
                    {
                        "authorId": "2153585312",
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "authorId": "2149919635",
                        "name": "Siqiao Xue"
                    },
                    {
                        "authorId": "2119204984",
                        "name": "X. Shi"
                    },
                    {
                        "authorId": "2108020140",
                        "name": "James Zhang"
                    },
                    {
                        "authorId": "33344744",
                        "name": "Hongyuan Mei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The above hyper-parameter configurations in the online setting are adapted from MBPO.",
                ", 2021] are existing offline-to-online algorithms, and MBPO [Janner et al., 2019] is a pure online RL algorithm.",
                "Therefore, the online training process quickly turns into a MBPO-like training process, an efficient model-based online RL algorithm [Janner et al., 2019].",
                "We compare our algorithm with three baselines, where balance replay [Lee et al., 2021] and AWAC [Nair et al., 2021] are existing offline-to-online algorithms, and MBPO [Janner et al., 2019] is a pure online RL algorithm."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "15690113ac1a9aed37c53f0196d1c6629d4e7773",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-10070",
                    "ArXiv": "2201.10070",
                    "CorpusId": 246275860
                },
                "corpusId": 246275860,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/15690113ac1a9aed37c53f0196d1c6629d4e7773",
                "title": "MOORe: Model-based Offline-to-Online Reinforcement Learning",
                "abstract": "With the success of offline reinforcement learning (RL), offline trained RL policies have the potential to be further improved when deployed online. A smooth transfer of the policy matters in safe real-world deployment. Besides, fast adaptation of the policy plays a vital role in practical online performance improvement. To tackle these challenges, we propose a simple yet efficient algorithm, Model-based Offline-to-Online Reinforcement learning (MOORe), which employs a prioritized sampling scheme that can dynamically adjust the offline and online data for smooth and efficient online adaptation of the policy. We provide a theoretical foundation for our algorithms design. Experiment results on the D4RL benchmark show that our algorithm smoothly transfers from offline to online stages while enabling sample-efficient online adaption, and also significantly outperforms existing methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2087640445",
                        "name": "Yihuan Mao"
                    },
                    {
                        "authorId": "2144448125",
                        "name": "Chao Wang"
                    },
                    {
                        "authorId": "2222324589",
                        "name": "Bin Wang"
                    },
                    {
                        "authorId": "2111387140",
                        "name": "Chongjie Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A promising alternative to improve sample efficiency is to use model-based reinforcement learning (MBRL) approaches\n(Deisenroth & Rasmussen, 2011; Chua et al., 2018; Hafner et al., 2019a; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f1e48bfb4464fedb94ced2d85b74991efcfe2856",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-09802",
                    "ArXiv": "2201.09802",
                    "CorpusId": 246240506
                },
                "corpusId": 246240506,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f1e48bfb4464fedb94ced2d85b74991efcfe2856",
                "title": "Constrained Policy Optimization via Bayesian World Models",
                "abstract": "Improving sample-efficiency and safety are crucial challenges when deploying reinforcement learning in high-stakes real world applications. We propose LAMBDA, a novel model-based approach for policy optimization in safety critical tasks modeled via constrained Markov decision processes. Our approach utilizes Bayesian world models, and harnesses the resulting uncertainty to maximize optimistic upper bounds on the task objective, as well as pessimistic upper bounds on the safety constraints. We demonstrate LAMBDA's state of the art performance on the Safety-Gym benchmark suite in terms of sample efficiency and constraint violation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151001957",
                        "name": "Yarden As"
                    },
                    {
                        "authorId": "50824386",
                        "name": "I. Usmanova"
                    },
                    {
                        "authorId": "12646136",
                        "name": "Sebastian Curi"
                    },
                    {
                        "authorId": "153243248",
                        "name": "A. Krause"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e4241619e9af5b41150b9a4d0982ac02886899de",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08894",
                    "ArXiv": "2201.08894",
                    "CorpusId": 246240396
                },
                "corpusId": 246240396,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e4241619e9af5b41150b9a4d0982ac02886899de",
                "title": "Reinforcement Learning for Personalized Drug Discovery and Design for Complex Diseases: A Systems Pharmacology Perspective",
                "abstract": "Many multi-genic systemic diseases such as neurological disorders, inflammatory diseases, and the majority of cancers do not have effective treatments yet. Reinforcement learning powered systems pharmacology is a potentially effective approach to design personalized therapies for untreatable complex diseases. In this survey, state-of-the-art reinforcement learning methods and their latest applications to drug design are reviewed. The challenges on harnessing reinforcement learning for systems pharmacology and personalized medicine are discussed. Potential solutions to overcome the challenges are proposed. In spite of successful application of advanced reinforcement learning techniques to target-based drug discovery, new reinforcement learning strategies are needed to address systems pharmacology-oriented personalized de novo drug design.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151006165",
                        "name": "Ryan K. Tan"
                    },
                    {
                        "authorId": "145595037",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "36523530",
                        "name": "Lei Xie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Lemma 3.1 (Janner et al., 2019): Let the expected total variation distance (TV-distance)6 Es\u223c\u03c0D,k[DTV(p(s\u2032, r|s, a)\u2016pw(s\u2032, r|s, a))] between two transition distributions be bounded at each time step by m and the policy divergence DTV [\u03c0\u2016\u03c0D] be bounded by \u03c0 .",
                "Janner et al. (2019) provided conditions under which model usage can facilitate policy optimisation."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b85bf7814429fb836235f464fc67ce10815a9959",
                "externalIds": {
                    "DBLP": "journals/ijcon/BaoV23",
                    "DOI": "10.1080/00207179.2022.2029945",
                    "CorpusId": 246019320
                },
                "corpusId": 246019320,
                "publicationVenue": {
                    "id": "ba4c31cb-e149-49eb-a410-34b9f5a7d653",
                    "name": "International Journal of Control",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Control"
                    ],
                    "issn": "0020-7179",
                    "url": "http://www.tandfonline.com/loi/tcon20",
                    "alternate_urls": [
                        "http://www.tandf.co.uk/journals/alphalist.htm",
                        "http://www.tandf.co.uk/journals/default.asp"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b85bf7814429fb836235f464fc67ce10815a9959",
                "title": "Safe control of nonlinear systems in LPV framework using model-based reinforcement learning",
                "abstract": "This paper presents a safe model-based reinforcement learning (MBRL) approach to control nonlinear systems described by linear parameter-varying (LPV) models. A variational Bayesian inference Neural Network (BNN) approach is first employed to learn a state-space model with uncertainty quantification from input-output data collected from the system; the model is then utilised for training MBRL to learn control actions for the system with safety guarantees. Specifically, MBRL employs the BNN model to generate simulation environments for training, which avoids safety violations in the exploration stage. To adapt to dynamically varying environments, knowledge on the evolution of LPV model scheduling variables is incorporated in simulation to reduce the discrepancy between the transition distributions of simulation and real environments. Experiments on a parameter-varying double integrator system and a control moment gyroscope (CMG) simulation model demonstrate that the proposed approach can safely achieve desired control performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2075376698",
                        "name": "Yajie Bao"
                    },
                    {
                        "authorId": "2230164",
                        "name": "J. Mohammadpour"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Dyna-style approaches are popular since they facilitate faster inference at deployment time (forward-pass of the policy rather than an optimization loop) and have been shown to be highly sample-efficient both from proprioceptive states (Janner et al., 2019) and pixels (Hafner et al.",
                "\u2026RL. Dyna-style approaches are popular since they facilitate faster inference at deployment time (forward-pass of the policy rather than an optimization loop) and have been shown to be highly sample-efficient both from proprioceptive states (Janner et al., 2019) and pixels (Hafner et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c512d35fd20fbe4612f2bce2b6f5409c8b0a73e1",
                "externalIds": {
                    "DBLP": "journals/jair/Parker-HolderRS22",
                    "ArXiv": "2201.03916",
                    "DOI": "10.1613/jair.1.13596",
                    "CorpusId": 245853841
                },
                "corpusId": 245853841,
                "publicationVenue": {
                    "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
                    "name": "Journal of Artificial Intelligence Research",
                    "type": "journal",
                    "alternate_names": [
                        "JAIR",
                        "J Artif Intell Res",
                        "The Journal of Artificial Intelligence Research"
                    ],
                    "issn": "1076-9757",
                    "url": "http://www.jair.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c512d35fd20fbe4612f2bce2b6f5409c8b0a73e1",
                "title": "Automated Reinforcement Learning (AutoRL): A Survey and Open Problems",
                "abstract": "The combination of Reinforcement Learning (RL) with deep learning has led to a series of impressive feats, with many believing (deep) RL provides a path towards generally capable agents. However, the success of RL agents is often highly sensitive to design choices in the training process, which may require tedious and error-prone manual tuning. This makes it challenging to use RL for new problems and also limits its full potential. In many other areas of machine learning, AutoML has shown that it is possible to automate such design choices, and AutoML has also yielded promising initial results when applied to RL. However, Automated Reinforcement Learning (AutoRL) involves not only standard applications of AutoML but also includes additional challenges unique to RL, that naturally produce a different set of methods. As such, AutoRL has been emerging as an important area of research in RL, providing promise in a variety of applications from RNA design to playing games, such as Go. Given the diversity of methods and environments considered in RL, much of the research has been conducted in distinct subfields, ranging from meta-learning to evolution. In this survey, we seek to unify the field of AutoRL, provide a common taxonomy, discuss each area in detail and pose open problems of interest to researchers going forward.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1410302742",
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "authorId": "147958750",
                        "name": "Raghunandan Rajan"
                    },
                    {
                        "authorId": "32725720",
                        "name": "Xingyou Song"
                    },
                    {
                        "authorId": "146024084",
                        "name": "Andr\u00e9 Biedenkapp"
                    },
                    {
                        "authorId": "2053093511",
                        "name": "Yingjie Miao"
                    },
                    {
                        "authorId": "1944688061",
                        "name": "Theresa Eimer"
                    },
                    {
                        "authorId": "2218953258",
                        "name": "Baohe Zhang"
                    },
                    {
                        "authorId": "1405961541",
                        "name": "V. Nguyen"
                    },
                    {
                        "authorId": "35159852",
                        "name": "R. Calandra"
                    },
                    {
                        "authorId": "145520045",
                        "name": "Aleksandra Faust"
                    },
                    {
                        "authorId": "144661829",
                        "name": "F. Hutter"
                    },
                    {
                        "authorId": "145963266",
                        "name": "M. Lindauer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A contrastive approach to constraining the RL to the dataset is to derive an MDP from the data and either solve it optimally or use model-based policy optimization (MBPO).",
                "Next, we use MOReL and MOPO as two representatives of the general MBPO [17] approach that covers both classical (n\u00e4\u0131ve) MBRL and Pessimistic MDP-based MBRL.",
                "A contrastive approach to constraining the RL to the dataset is to derive an MDP from the data and either solve it optimally or use model-based policy optimization (MBPO) [17].",
                "Next, we use MOReL [10] as a representative of a general MBPO approach that covers both a classical (na\u0308\u0131ve) MBRL and a Pessimistic MDPbased MBRL."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "634b6aa1f7f7296faf0b42cfdcc14f922082780c",
                "externalIds": {
                    "ArXiv": "2201.02381",
                    "CorpusId": 245827952
                },
                "corpusId": 245827952,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/634b6aa1f7f7296faf0b42cfdcc14f922082780c",
                "title": "Offline Reinforcement Learning for Road Traffic Control",
                "abstract": "Traffic signal control is an important problem in urban mobility with a significant potential of economic and environmental impact. While there is a growing interest in Reinforcement Learning (RL) for traffic signal control, the work so far has focussed on learning through simulations which could lead to inaccuracies due to simplifying assumptions. Instead, real experience data on traffic is available and could be exploited at minimal costs. Recent progress in offline or batch RL has enabled just that. Model-based offline RL methods, in particular, have been shown to generalize from the experience data much better than others. We build a model-based learning framework which infers a Markov Decision Process (MDP) from a dataset collected using a cyclic traffic signal control policy that is both commonplace and easy to gather. The MDP is built with pessimistic costs to manage out-of-distribution scenarios using an adaptive shaping of rewards which is shown to provide better regularization compared to the prior related work in addition to being PAC-optimal. Our model is evaluated on a complex signalized roundabout showing that it is possible to build highly performant traffic control policies in a data efficient manner.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3094459",
                        "name": "Mayuresh Kunjir"
                    },
                    {
                        "authorId": "50793091",
                        "name": "S. Chawla"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other than applying the IC-INVASE algorithm to solve Equation (6), another way of leveraging IC-INVASE in action space pruning is to combine it with the model-based methods [11, 16, 13, 14], where a dynamic model P : S \u00d7A 7\u2192 S is learned through regression: P = arg min P E(s,a,s\u2032)\u223c\u03c0,T (s\u2032 \u2212 P(s, a))(2) (10)"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a24e9d58962e94b3391a4a132e0eb19b5c3abd75",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-00354",
                    "ArXiv": "2201.00354",
                    "CorpusId": 245650878
                },
                "corpusId": 245650878,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a24e9d58962e94b3391a4a132e0eb19b5c3abd75",
                "title": "Toward Causal-Aware RL: State-Wise Action-Refined Temporal Difference",
                "abstract": "Although it is well known that exploration plays a key role in Reinforcement Learning (RL), prevailing exploration strategies for continuous control tasks in RL are mainly based on naive isotropic Gaussian noise regardless of the causality relationship between action space and the task and consider all dimensions of actions equally important. In this work, we propose to conduct interventions on the primal action space to discover the causal relationship between the action space and the task reward. We propose the method of State-Wise Action Refined (SWAR), which addresses the issue of action space redundancy and promote causality discovery in RL. We formulate causality discovery in RL tasks as a state-dependent action space selection problem and propose two practical algorithms as solutions. The first approach, TD-SWAR, detects task-related actions during temporal difference learning, while the second approach, Dyn-SWAR, reveals important actions through dynamic model prediction. Empirically, both methods provide approaches to understand the decisions made by RL agents and improve learning efficiency in action-redundant tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156232554",
                        "name": "Hao Sun"
                    },
                    {
                        "authorId": "2115984858",
                        "name": "Taiyi A. Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The model can be used in variousways, such as execution-time planning [5, 21], generating imaginary experiences for training the control policy [12, 32]), etc."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "97ea926d424ad8d426661e6fca702863f6fd8015",
                "externalIds": {
                    "DBLP": "conf/atal/HanLMW22",
                    "ArXiv": "2112.13937",
                    "DOI": "10.5555/3535850.3535915",
                    "CorpusId": 245537932
                },
                "corpusId": 245537932,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/97ea926d424ad8d426661e6fca702863f6fd8015",
                "title": "Multiagent Model-based Credit Assignment for Continuous Control",
                "abstract": "Deep reinforcement learning (RL) has recently shown great promise in robotic continuous control tasks. Nevertheless, prior research in this vein center around the centralized learning setting that largely relies on the communication availability among all the components of a robot. However, agents in the real world often operate in a decentralised fashion without communication due to latency requirements, limited power budgets and safety concerns. By formulating robotic components as a system of decentralised agents, this work presents a decentralised multiagent reinforcement learning framework for continuous control. To this end, we first develop a cooperative multiagent PPO framework that allows for centralized optimisation during training and decentralised operation during execution. However, the system only receives a global reward signal which is not attributed towards each agent. To address this challenge, we further propose a generic game-theoretic credit assignment framework which computes agent-specific reward signals. Last but not least, we also incorporate a model-based RL module into our credit assignment framework, which leads to significant improvement in sample efficiency. We demonstrate the effectiveness of our framework on experimental results on Mujoco locomotion control tasks. For a demo video please visit: https://youtu.be/gFyVPm4svEY.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2829951",
                        "name": "Dongge Han"
                    },
                    {
                        "authorId": "11614724",
                        "name": "Chris Xiaoxuan Lu"
                    },
                    {
                        "authorId": "37436656",
                        "name": "Tomasz P. Michalak"
                    },
                    {
                        "authorId": "48106342",
                        "name": "M. Wooldridge"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, efficiently learning accurate models remains challenging, especially in complex and noisy environments (Janner et al. 2019; Wang et al. 2019; Pan et al. 2020), which creates a hindrance in further improving the sample efficiency of model-based approaches.",
                ", Model-Based Policy Optimization (Janner et al. 2019).",
                ", the dyna-style algorithm, which has recently shown the potential to achieve high sample efficiency (Janner et al. 2019).",
                "Contribution of each component The path from MBPO (Janner et al. 2019) to CMBAC comprises three modifica-\ntions: Q-network size increase (Big), learning multiple estimates of the Q-value (LMEQ), and conservative policy optimization (CPO).",
                "In this section, we present the notation and provide a brief introduction to the state-of-the-art model-based algorithm, i.e., Model-Based Policy Optimization (Janner et al. 2019).",
                "\u2026fall into three categories according to the way of model usage: (1) dynastyle methods (Sutton 1990; Luo et al. 2019; Zhou, Li, and Wang 2020; Janner et al. 2019), which use the model to generate imaginary samples as additional training data; (2) shooting algorithms (de Boer et al. 2005;\u2026",
                "Our work falls into the first category, i.e., the dyna-style algorithm, which has recently shown the potential to achieve high sample efficiency (Janner et al. 2019).",
                "However, many of these results are achieved by model-free algorithms and generally require a massive number of samples, which significantly hinders the applications of modelfree methods in real-world tasks (Kurutach et al. 2018; Janner et al. 2019).",
                "Contribution of each component The path from MBPO (Janner et al. 2019) to CMBAC comprises three modifica-",
                "Model-based policy optimization (MBPO) is a state-of-theart model-based algorithm that has achieved impressive performance (Janner et al. 2019).",
                "Roughly speaking, model-based approaches fall into three categories according to the way of model usage: (1) dynastyle methods (Sutton 1990; Luo et al. 2019; Zhou, Li, and Wang 2020; Janner et al. 2019), which use the model to generate imaginary samples as additional training data; (2) shooting algorithms (de Boer et al.",
                "Model-Based Policy Optimization Model-based policy optimization (MBPO) is a state-of-theart model-based algorithm that has achieved impressive performance (Janner et al. 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b2d7ffedc614de190df02ef8613743b5a76c578d",
                "externalIds": {
                    "DBLP": "conf/aaai/Wang00LL22",
                    "ArXiv": "2112.10504",
                    "DOI": "10.1609/aaai.v36i8.20839",
                    "CorpusId": 245334819
                },
                "corpusId": 245334819,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b2d7ffedc614de190df02ef8613743b5a76c578d",
                "title": "Sample-Efficient Reinforcement Learning via Conservative Model-Based Actor-Critic",
                "abstract": "Model-based reinforcement learning algorithms, which aim to learn a model of the environment to make decisions, are more sample efficient than their model-free counterparts. The sample efficiency of model-based approaches relies on whether the model can well approximate the environment. However, learning an accurate model is challenging, especially in complex and noisy environments. To tackle this problem, we propose the conservative model-based actor-critic (CMBAC), a novel approach that achieves high sample efficiency without the strong reliance on accurate learned models. Specifically, CMBAC learns multiple estimates of the Q-value function from a set of inaccurate models and uses the average of the bottom-k estimates---a conservative estimate---to optimize the policy. An appealing feature of CMBAC is that the conservative estimates effectively encourage the agent to avoid unreliable \u201cpromising actions\u201d---whose values are high in only a small fraction of the models. Experiments demonstrate that CMBAC significantly outperforms state-of-the-art approaches in terms of sample efficiency on several challenging control tasks, and the proposed method is more robust than previous methods in noisy environments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108349247",
                        "name": "Zhihai Wang"
                    },
                    {
                        "authorId": "2146042860",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "2118293921",
                        "name": "Qi Zhou"
                    },
                    {
                        "authorId": "2156072615",
                        "name": "Bin Li"
                    },
                    {
                        "authorId": "2144406784",
                        "name": "Houqiang Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5f3ee78d49e853b8b644d3ad16b18a4b2b8b5d0d",
                "externalIds": {
                    "ArXiv": "2112.07746",
                    "DBLP": "journals/corr/abs-2112-07746",
                    "CorpusId": 245144444
                },
                "corpusId": 245144444,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5f3ee78d49e853b8b644d3ad16b18a4b2b8b5d0d",
                "title": "CEM-GD: Cross-Entropy Method with Gradient Descent Planner for Model-Based Reinforcement Learning",
                "abstract": "Current state-of-the-art model-based reinforcement learning algorithms use trajectory sampling methods, such as the Cross-Entropy Method (CEM), for planning in continuous control settings. These zeroth-order optimizers require sampling a large number of trajectory rollouts to select an optimal action, which scales poorly for large prediction horizons or high dimensional action spaces. First-order methods that use the gradients of the rewards with respect to the actions as an update can mitigate this issue, but suffer from local optima due to the non-convex optimization landscape. To overcome these issues and achieve the best of both worlds, we propose a novel planner, Cross-Entropy Method with Gradient Descent (CEM-GD), that combines first-order methods with CEM. At the beginning of execution, CEM-GD uses CEM to sample a significant amount of trajectory rollouts to explore the optimization landscape and avoid poor local minima. It then uses the top trajectories as initialization for gradient descent and applies gradient updates to each of these trajectories to find the optimal action sequence. At each subsequent time step, however, CEM-GD samples much fewer trajectories from CEM before applying gradient updates. We show that as the dimensionality of the planning problem increases, CEM-GD maintains desirable performance with a constant small number of samples by using the gradient information, while avoiding local optima using initially well-sampled trajectories. Furthermore, CEM-GD achieves better performance than CEM on a variety of continuous control benchmarks in MuJoCo with 100x fewer samples per time step, resulting in around 25% less computation time and 10% less memory usage. The implementation of CEM-GD is available at $\\href{https://github.com/KevinHuang8/CEM-GD}{\\text{https://github.com/KevinHuang8/CEM-GD}}$.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152530947",
                        "name": "Kevin Huang"
                    },
                    {
                        "authorId": "66230713",
                        "name": "Sahin Lale"
                    },
                    {
                        "authorId": "3468386",
                        "name": "Ugo Rosolia"
                    },
                    {
                        "authorId": "48081130",
                        "name": "Yuanyuan Shi"
                    },
                    {
                        "authorId": "2047844",
                        "name": "Anima Anandkumar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Off-Policy Model-Based: In particular, our proposed approaches share similarities with off-policy model-based approaches such as Dyna (Sutton, 1991) and more modern variants for deep RL like MBPO (Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0998c71ab9c2c8ab22388a597bd3dedb591bb950",
                "externalIds": {
                    "ArXiv": "2112.07066",
                    "DBLP": "conf/nips/RiemerRCSTR22",
                    "CorpusId": 245131574
                },
                "corpusId": 245131574,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0998c71ab9c2c8ab22388a597bd3dedb591bb950",
                "title": "Continual Learning In Environments With Polynomial Mixing Times",
                "abstract": "The mixing time of the Markov chain induced by a policy limits performance in real-world continual learning scenarios. Yet, the effect of mixing times on learning in continual reinforcement learning (RL) remains underexplored. In this paper, we characterize problems that are of long-term interest to the development of continual RL, which we call scalable MDPs, through the lens of mixing times. In particular, we theoretically establish that scalable MDPs have mixing times that scale polynomially with the size of the problem. We go on to demonstrate that polynomial mixing times present significant difficulties for existing approaches, which suffer from myopic bias and stale bootstrapped estimates. To validate our theory, we study the empirical scaling behavior of mixing times with respect to the number of tasks and task duration for high performing policies deployed across multiple Atari games. Our analysis demonstrates both that polynomial mixing times do emerge in practice and how their existence may lead to unstable learning behavior like catastrophic forgetting in continual learning settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40497459",
                        "name": "M. Riemer"
                    },
                    {
                        "authorId": "1498636613",
                        "name": "Sharath Chandra Raparthy"
                    },
                    {
                        "authorId": "2421422",
                        "name": "Ignacio Cases"
                    },
                    {
                        "authorId": "2145260867",
                        "name": "G. Subbaraj"
                    },
                    {
                        "authorId": "2775077",
                        "name": "M. P. Touzel"
                    },
                    {
                        "authorId": "2109771",
                        "name": "I. Rish"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ae6c5559fc682802e86343c48409c470719836a7",
                "externalIds": {
                    "ArXiv": "2112.05218",
                    "DBLP": "journals/corr/abs-2112-05218",
                    "CorpusId": 245117307
                },
                "corpusId": 245117307,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ae6c5559fc682802e86343c48409c470719836a7",
                "title": "Learning Generalizable Behavior via Visual Rewrite Rules",
                "abstract": "Though deep reinforcement learning agents have achieved unprecedented success in recent years, their learned policies can be brittle, failing to generalize to even slight modifications of their environments or unfamiliar situations. The black-box nature of the neural network learning dynamics makes it impossible to audit trained deep agents and recover from such failures. In this paper, we propose a novel representation and learning approach to capture environment dynamics without using neural networks. It originates from the observation that, in games designed for people, the effect of an action can often be perceived in the form of local changes in consecutive visual observations. Our algorithm is designed to extract such vision-based changes and condense them into a set of action-dependent descriptive rules, which we call ''visual rewrite rules'' (VRRs). We also present preliminary results from a VRR agent that can explore, expand its rule set, and solve a game via planning with its learned VRR world model. In several classical games, our non-deep agent demonstrates superior performance, extreme sample efficiency, and robust generalization ability compared with several mainstream deep agents.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "26878805",
                        "name": "Yiheng Xie"
                    },
                    {
                        "authorId": "2112131754",
                        "name": "Mingxuan Li"
                    },
                    {
                        "authorId": "2148459804",
                        "name": "Shangqun Yu"
                    },
                    {
                        "authorId": "2056611060",
                        "name": "Michael S. Littman"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020) (Algorithm 2) and we also combine ED2 with MBPO (Janner et al., 2019) in the appendix.",
                "Baselines & Benchmarks: For a comparative study, we take two state-of-the-art MBRL methods: MBPO (Janner et al., 2019) and Dreamer (Hafner et al., 2020) as the baselines, and extend them with ED2 as ED2-MBPO, ED2-Dreamer.",
                "Here we provide the practical combination implementation of ED2 with Dreamer(Hafner et al., 2020) (Algorithm 2) and we also combine ED2 with MBPO (Janner et al., 2019) in the appendix.",
                "Model ensemble is also widely used in model construction for uncertainty estimation, which provides a more reliable prediction (Janner et al., 2019; Pan et al., 2020).",
                "World models are usually formulated with latent dynamics (Janner et al., 2019; Hafner et al., 2020), and the general form of the latent dynamics model can be summarized as follows:\nLatent transition kernel: ht = f(s\u2264t\u22121, a\u2264t\u22121) Stochastic state function: p(st|ht) Reward function: p(rt|ht)\nThe\u2026",
                "World models are usually formulated with latent dynamics (Janner et al., 2019; Hafner et al., 2020), and the general form of the latent dynamics model can be summarized as follows: Latent transition kernel: ht = f(s\u2264t\u22121, a\u2264t\u22121) Stochastic state function: p(st|ht) Reward function: p(rt|ht) The latent transition kernel (shorthand as kernel) predicts the latent state ht with input s\u2264t\u22121 and a\u2264t\u22121.",
                "Baselines & Benchmarks: For a comparative study, we take two state-of-the-art MBRL methods: MBPO (Janner et al., 2019) and Dreamer (Hafner et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "083f3913b21eb88b9a7dd51d290f9befa608ce7a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-02817",
                    "ArXiv": "2112.02817",
                    "CorpusId": 244908729
                },
                "corpusId": 244908729,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/083f3913b21eb88b9a7dd51d290f9befa608ce7a",
                "title": "ED2: An Environment Dynamics Decomposition Framework for World Model Construction",
                "abstract": "Model-based reinforcement learning methods achieve significant sample efficiency in many tasks, but their performance is often limited by the existence of the model error. To reduce the model error, previous works use a single well-designed network to fit the entire environment dynamics, which treats the environment dynamics as a black box. However, these methods lack to consider the environmental decomposed property that the dynamics may contain multiple sub-dynamics, which can be modeled separately, allowing us to construct the world model more accurately. In this paper, we propose the Environment Dynamics Decomposition (ED2), a novel world model construction framework that models the environment in a decomposing manner. ED2 contains two key components: sub-dynamics discovery (SD2) and dynamics decomposition prediction (D2P). SD2 discovers the sub-dynamics in an environment and then D2P constructs the decomposed world model following the sub-dynamics. ED2 can be easily combined with existing MBRL algorithms and empirical results show that ED2 significantly reduces the model error and boosts the performance of the state-of-the-art MBRL algorithms on various tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2155373269",
                        "name": "Cong Wang"
                    },
                    {
                        "authorId": "3449314",
                        "name": "Tianpei Yang"
                    },
                    {
                        "authorId": "40513470",
                        "name": "Jianye Hao"
                    },
                    {
                        "authorId": "1752775197",
                        "name": "Yan Zheng"
                    },
                    {
                        "authorId": "31190626",
                        "name": "Hongyao Tang"
                    },
                    {
                        "authorId": "2143198655",
                        "name": "Fazl Barez"
                    },
                    {
                        "authorId": "2124810107",
                        "name": "Jinyi Liu"
                    },
                    {
                        "authorId": "2122807664",
                        "name": "J. Peng"
                    },
                    {
                        "authorId": "66711267",
                        "name": "Haiyin Piao"
                    },
                    {
                        "authorId": "2133864289",
                        "name": "Zhixiao Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other side of things stands the hands off approach taken in the RL community, where general and unstructured neural networks are used for both transition models [9, 52, 22] as well as policies and value functions [17].",
                "State of the art model based approaches on Mujoco tend to use an ensemble of small MLPs that predict the state transitions [9, 52, 22, 2], without exploiting any structure of the state space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7502bf121c7103660b3f7ff1b5033b705263c246",
                "externalIds": {
                    "DBLP": "conf/nips/FinziBW21",
                    "ArXiv": "2112.01388",
                    "CorpusId": 244799202
                },
                "corpusId": 244799202,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7502bf121c7103660b3f7ff1b5033b705263c246",
                "title": "Residual Pathway Priors for Soft Equivariance Constraints",
                "abstract": "There is often a trade-off between building deep learning systems that are expressive enough to capture the nuances of the reality, and having the right inductive biases for efficient learning. We introduce Residual Pathway Priors (RPPs) as a method for converting hard architectural constraints into soft priors, guiding models towards structured solutions, while retaining the ability to capture additional complexity. Using RPPs, we construct neural network priors with inductive biases for equivariances, but without limiting flexibility. We show that RPPs are resilient to approximate or misspecified symmetries, and are as effective as fully constrained models even when symmetries are exact. We showcase the broad applicability of RPPs with dynamical systems, tabular data, and reinforcement learning. In Mujoco locomotion tasks, where contact forces and directional rewards violate strict equivariance assumptions, the RPP outperforms baseline model-free RL agents, and also improves the learned transition models for model-based RL.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51007156",
                        "name": "Marc Finzi"
                    },
                    {
                        "authorId": "2031420788",
                        "name": "Gregory W. Benton"
                    },
                    {
                        "authorId": "145771261",
                        "name": "A. Wilson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-free approaches tend to require many more interactions with the environment [22], hence our choice of a model-based approach for this work."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "af5a3e085b84544f684a7036b3b4d3ea9deea879",
                "externalIds": {
                    "ArXiv": "2112.00529",
                    "DBLP": "journals/corr/abs-2112-00529",
                    "DOI": "10.1016/j.mechmachtheory.2021.104654",
                    "CorpusId": 244773483
                },
                "corpusId": 244773483,
                "publicationVenue": {
                    "id": "dcae6ae5-2d7b-4ea9-8a6d-824ccbd3b843",
                    "name": "Mechanism and Machine Theory",
                    "type": "journal",
                    "alternate_names": [
                        "Mech Mach Theory"
                    ],
                    "issn": "0094-114X",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/303/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/mechanism-and-machine-theory",
                        "http://www.sciencedirect.com/science/journal/0094114X"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/af5a3e085b84544f684a7036b3b4d3ea9deea879",
                "title": "Improving gearshift controllers for electric vehicles with reinforcement learning",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144212889",
                        "name": "Marc-Antoine Beaudoin"
                    },
                    {
                        "authorId": "2500923",
                        "name": "B. Boulet"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Lemma C.3 (Theorem 2 of (Syed, Bowling, and Schapire 2008)). if \u03c1 \u2208 D, then \u03c1 is the occupancy measure for \u03c0\u03c1 (a|s) , \u03c1(s,a)\u2211\na\u2032 \u03c1(s,a \u2032) , and \u03c0\u03c1 is the only policy whose oc-\ncupancy measure is \u03c1. Lemma C.4 (Lemma 3.2 of (Ho and Ermon 2016)) Let H\u0304 (\u03c1) = \u2212 \u2211 s,a \u03c1 (s, a) log (\u03c1(s, a)/ \u2211 a\u2032 \u03c1 (s,\u2026"
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "bd2d5dfd424e6342f33ebeeebcdaf7974db684c9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-11711",
                    "ArXiv": "2111.11711",
                    "CorpusId": 244488702
                },
                "corpusId": 244488702,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bd2d5dfd424e6342f33ebeeebcdaf7974db684c9",
                "title": "Sample Efficient Imitation Learning via Reward Function Trained in Advance",
                "abstract": "Imitation learning (IL) is a framework that learns to imitate expert behavior from demonstrations. Recently, IL shows promising results on high dimensional and control tasks. However, IL typically suffers from sample inefficiency in terms of environment interaction, which severely limits their application to simulated domains. In industrial applications, learner usually have a high interaction cost, the more interactions with environment, the more damage it causes to the environment and the learner itself. In this article, we make an effort to improve sample efficiency by introducing a novel scheme of inverse reinforcement learning. Our method, which we call Model Reward Function Based Imitation Learning (MRFIL), uses an ensemble dynamic model as a reward function, what is trained with expert demonstrations. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by providing a positive reward upon encountering states in line with the expert demonstration distribution. In addition, we demonstrate the convergence guarantee for new objective function. Experimental results show that our algorithm reaches the competitive performance and significantly reducing the environment interactions compared to IL methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108065822",
                        "name": "Lihua Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Compared to Soft-Actor-Critic (SAC), which is model-free and uses a UTD of 1, MBPO achieves much higher sample efficiency in the OpenAI MuJoCo benchmark (Todorov et al., 2012; Brockman et al., 2016).",
                "In particular, Model-Based Policy Optimization (MBPO) (Janner et al., 2019) uses a large UTD ratio of 20-40."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b9183999b5acdb320a8880a7375c9eb92403b1b5",
                "externalIds": {
                    "ArXiv": "2111.09159",
                    "DBLP": "journals/corr/abs-2111-09159",
                    "CorpusId": 244270282
                },
                "corpusId": 244270282,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b9183999b5acdb320a8880a7375c9eb92403b1b5",
                "title": "Aggressive Q-Learning with Ensembles: Achieving Both High Sample Efficiency and High Asymptotic Performance",
                "abstract": "Recent advances in model-free deep reinforcement learning (DRL) show that simple model-free methods can be highly effective in challenging high-dimensional continuous control tasks. In particular, Truncated Quantile Critics (TQC) achieves state-of-the-art asymptotic training performance on the MuJoCo benchmark with a distributional representation of critics; and Randomized Ensemble Double Q-Learning (REDQ) achieves high sample efficiency that is competitive with state-of-the-art model-based methods using a high update-to-data ratio and target randomization. In this paper, we propose a novel model-free algorithm, Aggressive Q-Learning with Ensembles (AQE), which improves the sample-efficiency performance of REDQ and the asymptotic performance of TQC, thereby providing overall state-of-the-art performance during all stages of training. Moreover, AQE is very simple, requiring neither distributional representation of critics nor target randomization. The effectiveness of AQE is further supported by our extensive experiments, ablations, and theoretical results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2134152070",
                        "name": "Yanqiu Wu"
                    },
                    {
                        "authorId": null,
                        "name": "Xinyue Chen"
                    },
                    {
                        "authorId": "50879688",
                        "name": "Che Wang"
                    },
                    {
                        "authorId": null,
                        "name": "Yiming Zhang"
                    },
                    {
                        "authorId": "2142727747",
                        "name": "Zijian Zhou"
                    },
                    {
                        "authorId": "1829862",
                        "name": "K. Ross"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although some theoretical analysis [17, 12, 22] and thorough empirical evaluation [30] have been conducted in the previous literature, it remains unclear how to appropriately schedule the hyperparameters to achieve optimum performance when training a Dyna-style MBRL algorithm.",
                "On all these tasks, the MBPO instance trained with hyperparameters scheduled by AutoMBPO can significantly surpass the one with original configuration [12].",
                "[12] manually design a schedule that linearly increases the rollout length across epochs.",
                "Although real data ratio is an essential factor empirically [13, 12], it has not yet been studied thoroughly in theory.",
                "MBRL methods can be roughly categorized into four types according to different model usage: (i) Dyna-style algorithms [28, 17, 4, 14, 12, 15] leverage the model to generate imaginary data and adopt some off-the-shelf MFRL algorithms to train a policy using both real data and imaginary data; (ii) shooting algorithms [19, 3] use model predictive control (MPC) to plan directly without explicit policy; (iii) analytic-gradient algorithms [7, 16, 5] search policies with back-propagation through time by exploiting the model derivatives; (iv) model-augmented value expansion algorithms [9, 2] utilize model rollouts to improve the target value for temporal difference (TD) updates.",
                "When optimizing the policy, we can use merely imaginary data [17, 14] or a mixture of real data and imaginary one in a fixed ratio [12].",
                "is also critical [12] since too short rollout length fails to sufficiently leverage the model to plan forward, while too long rollout length may bring disastrous compounding error [1].",
                "Since Dyna-style algorithms can seamlessly take advantage of innovations in MFRL literature and have recently shown impressive performance [12, 30], this paper mainly focuses on Dyna-style algorithms.",
                "[12] fix the ratio of real data as 5%, while Kalweit and Boedecker [13] use the uncertainty to adaptively choose the ratio, which tends to use more imaginary samples initially and gradually use more real samples afterward.",
                "As for imaginary data generation, it is suggested to generate short rollouts to reduce compounding model error [12], and the model\u2019s uncertainty is further incorporated to choose reliable imaginary data dynamically [20]."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "8d6f76c623e818021bc3dd754d2077d9dafc6307",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-08550",
                    "ArXiv": "2111.08550",
                    "CorpusId": 244129955
                },
                "corpusId": 244129955,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8d6f76c623e818021bc3dd754d2077d9dafc6307",
                "title": "On Effective Scheduling of Model-based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning has attracted wide attention due to its su-perior sample ef\ufb01ciency. Despite its impressive success so far, it is still unclear how to appropriately schedule the important hyperparameters to achieve adequate performance, such as the real data ratio for policy optimization in Dyna-style model-based algorithms. In this paper, we \ufb01rst theoretically analyze the role of real data in policy training, which suggests that gradually increasing the ratio of real data yields better performance. Inspired by the analysis, we propose a framework named AutoMBPO to automatically schedule the real data ratio as well as other hyperparameters in training model-based policy optimization (MBPO) algorithm, a representative running case of model-based methods. On several continuous control tasks, the MBPO instance trained with hyperparameters scheduled by AutoMBPO can signi\ufb01cantly surpass the original one, and the real data ratio schedule found by AutoMBPO shows consistency with our theoretical analysis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2054235888",
                        "name": "Hang Lai"
                    },
                    {
                        "authorId": "2115732606",
                        "name": "Jian Shen"
                    },
                    {
                        "authorId": "2108309275",
                        "name": "Weinan Zhang"
                    },
                    {
                        "authorId": "49866668",
                        "name": "Yimin Huang"
                    },
                    {
                        "authorId": "102356716",
                        "name": "Xingzhi Zhang"
                    },
                    {
                        "authorId": "2824766",
                        "name": "Ruiming Tang"
                    },
                    {
                        "authorId": "2119021541",
                        "name": "Yong Yu"
                    },
                    {
                        "authorId": "7718952",
                        "name": "Zhenguo Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The idea of integrating model-based and model-free RL has also been studied independently of planning (Pong et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e1ed9c7976628a03b7e1d8ea0386c97c45c8ec2f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-07908",
                    "ArXiv": "2111.07908",
                    "CorpusId": 244117851
                },
                "corpusId": 244117851,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e1ed9c7976628a03b7e1d8ea0386c97c45c8ec2f",
                "title": "Learning to Execute: Efficient Learning of Universal Plan-Conditioned Policies in Robotics",
                "abstract": "Applications of Reinforcement Learning (RL) in robotics are often limited by high data demand. On the other hand, approximate models are readily available in many robotics scenarios, making model-based approaches like planning a data-efficient alternative. Still, the performance of these methods suffers if the model is imprecise or wrong. In this sense, the respective strengths and weaknesses of RL and model-based planners are. In the present work, we investigate how both approaches can be integrated into one framework that combines their strengths. We introduce Learning to Execute (L2E), which leverages information contained in approximate plans to learn universal policies that are conditioned on plans. In our robotic manipulation experiments, L2E exhibits increased performance when compared to pure RL, pure planning, or baseline methods combining learning and planning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2091687904",
                        "name": "Ingmar Schubert"
                    },
                    {
                        "authorId": "30837327",
                        "name": "Danny Driess"
                    },
                    {
                        "authorId": "32408015",
                        "name": "Ozgur S. Oguz"
                    },
                    {
                        "authorId": "144918851",
                        "name": "Marc Toussaint"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In general, we cannot guarantee that the reward model will generalize well to regions of the state-action space that are underexplored by the logging policy (Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5d0c25cd1eb410e04eaa108565e084ae4e72e0ef",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-08587",
                    "ArXiv": "2111.08587",
                    "CorpusId": 244130288
                },
                "corpusId": 244130288,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5d0c25cd1eb410e04eaa108565e084ae4e72e0ef",
                "title": "Offline Contextual Bandits for Wireless Network Optimization",
                "abstract": "The explosion in mobile data traffic together with the ever-increasing expectations for higher quality of service call for the development of AI algorithms for wireless network optimization. In this paper, we investigate how to learn policies that can automatically adjust the configuration parameters of every cell in the network in response to the changes in the user demand. Our solution combines existent methods for offline learning and adapts them in a principled way to overcome crucial challenges arising in this context. Empirical results suggest that our proposed method will achieve important performance gains when deployed in the real network while satisfying practical constrains on computational efficiency.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1429191192",
                        "name": "Miguel Suau"
                    },
                    {
                        "authorId": "1703555",
                        "name": "A. Agapitos"
                    },
                    {
                        "authorId": "152483360",
                        "name": "David Lynch"
                    },
                    {
                        "authorId": "2140580376",
                        "name": "Derek Farrell"
                    },
                    {
                        "authorId": "2123361048",
                        "name": "M. Zhou"
                    },
                    {
                        "authorId": "1845912878",
                        "name": "Aleksandar Milenovic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this work, STEVE is used as the base learning algorithm, but our framework is generally applicable to other model-based algorithms with uncertainty quantification, such as MBPO, M2AC, etc.",
                "ME-TRPO [26] and MBPO [27] are directly trained on imaginary data to accelerate policy learning.",
                "The learned dynamics model can be used for planning [4, 23], value expansion [24, 25], or imaginary training [26, 27, 28]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "84a935e8717afe779a3c431a36d2e745606773c0",
                "externalIds": {
                    "DBLP": "conf/corl/Xu0DZBT21",
                    "ArXiv": "2111.05819",
                    "CorpusId": 243938382
                },
                "corpusId": 243938382,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/84a935e8717afe779a3c431a36d2e745606773c0",
                "title": "Look Before You Leap: Safe Model-Based Reinforcement Learning with Human Intervention",
                "abstract": "Safety has become one of the main challenges of applying deep reinforcement learning to real world systems. Currently, the incorporation of external knowledge such as human oversight is the only means to prevent the agent from visiting the catastrophic state. In this paper, we propose MBHI, a novel framework for safe model-based reinforcement learning, which ensures safety in the state-level and can effectively avoid both \u201dlocal\u201d and \u201dnon-local\u201d catastrophes. An ensemble of supervised learners are trained in MBHI to imitate human blocking decisions. Similar to human decision-making process, MBHI will roll out an imagined trajectory in the dynamics model before executing actions to the environment, and estimate its safety. When the imagination encounters a catastrophe, MBHI will block the current action and use an efficient MPC method to output a safety policy. We evaluate our method on several safety tasks, and the results show that MBHI achieved better performance in terms of sample efficiency and number of catastrophes compared to the baselines.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "66607858",
                        "name": "Yunkun Xu"
                    },
                    {
                        "authorId": "47781645",
                        "name": "Zhenyu Liu"
                    },
                    {
                        "authorId": "2274907",
                        "name": "Guifang Duan"
                    },
                    {
                        "authorId": "2146281505",
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "authorId": "2114141403",
                        "name": "X. Bai"
                    },
                    {
                        "authorId": "40504890",
                        "name": "Jianrong Tan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "63 In model-based reinforcement learning (MBRL), there are also many algorithms that constrain the 64 exploitation in the environment with effective uncertainty estimation methods [23, 24, 25, 26, 27, 10]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "798da6871400a119e70a46dba963e0bcdf2986d5",
                "externalIds": {
                    "ArXiv": "2111.05440",
                    "DBLP": "conf/corl/LiTTZ21",
                    "CorpusId": 237263626
                },
                "corpusId": 237263626,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/798da6871400a119e70a46dba963e0bcdf2986d5",
                "title": "Dealing with the Unknown: Pessimistic Offline Reinforcement Learning",
                "abstract": "Reinforcement Learning (RL) has been shown effective in domains where the agent can learn policies by actively interacting with its operating environment. However, if we change the RL scheme to offline setting where the agent can only update its policy via static datasets, one of the major issues in offline reinforcement learning emerges, i.e. distributional shift. We propose a Pessimistic Offline Reinforcement Learning (PessORL) algorithm to actively lead the agent back to the area where it is familiar by manipulating the value function. We focus on problems caused by out-of-distribution (OOD) states, and deliberately penalize high values at states that are absent in the training dataset, so that the learned pessimistic value function lower bounds the true value anywhere within the state space. We evaluate the PessORL algorithm on various benchmark tasks, where we show that our method gains better performance by explicitly handling OOD states, when compared to those methods merely considering OOD actions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109377974",
                        "name": "Jinning Li"
                    },
                    {
                        "authorId": "1491105028",
                        "name": "Chen Tang"
                    },
                    {
                        "authorId": "1680165",
                        "name": "M. Tomizuka"
                    },
                    {
                        "authorId": "144267500",
                        "name": "W. Zhan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As the baseline of our framework is built upon MBPO implementation, we derive the \u201csame hyperparameters\u201d for our experiments and all the baseline algorithms.",
                "Further, the number of interactions with the true environment for outer loop policy were kept constant to 1000 for each epoch, same as MoPAC and MBPO.",
                "Our rewards in Ant-v2 were comparable with MoPAC but still significantly better than MBPO.",
                "This will be compared with the existing approaches MoPAC [12] and MBPO [10] on the benchmark MuJoCo control environments.",
                "Model Based Policy Optimisation (MBPO) [10] introduced the outer loop policy to collect transition to train approximate model and sample over it to train the policy.",
                "We would like to emphasize that our final rewards are eventually the same as achieved by MoPAC and MBPO, however the progress rate is faster for all our experiments with lesser true environment interactions.",
                "Several experiments were conducted on the MuJoCo [18] continuous control tasks with the OpenAI-Gym benchmark and the performance was compared with recent related works MoPAC [12] and MBPO [10]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "e06b2fc2fe5bf9e2d2ece2201bd84d18cb82be1a",
                "externalIds": {
                    "ArXiv": "2112.02999",
                    "DBLP": "conf/icra/MishraSGKLSSBK22",
                    "DOI": "10.1109/icra46639.2022.9812089",
                    "CorpusId": 244908302
                },
                "corpusId": 244908302,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e06b2fc2fe5bf9e2d2ece2201bd84d18cb82be1a",
                "title": "Dynamic Mirror Descent based Model Predictive Control for Accelerating Robot Learning",
                "abstract": "Recent works in Reinforcement Learning (RL) combine model-free (Mf)-RL algorithms with model-based (Mb)-RL approaches to get the best from both: asymptotic performance of Mf-RL and high sample-efficiency of Mb-RL. Inspired by these works, we propose a hierarchical framework that integrates online learning for the Mb-trajectory optimization with off-policy methods for the Mf-RL. In particular, two loops are proposed, where the Dynamic Mirror Descent based Model Predictive Control (DMD-MPC) is used as the inner loop Mb-RL to obtain an optimal sequence of actions. These actions are in turn used to significantly accelerate the outer loop Mf-RL. We show that our formulation is generic for a broad class of MPC based policies and objectives, and includes some of the well-known Mb-Mf approaches. We finally introduce a new algorithm: Mirror-Descent Model Predictive RL (M-DeMoRL), which uses Cross-Entropy Method (CEM) with elite fractions for the inner loop. Our experiments show faster convergence of the proposed hierarchical approach on benchmark MuJoCo tasks. We also demonstrate hardware training for trajectory tracking in a 2R leg, and hardware transfer for robust walking in a quadruped. We show that the inner-loop Mb-RL significantly decreases the number of training iterations required in the hardware setting, thereby validating the proposed approach.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1927638269",
                        "name": "Utkarsh Aashu Mishra"
                    },
                    {
                        "authorId": "2134899829",
                        "name": "Soumya R. Samineni"
                    },
                    {
                        "authorId": "2000932039",
                        "name": "Prakhar Goel"
                    },
                    {
                        "authorId": "2143195117",
                        "name": "Chandravaran Kunjeti"
                    },
                    {
                        "authorId": "92468070",
                        "name": "Himanshu Lodha"
                    },
                    {
                        "authorId": "2151564277",
                        "name": "Aman Singh"
                    },
                    {
                        "authorId": "66660719",
                        "name": "Aditya Sagi"
                    },
                    {
                        "authorId": "143683893",
                        "name": "S. Bhatnagar"
                    },
                    {
                        "authorId": "2767923",
                        "name": "Shishir Kolathaya"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2 FACTORS OF GENERALIZATION Planning Model-based RL is an active area of research [28, 46] with the majority of work focusing on gains in data efficiency [26, 27, 34, 37, 61], though it is often motivated by a desire for better zero- or few-shot generalization as well [18, 29, 48, 64, 70]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "44164c068499fbe387a1765104d69a8cbc5f0327",
                "externalIds": {
                    "ArXiv": "2111.01587",
                    "DBLP": "journals/corr/abs-2111-01587",
                    "CorpusId": 240419913
                },
                "corpusId": 240419913,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/44164c068499fbe387a1765104d69a8cbc5f0327",
                "title": "Procedural Generalization by Planning with Self-Supervised World Models",
                "abstract": "One of the key promises of model-based reinforcement learning is the ability to generalize using an internal model of the world to make predictions in novel environments and tasks. However, the generalization ability of model-based agents is not well understood because existing work has focused on model-free agents when benchmarking generalization. Here, we explicitly measure the generalization ability of model-based agents in comparison to their model-free counterparts. We focus our analysis on MuZero (Schrittwieser et al., 2020), a powerful model-based agent, and evaluate its performance on both procedural and task generalization. We identify three factors of procedural generalization -- planning, self-supervised representation learning, and procedural data diversity -- and show that by combining these techniques, we achieve state-of-the art generalization performance and data efficiency on Procgen (Cobbe et al., 2019). However, we find that these factors do not always provide the same benefits for the task generalization benchmarks in Meta-World (Yu et al., 2019), indicating that transfer remains a challenge and may require different approaches than procedural generalization. Overall, we suggest that building generalizable agents requires moving beyond the single-task, model-free paradigm and towards self-supervised model-based agents that are trained in rich, procedural, multi-task environments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "12679121",
                        "name": "Ankesh Anand"
                    },
                    {
                        "authorId": "1944655894",
                        "name": "Jacob Walker"
                    },
                    {
                        "authorId": "2144417088",
                        "name": "Yazhe Li"
                    },
                    {
                        "authorId": "2136446499",
                        "name": "Eszter V'ertes"
                    },
                    {
                        "authorId": "4337102",
                        "name": "Julian Schrittwieser"
                    },
                    {
                        "authorId": "1955694",
                        "name": "Sherjil Ozair"
                    },
                    {
                        "authorId": "143947744",
                        "name": "T. Weber"
                    },
                    {
                        "authorId": "2158860",
                        "name": "Jessica B. Hamrick"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2d7af0dbf019ff6961364f761038ce0781654641",
                "externalIds": {
                    "ArXiv": "2110.15237",
                    "CorpusId": 247518543
                },
                "corpusId": 247518543,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2d7af0dbf019ff6961364f761038ce0781654641",
                "title": "Data Informed Residual Reinforcement Learning for High-Dimensional Robotic Tracking Control",
                "abstract": "The learning inefficiency of reinforcement learning (RL) from scratch hinders its practical application towards continuous robotic tracking control, especially for high-dimensional robots. This work proposes a data informed residual reinforcement learning (DR-RL) based robotic tracking control scheme applicable to robots with high dimensionality. The proposed DR-RL methodology outperforms its standard RL from scratch counterpart regarding sample efficiency and scalability. Specifically, we first decouple the original robot into low-dimensional robotic subsystems; and further utilize one-step backward (OSBK) data to construct incremental subsystems that are equivalent model-free representations of the above decoupled robotic subsystems. The formulated incremental subsystems allow for parallel learning to relieve computation load and offer us mathematical descriptions of robotic movements for conducting theoretical analysis. Then, we apply DR-RL to learn the tracking control policy, a combination of incremental base policy and incremental residual policy, under a parallel learning architecture. The incremental residual policy uses the guidance from the incremental base policy as the learning initialization and further learns from interactions with environments to endow the tracking control policy with adaptability towards dynamically changing environments. Our proposed DR-RL based tracking control scheme is developed with rigorous theoretical analysis of system stability and weight convergence, and validated numerically on comparative simulations and also experimentally on a 3-DoF robot manipulator that would fail for other counterpart RL methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116684932",
                        "name": "Cong Li"
                    },
                    {
                        "authorId": "3051026",
                        "name": "Fangzhou Liu"
                    },
                    {
                        "authorId": "2108745806",
                        "name": "Yongchao Wang"
                    },
                    {
                        "authorId": "20631844",
                        "name": "M. Buss"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This can be alleviated in practice by rolling out the state predictions on short horizons, similarly as in Janner et al. (2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4e09f0183497351d69adba495e638f5d62db69e1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-13576",
                    "ArXiv": "2110.13576",
                    "CorpusId": 239885352
                },
                "corpusId": 239885352,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4e09f0183497351d69adba495e638f5d62db69e1",
                "title": "Learning Robust Controllers Via Probabilistic Model-Based Policy Search",
                "abstract": "Model-based Reinforcement Learning estimates the true environment through a world model in order to approximate the optimal policy. This family of algorithms usually benefits from better sample efficiency than their model-free counterparts. We investigate whether controllers learned in such a way are robust and able to generalize under small perturbations of the environment. Our work is inspired by the PILCO algorithm, a method for probabilistic policy search. We show that enforcing a lower bound to the likelihood noise in the Gaussian Process dynamics model regularizes the policy updates and yields more robust controllers. We demonstrate the empirical benefits of our method in a simulation benchmark.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "151243782",
                        "name": "V. Charvet"
                    },
                    {
                        "authorId": "11002730",
                        "name": "B. S. Jensen"
                    },
                    {
                        "authorId": "1402170019",
                        "name": "R. Murray-Smith"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020), due to the higher sample-efficiency compared to model-free algorithms (Chua et al., 2018; Gal et al., 2016; Janner et al., 2019; Nagabandi et al., 2018; Wang et al., 2019).",
                ", 2016; Gal & Ghahramani, 2016) or ensembles of probabilistic NNs (Janner et al., 2019; Chua et al., 2018).",
                "\u2026are a popular alternative with several implementation variants, e.g. deterministic NNs (Nagabandi et al., 2018), Bayesian NNs relying on the Monte Carlo (MC) Dropout approximation (Gal et al., 2016; Gal & Ghahramani, 2016) or ensembles of probabilistic NNs (Janner et al., 2019; Chua et al., 2018).",
                "\u2026attention as this approach is expected to close the gap between simulated and real world tasks (Nagabandi et al., 2020), due to the higher sample-efficiency compared to model-free algorithms (Chua et al., 2018; Gal et al., 2016; Janner et al., 2019; Nagabandi et al., 2018; Wang et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a2ebbef89ff2bb341a69032e63af5a4ce187688e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-13079",
                    "ArXiv": "2110.13079",
                    "CorpusId": 239768597
                },
                "corpusId": 239768597,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a2ebbef89ff2bb341a69032e63af5a4ce187688e",
                "title": "Which Model To Trust: Assessing the Influence of Models on the Performance of Reinforcement Learning Algorithms for Continuous Control Tasks",
                "abstract": "The need for algorithms able to solve Reinforcement Learning (RL) problems with few trials has motivated the advent of model-based RL methods. The reported performance of model-based algorithms has dramatically increased within recent years. However, it is not clear how much of the recent progress is due to improved algorithms or due to improved models. While different modeling options are available to choose from when applying a model-based approach, the distinguishing traits and particular strengths of different models are not clear. The main contribution of this work lies precisely in assessing the model influence on the performance of RL algorithms. A set of commonly adopted models is established for the purpose of model comparison. These include Neural Networks (NNs), ensembles of NNs, two different approximations of Bayesian NNs (BNNs), that is, the Concrete Dropout NN and the Anchored Ensembling, and Gaussian Processes (GPs). The model comparison is evaluated on a suite of continuous control benchmarking tasks. Our results reveal that significant differences in model performance do exist. The Concrete Dropout NN reports persistently superior performance. We summarize these differences for the benefit of the modeler and suggest that the model choice is tailored to the standards required by each specific application.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2134901070",
                        "name": "Giacomo Arcieri"
                    },
                    {
                        "authorId": "101338979",
                        "name": "David W\u00f6lfle"
                    },
                    {
                        "authorId": "66773557",
                        "name": "E. Chatzi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The automatic learning and use of the agent-environment interaction model have shown its effectiveness in game [13, 14] and robotic [15, 16] environments."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9291fab6800d152955c813331ef9465efad3d07f",
                "externalIds": {
                    "ArXiv": "2110.13241",
                    "DBLP": "journals/corr/abs-2110-13241",
                    "CorpusId": 239885911
                },
                "corpusId": 239885911,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9291fab6800d152955c813331ef9465efad3d07f",
                "title": "Multitask Adaptation by Retrospective Exploration with Learned World Models",
                "abstract": "Model-based reinforcement learning (MBRL) allows solving complex tasks in a sample-efficient manner. However, no information is reused between the tasks. In this work, we propose a meta-learned addressing model called RAMa that provides training samples for the MBRL agent taken from continuously growing task-agnostic storage. The model is trained to maximize the expected agent's performance by selecting promising trajectories solving prior tasks from the storage. We show that such retrospective exploration can accelerate the learning process of the MBRL agent by better informing learned dynamics and prompting agent with exploratory trajectories. We test the performance of our approach on several domains from the DeepMind control suite, from Metaworld multitask benchmark, and from our bespoke environment implemented with a robotic NVIDIA Isaac simulator to test the ability of the model to act in a photorealistic, ray-traced environment.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1416761815",
                        "name": "Artem Zholus"
                    },
                    {
                        "authorId": "35234816",
                        "name": "A. Panov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Simulation results show that the proposed methodology is better than or at least as good as MoPAC [15] and MBPO [8] in terms of sample-efficiency.",
                "In the context of robotics, this model has proven to be very beneficial in developing robust control strategies based on predictive simulations [8].",
                "We will compare the DeMo RL with the existing approaches MoPAC [15] and MBPO [8] on the benchmark MuJoCo control environments.",
                "This shows that our formulation is more generic and some of the existing approaches could be derived with suitable choice: [15, 1] and [8].",
                "1 DeMo RL: Results and Comparison Several experiments were conducted on the MuJoCo [25] continuous control tasks with the OpenAI-Gym benchmark and the performance was compared with recent related works MoPAC [15] and MBPO [8]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9d95379a397a6a63c0cb9e4a8016cb8ac3d399bc",
                "externalIds": {
                    "ArXiv": "2110.12239",
                    "DBLP": "journals/corr/abs-2110-12239",
                    "CorpusId": 239768232
                },
                "corpusId": 239768232,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9d95379a397a6a63c0cb9e4a8016cb8ac3d399bc",
                "title": "Policy Search using Dynamic Mirror Descent MPC for Model Free Off Policy RL",
                "abstract": "Recent works in Reinforcement Learning (RL) combine model-free (Mf)-RL algorithms with model-based (Mb)-RL approaches to get the best from both: asymptotic performance of Mf-RL and high sample-efficiency of Mb-RL. Inspired by these works, we propose a hierarchical framework that integrates online learning for the Mb-trajectory optimization with off-policy methods for the Mf-RL. In particular, two loops are proposed, where the Dynamic Mirror Descent based Model Predictive Control (DMD-MPC) is used as the inner loop to obtain an optimal sequence of actions. These actions are in turn used to significantly accelerate the outer loop Mf-RL. We show that our formulation is generic for a broad class of MPC based policies and objectives, and includes some of the well-known Mb-Mf approaches. Based on the frame work we define two algorithms to increase sample efficiency of Off Policy RL and to guide end to end RL algorithms for online adaption respectively. Thus we finally introduce two novel algorithms: DynamicMirror Descent Model Predictive RL (DeMoRL), which uses the method of elite fractions for the inner loop and Soft Actor-Critic (SAC) as the off-policy RL for the outer loop and Dynamic-Mirror Descent Model Predictive Layer (DeMo Layer), a special case of hierarchical framework which guides linear policies trained using Augmented Random Search(ARS). Our experiments show faster convergence of the proposed DeMo RL, and better or equal performance compared to other Mf-Mb approaches on benchmark MuJoCo control tasks. The DeMo Layer was tested on classical Cartpole and custom built Quadruped trained using Linear Policy Approach. The results shows that DeMo Layer significantly increases performance of the Linear Policy in both the settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2134899829",
                        "name": "Soumya R. Samineni"
                    }
                ]
            }
        },
        {
            "contexts": [
                "to learn a model from offline data, with minimal modification to the policy learning (Kidambi et al., 2020; Yu et al., 2020; Janner et al., 2019).",
                "Model-based methods attempt\nto learn a model from offline data, with minimal modification to the policy learning (Kidambi et al., 2020; Yu et al., 2020; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b7ba291af983b5e31f3adfa9a0ceb7a7b3114c7f",
                "externalIds": {
                    "ArXiv": "2110.09796",
                    "DBLP": "journals/corr/abs-2110-09796",
                    "CorpusId": 239024500
                },
                "corpusId": 239024500,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b7ba291af983b5e31f3adfa9a0ceb7a7b3114c7f",
                "title": "Offline Reinforcement Learning with Value-based Episodic Memory",
                "abstract": "Offline reinforcement learning (RL) shows promise of applying RL to real-world problems by effectively utilizing previously collected data. Most existing offline RL algorithms use regularization or constraints to suppress extrapolation error for actions outside the dataset. In this paper, we adopt a different framework, which learns the V-function instead of the Q-function to naturally keep the learning procedure within the support of an offline dataset. To enable effective generalization while maintaining proper conservatism in offline learning, we propose Expectile V-Learning (EVL), which smoothly interpolates between the optimal value learning and behavior cloning. Further, we introduce implicit planning along offline trajectories to enhance learned V-values and accelerate convergence. Together, we present a new offline method called Value-based Episodic Memory (VEM). We provide theoretical analysis for the convergence properties of our proposed VEM method, and empirical results in the D4RL benchmark show that our method achieves superior performance in most tasks, particularly in sparse-reward tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2125106047",
                        "name": "Xiaoteng Ma"
                    },
                    {
                        "authorId": "2145436135",
                        "name": "Yiqin Yang"
                    },
                    {
                        "authorId": "1752767710",
                        "name": "Haotian Hu"
                    },
                    {
                        "authorId": "2112261363",
                        "name": "Qihan Liu"
                    },
                    {
                        "authorId": "2146157882",
                        "name": "Jun Yang"
                    },
                    {
                        "authorId": "2111387140",
                        "name": "Chongjie Zhang"
                    },
                    {
                        "authorId": "36281262",
                        "name": "Qianchuan Zhao"
                    },
                    {
                        "authorId": "144691690",
                        "name": "Bin Liang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The baseline algorithms are REDQ [3], MBPO [19], SAC [16], TD3 [6] and TQC [9].",
                "For MBPO1, REDQ2, TD33 and TQC4, we use the authors\u2019s code.",
                "L G\n] 1\n0 N\nov 2\n02 1\n2 0.0 1.0 2.0 3.0 Time Steps 1e5 0.0 2.0 4.0 6.0 Av er ag e R et ur n 1e3 Ant 0.0 0.2 0.4 0.6 0.8 1.0 Time Steps 1e6 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1e4 Humanmoid 0.0 1.0 2.0 3.0 Time Steps 1e5 0.0 1.0 2.0 3.0 4.0 1e3 Hopper\nRAC-SAC RAC-TD3 REDQ SAC TQC TQC20 MBPO\n0.0 1.0 2.0 3.0 Time Steps 1e5\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n1e3 Walker2d\nFig.",
                "We compare to state-of-the-art algorithms: SAC [16], TD3 [6], MBPO [19], REDQ [3] and TQC [9] on 4 challenging continuous control tasks (Walker2d, HalfCheetah, Ant and Humanoid) from MuJoCo environments [18] in the OpenAI gym benchmark [17].",
                "the-art algorithms (MBPO [19], REDQ [3] and TQC [9]), achieving state-of-the-art sample efficiency on the Humanoid benchmark.",
                "The shaded areas denote one standard deviation.\nthe-art algorithms (MBPO [19], REDQ [3] and TQC [9]), achieving state-of-the-art sample efficiency on the Humanoid benchmark."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5363112778f8e71a69ca717611550bbeaabc0be2",
                "externalIds": {
                    "ArXiv": "2110.09712",
                    "DBLP": "journals/corr/abs-2110-09712",
                    "CorpusId": 239024732
                },
                "corpusId": 239024732,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5363112778f8e71a69ca717611550bbeaabc0be2",
                "title": "Balancing Value Underestimation and Overestimationwith Realistic Actor-Critic",
                "abstract": "Model-free deep reinforcement learning (RL) has been successfully applied to challenging continuous control domains. However, poor sample ef\ufb01ciency prevents these methods from being widely used in real-world domains. This paper introduces a novel model-free algorithm, Realistic Actor-Critic(RAC), which can be incorporated with any off-policy RL algorithms to improve sample ef\ufb01ciency. RAC employs Universal Value Function Approximators (UVFA) to simultaneously learn a policy family with the same neural network, each with different trade-offs between underestimation and overestimation. To learn such policies, we introduce uncertainty punished Q-learning, which uses uncertainty from the ensembling of multiple critics to build various con\ufb01dence-bounds of Q-function. We evaluate RAC on the MuJoCo benchmark, achieving 10x sample ef\ufb01ciency and 25% performance improvement on the most challenging Humanoid environment compared to SAC.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143204744",
                        "name": "Sicen Li"
                    },
                    {
                        "authorId": "2096527",
                        "name": "G. Wang"
                    },
                    {
                        "authorId": "2151875551",
                        "name": "Qinyun Tang"
                    },
                    {
                        "authorId": "49681190",
                        "name": "Li-quan Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "An important line of research focuses on correctly balancing real-world experience with data generated from the internal model of the agent [25, 11]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d825092cebfba0321e821d669e837fc03d5fb1f9",
                "externalIds": {
                    "DBLP": "conf/nips/MazzagliaVD21",
                    "ArXiv": "2110.10083",
                    "CorpusId": 239024470
                },
                "corpusId": 239024470,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d825092cebfba0321e821d669e837fc03d5fb1f9",
                "title": "Contrastive Active Inference",
                "abstract": "Active inference is a unifying theory for perception and action resting upon the idea that the brain maintains an internal model of the world by minimizing free energy. From a behavioral perspective, active inference agents can be seen as self-evidencing beings that act to fulfill their optimistic predictions, namely preferred outcomes or goals. In contrast, reinforcement learning requires human-designed rewards to accomplish any desired outcome. Although active inference could provide a more natural self-supervised objective for control, its applicability has been limited because of the shortcomings in scaling the approach to complex environments. In this work, we propose a contrastive objective for active inference that strongly reduces the computational burden in learning the agent's generative model and planning future actions. Our method performs notably better than likelihood-based active inference in image-based tasks, while also being computationally cheaper and easier to train. We compare to reinforcement learning agents that have access to human-designed reward functions, showing that our approach closely matches their performance. Finally, we also show that contrastive methods perform significantly better in the case of distractors in the environment and that our method is able to generalize goals to variations in the background.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2098445287",
                        "name": "Pietro Mazzaglia"
                    },
                    {
                        "authorId": "2413244",
                        "name": "Tim Verbelen"
                    },
                    {
                        "authorId": "1733741",
                        "name": "B. Dhoedt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that in contrast to the result by Janner et al. (2019), Eq. (5) is a bound on the policy improvement instead of a lower bound on \u03b7n+1. The first error term compares \u03b7n+1 and \u03b7\u0303n+1, the performance estimation gap under the optimized policy \u03c0n+1 that we obtain in Line 4 of Algorithm 1. Since at this point we have only collected data with \u03c0n in Line 2, this term depends on the generalization properties of our model to new data; what we call the off-policy model error. For our data-based model p\u0303 that just replays data under \u03c0n independently of the action, this term can be bounded for stochastic policies. For example, Schulman et al. (2015) bound it by the average KL-divergence between \u03c0n and \u03c0n+1.",
                "In the following section, we investigate the impact of OPC on a range of continuous control tasks and compare it to the current state-of-the-art model-based and model-free RL algorithms MBPO (Janner et al., 2019) and SAC (Haarnoja et al.",
                "Note that in contrast to the result by Janner et al. (2019), Eq.",
                "The original data for MBPO and the other baselines were provided by Janner et al. (2019).",
                "Our implementation is based on the code from MBPO (Janner et al., 2019), which is open-sourced under the MIT license.",
                "If the reward function is known, this term only depends on the model quality of p\u0303 relative to p. Note that in contrast to the result by Janner et al. (2019), Eq.",
                "To alleviate this issue, one could extend Theorem 1 similarly to the results in Janner et al. (2019), such that the rollouts are only of length H T .",
                "For H \u2192\u221e, we obtain the original result form Janner et al. (2019) with \u2211 t\u22651 t\u03b3 t = \u03b3/(1\u2212\u03b3)2.",
                "Janner et al. (2019) use the learned model only for short simulated rollouts starting from real observed states to mitigate the issue of compounding errors for long-term predictions.",
                "The comparison includes the following methods:\n\u2022 MBPO (Janner et al., 2019), \u2022 PETS (Chua et al., 2018), \u2022 SAC (Haarnoja et al., 2018), \u2022 PPO (Schulman et al., 2017), \u2022 STEVE (Buckman et al., 2018), \u2022 SLBO (Luo et al., 2018).",
                "F IMPLEMENTATION AND COMPUTATIONAL RESOURCES\nOur implementation is based on the code from MBPO (Janner et al., 2019), which is open-sourced under the MIT license.",
                "\u2026choice of initial state distribution \u03c1\u0303(s0) depends on the optimization method: For model predictive control, Chua et al. (2018) use the current state to optimize the next action based on a rollout, while Janner et al. (2019) use the empirical state distribution observed from the true environment.",
                "\u2026Schulman et al. (2015) to provide guarantees for MBRL. Luo et al. (2018) provide a general framework to show monotonic improvement towards a local optimum of the value function, while Janner et al. (2019) present a lower-bound on performance for different rollout schemes and horizon lengths.",
                "Note that this result contradicts the findings by Janner et al. (2019) that one-step predictions are oftentimes sufficient for MBPO.",
                "Our implementation is based upon the code from Janner et al. (2019).",
                "In the following section, we investigate the impact of OPC on a range of continuous control tasks and compare it to the current state-of-the-art model-based and model-free RL algorithms MBPO (Janner et al., 2019) and SAC (Haarnoja et al., 2018).",
                ", 2019), robotics (Kalashnikov et al., 2018) and neural architecture search (Zoph & Le, 2016). All of these model-free approaches rely on large numbers of interactions with the environment to ensure successful learning. While this issue is less severe for environments that can easily be simulated, it limits the applicability of model-free RL to (real-world) domains where data is scarce. Model-based RL (MBRL) aims to reduce the amount of data required for policy optimization by learning a model that approximates the true environment, which we can use to generate cheap, simulated state transitions (Sutton, 1990; Racani\u00e8re et al., 2017; Moerland et al., 2020). While early approaches on low-dimensional tasks by Schneider (1997); Deisenroth & Rasmussen (2011) used probabilistic models with closed-form posteriors, recent methods rely on neural networks to scale to more complex tasks on discrete (Kaiser et al.",
                "Consequently, for prediction horizons shorter than the effective horizon encoded in the discount factor, H   \u03b3/(1 \u2212 \u03b3), the on-policy error term grows as O(min(H/(1 \u2212 \u03b3), H2)), c.f., (Janner et al., 2019) and Appendix A.3.",
                "For the dynamics model p\u0303model, we follow Chua et al. (2018); Janner et al. (2019) and use a probabilistic ensemble of neural networks, where each head predicts a Gaussian distribution over the next state and reward.",
                "The results for all baselines were obtained from Janner et al. (2019) via personal communication.",
                "For H \u2192\u221e, we obtain the original result form Janner et al. (2019) with \u2211 t\u22651 t\u03b3\nt = \u03b3/(1\u2212\u03b3)2."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "c425b527bbeffc2e4b5bc7a42649cd16a3fa216f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-07985",
                    "ArXiv": "2110.07985",
                    "CorpusId": 239009555
                },
                "corpusId": 239009555,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c425b527bbeffc2e4b5bc7a42649cd16a3fa216f",
                "title": "On-Policy Model Errors in Reinforcement Learning",
                "abstract": "Model-free reinforcement learning algorithms can compute policy gradients given sampled environment transitions, but require large amounts of data. In contrast, model-based methods can use the learned model to generate new data, but model errors and bias can render learning unstable or suboptimal. In this paper, we present a novel method that combines real-world data and a learned model in order to get the best of both worlds. The core idea is to exploit the real-world data for on-policy predictions and use the learned model only to generalize to different actions. Specifically, we use the data as time-dependent on-policy correction terms on top of a learned model, to retain the ability to generate data without accumulating errors over long prediction horizons. We motivate this method theoretically and show that it counteracts an error term for model-based policy improvement. Experiments on MuJoCo- and PyBullet-benchmarks show that our method can drastically improve existing model-based approaches without introducing additional tuning parameters.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2131019148",
                        "name": "Lukas P. Frohlich"
                    },
                    {
                        "authorId": "2064594336",
                        "name": "Maksym Lefarov"
                    },
                    {
                        "authorId": "2176899",
                        "name": "M. Zeilinger"
                    },
                    {
                        "authorId": "2064772",
                        "name": "Felix Berkenkamp"
                    }
                ]
            }
        },
        {
            "contexts": [
                "SAC-RCBF, we leverage the partially learned dynamics, the reward function and the RCBF constraints to generate shorthorizon rollouts as in [35]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "22e586f420b596e398072b07b1d4a08c2a067c04",
                "externalIds": {
                    "ArXiv": "2110.05415",
                    "DOI": "10.1109/lra.2022.3216996",
                    "CorpusId": 249954121
                },
                "corpusId": 249954121,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/22e586f420b596e398072b07b1d4a08c2a067c04",
                "title": "Safe Reinforcement Learning Using Robust Control Barrier Functions",
                "abstract": "Reinforcement Learning (RL) has been shown to be effective in many scenarios. However, it typically requires the exploration of a sufficiently large number of state-action pairs, some of which may be unsafe. Consequently, its application to safety-critical systems remains a challenge. An increasingly common approach to address safety involves the addition of a safety layer that projects the RL actions onto a safe set of actions. In turn, a difficulty for such frameworks is how to effectively couple RL with the safety layer to improve the learning performance. In this paper, we frame safety as a differentiable robust-control-barrier-function layer in a model-based RL framework. Moreover, we also propose an approach to modularly learn the underlying reward-driven task, independent of safety constraints. We demonstrate that this approach both ensures safety and effectively guides exploration during training in a range of experiments, including zero-shot transfer when the reward is learned in a modular way.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153025282",
                        "name": "Y. Emam"
                    },
                    {
                        "authorId": "2383894",
                        "name": "Gennaro Notomista"
                    },
                    {
                        "authorId": "2226508",
                        "name": "Paul Glotfelter"
                    },
                    {
                        "authorId": "145276578",
                        "name": "Z. Kira"
                    },
                    {
                        "authorId": "152599370",
                        "name": "M. Egerstedt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "At present the number of models used has not been discussed since MBPO, which trains seven probabilistic dynamics models of the same architecture (with different initializations), using only the top five models based on validation accuracy (referred to as \u201cElites\u201d in the Evolutionary community, e.g. Mouret & Clune (2015)).",
                ", 2020) follows the successful online RL algorithm MBPO (Janner et al., 2019) but trains inside a conservative MDP, penalizing the reward based on the maximum aleatoric uncertainty over the ensemble members.",
                "D), but conversely can improve performance when errors are properly managed (Janner et al., 2019; Pan et al., 2020).",
                "MOPO (Yu et al., 2020) follows the successful online RL algorithm MBPO (Janner et al., 2019) but trains inside a conservative MDP, penalizing the reward based on the maximum aleatoric uncertainty over the ensemble members."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "877e8140cb7f26042f6c5f1eefcf68a2748721f0",
                "externalIds": {
                    "DBLP": "conf/iclr/LuBPOR22",
                    "ArXiv": "2110.04135",
                    "CorpusId": 247476396
                },
                "corpusId": 247476396,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/877e8140cb7f26042f6c5f1eefcf68a2748721f0",
                "title": "Revisiting Design Choices in Offline Model Based Reinforcement Learning",
                "abstract": "Offline reinforcement learning enables agents to leverage large pre-collected datasets of environment transitions to learn control policies, circumventing the need for potentially expensive or unsafe online data collection. Significant progress has been made recently in offline model-based reinforcement learning, approaches which leverage a learned dynamics model. This typically involves constructing a probabilistic model, and using the model uncertainty to penalize rewards where there is insufficient data, solving for a pessimistic MDP that lower bounds the true MDP. Existing methods, however, exhibit a breakdown between theory and practice, whereby pessimistic return ought to be bounded by the total variation distance of the model from the true dynamics, but is instead implemented through a penalty based on estimated model uncertainty. This has spawned a variety of uncertainty heuristics, with little to no comparison between differing approaches. In this paper, we compare these heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. Using these insights, we show that selecting these key hyperparameters using Bayesian Optimization produces superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110752472",
                        "name": "Cong Lu"
                    },
                    {
                        "authorId": "2053179501",
                        "name": "Philip J. Ball"
                    },
                    {
                        "authorId": "1410302742",
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "authorId": "2053878538",
                        "name": "Michael A. Osborne"
                    },
                    {
                        "authorId": "145029236",
                        "name": "S. Roberts"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MBPO (Janner et al. 2019), state-of-the-art, model-based algorithm on OpenAI Gym.",
                "2021) and model-based algorithms (Janner et al. 2019) advanced RL\u2019s sample-efficiency on several benchmark tasks.",
                "2016), GPL-SAC outperforms both model-based (Janner et al. 2019) and model-free (Chen et al.",
                "Inline with the other considered state-of-the-art baselines (Chen et al. 2021; Janner et al. 2019), we use an increased ensemble size and update-to-data (UTD) ratio for the critic.",
                "Within model-based RL, recent works have achieved remarkable sample efficiency by learning large ensembles of dynamic models for better predictions (Chua et al. 2018; Wang and Ba 2019; Janner et al. 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4729d7b34c60de21fc5f7c9e0e9525936f644ca6",
                "externalIds": {
                    "DBLP": "conf/aaai/CetinC23",
                    "ArXiv": "2110.03375",
                    "DOI": "10.1609/aaai.v37i6.25852",
                    "CorpusId": 238419669
                },
                "corpusId": 238419669,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4729d7b34c60de21fc5f7c9e0e9525936f644ca6",
                "title": "Learning Pessimism for Reinforcement Learning",
                "abstract": "Off-policy deep reinforcement learning algorithms commonly compensate for overestimation bias during temporal-difference learning by utilizing pessimistic estimates of the expected target returns. In this work, we propose Generalized Pessimism Learning (GPL), a strategy employing a novel learnable penalty to enact such pessimism. In particular, we propose to learn this penalty alongside the critic with dual TD-learning, a new procedure to estimate and minimize the magnitude of the target returns bias with trivial computational cost. GPL enables us to accurately counteract overestimation bias throughout training without incurring the downsides of overly pessimistic targets. By integrating GPL with popular off-policy algorithms, we achieve state-of-the-art results in both competitive proprioceptive and pixel-based benchmarks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2052306608",
                        "name": "Edoardo Cetin"
                    },
                    {
                        "authorId": "1386958722",
                        "name": "O. \u00c7eliktutan"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020) and it is possible to train policies on model rollouts to improve data efficiency (Sutton, 1991; Janner et al., 2019).",
                "\u2026two of these approaches: value gradients can be backpropagated through dynamics models to improve credit assignment (Nguyen & Widrow, 1990; Heess et al., 2015; Amos et al., 2020) and it is possible to train policies on model rollouts to improve data efficiency (Sutton, 1991; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "90ab9f71262e03ba7429b3fc4b631aa8ab1ddd28",
                "externalIds": {
                    "DBLP": "conf/iclr/ByravanHTMITSAH22",
                    "ArXiv": "2110.03363",
                    "CorpusId": 238419356
                },
                "corpusId": 238419356,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/90ab9f71262e03ba7429b3fc4b631aa8ab1ddd28",
                "title": "Evaluating model-based planning and planner amortization for continuous control",
                "abstract": "There is a widespread intuition that model-based control methods should be able to surpass the data efficiency of model-free approaches. In this paper we attempt to evaluate this intuition on various challenging locomotion tasks. We take a hybrid approach, combining model predictive control (MPC) with a learned model and model-free policy learning; the learned policy serves as a proposal for MPC. We find that well-tuned model-free agents are strong baselines even for high DoF control problems but MPC with learned proposals and models (trained on the fly or transferred from related tasks) can significantly improve performance and data efficiency in hard multi-task/multi-goal settings. Finally, we show that it is possible to distil a model-based planner into a policy that amortizes the planning computation without any loss of performance. Videos of agents performing different tasks can be seen at https://sites.google.com/view/mbrl-amortization/home.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2631257",
                        "name": "Arunkumar Byravan"
                    },
                    {
                        "authorId": "40401956",
                        "name": "Leonard Hasenclever"
                    },
                    {
                        "authorId": "1388022743",
                        "name": "P. Trochim"
                    },
                    {
                        "authorId": "145316259",
                        "name": "M. Berk Mirza"
                    },
                    {
                        "authorId": "46403120",
                        "name": "Alessandro Davide Ialongo"
                    },
                    {
                        "authorId": "2109481",
                        "name": "Yuval Tassa"
                    },
                    {
                        "authorId": "2060551",
                        "name": "J. T. Springenberg"
                    },
                    {
                        "authorId": "2799799",
                        "name": "A. Abdolmaleki"
                    },
                    {
                        "authorId": "2801204",
                        "name": "N. Heess"
                    },
                    {
                        "authorId": "1879232",
                        "name": "J. Merel"
                    },
                    {
                        "authorId": "3137672",
                        "name": "Martin A. Riedmiller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most model-based RL methods use maximum likelihood to fit the dynamics model and then use RL to maximize the expected return under samples from that model [10, 11, 23, 24, 48].",
                "(Left) On the OpenAI gym benchmark [7], MnM-approx performs on par with a prior state-of-the-art method (MBPO [24]), while consistently outperforming a recent method that addresses objective mismatch (VMBPO [9]).",
                ", using maximum likelihood), but apply these models by using data sampled from the learned dynamics [11, 23, 24, 48].",
                "Following prior work [24], we learn a neural network to predict the true environment rewards.",
                "We use MBPO [24] as a baseline for model-based RL because it achieves state-of-the-art results and is a prototypical example of model-based RL algorithms that use maximum likelihood models.",
                "Figure 6: Alternative model learning objectives: Using the DClawScrewFixed-v0 task, we compare MnMapprox and MBPO [24] to two additional model learning objectives suggested in the literature, VAML [17] and value-weighted maximum likelihood [29]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5a8b4d43cdf73ef7218dfa5cb305064a4f55b8b5",
                "externalIds": {
                    "DBLP": "conf/nips/EysenbachKLS22",
                    "ArXiv": "2110.02758",
                    "CorpusId": 238408209
                },
                "corpusId": 238408209,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5a8b4d43cdf73ef7218dfa5cb305064a4f55b8b5",
                "title": "Mismatched No More: Joint Model-Policy Optimization for Model-Based RL",
                "abstract": "Many model-based reinforcement learning (RL) methods follow a similar template: fit a model to previously observed data, and then use data from that model for RL or planning. However, models that achieve better training performance (e.g., lower MSE) are not necessarily better for control: an RL agent may seek out the small fraction of states where an accurate model makes mistakes, or it might act in ways that do not expose the errors of an inaccurate model. As noted in prior work, there is an objective mismatch: models are useful if they yield good policies, but they are trained to maximize their accuracy, rather than the performance of the policies that result from them. In this work, we propose a single objective for jointly training the model and the policy, such that updates to either component increase a lower bound on expected return. To the best of our knowledge, this is the first lower bound for model-based RL that holds globally and can be efficiently estimated in continuous settings; it is the only lower bound that mends the objective mismatch problem. A version of this bound becomes tight under certain assumptions. Optimizing this bound resembles a GAN: a classifier distinguishes between real and fake transitions, the model is updated to produce transitions that look realistic, and the policy is updated to avoid states where the model predictions are unrealistic. Numerical simulations demonstrate that optimizing this bound yields reward maximizing policies and yields dynamics that (perhaps surprisingly) can aid in exploration. We also show that a deep RL algorithm loosely based on our lower bound can achieve performance competitive with prior model-based methods, and better performance on certain hard exploration tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8140754",
                        "name": "Benjamin Eysenbach"
                    },
                    {
                        "authorId": "121873407",
                        "name": "Alexander Khazatsky"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "145124475",
                        "name": "R. Salakhutdinov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Encouraged by the success of MBPO, many RL methods with high UTD ratios have been proposed (Shen et al., 2020; Lai et al., 2020).",
                "Following Chen et al. (2021b); Janner et al. (2019), we prepared the following environments: Hopper, Walker2d, Ant, and Humanoid.",
                "Ensemble transition models: Ensembles of transition (and reward) models have been introduced to model-based RL, e.g., (Chua et al., 2018; Kurutach et al., 2018; Janner et al., 2019; Shen et al., 2020; Yu et al., 2020; Lee et al., 2020; Hiraoka et al., 2021; Abraham et al., 2020).",
                "Chen et al. (2021b) demonstrated that the sample efficiency of REDQ is equal to or even better than that of MBPO.",
                "Model-Based Policy Optimization (MBPO) (Janner et al., 2019) is a seminal RL method that uses a high UTD ratio of 20\u201340 and achieves significantly higher sample efficiency than SAC, which uses a UTD ratio of 1.",
                "REDQ runs 1.1 to 1.4 times faster than MBPO (Chen et al., 2021b) but is still less computationally efficient than non-ensemble-based RL methods (e.g., SAC) due to the use of large ensembles.",
                "For this reason, instead of dropout approaches, ensemble approaches have been used in RL with high UTD ratio settings (Chen et al., 2021b; Janner et al., 2019; Shen et al., 2020; Hiraoka et al., 2021; Lai et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b2d931da61559c528c5d4eadcb939425a2531652",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-02034",
                    "ArXiv": "2110.02034",
                    "CorpusId": 238353966
                },
                "corpusId": 238353966,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b2d931da61559c528c5d4eadcb939425a2531652",
                "title": "Dropout Q-Functions for Doubly Efficient Reinforcement Learning",
                "abstract": "Randomized ensembled double Q-learning (REDQ) (Chen et al., 2021b) has recently achieved state-of-the-art sample efficiency on continuous-action reinforcement learning benchmarks. This superior sample efficiency is made possible by using a large Q-function ensemble. However, REDQ is much less computationally efficient than non-ensemble counterparts such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018a). To make REDQ more computationally efficient, we propose a method of improving computational efficiency called DroQ, which is a variant of REDQ that uses a small ensemble of dropout Q-functions. Our dropout Q-functions are simple Q-functions equipped with dropout connection and layer normalization. Despite its simplicity of implementation, our experimental results indicate that DroQ is doubly (sample and computationally) efficient. It achieved comparable sample efficiency with REDQ, much better computational efficiency than REDQ, and comparable computational efficiency with that of SAC.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3027595",
                        "name": "Takuya Hiraoka"
                    },
                    {
                        "authorId": "22312167",
                        "name": "Takahisa Imagawa"
                    },
                    {
                        "authorId": "2117567401",
                        "name": "Taisei Hashimoto"
                    },
                    {
                        "authorId": "2058274526",
                        "name": "Takashi Onishi"
                    },
                    {
                        "authorId": "143946906",
                        "name": "Yoshimasa Tsuruoka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MBPO [5] optimises the length of model-generated rollouts to avoid compounding model errors, while Ha and Schmidhuber [28] optimise stochasticity in the imagined environment to prevent exploitation of a deterministic model.",
                "Indeed, Yang et al. [33] find that MBPO and MVE [20] perform poorly in goal-based sparse-reward tasks."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "808bf26de91c06138d8d7b352f2080779b380c10",
                "externalIds": {
                    "ArXiv": "2110.02414",
                    "DBLP": "journals/corr/abs-2110-02414",
                    "CorpusId": 238408301
                },
                "corpusId": 238408301,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/808bf26de91c06138d8d7b352f2080779b380c10",
                "title": "Imaginary Hindsight Experience Replay: Curious Model-based Learning for Sparse Reward Tasks",
                "abstract": "Model-based reinforcement learning is a promising learning strategy for practical robotic applications due to its improved data-efficiency versus model-free counterparts. However, current state-of-the-art model-based methods rely on shaped reward signals, which can be difficult to design and implement. To remedy this, we propose a simple model-based method tailored for sparse-reward multi-goal tasks that foregoes the need for complicated reward engineering. This approach, termed Imaginary Hindsight Experience Replay, minimises real-world interactions by incorporating imaginary data into policy updates. To improve exploration in the sparse-reward setting, the policy is trained with standard Hindsight Experience Replay and endowed with curiosity-based intrinsic rewards. Upon evaluation, this approach provides an order of magnitude increase in data-efficiency on average versus the state-of-the-art model-free method in the benchmark OpenAI Gym Fetch Robotics tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144722083",
                        "name": "Robert McCarthy"
                    },
                    {
                        "authorId": "144869332",
                        "name": "S. Redmond"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Examples of these neural network dynamic models (NNDM) include virtual world models in video 34 games or dynamic models of a complicated integrated robot, etc [7, 8]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bf0537b35036ebf16616aa626c169b5814c2f766",
                "externalIds": {
                    "DBLP": "conf/l4dc/WeiL22",
                    "ArXiv": "2110.01110",
                    "CorpusId": 237344984
                },
                "corpusId": 237344984,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bf0537b35036ebf16616aa626c169b5814c2f766",
                "title": "Safe Control with Neural Network Dynamic Models",
                "abstract": "Safety is critical in autonomous robotic systems. A safe control law ensures forward invariance of a safe set (a subset in the state space). It has been extensively studied regarding how to derive a safe control law with a control-affine analytical dynamic model. However, in complex environments and tasks, it is challenging and time-consuming to obtain a principled analytical model of the system. In these situations, data-driven learning is extensively used and the learned models are encoded in neural networks. How to formally derive a safe control law with Neural Network Dynamic Models (NNDM) remains unclear due to the lack of computationally tractable methods to deal with these black-box functions. In fact, even finding the control that minimizes an objective for NNDM without any safety constraint is still challenging. In this work, we propose MIND-SIS (Mixed Integer for Neural network Dynamic model with Safety Index Synthesis), the first method to derive safe control laws for NNDM. The method includes two parts: 1) SIS: an algorithm for the offline synthesis of the safety index (also called as barrier function), which uses evolutionary methods and 2) MIND: an algorithm for online computation of the optimal and safe control signal, which solves a constrained optimization using a computationally efficient encoding of neural networks. It has been theoretically proved that MIND-SIS guarantees forward invariance and finite convergence. And it has been numerically validated that MIND-SIS achieves safe and optimal control of NNDM. From our experiments, the optimality gap is less than $10^{-8}$, and the safety constraint violation is $0$.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51045762",
                        "name": "Tianhao Wei"
                    },
                    {
                        "authorId": "3164672",
                        "name": "Changliu Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent work has shown that model-based RL algorithms can be a magnitude more dataefficient on some problems [5, 6, 7, 8, 9, 10]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c7ea5f0d4e6c27364c557b90e827e2572f619ca3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-00808",
                    "ArXiv": "2110.00808",
                    "DOI": "10.1007/978-3-031-25072-9_38",
                    "CorpusId": 237258305
                },
                "corpusId": 237258305,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c7ea5f0d4e6c27364c557b90e827e2572f619ca3",
                "title": "Cycle-Consistent World Models for Domain Independent Latent Imagination",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1404309092",
                        "name": "Sidney Bender"
                    },
                    {
                        "authorId": "2066556486",
                        "name": "Tim Joseph"
                    },
                    {
                        "authorId": "32244386",
                        "name": "J. M. Z\u00f6llner"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020) that follows MBPO (Janner et al., 2019) with additional reward penalties and MBOP (Argenson and Dulac-Arnold, 2020) that learns an offline model to perform online planning.",
                "MOPO (Yu et al., 2020) follows MBPO Janner et al. (2019) with additional reward penalty on unreliable model-generated transitions.",
                "We also compare against model-based approaches including MOPO (Yu et al., 2020) that follows MBPO (Janner et al., 2019) with additional reward penalties and MBOP (Argenson and Dulac-Arnold, 2020) that learns an offline model to perform online planning."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ac215cee04eb7a5c41b8973c335ad5e82a836b3a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-00894",
                    "ArXiv": "2110.00894",
                    "CorpusId": 238259485
                },
                "corpusId": 238259485,
                "publicationVenue": {
                    "id": "2486528b-036c-4f3c-953f-c574eb381d12",
                    "name": "Asian Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Mach Learn",
                        "ACML"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=40"
                },
                "url": "https://www.semanticscholar.org/paper/ac215cee04eb7a5c41b8973c335ad5e82a836b3a",
                "title": "BRAC+: Improved Behavior Regularized Actor Critic for Offline Reinforcement Learning",
                "abstract": "Online interactions with the environment to collect data samples for training a Reinforcement Learning (RL) agent is not always feasible due to economic and safety concerns. The goal of Offline Reinforcement Learning is to address this problem by learning effective policies using previously collected datasets. Standard off-policy RL algorithms are prone to overestimations of the values of out-of-distribution (less explored) actions and are hence unsuitable for Offline RL. Behavior regularization, which constraints the learned policy within the support set of the dataset, has been proposed to tackle the limitations of standard off-policy algorithms. In this paper, we improve the behavior regularized offline reinforcement learning and propose BRAC+. First, we propose quantification of the out-of-distribution actions and conduct comparisons between using Kullback-Leibler divergence versus using Maximum Mean Discrepancy as the regularization protocol. We propose an analytical upper bound on the KL divergence as the behavior regularizer to reduce variance associated with sample based estimations. Second, we mathematically show that the learned Q values can diverge even using behavior regularized policy update under mild assumptions. This leads to large overestimations of the Q values and performance deterioration of the learned policy. To mitigate this issue, we add a gradient penalty term to the policy evaluation objective. By doing so, the Q values are guaranteed to converge. On challenging offline RL benchmarks, BRAC+ outperforms the baseline behavior regularized approaches by 40%~87% and the state-of-the-art approach by 6%.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115812350",
                        "name": "Chi Zhang"
                    },
                    {
                        "authorId": "2873546",
                        "name": "S. Kuppannagari"
                    },
                    {
                        "authorId": "1728271",
                        "name": "V. Prasanna"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9e8df3b1fad0e219bd96d254784dee40736057ff",
                "externalIds": {
                    "DBLP": "conf/nips/WangLJZLZ21",
                    "ArXiv": "2110.00188",
                    "CorpusId": 238253061
                },
                "corpusId": 238253061,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9e8df3b1fad0e219bd96d254784dee40736057ff",
                "title": "Offline Reinforcement Learning with Reverse Model-based Imagination",
                "abstract": "In offline reinforcement learning (offline RL), one of the main challenges is to deal with the distributional shift between the learning policy and the given dataset. To address this problem, recent offline RL methods attempt to introduce conservatism bias to encourage learning in high-confidence areas. Model-free approaches directly encode such bias into policy or value function learning using conservative regularizations or special network structures, but their constrained policy search limits the generalization beyond the offline dataset. Model-based approaches learn forward dynamics models with conservatism quantifications and then generate imaginary trajectories to extend the offline datasets. However, due to limited samples in offline datasets, conservatism quantifications often suffer from overgeneralization in out-of-support regions. The unreliable conservative measures will mislead forward model-based imaginations to undesired areas, leading to overaggressive behaviors. To encourage more conservatism, we propose a novel model-based offline RL framework, called Reverse Offline Model-based Imagination (ROMI). We learn a reverse dynamics model in conjunction with a novel reverse policy, which can generate rollouts leading to the target goal states within the offline dataset. These reverse imaginations provide informed data augmentation for model-free policy learning and enable conservative generalization beyond the offline dataset. ROMI can effectively combine with off-the-shelf model-free algorithms to enable model-based generalization with proper conservatism. Empirical results show that our method can generate more conservative behaviors and achieve state-of-the-art performance on offline RL benchmark tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110364952",
                        "name": "Jianhao Wang"
                    },
                    {
                        "authorId": "2135914554",
                        "name": "Wenzhe Li"
                    },
                    {
                        "authorId": "2152631650",
                        "name": "Haozhe Jiang"
                    },
                    {
                        "authorId": "48924218",
                        "name": "Guangxiang Zhu"
                    },
                    {
                        "authorId": "2118155623",
                        "name": "Siyuan Li"
                    },
                    {
                        "authorId": "2111387140",
                        "name": "Chongjie Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thus, another important direction for future work would be to reduce sample complexity to increase the feasibility of real robot training; perhaps achievable via a model-based reinforcement learning approach [18,33].",
                "Model-based RL methods, which explicitly learn a model of their environment, have been proposed to further improve sample complexity [17,18,19], and have seen success in real robot settings (e."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c58c5051859996768006aaf59f88d51f0e939b3b",
                "externalIds": {
                    "DBLP": "conf/aics/McCarthySWBMOR21",
                    "ArXiv": "2109.15233",
                    "CorpusId": 239050487
                },
                "corpusId": 239050487,
                "publicationVenue": {
                    "id": "ca07bf14-5b42-44fa-8fe3-58eb49d74e32",
                    "name": "Irish Conference on Artificial Intelligence and Cognitive Science",
                    "type": "conference",
                    "alternate_names": [
                        "AICS",
                        "Ir Conf Artif Intell Cogn Sci"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c58c5051859996768006aaf59f88d51f0e939b3b",
                "title": "Solving the Real Robot Challenge Using Deep Reinforcement Learning",
                "abstract": "This paper details our winning submission to Phase 1 of the 2021 Real Robot Challenge; a challenge in which a three-fingered robot must carry a cube along specified goal trajectories. To solve Phase 1, we use a pure reinforcement learning approach which requires minimal expert knowledge of the robotic system, or of robotic grasping in general. A sparse, goal-based reward is employed in conjunction with Hindsight Experience Replay to teach the control policy to move the cube to the desired x and y coordinates of the goal. Simultaneously, a dense distance-based reward is employed to teach the policy to lift the cube to the z coordinate (the height component) of the goal. The policy is trained in simulation with domain randomisation before being transferred to the real robot for evaluation. Although performance tends to worsen after this transfer, our best policy can successfully lift the real cube along goal trajectories via an effective pinching grasp. Our approach outperforms all other submissions, including those leveraging more traditional robotic control techniques, and is the first pure learning-based method to solve this challenge.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144722083",
                        "name": "Robert McCarthy"
                    },
                    {
                        "authorId": "2065325865",
                        "name": "Francisco Rold\u00e1n S\u00e1nchez"
                    },
                    {
                        "authorId": "2183630851",
                        "name": "Qiang Wang"
                    },
                    {
                        "authorId": "8399970",
                        "name": "David C\u00f3rdova Bulens"
                    },
                    {
                        "authorId": "145470864",
                        "name": "Kevin McGuinness"
                    },
                    {
                        "authorId": "98536322",
                        "name": "N. O\u2019Connor"
                    },
                    {
                        "authorId": "144869332",
                        "name": "S. Redmond"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fcd1d1f393025ae7d7ce11b960421d38fbdf319b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-14311",
                    "ArXiv": "2109.14311",
                    "CorpusId": 238215780
                },
                "corpusId": 238215780,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fcd1d1f393025ae7d7ce11b960421d38fbdf319b",
                "title": "Learning Dynamics Models for Model Predictive Agents",
                "abstract": "Model-Based Reinforcement Learning involves learning a \\textit{dynamics model} from data, and then using this model to optimise behaviour, most often with an online \\textit{planner}. Much of the recent research along these lines presents a particular set of design choices, involving problem definition, model learning and planning. Given the multiple contributions, it is difficult to evaluate the effects of each. This paper sets out to disambiguate the role of different design choices for learning dynamics models, by comparing their performance to planning with a ground-truth model -- the simulator. First, we collect a rich dataset from the training sequence of a model-free agent on 5 domains of the DeepMind Control Suite. Second, we train feed-forward dynamics models in a supervised fashion, and evaluate planner performance while varying and analysing different model design choices, including ensembling, stochasticity, multi-step training and timestep size. Besides the quantitative analysis, we describe a set of qualitative findings, rules of thumb, and future research directions for planning with learned dynamics models. Videos of the results are available at https://sites.google.com/view/learning-better-models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49199170",
                        "name": "M. Lutter"
                    },
                    {
                        "authorId": "40401956",
                        "name": "Leonard Hasenclever"
                    },
                    {
                        "authorId": "2631257",
                        "name": "Arunkumar Byravan"
                    },
                    {
                        "authorId": "1387885286",
                        "name": "Gabriel Dulac-Arnold"
                    },
                    {
                        "authorId": "1388022743",
                        "name": "P. Trochim"
                    },
                    {
                        "authorId": "2801204",
                        "name": "N. Heess"
                    },
                    {
                        "authorId": "1879232",
                        "name": "J. Merel"
                    },
                    {
                        "authorId": "2109481",
                        "name": "Yuval Tassa"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Prior work either learns forward models using random policies [14] or update and refine the dynamics model incrementally as the policy is being optimized for the task [10], [15], [16], [17], [18].",
                "using imagined rollouts from the model\u2019s predictions [18], [16], [38], and ii) model-based planning methods which generally use online planning algorithms with the model to choose optimal actions in a model predictive control (MPC) loop [10], [15]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1f312f9363eecaae22b65b1d9397cc2737168143",
                "externalIds": {
                    "DBLP": "conf/icra/LimGBC22",
                    "ArXiv": "2109.08522",
                    "DOI": "10.1109/ICRA46639.2022.9811559",
                    "CorpusId": 237344265
                },
                "corpusId": 237344265,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1f312f9363eecaae22b65b1d9397cc2737168143",
                "title": "Dynamics-Aware Quality-Diversity for Efficient Learning of Skill Repertoires",
                "abstract": "Quality-Diversity (QD) algorithms are powerful exploration algorithms that allow robots to discover large repertoires of diverse and high-performing skills. However, QD algorithms are sample inefficient and require millions of evaluations. In this paper, we propose Dynamics-Aware Quality-Diversity (DA-QD), a framework to improve the sample efficiency of QD algorithms through the use of dynamics models. We also show how DA-QD can then be used for continual acquisition of new skill repertoires. To do so, we incrementally train a deep dynamics model from experience obtained when performing skill discovery using QD. We can then perform QD exploration in imagination with an imagined skill repertoire. We evaluate our approach on three robotic experiments. First, our experiments show DA-QD is 20 times more sample efficient than existing QD approaches for skill discovery. Second, we demonstrate learning an entirely new skill repertoire in imagination to perform zero-shot learning. Finally, we show how DA-QD is useful and effective for solving a long horizon navigation task and for damage adaptation in the real world. Videos and source code are available at: https://sites.google.com/view/da-qd.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143845299",
                        "name": "Bryan Lim"
                    },
                    {
                        "authorId": "2110684692",
                        "name": "Luca Grillotti"
                    },
                    {
                        "authorId": "2127389361",
                        "name": "Lorenzo Bernasconi"
                    },
                    {
                        "authorId": "1934171",
                        "name": "Antoine Cully"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We hypothesize that this is because the complexity of the physical system exceeds the expressive power of the model network in MBPO.",
                "We observed that around the 100th epoch, the losses for the Q function of the MBPO method increased a lot.",
                "For example, in MBPO (Janner et al., 2019), the policy network is updated using the gradients of the critic:",
                "5 that shows our MuJoCo Ant experiment results, we observe an even faster convergence than MBPO, while MBPO already outperforms most of the RL algorithms.",
                "MBPO works well in easy scenarios with up to 3 links, but its performance degrades starting at 4 links.",
                "As mentioned in the main text, the performance of MBPO degrades while ours does not.",
                "Other than that, everything is the same as in MBPO.",
                "For example, in MBPO (Janner et al., 2019), the policy network is updated using the gradients of the critic:\nL\u00b5 = \u2212Q(s, \u00b5(s)) + Z, (12)\nwhere L\u00b5 is the loss for the policy network \u00b5, Q is the value function for state-action pairs, and Z is the regularization term.",
                "We use the model-based MBPO optimizer as the main baseline (Janner et al., 2019).",
                "MBPO does not attain satisfactory results for 6- and 7-link systems."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "acabbe1a3a1754acf69cd8535b82016d275814a3",
                "externalIds": {
                    "ArXiv": "2109.07719",
                    "DBLP": "journals/corr/abs-2109-07719",
                    "CorpusId": 235631883
                },
                "corpusId": 235631883,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/acabbe1a3a1754acf69cd8535b82016d275814a3",
                "title": "Efficient Differentiable Simulation of Articulated Bodies",
                "abstract": "We present a method for efficient differentiable simulation of articulated bodies. This enables integration of articulated body dynamics into deep learning frameworks, and gradient-based optimization of neural networks that operate on articulated bodies. We derive the gradients of the forward dynamics using spatial algebra and the adjoint method. Our approach is an order of magnitude faster than autodiff tools. By only saving the initial states throughout the simulation process, our method reduces memory requirements by two orders of magnitude. We demonstrate the utility of efficient differentiable dynamics for articulated bodies in a variety of applications. We show that reinforcement learning with articulated systems can be accelerated using gradients provided by our method. In applications to control and inverse problems, gradient-based optimization enabled by our work accelerates convergence by more than an order of magnitude.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51300425",
                        "name": "Yi-Ling Qiao"
                    },
                    {
                        "authorId": "32417403",
                        "name": "Junbang Liang"
                    },
                    {
                        "authorId": "145231047",
                        "name": "V. Koltun"
                    },
                    {
                        "authorId": "144247566",
                        "name": "M. Lin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "631380958868708984bbddca8770822cd6b53777",
                "externalIds": {
                    "ArXiv": "2109.07275",
                    "DBLP": "journals/corr/abs-2109-07275",
                    "CorpusId": 237513585
                },
                "corpusId": 237513585,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/631380958868708984bbddca8770822cd6b53777",
                "title": "DROMO: Distributionally Robust Offline Model-based Policy Optimization",
                "abstract": "We consider the problem of offline reinforcement learning with model-based control, whose goal is to learn a dynamics model from the experience replay and obtain a pessimism-oriented agent under the learned model. Current model-based constraint includes explicit uncertainty penalty and implicit conservative regularization that pushes Q-values of out-of-distribution state-action pairs down and the in-distribution up. While the uncertainty estimation, on which the former relies on, can be loosely calibrated for complex dynamics, the latter performs slightly better. To extend the basic idea of regularization without uncertainty quantification, we propose distributionally robust offline model-based policy optimization (DROMO), which leverages the ideas in distributionally robust optimization to penalize a broader range of out-of-distribution state-action pairs beyond the standard empirical out-of-distribution Q-value minimization. We theoretically show that our method optimizes a lower bound on the ground-truth policy evaluation, and it can be incorporated into any existing policy gradient algorithms. We also analyze the theoretical properties of DROMO's linear and non-linear instantiations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3339740",
                        "name": "Ruizhen Liu"
                    },
                    {
                        "authorId": "2128842236",
                        "name": "Dazhi Zhong"
                    },
                    {
                        "authorId": "2109314882",
                        "name": "Zhi-Cong Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Based on previous works [11], [19], [38], we have the following Lemma to build the lower bound of the discrepancy of the total returns from the true model and the learned model in conventional model-based RL:",
                "From the theoretical perspective, previous works [11], [19]\u2013[21] have provided general frameworks for analysing model-based RL, which include monotonic improvement guarantees.",
                "The dependency of two policies \u03c0 and \u03c0D is measured by the TVD \u03c0 = DTV(\u03c0(a|s)|\u03c0D(a|s)), and is bounded by a constant \u03b4\u03c0 , where DTV(\u03c0(a|s)|\u03c0D(a|s)) \u2264 \u03b4\u03c0 .",
                "Related research topics References Support federated training Support decision-making Sample efficiency Federated reinforcement learning [7], [9], [10], [15], [16] X X low Model-based reinforcement learning [11], [12], [17]\u2013[21] \u00d7 X high Federated ensemble distillation [22], [23] X \u00d7 high Federated ensemble model-based reinforcement learning our work X X high",
                "As long as we improve the returns under the learned model by more than B, we can guarantee improvement under the environment [11].",
                "One assumes that the dynamics model is a complex probability distribution, and measures the distance using Total Variation Distance (TVD) [11].",
                "Since TVD requires weaker assumptions and is typically more practical than 1-Wasserstein distance, we use TVD in our analysis.",
                "The generalisation error is measured by the TVD, defined as m := DTV(T\u0302 (\u00b7|s, a)|T (\u00b7|s, a)) =\nAlgorithm 1 FEMRL running on K clients (indexed by k) for E epochs, each consisting of Tc rounds of federated communication and G steps of policy update.",
                "Compared to model-free methods, model-based RL [11]\u2013 [13] is much more sample efficient.",
                "There are three key factors affecting the maximal value of generalisation error m: the virtual global empirical error S\u0302k(hS\u0302k), the number of training samples m, and the sum of TVDs between the clients\u2019 sample policies and the virtual global sample policy, \u0393.\nNote that, The virtual global empirical error can in principle be estimated and optimised approximately by the training loss.",
                "In addition, [38] uses a general measurement, Integral Probability Metric, where TVD and 1-Wasserstein distance are two special cases."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7897139cab076eecd91d525a013b1fb10222abc7",
                "externalIds": {
                    "ArXiv": "2109.05549",
                    "DBLP": "journals/tpds/WangHMMXG23",
                    "DOI": "10.1109/TPDS.2023.3264480",
                    "CorpusId": 248987639
                },
                "corpusId": 248987639,
                "publicationVenue": {
                    "id": "7c9d091e-015e-4e5d-a11f-9bc369fcf414",
                    "name": "IEEE Transactions on Parallel and Distributed Systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Parallel Distrib Syst"
                    ],
                    "issn": "1045-9219",
                    "url": "http://www.computer.org/tpds",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=71"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7897139cab076eecd91d525a013b1fb10222abc7",
                "title": "Federated Ensemble Model-Based Reinforcement Learning in Edge Computing",
                "abstract": "Federated learning (FL) is a privacy-preserving distributed machine learning paradigm that enables collaborative training among geographically distributed and heterogeneous devices without gathering their data. Extending FL beyond the supervised learning models, federated reinforcement learning (FRL) was proposed to handle sequential decision-making problems in edge computing systems. However, the existing FRL algorithms directly combine model-free RL with FL, thus often leading to high sample complexity and lacking theoretical guarantees. To address the challenges, we propose a novel FRL algorithm that effectively incorporates model-based RL and ensemble knowledge distillation into FL for the first time. Specifically, we utilise FL and knowledge distillation to create an ensemble of dynamics models for clients, and then train the policy by solely using the ensemble model without interacting with the environment. Furthermore, we theoretically prove that the monotonic improvement of the proposed algorithm is guaranteed. The extensive experimental results demonstrate that our algorithm obtains much higher sample efficiency compared to classic model-free FRL algorithms in the challenging continuous control benchmark environments under edge computing settings. The results also highlight the significant impact of heterogeneous client data and local model update steps on the performance of FRL, validating the insights obtained from our theoretical analysis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143718761",
                        "name": "Jin Wang"
                    },
                    {
                        "authorId": "31703425",
                        "name": "Jia Hu"
                    },
                    {
                        "authorId": "1581440302",
                        "name": "Jed Mills"
                    },
                    {
                        "authorId": "145896559",
                        "name": "G. Min"
                    },
                    {
                        "authorId": "2056031315",
                        "name": "Ming Xia"
                    },
                    {
                        "authorId": "1732833",
                        "name": "N. Georgalas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We adapt Model-based Policy Optimization (MBPO) [29], one of the most popular model-based RL (MBRL) algorithm as our offline RL baseline."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d6a1e9699d4e3571ab1eb74ab9eaba75095b809c",
                "externalIds": {
                    "ArXiv": "2109.04869",
                    "DBLP": "journals/ral/SunHLLZG22",
                    "DOI": "10.1109/LRA.2022.3150855",
                    "CorpusId": 237485498
                },
                "corpusId": 237485498,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d6a1e9699d4e3571ab1eb74ab9eaba75095b809c",
                "title": "PlaTe: Visually-Grounded Planning With Transformers in Procedural Tasks",
                "abstract": "In this work, we study the problem of how to leverage instructional videos to facilitate the understanding of human decision-making processes, focusing on training a model with the ability to plan a goal-directed procedure from real-world videos. Learning structured and plannable state and action spaces directly from unstructured videos is the key technical challenge of our task. There are two problems: first, the appearance gap between the training and validation datasets could be large for unstructured videos; second, these gaps lead to decision errors that compound over the steps. We address these limitations with Planning Transformer (PlaTe), which has the advantage of circumventing the compounding prediction errors that occur with single-step models during long model-based rollouts. Our method simultaneously learns the latent state and action information of assigned tasks and the representations of the decision-making process from human demonstrations. Experiments conducted on real-world instructional videos show that our method can achieve a better performance in reaching the indicated goal than previous algorithms. We also validated the possibility of applying procedural tasks on a UR-5 platform. Please see 1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2282025",
                        "name": "Jiankai Sun"
                    },
                    {
                        "authorId": "38485317",
                        "name": "De-An Huang"
                    },
                    {
                        "authorId": "2075390730",
                        "name": "Bo Lu"
                    },
                    {
                        "authorId": "2136355855",
                        "name": "Yunhui Liu"
                    },
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    },
                    {
                        "authorId": "2054554660",
                        "name": "Animesh Garg"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "03e19dbf435d39d729d8e6d44cb36e422f66b2be",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-04640",
                    "ArXiv": "2109.04640",
                    "CorpusId": 237485228
                },
                "corpusId": 237485228,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/03e19dbf435d39d729d8e6d44cb36e422f66b2be",
                "title": "Projected State-action Balancing Weights for Offline Reinforcement Learning",
                "abstract": "Offline policy evaluation (OPE) is considered a fundamental and challenging problem in reinforcement learning (RL). This paper focuses on the value estimation of a target policy based on pre-collected data generated from a possibly different policy, under the framework of infinite-horizon Markov decision processes. Motivated by the recently developed marginal importance sampling method in RL and the covariate balancing idea in causal inference, we propose a novel estimator with approximately projected state-action balancing weights for the policy value estimation. We obtain the convergence rate of these weights, and show that the proposed value estimator is semi-parametric efficient under technical conditions. In terms of asymptotics, our results scale with both the number of trajectories and the number of decision points at each trajectory. As such, consistency can still be achieved with a limited number of subjects when the number of decision points diverges. In addition, we make a first attempt towards characterizing the difficulty of OPE problems, which may be of independent interest. Numerical experiments demonstrate the promising performance of our proposed estimator.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110272295",
                        "name": "Jiayi Wang"
                    },
                    {
                        "authorId": "3146500",
                        "name": "Zhengling Qi"
                    },
                    {
                        "authorId": "144717679",
                        "name": "Raymond K. W. Wong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c89db280fc506203dbbd73559eb8125a217ad0a6",
                "externalIds": {
                    "MAG": "3197838560",
                    "DOI": "10.1021/acsmaterialslett.1c00390",
                    "CorpusId": 239672692
                },
                "corpusId": 239672692,
                "publicationVenue": {
                    "id": "c05aa30f-5194-40a0-a689-6ccb116a053d",
                    "name": "ACS Materials Letters",
                    "type": "journal",
                    "alternate_names": [
                        "AC Mater Lett"
                    ],
                    "issn": "2639-4979",
                    "url": "https://pubs.acs.org/journal/amlcef"
                },
                "url": "https://www.semanticscholar.org/paper/c89db280fc506203dbbd73559eb8125a217ad0a6",
                "title": "Deep Reinforcement Learning for Digital Materials Design",
                "abstract": "Designing composites has been a research topic of interest in the field of materials science. As an elegant mathematical representation for composites, the concept of digital materials (DMs) was developed to express structures with complex geometries and various material distributions. DMs have a vast design space to achieve targeted physical properties, which makes it challenging for solving inverse problems. Here, a deep reinforcement learning (DRL) scheme is utilized to automate the DM design process without the designer\u2019s prior knowledge. Based on the reward signal of structural mechanical property changes, DRL algorithms can initiate new design patterns in a self-updating process. As a demonstration example, a DM system composed of two different materials are selected as testing environments with three different levels of design space sizes. The collaborative deep Q network (DQN) architecture is developed to comprise two cooperative agents for two types of element-level modification operations to satisfy the design constraints, such as material fraction. The quality of each composite pattern is calculated through the finite element analysis (FEA) simulation. Results show the proposed approach can effectively handle the complex state-action space problems for the digital material design process with significant computation advantages, compared with those of the genetic algorithm with a 15.9% final design quality enhancement. As such, this new class of DRL scheme could be a powerful tool to enable the autonomous discovery process for next-generation free-form DM designs. Additive manufacturing (AM), also known as threedimensional (3D) printing, can fabricate structures with complex geometries and various material distributions. Commercial 3D printers build a structure in a layer-by-layer manner consisting of numerous material compositions in each layer. The high degree of freedom in the material placements inspires next-generation composite material designs with varying physical properties and free-form geometries. These composites are sometimes characterized as digital materials (DMs), utilizing the 3D printing process to assemble finite material voxels. Different from the regular geometries that can be described with only a few discrete parameters, DM design considers the structures as pixelated images. As the design space is enlarged by orders of magnitude in DM structures, the design screening and optimization process has become rather complex. A robust method that can automatically and efficiently generate optimal designs has become a key research topic in this field. Recent advances in artificial intelligence have shown various machine learning (ML) applications in a wide range of science and engineering, such as the image recognition, controlling system, speech recognition, neuroscience, energy and environmental technologies. In the field of DM design, the ML applications have mostly been in the form of supervised learning. The supervised learning method can solve the forward problem: identifying the physical property of a given design configuration, such as the stiffness and toughness, thermal conductivity, vibration frequency, and material defects. Supervised learning models, especially the trained neural networks, can operate at orders of magnitude faster than the conventional commercial numerical simulators. While these attempts have undoubtedly led to pioneering results, there are still important applications that have yet to be fully exploited, especially when solving inverse problems: exploring the optimal design pattern to achieve the desired physical property. The solution space of complex inverse problems is nonconvex and there are many local optima; thus, it is Received: July 6, 2021 Accepted: August 23, 2021 Published: August 27, 2021 Leter www.acsmaterialsletters.org \u00a9 2021 American Chemical Society 1433 https://doi.org/10.1021/acsmaterialslett.1c00390 ACS Materials Lett. 2021, 3, 1433\u22121439 D ow nl oa de d vi a U N IV O F C A L IF O R N IA B E R K E L E Y o n Ja nu ar y 26 , 2 02 2 at 2 3: 13 :1 6 (U T C ). Se e ht tp s: //p ub s. ac s. or g/ sh ar in gg ui de lin es f or o pt io ns o n ho w to le gi tim at el y sh ar e pu bl is he d ar tic le s.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152406706",
                        "name": "Fanping Sui"
                    },
                    {
                        "authorId": "37495246",
                        "name": "Ruiqi Guo"
                    },
                    {
                        "authorId": "1927642",
                        "name": "Zhizhou Zhang"
                    },
                    {
                        "authorId": "11329466",
                        "name": "Grace X. Gu"
                    },
                    {
                        "authorId": "2148468972",
                        "name": "Liwei Lin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "99e3171281d5b4cbb4ede8c272feca32e408008a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-11645",
                    "ArXiv": "2108.11645",
                    "CorpusId": 237303996
                },
                "corpusId": 237303996,
                "publicationVenue": {
                    "id": "2486528b-036c-4f3c-953f-c574eb381d12",
                    "name": "Asian Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Mach Learn",
                        "ACML"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=40"
                },
                "url": "https://www.semanticscholar.org/paper/99e3171281d5b4cbb4ede8c272feca32e408008a",
                "title": "Robust Model-based Reinforcement Learning for Autonomous Greenhouse Control",
                "abstract": "Due to the high efficiency and less weather dependency, autonomous greenhouses provide an ideal solution to meet the increasing demand for fresh food. However, managers are faced with some challenges in finding appropriate control strategies for crop growth, since the decision space of the greenhouse control problem is an astronomical number. Therefore, an intelligent closed-loop control framework is highly desired to generate an automatic control policy. As a powerful tool for optimal control, reinforcement learning (RL) algorithms can surpass human beings' decision-making and can also be seamlessly integrated into the closed-loop control framework. However, in complex real-world scenarios such as agricultural automation control, where the interaction with the environment is time-consuming and expensive, the application of RL algorithms encounters two main challenges, i.e., sample efficiency and safety. Although model-based RL methods can greatly mitigate the efficiency problem of greenhouse control, the safety problem has not got too much attention. In this paper, we present a model-based robust RL framework for autonomous greenhouse control to meet the sample efficiency and safety challenges. Specifically, our framework introduces an ensemble of environment models to work as a simulator and assist in policy optimization, thereby addressing the low sample efficiency problem. As for the safety concern, we propose a sample dropout module to focus more on worst-case samples, which can help improve the adaptability of the greenhouse planting policy in extreme cases. Experimental results demonstrate that our approach can learn a more effective greenhouse planting policy with better robustness than existing methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2984723",
                        "name": "Wanpeng Zhang"
                    },
                    {
                        "authorId": "2033573415",
                        "name": "Xiaoyan Cao"
                    },
                    {
                        "authorId": "9947283",
                        "name": "Yaowen Yao"
                    },
                    {
                        "authorId": "2111959325",
                        "name": "Zhicheng An"
                    },
                    {
                        "authorId": "2061549073",
                        "name": "Dijun Luo"
                    },
                    {
                        "authorId": "2153019958",
                        "name": "Xi Xiao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Lemma B.1 of Janner et al. (2019) allows us to bound difference in the joint distribution of states and actions at a time step t by the sum of the individual errors.",
                "We can leverage proofs from Janner et al. (2019) to simplify the analysis.",
                "Then, we have:\nDTV (p\u0302(a1:N,t|st)||p(a1:N,t|st)) \u2264 c(N \u2212 1)\nWe can then apply Lemma B.2 of Janner et al. (2019) to bound the overall state distribution at time step t as:\nDTV (p\u0302(st, a1:N,t)||p(st, a1:N,t)) \u2264 tc(N \u2212 1)\nNext, we let p(s, a1:N ) = (1\u2212 \u03b3) \u2211T t=0 \u03b3 tp(st, a1:N,t) denote the discounted\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9bc37d8c043908d657b9ccac635d1f8dc399e750",
                "externalIds": {
                    "DBLP": "journals/jair/FuTPB21",
                    "DOI": "10.1613/jair.1.12594",
                    "CorpusId": 237333523
                },
                "corpusId": 237333523,
                "publicationVenue": {
                    "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
                    "name": "Journal of Artificial Intelligence Research",
                    "type": "journal",
                    "alternate_names": [
                        "JAIR",
                        "J Artif Intell Res",
                        "The Journal of Artificial Intelligence Research"
                    ],
                    "issn": "1076-9757",
                    "url": "http://www.jair.org/"
                },
                "url": "https://www.semanticscholar.org/paper/9bc37d8c043908d657b9ccac635d1f8dc399e750",
                "title": "Evaluating Strategic Structures in Multi-Agent Inverse Reinforcement Learning",
                "abstract": "A core question in multi-agent systems is understanding the motivations for an agent's actions based on their behavior. Inverse reinforcement learning provides a framework for extracting utility functions from observed agent behavior, casting the problem as finding domain parameters which induce such a behavior from rational decision makers.\u00a0 We show how to efficiently and scalably extend inverse reinforcement learning to multi-agent settings, by reducing the multi-agent problem to N single-agent problems while still satisfying rationality conditions such as strong rationality. However, we observe that rewards learned naively tend to lack insightful structure, which causes them to produce undesirable behavior when optimized in games with different players from those encountered during training. We further investigate conditions under which rewards or utility functions can be precisely identified, on problem domains such as normal-form and Markov games, as well as auctions, where we show we can learn reward functions that properly generalize to new settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2550764",
                        "name": "Justin Fu"
                    },
                    {
                        "authorId": "2844530",
                        "name": "A. Tacchetti"
                    },
                    {
                        "authorId": "3422031",
                        "name": "J. P\u00e9rolat"
                    },
                    {
                        "authorId": "1698412",
                        "name": "Yoram Bachrach"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based RL methods attempt to learn an MDP and often use it for planning online [9, 14, 15, 21, 22]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "57e426e9a20124dda030e4effd69d8459aab5759",
                "externalIds": {
                    "ArXiv": "2108.05713",
                    "DBLP": "conf/cvpr/IshidaH22",
                    "DOI": "10.1109/CVPR52688.2022.01681",
                    "CorpusId": 236987328
                },
                "corpusId": 236987328,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/57e426e9a20124dda030e4effd69d8459aab5759",
                "title": "Towards real-world navigation with deep differentiable planners",
                "abstract": "We train embodied neural networks to plan and navigate unseen complex 3D environments, emphasising real-world deployment. Rather than requiring prior knowledge of the agent or environment, the planner learns to model the state transitions and rewards. To avoid the potentially hazardous trial-and-error of reinforcement learning, we focus on differentiable planners such as Value Iteration Networks (VIN), which are trained offline from safe expert demonstrations. Although they work well in small simulations, we address two major limitations that hinder their deployment. First, we observed that current differentiable planners struggle to plan long-term in environments with a high branching complexity. While they should ideally learn to assign low rewards to obstacles to avoid collisions, these penalties are not strong enough to guarantee collision-free operation. We thus impose a structural constraint on the value iteration, which explicitly learns to model impossible actions and noisy motion. Secondly, we extend the model to plan exploration with a limited perspective camera under translation and fine rotations, which is crucial for real robot deployment. Our proposals significantly improve semantic navigation and exploration on several 2D and 3D environments, succeeding in settings that are otherwise challenging for differentiable planners. As far as we know, we are the first to successfully apply them to the difficult Active Vision Dataset, consisting of real images captured from a robot. 11Code available: https://github.com/shuishida/calvin",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153075776",
                        "name": "Shu Ishida"
                    },
                    {
                        "authorId": "143848064",
                        "name": "Jo\u00e3o F. Henriques"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For larger problems it can be a parameterized estimate instead (Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7e1d5025c55e068b3645ff326c316ab4f31b72b8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-02701",
                    "ArXiv": "2108.02701",
                    "CorpusId": 236924725
                },
                "corpusId": 236924725,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7e1d5025c55e068b3645ff326c316ab4f31b72b8",
                "title": "Lyapunov Robust Constrained-MDPs: Soft-Constrained Robustly Stable Policy Optimization under Model Uncertainty",
                "abstract": "Safety and robustness are two desired properties for any reinforcement learning algorithm. CMDPs can handle additional safety constraints and RMDPs can perform well under model uncertainties. In this paper, we propose to unite these two frameworks resulting in robust constrained MDPs (RCMDPs). The motivation is to develop a framework that can satisfy safety constraints while also simultaneously offer robustness to model uncertainties. We develop the RCMDP objective, derive gradient update formula to optimize this objective and then propose policy gradient based algorithms. We also independently propose Lyapunov based reward shaping for RCMDPs, yielding better stability and convergence properties.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10766679",
                        "name": "R. Russel"
                    },
                    {
                        "authorId": "1730046",
                        "name": "M. Benosman"
                    },
                    {
                        "authorId": "1751880",
                        "name": "J. Baar"
                    },
                    {
                        "authorId": "102147358",
                        "name": "Radu Corcodel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, the computation cost of rollout increases exponentially with the planning horizon to get an accurate estimate of ao\u2217, while in practice the compounding error of the environmental model also increases with the planning horizon [15].",
                "[15] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "94e75cdb50261cb4b6318d41edaf43c54ec27c96",
                "externalIds": {
                    "DBLP": "conf/nips/YuJZJL22",
                    "ArXiv": "2108.01843",
                    "CorpusId": 236912920
                },
                "corpusId": 236912920,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/94e75cdb50261cb4b6318d41edaf43c54ec27c96",
                "title": "Model-Based Opponent Modeling",
                "abstract": "When one agent interacts with a multi-agent environment, it is challenging to deal with various opponents unseen before. Modeling the behaviors, goals, or beliefs of opponents could help the agent adjust its policy to adapt to different opponents. In addition, it is also important to consider opponents who are learning simultaneously or capable of reasoning. However, existing work usually tackles only one of the aforementioned types of opponents. In this paper, we propose model-based opponent modeling (MBOM), which employs the environment model to adapt to all kinds of opponents. MBOM simulates the recursive reasoning process in the environment model and imagines a set of improving opponent policies. To effectively and accurately represent the opponent policy, MBOM further mixes the imagined opponent policies according to the similarity with the real behaviors of opponents. Empirically, we show that MBOM achieves more effective adaptation than existing methods in a variety of tasks, respectively with different types of opponents, i.e., fixed policy, na\\\"ive learner, and reasoning learner.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Xiaopeng Yu"
                    },
                    {
                        "authorId": "46179766",
                        "name": "Jiechuan Jiang"
                    },
                    {
                        "authorId": "2984723",
                        "name": "Wanpeng Zhang"
                    },
                    {
                        "authorId": "2152632624",
                        "name": "Haobin Jiang"
                    },
                    {
                        "authorId": "2265693",
                        "name": "Zongqing Lu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In practice, we heuristically approximate a calibrated dynamics model by learning an ensemble of probabilistic dynamics models, following common practice in RL [Yu et al., 2020, Janner et al., 2019, Chua et al., 2018].",
                "(Interestingly, prior work shows that an ensemble of probabilistic models can still capture the error of estimating a deterministic ground-truth dynamics model [Janner et al., 2019, Chua et al., 2018].)"
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "36d55c909e8c1be84f3a4f2631e3303ef5392fb0",
                "externalIds": {
                    "DBLP": "conf/nips/LuoM21",
                    "ArXiv": "2108.01846",
                    "CorpusId": 236912869
                },
                "corpusId": 236912869,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/36d55c909e8c1be84f3a4f2631e3303ef5392fb0",
                "title": "Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations",
                "abstract": "Training-time safety violations have been a major concern when we deploy reinforcement learning algorithms in the real world. This paper explores the possibility of safe RL algorithms with zero training-time safety violations in the challenging setting where we are only given a safe but trivial-reward initial policy without any prior knowledge of the dynamics model and additional offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe RL (CRABS), which iteratively learns barrier certificates, dynamics models, and policies. The barrier certificates, learned via adversarial training, ensure the policy's safety assuming calibrated learned dynamics model. We also add a regularization term to encourage larger certified regions to enable better exploration. Empirical simulations show that zero safety violations are already challenging for a suite of simple environments with only 2-4 dimensional state space, especially if high-reward policies have to visit regions near the safety boundary. Prior methods require hundreds of violations to achieve decent rewards on these tasks, whereas our proposed algorithms incur zero violations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1491625903",
                        "name": "Yuping Luo"
                    },
                    {
                        "authorId": "2114186424",
                        "name": "Tengyu Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To further enhance the interpretability of the predictive models and improve the robustness of the learned policies [12, 13], ensemble-based methods [14, 15] train an ensemble of models to comprehensively capture the uncertainty in the environment and have been empirically shown to obtain significant improvements in sample efficiency [5, 12, 16].",
                "For model-based methods, we compare against MBPO [16], which uses short-horizon model-based rollouts started from samples in the real environment; STEVE [30], which dynamically incorporates data from rollouts into value estimation rather than policy learning; and SLBO [31], a model-based algorithm with performance guarantees."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "92a97c7efe4c34f83821593b93b84e66fe59f0bb",
                "externalIds": {
                    "ArXiv": "2108.01295",
                    "DBLP": "journals/corr/abs-2108-01295",
                    "CorpusId": 236881125
                },
                "corpusId": 236881125,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/92a97c7efe4c34f83821593b93b84e66fe59f0bb",
                "title": "MBDP: A Model-based Approach to Achieve both Robustness and Sample Efficiency via Double Dropout Planning",
                "abstract": "Model-based reinforcement learning is a widely accepted solution for solving excessive sample demands. However, the predictions of the dynamics models are often not accurate enough, and the resulting bias may incur catastrophic decisions due to insufficient robustness. Therefore, it is highly desired to investigate how to improve the robustness of model-based RL algorithms while maintaining high sampling efficiency. In this paper, we propose Model-Based Double-dropout Planning (MBDP) to balance robustness and efficiency. MBDP consists of two kinds of dropout mechanisms, where the rollout-dropout aims to improve the robustness with a small cost of sample efficiency, while the model-dropout is designed to compensate for the lost efficiency at a slight expense of robustness. By combining them in a complementary way, MBDP provides a flexible control mechanism to meet different demands of robustness and efficiency by tuning two corresponding dropout ratios. The effectiveness of MBDP is demonstrated both theoretically and experimentally.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2984723",
                        "name": "Wanpeng Zhang"
                    },
                    {
                        "authorId": "2153019958",
                        "name": "Xi Xiao"
                    },
                    {
                        "authorId": "9947283",
                        "name": "Yaowen Yao"
                    },
                    {
                        "authorId": "2107942948",
                        "name": "Mingzhe Chen"
                    },
                    {
                        "authorId": "2061549073",
                        "name": "Dijun Luo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "cea980158cad122bd7d760185d64c1a8aa99cdb7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-00128",
                    "ArXiv": "2108.00128",
                    "DOI": "10.1098/rspa.2021.0618",
                    "CorpusId": 236772244
                },
                "corpusId": 236772244,
                "publicationVenue": {
                    "id": "b61ce141-a434-431b-a154-68fc26e348f3",
                    "name": "Proceedings of the Royal Society A",
                    "type": "journal",
                    "alternate_names": [
                        "Proc R Soc A",
                        "Proc R Soc Math Phys Eng Sci",
                        "Proceedings of The Royal Society A: Mathematical, Physical and Engineering Sciences"
                    ],
                    "issn": "1364-5021",
                    "url": "https://www.jstor.org/journal/procmathphysengi",
                    "alternate_urls": [
                        "http://rspa.royalsocietypublishing.org/content/by/year",
                        "http://rspa.royalsocietypublishing.org/about",
                        "http://rspa.royalsocietypublishing.org/",
                        "https://royalsocietypublishing.org/journal/rspa"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cea980158cad122bd7d760185d64c1a8aa99cdb7",
                "title": "Physics-informed Dyna-style model-based deep reinforcement learning for dynamic control",
                "abstract": "Model-based reinforcement learning (MBRL) is believed to have much higher sample efficiency compared with model-free algorithms by learning a predictive model of the environment. However, the performance of MBRL highly relies on the quality of the learned model, which is usually built in a black-box manner and may have poor predictive accuracy outside of the data distribution. The deficiencies of the learned model may prevent the policy from being fully optimized. Although some uncertainty analysis-based remedies have been proposed to alleviate this issue, model bias still poses a great challenge for MBRL. In this work, we propose to leverage the prior knowledge of underlying physics of the environment, where the governing laws are (partially) known. In particular, we developed a physics-informed MBRL framework, where governing equations and physical constraints are used to inform the model learning and policy search. By incorporating the prior information of the environment, the quality of the learned model can be notably improved, while the required interactions with the environment are significantly reduced, leading to better sample efficiency and learning performance. The effectiveness and merit have been demonstrated over a handful of classic control problems, where the environments are governed by canonical ordinary/partial differential equations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2155562194",
                        "name": "Xin-Yang Liu"
                    },
                    {
                        "authorId": "2110206668",
                        "name": "Jian-Xun Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "8632fc7c21c7f2cac630f2c989cb4f3d2e6cc86b",
                "externalIds": {
                    "ArXiv": "2107.08241",
                    "DBLP": "journals/air/PlaatKP23",
                    "DOI": "10.1007/s10462-022-10335-w",
                    "CorpusId": 236087473
                },
                "corpusId": 236087473,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/8632fc7c21c7f2cac630f2c989cb4f3d2e6cc86b",
                "title": "High-accuracy model-based reinforcement learning, a survey",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2562595",
                        "name": "A. Plaat"
                    },
                    {
                        "authorId": "1680388",
                        "name": "W. Kosters"
                    },
                    {
                        "authorId": "1950379",
                        "name": "M. Preuss"
                    }
                ]
            }
        },
        {
            "contexts": [
                "significantly influence the performance of later planning [13]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d7748e70ffb52b227e925eb7a62754b1be059e44",
                "externalIds": {
                    "DBLP": "conf/nossdav/KanLYDZX21",
                    "DOI": "10.1145/3458306.3458872",
                    "CorpusId": 235689559
                },
                "corpusId": 235689559,
                "publicationVenue": {
                    "id": "193beb19-09ca-4e40-97b9-20e911993ff9",
                    "name": "International Workshop on Network and Operating System Support for Digital Audio and Video",
                    "type": "conference",
                    "alternate_names": [
                        "Network and Operating System Support for Digital Audio and Video",
                        "Netw Oper Syst Support Digit Audio Video",
                        "Int Workshop Netw Oper Syst Support Digit Audio Video",
                        "NOSSDAV"
                    ],
                    "url": "http://www.nossdav.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d7748e70ffb52b227e925eb7a62754b1be059e44",
                "title": "Uncertainty-aware robust adaptive video streaming with bayesian neural network and model predictive control",
                "abstract": "In this paper, we propose BayesMPC, an uncertainty-aware robust adaptive bitrate (ABR) algorithm on the basis of Bayesian neural network (BNN) and model predictive control (MPC). Specifically, to improve the capacity of learning transition probability of the network throughput, we adopt a BNN-based predictor that is able to predict the statistical distribution of future throughput from the past throughput by not only considering the aleatoric uncertainty (e.g., noise), but also capturing the epistemic uncertainty incurred by lack of adequate training samples. We further show that by using the negative log-likelihood loss function to train this BNN-based throughput predictor, the generalization error can be minimized with the guarantee of PAC-Bayesian theorem. Rather than a point estimate, the learnt uncertainty can contribute to a confidence region for the future throughput, the lower bound of which then leads to an uncertainty-aware robust MPC strategy to maximize the worst-case user quality-of-experience (QoE) w.r.t. this confidence region. Finally, experimental results on three real-world network trace datasets validate the efficiency of both the proposed BNN-based predictor and uncertainty-aware robust MPC strategy, and demonstrate the superior performance compared to other baselines, in terms of both the overall QoE performance and generalization across all ranges of heterogeneous network and user conditions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51265968",
                        "name": "Nuowen Kan"
                    },
                    {
                        "authorId": "144535686",
                        "name": "Chenglin Li"
                    },
                    {
                        "authorId": "2117893212",
                        "name": "Caiyi Yang"
                    },
                    {
                        "authorId": "3207464",
                        "name": "Wenrui Dai"
                    },
                    {
                        "authorId": "38871632",
                        "name": "Junni Zou"
                    },
                    {
                        "authorId": "144045763",
                        "name": "H. Xiong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026include baselines with moderate explo-\nration power: b) SLBO (Luo et al., 2018) enforces entropy regularization during TRPO updates and adding OrnsteinUhlunbeck noise while collecting samples. c) MBPO (Janner et al., 2019) uses SAC (Haarnoja et al., 2018) as the planner to encourage exploration.",
                "Top Right: PC-MLP can explore strategically and thus learns much faster than other MBRL (SLBO and MBPO) baselines which rely on random exploration.",
                "We also include baselines with moderate explo-\nration power: b) SLBO (Luo et al., 2018) enforces entropy regularization during TRPO updates and adding OrnsteinUhlunbeck noise while collecting samples. c) MBPO (Janner et al., 2019) uses SAC (Haarnoja et al., 2018) as the planner to encourage exploration.",
                "Here \u201cET\u201d in the environment name denotes that the planner has access to the termination function, which is a common assumption in current MBRL algorithms (Janner et al., 2019; Rajeswaran et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "1a39ad2d1553d07084b5e44a28878c8bb5018cef",
                "externalIds": {
                    "ArXiv": "2107.07410",
                    "DBLP": "conf/icml/SongS21",
                    "CorpusId": 235826015
                },
                "corpusId": 235826015,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1a39ad2d1553d07084b5e44a28878c8bb5018cef",
                "title": "PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration",
                "abstract": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to its potential sample efficiency compared to model-free RL. However, existing empirical model-based RL approaches lack the ability to explore. This work studies a computationally and statistically efficient model-based algorithm for both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). For both models, our algorithm guarantees polynomial sample complexity and only uses access to a planning oracle. Experimentally, we first demonstrate the flexibility and efficacy of our algorithm on a set of exploration challenging control tasks where existing empirical model-based RL approaches completely fail. We then show that our approach retains excellent performance even in common dense reward control benchmarks that do not require heavy exploration. Finally, we demonstrate that our method can also perform reward-free exploration efficiently. Our code can be found at https://github.com/yudasong/PCMLP.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152602077",
                        "name": "Yuda Song"
                    },
                    {
                        "authorId": "144426657",
                        "name": "Wen Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, neural networks have been used widely for model-based RL (Gal et al., 2016; Depeweg et al., 2016; Nagabandi et al., 2018; Chua et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30",
                "externalIds": {
                    "DBLP": "conf/icml/ZhuNB21",
                    "ArXiv": "2107.05697",
                    "CorpusId": 235826075
                },
                "corpusId": 235826075,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30",
                "title": "Few-shot Language Coordination by Modeling Theory of Mind",
                "abstract": "$\\textit{No man is an island.}$ Humans communicate with a large community by coordinating with different interlocutors within short conversations. This ability has been understudied by the research on building neural communicative agents. We study the task of few-shot $\\textit{language coordination}$: agents quickly adapting to their conversational partners' language abilities. Different from current communicative agents trained with self-play, we require the lead agent to coordinate with a $\\textit{population}$ of agents with different linguistic abilities, quickly adapting to communicate with unseen agents in the population. This requires the ability to model the partner's beliefs, a vital component of human communication. Drawing inspiration from theory-of-mind (ToM; Premack&Woodruff (1978)), we study the effect of the speaker explicitly modeling the listeners' mental states. The speakers, as shown in our experiments, acquire the ability to predict the reactions of their partner, which helps it generate instructions that concisely express its communicative goal. We examine our hypothesis that the instructions generated with ToM modeling yield better communication performance in both a referential game and a language navigation task. Positive results from our experiments hint at the importance of explicitly modeling communication as a socio-pragmatic progress.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144459859",
                        "name": "Hao Zhu"
                    },
                    {
                        "authorId": "1700325",
                        "name": "Graham Neubig"
                    },
                    {
                        "authorId": "3312309",
                        "name": "Yonatan Bisk"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Theorem 1 [20] indicates that as long as we improve the returns under the model RT [\u03c0] by more than",
                "The inherent model bias in the simulation can be exploited by the optimization algorithms, leading to large reality gap between the actual and simulated policy performance [20]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3657e1a3bd4b24d8bf24801b606999aeffc92ff1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-05464",
                    "ArXiv": "2107.05464",
                    "MAG": "3178885704",
                    "DOI": "10.21203/RS.3.RS-687625/V1",
                    "CorpusId": 235795067
                },
                "corpusId": 235795067,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3657e1a3bd4b24d8bf24801b606999aeffc92ff1",
                "title": "IGrow: A Smart Agriculture Solution to Autonomous Greenhouse Control",
                "abstract": "\n Agriculture is the foundation of human civilization. However, the rapid increase and aging of the global population pose challenges on this cornerstone by demanding more healthy and fresh food. Internet of Things (IoT) technology makes modern autonomous greenhouse a viable and reliable engine of food production. However, the educated and skilled labor capable of overseeing high-tech greenhouses is scarce. Artificial intelligence (AI) and cloud computing technologies are promising solutions for precision control and high-efficiency production in such controlled environments. In this paper, we propose a smart agriculture solution, namely iGrow: (1) we use IoT and cloud computing technologies to measure, collect, and manage growing data, to support iteration of our decision-making AI module, which consists of an incremental model and an optimization algorithm; (2) we propose a three-stage incremental model based on accumulating data, enabling growers/central computers to schedule control strategies conveniently and at low cost; (3) we propose a model-based iterative optimization algorithm, which can dynamically optimize the greenhouse control strategy in real-time production. In the simulated experiment, evaluation results show the accuracy of our incremental model is comparable to an advanced tomato simulator, while our optimization algorithms can beat the champion of the 2nd Autonomous Greenhouse Challenge. Compelling results from the A/B test in real greenhouses demonstrate that our solution significantly increases production (commercially sellable fruits) (+10.15%) and net profit (+87.07%) with statistical significance compared to planting experts. The data and source codes of our work are provided as supplementary materials, and more details are available at: https://github.com/holmescao/SmartAgricultureSolution-iGrow.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2033573415",
                        "name": "Xiaoyan Cao"
                    },
                    {
                        "authorId": "9947283",
                        "name": "Yaowen Yao"
                    },
                    {
                        "authorId": "2117007545",
                        "name": "Lanqing Li"
                    },
                    {
                        "authorId": "2984723",
                        "name": "Wanpeng Zhang"
                    },
                    {
                        "authorId": "2111959325",
                        "name": "Zhicheng An"
                    },
                    {
                        "authorId": "2155968325",
                        "name": "Zhong Zhang"
                    },
                    {
                        "authorId": "49020037",
                        "name": "Shihui Guo"
                    },
                    {
                        "authorId": "2149550792",
                        "name": "Li Xiao"
                    },
                    {
                        "authorId": "2149214496",
                        "name": "Xiaoyu Cao"
                    },
                    {
                        "authorId": "2061549073",
                        "name": "Dijun Luo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, model-based reinforcement learning (RL) has achieved a lot of progress in terms of data efficiency and model estimation accuracy [22, 23, 24]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f75d5e708279271e3eca8327cb2e988bf13bc62e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-01360",
                    "ArXiv": "2107.01360",
                    "CorpusId": 235732338
                },
                "corpusId": 235732338,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f75d5e708279271e3eca8327cb2e988bf13bc62e",
                "title": "Supervised Off-Policy Ranking",
                "abstract": "Off-policy evaluation (OPE) is to evaluate a target policy with data generated by other policies. Most previous OPE methods focus on precisely estimating the true performance of a policy. We observe that in many applications, (1) the end goal of OPE is to compare two or multiple candidate policies and choose a good one, which is a much simpler task than precisely evaluating their true performance; and (2) there are usually multiple policies that have been deployed to serve users in real-world systems and thus the true performance of these policies can be known. Inspired by the two observations, in this work, we study a new problem, supervised off-policy ranking (SOPR), which aims to rank a set of target policies based on supervised learning by leveraging off-policy data and policies with known performance. We propose a method to solve SOPR, which learns a policy scoring model by minimizing a ranking loss of the training policies rather than estimating the precise policy performance. The scoring model in our method, a hierarchical Transformer based model, maps a set of state-action pairs to a score, where the state of each pair comes from the off-policy data and the action is taken by a target policy on the state in an offline manner. Extensive experiments on public datasets show that our method outperforms baseline methods in terms of rank correlation, regret value, and stability. Our code is publicly available at GitHub.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152804142",
                        "name": "Yue Jin"
                    },
                    {
                        "authorId": "1591125925",
                        "name": "Yue Zhang"
                    },
                    {
                        "authorId": "143826491",
                        "name": "Tao Qin"
                    },
                    {
                        "authorId": "2108094396",
                        "name": "Xu-Dong Zhang"
                    },
                    {
                        "authorId": "150167381",
                        "name": "Jian Yuan"
                    },
                    {
                        "authorId": "2144406784",
                        "name": "Houqiang Li"
                    },
                    {
                        "authorId": "2110264337",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also make fair comparison with model-based RL baselines in Figure 3 (c) and (d), including Model-based Value Expansion (MVE) [10] and Model-based Policy Optimization (MBPO) [15].",
                "Implementation details of MVE and MBPO are provided in Appendix D.",
                "Model-Based Policy Optimization (MBPO) [15] proves a monotonic improvement with limited use of a predictive model.",
                "Existing methods [21, 15, 14, 28] have apply neural models to greatly facilitate predicting physical dynamics and the consequences of actions, and provide a strong inductive bias for generalization to novel environment situations."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "69037430e2f867ed155c7a3210d4d2578deaf993",
                "externalIds": {
                    "ArXiv": "2107.00306",
                    "DBLP": "journals/corr/abs-2107-00306",
                    "CorpusId": 235694301
                },
                "corpusId": 235694301,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/69037430e2f867ed155c7a3210d4d2578deaf993",
                "title": "MHER: Model-based Hindsight Experience Replay",
                "abstract": "Solving multi-goal reinforcement learning (RL) problems with sparse rewards is generally challenging. Existing approaches have utilized goal relabeling on collected experiences to alleviate issues raised from sparse rewards. However, these methods are still limited in efficiency and cannot make full use of experiences. In this paper, we propose Model-based Hindsight Experience Replay (MHER), which exploits experiences more efficiently by leveraging environmental dynamics to generate virtual achieved goals. Replacing original goals with virtual goals generated from interaction with a trained dynamics model leads to a novel relabeling method, model-based relabeling (MBR). Based on MBR, MHER performs both reinforcement learning and supervised learning for efficient policy improvement. Theoretically, we also prove the supervised part in MHER, i.e., goal-conditioned supervised learning with MBR data, optimizes a lower bound on the multi-goal RL objective. Experimental results in several point-based tasks and simulated robotics environments show that MHER achieves significantly higher sample efficiency than previous model-free and model-based multi-goal methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145094495",
                        "name": "Rui Yang"
                    },
                    {
                        "authorId": "2055723958",
                        "name": "Meng Fang"
                    },
                    {
                        "authorId": "2112661118",
                        "name": "Lei Han"
                    },
                    {
                        "authorId": "1390662136",
                        "name": "Yali Du"
                    },
                    {
                        "authorId": "2072689111",
                        "name": "Feng Luo"
                    },
                    {
                        "authorId": "2127382771",
                        "name": "Xiu Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Different computational approaches have been proposed covering Gradient Descent [9], Model-based Reinforcement Learning (RL) [4], [5], explanations with recommender systems [15], etc.",
                "Advance in Reinforcement Learning methods have proven efficiency to solve a large scale of decision making problems [4] [5] [1], [6], [7]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "dccfbff21ebb17c16a7876332130b7c47428991e",
                "externalIds": {
                    "DBLP": "conf/compsac/ZouhaierHA21",
                    "DOI": "10.1109/COMPSAC51774.2021.00217",
                    "CorpusId": 237473772
                },
                "corpusId": 237473772,
                "publicationVenue": {
                    "id": "e574da12-4e4c-4b78-a203-ae3719c0c4f7",
                    "name": "Annual International Computer Software and Applications Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Comput Softw Appl Conf",
                        "Annu Int Comput Softw Appl Conf",
                        "COMPSAC Work",
                        "Computer Software and Applications Conference",
                        "COMPSAC Workshops",
                        "COMPSAC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=548"
                },
                "url": "https://www.semanticscholar.org/paper/dccfbff21ebb17c16a7876332130b7c47428991e",
                "title": "A Reinforcement Learning Based Approach of Context-driven Adaptive User Interfaces",
                "abstract": "Adaptive User Interfaces (AUI) have to exploit Artificial Intelligence powerful to adapt User Interfaces (UI)s to People With Disabilities (PWD). Thus, Machine Learning methods could master disabilities problems and barriers. In this paper, we propose a Reinforcement Learning (RL) based approach for resolving and surmounting PWD-UI interactions barriers since RL is good for learning good behavior. Hence, we have called the approach as RL-AUIAC as Reinforcement Learning of Adaptive User Interfaces for Accessibility Context. RL-AUIAC is based on three Knowledge Layers (KL)s depending on the kind of adaptation and the resolved problem at each layer. KL uses the Exploration-Exploitation dilemma to respond to the question: what to learn from each planned-adaptation sequences? In fact, Disability Knowledge Layer (DKL)learns UI structure behavior depending on the disability profile. Modality Knowledge Layer(MKL) learns facilities of adaptation on the basis of the couple . Platform Knowledge Layer (PKL) explores-exploits platform-knowledge to learn adaptation facilities on the basis of .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2023965",
                        "name": "Lamia Zouhaier"
                    },
                    {
                        "authorId": "51231234",
                        "name": "Yosra Ben Dali Hlaoui"
                    },
                    {
                        "authorId": "1924819",
                        "name": "Leila Jemni Ben Ayed"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More sophisticated approaches use function approximators and minimize various statistical distances \u2013 e.g. KL (Ross & Bagnell, 2012), total-variation (Janner et al., 2019) or Wasserstein metrics (Wu et al., 2019).",
                ", 2018) and MBPO (Janner et al., 2019) as a foundation for evaluating value-aware approaches in continuous control.",
                "\u2026optimizing for auxiliary objectives (Lee et al., 2020; Nair et al., 2020; Tomar et al., 2021), augmenting model-learning with exploration strategies (Janner et al., 2019; Kidambi et al., 2020), meta-learning to closely intertwine the two objectives (Nagabandi et al., 2018) and introducing\u2026",
                "KL (Ross & Bagnell, 2012), total-variation (Janner et al., 2019) or Wasserstein metrics (Wu et al.",
                "We select two commonly adopted dyna-style MBRL algorithms \u2013 SLBO (Luo et al., 2018) and MBPO (Janner et al., 2019) as a foundation for evaluating value-aware approaches in continuous control.",
                "Figure 3: Evaluation on continuous control environments for value aware methods and baselines with MBPO (Janner et al., 2019), without tuning existing parameters, over 5 random seeds.",
                ", 2018) on Swimmer-v1, Hopper-v1 and Ant-v1 environments and in conjunction with the MBPO algorithm (Janner et al., 2019) on Walker-v2 and Hopper-v2 environments.",
                "Next, we describe our algorithm with which we find positive results in conjunction with the SLBO algorithm (Luo et al., 2018) on Swimmer-v1, Hopper-v1 and Ant-v1 environments and in conjunction with the MBPO algorithm (Janner et al., 2019) on Walker-v2 and Hopper-v2 environments.",
                "Second, we evaluate Algorithm 2 together with our proposed and a prior value aware objective on several continuous control tasks, with two recent dyna-style MBRL algorithms \u2013 SLBO (Luo et al., 2018) and MBPO (Janner et al., 2019).",
                "In contrast, we focus on the class of MBRL methods that explicitly make predictions in the state space, allowing for simple adaptations on top of of well-known MBRL frameworks e.g. Dyna-style algorithms (Sutton, 1990) such as SLBO (Luo et al., 2018) and MBPO (Janner et al., 2019).",
                "We empirically test our proposed algorithm and novel upper bound on two recent dyna-style MBRL algorithms \u2013 SLBO (Luo et al., 2018) and MBPO (Janner et al., 2019).",
                ", 2021), augmenting model-learning with exploration strategies (Janner et al., 2019; Kidambi et al., 2020), meta-learning to closely intertwine the two objectives (Nagabandi et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "88771555a4f3e8aaa5b75181cfdcb7e86aebde81",
                "externalIds": {
                    "ArXiv": "2106.14080",
                    "CorpusId": 246411724
                },
                "corpusId": 246411724,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/88771555a4f3e8aaa5b75181cfdcb7e86aebde81",
                "title": "Model-Advantage and Value-Aware Models for Model-Based Reinforcement Learning: Bridging the Gap in Theory and Practice",
                "abstract": "This work shows that value-aware model learning, known for its numerous theoretical benefits, is also practically viable for solving challenging continuous control tasks in prevalent model-based reinforcement learning algorithms. First, we derive a novel value-aware model learning objective by bounding the model-advantage i.e. model performance difference, between two MDPs or models given a fixed policy, achieving superior performance to prior value-aware objectives in most continuous control environments. Second, we identify the issue of stale value estimates in naively substituting value-aware objectives in place of maximum-likelihood in dyna-style model-based RL algorithms. Our proposed remedy to this issue bridges the long-standing gap in theory and practice of value-aware model learning by enabling successful deployment of all value-aware objectives in solving several continuous control robotic manipulation and locomotion tasks. Our results are obtained with minimal modifications to two popular and open-source model-based RL algorithms \u2013 SLBO and MBPO, without tuning any existing hyper-parameters, while also demonstrating better performance of value-aware objectives than these baseline in some environments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "20809363",
                        "name": "Nirbhay Modhe"
                    },
                    {
                        "authorId": "2074872694",
                        "name": "Harish Kamath"
                    },
                    {
                        "authorId": "1746610",
                        "name": "Dhruv Batra"
                    },
                    {
                        "authorId": "51043791",
                        "name": "A. Kalyan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(12) Taking the cumulative reward subjected to a policy in the actual environment as and its counterpart in the constructed virtual environment model as , we can achieve the relationship between and within rollout steps as [28]:"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "84d84170114afbd45a2a69743344610fd7cc2a3c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-12194",
                    "ArXiv": "2106.12194",
                    "CorpusId": 235606254
                },
                "corpusId": 235606254,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/84d84170114afbd45a2a69743344610fd7cc2a3c",
                "title": "Uncertainty-Aware Model-Based Reinforcement Learning with Application to Autonomous Driving",
                "abstract": "To further improve the learning efficiency and performance of the reinforcement learning (RL), in this paper we propose a novel uncertainty-aware model-based RL (UA-MBRL) framework, and then implement and validate it in autonomous driving under various task scenarios. First, an action-conditioned ensemble model with the ability of uncertainty assessment is established as the virtual environment model. Then, a novel uncertainty-aware model-based RL framework is developed based on the adaptive truncation approach, providing virtual interactions between the agent and environment model, and improving RL\u2019s training efficiency and performance. The developed algorithms are then implemented in end-to-end autonomous vehicle control tasks, validated and compared with state-of-the-art methods under various driving scenarios. The validation results suggest that the proposed UA-MBRL method surpasses the existing model-based and model-free RL approaches, in terms of learning efficiency and achieved performance. The results also demonstrate the good ability of the proposed method with respect to the adaptiveness and robustness, under various autonomous driving scenarios.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47875689",
                        "name": "Jingda Wu"
                    },
                    {
                        "authorId": "47272000",
                        "name": "Zhiyu Huang"
                    },
                    {
                        "authorId": "144818584",
                        "name": "Chen Lv"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d430b57001222d1fff7638dfc7fb4eb7c5a361ab",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-11417",
                    "ArXiv": "2106.11417",
                    "CorpusId": 235593038
                },
                "corpusId": 235593038,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d430b57001222d1fff7638dfc7fb4eb7c5a361ab",
                "title": "Interpretable Model-based Hierarchical Reinforcement Learning using Inductive Logic Programming",
                "abstract": "Recently deep reinforcement learning has achieved tremendous success in wide ranges of applications. However, it notoriously lacks data-efficiency and interpretability. Data-efficiency is important as interacting with the environment is expensive. Further, interpretability can increase the transparency of the black-box-style deep RL models and hence gain trust from the users. In this work, we propose a new hierarchical framework via symbolic RL, leveraging a symbolic transition model to improve the data-efficiency and introduce the interpretability for learned policy. This framework consists of a high-level agent, a subtask solver and a symbolic transition model. Without assuming any prior knowledge on the state transition, we adopt inductive logic programming (ILP) to learn the rules of symbolic state transitions, introducing interpretability and making the learned behavior understandable to users. In empirical experiments, we confirmed that the proposed framework offers approximately between 30\\% to 40\\% more data efficiency over previous methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145548210",
                        "name": "Duo Xu"
                    },
                    {
                        "authorId": "1730720",
                        "name": "F. Fekri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some representative examples of Dyna-style algorithms include MBPO [31], METRPO [44], PAL/MAL [30], and Dreamer [32].",
                "We learn the policy and Q-function using MBPO [31] (which itself uses SAC [38] internally), similar to MOPO.",
                "We learn a policy to solve this optimization using SAC [38], resulting in an algorithm that is similar to a behavior regularized version of Dyna [39] and MBPO [31].",
                "Recently, MBRL algorithms have demonstrated strong results in a variety of RL tasks [30, 31, 32, 33], including offline RL [20, 21, 22]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f71da178cd63958fe659ad613d474b67c5615bd3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-09119",
                    "ArXiv": "2106.09119",
                    "CorpusId": 235457973
                },
                "corpusId": 235457973,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f71da178cd63958fe659ad613d474b67c5615bd3",
                "title": "Behavioral Priors and Dynamics Models: Improving Performance and Domain Transfer in Offline RL",
                "abstract": "Offline Reinforcement Learning (RL) aims to extract near-optimal policies from imperfect offline data without additional environment interactions. Extracting policies from diverse offline datasets has the potential to expand the range of applicability of RL by making the training process safer, faster, and more streamlined. We investigate how to improve the performance of offline RL algorithms, its robustness to the quality of offline data, as well as its generalization capabilities. To this end, we introduce Offline Model-based RL with Adaptive Behavioral Priors (MABE). Our algorithm is based on the finding that dynamics models, which support within-domain generalization, and behavioral priors, which support cross-domain generalization, are complementary. When combined together, they substantially improve the performance and generalization of offline RL policies. In the widely studied D4RL offline RL benchmark, we find that MABE achieves higher average performance compared to prior model-free and model-based algorithms. In experiments that require cross-domain generalization, we find that MABE outperforms prior methods. Our website is available at https://sites.google.com/berkeley.edu/mabe .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113236642",
                        "name": "Catherine Cang"
                    },
                    {
                        "authorId": "19275599",
                        "name": "A. Rajeswaran"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    },
                    {
                        "authorId": "51093256",
                        "name": "M. Laskin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b5fcec8169bc021fcd2905e39b8b26f3eb63419a",
                "externalIds": {
                    "ArXiv": "2106.08272",
                    "DBLP": "journals/corr/abs-2106-08272",
                    "DOI": "10.1111/2041-210X.13954",
                    "CorpusId": 235435978
                },
                "corpusId": 235435978,
                "publicationVenue": {
                    "id": "d66cff25-03a5-432c-b3c8-fbdc87233070",
                    "name": "Methods in Ecology and Evolution",
                    "type": "journal",
                    "alternate_names": [
                        "Method Ecol Evol"
                    ],
                    "issn": "2041-210X",
                    "url": "http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)2041-210X",
                    "alternate_urls": [
                        "http://www.methodsinecologyandevolution.org/view/0/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b5fcec8169bc021fcd2905e39b8b26f3eb63419a",
                "title": "Deep reinforcement learning for conservation decisions",
                "abstract": "Can machine learning help us make better decisions about a changing planet? In this paper, we illustrate and discuss the potential of a promising corner of machine learning known as deep reinforcement learning (RL) to help tackle the most challenging conservation decision problems. We provide a conceptual and technical introduction to deep RL as well as annotated code so that researchers can adopt, evaluate and extend these approaches. RL explicitly focuses on designing an agent who interacts with an environment that is dynamic and uncertain. Deep RL is the subfield of RL that incorporates deep neural networks into the agent. We train deep RL agents to solve sequential decision\u2010making problems in setting fisheries quotas and managing ecological tipping points. We show that a deep RL agent is able to learn a nearly optimal solution for the fisheries management problem. For the tipping point problem, we show that a deep RL agent can outperform a sensible rule\u2010of\u2010thumb strategy. Our results demonstrate that deep RL has the potential to solve challenging decision problems in conservation. While this potential may be compelling, the challenges involved in successfully deploying RL\u2010based management to realistic scenarios are formidable\u2014the required expertise and computational cost may place these applications beyond the reach of all but large, international technology firms. Ecologists must establish a better understanding of how these algorithms work and fail if we are to realize this potential and avoid the pitfalls such a transition would bring. We ultimately set forth a research framework based on well\u2010posed, public challenges so that ecologists and computer scientists can collaborate towards solving hard decision\u2010making problems in conservation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2112209593",
                        "name": "Marcus Lapeyrolerie"
                    },
                    {
                        "authorId": "50774682",
                        "name": "Melissa S. Chapman"
                    },
                    {
                        "authorId": "103966555",
                        "name": "Kari E. A. Norman"
                    },
                    {
                        "authorId": "72997413",
                        "name": "C. Boettiger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, there are online RL algorithms that alternate between data collection steps using a fixed policy, and policy improvement steps by learning on the collected dataset (Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d769ca62d90adc7e7869849a421426bdc54a32fb",
                "externalIds": {
                    "ArXiv": "2106.04895",
                    "DBLP": "conf/nips/XieJWXB21",
                    "CorpusId": 235377259
                },
                "corpusId": 235377259,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d769ca62d90adc7e7869849a421426bdc54a32fb",
                "title": "Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning",
                "abstract": "Recent theoretical work studies sample-efficient reinforcement learning (RL) extensively in two settings: learning interactively in the environment (online RL), or learning from an offline dataset (offline RL). However, existing algorithms and theories for learning near-optimal policies in these two settings are rather different and disconnected. Towards bridging this gap, this paper initiates the theoretical study of policy finetuning, that is, online RL where the learner has additional access to a\"reference policy\"$\\mu$ close to the optimal policy $\\pi_\\star$ in a certain sense. We consider the policy finetuning problem in episodic Markov Decision Processes (MDPs) with $S$ states, $A$ actions, and horizon length $H$. We first design a sharp offline reduction algorithm -- which simply executes $\\mu$ and runs offline policy optimization on the collected dataset -- that finds an $\\varepsilon$ near-optimal policy within $\\widetilde{O}(H^3SC^\\star/\\varepsilon^2)$ episodes, where $C^\\star$ is the single-policy concentrability coefficient between $\\mu$ and $\\pi_\\star$. This offline result is the first that matches the sample complexity lower bound in this setting, and resolves a recent open question in offline RL. We then establish an $\\Omega(H^3S\\min\\{C^\\star, A\\}/\\varepsilon^2)$ sample complexity lower bound for any policy finetuning algorithm, including those that can adaptively explore the environment. This implies that -- perhaps surprisingly -- the optimal policy finetuning algorithm is either offline reduction or a purely online RL algorithm that does not use $\\mu$. Finally, we design a new hybrid offline/online algorithm for policy finetuning that achieves better sample complexity than both vanilla offline reduction and purely online RL algorithms, in a relaxed setting where $\\mu$ only satisfies concentrability partially up to a certain time step.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51437774",
                        "name": "Tengyang Xie"
                    },
                    {
                        "authorId": "48272707",
                        "name": "Nan Jiang"
                    },
                    {
                        "authorId": "2197900626",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": "2228109",
                        "name": "Caiming Xiong"
                    },
                    {
                        "authorId": "1491626939",
                        "name": "Yu Bai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our contributions can be summarized as follows:\n\u2022 We propose OMD, an end-to-end MBRL method that optimizes expected returns directly.",
                "In the previous section, we have empirically demonstrated that OMD outperforms Dyna-style (Sutton, 1991) MBRL agents when the model capacity is limited.",
                "Overall, these findings suggest that the OMD agent achieves near-optimal returns, performs better than the MLE-based MBRL agent as well as VEP under model misspecification, and learns a model that is useful for control despite having low likelihood.",
                "A standard MBRL agent first estimates the transition parameters and the reward function of a Markov Decision Process and then uses the approximate model for planning (Theil, 1957; Kurano, 1972; Mandl, 1974; Georgin, 1978; Borkar & Varaiya, 1979; Herna\u0301ndez-Lerma & Marcus, 1985; Manfred, 1987; Sato et al., 1988).",
                "The experiments reflect the challenges an MBRL agent will face in complex domains such as (Bellemare et al., 2013; Beattie et al., 2016; Kalashnikov et al., 2021): accurately predicting the next observations can be infeasible because the underlying dynamics can be too involved and there might be few components that are important for taking action.",
                "We use the principle of value equivalence for MBRL (Grimm et al., 2020) and argue that value equivalent models are optimal solutions to (2) and (6).",
                "Grimm et al. (2020) introduce the principle of value equivalence for MBRL defining two models to be equivalent if they induce the same Bellman operator.",
                "\u2026separation between model learning and policy optimization is the basis for much of the work on modelbased reinforcement learning (MBRL) (Grefenstette et al., 1990; Sutton, 1991; Lin, 1992; Boots et al., 2011; Chua et al., 2018; Hafner et al., 2019; Janner et al., 2019; Kaiser et al., 2019).",
                "The result suggests that likelihood optimization may be an unnecessary step for MBRL algorithms.",
                "\u2022 We demonstrate that OMD outperforms likelihoodbased MBRL agents under the model misspecification in both tabular and non-tabular settings.",
                "The paper proposes optimal model design (OMD), a method for learning control-oriented models that addresses the short-\ncomings of likelihood-based MBRL approaches.",
                "OMD optimizes the expected returns in an end-to-end manner and alleviates the objective mismatch of standard MBRL methods that train models using a proxy of the true RL objective.",
                "This finding suggests that likelihood optimization might be an unnecessary step for MBRL.",
                "The conceptual separation between model learning and policy optimization is the basis for much of the work on modelbased reinforcement learning (MBRL) (Grefenstette et al., 1990; Sutton, 1991; Lin, 1992; Boots et al., 2011; Chua et al., 2018; Hafner et al., 2019; Janner et al., 2019; Kaiser et al., 2019).",
                "The MLE agent trains the model with MSE effectively becoming the MBPO algorithm (Janner et al., 2019) without having an ensemble of models and learning\nthe variance of the predictions.",
                "Several works (Skelton, 1989; Joseph et al., 2013; Lambert et al., 2020) have pointed out on the objective mismatch in MBRL and demonstrated that optimization of model likelihood might be unrelated to optimization of the returns achieved by the agent that uses the model."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "766c7366ca956634cb15d8238d15cadebe8217c1",
                "externalIds": {
                    "DBLP": "conf/aaai/NikishinAAB22",
                    "ArXiv": "2106.03273",
                    "DOI": "10.1609/aaai.v36i7.20758",
                    "CorpusId": 235358502
                },
                "corpusId": 235358502,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/766c7366ca956634cb15d8238d15cadebe8217c1",
                "title": "Control-Oriented Model-Based Reinforcement Learning with Implicit Differentiation",
                "abstract": "The shortcomings of maximum likelihood estimation in the context of model-based reinforcement learning have been highlighted by an increasing number of papers. When the model class is misspecified or has a limited representational capacity, model parameters with high likelihood might not necessarily result in high performance of the agent on a downstream control task. To alleviate this problem, we propose an end-to-end approach for model learning which directly optimizes the expected returns using implicit differentiation. We treat a value function that satisfies the Bellman optimality operator induced by the model as an implicit function of model parameters and show how to differentiate the function. We provide theoretical and empirical evidence highlighting the benefits of our approach in the model misspecification regime compared to likelihood-based methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "101169516",
                        "name": "Evgenii Nikishin"
                    },
                    {
                        "authorId": "1515553184",
                        "name": "Romina Abachi"
                    },
                    {
                        "authorId": "29767024",
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "authorId": "145180695",
                        "name": "Pierre-Luc Bacon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026impose a great amount of inductive bias (fundamental assumptions about the output function), but as we know incorrect bias can be detrimental to decision-making agents and fully modeling every contributing process can be costly and laborious (Dullerud and Paganini (2013); Janner et al. (2019))."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "478d4968d17d1ab556e574c32fc388f9e40b583a",
                "externalIds": {
                    "ArXiv": "2106.02973",
                    "MAG": "3174919583",
                    "DBLP": "conf/l4dc/Havens021",
                    "CorpusId": 235358317
                },
                "corpusId": 235358317,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/478d4968d17d1ab556e574c32fc388f9e40b583a",
                "title": "Forced Variational Integrator Networks for Prediction and Control of Mechanical Systems",
                "abstract": "As deep learning becomes more prevalent for prediction and control of real physical systems, it is important that these overparameterized models are consistent with physically plausible dynamics. This elicits a problem with how much inductive bias to impose on the model through known physical parameters and principles to reduce complexity of the learning problem to give us more reliable predictions. Recent work employs discrete variational integrators parameterized as a neural network architecture to learn conservative Lagrangian systems. The learned model captures and enforces global energy preserving properties of the system from very few trajectories. However, most real systems are inherently non-conservative and, in practice, we would also like to apply actuation. In this paper we extend this paradigm to account for general forcing (e.g. control input and damping) via discrete d'Alembert's principle which may ultimately be used for control applications. We show that this forced variational integrator networks (FVIN) architecture allows us to accurately account for energy dissipation and external forcing while still capturing the true underlying energy-based passive dynamics. We show that in application this can result in highly-data efficient model-based control and can predict on real non-conservative systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51132368",
                        "name": "Aaron J. Havens"
                    },
                    {
                        "authorId": "1733356",
                        "name": "Girish V. Chowdhary"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Though model-based methods have been applied to the humanoid task, prior works tend to keep the horizon\nintentionally short to prevent the accumulation of model errors (Janner et al., 2019; Amos et al., 2021).",
                "intentionally short to prevent the accumulation of model errors (Janner et al., 2019; Amos et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f864d4d2267abba15eb43db54f58286aef78292b",
                "externalIds": {
                    "DBLP": "conf/nips/JannerLL21",
                    "ArXiv": "2106.02039",
                    "CorpusId": 235313679
                },
                "corpusId": 235313679,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f864d4d2267abba15eb43db54f58286aef78292b",
                "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem",
                "abstract": "Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35163402",
                        "name": "Michael Janner"
                    },
                    {
                        "authorId": "8194287",
                        "name": "Qiyang Li"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is healthy to point out the limitations of this work as well as some interesting future research directions: \u2022 This paper does not solve the \u201cplanning horizon dilemma\u201d, a fundamental issue of error accumulation of tree search expansion using imperfect models [23]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c0abc045e176c6a8f715aa5f9cd0d37c537ba2c3",
                "externalIds": {
                    "DBLP": "conf/nips/ZhaoLLZPB21",
                    "ArXiv": "2106.02097",
                    "CorpusId": 235352975
                },
                "corpusId": 235352975,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c0abc045e176c6a8f715aa5f9cd0d37c537ba2c3",
                "title": "A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning",
                "abstract": "We present an end-to-end, model-based deep reinforcement learning agent which dynamically attends to relevant parts of its state during planning. The agent uses a bottleneck mechanism over a set-based representation to force the number of entities to which the agent attends at each planning step to be small. In experiments, we investigate the bottleneck mechanism with several sets of customized environments featuring different challenges. We consistently observe that the design allows the planning agents to generalize their learned task-solving abilities in compatible unseen environments by attending to the relevant objects, leading to better out-of-distribution generalization performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152527029",
                        "name": "Mingde Zhao"
                    },
                    {
                        "authorId": "2109343010",
                        "name": "Zhen Liu"
                    },
                    {
                        "authorId": "121556146",
                        "name": "Sitao Luan"
                    },
                    {
                        "authorId": "2108021834",
                        "name": "Shuyuan Zhang"
                    },
                    {
                        "authorId": "144368601",
                        "name": "Doina Precup"
                    },
                    {
                        "authorId": "1865800402",
                        "name": "Y. Bengio"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Figure 1 illustrates the last idea, which has received much attention recently (Chua et al., 2018; Janner et al., 2019).",
                "The follow-up work (Janner et al., 2019) extended PETS with policy learning.",
                "\u2026et al., 2017; Ruiz et al., 2019), iii) randomizing trajectory synthesis (Andrychowicz et al., 2017; Lu et al., 2019), iv) selecting more diverse data contributors (Stasaski et al., 2020), and v) sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",
                "Different from (Janner et al., 2019), our method is designed for dialogue agents\u2019 discrete action space.",
                "In our method, each training trajectory has an overlap much larger than (Janner et al., 2019) has with the expert trajectory.",
                ", 2020), and sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",
                "\u2026parameters (Tobin et al., 2017; Ruiz et al., 2019), randomizing trajectory synthesis (Andrychowicz et al., 2017; Lu et al., 2019), selecting more diverse data contributors (Stasaski et al., 2020), and sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",
                ", 2020), and v) sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ec7805fef3033b513946fd8ffe66bca42c3a9112",
                "externalIds": {
                    "ArXiv": "2106.00891",
                    "DBLP": "journals/corr/abs-2106-00891",
                    "CorpusId": 235293733
                },
                "corpusId": 235293733,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ec7805fef3033b513946fd8ffe66bca42c3a9112",
                "title": "High-Quality Diversification for Task-Oriented Dialogue Systems",
                "abstract": "Many task-oriented dialogue systems use deep reinforcement learning (DRL) to learn policies that respond to the user appropriately and complete the tasks successfully. Training DRL agents with diverse dialogue trajectories prepare them well for rare user requests and unseen situations. One effective diversification method is to let the agent interact with a diverse set of learned user models. However, trajectories created by these artificial user models may contain generation errors, which can quickly propagate into the agent's policy. It is thus important to control the quality of the diversification and resist the noise. In this paper, we propose a novel dialogue diversification method for task-oriented dialogue systems trained in simulators. Our method, Intermittent Short Extension Ensemble (I-SEE), constrains the intensity to interact with an ensemble of diverse user models and effectively controls the quality of the diversification. Evaluations on the Multiwoz dataset show that I-SEE successfully boosts the performance of several state-of-the-art DRL dialogue agents.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115295389",
                        "name": "Zhiwen Tang"
                    },
                    {
                        "authorId": "50359017",
                        "name": "Hrishikesh Kulkarni"
                    },
                    {
                        "authorId": "2109540870",
                        "name": "Grace Hui Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, TD-learning is the dominant paradigm in RL for sample efficiency, and also features prominently as a sub-routine in many model-based RL algorithms (Sutton, 1990; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-01345",
                    "MAG": "3169291081",
                    "ArXiv": "2106.01345",
                    "CorpusId": 235294299
                },
                "corpusId": 235294299,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
                "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
                "abstract": "We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108435457",
                        "name": "Lili Chen"
                    },
                    {
                        "authorId": "2070275468",
                        "name": "Kevin Lu"
                    },
                    {
                        "authorId": "19275599",
                        "name": "A. Rajeswaran"
                    },
                    {
                        "authorId": "3436470",
                        "name": "Kimin Lee"
                    },
                    {
                        "authorId": "1954250",
                        "name": "Aditya Grover"
                    },
                    {
                        "authorId": "51093256",
                        "name": "M. Laskin"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    },
                    {
                        "authorId": "41207614",
                        "name": "A. Srinivas"
                    },
                    {
                        "authorId": "2080746",
                        "name": "Igor Mordatch"
                    }
                ]
            }
        },
        {
            "contexts": [
                "TRPO [9] and MBPO [11] utilize Bmodel, the compounding error inheriting from model-bias would propagate during agent training.",
                "For example, Dyna-style methods [9], [11], [15] use the models to generate augmented samples.",
                "In our comparisons, we compare to SAC [20] and MBPO [11], which represent the state-of-the-art in both model-free and model-based RL.",
                "In our experiments, MEEE consistently improves the performance of state-of-the-art RL methods and outperforms baselines, including SAC [20] and MBPO [11].",
                "This error propagation is shown to cause convergence and divergence between the optimal policies under the true dynamic P and the model-ensemble P\u03c6 [11]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7a62d9d1f5617559fd446596cc286eeefd39b959",
                "externalIds": {
                    "ArXiv": "2107.01825",
                    "DBLP": "conf/icra/YaoXAZL21",
                    "DOI": "10.1109/ICRA48506.2021.9561842",
                    "CorpusId": 235732280
                },
                "corpusId": 235732280,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7a62d9d1f5617559fd446596cc286eeefd39b959",
                "title": "Sample Efficient Reinforcement Learning via Model-Ensemble Exploration and Exploitation",
                "abstract": "Model-based deep reinforcement learning has achieved success in various domains that require high sample efficiencies, such as Go and robotics. However, there are some remaining issues, such as planning efficient explorations to learn more accurate dynamic models, evaluating the uncertainty of the learned models, and more rational utilization of models. To mitigate these issues, we present MEEE, a model-ensemble method that consists of optimistic exploration and weighted exploitation. During exploration, unlike prior methods directly selecting the optimal action that maximizes the expected accumulative return, our agent first generates a set of action candidates and then seeks out the optimal action that takes both expected return and future observation novelty into account. During exploitation, different discounted weights are assigned to imagined transition tuples according to their model uncertainty respectively, which will prevent model predictive error propagation in agent training. Experiments on several challenging continuous control benchmark tasks demonstrated that our approach outperforms other model-free and model-based state-of-the-art methods, especially in sample complexity.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9947283",
                        "name": "Yaowen Yao"
                    },
                    {
                        "authorId": "2149550792",
                        "name": "Li Xiao"
                    },
                    {
                        "authorId": "2111959325",
                        "name": "Zhicheng An"
                    },
                    {
                        "authorId": "2984723",
                        "name": "Wanpeng Zhang"
                    },
                    {
                        "authorId": "2061549073",
                        "name": "Dijun Luo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "deep neural networks [12], [15], [16]) or even non-parametric models (e.",
                "deep neural networks ([12], [15], [16])).",
                "Finally, the recently used probabilistic model ensembles [15], [16] allowed model-based methods to achieve the same asymptotic performance as state-of-the-art model-free methods, with higher sample efficiency.",
                "In probabilistic model-based RL algorithms, this corresponds to the policy improvement step, where the agent seeks to find the best control given a probabilistic model \u03c0 , using a policy search method [10], [12], [16] or MPC [13], [15] or other methods.",
                "In BRL algorithms [10], [12], [13], [15], [16], [14], the probability distribution, which is built upon the data collected while exploring the partially unknown environment, is updated when new experience is gained."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c748f778b31bd5d098b5c739f7b5eb09c6013536",
                "externalIds": {
                    "DBLP": "conf/eucc/PesarePF21",
                    "ArXiv": "2105.13708",
                    "DOI": "10.23919/ecc54610.2021.9655079",
                    "CorpusId": 235247904
                },
                "corpusId": 235247904,
                "publicationVenue": {
                    "id": "58c057a8-5760-4108-b223-e276334bb8cd",
                    "name": "European Control Conference",
                    "type": "conference",
                    "alternate_names": [
                        "ECC",
                        "Euro-china Conf Intell Data Anal Appl",
                        "Eur Control Conf",
                        "Euro-China Conference on Intelligent Data Analysis and Applications"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c748f778b31bd5d098b5c739f7b5eb09c6013536",
                "title": "Convergence of the Value Function in Optimal Control Problems with Unknown Dynamics",
                "abstract": "We deal with the convergence of the value function of an approximate control problem with uncertain dynamics to the value function of a nonlinear optimal control problem. The assumptions on the dynamics and the costs are rather general and we assume to represent uncertainty in the dynamics by a probability distribution. The proposed framework aims to describe and motivate some model-based Reinforcement Learning algorithms where the model is probabilistic. We also show some numerical experiments which confirm the theoretical results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "14832671",
                        "name": "A. Pesare"
                    },
                    {
                        "authorId": "47457447",
                        "name": "M. Palladino"
                    },
                    {
                        "authorId": "144704696",
                        "name": "M. Falcone"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We emphasize that the performance difference below measures the difference in returns in the true MDPM, rather than, as is commonly seen in model-based RL [23], the difference in the latent MDP defined byRZ ,PZ ."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bb18b3e20ba195397a4d34c663fed485dce5fbee",
                "externalIds": {
                    "ArXiv": "2105.12272",
                    "DBLP": "conf/nips/NachumY21",
                    "CorpusId": 235196059
                },
                "corpusId": 235196059,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bb18b3e20ba195397a4d34c663fed485dce5fbee",
                "title": "Provable Representation Learning for Imitation with Contrastive Fourier Features",
                "abstract": "In imitation learning, it is common to learn a behavior policy to match an unknown target policy via max-likelihood training on a collected set of target demonstrations. In this work, we consider using offline experience datasets - potentially far from the target distribution - to learn low-dimensional state representations that provably accelerate the sample-efficiency of downstream imitation learning. A central challenge in this setting is that the unknown target policy itself may not exhibit low-dimensional behavior, and so there is a potential for the representation learning objective to alias states in which the target policy acts differently. Circumventing this challenge, we derive a representation learning objective that provides an upper bound on the performance difference between the target policy and a lowdimensional policy trained with max-likelihood, and this bound is tight regardless of whether the target policy itself exhibits low-dimensional structure. Moving to the practicality of our method, we show that our objective can be implemented as contrastive learning, in which the transition dynamics are approximated by either an implicit energy-based model or, in some special cases, an implicit linear model with representations given by random Fourier features. Experiments on both tabular environments and high-dimensional Atari games provide quantitative evidence for the practical benefits of our proposed objective.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7624658",
                        "name": "Ofir Nachum"
                    },
                    {
                        "authorId": "2111076891",
                        "name": "Mengjiao Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, as the agent encounters contexts with structural similarities with respect to previously-encounters ones (around time step 160k), MBCD\u2019s performance becomes near-optimal: it rapidly identifies whenever a context change has occurred and deploys an appropriate policy.3 MBPO and SAC, on the other hand, suffer from negative transfer due to learning average policies or dynamics models.",
                "2 Notice that this is similar to the procedure used by the Model-Based Policy Optimization (MBPO) algorithm [11].",
                "In particular, following recent work on model-based RL [4, 11], MBCD models the dynamics of a given environmentMz , p\u03b8z (St+1, Rt |St , At ), via a bootstrap ensemble of probabilistic neural networks whose outputs parameterize a multivariate Gaussian distribution with diagonal covariance matrix.",
                "All rollouts are stored in a buffer, \ud835\udc37\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 .2 Notice that this is similar to the procedure used by the Model-Based Policy Optimization (MBPO) algorithm [11].",
                "5 compares MBCD, MBPO, and SAC in the fully-online setting\u2014no pre-training phase is allowed.",
                "To do this, we compare MBCD, MBPO, SAC, ReBAL, and GrBAL, in the nonstationary continuous particle maze domain, where the sources of non-stationarity are as discussed earlier.",
                "In particular, both MBCD and MBPO\u2019s performances temporarily drop when a novel context is encountered for the first time.",
                "MBCD\u2019s performance drops because it instantiates a new dynamics model for the newly encountered context, while MBPO\u2019s performance drops because it undergoes negative transfer.",
                "3 shows the cumulative sum of rewards achieved by MBCD, MBPO, SAC, and by an Oracle algorithm that is initialized with optimal policies for all contexts and that detects context changes with zero delay.",
                "In our setting, MBPO can be seen as a particular case of our algorithm, where a single dynamics model and policy are tasked with optimizing behavior under changing\n0 40 k 80 k 120 k 160 k 200 k 240 k 280 k 320 k 360 k 400 k 440 k Step\n\u22122000\n\u22121500\n\u22121000\n\u2212500\n0\nE pi\nso de\nT ot\nal R\new ar\nd\nMBCD (ours) MBPO SAC\n(a) Total reward achieved by different methods (MBCD, MBPO, and SAC) as contexts change.",
                "2a shows the total reward achieved by different methods (ours, MBPO, SAC) as contexts change.",
                "We first evaluate ourmethod on the non-stationaryHalf-Cheetah domain and compare it with two state-of-the-art RL algorithms: MBPO [11] and SAC [8].",
                "Notice that our method and MBPO have similar performances when interacting for the first time with the first three random contexts.",
                "SAC works similarly to MBPO but does not perform Dynastyle planning steps using a learned dynamics model."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e30bdcfc8cc2769ffd6d39c5d60651787a8d9ab3",
                "externalIds": {
                    "DBLP": "conf/atal/AlegreB021",
                    "ArXiv": "2105.09452",
                    "DOI": "10.5555/3463952.3463970",
                    "CorpusId": 232285561
                },
                "corpusId": 232285561,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e30bdcfc8cc2769ffd6d39c5d60651787a8d9ab3",
                "title": "Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via Online High-Confidence Change-Point Detection",
                "abstract": "Non-stationary environments are challenging for reinforcement learning algorithms. If the state transition and/or reward functions change based on latent factors, the agent is effectively tasked with optimizing a behavior that maximizes performance over a possibly infinite random sequence of Markov Decision Processes (MDPs), each of which drawn from some unknown distribution. We call each such MDP a context. Most related works make strong assumptions such as knowledge about the distribution over contexts, the existence of pre-training phases, or a priori knowledge about the number, sequence, or boundaries between contexts. We introduce an algorithm that efficiently learns policies in non-stationary environments. It analyzes a possibly infinite stream of data and computes, in real-time, high-confidence change-point detection statistics that reflect whether novel, specialized policies need to be created and deployed to tackle novel contexts, or whether previously-optimized ones might be reused. We show that (i) this algorithm minimizes the delay until unforeseen changes to a context are detected, thereby allowing for rapid responses; and (ii) it bounds the rate of false alarm, which is important in order to minimize regret. Our method constructs a mixture model composed of a (possibly infinite) ensemble of probabilistic dynamics predictors that model the different modes of the distribution over underlying latent MDPs. We evaluate our algorithm on high-dimensional continuous reinforcement learning problems and show that it outperforms state-of-the-art (model-free and model-based) RL algorithms, as well as state-of-the-art meta-learning methods specially designed to deal with non-stationarity.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1411722140",
                        "name": "L. N. Alegre"
                    },
                    {
                        "authorId": "1707374",
                        "name": "A. Bazzan"
                    },
                    {
                        "authorId": "145471664",
                        "name": "Bruno C. da Silva"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "25009d389885645c1210ae6382a4c8b2439230af",
                "externalIds": {
                    "DBLP": "conf/ijcai/ZhanZX22",
                    "ArXiv": "2105.07351",
                    "DOI": "10.24963/ijcai.2022/516",
                    "CorpusId": 234742314
                },
                "corpusId": 234742314,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/25009d389885645c1210ae6382a4c8b2439230af",
                "title": "Model-Based Offline Planning with Trajectory Pruning",
                "abstract": "The recent offline reinforcement learning (RL) studies have achieved much progress to make RL usable in real-world systems by learning policies from pre-collected datasets without environment interaction. Unfortunately, existing offline RL methods still face many practical challenges in real-world system control tasks, such as computational restriction during agent training and the requirement of extra control flexibility. The model-based planning framework provides an attractive alternative. However, most model-based planning algorithms are not designed for offline settings. Simply combining the ingredients of offline RL with existing methods either provides over-restrictive planning or leads to inferior performance. We propose a new light-weighted model-based offline planning framework, namely MOPP, which tackles the dilemma between the restrictions of offline learning and high-performance planning. MOPP encourages more aggressive trajectory rollout guided by the behavior policy learned from data, and prunes out problematic trajectories to avoid potential out-of-distribution samples. Experimental results show that MOPP provides competitive performance compared with existing model-based offline planning and RL approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3415564",
                        "name": "Xianyuan Zhan"
                    },
                    {
                        "authorId": "2144104090",
                        "name": "Xiangyu Zhu"
                    },
                    {
                        "authorId": "49507262",
                        "name": "Haoran Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this paper, we expand model-based policy optimization [Janner et al., 2019] into the goal-based setting and propose Universal Model-based Policy Optimization (UMPO)\nfor efficient goal-based policy learning.",
                "A.3 Details of Implementation The code of dynamics model is based on the realization of [Janner et al., 2019] and we modified it slightly to fix the bug that using an increasing number of video memory.",
                "Besides, the learned dynamics models are further used to generate branched short rollouts for policy optimization in a Dyna-style manner [Janner et al., 2019].",
                "In this paper, we expand model-based policy optimization [Janner et al., 2019] into the goal-based setting and propose Universal Model-based Policy Optimization (UMPO)"
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "59a92066b7e59f63413f46b3bfe551f42f6c80b4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-06350",
                    "ArXiv": "2105.06350",
                    "DOI": "10.24963/ijcai.2021/480",
                    "CorpusId": 234482728
                },
                "corpusId": 234482728,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/59a92066b7e59f63413f46b3bfe551f42f6c80b4",
                "title": "MapGo: Model-Assisted Policy Optimization for Goal-Oriented Tasks",
                "abstract": "In Goal-oriented Reinforcement learning, relabeling the raw goals in past experience to provide agents with hindsight ability is a major solution to the reward sparsity problem. In this paper, to enhance the diversity of relabeled goals, we develop FGI (Foresight Goal Inference), a new relabeling strategy that relabels the goals by looking into the future with a learned dynamics model. Besides, to improve sample efficiency, we propose to use the dynamics model to generate simulated trajectories for policy training. By integrating these two improvements, we introduce the MapGo framework (Model-Assisted Policy optimization for Goal-oriented tasks). In our experiments, we first show the effectiveness of the FGI strategy compared with the hindsight one, and then show that the MapGo framework achieves higher sample efficiency when compared to model-free baselines on a set of complicated tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1384787219",
                        "name": "Menghui Zhu"
                    },
                    {
                        "authorId": null,
                        "name": "Minghuan Liu"
                    },
                    {
                        "authorId": "2115732606",
                        "name": "Jian Shen"
                    },
                    {
                        "authorId": "2116706397",
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "authorId": "2118529933",
                        "name": "Sheng Chen"
                    },
                    {
                        "authorId": "2108309275",
                        "name": "Weinan Zhang"
                    },
                    {
                        "authorId": "2055648566",
                        "name": "Deheng Ye"
                    },
                    {
                        "authorId": "1811427",
                        "name": "Yong Yu"
                    },
                    {
                        "authorId": "2091914469",
                        "name": "Qiang Fu"
                    },
                    {
                        "authorId": "2005150594",
                        "name": "Wei Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[Janner et al., 2019] generate (truncated) short trajectories with a probabilistic ensemble to train the policy of a MFRL, thus improving significantly its sampling efficiency."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c15a00bcb1531d964de712c179c08e592e70e079",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-05716",
                    "ArXiv": "2105.05716",
                    "CorpusId": 234469755
                },
                "corpusId": 234469755,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c15a00bcb1531d964de712c179c08e592e70e079",
                "title": "Acting upon Imagination: when to trust imagined trajectories in model based reinforcement learning",
                "abstract": "Model based reinforcement learning (MBRL) uses an imperfect model of the world to imagine trajectories of future states and plan the best actions that maximize a given reward. These trajectories are imperfect and MBRL attempts to overcome this by relying on model predictive control (MPC) to continuously re-imagine trajectories from scratch. Such re-generation of imagined trajectories carries the major computational cost and increasing complexity in tasks with longer receding horizon. We investigate how far in the future the imagined trajectories can be relied upon while still maintaining acceptable reward. After taking each action, information becomes available about its immediate effect and its impact on outcomes expected of future actions. Hereby, we propose four methods for deciding whether to trust and act upon imagined trajectories: i) looking at recent errors with respect to expectations, ii) comparing the confidence in an action imagined against its execution, iii) observing the deviation in projected future states iv) observing the deviation in projected future rewards. An experiment analyzing the effects of acting upon imagination shows that our methods reduce computation by at least 20\\% and up to 80\\%, depending on the environment, while retaining acceptable reward.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1413516516",
                        "name": "Adrian Remonda"
                    },
                    {
                        "authorId": "89180102",
                        "name": "Eduardo E. Veas"
                    },
                    {
                        "authorId": "2270659",
                        "name": "Granit Luzhnica"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Here we first investigate a k-step rollout scheme, which is a natural extension of MBPO [Janner et al., 2019] to multi-agent scenario."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "309741e91c671bf57b8fee60a892af7d6f264eeb",
                "externalIds": {
                    "DBLP": "conf/ijcai/0001WSZ21",
                    "ArXiv": "2105.03363",
                    "DOI": "10.24963/ijcai.2021/466",
                    "CorpusId": 234093673
                },
                "corpusId": 234093673,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/309741e91c671bf57b8fee60a892af7d6f264eeb",
                "title": "Model-based Multi-agent Policy Optimization with Adaptive Opponent-wise Rollouts",
                "abstract": "This paper investigates the model-based methods in multi-agent reinforcement learning (MARL). We specify the dynamics sample complexity and the opponent sample complexity in MARL, and conduct a theoretic analysis of return discrepancy upper bound. To reduce the upper bound with the intention of low sample complexity during the whole learning process, we propose a novel decentralized model-based MARL method, named Adaptive Opponent-wise Rollout Policy Optimization (AORPO). In AORPO, each agent builds its multi-agent environment model, consisting of a dynamics model and multiple opponent models, and trains its policy with the adaptive opponent-wise rollout. We further prove the theoretic convergence of AORPO under reasonable assumptions. Empirical experiments on competitive and cooperative tasks demonstrate that AORPO can achieve improved sample efficiency with comparable asymptotic performance over the compared MARL methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108309275",
                        "name": "Weinan Zhang"
                    },
                    {
                        "authorId": "2178008",
                        "name": "Xihuai Wang"
                    },
                    {
                        "authorId": "2115732606",
                        "name": "Jian Shen"
                    },
                    {
                        "authorId": "2152174952",
                        "name": "Ming Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use neural nets to parameterize both distributions since they are powerful function approximators that have been effective for model-based RL (Chua et al., 2018; Nagabandi et al., 2018; Janner et al., 2019).",
                "Similar to prior work (Janner et al., 2019), our baseline feedforward model outputs the mean and log variance of all state dimensions and reward simultaneously, as follows: p\u03b8(st+1, rt+1 | st, at) = N ( \u03bc(st, at),Diag(exp{l(st, at)}) ) , (3) where \u03bc(st, at) \u2208 R denotes the mean for the concatenation of the next state and reward, l(st, at) \u2208 R denotes the log variance, and Diag(v) is an operator that creates a diagonal matrix with the main diagonal specified by the vector v.",
                "The field of model-based RL has matured in recent years to yield impressive results for both online (Nagabandi et al., 2018; Chua et al., 2018; Kurutach et al., 2018; Janner et al., 2019) and offline (Matsushima et al."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7e687699b6c2e075091cebe4b7b8dbd4dc3a7406",
                "externalIds": {
                    "MAG": "3157293568",
                    "DBLP": "conf/iclr/ZhangPNPT0021",
                    "ArXiv": "2104.13877",
                    "CorpusId": 233423379
                },
                "corpusId": 233423379,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7e687699b6c2e075091cebe4b7b8dbd4dc3a7406",
                "title": "Autoregressive Dynamics Models for Offline Policy Evaluation and Optimization",
                "abstract": "Standard dynamics models for continuous control make use of feedforward computation to predict the conditional distribution of next state and reward given current state and action using a multivariate Gaussian with a diagonal covariance structure. This modeling choice assumes that different dimensions of the next state and reward are conditionally independent given the current state and action and may be driven by the fact that fully observable physics-based simulation environments entail deterministic transition dynamics. In this paper, we challenge this conditional independence assumption and propose a family of expressive autoregressive dynamics models that generate different dimensions of the next state and reward sequentially conditioned on previous dimensions. We demonstrate that autoregressive dynamics models indeed outperform standard feedforward models in log-likelihood on heldout transitions. Furthermore, we compare different model-based and model-free off-policy evaluation (OPE) methods on RL Unplugged, a suite of offline MuJoCo datasets, and find that autoregressive dynamics models consistently outperform all baselines, achieving a new state-of-the-art. Finally, we show that autoregressive dynamics models are useful for offline policy optimization by serving as a way to enrich the replay buffer through data augmentation and improving performance using model-based planning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109123395",
                        "name": "Michael R. Zhang"
                    },
                    {
                        "authorId": "40470211",
                        "name": "T. Paine"
                    },
                    {
                        "authorId": "7624658",
                        "name": "Ofir Nachum"
                    },
                    {
                        "authorId": "3316271",
                        "name": "Cosmin Paduraru"
                    },
                    {
                        "authorId": "145499435",
                        "name": "G. Tucker"
                    },
                    {
                        "authorId": "2117966548",
                        "name": "Ziyun Wang"
                    },
                    {
                        "authorId": "144739074",
                        "name": "Mohammad Norouzi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As proof of concept for MBRL-Lib, we provide implementations for two state-of-the-art MBRL algorithms, namely, PETS [Chua et al., 2018] and MBPO [Janner et al., 2019].",
                "In MBRL, the code landscape consists mostly of a relatively limited number of specific algorithm implementations that are publicly available [Chua et al., 2018, Janner et al., 2019, Wang and Ba, 2019].",
                "\u2022 Use a model-free policy: In this case a model-free learner, such as SAC [Haarnoja et al., 2018], is used over the predicted dynamics, often by populating a populating a replay buffer with \u201cimagined\u201d trajectories obtained from the model [Gu et al., 2016, Kurutach et al., 2018, Janner et al., 2019]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "16a3dca5363a464fb689f2027f1531751e5617ec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-10159",
                    "ArXiv": "2104.10159",
                    "CorpusId": 233307364
                },
                "corpusId": 233307364,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/16a3dca5363a464fb689f2027f1531751e5617ec",
                "title": "MBRL-Lib: A Modular Library for Model-based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning is a compelling framework for data-efficient learning of agents that interact with the world. This family of algorithms has many subcomponents that need to be carefully selected and tuned. As a result the entry-bar for researchers to approach the field and to deploy it in real-world tasks can be daunting. In this paper, we present MBRL-Lib -- a machine learning library for model-based reinforcement learning in continuous state-action spaces based on PyTorch. MBRL-Lib is designed as a platform for both researchers, to easily develop, debug and compare new algorithms, and non-expert user, to lower the entry-bar of deploying state-of-the-art algorithms. MBRL-Lib is open-source at https://github.com/facebookresearch/mbrl-lib.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2066236763",
                        "name": "Luis Pineda"
                    },
                    {
                        "authorId": "1773498",
                        "name": "Brandon Amos"
                    },
                    {
                        "authorId": "2111672235",
                        "name": "Amy Zhang"
                    },
                    {
                        "authorId": "2052363815",
                        "name": "Nathan Lambert"
                    },
                    {
                        "authorId": "35159852",
                        "name": "R. Calandra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[21] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7a5be21149843fbe210f88ea368888a381f6a301",
                "externalIds": {
                    "DBLP": "conf/nips/RudnerPMGL21",
                    "ArXiv": "2104.10190",
                    "CorpusId": 233324592
                },
                "corpusId": 233324592,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7a5be21149843fbe210f88ea368888a381f6a301",
                "title": "Outcome-Driven Reinforcement Learning via Variational Inference",
                "abstract": "While reinforcement learning algorithms provide automated acquisition of optimal policies, practical application of such methods requires a number of design decisions, such as manually designing reward functions that not only define the task, but also provide sufficient shaping to accomplish it. In this paper, we view reinforcement learning as inferring policies that achieve desired outcomes, rather than as a problem of maximizing rewards. To solve this inference problem, we establish a novel variational inference formulation that allows us to derive a well-shaped reward function which can be learned directly from environment interactions. From the corresponding variational objective, we also derive a new probabilistic Bellman backup operator and use it to develop an off-policy algorithm to solve goal-directed tasks. We empirically demonstrate that this method eliminates the need to hand-craft reward functions for a suite of diverse manipulation and locomotion tasks and leads to effective goal-directed behaviors.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51918914",
                        "name": "Tim G. J. Rudner"
                    },
                    {
                        "authorId": "144401061",
                        "name": "Vitchyr H. Pong"
                    },
                    {
                        "authorId": "49686609",
                        "name": "R. McAllister"
                    },
                    {
                        "authorId": "2681954",
                        "name": "Y. Gal"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Likewise, [27] gives some hints when to trust"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "16f940a1bba3cb1b6d2c9ed057ceef043f0961de",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-10483",
                    "ArXiv": "2104.10483",
                    "DOI": "10.2139/ssrn.3830012",
                    "CorpusId": 233324242
                },
                "corpusId": 233324242,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/16f940a1bba3cb1b6d2c9ed057ceef043f0961de",
                "title": "Adaptive Learning for Financial Markets Mixing Model-Based and Model-Free RL for Volatility Targeting",
                "abstract": "Model-Free Reinforcement Learning has achieved meaningful results in stable environments but, to this day, it remains problematic in regime changing environments like financial markets. In contrast, model-based RL is able to capture some fundamental and dynamical concepts of the environment but suffer from cognitive bias. In this work, we propose to combine the best of the two techniques by selecting various model-based approaches thanks to Model-Free Deep Reinforcement Learning. Using not only past performance and volatility, we include additional contextual information such as macro and risk appetite signals to account for implicit regime changes. We also adapt traditional RL methods to real-life situations by considering only past data for the training sets. Hence, we cannot use future information in our training data set as implied by K-fold cross validation. Building on traditional statistical methods, we use the traditional \"walk-forward analysis\", which is defined by successive training and testing based on expanding periods, to assert the robustness of the resulting agent. <br><br>Finally, we present the concept of statistical difference's significance based on a two-tailed T-test, to highlight the ways in which our models differ from more traditional ones. Our experimental results show that our approach outperforms traditional financial baseline portfolio models such as the Markowitz model in almost all evaluation metrics commonly used in financial mathematics, namely net performance, Sharpe and Sortino ratios, maximum drawdown, maximum drawdown over volatility.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2064931395",
                        "name": "E. Benhamou"
                    },
                    {
                        "authorId": "21678767",
                        "name": "D. Saltiel"
                    },
                    {
                        "authorId": "40810103",
                        "name": "S. Tabachnik"
                    },
                    {
                        "authorId": "145988612",
                        "name": "Sui Kai Wong"
                    },
                    {
                        "authorId": "2127870142",
                        "name": "Fran\u00e7ois Chareyron"
                    }
                ]
            }
        },
        {
            "contexts": [
                "namely Trust-Region Policy Optimization (TRPO) [26], and Model-Based Policy Optimization (MBPO) [23].",
                "Our aim is to better understand the practical merits and limitations of the proposed approach by assessing the results in light of the following questions:\n1) Can CMBPO maintain safety constraints throughout training in high-dimensional state- and action-spaces?",
                "CMBPO reaches model-free asymptotic performances on all tested experiments with an increase in sample efficiency by 1-2 orders of magnitude relative to model-free methods.",
                "Finally, a particularly exciting avenue is the application and evaluation of CMBPO for safe control on physical robots.",
                "1d, [8]) rewards an agent for running along a circle with the constraint of staying within a safe corridor.\na) Comparative Evaluation: We compare CMBPO to three safe exploration algorithms, namely Constrained Policy Optimization (CPO) [8], Lagrangian Trust Region Policy Optimization (TRPO-L), Lagrangian Proximal Policy Optimization (PPO-L) [7], and two unconstrained algorithms,\nnamely Trust-Region Policy Optimization (TRPO) [26], and Model-Based Policy Optimization (MBPO) [23].",
                "Lastly, we point out that preliminary experiments on the recently published benchmark suite safety-gym [7] showed poor performances of CMBPO due to partial observability and strong covariances between state dimensions.",
                "We observe that the unconstrained algorithms TRPO and MBPO exceed cost constraints on all experiments, highlighting the trade-off between greedy return maximization and constraint satisfaction in our environments.",
                "The model-based baseline MBPO exceeds CMBPO\u2019s sample efficiency considerably, an observation we mainly attribute to its high reliance on modelgenerated and off-policy samples.",
                "Compared to the safe baseline algorithms, CMBPO exhibits slightly less adherence to constant constraint satisfaction but succeeds in reaching safe policies asymptotically.",
                "A comprehensive list of the hyperparameters for CMBPO in our experiments is given in Table II.",
                "propose a branched rollout scheme where short model- trajectories are started from previously observed off-policy states [23].",
                "[23], where starting states are sampled from an off-policy buffer, thus allowing coverage of the whole task",
                "3 shows training curves for CMBPO compared to variants with a constant real-tomodel sample ratio \u03b1, and a rollout schedule with fixed horizons.",
                "To our surprise, we find that CMBPO in some cases exceeds the asymptotic performance of model-free optimizers, which we suspect may be caused by exploration through temporary constraint violation or lower-variance gradients due to only performing expected state-transitions in model-trajectories.\nb) Ablation on Sample Mixing and Adaptive Rollouts: Our ablation studies are aimed to better our understanding of the influence of sample mixing and adaptive rollouts on CMBPO\u2019s performance.",
                "We evaluate the efficacy of our algorithm, labeled Constrained Model-Based Policy Optimization (CMBPO), on several simulated high-dimensional robot locomotion tasks with continuous state- and action spaces."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "068660cdd795e20177a9ea7314e103e740f85e5c",
                "externalIds": {
                    "DBLP": "conf/iros/ZangerDZ21",
                    "ArXiv": "2104.06922",
                    "DOI": "10.1109/IROS51168.2021.9635984",
                    "CorpusId": 233231371
                },
                "corpusId": 233231371,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/068660cdd795e20177a9ea7314e103e740f85e5c",
                "title": "Safe Continuous Control with Constrained Model-Based Policy Optimization",
                "abstract": "The applicability of reinforcement learning (RL) algorithms in real-world domains often requires adherence to safety constraints, a need difficult to address given the asymptotic nature of the classic RL optimization objective. In contrast to the traditional RL objective, safe exploration considers the maximization of expected returns under safety constraints expressed in expected cost returns. We introduce a model-based safe exploration algorithm for constrained high-dimensional control to address the often prohibitively high sample complexity of model-free safe exploration algorithms. Further, we provide theoretical and empirical analyses regarding the implications of model-usage on constrained policy optimization problems and introduce a practical algorithm that accelerates policy search with model-generated data. The need for accurate estimates of a policy\u2019s constraint satisfaction is in conflict with accumulating model-errors. We address this issue by quantifying model-uncertainty as the expected Kullback-Leibler divergence between predictions of an ensemble of probabilistic dynamics models and constrain this error-measure, resulting in an adaptive resampling scheme and dynamically limited rollout horizons. We evaluate this approach on several simulated constrained robot locomotion tasks with high-dimensional action- and state-spaces. Our empirical studies find that our algorithm reaches model-free performances with a 10-20 fold reduction of training samples while maintaining approximate constraint satisfaction levels of model-free methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2075341905",
                        "name": "Moritz A. Zanger"
                    },
                    {
                        "authorId": "66438528",
                        "name": "Karam Daaboul"
                    },
                    {
                        "authorId": "32244386",
                        "name": "J. M. Z\u00f6llner"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In model-based reinforcement learning an unrolled one-step model would struggle with compounding errors (Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d4042b369e92c827a26dc62fdb047e00af332467",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-06159",
                    "MAG": "3156693571",
                    "ArXiv": "2104.06159",
                    "CorpusId": 233219421
                },
                "corpusId": 233219421,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d4042b369e92c827a26dc62fdb047e00af332467",
                "title": "Muesli: Combining Improvements in Policy Optimization",
                "abstract": "We propose a novel policy update that combines regularized policy optimization with model learning as an auxiliary loss. The update (henceforth Muesli) matches MuZero's state-of-the-art performance on Atari. Notably, Muesli does so without using deep search: it acts directly with a policy network and has computation speed comparable to model-free baselines. The Atari results are complemented by extensive ablations, and by additional results on continuous control and 9x9 Go.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39357484",
                        "name": "Matteo Hessel"
                    },
                    {
                        "authorId": "1841008",
                        "name": "Ivo Danihelka"
                    },
                    {
                        "authorId": "47963165",
                        "name": "Fabio Viola"
                    },
                    {
                        "authorId": "35099444",
                        "name": "A. Guez"
                    },
                    {
                        "authorId": "152380508",
                        "name": "Simon Schmitt"
                    },
                    {
                        "authorId": "2175946",
                        "name": "L. Sifre"
                    },
                    {
                        "authorId": "143947744",
                        "name": "T. Weber"
                    },
                    {
                        "authorId": "145824029",
                        "name": "David Silver"
                    },
                    {
                        "authorId": "7634925",
                        "name": "H. V. Hasselt"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "21a7c5e95aeddc2b208f425cd3fa0c56e9e60692",
                "externalIds": {
                    "DBLP": "conf/icml/BallLPR21",
                    "ArXiv": "2104.05632",
                    "CorpusId": 233210021
                },
                "corpusId": 233210021,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/21a7c5e95aeddc2b208f425cd3fa0c56e9e60692",
                "title": "Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment",
                "abstract": "Reinforcement learning from large-scale offline datasets provides us with the ability to learn policies without potentially unsafe or impractical exploration. Significant progress has been made in the past few years in dealing with the challenge of correcting for differing behavior between the data collection and learned policies. However, little attention has been paid to potentially changing dynamics when transferring a policy to the online setting, where performance can be up to 90% reduced for existing methods. In this paper we address this problem with Augmented World Models (AugWM). We augment a learned dynamics model with simple transformations that seek to capture potential changes in physical properties of the robot, leading to more robust policies. We not only train our policy in this new setting, but also provide it with the sampled augmentation as a context, allowing it to adapt to changes in the environment. At test time we learn the context in a self-supervised fashion by approximating the augmentation which corresponds to the new environment. We rigorously evaluate our approach on over 100 different changed dynamics settings, and show that this simple approach can significantly improve the zero-shot generalization of a recent state-of-the-art baseline, often achieving successful policies where the baseline fails.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2053179501",
                        "name": "Philip J. Ball"
                    },
                    {
                        "authorId": "2110752472",
                        "name": "Cong Lu"
                    },
                    {
                        "authorId": "1410302742",
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "authorId": "145029236",
                        "name": "S. Roberts"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026et al. 2018a,b)1, TD3 (Fujimoto, Hoof, and Meger 2018), ME-TRPO (Kurutach et al. 2018), MB-MPO(Clavera et al. 2018), PETS (Chua et al. 2018), MBPO (Janner et al. 2019)\n1We select the PyTorch implement of soft actor-critic in https://github.com/pranz24/pytorch-soft-actor-critic to evaluate the\u2026",
                "Compared with MBPO, ReW-PE-SAC is better on four environments and is slightly weaker in the tasks of HalfCheetah and Hopper.",
                "We reproduce results from (Wang et al. 2019; Janner et al. 2019) and additionally run MBPO on the tasks of Slimhumanoid and Swimmer as the according experimental results are absent.",
                "As shown in Table 1, ReW-PE-SAC achieves better performance compared with all other state-of-the-art algorithms except MBPO running with 200, 000 time-steps in all the environments.",
                "(Janner et al. 2019) replaces model-generated rollouts begin from the initial state distribution with short model-generated rollouts branched from the real data.",
                ", 2018), MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019).",
                "We reproduce results from (Wang et al., 2019; Janner et al., 2019) and additionally run MBPO on the tasks of Slimhumanoid and Swimmer as the according experimental results are absent.",
                "(Janner et al., 2019) replaces model-generated rollouts begin from the initial state distribution with short model-generated rollouts branched from the real data.",
                "We compare ReW-PE-SAC with state-of-the-art model-free and model-based RL methods, including SAC (Haarnoja et al. 2018a,b)1, TD3 (Fujimoto, Hoof, and Meger 2018), ME-TRPO (Kurutach et al. 2018), MB-MPO(Clavera et al. 2018), PETS (Chua et al. 2018), MBPO (Janner et al. 2019)\n1We select the PyTorch implement of soft actor-critic in https://github.com/pranz24/pytorch-soft-actor-critic to evaluate the performance."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5b201baf0b648781ef5c23d5f4344fb19504eb95",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-04174",
                    "ArXiv": "2104.04174",
                    "DOI": "10.1609/aaai.v35i9.16958",
                    "CorpusId": 233204546
                },
                "corpusId": 233204546,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5b201baf0b648781ef5c23d5f4344fb19504eb95",
                "title": "Learning to Reweight Imaginary Transitions for Model-Based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (RL) is more sample efficient than model-free RL by using imaginary trajectories generated by the learned dynamics model. When the model is inaccurate or biased, imaginary trajectories may be deleterious for training the action-value and policy functions. To alleviate such problem, this paper proposes to adaptively reweight the imaginary transitions, so as to reduce the negative effects of poorly generated trajectories. More specifically, we evaluate the effect of an imaginary transition by calculating the change of the loss computed on the real samples when we use the transition to train the action-value and policy functions. Based on this evaluation criterion, we construct the idea of reweighting each imaginary transition by a well-designed meta-gradient algorithm. Extensive experimental results demonstrate that our method outperforms state-of-the-art model-based and model-free RL algorithms on multiple tasks. Visualization of our changing weights further validates the necessity of utilizing reweight scheme.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2564033",
                        "name": "Wenzhen Huang"
                    },
                    {
                        "authorId": "2397961",
                        "name": "Qiyue Yin"
                    },
                    {
                        "authorId": "2086001",
                        "name": "Junge Zhang"
                    },
                    {
                        "authorId": "2887871",
                        "name": "Kaiqi Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Long horizon prediction is a commonly used task to test the quality of learned dynamical models (Sanchez-Gonzalez et al., 2018; Lutter et al., 2019; Greydanus et al., 2019; Miles et al., 2020; Janner et al., 2019).",
                "Feed-forward networks are commonly used model classes to parametrize these forward models (Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a240446d138816cbb0a36d9d1e68f20c25d2923b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-02844",
                    "ArXiv": "2104.02844",
                    "CorpusId": 233168688
                },
                "corpusId": 233168688,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a240446d138816cbb0a36d9d1e68f20c25d2923b",
                "title": "GEM: Group Enhanced Model for Learning Dynamical Control Systems",
                "abstract": "Learning the dynamics of a physical system wherein an autonomous agent operates is an important task. Often these systems present apparent geometric structures. For instance, the trajectories of a robotic manipulator can be broken down into a collection of its transitional and rotational motions, fully characterized by the corresponding Lie groups and Lie algebras. In this work, we take advantage of these structures to build effective dynamical models that are amenable to sample-based learning. We hypothesize that learning the dynamics on a Lie algebra vector space is more effective than learning a direct state transition model. To verify this hypothesis, we introduce the Group Enhanced Model (GEM). GEMs significantly outperform conventional transition models on tasks of long-term prediction, planning, and model-based reinforcement learning across a diverse suite of standard continuous-control environments, including Walker, Hopper, Reacher, Half-Cheetah, Inverted Pendulums, Ant, and Humanoid. Furthermore, plugging GEM into existing state of the art systems enhances their performance, which we demonstrate on the PETS system. This work sheds light on a connection between learning of dynamics and Lie group properties, which opens doors for new research directions and practical applications along this direction. Our code is publicly available at: https://tinyurl.com/GEMMBRL.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2067291211",
                        "name": "Philippe Hansen-Estruch"
                    },
                    {
                        "authorId": "3163480",
                        "name": "Wenling Shang"
                    },
                    {
                        "authorId": "34026610",
                        "name": "Lerrel Pinto"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    },
                    {
                        "authorId": "3266876",
                        "name": "Stas Tiomkin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "be3e601b21630aea8b133354a56fb55f881a1fed",
                "externalIds": {
                    "DBLP": "journals/ral/SunYDLZ21",
                    "DOI": "10.1109/LRA.2021.3061397",
                    "CorpusId": 231815587
                },
                "corpusId": 231815587,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/be3e601b21630aea8b133354a56fb55f881a1fed",
                "title": "Adversarial Inverse Reinforcement Learning With Self-Attention Dynamics Model",
                "abstract": "In many real-world applications where specifying a proper reward function is difficult, it is desirable to learn policies from expert demonstrations. Adversarial Inverse Reinforcement Learning (AIRL) is one of the most common approaches for learning from demonstrations. However, due to the stochastic policy, current computation graph of AIRL is no longer end-to-end differentiable like Generative Adversarial Networks (GANs), resulting in the need for high-variance gradient estimation methods and large sample size. In this work, we propose the Model-based Adversarial Inverse Reinforcement Learning (MAIRL), an end-to-end model-based policy optimization method with self-attention. By adopting the self-attention dynamics model to make the computation graph end-to-end differentiable, MAIRL has the low variance for policy optimization. We evaluate our approach thoroughly on various control tasks. The experimental results show that our approach not only learns near-optimal rewards and policies that match expert behavior but also outperforms previous inverse reinforcement learning algorithms in real robot experiments. Code is available at https://decisionforce.github.io/MAIRL/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2282025",
                        "name": "Jiankai Sun"
                    },
                    {
                        "authorId": "3469209",
                        "name": "Lantao Yu"
                    },
                    {
                        "authorId": "2052289733",
                        "name": "Pinqian Dong"
                    },
                    {
                        "authorId": "2075390730",
                        "name": "Bo Lu"
                    },
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our models are deep neural networks trained to maximize the log likelihood of the next state and reward given the current state and action, similar to models from successful model-based RL algorithms (Chua et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d86dfdbb8eab91cf23b81c541b4f741f88b7d756",
                "externalIds": {
                    "DBLP": "conf/iclr/Fu0NTw0YZCKPLP21",
                    "MAG": "3133218073",
                    "ArXiv": "2103.16596",
                    "CorpusId": 232428253
                },
                "corpusId": 232428253,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d86dfdbb8eab91cf23b81c541b4f741f88b7d756",
                "title": "Benchmarks for Deep Off-Policy Evaluation",
                "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both obtaining and selecting complex policies for decision making. The ability to perform evaluation offline is particularly important in many real-world domains, such as healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between works is difficult because there is currently a lack of a comprehensive and unified benchmark. Moreover, it is difficult to measure how far algorithms have progressed, due to the lack of challenging evaluation tasks. In order to address this gap, we propose a new benchmark for off-policy evaluation which includes tasks on a range of challenging, high-dimensional control problems, with wide selections of datasets and policies for performing policy selection. The goal of of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform a comprehensive evaluation of state-of-the-art algorithms, and we will provide open-source access to all data and code to foster future research in this area.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2550764",
                        "name": "Justin Fu"
                    },
                    {
                        "authorId": "144739074",
                        "name": "Mohammad Norouzi"
                    },
                    {
                        "authorId": "7624658",
                        "name": "Ofir Nachum"
                    },
                    {
                        "authorId": "145499435",
                        "name": "G. Tucker"
                    },
                    {
                        "authorId": "2117966548",
                        "name": "Ziyun Wang"
                    },
                    {
                        "authorId": "2050212830",
                        "name": "Alexander Novikov"
                    },
                    {
                        "authorId": "2111076891",
                        "name": "Mengjiao Yang"
                    },
                    {
                        "authorId": "2109123395",
                        "name": "Michael R. Zhang"
                    },
                    {
                        "authorId": "2275897",
                        "name": "Yutian Chen"
                    },
                    {
                        "authorId": "1488785534",
                        "name": "Aviral Kumar"
                    },
                    {
                        "authorId": "3316271",
                        "name": "Cosmin Paduraru"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "40470211",
                        "name": "T. Paine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[1], [2], [3], [4]), robotic performance remains decidedly sub-human in almost all scenarios.",
                "RNN\u2019s are worse than MLP\u2019s on Hard model rollouts, in spite of their superior performance on single-step predictions.",
                "6: We plot the performance of our optimized MLP networks on long-term prediction\nof position and orientation with 95% confidence intervals.",
                "5: The structure of our RNN predictors. \u03c6 is a recurrent unit (GRU), while \u03c6dec is an\nMLP decoder.",
                "To make a fair comparison with our RNN\u2019s of history-length h = 16, our MLP rollout experiments also start from the 16th time-step.",
                "The first, and most elementary, is to pick h = 1 and map xt to xt+1 with a simple multilayer perceptron (MLP) ([2], [4]).",
                "For each MLP and RNN architecture, we tried different target variables and sweep over different values of learning-rate, hidden-layer size, and weight-decay, centered around hand-tuned values.",
                "Each model is an MLP with input z\u0307t , two hidden layers of width 128, and output z\u0307t+1.",
                "[1], [2], [3], [4], [7]) follow the same fundamental approach: fitting a neural network approximation of the system dynamics (3) directly to data.",
                "While the history-length is 1 for MLPs, for RNNs we also tried different history-lengths.",
                "APPENDIX III LEARNING DETAILS\nOur MLPs consist of 4 hidden fully-connected layers with ReLU activations, plus a final linear layer.",
                "While MLP\u2019s and RNN\u2019s had similar training error and generalization error trends, the long-term prediction error was noticeably different (see Fig."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bfbd66fc944115ed6c8ca8755db3daec1b7b2621",
                "externalIds": {
                    "DBLP": "conf/iros/ParmarHP21",
                    "ArXiv": "2103.15406",
                    "DOI": "10.1109/IROS51168.2021.9636383",
                    "CorpusId": 225101723
                },
                "corpusId": 225101723,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bfbd66fc944115ed6c8ca8755db3daec1b7b2621",
                "title": "Fundamental Challenges in Deep Learning for Stiff Contact Dynamics",
                "abstract": "Frictional contact has been extensively studied as the core underlying behavior of legged locomotion and manipulation, and its nearly-discontinuous nature makes planning and control difficult even when an accurate model of the robot is available. Here, we present empirical evidence that learning an accurate model in the first place can be confounded by contact, as modern deep learning approaches are not designed to capture this non-smoothness. We isolate the effects of contact\u2019s non-smoothness by varying the mechanical stiffness of a compliant contact simulator. Even for a simple system, we find that stiffness alone dramatically degrades training processes, generalization, and data-efficiency. Our results raise serious questions about simulated testing environments which do not accurately reflect the stiffness of rigid robotic hardware. Significant additional investigation will be necessary to fully understand and mitigate these effects, and we suggest several avenues for future study.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Mihir Parmar"
                    },
                    {
                        "authorId": "145831729",
                        "name": "Mathew Halm"
                    },
                    {
                        "authorId": "3310324",
                        "name": "Michael Posa"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another specific model-based off-policy subclass recovers MBPO (Janner et al., 2019) which uses SAC (Haarnoja et al., 2019) internally, but SAC can be replaced by either DDPG (Lillicrap et al., 2016) or TD3 (Fujimoto et al., 2018), as all of those model-free off-policy algorithms are provided by\u2026",
                "Another specific model-based off-policy subclass recovers MBPO (Janner et al., 2019) which uses SAC (Haarnoja et al.",
                "Another specific model-based off-policy subclass recovers MBPO (Janner et al., 2019) which uses SAC (Haarnoja et al., 2019) internally, but SAC can be replaced by either DDPG (Lillicrap et al., 2016) or TD3 (Fujimoto et al., 2018), as all of those model-free off-policy algorithms are provided by TF-Agents.",
                "This enables e.g. to systematically compare Bellman\u2019s PETS, ME-TRPO, MBPO and TRPO implementations against PPO, SAC, DDPG and TD3 from TF-Agents."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "877286fa23fc3ea3ddc52e111871b9a5f3aa1d90",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-14407",
                    "ArXiv": "2103.14407",
                    "CorpusId": 232380390
                },
                "corpusId": 232380390,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/877286fa23fc3ea3ddc52e111871b9a5f3aa1d90",
                "title": "Bellman: A Toolbox for Model-Based Reinforcement Learning in TensorFlow",
                "abstract": "In the past decade, model-free reinforcement learning (RL) has provided solutions to challenging domains such as robotics. Model-based RL shows the prospect of being more sample-efficient than model-free methods in terms of agent-environment interactions, because the model enables to extrapolate to unseen situations. In the more recent past, model-based methods have shown superior results compared to model-free methods in some challenging domains with non-linear state transitions. At the same time, it has become apparent that RL is not market-ready yet and that many real-world applications are going to require model-based approaches, because model-free methods are too sample-inefficient and show poor performance in early stages of training. The latter is particularly important in industry, e.g. in production systems that directly impact a company's revenue. This demonstrates the necessity for a toolbox to push the boundaries for model-based RL. While there is a plethora of toolboxes for model-free RL, model-based RL has received little attention in terms of toolbox development. Bellman aims to fill this gap and introduces the first thoroughly designed and tested model-based RL toolbox using state-of-the-art software engineering practices. Our modular approach enables to combine a wide range of environment models with generic model-based agent classes that recover state-of-the-art algorithms. We also provide an experiment harness to compare both model-free and model-based agents in a systematic fashion w.r.t. user-defined evaluation metrics (e.g. cumulative reward). This paves the way for new research directions, e.g. investigating uncertainty-aware environment models that are not necessarily neural-network-based, or developing algorithms to solve industrially-motivated benchmarks that share characteristics with real-world problems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2060443962",
                        "name": "John Mcleod"
                    },
                    {
                        "authorId": "2540641",
                        "name": "Hrvoje Stoji\u0107"
                    },
                    {
                        "authorId": "37688607",
                        "name": "Vincent Adam"
                    },
                    {
                        "authorId": "2145184144",
                        "name": "Dongho Kim"
                    },
                    {
                        "authorId": "1399315491",
                        "name": "Jordi Grau-Moya"
                    },
                    {
                        "authorId": "2528631",
                        "name": "Peter Vrancx"
                    },
                    {
                        "authorId": "2505365",
                        "name": "Felix Leibfried"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One approach to prevent such problems is to not rely on the learned models entirely [1], [2], rather use them as close approximations of the system\u2019s behaviour, and at the same time design/learn a controller that is both robust to the model"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b639f1882db3154936110dbad8385d85491afefd",
                "externalIds": {
                    "DBLP": "conf/icra/SaxenaLK21",
                    "ArXiv": "2103.14256",
                    "DOI": "10.1109/ICRA48506.2021.9561083",
                    "CorpusId": 232380207
                },
                "corpusId": 232380207,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b639f1882db3154936110dbad8385d85491afefd",
                "title": "Learning Reactive and Predictive Differentiable Controllers for Switching Linear Dynamical Models",
                "abstract": "Humans leverage the dynamics of the environment and their own bodies to accomplish challenging tasks such as grasping an object while walking past it or pushing off a wall to turn a corner. Such tasks often involve switching dynamics as the robot makes and breaks contact. Learning these dynamics is a challenging problem and prone to model inaccuracies, especially near contact regions. In this work, we present a framework for learning composite dynamical behaviors from expert demonstrations. We learn a switching linear dynamical model with contacts encoded in switching conditions as a close approximation of our system dynamics. We then use discrete-time LQR as the differentiable policy class for data-efficient learning of control to develop a control strategy that operates over multiple dynamical modes and takes into account discontinuities due to contact. In addition to predicting interactions with the environment, our policy effectively reacts to inaccurate predictions such as unanticipated contacts. Through simulation and real world experiments, we demonstrate generalization of learned behaviors to different scenarios and robustness to model inaccuracies during execution.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1600548605",
                        "name": "Saumya Saxena"
                    },
                    {
                        "authorId": "1974255802",
                        "name": "A. LaGrassa"
                    },
                    {
                        "authorId": "1785853",
                        "name": "Oliver Kroemer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In all environments MoPAC and MBPO outperform both SAC, evincing the advantage of using model rollouts, and MBRL, that strongly suffers from poor exploration in the use of the learned model.",
                "MBPO [14] uses branched rollouts in an actor-critic setting to exploit the learned dynamics for modelbased policy optimization.",
                "known regions of the learned dynamics without exploring outwards to new, unrevealed states [12]\u2013[14].",
                "interactions per epoch, against the baselines SAC [25], MBPO [14], and MBRL [10].",
                "We compare the average return of MoPAC over 5 trials, consisting of 1, 000 true environment interactions per epoch, against the baselines SAC [25], MBPO [14], and MBRL [10].",
                "Here, SAC and MBPO are trained according to the hyperparameter settings provided by their respective works, while for MBRL we use the same settings as MoPAC, namely using horizons 5\u2212 15 with linear annealing for all tasks.",
                "The Ant-v2 and Hopper-v2 are more challenging tasks, as they require more interactions with the environment to learn\ntheir dynamics; in Hopper-v2 we observe a speedup in learning and convergence, while in Ant-v2 MoPAC learns faster in the first epochs and ends up with slightly better performance than MBPO.",
                "A batch size of 10, 000 is chosen for the model rollouts in both MoPAC and MBPO.",
                "[33] proposes an extension to MBPO, by performing model rollouts of specific horizons, while optimizing the policy objective with back-propagation through time.",
                "Penta-Valve Round-Valve\nFinger Gaiting\n(a)\n(b)\ntime\nMoPAC (ours)\nMBPO\nknown regions of the learned dynamics without exploring outwards to new, unrevealed states [12]\u2013[14].",
                "Notably, MoPAC learns faster than MBPO.",
                "We further underscore the efficacy of MoPAC by comparing our algorithm with SAC and MBPO on a Yale Openhand Model Q [41], [42] through two different manipulation tasks\u2013 valve rotation and finger gaiting.",
                "Model-based monotonic improvement, as proven by [14], can be achieved when learning the dynamics model together with the policy."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "23bb22710f7be585305bf01841b74ed167a706ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-13842",
                    "ArXiv": "2103.13842",
                    "DOI": "10.1109/ICRA48506.2021.9561298",
                    "CorpusId": 232352630
                },
                "corpusId": 232352630,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/23bb22710f7be585305bf01841b74ed167a706ce",
                "title": "Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning",
                "abstract": "Substantial advancements to model-based reinforcement learning algorithms have been impeded by the model-bias induced by the collected data, which generally hurts performance. Meanwhile, their inherent sample efficiency warrants utility for most robot applications, limiting potential damage to the robot and its environment during training. Inspired by information theoretic model predictive control and advances in deep reinforcement learning, we introduce Model Predictive Actor-Critic (MoPAC)\u2020, a hybrid model-based/model-free method that combines model predictive rollouts with policy optimization as to mitigate model bias. MoPAC leverages optimal trajectories to guide policy learning, but explores via its model-free method, allowing the algorithm to learn more expressive dynamics models. This combination guarantees optimal skill learning up to an approximation error and reduces necessary physical interaction with the environment, making it suitable for real-robot training. We provide extensive results showcasing how our proposed method generally outperforms current state-of-the-art and conclude by evaluating MoPAC for learning on a physical robotic hand performing valve rotation and finger gaiting\u2013a task that requires grasping, manipulation, and then regrasping of an object.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144687315",
                        "name": "A. S. Morgan"
                    },
                    {
                        "authorId": "2058392921",
                        "name": "Daljeet Nandha"
                    },
                    {
                        "authorId": "1989757",
                        "name": "G. Chalvatzaki"
                    },
                    {
                        "authorId": "1399348619",
                        "name": "Carlo D'Eramo"
                    },
                    {
                        "authorId": "1797110",
                        "name": "A. Dollar"
                    },
                    {
                        "authorId": "145197867",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e813e2a1c5b82b7e609ce0b9a7af90061373b542",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-14060",
                    "ArXiv": "2103.14060",
                    "DOI": "10.1016/j.ifacol.2021.08.321",
                    "CorpusId": 232380181
                },
                "corpusId": 232380181,
                "publicationVenue": {
                    "id": "af98f1eb-affb-4b55-b8ff-1964b29cf894",
                    "name": "IFAC-PapersOnLine",
                    "type": "journal",
                    "issn": "2405-8963",
                    "url": "https://www.journals.elsevier.com/ifac-papersonline/",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/24058963",
                        "https://www.journals.elsevier.com/ifac-papersonline"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e813e2a1c5b82b7e609ce0b9a7af90061373b542",
                "title": "A Meta-Reinforcement Learning Approach to Process Control",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2060208196",
                        "name": "Daniel G. McClement"
                    },
                    {
                        "authorId": "36835885",
                        "name": "Nathan P. Lawrence"
                    },
                    {
                        "authorId": "30734508",
                        "name": "Philip D. Loewen"
                    },
                    {
                        "authorId": "49979806",
                        "name": "M. Forbes"
                    },
                    {
                        "authorId": "69922276",
                        "name": "Johan U. Backstrom"
                    },
                    {
                        "authorId": "1766686",
                        "name": "R. B. Gopaluni"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Concurrently, some recent work has integrated policy networks\nwith deep models (Janner et al., 2019; Wang & Ba, 2019) in spirit of Dyna (Sutton, 1990).",
                "Recent techniques attempt to avoid compounding model error by restricting the number of model unrolling steps (Janner et al., 2019; Feinberg et al., 2018), but this creates a trade-off between planning performance and sample efficiency.",
                "173 Concurrently, some recent work has integrated policy networks with deep models [16, 37] in spirit 174 of Dyna [34].",
                "18 Recent techniques attempt to avoid compounding model error by restricting the number of model 19 unrolling steps [16, 11], but this creates a trade-off between planning performance and sample 20 efficiency."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4ac04e2267071df8c0d4503f1dec9a336f9fb108",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-12999",
                    "ArXiv": "2103.12999",
                    "CorpusId": 232335890
                },
                "corpusId": 232335890,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4ac04e2267071df8c0d4503f1dec9a336f9fb108",
                "title": "Discriminator Augmented Model-Based Reinforcement Learning",
                "abstract": "By planning through a learned dynamics model, model-based reinforcement learning (MBRL) offers the prospect of good performance with little environment interaction. However, it is common in practice for the learned model to be inaccurate, impairing planning and leading to poor performance. This paper aims to improve planning with an importance sampling framework that accounts and corrects for discrepancy between the true and learned dynamics. This framework also motivates an alternative objective for fitting the dynamics model: to minimize the variance of value estimation during planning. We derive and implement this objective, which encourages better prediction on trajectories with larger returns. We observe empirically that our approach improves the performance of current MBRL algorithms on two stochastic control problems, and provide a theoretical basis for our method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1411409755",
                        "name": "Behzad Haghgoo"
                    },
                    {
                        "authorId": "2064472884",
                        "name": "Allan Zhou"
                    },
                    {
                        "authorId": "50465276",
                        "name": "Archit Sharma"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026use of neural network function approximators, spurring new algorithmic developments in both model-free (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Gu et al., 2016b; 2017; Haarnoja et al., 2018) and model-based (Chua et al., 2018; Janner et al., 2019; Hafner et al., 2020a) RL."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "17051a6fb19dfa224bd4c4de5825ea15d765e723",
                "externalIds": {
                    "DBLP": "conf/icml/FurutaMKMLNG21",
                    "ArXiv": "2103.12726",
                    "CorpusId": 232320312
                },
                "corpusId": 232320312,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/17051a6fb19dfa224bd4c4de5825ea15d765e723",
                "title": "Policy Information Capacity: Information-Theoretic Measure for Task Complexity in Deep Reinforcement Learning",
                "abstract": "Progress in deep reinforcement learning (RL) research is largely enabled by benchmark task environments. However, analyzing the nature of those environments is often overlooked. In particular, we still do not have agreeable ways to measure the difficulty or solvability of a task, given that each has fundamentally different actions, observations, dynamics, rewards, and can be tackled with diverse RL algorithms. In this work, we propose policy information capacity (PIC) -- the mutual information between policy parameters and episodic return -- and policy-optimal information capacity (POIC) -- between policy parameters and episodic optimality -- as two environment-agnostic, algorithm-agnostic quantitative metrics for task difficulty. Evaluating our metrics across toy environments as well as continuous control benchmark tasks from OpenAI Gym and DeepMind Control Suite, we empirically demonstrate that these information-theoretic metrics have higher correlations with normalized task solvability scores than a variety of alternatives. Lastly, we show that these metrics can also be used for fast and compute-efficient optimizations of key design parameters such as reward shaping, policy architectures, and MDP properties for better solvability by RL algorithms without ever running full RL experiments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2052903664",
                        "name": "Hiroki Furuta"
                    },
                    {
                        "authorId": "145930468",
                        "name": "T. Matsushima"
                    },
                    {
                        "authorId": "2237799353",
                        "name": "Tadashi Kozuno"
                    },
                    {
                        "authorId": "49484314",
                        "name": "Y. Matsuo"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "7624658",
                        "name": "Ofir Nachum"
                    },
                    {
                        "authorId": "2046135",
                        "name": "S. Gu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6f8df2fa8e893e099c722e7c4a1cd12d9813d0a2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-08137",
                    "PubMedCentral": "9849740",
                    "ArXiv": "2103.08137",
                    "DOI": "10.3389/fnbot.2022.1045747",
                    "CorpusId": 232233289,
                    "PubMed": "36687204"
                },
                "corpusId": 232233289,
                "publicationVenue": {
                    "id": "de454aec-8c73-4737-bb1f-5231453ca8fa",
                    "name": "Frontiers in Neurorobotics",
                    "type": "journal",
                    "alternate_names": [
                        "Front Neurorobotics"
                    ],
                    "issn": "1662-5218",
                    "url": "https://www.frontiersin.org/journals/neurorobotics#articles",
                    "alternate_urls": [
                        "http://www.frontiersin.org/neurorobotics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6f8df2fa8e893e099c722e7c4a1cd12d9813d0a2",
                "title": "Cloth manipulation planning on basis of mesh representations with incomplete domain knowledge and voxel-to-mesh estimation",
                "abstract": "Cloth manipulation is common in both housework and manufacturing. However, robotic cloth manipulation remains challenging, especially for less controlled and open-goal settings. We consider the problem of open-goal planning for robotic cloth manipulation, with focus on the roles of cloth representation and epistemic uncertainty. Core of our system is a neural network trained as a forward model of cloth behaviour under manipulation, with planning performed through backpropagation. We introduce a neural network-based routine for estimating mesh representations from voxel input, and perform planning in mesh format internally. We address the problem of planning with incomplete domain knowledge by introducing an explicit epistemic uncertainty penalty, using prediction divergence between two instances of the forward model network as a proxy of epistemic uncertainty. This allows us to avoid plans with high epistemic uncertainty during planning. Finally, we introduce logic for handling restriction of grasp points to a discrete set of candidates, in order to accommodate graspability constraints imposed by robotic hardware. We evaluate the system\u2019s mesh estimation, prediction, and planning ability on simulated cloth for sequences of one to three manipulations. Comparative experiments confirm that planning on basis of estimated meshes improves accuracy compared to voxel-based planning, and that epistemic uncertainty avoidance improves performance under conditions of incomplete domain knowledge. Planning time cost is a few seconds. We additionally present qualitative results on robot hardware. Our results indicate that representation format and epistemic uncertainty are important factors to consider for open-goal cloth manipulation planning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34306524",
                        "name": "S. Arnold"
                    },
                    {
                        "authorId": "2082311506",
                        "name": "Daisuke Tanaka"
                    },
                    {
                        "authorId": "1747938",
                        "name": "Kimitoshi Yamazaki"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As for the cases with 1,000 dimensions (Figure 12), RAMCO (Random) shows performance similar to PETS, which are still better than the model-based methodMBMF andMBPO.",
                "MBPO uses its official implementation and PETS uses the implementation from (Vuong, 2020).",
                "\u2022 Model-Based Policy Optimization (MBPO) (Nagabandi et al., 2018): MBPO uses a probabilistic dynamics model to generate additional data to a replay buffer for training a SAC-based model-free agent.",
                "Recently proposed Dyna-style methods including Model-Based Acceleration (MBA) (Gu et al., 2016), Model-Based Value Expansion (MVE) (Feinberg et al., 2018), Model-Based Policy Optimization (MBPO) (Janner et al., 2019) and so on.",
                "As we can see, while the dimensionality of the environment increases, model-based methods (including PETS and MBMF, and hybrid methods like MBPO) begin to suffer from model bias and hence produce much worse results than model-free methods.",
                "However, RAMCO based on PPO-generated training data still shows a jump in its training curve after the warm-up phase, which makes it outperforms other state-of-theart model-free methods including PPO, DDPG and SAC and\nmodel-based methods including PETS, MBMF and MBPO.",
                "RAMCO (Random) shows performance similar to PETS, which are still better than the model-based method MBMF and hybrid method MBPO.",
                "The experiment of the MBPO method shows that it can obtain better sample efficiency than prior model-based methods and asymptotic performance of the state-of-the-art model-free algorithms.",
                "MBPO can be regarded as an SAC algorithm with a dynamics model added to it, and hence an evident advantage over SAC and other model-free methods.",
                "However, RAMCO based on PPO-generated training data still outperforms other state-of-the-art model-free methods including PPO, DDPG and SAC andmodel-based methods including PETS, MBMF and MBPO after the warm-up phase.",
                ", 2018), Model-Based Policy Optimization (MBPO) (Janner et al., 2019) and so on."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "99b1d307326bbd7e7581d82b3998f43327ac53fe",
                "externalIds": {
                    "PubMedCentral": "7990789",
                    "DBLP": "journals/firai/YuR21",
                    "DOI": "10.3389/frobt.2021.617839",
                    "CorpusId": 232172034,
                    "PubMed": "33778013"
                },
                "corpusId": 232172034,
                "publicationVenue": {
                    "id": "2ee61499-676f-46c2-afde-d4c0cb4393e6",
                    "name": "Frontiers in Robotics and AI",
                    "type": "journal",
                    "alternate_names": [
                        "Front Robot AI"
                    ],
                    "issn": "2296-9144",
                    "url": "https://www.frontiersin.org/journals/robotics-and-ai",
                    "alternate_urls": [
                        "http://www.frontiersin.org/Robotics_and_AI/archive",
                        "http://www.frontiersin.org/Robotics_and_AI/about",
                        "http://www.frontiersin.org/Robotics_and_AI"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/99b1d307326bbd7e7581d82b3998f43327ac53fe",
                "title": "Risk-Aware Model-Based Control",
                "abstract": "Model-Based Reinforcement Learning (MBRL) algorithms have been shown to have an advantage on data-efficiency, but often overshadowed by state-of-the-art model-free methods in performance, especially when facing high-dimensional and complex problems. In this work, a novel MBRL method is proposed, called Risk-Aware Model-Based Control (RAMCO). It combines uncertainty-aware deep dynamics models and the risk assessment technique Conditional Value at Risk (CVaR). This mechanism is appropriate for real-world application since it takes epistemic risk into consideration. In addition, we use a model-free solver to produce warm-up training data, and this setting improves the performance in low-dimensional environments and covers the shortage of MBRL\u2019s nature in the high-dimensional scenarios. In comparison with other state-of-the-art reinforcement learning algorithms, we show that it produces superior results on a walking robot model. We also evaluate the method with an Eidos environment, which is a novel experimental method with multi-dimensional randomly initialized deep neural networks to measure the performance of any reinforcement learning algorithm, and the advantages of RAMCO are highlighted.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116579171",
                        "name": "Chengbo Yu"
                    },
                    {
                        "authorId": "143944778",
                        "name": "A. Rosendo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[26]): \" # \u221e \u00d5 \ufffd \u2217 = argmax E\ufffd \ufffd \ufffd \ufffd (\ufffd\ufffd , \ufffd\ufffd ) (1)"
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "223d97f0fe491c66508369b7f096b36344c2e698",
                "externalIds": {
                    "ArXiv": "2103.06807",
                    "DBLP": "conf/chi/TodiBLO21",
                    "DOI": "10.1145/3411764.3445497",
                    "CorpusId": 232185198
                },
                "corpusId": 232185198,
                "publicationVenue": {
                    "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
                    "name": "International Conference on Human Factors in Computing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "CHI",
                        "Int Conf Hum Factor Comput Syst",
                        "Human Factors in Computing Systems",
                        "Conference on Human Interface",
                        "Conf Hum Interface",
                        "Hum Factor Comput Syst"
                    ],
                    "url": "http://www.acm.org/sigchi/"
                },
                "url": "https://www.semanticscholar.org/paper/223d97f0fe491c66508369b7f096b36344c2e698",
                "title": "Adapting User Interfaces with Model-based Reinforcement Learning",
                "abstract": "Adapting an interface requires taking into account both the positive and negative effects that changes may have on the user. A carelessly picked adaptation may impose high costs to the user \u2013 for example, due to surprise or relearning effort \u2013 or \u201ctrap\u201d the process to a suboptimal design immaturely. However, effects on users are hard to predict as they depend on factors that are latent and evolve over the course of interaction. We propose a novel approach for adaptive user interfaces that yields a conservative adaptation policy: It finds beneficial changes when there are such and avoids changes when there are none. Our model-based reinforcement learning method plans sequences of adaptations and consults predictive HCI models to estimate their effects. We present empirical and simulation results from the case of adaptive menus, showing that the method outperforms both a non-adaptive and a frequency-based policy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1817239",
                        "name": "Kashyap Todi"
                    },
                    {
                        "authorId": "1693915",
                        "name": "G. Bailly"
                    },
                    {
                        "authorId": "145800450",
                        "name": "Luis A. Leiva"
                    },
                    {
                        "authorId": "2663734",
                        "name": "Antti Oulasvirta"
                    }
                ]
            }
        },
        {
            "contexts": [
                "RL algorithm: Multi-Agent Model-Based Policy Optimization (MAMBPO), which is a multi-agent adaptation of the Model-Based Policy Optimization (MBPO) algorithm [2].",
                "The degree of the improvement varies per domain, as is also the case for the single-agent MBPO [2].",
                "data, similar to the original MBPO implementation [2].",
                "a) Model-Based Learning through MBPO: To learn a world model and generate experience to train the policy on, we adapt the single-agent MBPO algorithm [2] to be suitable for multi-agent domains.",
                "Such models have resulted in algorithms that are comparable to state-of-the-art model-free algorithms in terms of asymptotic performance, but can learn with up to an order of magnitude greater sample efficiency [2]."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b7cb2bb1c116efd825d391c6e17028f51770cac7",
                "externalIds": {
                    "DBLP": "conf/iros/WillemsenCC21",
                    "ArXiv": "2103.03662",
                    "DOI": "10.1109/IROS51168.2021.9635836",
                    "CorpusId": 232135269
                },
                "corpusId": 232135269,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b7cb2bb1c116efd825d391c6e17028f51770cac7",
                "title": "MAMBPO: Sample-efficient multi-robot reinforcement learning using learned world models",
                "abstract": "Multi-robot systems can benefit from reinforcement learning (RL) algorithms that learn behaviours in a small number of trials, a property known as sample efficiency. This research thus investigates the use of learned world models to improve sample efficiency. We present a novel multi-agent model-based RL algorithm: Multi-Agent Model-Based Policy Optimization (MAMBPO), utilizing the Centralized Learning for Decentralized Execution (CLDE) framework. CLDE algorithms allow a group of agents to act in a fully decentralized manner after training. This is a desirable property for many systems comprising of multiple robots. MAMBPO uses a learned world model to improve sample efficiency compared to model-free Multi-Agent Soft Actor-Critic (MASAC). We demonstrate this on two simulated multi-robot tasks, where MAMBPO achieves a similar performance to MASAC, but requires far fewer samples to do so. Through this, we take an important step towards making real-life learning for multi-robot systems possible.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2052022221",
                        "name": "Daniel Willemsen"
                    },
                    {
                        "authorId": "144571873",
                        "name": "M. Coppola"
                    },
                    {
                        "authorId": "145346217",
                        "name": "G. D. Croon"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "88f4c0b5a0d082e62305fb6f5750a2366f0222a2",
                "externalIds": {
                    "DBLP": "conf/aistats/VoloshinJY21",
                    "ArXiv": "2103.02084",
                    "CorpusId": 232104968
                },
                "corpusId": 232104968,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/88f4c0b5a0d082e62305fb6f5750a2366f0222a2",
                "title": "Minimax Model Learning",
                "abstract": "We present a novel off-policy loss function for learning a transition model in model-based reinforcement learning. Notably, our loss is derived from the off-policy policy evaluation objective with an emphasis on correcting distribution shift. Compared to previous model-based techniques, our approach allows for greater robustness under model misspecification or distribution shift induced by learning/evaluating policies that are distinct from the data-generating policy. We provide a theoretical analysis and show empirical improvements over existing model-based off-policy evaluation methods. We provide further analysis showing our loss can be used for off-policy optimization (OPO) and demonstrate its integration with more recent improvements in OPO.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "88658266",
                        "name": "Cameron Voloshin"
                    },
                    {
                        "authorId": "48272707",
                        "name": "Nan Jiang"
                    },
                    {
                        "authorId": "2046181102",
                        "name": "Yisong Yue"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, model-based methods encounter with model bias caused by the difference between the trained model and real environment [9], especially when the environment has high-dimensional states and complex dynamics.",
                "However, model-based methods are limited by model bias, as previous work [9] addressed."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "da2598db8d4c23ab32bc2305de0da4e51173ec97",
                "externalIds": {
                    "ArXiv": "2102.12962",
                    "DBLP": "journals/corr/abs-2102-12962",
                    "CorpusId": 232046007
                },
                "corpusId": 232046007,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/da2598db8d4c23ab32bc2305de0da4e51173ec97",
                "title": "Bias-reduced multi-step hindsight experience replay",
                "abstract": "Multi-goal reinforcement learning is widely applied in planning and robot manipulation. Two main challenges in multi-goal reinforcement learning are sparse rewards and sample inefficiency. Hindsight Experience Replay (HER) aims to tackle the two challenges via goal relabeling. However, HER-related works still need millions of samples and a huge computation. In this paper, we propose Multi-step Hindsight Experience Replay (MHER), incorporating multi-step relabeled returns based on $n$-step relabeling to improve sample efficiency. Despite the advantages of $n$-step relabeling, we theoretically and experimentally prove the off-policy $n$-step bias introduced by $n$-step relabeling may lead to poor performance in many environments. To address the above issue, two bias-reduced MHER algorithms, MHER($\\lambda$) and Model-based MHER (MMHER) are presented. MHER($\\lambda$) exploits the $\\lambda$ return while MMHER benefits from model-based value expansions. Experimental results on numerous multi-goal robotic tasks show that our solutions can successfully alleviate off-policy $n$-step bias and achieve significantly higher sample efficiency than HER and Curriculum-guided HER with little additional computation beyond HER.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145094495",
                        "name": "Rui Yang"
                    },
                    {
                        "authorId": "2008151131",
                        "name": "Jiafei Lyu"
                    },
                    {
                        "authorId": "2116465145",
                        "name": "Yu Yang"
                    },
                    {
                        "authorId": "30411824",
                        "name": "Jiangpeng Yan"
                    },
                    {
                        "authorId": "2072689111",
                        "name": "Feng Luo"
                    },
                    {
                        "authorId": "2061549073",
                        "name": "Dijun Luo"
                    },
                    {
                        "authorId": "2117007545",
                        "name": "Lanqing Li"
                    },
                    {
                        "authorId": "2127382771",
                        "name": "Xiu Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also compare against model-based offline RL algorithms including MOPO (Yu et al. 2020) that follows MBPO (Janner et al. 2019) with additional reward penalties.",
                "MOPO (Yu et al. 2020) extends MBPO (Janner et al. 2019) with an additional reward penalty on generated transitions with large variance from the learned dynamic model.",
                "2020) that follows MBPO (Janner et al. 2019) with additional reward penalties.",
                "2020) extends MBPO (Janner et al. 2019) with an additional reward penalty on generated transitions with large variance from the learned dynamic model."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4e90ab5310055d1cc9a951a476ed241f97fe76b0",
                "externalIds": {
                    "DBLP": "conf/aaai/ZhanXZZY022",
                    "ArXiv": "2102.11492",
                    "DOI": "10.1609/aaai.v36i4.20393",
                    "CorpusId": 232014499
                },
                "corpusId": 232014499,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4e90ab5310055d1cc9a951a476ed241f97fe76b0",
                "title": "DeepThermal: Combustion Optimization for Thermal Power Generating Units Using Offline Reinforcement Learning",
                "abstract": "Optimizing the combustion efficiency of a thermal power generating unit (TPGU) is a highly challenging and critical task in the energy industry. We develop a new data-driven AI system, namely DeepThermal, to optimize the combustion control strategy for TPGUs. At its core, is a new model-based offline reinforcement learning (RL) framework, called MORE, which leverages historical operational data of a TGPU to solve a highly complex constrained Markov decision process problem via purely offline training. In DeepThermal, we first learn a data-driven combustion process simulator from the offline dataset. The RL agent of MORE is then trained by combining real historical data as well as carefully filtered and processed simulation data through a novel restrictive exploration scheme. DeepThermal has been successfully deployed in four large coal-fired thermal power plants in China. Real-world experiments show that DeepThermal effectively improves the combustion efficiency of TPGUs. We also report the superior performance of MORE by comparing with the state-of-the-art algorithms on the standard offline RL benchmarks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3415564",
                        "name": "Xianyuan Zhan"
                    },
                    {
                        "authorId": "49507262",
                        "name": "Haoran Xu"
                    },
                    {
                        "authorId": "49890778",
                        "name": "Yueying Zhang"
                    },
                    {
                        "authorId": "13772718",
                        "name": "Yusen Huo"
                    },
                    {
                        "authorId": "8362374",
                        "name": "Xiangyu Zhu"
                    },
                    {
                        "authorId": "1919918492",
                        "name": "Honglei Yin"
                    },
                    {
                        "authorId": "2149515044",
                        "name": "Yu Zheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, Janner et al. (2019) used short model-generated rollouts branched from real data for both value and policy learning, matching the asymptotic performance of the best model-free methods [14].",
                "(2019) used short model-generated rollouts branched from real data for both value and policy learning, matching the asymptotic performance of the best model-free methods [14]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "400698da6f534d05d8c2b92e25ad143467255893",
                "externalIds": {
                    "ArXiv": "2102.11513",
                    "DBLP": "journals/corr/abs-2102-11513",
                    "CorpusId": 232013383
                },
                "corpusId": 232013383,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/400698da6f534d05d8c2b92e25ad143467255893",
                "title": "Mixed Policy Gradient",
                "abstract": "Reinforcement learning (RL) has great potential in sequential decision-making. At present, the mainstream RL algorithms are data-driven, relying on millions of iterations and a large number of empirical data to learn a policy. Although data-driven RL may have excellent asymptotic performance, it usually yields slow convergence speed. As a comparison, model-driven RL employs a differentiable transition model to improve convergence speed, in which the policy gradient (PG) is calculated by using the backpropagation through time (BPTT) technique. However, such methods suffer from numerical instability, model error sensitivity and low computing efficiency, which may lead to poor policies. In this paper, a mixed policy gradient (MPG) method is proposed, which uses both empirical data and the transition model to construct the PG, so as to accelerate the convergence speed without losing the optimality guarantee. MPG contains two types of PG: 1) data-driven PG, which is obtained by directly calculating the derivative of the learned Q-value function with respect to actions, and 2) model-driven PG, which is calculated using BPTT based on the model-predictive return. We unify them by revealing the correlation between the upper bound of the unified PG error and the predictive horizon, where the data-driven PG is regraded as 0-step model-predictive return. Relying on that, MPG employs a rule-based method to adaptively adjust the weights of data-driven and model-driven PGs. In particular, to get a more accurate PG, the weight of the data-driven PG is designed to grow along the learning process while the other to decrease. Besides, an asynchronous learning framework is proposed to reduce the wall-clock time needed for each update iteration. Simulation results show that the MPG method achieves the best asymptotic performance and convergence speed compared with other baseline algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50463545",
                        "name": "Yang Guan"
                    },
                    {
                        "authorId": "23637596",
                        "name": "Jingliang Duan"
                    },
                    {
                        "authorId": "2023891",
                        "name": "S. Li"
                    },
                    {
                        "authorId": "2046898622",
                        "name": "Jie Li"
                    },
                    {
                        "authorId": "1391201846",
                        "name": "Jianyu Chen"
                    },
                    {
                        "authorId": "117675219",
                        "name": "B. Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Related research works are Chua et al. (2018); Janner et al. (2019); Luo et al. (2019); Munos & Szepesva\u0301ri (2008)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3806390b706d89f54bc7f5474b0f2c0347acb22d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-11448",
                    "ArXiv": "2102.11448",
                    "CorpusId": 232014566
                },
                "corpusId": 232014566,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3806390b706d89f54bc7f5474b0f2c0347acb22d",
                "title": "MUSBO: Model-based Uncertainty Regularized and Sample Efficient Batch Optimization for Deployment Constrained Reinforcement Learning",
                "abstract": "In many contemporary applications such as healthcare, finance, robotics, and recommendation systems, continuous deployment of new policies for data collection and online learning is either cost ineffective or impractical. We consider a setting that lies between pure offline reinforcement learning (RL) and pure online RL called deployment constrained RL in which the number of policy deployments for data sampling is limited. To solve this challenging task, we propose a new algorithmic learning framework called Model-based Uncertainty regularized and Sample Efficient Batch Optimization (MUSBO). Our framework discovers novel and high quality samples for each deployment to enable efficient data collection. During each offline training session, we bootstrap the policy update by quantifying the amount of uncertainty within our collected data. In the high support region (low uncertainty), we encourage our policy by taking an aggressive update. In the low support region (high uncertainty) when the policy bootstraps into the out-of-distribution region, we downweight it by our estimated uncertainty quantification. Experimental results show that MUSBO achieves state-of-the-art performance in the deployment constrained RL setting.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "114301476",
                        "name": "DiJia Su"
                    },
                    {
                        "authorId": "2421201",
                        "name": "J. Lee"
                    },
                    {
                        "authorId": "2092062",
                        "name": "J. Mulvey"
                    },
                    {
                        "authorId": "145967056",
                        "name": "H. Poor"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020] and MBPO [Janner et al., 2019], as well as the standard baselines of SAC [Haarnoja et al.",
                "We compared GELATO to contemporary model-based offline RL approaches; namely, MOPO [Yu et al., 2020] and MBPO [Janner et al., 2019], as well as the standard baselines of SAC [Haarnoja et al., 2018] and imitation (behavioral cloning, Bain and Sammut [1995], Ross et al. [2011])."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6dbff5c7a1992b039c50ac3e0170436808bbb0a0",
                "externalIds": {
                    "DBLP": "conf/nips/TennenholtzM22",
                    "ArXiv": "2102.11327",
                    "CorpusId": 253224447
                },
                "corpusId": 253224447,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6dbff5c7a1992b039c50ac3e0170436808bbb0a0",
                "title": "Uncertainty Estimation Using Riemannian Model Dynamics for Offline Reinforcement Learning",
                "abstract": "Model-based offline reinforcement learning approaches generally rely on bounds of model error. Estimating these bounds is usually achieved through uncertainty estimation methods. In this work, we combine parametric and nonparametric methods for uncertainty estimation through a novel latent space based metric. In particular, we build upon recent advances in Riemannian geometry of generative models to construct a pullback metric of an encoder-decoder based forward model. Our proposed metric measures both the quality of out-of-distribution samples as well as the discrepancy of examples in the data. We leverage our method for uncertainty estimation in a pessimistic model-based framework, showing a significant improvement upon contemporary model-based offline approaches on continuous control and autonomous driving benchmarks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "29978064",
                        "name": "Guy Tennenholtz"
                    },
                    {
                        "authorId": "1712535",
                        "name": "Shie Mannor"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ed RL has seen several advances (Sutton,1990;Li and Todorov,2004;Deisenroth and Rasmussen,2011) including ones based on deep learning (e.g.,Lampe and Riedmiller(2014);Gu et al.(2016);Luo et al.(2018);Janner et al. (2019);Lowrey et al.(2019);Wang et al.(2019)). Given MobILE\u2019s modularity, these advances in model-based RL can be used to design improved algorithms for the ILFO problem. MobILE bears parallels to provably "
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c50876bdaf75822a38b5a2512f577b53e9347b13",
                "externalIds": {
                    "MAG": "3129580072",
                    "DBLP": "journals/corr/abs-2102-10769",
                    "CorpusId": 231986522
                },
                "corpusId": 231986522,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c50876bdaf75822a38b5a2512f577b53e9347b13",
                "title": "Optimism is All You Need: Model-Based Imitation Learning From Observation Alone",
                "abstract": "This paper studies Imitation Learning from Observations alone (ILFO) where the learner is presented with expert demonstrations that only consist of states encountered by an expert (without access to actions taken by the expert). We present a provably efficient model-based framework MobILE to solve the ILFO problem. MobILE involves carefully trading off exploration against imitation - this is achieved by integrating the idea of optimism in the face of uncertainty into the distribution matching imitation learning (IL) framework. We provide a unified analysis for MobILE, and demonstrate that MobILE enjoys strong performance guarantees for classes of MDP dynamics that satisfy certain well studied notions of complexity. We also show that the ILFO problem is strictly harder than the standard IL problem by reducing ILFO to a multi-armed bandit problem indicating that exploration is necessary for ILFO. We complement these theoretical results with experimental simulations on benchmark OpenAI Gym tasks that indicate the efficacy of MobILE.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1891978",
                        "name": "Rahul Kidambi"
                    },
                    {
                        "authorId": "80936017",
                        "name": "Jonathan D. Chang"
                    },
                    {
                        "authorId": "144426657",
                        "name": "Wen Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[24] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3f8e2cdd7a73ea0193f43ee3b69a717baded5310",
                "externalIds": {
                    "ArXiv": "2102.10769",
                    "DBLP": "conf/nips/KidambiCS21",
                    "CorpusId": 235436359
                },
                "corpusId": 235436359,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3f8e2cdd7a73ea0193f43ee3b69a717baded5310",
                "title": "MobILE: Model-Based Imitation Learning From Observation Alone",
                "abstract": "This paper studies Imitation Learning from Observations alone (ILFO) where the learner is presented with expert demonstrations that consist only of states visited by an expert (without access to actions taken by the expert). We present a provably efficient model-based framework MobILE to solve the ILFO problem. MobILE involves carefully trading off strategic exploration against imitation - this is achieved by integrating the idea of optimism in the face of uncertainty into the distribution matching imitation learning (IL) framework. We provide a unified analysis for MobILE, and demonstrate that MobILE enjoys strong performance guarantees for classes of MDP dynamics that satisfy certain well studied notions of structural complexity. We also show that the ILFO problem is strictly harder than the standard IL problem by presenting an exponential sample complexity separation between IL and ILFO. We complement these theoretical results with experimental simulations on benchmark OpenAI Gym tasks that indicate the efficacy of MobILE. Code for implementing the MobILE framework is available at https://github.com/rahulkidambi/MobILE-NeurIPS2021.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1891978",
                        "name": "Rahul Kidambi"
                    },
                    {
                        "authorId": "80936017",
                        "name": "Jonathan D. Chang"
                    },
                    {
                        "authorId": "144426657",
                        "name": "Wen Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare the difference in performance to SACSVG when the horizon length is varied (see MBPO environments in Table 1) and then compare the performance of our method against multiple model based methods including PETS (Chua et al., 2018), POPLIN (Wang & Ba, 2019), METRPO (Kurutach et al., 2018), and the model free SAC (Haarnoja et al., 2018) algorithm (see POPLIN environments in Table 1).",
                "\u20262017; Chua et al., 2018; Nagabandi et al., 2018), 2) to improve estimates of the Q value by rolling out the model for a small number of steps (Feinberg et al., 2018; Amos et al., 2020) and 3) to provide synthetic data samples for a model-free learner (Janner et al., 2019; Kurutach et al., 2018).",
                "On the other hand, the MBPO based environments refer to the ones used by the paper (Janner et al., 2019) and largely correspond to the \u2018-v2\u2019 versions from OpenAI Gym.",
                "Invariant MBRL performance on four MuJoCo based domains from POPLIN (Wang & Ba, 2019) (left) and five MuJoCo based domains from MBPO (Janner et al., 2019) (right).",
                ", 2020) and 3) to provide synthetic data samples for a model-free learner (Janner et al., 2019; Kurutach et al., 2018)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d0f1750e3a77e51b6159b8d99b2d2de7039de124",
                "externalIds": {
                    "ArXiv": "2102.09850",
                    "DBLP": "journals/corr/abs-2102-09850",
                    "CorpusId": 231979321
                },
                "corpusId": 231979321,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d0f1750e3a77e51b6159b8d99b2d2de7039de124",
                "title": "Model-Invariant State Abstractions for Model-Based Reinforcement Learning",
                "abstract": "Accuracy and generalization of dynamics models is key to the success of model-based reinforcement learning (MBRL). As the complexity of tasks increases, so does the sample inefficiency of learning accurate dynamics models. However, many complex tasks also exhibit sparsity in the dynamics, i.e., actions have only a local effect on the system dynamics. In this paper, we exploit this property with a causal invariance perspective in the single-task setting, introducing a new type of state abstraction called \\textit{model-invariance}. Unlike previous forms of state abstractions, a model-invariance state abstraction leverages causal sparsity over state variables. This allows for compositional generalization to unseen states, something that non-factored forms of state abstractions cannot do. We prove that an optimal policy can be learned over this model-invariance state abstraction and show improved generalization in a simple toy domain. Next, we propose a practical method to approximately learn a model-invariant representation for complex domains and validate our approach by showing improved modelling performance over standard maximum likelihood approaches on challenging tasks, such as the MuJoCo-based Humanoid. Finally, within the MBRL setting we show strong performance gains with respect to sample efficiency across a host of other continuous control tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "119439163",
                        "name": "Manan Tomar"
                    },
                    {
                        "authorId": "2111672235",
                        "name": "Amy Zhang"
                    },
                    {
                        "authorId": "35159852",
                        "name": "R. Calandra"
                    },
                    {
                        "authorId": "39286677",
                        "name": "Matthew E. Taylor"
                    },
                    {
                        "authorId": "145134886",
                        "name": "Joelle Pineau"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While any RL or planning algorithm can be used to learn the optimal policy for M\u0302, we focus specifically on MBPO [20, 57] which was used in MOPO.",
                "Subsequently, it employs an actor-critic method where the value function is learned using both the offline dataset as well as synthetically generated data from the model, similar to Dyna [57] and a number of recent methods [20, 67, 7, 48].",
                "In high-dimensional image-based domains, which we use to answer question (3), we compare to LOMPO [48], which is a latent space offline model-based RL method that handles image inputs, latent space MBPO (denoted LMBPO), similar to Janner et al. [20] which uses the model to generate additional synthetic data, the fully offline version of SLAC [32] (denoted SLAC-off), which only uses a variational model for state representation purposes, and CQL from image inputs.",
                "Specifically, at each iteration, MBPO performs k-step rollouts using T\u0302 starting from state s \u2208 D with a particular rollout policy \u00b5(a|s), adds the model-generated data to Dmodel, and optimizes the policy with a batch of data sampled from D \u222aDmodel where each datapoint in the batch is drawn from D with probability f \u2208 [0, 1] and Dmodel with probability 1\u2212 f .",
                "[20] which uses the model to generate additional synthetic data, the fully offline version of SLAC [32] (denoted SLAC-off), which only uses a variational model for state representation purposes, and CQL from image inputs.",
                "To highlight the distinction between COMBO and a na\u00efve combination of CQL and MBPO, we perform such a comparison in Table 8 in Appendix C.",
                "MBPO follows the standard structure of actor-critic algorithms, but in each iteration uses an augmented dataset D \u222aDmodel for policy evaluation."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "245682e8b3fa76f4a3e2991b5497577af95cbb3f",
                "externalIds": {
                    "DBLP": "conf/nips/YuKRRLF21",
                    "ArXiv": "2102.08363",
                    "CorpusId": 231934209
                },
                "corpusId": 231934209,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/245682e8b3fa76f4a3e2991b5497577af95cbb3f",
                "title": "COMBO: Conservative Offline Model-Based Policy Optimization",
                "abstract": "Model-based algorithms, which learn a dynamics model from logged experience and perform some sort of pessimistic planning under the learned model, have emerged as a promising paradigm for offline reinforcement learning (offline RL). However, practical variants of such model-based algorithms rely on explicit uncertainty quantification for incorporating pessimism. Uncertainty estimation with complex models, such as deep neural networks, can be difficult and unreliable. We overcome this limitation by developing a new model-based offline RL algorithm, COMBO, that regularizes the value function on out-of-support state-action tuples generated via rollouts under the learned model. This results in a conservative estimate of the value function for out-of-support state-action tuples, without requiring explicit uncertainty estimation. We theoretically show that our method optimizes a lower bound on the true policy value, that this bound is tighter than that of prior methods, and our approach satisfies a policy improvement guarantee in the offline setting. Through experiments, we find that COMBO consistently performs as well or better as compared to prior offline model-free and model-based methods on widely studied offline RL benchmarks, including image-based tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10909315",
                        "name": "Tianhe Yu"
                    },
                    {
                        "authorId": "1488785534",
                        "name": "Aviral Kumar"
                    },
                    {
                        "authorId": "102801230",
                        "name": "Rafael Rafailov"
                    },
                    {
                        "authorId": "19275599",
                        "name": "A. Rajeswaran"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e302ce7c086a3a92bc3111a6c8dd8f5f178c0043",
                "externalIds": {
                    "DBLP": "conf/isgt/GaoY21",
                    "DOI": "10.1109/ISGT49243.2021.9372283",
                    "CorpusId": 227068326
                },
                "corpusId": 227068326,
                "publicationVenue": {
                    "id": "eed1ae5e-6152-44eb-8c1c-eda80eb227dc",
                    "name": "IEEE PES Innovative Smart Grid Technologies Conference",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE PE Innov Smart Grid Technol Conf",
                        "ISGT"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e302ce7c086a3a92bc3111a6c8dd8f5f178c0043",
                "title": "Deep Reinforcement Learning in Power Distribution Systems: Overview, Challenges, and Opportunities",
                "abstract": "To facilitate the integration of distributed energy resources and improve existing operational strategies, power distribution systems have seen a rapid proliferation of deep reinforcement learning (DRL) based applications. DRL approach is well suited for dynamic, complex, and uncertain operational environments such as power distribution systems. This paper reviews the rapidly growing body of literature that develops applications of reinforcement learning in power distribution systems. These applications include active grid management, energy management system, retail electricity market, and demand response. This paper also summarizes the challenges of deploying DRL based solutions in distribution systems such as safety, robustness, interpretability, and sample efficiency. Finally, the research opportunities that can be pursued to address the challenges are provided.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "6959558",
                        "name": "Yuanqi Gao"
                    },
                    {
                        "authorId": "2159070",
                        "name": "N. Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, one can use the transition model alongside a reward model to generate offline data to improve value function learning (Sutton, 1991; Janner et al., 2019).",
                "These approaches fall into two camps: using models to extract a policy in a Dynastyle approach (Sutton, 1991; Janner et al., 2019; Sutton et al., 2008; Yao et al., 2009; Kaiser et al., 2020), or incorporating the model in a planning loop, i.",
                "Offline model-based reinforcement learning Modelbased methods have shown promise by facilitating better generalization (Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "848d10bd56a9af0010f8444f81944b2e95a8b215",
                "externalIds": {
                    "DBLP": "conf/icml/PRM21",
                    "ArXiv": "2102.07456",
                    "CorpusId": 231925196
                },
                "corpusId": 231925196,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/848d10bd56a9af0010f8444f81944b2e95a8b215",
                "title": "Neuro-algorithmic Policies enable Fast Combinatorial Generalization",
                "abstract": "Although model-based and model-free approaches to learning the control of systems have achieved impressive results on standard benchmarks, generalization to task variations is still lacking. Recent results suggest that generalization for standard architectures improves only after obtaining exhaustive amounts of data. We give evidence that generalization capabilities are in many cases bottlenecked by the inability to generalize on the combinatorial aspects of the problem. Furthermore, we show that for a certain subclass of the MDP framework, this can be alleviated by neuro-algorithmic architectures. Many control problems require long-term planning that is hard to solve generically with neural networks alone. We introduce a neuro-algorithmic policy architecture consisting of a neural network and an embedded time-dependent shortest path solver. These policies can be trained end-to-end by blackbox differentiation. We show that this type of architecture generalizes well to unseen variations in the environment already after seeing a few examples.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51335057",
                        "name": "Marin Vlastelica"
                    },
                    {
                        "authorId": "2753055",
                        "name": "Michal Rolinek"
                    },
                    {
                        "authorId": "144247521",
                        "name": "G. Martius"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In model-based RL methods (Wang et al., 2019; Schrittwieser et al., 2020; Janner et al., 2019; Wang & Ba, 2019; Kaiser et al., 2019; Luo et al., 2018; Deisenroth & Rasmussen, 2011), the transition dynamics or simulator is learnt and subsequently utilized for policy learning.",
                "It has been exhibited (Janner et al., 2019; Yu et al., 2020; Levine et al., 2020) that certain model-based methods that were originally targeted for the online setting can potentially still deliver reasonable performance with offline data and minimal algorithmic change.",
                "Further, there has been some work showing the success of online model-based RL approaches with offline data, with minimal change in the algorithm (Janner et al., 2019; Yu et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c84149dfda2c78cc550ba067669e197377ed991d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-06961",
                    "ArXiv": "2102.06961",
                    "CorpusId": 231924971
                },
                "corpusId": 231924971,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c84149dfda2c78cc550ba067669e197377ed991d",
                "title": "PerSim: Data-Efficient Offline Reinforcement Learning with Heterogeneous Agents via Personalized Simulators",
                "abstract": "We consider offline reinforcement learning (RL) with heterogeneous agents under severe data scarcity, i.e., we only observe a single historical trajectory for every agent under an unknown, potentially sub-optimal policy. We find that the performance of state-of-the-art offline and model-based RL methods degrade significantly given such limited data availability, even for commonly perceived\"solved\"benchmark settings such as\"MountainCar\"and\"CartPole\". To address this challenge, we propose PerSim, a model-based offline RL approach which first learns a personalized simulator for each agent by collectively using the historical trajectories across all agents, prior to learning a policy. We do so by positing that the transition dynamics across agents can be represented as a latent function of latent factors associated with agents, states, and actions; subsequently, we theoretically establish that this function is well-approximated by a\"low-rank\"decomposition of separable agent, state, and action latent functions. This representation suggests a simple, regularized neural network architecture to effectively learn the transition dynamics per agent, even with scarce, offline data. We perform extensive experiments across several benchmark environments and RL methods. The consistent improvement of our approach, measured in terms of both state dynamics prediction and eventual reward, confirms the efficacy of our framework in leveraging limited historical data to simultaneously learn personalized policies across agents.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49203555",
                        "name": "A. Agarwal"
                    },
                    {
                        "authorId": "29855910",
                        "name": "Abdullah Alomar"
                    },
                    {
                        "authorId": "2050161761",
                        "name": "Varkey Alumootil"
                    },
                    {
                        "authorId": "145081804",
                        "name": "D. Shah"
                    },
                    {
                        "authorId": "3376895",
                        "name": "Dennis Shen"
                    },
                    {
                        "authorId": "2136390627",
                        "name": "Zhi Xu"
                    },
                    {
                        "authorId": "2109421881",
                        "name": "Cindy Yang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ec05bd6725ac6a5217021881cac8553581b3e313",
                "externalIds": {
                    "ArXiv": "2102.04881",
                    "DBLP": "journals/corr/abs-2102-04881",
                    "CorpusId": 231855257
                },
                "corpusId": 231855257,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ec05bd6725ac6a5217021881cac8553581b3e313",
                "title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency",
                "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the number of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2064453117",
                        "name": "Florian E. Dorner"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2a51f798cc61947eefb0f754f409dbd78cb69cda",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-04764",
                    "ArXiv": "2102.04764",
                    "CorpusId": 231855323
                },
                "corpusId": 231855323,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2a51f798cc61947eefb0f754f409dbd78cb69cda",
                "title": "Continuous-Time Model-Based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (MBRL) approaches rely on discrete-time state transition models whereas physical systems and the vast majority of control tasks operate in continuous-time. To avoid time-discretization approximation of the underlying process, we propose a continuous-time MBRL framework based on a novel actor-critic method. Our approach also infers the unknown state evolution differentials with Bayesian neural ordinary differential equations (ODE) to account for epistemic uncertainty. We implement and test our method on a new ODE-RL suite that explicitly solves continuous-time control systems. Our experiments illustrate that the model is robust against irregular and noisy data, is sample-efficient, and can solve control problems which pose challenges to discrete-time MBRL methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9112269",
                        "name": "\u00c7agatay Yildiz"
                    },
                    {
                        "authorId": "34066178",
                        "name": "Markus Heinonen"
                    },
                    {
                        "authorId": "49121467",
                        "name": "H. L\u00e4hdesm\u00e4ki"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2016; 2015; 2017; Ge, Ma, 2020; Lee et al., 2016)). In RL, local maxima can often be global as well for many cases (Agarwal et al., 2020b).2 Zero-order optimization or policy gradient algorithms can converge to local maxima and become natural potential competitors. They are widely believed to be less sampleefficient than the model-based approach because the latter can leverage the extrapolation power of the parameterized models. Theoretically, our formulation aims to characterize this phenomenon with results showing that the model-based approach\u2019s sample complexity mostly depends (polynomially) on the complexity of the model class, whereas policy gradient algorithms\u2019 sample complexity polynomially depend on the dimensionality of policy parameters (in RL) or actions (in bandit). Our technical goal is to answer the following question: Can we design algorithms that converge to approximate local maxima with sample complexities that depend only and polynomially on the complexity measure of the dynamics/reward class? We note that this question is open even if the dynamics hypothesis class is finite, and the complexity measure is the logarithm of its size. The question is also open even for nonlinear bandit problems (where dynamics class is replaced by reward function class), with which we start our research. We consider first nonlinear bandit with deterministic reward where the reward function is given by \u03b7(\u03b8, a) for action a \u2208 A under instance \u03b8 \u2208 \u0398. We use sequential Rademacher complexity (Rakhlin et al., 2015a;b) to capture the complexity of the reward function \u03b7. Our main result for nonlinear bandit is stated as follows. Theorem 1.1 (Informal version of Theorem 3.1). Suppose the sequential Rademacher complexity of a loss function class (defined later) induced by the reward function class {\u03b7(\u03b8, \u00b7) : \u03b8 \u2208 \u0398} is bounded by \u221a R(\u0398)Tpolylog(T ). Then, there exists an algorithm (ViOlin, Alg. 1) that finds an -approximate local maximum with \u00d5(R(\u0398) \u22128) samples. In contrast to zero-order optimization, which does not use the parameterization of \u03b7 and has a sample complexity depending on the action dimension, our bound only depends on the complexity of the reward function class. This suggests that our algorithm exploits the extrapolation power of the reward function class. To the best of our knowledge, this is the first action-dimension-free result for both linear and nonlinear bandit problems. More concretely, we instantiate our theorem to the following settings and get new results (2)The all-local-maxima-are-global condition only needs to hold to the ground-truth total expected reward function. This potentially can allow disentangled assumptions on the ground-truth instance and the hypothesis class. that leverage the model complexity (more in Section A.1). 1. Linear bandit with finite parameter space \u0398. Because \u03b7 is concave in action a, our result leads to a sample complexity O(poly(log|\u0398|, 1/ )) for finding an approximate optimal action. In this case both zero-order optimization and the SquareCB algorithm in Foster, Rakhlin (2020) have sample complexity/regret that depend on the dimension of action space dA. 2. Linear bandit with s-sparse or structured instance parameters. Our algorithm ViOlin achieves an O(poly(s, 1/ )) sample complexity when the instance/model parameter is s-sparse and the reward is deterministic. The sample complexity of zero-order optimization depends polynomially on dA. Carpentier, Munos (2012) achieve a stronger \u00d5(s \u221a T ) regret bound for s-sparse linear bandits with actions set A = Sd\u22121.",
                ", 2016; 2015; 2017; Ge, Ma, 2020; Lee et al., 2016)). In RL, local maxima can often be global as well for many cases (Agarwal et al., 2020b).2 Zero-order optimization or policy gradient algorithms can converge to local maxima and become natural potential competitors. They are widely believed to be less sampleefficient than the model-based approach because the latter can leverage the extrapolation power of the parameterized models. Theoretically, our formulation aims to characterize this phenomenon with results showing that the model-based approach\u2019s sample complexity mostly depends (polynomially) on the complexity of the model class, whereas policy gradient algorithms\u2019 sample complexity polynomially depend on the dimensionality of policy parameters (in RL) or actions (in bandit). Our technical goal is to answer the following question: Can we design algorithms that converge to approximate local maxima with sample complexities that depend only and polynomially on the complexity measure of the dynamics/reward class? We note that this question is open even if the dynamics hypothesis class is finite, and the complexity measure is the logarithm of its size. The question is also open even for nonlinear bandit problems (where dynamics class is replaced by reward function class), with which we start our research. We consider first nonlinear bandit with deterministic reward where the reward function is given by \u03b7(\u03b8, a) for action a \u2208 A under instance \u03b8 \u2208 \u0398. We use sequential Rademacher complexity (Rakhlin et al., 2015a;b) to capture the complexity of the reward function \u03b7. Our main result for nonlinear bandit is stated as follows. Theorem 1.1 (Informal version of Theorem 3.1). Suppose the sequential Rademacher complexity of a loss function class (defined later) induced by the reward function class {\u03b7(\u03b8, \u00b7) : \u03b8 \u2208 \u0398} is bounded by \u221a R(\u0398)Tpolylog(T ). Then, there exists an algorithm (ViOlin, Alg. 1) that finds an -approximate local maximum with \u00d5(R(\u0398) \u22128) samples. In contrast to zero-order optimization, which does not use the parameterization of \u03b7 and has a sample complexity depending on the action dimension, our bound only depends on the complexity of the reward function class. This suggests that our algorithm exploits the extrapolation power of the reward function class. To the best of our knowledge, this is the first action-dimension-free result for both linear and nonlinear bandit problems. More concretely, we instantiate our theorem to the following settings and get new results (2)The all-local-maxima-are-global condition only needs to hold to the ground-truth total expected reward function. This potentially can allow disentangled assumptions on the ground-truth instance and the hypothesis class. that leverage the model complexity (more in Section A.1). 1. Linear bandit with finite parameter space \u0398. Because \u03b7 is concave in action a, our result leads to a sample complexity O(poly(log|\u0398|, 1/ )) for finding an approximate optimal action. In this case both zero-order optimization and the SquareCB algorithm in Foster, Rakhlin (2020) have sample complexity/regret that depend on the dimension of action space dA."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "744fc61608d921e501abbcb1042c0cae15ed44d7",
                "externalIds": {
                    "ArXiv": "2102.04168",
                    "DBLP": "conf/nips/DongYM21",
                    "CorpusId": 231846402
                },
                "corpusId": 231846402,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/744fc61608d921e501abbcb1042c0cae15ed44d7",
                "title": "Provable Model-based Nonlinear Bandit and Reinforcement Learning: Shelve Optimism, Embrace Virtual Curvature",
                "abstract": "This paper studies model-based bandit and reinforcement learning (RL) with nonlinear function approximations. We propose to study convergence to approximate local maxima because we show that global convergence is statistically intractable even for one-layer neural net bandit with a deterministic reward. For both nonlinear bandit and RL, the paper presents a model-based algorithm, Virtual Ascent with Online Model Learner (ViOlin), which provably converges to a local maximum with sample complexity that only depends on the sequential Rademacher complexity of the model class. Our results imply novel global or local regret bounds on several concrete settings such as linear bandit with finite or sparse model class, and two-layer neural net bandit. A key algorithmic insight is that optimism may lead to over-exploration even for two-layer neural net model class. On the other hand, for convergence to local maxima, it suffices to maximize the virtual return if the model can also reasonably predict the size of the gradient and Hessian of the real return.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "66742509",
                        "name": "Kefan Dong"
                    },
                    {
                        "authorId": "2144518437",
                        "name": "Jiaqi Yang"
                    },
                    {
                        "authorId": "1901958",
                        "name": "Tengyu Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although there exists various methods (Kurutach et al., 2018; Luo et al., 2018; Clavera et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020) for MBRL, the common strategy is to first learn the environment model and use it to generate fictitious experiences for learning an agent\u2019s policy.",
                "The solid line and shaded regions represent the mean and standard deviation, respectively, across five runs with random seeds.\noriginal MBPO in Figure F.2.",
                "One can observe that the learning termination function is significantly harmful to the sample efficiency compared to the\n0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 TimeSteps(M)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nAc cu\nm ul\nat ed\nR ew\nar ds\nMBPO with Termination Ftn. MBPO without Termination Ftn.\nHopper-v3\nFigure F.2: Learning curves of MBPO on Hopper-v3 belonging to MuJoCo environments.",
                "One can observe that learning terminal functions is harmful to sample efficiency in MBPO.",
                "Many prior works (Janner et al., 2019; Nagabandi et al., 2018) have proposed to combine the\nPreprint.",
                "Many prior works (Janner et al., 2019; Nagabandi et al., 2018) have proposed to combine the",
                "Moreover, as we mentioned in Section D, to show that prior knowledge is crucial to MBRL, we provide an experimental result about one of the state-of-the-art MBRL methods, i.e., MBPO (Janner et al., 2019) with and without learning a termination function."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "904b95b2adec236cf328c18f9966dcf43805b2a1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-03866",
                    "ArXiv": "2102.03866",
                    "CorpusId": 231846463
                },
                "corpusId": 231846463,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/904b95b2adec236cf328c18f9966dcf43805b2a1",
                "title": "Model-Augmented Q-learning",
                "abstract": "In recent years, $Q$-learning has become indispensable for model-free reinforcement learning (MFRL). However, it suffers from well-known problems such as under- and overestimation bias of the value, which may adversely affect the policy learning. To resolve this issue, we propose a MFRL framework that is augmented with the components of model-based RL. Specifically, we propose to estimate not only the $Q$-values but also both the transition and the reward with a shared network. We further utilize the estimated reward from the model estimators for $Q$-learning, which promotes interaction between the estimators. We show that the proposed scheme, called Model-augmented $Q$-learning (MQL), obtains a policy-invariant solution which is identical to the solution obtained by learning with true reward. Finally, we also provide a trick to prioritize past experiences in the replay buffer by utilizing model-estimation errors. We experimentally validate MQL built upon state-of-the-art off-policy MFRL methods, and show that MQL largely improves their performance and convergence. The proposed scheme is simple to implement and does not require additional training cost.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Youngmin Oh"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    },
                    {
                        "authorId": "1720494",
                        "name": "Eunho Yang"
                    },
                    {
                        "authorId": "2110796623",
                        "name": "S. Hwang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "841f8e46c359b86ed1da7dafa3062ef9f351b5a4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-00714",
                    "ArXiv": "2102.00714",
                    "MAG": "3128328080",
                    "CorpusId": 231854937
                },
                "corpusId": 231854937,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/841f8e46c359b86ed1da7dafa3062ef9f351b5a4",
                "title": "NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning",
                "abstract": "Offline reinforcement learning (RL) aims at learning a good policy from a batch of collected data, without extra interactions with the environment during training. However, current offline RL benchmarks commonly have a large reality gap, because they involve large datasets collected by highly exploratory policies, and the trained policy is directly evaluated in the environment. In real-world situations, running a highly exploratory policy is prohibited to ensure system safety, the data is commonly very limited, and a trained policy should be well validated before deployment. In this paper, we present a near real-world offline RL benchmark, named NeoRL, which contains datasets from various domains with controlled sizes, and extra test datasets for policy validation. We evaluate existing offline RL algorithms on NeoRL and argue that the performance of a policy should also be compared with the deterministic version of the behavior policy, instead of the dataset reward. The empirical results demonstrate that the tested offline RL algorithms become less competitive to the deterministic policy on many datasets, and the offline policy evaluation hardly helps. The NeoRL suit can be found at this http URL. We hope this work will shed some light on future research and draw more attention when deploying RL in real-world systems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46938007",
                        "name": "Rongjun Qin"
                    },
                    {
                        "authorId": "27008440",
                        "name": "Songyi Gao"
                    },
                    {
                        "authorId": "2153649203",
                        "name": "Xingyuan Zhang"
                    },
                    {
                        "authorId": "2022816240",
                        "name": "Zhen Xu"
                    },
                    {
                        "authorId": "2042297846",
                        "name": "Shengkai Huang"
                    },
                    {
                        "authorId": "2145251694",
                        "name": "Zewen Li"
                    },
                    {
                        "authorId": "2155042473",
                        "name": "Weinan Zhang"
                    },
                    {
                        "authorId": "2152845240",
                        "name": "Yang Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is evidenced by the plethora of literature, both model-free and model-based, that chooses SAC as the standard [3, 4, 5, 6, 7], often showing it as the best performing model-free approach."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "639b555c90d00c920de9119bd504e0ffd14cab6b",
                "externalIds": {
                    "ArXiv": "2101.11331",
                    "DBLP": "journals/corr/abs-2101-11331",
                    "CorpusId": 231718686
                },
                "corpusId": 231718686,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/639b555c90d00c920de9119bd504e0ffd14cab6b",
                "title": "OffCon3: What is state of the art anyway?",
                "abstract": "Two popular approaches to model-free continuous control tasks are SAC and TD3. At first glance these approaches seem rather different; SAC aims to solve the entropy-augmented MDP by minimising the KL-divergence between a stochastic proposal policy and a hypotheical energy-basd soft Q-function policy, whereas TD3 is derived from DPG, which uses a deterministic policy to perform policy gradient ascent along the value function. In reality, both approaches are remarkably similar, and belong to a family of approaches we call `Off-Policy Continuous Generalized Policy Iteration'. This illuminates their similar performance in most continuous control benchmarks, and indeed when hyperparameters are matched, their performance can be statistically indistinguishable. To further remove any difference due to implementation, we provide OffCon$^3$ (Off-Policy Continuous Control: Consolidated), a code base featuring state-of-the-art versions of both algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2053179501",
                        "name": "Philip J. Ball"
                    },
                    {
                        "authorId": "145029236",
                        "name": "S. Roberts"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For MBPO, we use the default network architectures for the Q networks, policy network, and model ensembles (Janner et al., 2019).",
                "Since MBPO builds on top of a SAC agent, to make our comparisons fair, meaningful, and consistent with previous work, we make all SAC related hyperparameters exactly the same as used in the MBPO paper (Janner et al., 2019).",
                "Sampling a number of random datapoints at the start of training is also a common technique that has been used in previous model-free as well as model-based works (Haarnoja et al., 2018a; Fujimoto et al., 2018; Janner et al., 2019).",
                ", 2019), recent methods such as MBPO combine a model ensemble with a carefully controlled rollout horizon to obtain better performance (Janner et al., 2019; Buckman et al., 2018).",
                "To address some of the critical issues in model-based learning (Langlois et al., 2019), recent methods such as MBPO combine a model ensemble with a carefully controlled rollout horizon to obtain better performance (Janner et al., 2019; Buckman et al., 2018).",
                "For example, Model-Based Policy Optimization (MBPO) (Janner et al., 2019), is a state-of-the-art model-based algorithm which updates the agent with a mix of real data from the environment and \u201cfake\u201d data from its model, and uses a large UTD ratio of 20-40.",
                "Janner et al. (2019) proposed Model-Based Policy Optimization (MBPO), which was shown to be much more sample efficient than popular model-free algorithms such as SAC and PPO for the MuJoCo environments.",
                "As in the MBPO paper, we train for 125K for Hopper, and 300K for the other three environments (Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "736590f70e7f2dc464c1c62491cfa8adb4d718f3",
                "externalIds": {
                    "ArXiv": "2101.05982",
                    "DBLP": "conf/iclr/ChenWZR21",
                    "CorpusId": 231627730
                },
                "corpusId": 231627730,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/736590f70e7f2dc464c1c62491cfa8adb4d718f3",
                "title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model",
                "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio>>1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio>>1.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Xinyue Chen"
                    },
                    {
                        "authorId": "50879688",
                        "name": "Che Wang"
                    },
                    {
                        "authorId": "2142727747",
                        "name": "Zijian Zhou"
                    },
                    {
                        "authorId": "1829862",
                        "name": "K. Ross"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026McAllister & Rasmussen, 2016; Chua et al., 2018; Amos et al., 2018; Hafner et al., 2019b; Nagabandi et al., 2018; Kahn et al., 2020; Dong et al., 2020) or policy optimization (Sutton, 1991; Weber et al., 2017; Ha & Schmidhuber, 2018; Janner et al., 2019; Wang & Ba, 2019; Hafner et al., 2019a)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9a689727d040a6bf122f16ea50884d5cd5258321",
                "externalIds": {
                    "DBLP": "conf/iclr/TianNEDEFL21",
                    "ArXiv": "2012.15373",
                    "CorpusId": 229923865
                },
                "corpusId": 229923865,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9a689727d040a6bf122f16ea50884d5cd5258321",
                "title": "Model-Based Visual Planning with Self-Supervised Functional Distances",
                "abstract": "A generalist robot must be able to complete a variety of tasks in its environment. One appealing way to specify each task is in terms of a goal observation. However, learning goal-reaching policies with reinforcement learning remains a challenging problem, particularly when hand-engineered reward functions are not available. Learned dynamics models are a promising approach for learning about the environment without rewards or task-directed data, but planning to reach goals with such a model requires a notion of functional similarity between observations and goal states. We present a self-supervised method for model-based visual goal reaching, which uses both a visual dynamics model as well as a dynamical distance function learned using model-free reinforcement learning. Our approach learns entirely using offline, unlabeled data, making it practical to scale to large and diverse datasets. In our experiments, we find that our method can successfully learn models that perform a variety of tasks at test-time, moving objects amid distractors with a simulated robotic arm and even learning to open and close a drawer using a real-world robot. In comparisons, we find that this approach substantially outperforms both model-free and model-based prior methods. Videos and visualizations are available here: http://sites.google.com/berkeley.edu/mbold.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "71692259",
                        "name": "Stephen Tian"
                    },
                    {
                        "authorId": "4734949",
                        "name": "Suraj Nair"
                    },
                    {
                        "authorId": "27535721",
                        "name": "F. Ebert"
                    },
                    {
                        "authorId": "36076404",
                        "name": "S. Dasari"
                    },
                    {
                        "authorId": "8140754",
                        "name": "Benjamin Eysenbach"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One class of algorithms trains \u201con-policy\u201d model-free algorithms virtually inside the environment model [Kurutach et al., 2018, Luo et al., 2019] while the other trains \u201coff-policy\u201d model-free algorithms virtually [Janner et al., 2019].",
                ", 2019] while the other trains \u201coff-policy\u201d model-free algorithms virtually [Janner et al., 2019]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "63f5fab4b51fcdb7484cdb7cde5925d33824e798",
                "externalIds": {
                    "MAG": "3114713072",
                    "ArXiv": "2012.13962",
                    "DBLP": "journals/corr/abs-2012-13962",
                    "CorpusId": 229679986
                },
                "corpusId": 229679986,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/63f5fab4b51fcdb7484cdb7cde5925d33824e798",
                "title": "A Tutorial on Sparse Gaussian Processes and Variational Inference",
                "abstract": "Gaussian processes (GPs) provide a framework for Bayesian inference that can offer principled uncertainty estimates for a large range of problems. For example, if we consider regression problems with Gaussian likelihoods, a GP model enjoys a posterior in closed form. However, identifying the posterior GP scales cubically with the number of training examples and requires to store all examples in memory. In order to overcome these obstacles, sparse GPs have been proposed that approximate the true posterior GP with pseudo-training examples. Importantly, the number of pseudo-training examples is user-defined and enables control over computational and memory complexity. In the general case, sparse GPs do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference (VI), where the problem of Bayesian inference is cast as an optimization problem -- namely, to maximize a lower bound of the log marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identified together with hyperparameters of the generative model (i.e. prior and likelihood). The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classification problems with discrete labels, but also multilabel problems. The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both GPs and VI. A proper exposition to the subject enables also access to more recent advances (like importance-weighted VI as well as interdomain, multioutput and deep GPs) that can serve as an inspiration for new research ideas.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2505365",
                        "name": "Felix Leibfried"
                    },
                    {
                        "authorId": "27040809",
                        "name": "Vincent Dutordoir"
                    },
                    {
                        "authorId": "144104356",
                        "name": "S. T. John"
                    },
                    {
                        "authorId": "3110071",
                        "name": "N. Durrande"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We evaluate an MBPO based model (Janner et al. (2019)), which also carries out policy rollouts in latent space similar to LOMPO, but does not apply an uncertainty penalty.",
                "Model-based RL algorithms have demonstrated impressive sample efficiency in interactive RL (Janner et al., 2019; Rajeswaran et al., 2020; Hafner et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c32a6fc742b927e72ed496743a8dcf9ca1cc415c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-11547",
                    "ArXiv": "2012.11547",
                    "CorpusId": 229340500
                },
                "corpusId": 229340500,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c32a6fc742b927e72ed496743a8dcf9ca1cc415c",
                "title": "Offline Reinforcement Learning from Images with Latent Space Models",
                "abstract": "Offline reinforcement learning (RL) refers to the problem of learning policies from a static dataset of environment interactions. Offline RL enables extensive use and re-use of historical datasets, while also alleviating safety concerns associated with online exploration, thereby expanding the real-world applicability of RL. Most prior work in offline RL has focused on tasks with compact state representations. However, the ability to learn directly from rich observation spaces like images is critical for real-world applications such as robotics. In this work, we build on recent advances in model-based algorithms for offline RL, and extend them to high-dimensional visual observation spaces. Model-based offline RL algorithms have achieved state of the art results in state based tasks and have strong theoretical guarantees. However, they rely crucially on the ability to quantify uncertainty in the model predictions, which is particularly challenging with image observations. To overcome this challenge, we propose to learn a latent-state dynamics model, and represent the uncertainty in the latent space. Our approach is both tractable in practice and corresponds to maximizing a lower bound of the ELBO in the unknown POMDP. In experiments on a range of challenging image-based locomotion and manipulation tasks, we find that our algorithm significantly outperforms previous offline model-free RL methods as well as state-of-the-art online visual model-based RL methods. Moreover, we also find that our approach excels on an image-based drawer closing task on a real robot using a pre-existing dataset. All results including videos can be found online at https://sites.google.com/view/lompo/ .",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "102801230",
                        "name": "Rafael Rafailov"
                    },
                    {
                        "authorId": "10909315",
                        "name": "Tianhe Yu"
                    },
                    {
                        "authorId": "19275599",
                        "name": "A. Rajeswaran"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We decided to use a short horizon to diminish the impact of the compound error [53], the accumulation error following a wrong model, as well known in DYNA-style approaches.",
                "Alternatives as [46, 47, 53] also take these uncertainties into consideration using probabilistic ensembles."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "08defb0638acfd8ce28bb4610ac14090d6ca7d47",
                "externalIds": {
                    "ArXiv": "2012.09737",
                    "MAG": "3112056027",
                    "DBLP": "journals/corr/abs-2012-09737",
                    "CorpusId": 229297563
                },
                "corpusId": 229297563,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/08defb0638acfd8ce28bb4610ac14090d6ca7d47",
                "title": "Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL",
                "abstract": "Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system. We find that the model-based approach demonstrates higher representational power and sample-efficiency, while the asymptotic performance of the model-free method is slightly superior. The model-based algorithm is implemented in a DYNA-style using an uncertainty aware model, and the model-free algorithm is based on tailored deep Q-learning. In both cases, the algorithms were implemented in a way, which presents increased noise robustness as omnipresent in accelerator control problems. Code is released in this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "107672013",
                        "name": "Simon Hirlaender"
                    },
                    {
                        "authorId": "104165703",
                        "name": "N. Bruchon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MBRL has shown impressive performance and sample efficiency on many robotics tasks [13], [1], [2], [14].",
                "One-step models will likely remain useful for other algorithms less focused on the long-term future, such as MBPO [14], or in situations where data is non-episodic, so applying the timedependant structure could be forced.",
                "One-step models will likely remain useful for other algorithms less focused on the long-term future, such as MBPO [14], or in"
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "20a016b286c1e6cb5bb70bf6c6e79639a8993ac6",
                "externalIds": {
                    "MAG": "3113244285",
                    "DBLP": "conf/cdc/LambertWZPC21",
                    "ArXiv": "2012.09156",
                    "DOI": "10.1109/CDC45484.2021.9683134",
                    "CorpusId": 229211063
                },
                "corpusId": 229211063,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/20a016b286c1e6cb5bb70bf6c6e79639a8993ac6",
                "title": "Learning Accurate Long-term Dynamics for Model-based Reinforcement Learning",
                "abstract": "Accurately predicting the dynamics of robotic systems is crucial for model-based control and reinforcement learning. The most common way to estimate dynamics is by fitting a one-step ahead prediction model and using it to recursively propagate the predicted state distribution over long horizons. Unfortunately, this approach is known to compound even small prediction errors, making long-term predictions inaccurate. In this paper, we propose a new parametrization to supervised learning on state-action data to stably predict at longer horizons \u2013 that we call a trajectory-based model. This trajectory-based model takes an initial state, a future time index, and control parameters as inputs, and directly predicts the state at the future time index. Experimental results in simulated and real-world robotic tasks show that trajectory-based models yield significantly more accurate long term predictions, improved sample efficiency, and the ability to predict task reward. With these improved prediction properties, we conclude with a demonstration of methods for using the trajectory-based model for control.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2052363815",
                        "name": "Nathan Lambert"
                    },
                    {
                        "authorId": "2037793283",
                        "name": "Albert Wilcox"
                    },
                    {
                        "authorId": "2143574084",
                        "name": "Howard Zhang"
                    },
                    {
                        "authorId": "143648343",
                        "name": "K. Pister"
                    },
                    {
                        "authorId": "35159852",
                        "name": "R. Calandra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, as described in Figure 1, we found that SAC and MBPO fail in reset-free settings when denied access to resets if they continue gradient updates, which we attributed to gradient-based instability.",
                "In particular, it is imperative that the model not only be used\nfor sample-efficient policy optimization (as in MBPO), but also that the model be used to safely act in the environment.",
                "To illustrate this issue, we first pre-train agents to convergence in the episodic Hopper environment (Brockman et al., 2016) with state-of-the-art model-free and model-based RL algorithms: Soft Actor Critic (SAC) (Haarnoja et al., 2018) and Model-Based Policy Optimization (MBPO) (Janner et al., 2019), respectively.",
                "To actually learn the policy, we use the model to generate short rollouts, optimizing \u03c0\u03b8 with SAC, similar to model-based policy learning works that find long rollouts to destabilize learning due to compounding model errors (Janner et al., 2019).",
                "\u2026issue, we first pre-train agents to convergence in the episodic Hopper environment (Brockman et al., 2016) with state-of-the-art model-free and model-based RL algorithms: Soft Actor Critic (SAC) (Haarnoja et al., 2018) and Model-Based Policy Optimization (MBPO) (Janner et al., 2019), respectively."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "25c310d4f7bc581a94455a1e96373d924ec736ae",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-03548",
                    "MAG": "3111981703",
                    "ArXiv": "2012.03548",
                    "CorpusId": 227346855
                },
                "corpusId": 227346855,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/25c310d4f7bc581a94455a1e96373d924ec736ae",
                "title": "Reset-Free Lifelong Learning with Skill-Space Planning",
                "abstract": "The objective of lifelong reinforcement learning (RL) is to optimize agents which can continuously adapt and interact in changing environments. However, current RL approaches fail drastically when environments are non-stationary and interactions are non-episodic. We propose Lifelong Skill Planning (LiSP), an algorithmic framework for non-episodic lifelong RL based on planning in an abstract space of higher-order skills. We learn the skills in an unsupervised manner using intrinsic rewards and plan over the learned skills using a learned dynamics model. Moreover, our framework permits skill discovery even from offline data, thereby reducing the need for excessive real-world interactions. We demonstrate empirically that LiSP successfully enables long-horizon planning and learns agents that can avoid catastrophic failures even in challenging non-stationary and non-episodic environments derived from gridworld and MuJoCo benchmarks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2070275468",
                        "name": "Kevin Lu"
                    },
                    {
                        "authorId": "1954250",
                        "name": "Aditya Grover"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    },
                    {
                        "authorId": "2080746",
                        "name": "Igor Mordatch"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In model-free RL, short-range model-generated rollouts branched from the real-world data were demonstrated to avoid the model pitfalls [34]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "01e4564c141b04b74e2a21dc8273260e0600348a",
                "externalIds": {
                    "DBLP": "journals/sensors/SongCSL20",
                    "PubMedCentral": "7766926",
                    "DOI": "10.3390/s20247297",
                    "CorpusId": 229683268,
                    "PubMed": "33353153"
                },
                "corpusId": 229683268,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/01e4564c141b04b74e2a21dc8273260e0600348a",
                "title": "Data Efficient Reinforcement Learning for Integrated Lateral Planning and Control in Automated Parking System",
                "abstract": "Reinforcement learning (RL) is a promising direction in automated parking systems (APSs), as integrating planning and tracking control using RL can potentially maximize the overall performance. However, commonly used model-free RL requires many interactions to achieve acceptable performance, and model-based RL in APS cannot continuously learn. In this paper, a data-efficient RL method is constructed to learn from data by use of a model-based method. The proposed method uses a truncated Monte Carlo tree search to evaluate parking states and select moves. Two artificial neural networks are trained to provide the search probability of each tree branch and the final reward for each state using self-trained data. The data efficiency is enhanced by weighting exploration with parking trajectory returns, an adaptive exploration scheme, and experience augmentation with imaginary rollouts. Without human demonstrations, a novel training pipeline is also used to train the initial action guidance network and the state value network. Compared with path planning and path-following methods, the proposed integrated method can flexibly co-ordinate the longitudinal and lateral motion to park a smaller parking space in one maneuver. Its adaptability to changes in the vehicle model is verified by joint Carsim and MATLAB simulation, demonstrating that the algorithm converges within a few iterations. Finally, experiments using a real vehicle platform are used to further verify the effectiveness of the proposed method. Compared with obtaining rewards using simulation, the proposed method achieves a better final parking attitude and success rate.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1921921003",
                        "name": "Shaoyu Song"
                    },
                    {
                        "authorId": "2155550850",
                        "name": "Hui Chen"
                    },
                    {
                        "authorId": "1490938524",
                        "name": "Hongwei Sun"
                    },
                    {
                        "authorId": "14697876",
                        "name": "Meicen Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In contrast, (Janner et al., 2019; Hafner et al., 2019) fully amortize learned policies over the entire training experience, which is fast even for high-dimensional action spaces, but cannot directly transfer to new environments with different dynamics and reward functions.",
                "\u2026form of learned transition dynamics, and reward models, which are used to learn either parametric policies by amortizing over the entire training experience (Hafner et al., 2019; Janner et al., 2019), or perform online planning as done in Planet (Hafner et al., 2018), and PETS (Chua et al., 2018).",
                "World models summarize an agent\u2019s experience in the form of learned transition dynamics, and reward models, which are used to learn either parametric policies by amortizing over the entire training experience (Hafner et al., 2019; Janner et al., 2019), or perform online planning as done in Planet (Hafner et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "22a8ab2f4cd0777ebc93d8e414535c03d4d57615",
                "externalIds": {
                    "DBLP": "conf/iclr/XieBHGS21",
                    "ArXiv": "2011.13897",
                    "CorpusId": 233714270
                },
                "corpusId": 233714270,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/22a8ab2f4cd0777ebc93d8e414535c03d4d57615",
                "title": "Latent Skill Planning for Exploration and Transfer",
                "abstract": "To quickly solve new tasks in complex environments, intelligent agents need to build up reusable knowledge. For example, a learned world model captures knowledge about the environment that applies to new tasks. Similarly, skills capture general behaviors that can apply to new tasks. In this paper, we investigate how these two approaches can be integrated into a single reinforcement learning agent. Specifically, we leverage the idea of partial amortization for fast adaptation at test time. For this, actions are produced by a policy that is learned over time while the skills it conditions on are chosen using online planning. We demonstrate the benefits of our design decisions across a suite of challenging locomotion tasks and demonstrate improved sample efficiency in single tasks as well as in transfer from one task to another, as compared to competitive baselines. Videos are available at: https://sites.google.com/view/latent-skill-planning/",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47966782",
                        "name": "Kevin Xie"
                    },
                    {
                        "authorId": "51113848",
                        "name": "Homanga Bharadhwaj"
                    },
                    {
                        "authorId": "35006479",
                        "name": "Danijar Hafner"
                    },
                    {
                        "authorId": "2054554660",
                        "name": "Animesh Garg"
                    },
                    {
                        "authorId": "2162768",
                        "name": "F. Shkurti"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In contrast, (Janner et al., 2019; Hafner et al., 2019) fully amortize learned policies over the entire training experience, which is fast even for high-dimensional action spaces, but cannot directly transfer to new environments with different dynamics and reward functions.",
                "\u2026form of learned transition dynamics, and reward models, which are used to learn either parametric policies by amortizing over the entire training experience (Hafner et al., 2019; Janner et al., 2019), or perform online planning as done in Planet (Hafner et al., 2018), and PETS (Chua et al., 2018).",
                "World models summarize an agent\u2019s experience in the form of learned transition dynamics, and reward models, which are used to learn either parametric policies by amortizing over the entire training experience (Hafner et al., 2019; Janner et al., 2019), or perform online planning as done in Planet (Hafner et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4d1537347d8f5c463188166ae96c3c0d7a3260fa",
                "externalIds": {
                    "MAG": "3108183475",
                    "DBLP": "journals/corr/abs-2011-13897",
                    "CorpusId": 227209789
                },
                "corpusId": 227209789,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4d1537347d8f5c463188166ae96c3c0d7a3260fa",
                "title": "Skill Transfer via Partially Amortized Hierarchical Planning",
                "abstract": "To quickly solve new tasks in complex environments, intelligent agents need to build up reusable knowledge. For example, a learned world model captures knowledge about the environment that applies to new tasks. Similarly, skills capture general behaviors that can apply to new tasks. In this paper, we investigate how these two approaches can be integrated into a single reinforcement learning agent. Specifically, we leverage the idea of partial amortization for fast adaptation at test time. For this, actions are produced by a policy that is learned over time while the skills it conditions on are chosen using online planning. We demonstrate the benefits of our design decisions across a suite of challenging locomotion tasks and demonstrate improved sample efficiency in single tasks as well as in transfer from one task to another, as compared to competitive baselines. Videos are available at: this https URL",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47966782",
                        "name": "Kevin Xie"
                    },
                    {
                        "authorId": "51113848",
                        "name": "Homanga Bharadhwaj"
                    },
                    {
                        "authorId": "35006479",
                        "name": "Danijar Hafner"
                    },
                    {
                        "authorId": "1873736",
                        "name": "Animesh Garg"
                    },
                    {
                        "authorId": "2162768",
                        "name": "F. Shkurti"
                    }
                ]
            }
        },
        {
            "contexts": [
                "When combined with modern techniques (Kurutach et al., 2018; Luo et al., 2018; Nagabandi et al., 2018; Ha & Schmidhuber, 2018; Hafner et al., 2019; Wang & Ba, 2019; Janner et al., 2019), MBRL is able to achieve some level of success."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5cf549e26b4dce19d5bc783de83047479ce6218a",
                "externalIds": {
                    "ArXiv": "2011.12491",
                    "DBLP": "journals/corr/abs-2011-12491",
                    "MAG": "3109943994",
                    "CorpusId": 227162474
                },
                "corpusId": 227162474,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5cf549e26b4dce19d5bc783de83047479ce6218a",
                "title": "World Model as a Graph: Learning Latent Landmarks for Planning",
                "abstract": "Planning - the ability to analyze the structure of a problem in the large and decompose it into interrelated subproblems - is a hallmark of human intelligence. While deep reinforcement learning (RL) has shown great promise for solving relatively straightforward control tasks, it remains an open problem how to best incorporate planning into existing deep RL paradigms to handle increasingly complex environments. One prominent framework, Model-Based RL, learns a world model and plans using step-by-step virtual rollouts. This type of world model quickly diverges from reality when the planning horizon increases, thus struggling at long-horizon planning. How can we learn world models that endow agents with the ability to do temporally extended reasoning? In this work, we propose to learn graph-structured world models composed of sparse, multi-step transitions. We devise a novel algorithm to learn latent landmarks that are scattered (in terms of reachability) across the goal space as the nodes on the graph. In this same graph, the edges are the reachability estimates distilled from Q-functions. On a variety of high-dimensional continuous control tasks ranging from robotic manipulation to navigation, we demonstrate that our method, named L3P, significantly outperforms prior work, and is oftentimes the only method capable of leveraging both the robustness of model-free RL and generalization of graph-search algorithms. We believe our work is an important step towards scalable planning in reinforcement learning.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48571502",
                        "name": "Lunjun Zhang"
                    },
                    {
                        "authorId": "2052310563",
                        "name": "Ge Yang"
                    },
                    {
                        "authorId": "3275284",
                        "name": "Bradly C. Stadie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In our experiment, we have shown that our model-based algorithm outperforms Chua et al. (2018) and Janner et al. (2019) in given environments.",
                "However, in our method, there is a limitation of using MPC, which might fail in even higher-dimensional tasks as shown in Janner et al. (2019). Incorporating policy gradient techniques for action-selection might further improve the performance and we leave it for future work.",
                "Model-Based Policy\nOptimization (MBPO) from Janner et al. (2019) uses the same bootstrap ensemble techniques as PETS in modeling, but differs from PETS in policy optimization with a large amount of short model-generated rollouts, and can cope with environments with no oracle rewards provided.",
                "However, in our method, there is a limitation of using MPC, which might fail in even higher-dimensional tasks as shown in Janner et al. (2019).",
                "Notice that Chua et al. (2018), Janner et al. (2019) have already greatly outperformed stateof-the-art model-free methods in sample efficiency as shown in their papers."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3c9863eecca66c544ffc5f821d5703f9af179fd5",
                "externalIds": {
                    "DBLP": "conf/icml/FanM21",
                    "ArXiv": "2012.09613",
                    "CorpusId": 235825518
                },
                "corpusId": 235825518,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3c9863eecca66c544ffc5f821d5703f9af179fd5",
                "title": "Model-based Reinforcement Learning for Continuous Control with Posterior Sampling",
                "abstract": "Balancing exploration and exploitation is crucial in reinforcement learning (RL). In this paper, we study model-based posterior sampling for reinforcement learning (PSRL) in continuous state-action spaces theoretically and empirically. First, we show the first regret bound of PSRL in continuous spaces which is polynomial in the episode length to the best of our knowledge. With the assumption that reward and transition functions can be modeled by Bayesian linear regression, we develop a regret bound of $\\tilde{O}(H^{3/2}d\\sqrt{T})$, where $H$ is the episode length, $d$ is the dimension of the state-action space, and $T$ indicates the total time steps. This result matches the best-known regret bound of non-PSRL methods in linear MDPs. Our bound can be extended to nonlinear cases as well with feature embedding: using linear kernels on the feature representation $\\phi$, the regret bound becomes $\\tilde{O}(H^{3/2}d_{\\phi}\\sqrt{T})$, where $d_\\phi$ is the dimension of the representation space. Moreover, we present MPC-PSRL, a model-based posterior sampling algorithm with model predictive control for action selection. To capture the uncertainty in models, we use Bayesian linear regression on the penultimate layer (the feature representation layer $\\phi$) of neural networks. Empirical results show that our algorithm achieves the state-of-the-art sample efficiency in benchmark continuous control tasks compared to prior model-based algorithms, and matches the asymptotic performance of model-free algorithms.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109755002",
                        "name": "Ying Fan"
                    },
                    {
                        "authorId": "2056969978",
                        "name": "Yifei Ming"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[25] uses the same bootstrap ensemble techniques as PETS in modeling, but differs from PETS in policy optimization with a large amount of short model-generated rollouts, and can cope with environments with no oracle rewards provided."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b1006407b4afc47b2c29ed4b00a5cd5c6473bc97",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-09613",
                    "MAG": "3112113222",
                    "CorpusId": 229297856
                },
                "corpusId": 229297856,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b1006407b4afc47b2c29ed4b00a5cd5c6473bc97",
                "title": "Efficient Exploration for Model-based Reinforcement Learning with Continuous States and Actions",
                "abstract": "Balancing exploration and exploitation is crucial in reinforcement learning (RL). In this paper, we study the model-based posterior sampling algorithm in continuous state-action spaces theoretically and empirically. First, we improve the regret bound: with the assumption that reward and transition functions can be modeled as Gaussian Processes with linear kernels, we develop a Bayesian regret bound of $\\tilde{O}(H^{3/2}d\\sqrt{T})$, where $H$ is the episode length, $d$ is the dimension of the state-action space, and $T$ indicates the total time steps. Our bound can be extended to nonlinear cases as well: using linear kernels on the feature representation $\\phi$, the Bayesian regret bound becomes $\\tilde{O}(H^{3/2}d_{\\phi}\\sqrt{T})$, where $d_\\phi$ is the dimension of the representation space. Moreover, we present MPC-PSRL, a model-based posterior sampling algorithm with model predictive control for action selection. To capture the uncertainty in models and realize posterior sampling, we use Bayesian linear regression on the penultimate layer (the feature representation layer $\\phi$) of neural networks. Empirical results show that our algorithm achieves the best sample efficiency in benchmark control tasks compared to prior model-based algorithms, and matches the asymptotic performance of model-free algorithms.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109755002",
                        "name": "Ying Fan"
                    },
                    {
                        "authorId": "2056969978",
                        "name": "Yifei Ming"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, [33] proves the fact that even a small model error will cause great approximation bias of the value function obtained by simulating k steps in the incorrect predictive dynamics shown as (11)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4aeab70953736118c367d0f973b3a962990641f3",
                "externalIds": {
                    "MAG": "3104038931",
                    "DBLP": "journals/corr/abs-2011-06752",
                    "ArXiv": "2011.06752",
                    "DOI": "10.1109/ICARM52023.2021.9536131",
                    "CorpusId": 226955974
                },
                "corpusId": 226955974,
                "publicationVenue": {
                    "id": "d0caaee0-c398-40d1-ab96-5e50b8c1dc73",
                    "name": "International Conference on Advanced Robotics and Mechatronics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Adv Robot Mechatronics",
                        "ICARM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4aeab70953736118c367d0f973b3a962990641f3",
                "title": "Critic PI2: Master Continuous Planning via Policy Improvement with Path Integrals and Deep Actor-Critic Reinforcement Learning",
                "abstract": "Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods from AlphaGo to Muzero have enjoyed huge success in discrete domains, such as chess and Go. Unfortunately, in real-world applications like robot control and inverted pendulum, whose action space is normally continuous, those tree-based planning techniques will be struggling. To address those limitations, in this paper, we present a novel model-based reinforcement learning frameworks called Critic PI2, which combines the benefits from trajectory optimization, deep actor-critic learning, and model-based reinforcement learning. Our method is evaluated for inverted pendulum models with applicability to many continuous control systems. Extensive experiments demonstrate that Critic PI2 achieved a new state of the art in a range of challenging continuous domains. Furthermore, we show that planning with a critic significantly increases the sample efficiency and real-time performance. Our work opens a new direction toward learning the components of a model-based planning system and how to use them.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2047532575",
                        "name": "Jiajun Fan"
                    },
                    {
                        "authorId": "39684779",
                        "name": "He Ba"
                    },
                    {
                        "authorId": "2118282002",
                        "name": "Xian Guo"
                    },
                    {
                        "authorId": "40513470",
                        "name": "Jianye Hao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3e174dc0baf3c021c1e310569f96309572f8c241",
                "externalIds": {
                    "ArXiv": "2011.04764",
                    "DBLP": "conf/ijcai/AlonsoPGR21",
                    "MAG": "3102089800",
                    "DOI": "10.24963/ijcai.2021/294",
                    "CorpusId": 226290108
                },
                "corpusId": 226290108,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3e174dc0baf3c021c1e310569f96309572f8c241",
                "title": "Deep Reinforcement Learning for Navigation in AAA Video Games",
                "abstract": "In video games, \\non-player characters (NPCs) are used to enhance the players' experience in a variety of ways, e.g., as enemies, allies, or innocent bystanders. A crucial component of NPCs is navigation, which allows them to move from one point to another on the map. The most popular approach for NPC navigation in the video game industry is to use a navigation mesh (NavMesh), which is a graph representation of the map, with nodes and edges indicating traversable areas. Unfortunately, complex navigation abilities that extend the character's capacity for movement, e.g., grappling hooks, jetpacks, teleportation, or double-jumps, increase the complexity of the NavMesh, making it intractable in many practical scenarios. Game designers are thus constrained to only add abilities that can be handled by a NavMesh. As an alternative to the NavMesh, we propose to use Deep Reinforcement Learning (Deep RL) to learn how to navigate 3D maps in video games using any navigation ability. We test our approach on complex 3D environments that are notably an order of magnitude larger than maps typically used in the Deep RL literature. One of these environments is from a recently released AAA video game called Hyper Scape. We find that our approach performs surprisingly well, achieving at least 90% success rate in a variety of scenarios using complex navigation abilities.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144370326",
                        "name": "Eloi Alonso"
                    },
                    {
                        "authorId": "1470808701",
                        "name": "Maxim Peter"
                    },
                    {
                        "authorId": "2008244607",
                        "name": "David Goumard"
                    },
                    {
                        "authorId": "8365320",
                        "name": "Joshua Romoff"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, we chose the MPO update for its ease of implementation, but it is likely other forms of regularized policy gradient (e.g. TRPO [59], or more generally natural or mirror policy optimization [72, 2]) would result in quantitively similar findings.",
                "Recent work by Grill et al. [20] showed that the MuZero policy update approximates TRPO, making the learning algorithm implemented by these Dyna-style algorithms quite similar to that implemented by MuZero.",
                "[34] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",
                "For example, the \u201cLearn\u201d variant of MuZero is similar to Dyna-style methods that perform policy updates with TRPO [34, 39, 46, 52].",
                "There is also an interesting connection to recent Dyna-style MBRL algorithms via regularized policy optimization: specifically, these methods simulate data from the model and then update the policy on this data using TRPO [34, 39, 46, 52]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "8b5bc6b5e42b2ea39e65cb14c06d8a819a03a496",
                "externalIds": {
                    "MAG": "3102317072",
                    "DBLP": "journals/corr/abs-2011-04021",
                    "ArXiv": "2011.04021",
                    "CorpusId": 226281876
                },
                "corpusId": 226281876,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8b5bc6b5e42b2ea39e65cb14c06d8a819a03a496",
                "title": "On the role of planning in model-based deep reinforcement learning",
                "abstract": "Model-based planning is often thought to be necessary for deep, careful reasoning and generalization in artificial agents. While recent successes of model-based reinforcement learning (MBRL) with deep function approximation have strengthened this hypothesis, the resulting diversity of model-based methods has also made it difficult to track which components drive success and why. In this paper, we seek to disentangle the contributions of recent methods by focusing on three questions: (1) How does planning benefit MBRL agents? (2) Within planning, what choices drive performance? (3) To what extent does planning improve generalization? To answer these questions, we study the performance of MuZero (Schrittwieser et al., 2019), a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. We perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. Our results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization. These results indicate where and how to utilize planning in reinforcement learning settings, and highlight a number of open questions for future MBRL research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2158860",
                        "name": "Jessica B. Hamrick"
                    },
                    {
                        "authorId": "2231947",
                        "name": "A. Friesen"
                    },
                    {
                        "authorId": "145124447",
                        "name": "Feryal M. P. Behbahani"
                    },
                    {
                        "authorId": "35099444",
                        "name": "A. Guez"
                    },
                    {
                        "authorId": "47963165",
                        "name": "Fabio Viola"
                    },
                    {
                        "authorId": "2007790139",
                        "name": "Sims Witherspoon"
                    },
                    {
                        "authorId": "2072366312",
                        "name": "Thomas W. Anthony"
                    },
                    {
                        "authorId": "1981334",
                        "name": "Lars Buesing"
                    },
                    {
                        "authorId": "3444569",
                        "name": "Petar Velickovic"
                    },
                    {
                        "authorId": "143947744",
                        "name": "T. Weber"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In contrast, Kaiser et al. [17] use the MBPO [22] framework where policies are directly trained on the deep prediction model instead of the real world.",
                "[17] use the MBPO [22] framework where policies are directly trained on the deep prediction"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "65a00924cd94efd8d4bfd1c55f90f450d246a41a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-03922",
                    "ArXiv": "2011.03922",
                    "MAG": "3104395407",
                    "DOI": "10.1109/ICRA48506.2021.9561973",
                    "CorpusId": 226281698
                },
                "corpusId": 226281698,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/65a00924cd94efd8d4bfd1c55f90f450d246a41a",
                "title": "Learning World Transition Model for Socially Aware Robot Navigation",
                "abstract": "Moving in dynamic pedestrian environments is one of the important requirements for autonomous mobile robots. We present a model-based reinforcement learning approach for robots to navigate through crowded environments. The navigation policy is trained with both real interaction data from multi-agent simulation and virtual data from a deep transition model that predicts the evolution of surrounding dynamics of mobile robots. A reward function considering social conventions is designed to guide the training of the policy. Specifically, the policy model takes laser scan sequence and robot\u2019s own state as input and outputs steering command. The laser sequence is further transformed into stacked local obstacle maps disentangled from robot\u2019s ego motion to separate the static and dynamic obstacles, simplifying the model training. We observe that the policy using our method can be trained with significantly less real interaction data in simulator but achieve similar level of success rate in social navigation tasks compared with other methods. Experiments are conducted in multiple social scenarios both in simulation and on real robots, the learned policy can guide the robots to the final targets successfully in a socially compliant manner. Code is available at https://github.com/YuxiangCui/model-based-social-navigation.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2115440911",
                        "name": "Yuxiang Cui"
                    },
                    {
                        "authorId": "2135719210",
                        "name": "Haodong Zhang"
                    },
                    {
                        "authorId": "2118461398",
                        "name": "Yue Wang"
                    },
                    {
                        "authorId": "5738033",
                        "name": "R. Xiong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6b5e8a917cebbacc4937f6d033b12a7a5de683ca",
                "externalIds": {
                    "MAG": "3101189765",
                    "ArXiv": "2011.03615",
                    "DBLP": "journals/comsur/FerianiH21",
                    "DOI": "10.1109/COMST.2021.3063822",
                    "CorpusId": 226282141
                },
                "corpusId": 226282141,
                "publicationVenue": {
                    "id": "95d0dda7-5d58-4afd-b59f-315447b81992",
                    "name": "IEEE Communications Surveys and Tutorials",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Commun Surv Tutor"
                    ],
                    "issn": "1553-877X",
                    "url": "http://www.comsoc.org/cst",
                    "alternate_urls": [
                        "http://www.comsoc.org/livepubs/surveys/index.html",
                        "https://ieeexplore.ieee.org/servlet/opac?punumber=7041148",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=9739"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6b5e8a917cebbacc4937f6d033b12a7a5de683ca",
                "title": "Single and Multi-Agent Deep Reinforcement Learning for AI-Enabled Wireless Networks: A Tutorial",
                "abstract": "Deep Reinforcement Learning (DRL) has recently witnessed significant advances that have led to multiple successes in solving sequential decision-making problems in various domains, particularly in wireless communications. The next generation of wireless networks is expected to provide scalable, low-latency, ultra-reliable services empowered by the application of data-driven Artificial Intelligence (AI). The key enabling technologies of future wireless networks, such as intelligent meta-surfaces, aerial networks, and AI at the edge, involve more than one agent which motivates the importance of multi-agent learning techniques. Furthermore, cooperation is central to establishing self-organizing, self-sustaining, and decentralized networks. In this context, this tutorial focuses on the role of DRL with an emphasis on deep Multi-Agent Reinforcement Learning (MARL) for AI-enabled wireless networks. The first part of this paper will present a clear overview of the mathematical frameworks for single-agent RL and MARL. The main idea of this work is to motivate the application of RL beyond the model-free perspective which was extensively adopted in recent years. Thus, we provide a selective description of RL algorithms such as Model-Based RL (MBRL) and cooperative MARL and we highlight their potential applications in future wireless networks. Finally, we overview the state-of-the-art of MARL in fields such as Mobile Edge Computing (MEC), Unmanned Aerial Vehicles (UAV) networks, and cell-free massive MIMO, and identify promising future research directions. We expect this tutorial to stimulate more research endeavors to build scalable and decentralized systems based on MARL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "68974126",
                        "name": "Amal Feriani"
                    },
                    {
                        "authorId": "144158811",
                        "name": "E. Hossain"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fc4ba0b13ee4806894413c8d3d8e8649ecd97ebd",
                "externalIds": {
                    "ArXiv": "2011.03447",
                    "DBLP": "journals/mcss/PesarePF21",
                    "DOI": "10.1007/s00498-021-00294-y",
                    "CorpusId": 237404052
                },
                "corpusId": 237404052,
                "publicationVenue": {
                    "id": "1cf80dba-739d-4ddf-9354-fd28e0171f84",
                    "name": "MCSS. Mathematics of Control, Signals and Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Math Control Signal Syst",
                        "MC Math Control Signal Syst",
                        "Mathematics of Control, Signals, and Systems"
                    ],
                    "issn": "0932-4194",
                    "url": "https://www.springer.com/journal/498",
                    "alternate_urls": [
                        "http://www.springer.com/journal/498",
                        "https://link.springer.com/journal/498"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fc4ba0b13ee4806894413c8d3d8e8649ecd97ebd",
                "title": "Convergence results for an averaged LQR problem with applications to reinforcement learning",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "14832671",
                        "name": "A. Pesare"
                    },
                    {
                        "authorId": "47457447",
                        "name": "M. Palladino"
                    },
                    {
                        "authorId": "144704696",
                        "name": "M. Falcone"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1adf516a084b6e0df5572373b21ade8f96fb4d7d",
                "externalIds": {
                    "MAG": "3101875681",
                    "CorpusId": 226278459
                },
                "corpusId": 226278459,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1adf516a084b6e0df5572373b21ade8f96fb4d7d",
                "title": "A convergent approximation of the linear quadratic optimal control problem for Reinforcement Learning.",
                "abstract": "In this paper, we will deal with a Linear Quadratic Optimal Control problem with unknown dynamics. As a modeling assumption, we will suppose that the knowledge that an agent has on the current system is represented by a probability distribution $\\pi$ on the space of matrices. Furthermore, we will assume that such a probability measure is opportunely updated to take into account the increased experience that the agent obtains while exploring the environment, approximating with increasing accuracy the underlying dynamics. Under these assumptions, we will show that the optimal control obtained by solving the \"average\" Linear Quadratic Optimal Control problem with respect to a certain $\\pi$ converges to the optimal control driven related to the Linear Quadratic Optimal Control problem governed by the actual, underlying dynamics. This approach is closely related to Reinforcement Learning algorithms where prior and posterior probability distributions describing the knowledge on the uncertain system are recursively updated. In the last section, we will show a numerical test that confirms the theoretical results.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "14832671",
                        "name": "A. Pesare"
                    },
                    {
                        "authorId": "47457447",
                        "name": "M. Palladino"
                    },
                    {
                        "authorId": "144704696",
                        "name": "M. Falcone"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Modelbased RL algorithms have demonstrated significantly better sample-efficiency on the simulated Ant benchmark [24], [25]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b55ccadbe9973938c795add26512667945e7cd46",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-03085",
                    "MAG": "3106517126",
                    "CorpusId": 226278348
                },
                "corpusId": 226278348,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b55ccadbe9973938c795add26512667945e7cd46",
                "title": "RealAnt: An Open-Source Low-Cost Quadruped for Research in Real-World Reinforcement Learning",
                "abstract": "Current robot platforms available for research are either very expensive or unable to handle the abuse of exploratory controls in reinforcement learning. We develop RealAnt, a minimal low-cost physical version of the popular 'Ant' benchmark used in reinforcement learning. RealAnt costs only $410 in materials and can be assembled in less than an hour. We validate the platform with reinforcement learning experiments and provide baseline results on a set of benchmark tasks. We demonstrate that the TD3 algorithm can learn to walk the RealAnt from less than 45 minutes of experience. We also provide simulator versions of the robot (with the same dimensions, state-action spaces, and delayed noisy observations) in the MuJoCo and PyBullet simulators. We open-source hardware designs, supporting software, and baseline results for ease of reproducibility.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "22169323",
                        "name": "Rinu Boney"
                    },
                    {
                        "authorId": "79923770",
                        "name": "Jussi Sainio"
                    },
                    {
                        "authorId": "114638527",
                        "name": "M. Kaivola"
                    },
                    {
                        "authorId": "145410662",
                        "name": "A. Solin"
                    },
                    {
                        "authorId": "1776374",
                        "name": "Juho Kannala"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based RL algorithms have demonstrated significantly better sample-efficiency on the simulated Ant benchmark [25], [26]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a49d8fa196605760cbc07e4972d666a78242f2a1",
                "externalIds": {
                    "ArXiv": "2011.03085",
                    "CorpusId": 249395648
                },
                "corpusId": 249395648,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a49d8fa196605760cbc07e4972d666a78242f2a1",
                "title": "RealAnt: An Open-Source Low-Cost Quadruped for Education and Research in Real-World Reinforcement Learning",
                "abstract": "\u2014Current robot platforms available for research are either very expensive or unable to handle the abuse of exploratory controls in reinforcement learning. We develop RealAnt, a minimal low-cost physical version of the popular \u2018Ant\u2019 benchmark used in reinforcement learning. RealAnt costs only \u223c 350 A C ($410) in materials and can be assembled in less than an hour. We validate the platform with reinforcement learning experiments and provide baseline results on a set of benchmark tasks. We demonstrate that the RealAnt robot can learn to walk from scratch from less than 10 minutes of experience. We also provide simulator versions of the robot (with the same dimensions, state-action spaces, and delayed noisy observations) in the MuJoCo and PyBullet simulators. We open-source hardware designs, supporting software, and baseline results for educational use and reproducible research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "22169323",
                        "name": "Rinu Boney"
                    },
                    {
                        "authorId": "79923770",
                        "name": "Jussi Sainio"
                    },
                    {
                        "authorId": "114638527",
                        "name": "M. Kaivola"
                    },
                    {
                        "authorId": "145410662",
                        "name": "A. Solin"
                    },
                    {
                        "authorId": "1776374",
                        "name": "Juho Kannala"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For MBRL, black-box models have been widely adopted due to their generic applicability and simplicity [28]\u2013[30]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "81f348ead58c15c4cc9b444d27869283c299ba85",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-01734",
                    "MAG": "3095435181",
                    "ArXiv": "2011.01734",
                    "DOI": "10.1109/ICRA48506.2021.9561805",
                    "CorpusId": 226237495
                },
                "corpusId": 226237495,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/81f348ead58c15c4cc9b444d27869283c299ba85",
                "title": "Differentiable Physics Models for Real-world Offline Model-based Reinforcement Learning",
                "abstract": "A limitation of model-based reinforcement learning (MBRL) is the exploitation of errors in the learned models. Blackbox models can fit complex dynamics with high fidelity, but their behavior is undefined outside of the data distribution. Physics-based models are better at extrapolating, due to the general validity of their informed structure, but underfit in the real world due to the presence of unmodeled phenomena. In this work, we demonstrate experimentally that for the offline model-based reinforcement learning setting, physics-based models can be beneficial compared to high-capacity function approximators if the mechanical structure is known. Physics-based models can learn to perform the ball in a cup (BiC) task on a physical manipulator using only 4 minutes of sampled data using offline MBRL. We find that black-box models consistently produce unviable policies for BiC as all predicted trajectories diverge to physically impossible state, despite having access to more data than the physics-based model. In addition, we generalize the approach of physics parameter identification from modeling holonomic multi-body systems to systems with nonholonomic dynamics using end-to-end automatic differentiation.Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49199170",
                        "name": "M. Lutter"
                    },
                    {
                        "authorId": "17160667",
                        "name": "Johannes Silberbauer"
                    },
                    {
                        "authorId": "31349388",
                        "name": "Joe Watson"
                    },
                    {
                        "authorId": "145197867",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There is a natural tradeoff with -models: the higher is, the fewer model steps are needed to make long-horizon predictions, reducing model-based compounding prediction errors (Asadi et al., 2019; Janner et al., 2019).",
                ", 2017), MBPO (Janner et al., 2019), and MVE (Feinberg et al.",
                "Longer horizons carry more information, but present a more difficult prediction problem, as errors accumulate rapidly when a model is applied to its own previous outputs (Janner et al., 2019).",
                "We compare this approach to the state-of-the-art in model-based and model-free methods, with representative algorithms consisting of SAC, PPO (Schulman et al., 2017), MBPO (Janner et al., 2019), and MVE (Feinberg et al., 2018)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b85a70c16a1f961b42356469618faf279cdc98f1",
                "externalIds": {
                    "DBLP": "conf/nips/JannerML20",
                    "ArXiv": "2010.14496",
                    "MAG": "3098649479",
                    "CorpusId": 225075978
                },
                "corpusId": 225075978,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b85a70c16a1f961b42356469618faf279cdc98f1",
                "title": "\u03b3-Models: Generative Temporal Difference Learning for Infinite-Horizon Prediction",
                "abstract": "We introduce the $\\gamma$-model, a predictive model of environment dynamics with an infinite probabilistic horizon. Replacing standard single-step models with $\\gamma$-models leads to generalizations of the procedures that form the foundation of model-based control, including the model rollout and model-based value estimation. The $\\gamma$-model, trained with a generative reinterpretation of temporal difference learning, is a natural continuous analogue of the successor representation and a hybrid between model-free and model-based mechanisms. Like a value function, it contains information about the long-term future; like a standard predictive model, it is independent of task reward. We instantiate the $\\gamma$-model as both a generative adversarial network and normalizing flow, discuss how its training reflects an inescapable tradeoff between training-time and testing-time compounding errors, and empirically investigate its utility for prediction and control.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35163402",
                        "name": "Michael Janner"
                    },
                    {
                        "authorId": "2080746",
                        "name": "Igor Mordatch"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Overall, we believe our approach would further strengthen the understanding of dynamics generalization and could be useful to other relevant topics such as model-based policy optimization methods [15, 16].",
                "Such a learned dynamics model can be used as a simulator for model-free RL methods [16, 18, 40], providing a prior or additional features to a policy [9, 47], or planning ahead to select actions by predicting the future consequences of actions [1, 22, 42]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f5dcd9a94c49b8ee1941b99fc575068bd7431422",
                "externalIds": {
                    "DBLP": "conf/nips/SeoLGKSA20",
                    "ArXiv": "2010.13303",
                    "MAG": "3104444496",
                    "CorpusId": 225067752
                },
                "corpusId": 225067752,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f5dcd9a94c49b8ee1941b99fc575068bd7431422",
                "title": "Trajectory-wise Multiple Choice Learning for Dynamics Generalization in Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (RL) has shown great potential in various control tasks in terms of both sample-efficiency and final performance. However, learning a generalizable dynamics model robust to changes in dynamics remains a challenge since the target transition dynamics follow a multi-modal distribution. In this paper, we present a new model-based RL algorithm, coined trajectory-wise multiple choice learning, that learns a multi-headed dynamics model for dynamics generalization. The main idea is updating the most accurate prediction head to specialize each head in certain environments with similar dynamics, i.e., clustering environments. Moreover, we incorporate context learning, which encodes dynamics-specific information from past experiences into the context latent vector, enabling the model to perform online adaptation to unseen environments. Finally, to utilize the specialized prediction heads more effectively, we propose an adaptive planning method, which selects the most accurate prediction head over a recent experience. Our method exhibits superior zero-shot generalization performance across a variety of control tasks, compared to state-of-the-art RL methods. Source code and videos are available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2067714176",
                        "name": "Younggyo Seo"
                    },
                    {
                        "authorId": "3436470",
                        "name": "Kimin Lee"
                    },
                    {
                        "authorId": "15593386",
                        "name": "I. Clavera"
                    },
                    {
                        "authorId": "2765564",
                        "name": "Thanard Kurutach"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many previous work has focused on building better models to fit the data [9, 11, 13, 18, 20, 21], but ignoring the acquisition of diverse data, resulting in the learned model can only make good predictions for the limited state, lacking the ability to generalize the new state."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0ec8b4c485bb08eee00a2a54b123ecf0ad20b43a",
                "externalIds": {
                    "MAG": "3093582932",
                    "ArXiv": "2010.12914",
                    "DBLP": "journals/corr/abs-2010-12914",
                    "CorpusId": 225066715
                },
                "corpusId": 225066715,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0ec8b4c485bb08eee00a2a54b123ecf0ad20b43a",
                "title": "Planning with Exploration: Addressing Dynamics Bottleneck in Model-based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning is a framework in which an agent learns an environment model, makes planning and decision-making in this model, and finally interacts with the real environment. Model-based reinforcement learning has high sample efficiency compared with model-free reinforcement learning, and shows great potential in the real-world application. However, model-based reinforcement learning has been plagued by dynamics bottleneck. Dynamics bottleneck is the phenomenon that when the timestep to interact with the environment increases, the reward of the agent falls into the local optimum instead of increasing. In this paper, we analyze and explain how the coupling relationship between model and policy causes the dynamics bottleneck and shows improving the exploration ability of the agent can alleviate this issue. We then propose a new planning algorithm called Maximum Entropy Cross-Entropy Method (MECEM). MECEM can improve the agent's exploration ability by maximizing the distribution of action entropy in the planning process. We conduct experiments on fourteen well-recognized benchmark environments such as HalfCheetah, Ant and Swimmer. The results verify that our approach obtains the state-of-the-art performance on eleven benchmark environments and can effectively alleviate dynamics bottleneck on HalfCheetah, Ant and Walker2D.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48630770",
                        "name": "Xiyao Wang"
                    },
                    {
                        "authorId": "2086001",
                        "name": "Junge Zhang"
                    },
                    {
                        "authorId": "2564033",
                        "name": "Wenzhen Huang"
                    },
                    {
                        "authorId": "2397961",
                        "name": "Qiyue Yin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As they neither require accurate dynamics models nor try to explicitly fit them, the disadvantages of identifying models and quantifying their trustworthiness [4] are alleviated."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9a28a8ea9286123d1836619f76afcd1e619f0913",
                "externalIds": {
                    "DBLP": "conf/iros/HoppeG0T20",
                    "DOI": "10.1109/IROS45743.2020.9341390",
                    "CorpusId": 231914679
                },
                "corpusId": 231914679,
                "publicationVenue": {
                    "id": "37275deb-3fcf-4d16-ae77-95db9899b1f3",
                    "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "IROS",
                        "Intelligent Robots and Systems",
                        "Intell Robot Syst",
                        "IEEE/RJS Int Conf Intell Robot Syst"
                    ],
                    "url": "http://www.iros.org/"
                },
                "url": "https://www.semanticscholar.org/paper/9a28a8ea9286123d1836619f76afcd1e619f0913",
                "title": "Sample-Efficient Learning for Industrial Assembly using Qgraph-bounded DDPG",
                "abstract": "Recent progress in deep reinforcement learning has enabled agents to autonomously learn complex control strategies from scratch. Model-free approaches like Deep Deterministic Policy Gradients (DDPG) seem promising for applications with intricate dynamics, such as contact-rich manipulation tasks. However, these methods typically require large amounts of training data or meticulous hyperparameter tuning, limiting their usefulness for real-world robotics applications. In this paper, we evaluate and benchmark our recently proposed approach for improving model-free reinforcement learning with DDPG through Qgraph-based bounds in temporal difference learning. We directly apply the algorithm to a challenging real-world industrial insertion task and assess its performance (see https://youtu.be/Z_GcNbCWE-E). Empirical results show that the insertion task can be learned despite significant frictional forces and uncertainty, even in sparse-reward settings. We present an in-depth comparison based on a large number of experiments and demonstrate the advantages and performance of Qgraph-bounded DDPG: the learning process can be significantly sped up, robustified against bad choices of hyperparameters and runs with less memory requirements. Lastly, the presented results extend the current theoretical understanding of the link between data graph structure and soft divergence in DDPG.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144286396",
                        "name": "Sabrina Hoppe"
                    },
                    {
                        "authorId": "1907889",
                        "name": "Markus Giftthaler"
                    },
                    {
                        "authorId": "145369877",
                        "name": "R. Krug"
                    },
                    {
                        "authorId": "120284556",
                        "name": "M. Toussaint"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ilar results have appeared in [26], [27].",
                "Despite recent empirical advances in MBRL [26], [27], [38], [39], the performance of policy trained by MBRL significantly relies on the accuracy of an empirical model.",
                "the model bias, which is an improvement over the quadratic dependency in prior studies [26], [27].",
                "In the literature of MBRL [26], [27], we could assess the quality of the learned transition model Mu by the evaluation error of an arbitrary policy p, i.",
                "Many studies [26], [27], [31] have shown that if the empirical environment is trained with behavioral cloning (i.",
                "This environment imitation method suggests a promising application of GAIL for model-based reinforcement learning (MBRL) [3], [27].",
                "methods [26], [27], [30] learns the environment simply via fitting one-step transitions shown in Eq."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "83551ac1e6358182a2c0f2ec223fb3c6f736c8e1",
                "externalIds": {
                    "ArXiv": "2010.11876",
                    "DBLP": "conf/nips/XuLY20",
                    "MAG": "3104954646",
                    "DOI": "10.1109/TPAMI.2021.3096966",
                    "CorpusId": 225040357,
                    "PubMed": "34260348"
                },
                "corpusId": 225040357,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/83551ac1e6358182a2c0f2ec223fb3c6f736c8e1",
                "title": "Error Bounds of Imitating Policies and Environments for Reinforcement Learning",
                "abstract": "In sequential decision-making, imitation learning (IL) trains a policy efficiently by mimicking expert demonstrations. Various imitation methods were proposed and empirically evaluated, meanwhile, their theoretical understandings need further studies, among which the compounding error in long-horizon decisions is a major issue. In this paper, we first analyze the value gap between the expert policy and imitated policies by two imitation methods, behavioral cloning (BC) and generative adversarial imitation. The results support that generative adversarial imitation can reduce the compounding error compared to BC. Furthermore, we establish the lower bounds of IL under two settings, suggesting the significance of environment interactions in IL. By considering the environment transition model as a dual agent, IL can also be used to learn the environment model. Therefore, based on the bounds of imitating policies, we further analyze the performance of imitating environments. The results show that environment models can be more effectively imitated by generative adversarial imitation than BC. Particularly, we obtain a policy evaluation error that is linear with the effective planning horizon w.r.t. the model bias, suggesting a novel application of adversarial imitation for model-based reinforcement learning (MBRL). We hope these results could inspire future advances in IL and MBRL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40084973",
                        "name": "Tian Xu"
                    },
                    {
                        "authorId": "25841722",
                        "name": "Ziniu Li"
                    },
                    {
                        "authorId": "144705629",
                        "name": "Yang Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "13 to provide target values for the Q-networks, as in Janner et al. (2019).",
                "As model-based RL remains an active research area (Janner et al., 2019), we provide a proof-of-concept in this setting, using a learned deterministic model on HalfCheetah-v2 (see Appendix A.5)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b164a7dd0d6124e3a86df8a8e7374f9be8364033",
                "externalIds": {
                    "MAG": "3094567409",
                    "DBLP": "conf/nips/MarinoPIY21",
                    "ArXiv": "2010.10670",
                    "CorpusId": 224819491
                },
                "corpusId": 224819491,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b164a7dd0d6124e3a86df8a8e7374f9be8364033",
                "title": "Iterative Amortized Policy Optimization",
                "abstract": "Policy networks are a central feature of deep reinforcement learning (RL) algorithms for continuous control, enabling the estimation and sampling of high-value actions. From the variational inference perspective on RL, policy networks, when employed with entropy or KL regularization, are a form of amortized optimization, optimizing network parameters rather than the policy distributions directly. However, this direct amortized mapping can empirically yield suboptimal policy estimates. Given this perspective, we consider the more flexible class of iterative amortized optimizers. We demonstrate that the resulting technique, iterative amortized policy optimization, yields performance improvements over conventional direct amortization methods on benchmark continuous control tasks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2062768160",
                        "name": "Joseph Marino"
                    },
                    {
                        "authorId": "49504044",
                        "name": "Alexandre Pich\u00e9"
                    },
                    {
                        "authorId": "46403120",
                        "name": "Alessandro Davide Ialongo"
                    },
                    {
                        "authorId": "1740159",
                        "name": "Yisong Yue"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "372657f609f5a95b378a1aad7b08deb9b9b510c0",
                "externalIds": {
                    "DBLP": "conf/nips/0003ZZ020",
                    "ArXiv": "2010.09546",
                    "MAG": "3104329975",
                    "CorpusId": 224705902
                },
                "corpusId": 224705902,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/372657f609f5a95b378a1aad7b08deb9b9b510c0",
                "title": "Model-based Policy Optimization with Unsupervised Model Adaptation",
                "abstract": "Model-based reinforcement learning methods learn a dynamics model with real data sampled from the environment and leverage it to generate simulated data to derive an agent. However, due to the potential distribution mismatch between simulated data and real data, this could lead to degraded performance. Despite much effort being devoted to reducing this distribution mismatch, existing methods fail to solve it explicitly. In this paper, we investigate how to bridge the gap between real and simulated data due to inaccurate model estimation for better policy optimization. To begin with, we first derive a lower bound of the expected return, which naturally inspires a bound maximization algorithm by aligning the simulated and real data distributions. To this end, we propose a novel model-based reinforcement learning framework AMPO, which introduces unsupervised model adaptation to minimize the integral probability metric (IPM) between feature distributions from real and simulated data. Instantiating our framework with Wasserstein-1 distance gives a practical model-based approach. Empirically, our approach achieves state-of-the-art performance in terms of sample efficiency on a range of continuous control benchmark tasks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2115732606",
                        "name": "Jian Shen"
                    },
                    {
                        "authorId": "145034731",
                        "name": "Han Zhao"
                    },
                    {
                        "authorId": "2108309275",
                        "name": "Weinan Zhang"
                    },
                    {
                        "authorId": "1811427",
                        "name": "Yong Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides, in modelbased reinforcement learning, model deficiencies are typically handled by considering only short-term rollouts (Janner et al 2019) or by model predictive control (Nagabandi et al 2018)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "757a3c0e2a43f3d15b91c52fae32b081a6a66e3a",
                "externalIds": {
                    "ArXiv": "2010.04456",
                    "DBLP": "journals/corr/abs-2010-04456",
                    "MAG": "3092352028",
                    "DOI": "10.1088/1742-5468/ac3ae5",
                    "CorpusId": 222272443
                },
                "corpusId": 222272443,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/757a3c0e2a43f3d15b91c52fae32b081a6a66e3a",
                "title": "Augmenting physical models with deep networks for complex dynamics forecasting",
                "abstract": "Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling-based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists of decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model; no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefit generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction\u2013diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters. The code is available at https://github.com/yuan-yin/APHYNITY.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3965182",
                        "name": "V. Guen"
                    },
                    {
                        "authorId": "2109472874",
                        "name": "Yuan Yin"
                    },
                    {
                        "authorId": "1853488882",
                        "name": "J\u00e9r\u00e9mie Don\u00e0"
                    },
                    {
                        "authorId": "10771473",
                        "name": "Ibrahim Ayed"
                    },
                    {
                        "authorId": "2065044561",
                        "name": "Emmanuel de B'ezenac"
                    },
                    {
                        "authorId": "1728523",
                        "name": "Nicolas Thome"
                    },
                    {
                        "authorId": "1741426",
                        "name": "P. Gallinari"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based policy optimization (MBPO) [43] uses a probabilistic model ensemble and performs a large amount of short model rollouts that start from a state distribution with states from the real environment dynamics."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d0f68a31f9ecc3c51185fc7bae6b0629d3ef984c",
                "externalIds": {
                    "DBLP": "conf/icstcc/PalL20",
                    "MAG": "3106764401",
                    "DOI": "10.1109/ICSTCC50638.2020.9259716",
                    "CorpusId": 227221729
                },
                "corpusId": 227221729,
                "publicationVenue": {
                    "id": "0e5fac1a-c8c9-4b55-a332-5123949e7ab4",
                    "name": "International Conference on System Theory, Control and Computing",
                    "type": "conference",
                    "alternate_names": [
                        "ICSTCC",
                        "Int Conf Syst Theory Control Comput"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d0f68a31f9ecc3c51185fc7bae6b0629d3ef984c",
                "title": "Brief Survey of Model-Based Reinforcement Learning Techniques",
                "abstract": "Model-free reinforcement learning (MFRL) usually has better asymptotic performance than the model-based reinforcement (MBRL) learning algorithms, especially in complex environments. But MBRL algorithms are very often much more sample-efficient, and sometimes are able to learn control tasks in just a handful of trials. In addition, in some domains, the MBRL algorithms can reach the MFRL performance with better sample efficiency. In recent years, MBRL research has increased in various application domains, such as robot control tasks, or game environments with complex observations. In this paper, we review the most popular techniques used in MBRL and look at some useful classification of algorithms in this area.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1821759012",
                        "name": "Constantin-Valentin Pal"
                    },
                    {
                        "authorId": "144147338",
                        "name": "F. Leon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Generating data that are not so far away from ones in the batch prevents the accumulation of model error, but this theoretical aspect, even if already mentioned in (Janner et al. 2019), should not affect the bound that aims to be valid on any uncertainty penalized MDP independently of other factors."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ee325ec0e919dd7216f990e7fa6525419a806b67",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-01931",
                    "ArXiv": "2010.01931",
                    "MAG": "3092600649",
                    "CorpusId": 222132838
                },
                "corpusId": 222132838,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ee325ec0e919dd7216f990e7fa6525419a806b67",
                "title": "Offline Learning for Planning: A Summary",
                "abstract": "The training of autonomous agents often requires expensive and unsafe trial-and-error interactions with the environment. Nowadays several data sets containing recorded experiences of intelligent agents performing various tasks, spanning from the control of unmanned vehicles to human-robot interaction and medical applications are accessible on the internet. With the intention of limiting the costs of the learning procedure it is convenient to exploit the information that is already available rather than collecting new data. Nevertheless, the incapability to augment the batch can lead the autonomous agents to develop far from optimal behaviours when the sampled experiences do not allow for a good estimate of the true distribution of the environment. Offline learning is the area of machine learning concerned with efficiently obtaining an optimal policy with a batch of previously collected experiences without further interaction with the environment. In this paper we adumbrate the ideas motivating the development of the state-of-the-art offline learning baselines. The listed methods consist in the introduction of epistemic uncertainty dependent constraints during the classical resolution of a Markov Decision Process, with and without function approximators, that aims to alleviate the bad effects of the distributional mismatch between the available samples and real world. We provide comments on the practical utility of the theoretical bounds that justify the application of these algorithms and suggest the utilization of Generative Adversarial Networks to estimate the distributional shift that affects all of the proposed model-free and model-based approaches.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "77741750",
                        "name": "Giorgio Angelotti"
                    },
                    {
                        "authorId": "2387103",
                        "name": "Nicolas Drougard"
                    },
                    {
                        "authorId": "2033916",
                        "name": "Caroline Ponzoni Carvalho Chanel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[27] The authors empirically show that MBPO performs significantly better in continuous control tasks compared to previous methods.",
                "Furthermore, the authors show that as long as they can improve the C, the performance will increase monotonically [27]."
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "bb6ec4c47be59bdf6d807ba8768509719ca4217f",
                "externalIds": {
                    "MAG": "3036501456",
                    "DBLP": "journals/isci/AndersenGG20",
                    "DOI": "10.1016/j.ins.2020.06.010",
                    "CorpusId": 220975783
                },
                "corpusId": 220975783,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/bb6ec4c47be59bdf6d807ba8768509719ca4217f",
                "title": "Towards safe reinforcement-learning in industrial grid-warehousing",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "33999307",
                        "name": "Per-Arne Andersen"
                    },
                    {
                        "authorId": "1833672",
                        "name": "M. G. Olsen"
                    },
                    {
                        "authorId": "2493161",
                        "name": "Ole-Christoffer Granmo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MBPO (Janner et al. 2019) as well as BMPO (Lai et al.",
                "MBPO (Janner et al. 2019) as well as BMPO (Lai et al. 2020) combines model ensembles with short model rollouts for sufficient policy optimization."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f18cd315123f59f6d168188019071b582bf409b6",
                "externalIds": {
                    "MAG": "3087200438",
                    "DBLP": "journals/corr/abs-2009-09593",
                    "ArXiv": "2009.09593",
                    "CorpusId": 221818736
                },
                "corpusId": 221818736,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f18cd315123f59f6d168188019071b582bf409b6",
                "title": "Dynamic Horizon Value Estimation for Model-based Reinforcement Learning",
                "abstract": "Existing model-based value expansion methods typically leverage a world model for value estimation with a fixed rollout horizon to assist policy learning. However, the fixed rollout with an inaccurate model has a potential to harm the learning process. In this paper, we investigate the idea of using the model knowledge for value expansion adaptively. We propose a novel method called Dynamic-horizon Model-based Value Expansion (DMVE) to adjust the world model usage with different rollout horizons. Inspired by reconstruction-based techniques that can be applied for visual data novelty detection, we utilize a world model with a reconstruction module for image feature extraction, in order to acquire more precise value estimation. The raw and the reconstructed images are both used to determine the appropriate horizon for adaptive value expansion. On several benchmark visual control tasks, experimental results show that DMVE outperforms all baselines in sample efficiency and final performance, indicating that DMVE can achieve more effective and accurate value estimation than state-of-the-art model-based methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2120827853",
                        "name": "Junjie Wang"
                    },
                    {
                        "authorId": "3342918",
                        "name": "Qichao Zhang"
                    },
                    {
                        "authorId": "1699234",
                        "name": "Dongbin Zhao"
                    },
                    {
                        "authorId": "3386549",
                        "name": "Mengchen Zhao"
                    },
                    {
                        "authorId": "40513470",
                        "name": "Jianye Hao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(7) is for stochastic transitions, but the experiments in Janner et al. (2019) used deterministic transitions.",
                "Such a suggestion on branched length (or equivalently, the branched discount factor \u03b2) supports the experimental success of Janner et al. (2019) and their choice of hyperparameter, as mentioned in the last paragraph of \u00a7 4.2.",
                "3 in (Janner et al., 2019), branched rollouts of length k satisfy",
                "To mitigate the cumulative reward error, Janner et al. (2019) experimentally shows that branched rollouts (short model rollouts initialized by previous real rollouts) help reduce this error and improve experimental results.",
                "Most prior error analyses impose a strong assumption in their proofs; e.g., Lipschitz value function (Luo et al., 2019; Xiao et al., 2019; Yu et al., 2020) or maximum model error (Janner et al., 2019).",
                "A major contribution of Janner et al. (2019) is the use of branched rollouts generated by (\u03c1\u03c0DT , \u03c0, T\u0302 ).",
                "Also, this result is for deterministic transitions, so this resolves an open issue in Janner et al. (2019), as they proved for stochastic transitions but experimented with deterministic transitions.",
                "In addition, we enhance the results of Janner et al. (2019), by showing their constants \u201cin maxima\u201d can be replaced by constants \u201cin expectation\u201d.",
                "By Theorem 4.3 in (Janner et al., 2019), branched rollouts of length k satisfy\nR(\u03c0, T )\u2212Rbranch(\u03c0) \u2265\u22122rmax [ \u03b3k+1 \u03c0\n(1\u2212 \u03b3)2 + \u03b3k \u03c0 1\u2212 \u03b3 + k m 1\u2212 \u03b3\n] , (7)\nwith the same constants as Eq.",
                "Such closeness of policies is commonly used in the literature (Luo et al., 2019; Janner et al., 2019; Yu et al., 2020).",
                "Prior work has done extensive experiments on branched rollouts (Janner et al., 2019), Generative Adversarial Imitation Learning (Ho and Ermon, 2016) and the Ensemble Method (Kurutach et al., 2018).",
                "Prior work has done extensive experiments on branched rollouts (Janner et al., 2019), Generative Adversarial Imitation Learning (Ho and Ermon, 2016) and the Ensemble Method (Kurutach et al.",
                "However, the effectiveness of branched rollouts remains unclear since the experiments of Janner et al. (2019) use deterministic transitions (MuJoCo (Todorov et al., 2012))."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7be93daa9412136af577f0e55c7398aad524e27d",
                "externalIds": {
                    "DBLP": "conf/aistats/FanR21",
                    "ArXiv": "2009.08586",
                    "MAG": "3087571317",
                    "CorpusId": 221802544
                },
                "corpusId": 221802544,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7be93daa9412136af577f0e55c7398aad524e27d",
                "title": "A Contraction Approach to Model-based Reinforcement Learning",
                "abstract": "Model-based Reinforcement Learning has shown considerable experimental success. However, a theoretical understanding of it is still lacking. To this end, we analyze the error in cumulative reward for both stochastic and deterministic transitions using a contraction approach. We show that this approach doesn't require strong assumptions and can recover the typical quadratic error to the horizon. We prove that branched rollouts can reduce this error and are essential for deterministic transitions to have a Bellman contraction. Our results also apply to Imitation Learning, where we prove that GAN-type learning is better than Behavioral Cloning in continuous state and action spaces.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "32037089",
                        "name": "Ting-Han Fan"
                    },
                    {
                        "authorId": "1693135",
                        "name": "P. Ramadge"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based reinforcement learning (MBRL) has emerged as a functional candidate for robotic control in a data-efficient manner [1]\u2013[3], [29] \u2013 this letter extends the functionality of MBRL to nonholonomic planning for flying robots."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6b51f78305222a8efc63d26a48cd6272a2783a7a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2009-01221",
                    "MAG": "3082127065",
                    "ArXiv": "2009.01221",
                    "DOI": "10.1109/LRA.2020.3045930",
                    "CorpusId": 221446357
                },
                "corpusId": 221446357,
                "publicationVenue": {
                    "id": "93c335b7-edf4-45f5-8ddc-7c5835154945",
                    "name": "IEEE Robotics and Automation Letters",
                    "alternate_names": [
                        "IEEE Robot Autom Lett"
                    ],
                    "issn": "2377-3766",
                    "url": "https://www.ieee.org/membership-catalog/productdetail/showProductDetailPage.html?product=PER481-ELE",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=7083369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6b51f78305222a8efc63d26a48cd6272a2783a7a",
                "title": "Nonholonomic Yaw Control of an Underactuated Flying Robot With Model-Based Reinforcement Learning",
                "abstract": "Nonholonomic control is a candidate to control nonlinear systems with path-dependant states. We investigate an underactuated flying micro-aerial-vehicle, the ionocraft, that requires nonholonomic control in the yaw-direction for complete attitude control. Deploying an analytical control law involves substantial engineering design and is sensitive to inaccuracy in the system model. With specific assumptions on assembly and system dynamics, we derive a Lie bracket for yaw control of the ionocraft. As a comparison to the significant engineering effort required for an analytic control law, we implement a data-driven model-based reinforcement learning yaw controller in a simulated flight task. We demonstrate that a simple model-based reinforcement learning framework can match the derived Lie bracket control \u2013 in yaw rate and chosen actions \u2013 in a few minutes of flight data, without a pre-defined dynamics function. This letter shows that learning-based approaches are useful as a tool for synthesis of nonlinear control laws previously only addressable through expert-based design.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2052363815",
                        "name": "Nathan Lambert"
                    },
                    {
                        "authorId": "32597226",
                        "name": "Craig B. Schindler"
                    },
                    {
                        "authorId": "22297917",
                        "name": "Daniel S. Drew"
                    },
                    {
                        "authorId": "143648343",
                        "name": "K. Pister"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Dyna-Q (Sutton, 1990) is an early method which uses the model exclusively for 3, followed more recently by ME-TRPO (Kurutach et al., 2018) and MBPO (Janner et al., 2019).",
                "Three of the works we compared to in the previous section, Chua et al. (2018), Wang and Ba (2019), and Janner et al. (2019), all relied on very similar implementations of bootstrapped ensembles of fully-connected networks to model environment dynamics.",
                "Of particular interest is the first combination, since it bears close resemblance to a similar ablation performed in Janner et al. (2019).",
                "We evaluate SAC-SVG(H) on all of the MuJoCo (Todorov et al., 2012) locomotion experiments considered by POPLIN, MBPO, and STEVE, which are the most recent state-of-the-art related approaches that use model-based rollouts.",
                "Interestingly, even though the single recurrent model overfits much more heavily to the training data, the asymptotic reward of our humanoid agent is significantly higher and qualitatively different than that reported in Janner et al. (2019).",
                "The approaches in MBPO and STEVE are complimentary to ours as MBPO augments the replay buffer with imagined model rollouts and STEVE would improve the critic target.",
                "Model-based RL theoretical results often start with an assumed bound on the model error and proceed to establish a bound on the resulting error in the infinite-horizon model value expansion (Luo et al., 2018; Janner et al., 2019).",
                "In contrast, MBPO periodically trained the dynamics model to convergence on the full replay buffer, generated a large batch of fictional transitions, and proceeded to repeatedly update the actor/critic models on those stored transitions.",
                ", 2015), and show that with the addition of an entropy term to encourage exploration they yield competitive policies in comparison to more recent model-based agents (Buckman et al., 2018; Wang and Ba, 2019; Janner et al., 2019).",
                "\u2026expert domain knowledge (Todorov et al., 2012), and often find higher reward policies than their model-free counterparts when only a small amount of ground-truth data can be collected (Deisenroth and Rasmussen, 2011; Chua et al., 2018; Wang and Ba, 2019; Janner et al., 2019; Kaiser et al., 2019).",
                ", 2012), and often find higher reward policies than their model-free counterparts when only a small amount of ground-truth data can be collected (Deisenroth and Rasmussen, 2011; Chua et al., 2018; Wang and Ba, 2019; Janner et al., 2019; Kaiser et al., 2019).",
                "\u2026known family of methods, stochastic value gradients (SVG) (Heess et al., 2015), and show that with the addition of an entropy term to encourage exploration they yield competitive policies in comparison to more recent model-based agents (Buckman et al., 2018; Wang and Ba, 2019; Janner et al., 2019).",
                ", 2018) G MF+rollout data MF+rollout data Det NN Yes Proprio MBPO (Janner et al., 2019) G MF+rollout data MF+rollout data Prob NN Yes Proprio SAC (Haarnoja et al.",
                "We found that running the public MBPO code produces an agent that simply collects the keep-alive bonus by standing stock-still.",
                "Videos of our trained agents are available at sites.google.com/view/2020-svg.\nFigure 2 shows our results in comparison to MBPO and STEVE, which evaluate on the MuJoCo tasks in the OpenAI gym (Brockman et al., 2016) that are mostly the standard v2 tasks with early termination and alive bonuses, and with a truncated observation space for the humanoid and ant that discards the inertial measurement units."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5e06f64e91f4f48665c7407ea6eac74f93d5d0d0",
                "externalIds": {
                    "ArXiv": "2008.12775",
                    "DBLP": "journals/corr/abs-2008-12775",
                    "MAG": "3082349339",
                    "CorpusId": 221370875
                },
                "corpusId": 221370875,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5e06f64e91f4f48665c7407ea6eac74f93d5d0d0",
                "title": "On the model-based stochastic value gradient for continuous reinforcement learning",
                "abstract": "Model-based reinforcement learning approaches add explicit domain knowledge to agents in hopes of improving the sample-efficiency in comparison to model-free agents. However, in practice model-based methods are unable to achieve the same asymptotic performance on challenging continuous control tasks due to the complexity of learning and controlling an explicit world model. In this paper we investigate the stochastic value gradient (SVG), which is a well-known family of methods for controlling continuous systems which includes model-based approaches that distill a model-based value expansion into a model-free policy. We consider a variant of the model-based SVG that scales to larger systems and uses 1) an entropy regularization to help with exploration, 2) a learned deterministic world model to improve the short-horizon value estimate, and 3) a learned model-free value estimate after the model's rollout. This SVG variation captures the model-free soft actor-critic method as an instance when the model rollout horizon is zero, and otherwise uses short-horizon model rollouts to improve the value estimate for the policy update. We surpass the asymptotic performance of other model-based methods on the proprioceptive MuJoCo locomotion tasks from the OpenAI gym, including a humanoid. We notably achieve these results with a simple deterministic world model without requiring an ensemble.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1773498",
                        "name": "Brandon Amos"
                    },
                    {
                        "authorId": "2067201658",
                        "name": "S. Stanton"
                    },
                    {
                        "authorId": "13759615",
                        "name": "Denis Yarats"
                    },
                    {
                        "authorId": "145771261",
                        "name": "A. Wilson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In spite of the recent developments in machine learning to deal with complex problems [1\u20139], RL algorithms often have a hard time learning simple tasks that efficiently learnt by animals [10]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bc98b3547f7ee83ed121f1408ca5de17b7ab3836",
                "externalIds": {
                    "MAG": "3080275305",
                    "DOI": "10.1101/2020.08.21.260844",
                    "CorpusId": 221355719
                },
                "corpusId": 221355719,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bc98b3547f7ee83ed121f1408ca5de17b7ab3836",
                "title": "Biological Reinforcement Learning via Predictive Spacetime Encoding",
                "abstract": "Recent advances in reinforcement learning (RL) have successfully addressed several challenges, such as performance, scalability, or sample efficiency associated with the use of this technology. Although RL algorithms bear relevance to psychology and neuroscience in a broader context, they lack biological plausibility. Motivated by recent neural findings demonstrating the capacity of the hippocampus and prefrontal cortex to gather space and time information from the environment, this study presents a novel RL model, called spacetime Q-Network (STQN), that exploits predictive spatiotemporal encoding to reliably learn highly uncertain environment. The proposed method consists of two primary components. The first component is the successor representation with theta phase precession implements hippocampal spacetime encoding, acting as a rollout prediction. The second component, called Q switch ensemble, implements prefrontal population coding for reliable reward prediction. We also implement a single learning rule to accommodate both hippocampal-prefrontal replay and synaptic homeostasis, which subserves confidence-based metacognitive learning. To demonstrate the capacity of our model, we design a task array simulating various levels of environmental uncertainty and complexity. Results show that our model significantly outperforms a few state-of-the-art RL models. In the subsequent ablation study, we showed unique contributions of each component to resolving task uncertainty and complexity. Our study has two important implications. First, it provides the theoretical groundwork for closely linking unique characteristics of the distinct brain regions in the context of RL. Second, our implementation is performed in a simple matrix form that accommodates expansion into biologically-plausible, highly-scalable, and generalizable neural architectures.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "122694453",
                        "name": "M. Yang"
                    },
                    {
                        "authorId": "82536939",
                        "name": "J. Lee"
                    },
                    {
                        "authorId": "66152796",
                        "name": "Sang Wan Lee"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e2658b9abcbc170ae71744c7f27c0e6e27256ce2",
                "externalIds": {
                    "DBLP": "conf/corl/SikchiZH21",
                    "ArXiv": "2008.10066",
                    "MAG": "3080156643",
                    "CorpusId": 221092463
                },
                "corpusId": 221092463,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e2658b9abcbc170ae71744c7f27c0e6e27256ce2",
                "title": "Learning Off-Policy with Online Planning",
                "abstract": "We propose Learning Off-Policy with Online Planning (LOOP), combining the techniques from model-based and model-free reinforcement learning algorithms. The agent learns a model of the environment, and then uses trajectory optimization with the learned model to select actions. To sidestep the myopic effect of fixed horizon trajectory optimization, a value function is attached to the end of the planning horizon. This value function is learned through off-policy reinforcement learning, using trajectory optimization as its behavior policy. Furthermore, we introduce \"actor-guided\" trajectory optimization to mitigate the actor-divergence issue in the proposed method. We benchmark our methods on continuous control tasks and demonstrate that it offers a significant improvement over the underlying model-based and model-free algorithms.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51521430",
                        "name": "Harshit S. Sikchi"
                    },
                    {
                        "authorId": "2118882066",
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "authorId": "145641013",
                        "name": "David Held"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is similar to approaches used in MBPO (Janner et al., 2019) and DREAMER (Hafner et al.",
                ", 2020) and MBPO (Janner et al., 2019), with values taken from the MOPO paper (Yu et al."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "3cb8e96faba73efa027fa858e2a78cd1fc3c6e4d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2008-05556",
                    "MAG": "3048825312",
                    "ArXiv": "2008.05556",
                    "CorpusId": 221112471
                },
                "corpusId": 221112471,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3cb8e96faba73efa027fa858e2a78cd1fc3c6e4d",
                "title": "Model-Based Offline Planning",
                "abstract": "Offline learning is a key part of making reinforcement learning (RL) useable in real systems. Offline RL looks at scenarios where there is data from a system's operation, but no direct access to the system when learning a policy. Recent work on training RL policies from offline data has shown results both with model-free policies learned directly from the data, or with planning on top of learnt models of the data. Model-free policies tend to be more performant, but are more opaque, harder to command externally, and less easy to integrate into larger systems. We propose an offline learner that generates a model that can be used to control the system directly through planning. This allows us to have easily controllable policies directly from data, without ever interacting with the system. We show the performance of our algorithm, Model-Based Offline Planning (MBOP) on a series of robotics-inspired tasks, and demonstrate its ability leverage planning to respect environmental constraints. We are able to find near-optimal polices for certain simulated systems from as little as 50 seconds of real-time system interaction, and create zero-shot goal-conditioned policies on a series of environments.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1877004999",
                        "name": "Arthur Argenson"
                    },
                    {
                        "authorId": "1387885286",
                        "name": "Gabriel Dulac-Arnold"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018] Pix2pix MCTS + - Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts + - Cheetah Video-prediction [Oh et al.",
                "\u2026et al., 2018, Chua et al., 2018, Clavera et al., 2018, Feinberg et al., 2018, Gu et al., 2016, Hafner et al., 2018, Heess et al., 2015, Janner et al., 2019] and other robotics tasks [Finn and Levine, 2017, Hafner et al., 2019, Levine and Abbeel, 2014, Sekar et al., 2020, Tassa et al.,\u2026",
                "\u2026Policy [Clavera et al., 2018] Meta-ensembles Short rollouts + - Cheetah GATS [Azizzadenesheli et al., 2018] Pix2pix MCTS + - Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts + - Cheetah Video-prediction [Oh et al., 2015] CNN/LSTM Action + + Atari VPN [Oh et al., 2017] CNN\u2026",
                "propose in Modelbased Policy Optimization (MBPO) a new approach to short rollouts with ensembles [Janner et al., 2019].",
                "The results reported indicate that meta-learning a policy over an ensemble of learned models approaches the level of performance of model-free methods with\n10\nApproach Learning Planning Reinforcement Learning Application Local Model [Gu et al., 2016] Quadratic Non-linear Short rollouts Q-learning Cheetah MVE [Feinberg et al., 2018] Samples Short rollouts Actor-critic Cheetah Meta Policy [Clavera et al., 2018] Meta-ensembles Short rollouts Policy optimization Cheetah GATS [Azizzadenesheli et al., 2018] Pix2pix MCTS Deep Q Network Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts Soft-Actor-Critic Cheetah Video predict [Oh et al., 2015] CNN/LSTM Action Curriculum Atari VPN [Oh et al., 2017] CNN encoder d-step k-step Mazes, Atari SimPLe [Kaiser et al., 2019] VAE, LSTM MPC PPO Atari\nTABLE 4 Overview of Hybrid Model-Free/Model-based Imagination Methods\nsubstantially better sample complexity.",
                "\u2026Meta-ensembles Short rollouts Policy optimization Cheetah GATS [Azizzadenesheli et al., 2018] Pix2pix MCTS Deep Q Network Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts Soft-Actor-Critic Cheetah Video predict [Oh et al., 2015] CNN/LSTM Action Curriculum Atari VPN [Oh et\u2026",
                "Janner et al. propose in Modelbased Policy Optimization (MBPO) a new approach to short rollouts with ensembles [Janner et al., 2019].",
                "3.2) GPS [Levine and Abbeel, 2014] iLQG Trajectory - - Swimmer\nSVG [Heess et al., 2015] Value Gradients Trajectory - - Swimmer PETS [Chua et al., 2018] Uncertainty Ensemble MPC - - Cheetah Visual Foresight [Finn and Levine, 2017] Video Prediction MPC - - Manipulation Local Model [Gu et al., 2016] Quadratic Non-linear Short rollouts + - Cheetah MVE [Feinberg et al., 2018] Samples Short rollouts + - Cheetah Meta Policy [Clavera et al., 2018] Meta-ensembles Short rollouts + - Cheetah GATS [Azizzadenesheli et al., 2018] Pix2pix MCTS + - Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts + - Cheetah Video-prediction [Oh et al., 2015] CNN/LSTM Action + + Atari VPN [Oh et al., 2017] CNN encoder d-step + + Atari SimPLe [Kaiser et al., 2019] VAE, LSTM MPC + + Atari PlaNet [Hafner et al., 2018] RSSM (VAE/RNN) CEM - + Cheetah Dreamer [Hafner et al., 2019] RSSM+CNN Imagine - + Hopper Plan2Explore [Sekar et al., 2020] RSSM Planning - + Hopper\nEnd-to-End Learning VIN [Tamar et al., 2016] CNN Rollout in network + - Mazes Planning & Transitions VProp [Nardelli et al., 2018] CNN Hierarch Rollouts + - Maze, nav (Sect.",
                ", 2018] Pix2pix MCTS Deep Q Network Cheetah Policy Optim [Janner et al., 2019] Ensemble Short rollouts Soft-Actor-Critic Cheetah Video predict [Oh et al."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c777ec0e47d6347e536ef1da4c455873109f2c62",
                "externalIds": {
                    "ArXiv": "2008.05598",
                    "DBLP": "journals/corr/abs-2008-05598",
                    "MAG": "3048681560",
                    "CorpusId": 221112258
                },
                "corpusId": 221112258,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c777ec0e47d6347e536ef1da4c455873109f2c62",
                "title": "Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey",
                "abstract": "Deep reinforcement learning has shown remarkable success in the past few years. Highly complex sequential decision making problems have been solved in tasks such as game playing and robotics. Unfortunately, the sample complexity of most deep reinforcement learning methods is high, precluding their use in some important applications. Model-based reinforcement learning creates an explicit model of the environment dynamics to reduce the need for environment samples. Current deep learning methods use high-capacity networks to solve high-dimensional problems. Unfortunately, high-capacity models typically require many samples, negating the potential benefit of lower sample complexity in model-based methods. A challenge for deep model-based methods is therefore to achieve high predictive power while maintaining low sample complexity. In recent years, many model-based methods have been introduced to address this challenge. In this paper, we survey the contemporary model-based landscape. First we discuss definitions and relations to other fields. We propose a taxonomy based on three approaches: using explicit planning on given transitions, using explicit planning on learned transitions, and end-to-end learning of both planning and transitions. We use these approaches to organize a comprehensive overview of important recent developments such as latent models. We describe methods and benchmarks, and we suggest directions for future work for each of the approaches. Among promising research directions are curriculum learning, uncertainty modeling, and use of latent models for transfer learning.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2562595",
                        "name": "A. Plaat"
                    },
                    {
                        "authorId": "1680388",
                        "name": "W. Kosters"
                    },
                    {
                        "authorId": "1950379",
                        "name": "M. Preuss"
                    }
                ]
            }
        },
        {
            "contexts": [
                " with zero mean and w = diag(4 10 4;0:1;1 10 8), v = diag(4 10 5;0:01;1 10 9). The same uncertainty for the initial conditions is assumed. Bayesian frameworks have been used in reinforcement learning [45, 46] as they can model both the epistemic and aleatoric uncertainty (in Gaussian processes, usually homoscedastic aleatoric uncertainty is considered). One of the fundamental steps here is the propagation"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1e56922ff44b542c7fd66ecbe99144c9823fd9ad",
                "externalIds": {
                    "MAG": "3046758693",
                    "ArXiv": "2008.00030",
                    "DBLP": "journals/corr/abs-2008-00030",
                    "DOI": "10.1016/j.jprocont.2022.01.003",
                    "CorpusId": 220936436
                },
                "corpusId": 220936436,
                "publicationVenue": {
                    "id": "40d78211-eba2-4757-9427-598f366f3fa9",
                    "name": "Journal of Process Control",
                    "type": "journal",
                    "alternate_names": [
                        "J Process Control"
                    ],
                    "issn": "0959-1524",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/30445/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09591524"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1e56922ff44b542c7fd66ecbe99144c9823fd9ad",
                "title": "Chance Constrained Policy Optimization for Process Control and Optimization",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51221068",
                        "name": "Panagiotis Petsagkourakis"
                    },
                    {
                        "authorId": "2065434808",
                        "name": "I. Sandoval"
                    },
                    {
                        "authorId": "153891542",
                        "name": "E. Bradford"
                    },
                    {
                        "authorId": "2343546",
                        "name": "F. Galvanin"
                    },
                    {
                        "authorId": "5090322",
                        "name": "Dongda Zhang"
                    },
                    {
                        "authorId": "1401647281",
                        "name": "E. A. Rio-Chanona"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based RL algorithms build world model(s) to predict the future conditioned on the past and actions, and then act through planning [6, 14, 17, 18, 23, 25, 41]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4470a1cb348dc15bcec0020930dec6a1d3cb4376",
                "externalIds": {
                    "DBLP": "conf/nips/LeeFLGLCG20",
                    "ArXiv": "2007.12401",
                    "MAG": "3099621306",
                    "CorpusId": 220769202
                },
                "corpusId": 220769202,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4470a1cb348dc15bcec0020930dec6a1d3cb4376",
                "title": "Predictive Information Accelerates Learning in RL",
                "abstract": "The Predictive Information is the mutual information between the past and the future, I(X_past; X_future). We hypothesize that capturing the predictive information is useful in RL, since the ability to model what will happen next is necessary for success on many tasks. To test our hypothesis, we train Soft Actor-Critic (SAC) agents from pixels with an auxiliary task that learns a compressed representation of the predictive information of the RL environment dynamics using a contrastive version of the Conditional Entropy Bottleneck (CEB) objective. We refer to these as Predictive Information SAC (PI-SAC) agents. We show that PI-SAC agents can substantially improve sample efficiency over challenging baselines on tasks from the DM Control suite of continuous control environments. We evaluate PI-SAC agents by comparing against uncompressed PI-SAC agents, other compressed and uncompressed agents, and SAC agents directly trained from pixels. Our implementation is given on GitHub.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1863953",
                        "name": "Kuang-Huei Lee"
                    },
                    {
                        "authorId": "2057616290",
                        "name": "Ian Fischer"
                    },
                    {
                        "authorId": "1738283984",
                        "name": "Anthony Z. Liu"
                    },
                    {
                        "authorId": "1857914",
                        "name": "Yijie Guo"
                    },
                    {
                        "authorId": "1697141",
                        "name": "Honglak Lee"
                    },
                    {
                        "authorId": "1729041",
                        "name": "J. Canny"
                    },
                    {
                        "authorId": "1687120",
                        "name": "S. Guadarrama"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The choice of pairing states with on-policy actions to form hypothetical experiences has been reported to be beneficial [Gu et al., 2016, Pan et al., 2018, Janner et al., 2019].",
                "Existing works show that smart search-control strategies can further improve sample efficiency of a Dyna agent [Sutton et al., 2008, Gu et al., 2016, Goyal et al., 2019, Holland et al., 2018, Pan et al., 2018, Corneil et al., 2018, Janner et al., 2019, Chelu et al., 2020]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "253a9cc41c91dcbae2ca4815d30bfbddc60c392d",
                "externalIds": {
                    "ArXiv": "2007.09569",
                    "DBLP": "conf/uai/PanMFWYR022",
                    "CorpusId": 249626077
                },
                "corpusId": 249626077,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/253a9cc41c91dcbae2ca4815d30bfbddc60c392d",
                "title": "Understanding and mitigating the limitations of prioritized experience replay",
                "abstract": "Prioritized Experience Replay (ER) has been empirically shown to improve sample efficiency across many domains and attracted great attention; however, there is little theoretical understanding of why such prioritized sampling helps and its limitations. In this work, we take a deep look at the prioritized ER. In a supervised learning setting, we show the equivalence between the error-based prioritized sampling method for mean squared error and uniform sampling for cubic power loss. We then provide theoretical insight into why it improves convergence rate upon uniform sampling during early learning. Based on the insight, we further point out two limitations of the prioritized ER method: 1) outdated priorities and 2) insufficient coverage of the sample space. To mitigate the limitations, we propose our model-based stochastic gradient Langevin dynamics sampling method. We show that our method does provide states distributed close to an ideal prioritized sampling distribution estimated by the brute-force method, which does not suffer from the two limitations. We conduct experiments on both discrete and continuous control problems to show our approach's efficacy and examine the practical implication of our method in an autonomous driving application.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "7303313",
                        "name": "Yangchen Pan"
                    },
                    {
                        "authorId": "3288319",
                        "name": "Jincheng Mei"
                    },
                    {
                        "authorId": "5689899",
                        "name": "Amir-massoud Farahmand"
                    },
                    {
                        "authorId": "144542337",
                        "name": "Martha White"
                    },
                    {
                        "authorId": "40609469",
                        "name": "Hengshuai Yao"
                    },
                    {
                        "authorId": "145367501",
                        "name": "Mohsen Rohani"
                    },
                    {
                        "authorId": "2116813414",
                        "name": "Jun Luo"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018), or to modulate the distance of such states from real experience (Janner et al., 2019).",
                "\u2026abundant existing works (Moore & Atkeson, 1993; Sutton et al., 2008; Gu et al., 2016; Pan et al., 2018; Corneil et al., 2018; Goyal et al., 2019; Janner et al., 2019; Pan et al., 2019) report different level of sample efficiency improvements by using different way of generating hypothetical\u2026",
                "Another strategy has been to generate a more diverse set of states from which to sample (Gu et al., 2016; Holland et al., 2018), or to modulate the distance of such states from real experience (Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2dd22a920c224a130979e0ad4ceec7f28b4e7c03",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-09569",
                    "MAG": "3043761458",
                    "CorpusId": 220647449
                },
                "corpusId": 220647449,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2dd22a920c224a130979e0ad4ceec7f28b4e7c03",
                "title": "Beyond Prioritized Replay: Sampling States in Model-Based RL via Simulated Priorities",
                "abstract": "Model-based reinforcement learning (MBRL) can significantly improve sample efficiency, particularly when carefully choosing the states from which to sample hypothetical transitions. Such prioritization has been empirically shown to be useful for both experience replay (ER) and Dyna-style planning. However, there is as yet little theoretical understanding in RL about such prioritization strategies, and why they help. In this work, we revisit prioritized ER and, in an ideal setting, show an equivalence to minimizing cubic loss, providing theoretical insight into why it improves upon uniform sampling. This ideal setting, however, cannot be realized in practice, due to insufficient coverage of the sample space and outdated priorities of training samples. This motivates our model-based approach, which does not suffer from these limitations. Our key idea is to actively search for high priority states using gradient ascent. Under certain conditions, we prove that the distribution of hypothetical experiences generated from these states provides a diverse set of states, sampled proportionally to approximately true priorities. Our experiments on both benchmark and application-oriented domain show that our approach achieves superior performance over both the model-free prioritized ER method and several closely related model-based baselines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3288319",
                        "name": "Jincheng Mei"
                    },
                    {
                        "authorId": "7303313",
                        "name": "Yangchen Pan"
                    },
                    {
                        "authorId": "144542337",
                        "name": "Martha White"
                    },
                    {
                        "authorId": "5689899",
                        "name": "Amir-massoud Farahmand"
                    },
                    {
                        "authorId": "40609469",
                        "name": "Hengshuai Yao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, Janner et al. (2019) explore only leveraging the learned model over finite horizons where it has accurate predictions and Levine et al. (2016) use local models.",
                "\u2026et al., 2018a; Hafner\net al., 2019b; Nagabandi et al., 2019) or optimizing a policy in the model (Racani\u00e8re et al., 2017; Ha and Schmidhuber, 2018; \u0141ukasz Kaiser et al., 2020; Lee et al., 2019; Janner et al., 2019; Wang and Ba, 2019; Hafner et al., 2019a; Gregor et al., 2019; Byravan et al., 2019).",
                ", 2019) or optimizing a policy in the model (Racani\u00e8re et al., 2017; Ha and Schmidhuber, 2018; \u0141ukasz Kaiser et al., 2020; Lee et al., 2019; Janner et al., 2019; Wang and Ba, 2019; Hafner et al., 2019a; Gregor et al., 2019; Byravan et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "2b6909d9302389f0fd981acdb99a9542efcbf168",
                "externalIds": {
                    "MAG": "3035608172",
                    "DBLP": "journals/corr/abs-2007-07170",
                    "ArXiv": "2007.07170",
                    "CorpusId": 220514423
                },
                "corpusId": 220514423,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2b6909d9302389f0fd981acdb99a9542efcbf168",
                "title": "Goal-Aware Prediction: Learning to Model What Matters",
                "abstract": "Learned dynamics models combined with both planning and policy learning algorithms have shown promise in enabling artificial agents to learn to perform many diverse tasks with limited supervision. However, one of the fundamental challenges in using a learned forward dynamics model is the mismatch between the objective of the learned model (future state reconstruction), and that of the downstream planner or policy (completing a specified task). This issue is exacerbated by vision-based control tasks in diverse real-world environments, where the complexity of the real world dwarfs model capacity. In this paper, we propose to direct prediction towards task relevant information, enabling the model to be aware of the current task and encouraging it to only model relevant quantities of the state space, resulting in a learning objective that more closely matches the downstream task. Further, we do so in an entirely self-supervised manner, without the need for a reward function or image labels. We find that our method more effectively models the relevant parts of the scene conditioned on the goal, and as a result outperforms standard task-agnostic dynamics models and model-free reinforcement learning.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "4734949",
                        "name": "Suraj Nair"
                    },
                    {
                        "authorId": "1702137",
                        "name": "S. Savarese"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Theoretical analyses similar to the bounds we propose have also been presented in a model-based RL context (Sun et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "07a11212bd1178bdd838097e9a76ca1a64f34286",
                "externalIds": {
                    "DBLP": "conf/icml/LeeLVKK20",
                    "MAG": "3035267056",
                    "CorpusId": 221088440
                },
                "corpusId": 221088440,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/07a11212bd1178bdd838097e9a76ca1a64f34286",
                "title": "Batch Reinforcement Learning with Hyperparameter Gradients",
                "abstract": "We consider the batch reinforcement learning problem where the agent needs to learn only from a fixed batch of data, without further interaction with the environment. In such a scenario, we want to prevent the optimized policy from deviating too much from the data collection policy since the estimation becomes highly unstable otherwise due to the off-policy nature of the problem. However, imposing this requirement too strongly will result in a policy that merely follows the data collection policy. Unlike prior work where this trade-off is controlled by hand-tuned hyperparameters, we propose a novel batch reinforcement learning approach, batch optimization of policy and hyperparameter (BOPAH), that uses a gradient-based optimization of the hyperparameter using held-out data. We show that BOPAH outperforms other batch reinforcement learning algorithms in tabular and continuous control tasks, by finding a good balance to the trade-off between adhering to the data collection policy and pursuing the possible policy improvement.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3098985",
                        "name": "Byung-Jun Lee"
                    },
                    {
                        "authorId": "38726140",
                        "name": "Jongmin Lee"
                    },
                    {
                        "authorId": "2528631",
                        "name": "Peter Vrancx"
                    },
                    {
                        "authorId": "2145184144",
                        "name": "Dongho Kim"
                    },
                    {
                        "authorId": "1741330",
                        "name": "Kee-Eung Kim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5e40bdbc8db978cd5a4c64912e438b1f96c5dc34",
                "externalIds": {
                    "ArXiv": "2007.04578",
                    "MAG": "3040981400",
                    "DBLP": "journals/corr/abs-2007-04578",
                    "CorpusId": 220424447
                },
                "corpusId": 220424447,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5e40bdbc8db978cd5a4c64912e438b1f96c5dc34",
                "title": "On the Reliability and Generalizability of Brain-inspired Reinforcement Learning Algorithms",
                "abstract": "Although deep RL models have shown a great potential for solving various types of tasks with minimal supervision, several key challenges remain in terms of learning from limited experience, adapting to environmental changes, and generalizing learning from a single task. Recent evidence in decision neuroscience has shown that the human brain has an innate capacity to resolve these issues, leading to optimism regarding the development of neuroscience-inspired solutions toward sample-efficient, and generalizable RL algorithms. We show that the computational model combining model-based and model-free control, which we term the prefrontal RL, reliably encodes the information of high-level policy that humans learned, and this model can generalize the learned policy to a wide range of tasks. First, we trained the prefrontal RL, and deep RL algorithms on 82 subjects' data, collected while human participants were performing two-stage Markov decision tasks, in which we manipulated the goal, state-transition uncertainty and state-space complexity. In the reliability test, which includes the latent behavior profile and the parameter recoverability test, we showed that the prefrontal RL reliably learned the latent policies of the humans, while all the other models failed. Second, to test the ability to generalize what these models learned from the original task, we situated them in the context of environmental volatility. Specifically, we ran large-scale simulations with 10 Markov decision tasks, in which latent context variables change over time. Our information-theoretic analysis showed that the prefrontal RL showed the highest level of adaptability and episodic encoding efficacy. This is the first attempt to formally test the possibility that computational models mimicking the way the brain solves general problems can lead to practical solutions to key challenges in machine learning.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47181902",
                        "name": "Dongjae Kim"
                    },
                    {
                        "authorId": "82536939",
                        "name": "J. Lee"
                    },
                    {
                        "authorId": "2200460266",
                        "name": "J. Shin"
                    },
                    {
                        "authorId": "122694453",
                        "name": "M. Yang"
                    },
                    {
                        "authorId": "66152796",
                        "name": "Sang Wan Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since our transformer model performed poorly when used as a dynamics model, our Dyna baseline for batch RL adopts a state-of-the-art architecture [34] that employs a 7-model ensemble (MBPO).",
                "We also consider combining CoDA with MBPO, by first expanding the dataset with MBPO and then applying CoDA to the result.",
                "Third: Dyna [82], including MBPO [34], augments real states with new actions and resamples the next state using a learned dynamics model.",
                "For each dataset, we train both mask and reward functions (and in case of MBPO, the dynamics model) on the provided data and use them to generate different amounts of counterfactual data."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "119ad6b55970b90696c620b7b3985b86845cb533",
                "externalIds": {
                    "ArXiv": "2007.02863",
                    "MAG": "3106274014",
                    "DBLP": "journals/corr/abs-2007-02863",
                    "CorpusId": 220363457
                },
                "corpusId": 220363457,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/119ad6b55970b90696c620b7b3985b86845cb533",
                "title": "Counterfactual Data Augmentation using Locally Factored Dynamics",
                "abstract": "Many dynamic processes, including common scenarios in robotic control and reinforcement learning (RL), involve a set of interacting subprocesses. Though the subprocesses are not independent, their interactions are often sparse, and the dynamics at any given time step can often be decomposed into locally independent causal mechanisms. Such local causal structures can be leveraged to improve the sample efficiency of sequence prediction and off-policy reinforcement learning. We formalize this by introducing local causal models (LCMs), which are induced from a global causal model by conditioning on a subset of the state space. We propose an approach to inferring these structures given an object-oriented state representation, as well as a novel algorithm for model-free Counterfactual Data Augmentation (CoDA). CoDA uses local structures and an experience replay to generate counterfactual experiences that are causally valid in the global model. We find that CoDA significantly improves the performance of RL agents in locally factored tasks, including the batch-constrained and goal-conditioned settings.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "32305445",
                        "name": "Silviu Pitis"
                    },
                    {
                        "authorId": "3422145",
                        "name": "Elliot Creager"
                    },
                    {
                        "authorId": "1873736",
                        "name": "Animesh Garg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides the above innovations in model learning, Nguyen et al. and Xiao et al. (2019) developed methods of using adaptive rollout horizon according to the estimated compounding error, and Janner et al. (2019) proposed to use truncated short rollouts branched from real states.",
                "The original MBPO (Janner et al., 2019) algorithm iterates between three stages: data collection, model learning, and policy optimization.",
                "We evaluate our BMPO and previous state-of-the-art algorithms (Haarnoja et al., 2018; Janner et al., 2019) on a range of continuous control benchmark tasks.",
                "With this insight, we combine bidirectional models with recent MBPO method (Janner et al., 2019) and propose a practical MBRL algorithm called Bidirectional Modelbased Policy Optimization (BMPO).",
                "Prior works (Chua et al., 2018; Janner et al., 2019) have demonstrated that the ensemble of probabilistic models is quite effective in MBRL, even when the ground truth dynamics are deterministic.",
                "Janner et al. (2019), instead, derived a bound of discrepancy between returns in real environment and those under the branched rollout scheme of MBPO in terms of rollout length, model error, and policy shift divergence.",
                "Specifically, for model-based methods, we compare against MBPO (Janner et al., 2019), as our method builds on top of it; SLBO (Luo et al., 2018) and PETS (Chua et al., 2018), both performing well in the model-based benchmarking test (Langlois et al., 2019).",
                "Specifically, for model-based methods, we compare against MBPO (Janner et al., 2019), as our method builds on top of it; SLBO (Luo et al.",
                "We notice that in MBPO (Janner et al., 2019), the authors derived a similar return discrepancy bound (refer to Theorem 4.3 therein) with only one forward dynamics model.",
                "Although bidirectional models can be incorporated into almost any Dyna-style model-based algorithms (Sutton, 1991), we choose the Model-based Policy Optimization (MBPO) (Janner et al., 2019) algorithm as the framework backbone since it is the state-of-the-art MBRL method and is sufficiently general.",
                "Though it has been discovered that linearly increasing rollout length achieves excellent performance (Janner et al., 2019), it remains a problem that how to choose the backward rollout length k1 according to the forward length k2.",
                "Model ensembles have shown to be effective in preventing a policy or a controller from exploiting the inaccuracies of any single model (Rajeswaran et al., 2016; Chua et al., 2018; Kurutach et al., 2018; Janner et al., 2019).",
                "We notice that in MBPO (Janner et al., 2019), the authors derived a similar return discrepancy bound (refer to Theorem 4.",
                "The MBPO algorithm (Janner et al., 2019) avoided the compounding error by generating short branched rollouts from real states.",
                "We fix k2 to be the same as Janner et al. (2019) and vary k1 from 0 to 2k2."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "1cf3b7ef431c68eeea0f266bc5ec1b89bf223368",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-01995",
                    "ArXiv": "2007.01995",
                    "MAG": "3040680121",
                    "CorpusId": 220364497
                },
                "corpusId": 220364497,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1cf3b7ef431c68eeea0f266bc5ec1b89bf223368",
                "title": "Bidirectional Model-based Policy Optimization",
                "abstract": "Model-based reinforcement learning approaches leverage a forward dynamics model to support planning and decision making, which, however, may fail catastrophically if the model is inaccurate. Although there are several existing methods dedicated to combating the model error, the potential of the single forward model is still limited. In this paper, we propose to additionally construct a backward dynamics model to reduce the reliance on accuracy in forward model predictions. We develop a novel method, called Bidirectional Model-based Policy Optimization (BMPO) to utilize both the forward model and backward model to generate short branched rollouts for policy optimization. Furthermore, we theoretically derive a tighter bound of return discrepancy, which shows the superiority of BMPO against the one using merely the forward model. Extensive experiments demonstrate that BMPO outperforms state-of-the-art model-based methods in terms of sample efficiency and asymptotic performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2054235888",
                        "name": "Hang Lai"
                    },
                    {
                        "authorId": "2115732606",
                        "name": "Jian Shen"
                    },
                    {
                        "authorId": "2108309275",
                        "name": "Weinan Zhang"
                    },
                    {
                        "authorId": "1811427",
                        "name": "Yong Yu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b0d65b8a41636def1c4a606d186eed41e03b7925",
                "externalIds": {
                    "ArXiv": "2007.00169",
                    "DBLP": "journals/kbs/HanZLY21",
                    "MAG": "3039937608",
                    "DOI": "10.1016/j.knosys.2020.106736",
                    "CorpusId": 220280351
                },
                "corpusId": 220280351,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b0d65b8a41636def1c4a606d186eed41e03b7925",
                "title": "Regularly Updated Deterministic Policy Gradient Algorithm",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109748783",
                        "name": "Shuai Han"
                    },
                    {
                        "authorId": "2118882704",
                        "name": "Wenbo Zhou"
                    },
                    {
                        "authorId": "34176934",
                        "name": "Shuai L\u00fc"
                    },
                    {
                        "authorId": "2115938349",
                        "name": "Jiayu Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018) used a bootstrap ensemble to account for uncertainty, and scales up to a 7 degrees-offreedom (DOF) action space, while model-based policy optimization (MBPO) (Janner et al., 2019), using a similar bootstrap ensemble for model estimation, even scales up to a 22 DOF humanoid robot (in simulation)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1c6435cb353271f3cb87b27ccc6df5b727d55f26",
                "externalIds": {
                    "ArXiv": "2006.16712",
                    "MAG": "3038822267",
                    "DBLP": "journals/corr/abs-2006-16712",
                    "DOI": "10.1561/9781638280576",
                    "CorpusId": 220265929
                },
                "corpusId": 220265929,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1c6435cb353271f3cb87b27ccc6df5b727d55f26",
                "title": "Model-based Reinforcement Learning: A Survey",
                "abstract": "Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a key challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two key sections, we also discuss the potential benefits of model-based RL, like enhanced data efficiency, targeted exploration, and improved stability. Along the survey, we also draw connections to several related RL fields, like hierarchical RL and transfer, and other research disciplines, like behavioural psychology. Altogether, the survey presents a broad conceptual overview of planning-learning combinations for MDP optimization.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "13477045",
                        "name": "T. Moerland"
                    },
                    {
                        "authorId": "1735303",
                        "name": "J. Broekens"
                    },
                    {
                        "authorId": "1689001",
                        "name": "C. Jonker"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a6b67e651b03b78239f5e9acb54ffd11247b83b0",
                "externalIds": {
                    "DBLP": "conf/nips/DuFD20",
                    "MAG": "3038015780",
                    "ArXiv": "2006.16210",
                    "CorpusId": 220250275
                },
                "corpusId": 220250275,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a6b67e651b03b78239f5e9acb54ffd11247b83b0",
                "title": "Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs",
                "abstract": "We present two elegant solutions for modeling continuous-time dynamics, in a novel model-based reinforcement learning (RL) framework for semi-Markov decision processes (SMDPs), using neural ordinary differential equations (ODEs). Our models accurately characterize continuous-time dynamics and enable us to develop high-performing policies using a small amount of data. We also develop a model-based approach for optimizing time schedules to reduce interaction rates with the environment while maintaining the near-optimal performance, which is not possible for model-free methods. We experimentally demonstrate the efficacy of our methods across various continuous-time domains.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1776520344",
                        "name": "Jianzhun Du"
                    },
                    {
                        "authorId": "2585470",
                        "name": "Joseph D. Futoma"
                    },
                    {
                        "authorId": "1388372395",
                        "name": "F. Doshi-Velez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, model-based RL algorithms, such as PILCO [Deisenroth and Rasmussen, 2011], MBPO [Janner et al., 2019], and Visual Foresight [Ebert et al., 2018], despite their success, still have many issues in handling the difficulties of learning a model (dynamics) in a high-dimensional\u2026",
                "On the other hand, model-based RL algorithms, such as PILCO [Deisenroth and Rasmussen, 2011], MBPO [Janner et al., 2019], and Visual Foresight [Ebert et al.",
                "While CARL is compatible with most PI-style (actor-critic) RL algorithms, following a recent work, MBRL [Janner et al., 2019], we choose SAC as the RL algorithm in CARL.",
                "To incorporate this extra piece of information in the representation learning process, we utilize results from variational model-based policy optimization (VMBPO) work by Chow et al. [2020].",
                "On the other hand, model-based RL algorithms, such as PILCO [Deisenroth and Rasmussen, 2011], MBPO [Janner et al., 2019], and Visual Foresight [Ebert et al., 2018], despite their success, still have many issues in handling the difficulties of learning a model (dynamics) in a high-dimensional (pixel) space."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b50da2e0bf200bb481725d92e5e3c80f8273dacc",
                "externalIds": {
                    "ArXiv": "2006.13408",
                    "MAG": "3037494843",
                    "DBLP": "journals/corr/abs-2006-13408",
                    "CorpusId": 220042361
                },
                "corpusId": 220042361,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b50da2e0bf200bb481725d92e5e3c80f8273dacc",
                "title": "Control-Aware Representations for Model-based Reinforcement Learning",
                "abstract": "A major challenge in modern reinforcement learning (RL) is efficient control of dynamical systems from high-dimensional sensory observations. Learning controllable embedding (LCE) is a promising approach that addresses this challenge by embedding the observations into a lower-dimensional latent space, estimating the latent dynamics, and utilizing it to perform control in the latent space. Two important questions in this area are how to learn a representation that is amenable to the control problem at hand, and how to achieve an end-to-end framework for representation learning and control. In this paper, we take a few steps towards addressing these questions. We first formulate a LCE model to learn representations that are suitable to be used by a policy iteration style algorithm in the latent space. We call this model control-aware representation learning (CARL). We derive a loss function for CARL that has close connection to the prediction, consistency, and curvature (PCC) principle for representation learning. We derive three implementations of CARL. In the offline implementation, we replace the locally-linear control algorithm (e.g.,~iLQR) used by the existing LCE methods with a RL algorithm, namely model-based soft actor-critic, and show that it results in significant improvement. In online CARL, we interleave representation learning and control, and demonstrate further gain in performance. Finally, we propose value-guided CARL, a variation in which we optimize a weighted version of the CARL loss function, where the weights depend on the TD-error of the current policy. We evaluate the proposed algorithms by extensive experiments on benchmark tasks and compare them with several LCE baselines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "30978802",
                        "name": "Brandon Cui"
                    },
                    {
                        "authorId": "1819830",
                        "name": "Yinlam Chow"
                    },
                    {
                        "authorId": "1678622",
                        "name": "M. Ghavamzadeh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "121b6fcd5b6aa82ae7e5ad7cf21eee7e8cd63407",
                "externalIds": {
                    "MAG": "3035993323",
                    "DBLP": "conf/cdc/MehtaCNCNBKS21",
                    "ArXiv": "2006.12682",
                    "DOI": "10.1109/CDC45484.2021.9682807",
                    "CorpusId": 219980261
                },
                "corpusId": 219980261,
                "publicationVenue": {
                    "id": "ab066af1-bfee-42da-84bb-42f7e199d0d0",
                    "name": "IEEE Conference on Decision and Control",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Decision and Control",
                        "IEEE Conf Decis Control",
                        "Conf Decis Control",
                        "CDC"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=403"
                },
                "url": "https://www.semanticscholar.org/paper/121b6fcd5b6aa82ae7e5ad7cf21eee7e8cd63407",
                "title": "Neural Dynamical Systems: Balancing Structure and Flexibility in Physical Prediction",
                "abstract": "We introduce Neural Dynamical Systems (NDS), a method of learning dynamical models in various gray-box settings which incorporates prior knowledge in the form of systems of ordinary differential equations. NDS uses neural networks to estimate free parameters of the system, predicts residual terms, and numerically integrates over time to predict future states. A key insight is that many real dynamical systems of interest are hard to model because the dynamics may vary across rollouts. We mitigate this problem by taking a trajectory of prior states as the input to NDS and train it to dynamically estimate system parameters using the preceding trajectory. We find that NDS learns dynamics with higher accuracy and fewer samples than a variety of deep learning methods that do not incorporate the prior knowledge and methods from the system identification literature which do. We demonstrate these advantages first on synthetic dynamical systems and then on real data captured from deuterium shots from a nuclear fusion reactor. Finally, we demonstrate that these benefits can be utilized for control in small-scale experiments.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49061544",
                        "name": "Viraj Mehta"
                    },
                    {
                        "authorId": "2306889",
                        "name": "I. Char"
                    },
                    {
                        "authorId": "2934259",
                        "name": "W. Neiswanger"
                    },
                    {
                        "authorId": "1387920173",
                        "name": "Youngseog Chung"
                    },
                    {
                        "authorId": "79741785",
                        "name": "A. Nelson"
                    },
                    {
                        "authorId": "39062485",
                        "name": "M. Boyer"
                    },
                    {
                        "authorId": "4372060",
                        "name": "E. Kolemen"
                    },
                    {
                        "authorId": "1753432",
                        "name": "J. Schneider"
                    }
                ]
            }
        },
        {
            "contexts": [
                "3 we compare NARL against the publicly released data from MBPO [Janner et al., 2019] on the InvertedPendulum, Hopper and HalfCheetah environments.",
                "We implement our algorithm in by using an ensemble, as is common in existing state-of-the-art methods [Janner et al., 2019, Clavera et al., 2018, Kurutach et al., 2018, Chua et al., 2018, Ball et al., 2020].",
                "Again, we are able to perform favorably vs. MBPO, demonstrating the potential for our approach to scale to larger environments.",
                "This performance comes despite using over 50% fewer models than MBPO (3 models vs. 7).",
                "To prevent this, state-of-theart methods such as MBPO randomly sample models from the ensemble to prevent the policy exploiting an individual (potentially biased) model.",
                "Interestingly, an undocumented feature in MBPO [Janner et al., 2019] that mirrors this is the idea of maintaining a subset of \u201celite\" models.",
                "Since the latter technique is used in many prominent state-of-the-art deep MBRL algorithms [Kurutach et al., 2018, Janner et al., 2019, Chua et al., 2018, Ball et al., 2020], we have all the ingredients we need to scale to that paradigm."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f018493d8f64955f9fdd80a4b95465a327a771cd",
                "externalIds": {
                    "ArXiv": "2006.11911",
                    "DBLP": "conf/uai/PacchianoBPCR21",
                    "CorpusId": 237511605
                },
                "corpusId": 237511605,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f018493d8f64955f9fdd80a4b95465a327a771cd",
                "title": "Towards tractable optimism in model-based reinforcement learning",
                "abstract": "The principle of optimism in the face of uncertainty is prevalent throughout sequential decision making problems such as multi-armed bandits and reinforcement learning (RL). To be successful, an optimistic RL algorithm must over-estimate the true value function (optimism) but not by so much that it is inaccurate (estimation error). In the tabular setting, many state-of-the-art methods produce the required optimism through approaches which are intractable when scaling to deep RL. We re-interpret these scalable optimistic model-based algorithms as solving a tractable noise augmented MDP. This formulation achieves a competitive regret bound: $\\tilde{\\mathcal{O}}( |\\mathcal{S}|H\\sqrt{|\\mathcal{A}| T } )$ when augmenting using Gaussian noise, where $T$ is the total number of environment steps. We also explore how this trade-off changes in the deep RL setting, where we show empirically that estimation error is significantly more troublesome. However, we also show that if this error is reduced, optimistic model-based RL algorithms can match state-of-the-art performance in continuous control problems.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3124110",
                        "name": "Aldo Pacchiano"
                    },
                    {
                        "authorId": "2053179501",
                        "name": "Philip J. Ball"
                    },
                    {
                        "authorId": "2107058201",
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "authorId": "1805203",
                        "name": "K. Choromanski"
                    },
                    {
                        "authorId": "143841496",
                        "name": "Stephen J. Roberts"
                    }
                ]
            }
        },
        {
            "contexts": [
                "3 we compare NARL against the publicly released data from [34], setting (M = 3, M = 1) and (M = 5, M = 0.",
                "1, we must focus on term II by mitigating model errors [34].",
                "This architecture was extended with an ensemble of probabilistic neural networks (from [46]) alongside MPC [17], and used alongside the Soft Actor Critic [30, 31] and shortened horizon rollouts (eschewing the Dyna-approach due to SAC being off-policy) to achieve the current state-of-the-art [34].",
                "For our implementation, we focus on [34], using probabilistic dynamics models [46] and a Soft Actor Critic (SAC, [30, 31]) agent learning inside the model.",
                "We implement our algorithm in deep RL by using an ensemble, as is common in existing state-ofthe-art methods [34, 19, 42, 17, 12].",
                "Since bootstrapped ensembles are commonly used in deep reinforcement learning [42, 34, 17, 12], we have all the ingredients we need to scale NARL to that paradigm."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7d3699cf0e8a822cd8ae99b8eeed146930d24a0f",
                "externalIds": {
                    "MAG": "3035880215",
                    "DBLP": "journals/corr/abs-2006-11911",
                    "CorpusId": 219966592
                },
                "corpusId": 219966592,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7d3699cf0e8a822cd8ae99b8eeed146930d24a0f",
                "title": "On Optimism in Model-Based Reinforcement Learning",
                "abstract": "The principle of optimism in the face of uncertainty is prevalent throughout sequential decision making problems such as multi-armed bandits and reinforcement learning (RL), often coming with strong theoretical guarantees. However, it remains a challenge to scale these approaches to the deep RL paradigm, which has achieved a great deal of attention in recent years. In this paper, we introduce a tractable approach to optimism via noise augmented Markov Decision Processes (MDPs), which we show can obtain a competitive regret bound: $\\tilde{\\mathcal{O}}( |\\mathcal{S}|H\\sqrt{|\\mathcal{S}||\\mathcal{A}| T } )$ when augmenting using Gaussian noise, where $T$ is the total number of environment steps. This tractability allows us to apply our approach to the deep RL setting, where we rigorously evaluate the key factors for success of optimistic model-based RL algorithms, bridging the gap between theory and practice.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3124110",
                        "name": "Aldo Pacchiano"
                    },
                    {
                        "authorId": "2053179501",
                        "name": "Philip J. Ball"
                    },
                    {
                        "authorId": "1410302742",
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "authorId": "1805203",
                        "name": "K. Choromanski"
                    },
                    {
                        "authorId": "145029236",
                        "name": "S. Roberts"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This way reduces to the work (Luo et al., 2018; Chua et al., 2018; Janner et al., 2019).",
                "A straightforward way is to optimize Q, V and \u03c0 using the imaginary data from the rollout, which reduces to Luo et al. (2018); Janner et al. (2019) and many others.",
                "Follow the notion in (Janner et al., 2019), we call it k-step branched rollout.",
                "Several state-of-the-art MBRL algorithms generate hundreds of thousands imaginary data from the model and a few real samples (Luo et al., 2018; Janner et al., 2019).",
                "Janner et al. (2019) provide an error bound on the long term return of the k-step rollout given that the total variation of model bias and policy distribution are bounded by .",
                ", 2018), MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019).",
                "Comparing with the total variation used in (Janner et al., 2019), the Wasserstein distance has better representation in the sense of how close p\u0302 approximate p (Asadi et al.",
                "Four model-based reinforcement learning baselines are SVG (Heess et al., 2015),SLBO (Luo et al., 2018), MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019).",
                "MBPO has the similar spirit but with SAC as the learning algorithm on the imaginary data.",
                "We assume W (p(s\u2032|s, a), p\u0302(s\u2032|s, a)) \u2264 m, \u2200s, a and W (\u03c0(a|s), \u03c0D(a|s)) \u2264 \u03c0, \u2200s. Comparing with the total variation used in (Janner et al., 2019), the Wasserstein distance has better representation in the sense of how close p\u0302 approximate p (Asadi et al., 2018).",
                "Luo et al. (2018); Chua et al. (2018); Kurutach et al. (2018); Janner et al. (2019) use the current policy to gather the data from the interaction with the environment and then learn the dynamics model."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "002845b4b21c2ca1206474abe5df43a473f165bc",
                "externalIds": {
                    "MAG": "3035337758",
                    "ArXiv": "2006.09234",
                    "DBLP": "journals/corr/abs-2006-09234",
                    "CorpusId": 219708924
                },
                "corpusId": 219708924,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/002845b4b21c2ca1206474abe5df43a473f165bc",
                "title": "Model Embedding Model-Based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (MBRL) has shown its advantages in sample-efficiency over model-free reinforcement learning (MFRL). Despite the impressive results it achieves, it still faces a trade-off between the ease of data generation and model bias. In this paper, we propose a simple and elegant model-embedding model-based reinforcement learning (MEMB) algorithm in the framework of the probabilistic reinforcement learning. To balance the sample-efficiency and model bias, we exploit both real and imaginary data in the training. In particular, we embed the model in the policy update and learn $Q$ and $V$ functions from the real data set. We provide the theoretical analysis of MEMB with the Lipschitz continuity assumption on the model and policy. At last, we evaluate MEMB on several benchmarks and demonstrate our algorithm can achieve state-of-the-art performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2112781363",
                        "name": "Xiao Tan"
                    },
                    {
                        "authorId": "2064702529",
                        "name": "C. Qu"
                    },
                    {
                        "authorId": "2663138",
                        "name": "Junwu Xiong"
                    },
                    {
                        "authorId": "2108020140",
                        "name": "James Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "638538253332ebeba83f8de1d66f1eb4d2fe61b5",
                "externalIds": {
                    "ArXiv": "2006.08875",
                    "MAG": "3104364497",
                    "DBLP": "conf/nips/LinTYM20",
                    "CorpusId": 219708414
                },
                "corpusId": 219708414,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/638538253332ebeba83f8de1d66f1eb4d2fe61b5",
                "title": "Model-based Adversarial Meta-Reinforcement Learning",
                "abstract": "Meta-reinforcement learning (meta-RL) aims to learn from multiple training tasks the ability to adapt efficiently to unseen test tasks. Despite the success, existing meta-RL algorithms are known to be sensitive to the task distribution shift. When the test task distribution is different from the training task distribution, the performance may degrade significantly. To address this issue, this paper proposes Model-based Adversarial Meta-Reinforcement Learning (AdMRL), where we aim to minimize the worst-case sub-optimality gap -- the difference between the optimal return and the return that the algorithm achieves after adaptation -- across all tasks in a family of tasks, with a model-based approach. We propose a minimax objective and optimize it by alternating between learning the dynamics model on a fixed task and finding the adversarial task for the current model -- the task for which the policy induced by the model is maximally suboptimal. Assuming the family of tasks is parameterized, we derive a formula for the gradient of the suboptimality with respect to the task parameters via the implicit function theorem, and show how the gradient estimator can be efficiently implemented by the conjugate gradient method and a novel use of the REINFORCE estimator. We evaluate our approach on several continuous control benchmarks and demonstrate its efficacy in the worst-case performance over all tasks, the generalization power to out-of-distribution tasks, and in training and test time sample efficiency, over existing state-of-the-art meta-RL algorithms.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "41123614",
                        "name": "Zichuan Lin"
                    },
                    {
                        "authorId": "8234443",
                        "name": "G. Thomas"
                    },
                    {
                        "authorId": "145789924",
                        "name": "Guangwen Yang"
                    },
                    {
                        "authorId": "1901958",
                        "name": "Tengyu Ma"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3a07e0157a0c8b6321496b01c299319cadd5ec15",
                "externalIds": {
                    "MAG": "3098239427",
                    "ArXiv": "2006.08684",
                    "DBLP": "journals/corr/abs-2006-08684",
                    "CorpusId": 219708852
                },
                "corpusId": 219708852,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3a07e0157a0c8b6321496b01c299319cadd5ec15",
                "title": "Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning",
                "abstract": "Model-based reinforcement learning algorithms with probabilistic dynamical models are amongst the most data-efficient learning methods. This is often attributed to their ability to distinguish between epistemic and aleatoric uncertainty. However, while most algorithms distinguish these two uncertainties for {\\em learning} the model, they ignore it when {\\em optimizing} the policy. In this paper, we show that ignoring the epistemic uncertainty leads to greedy algorithms that do not explore sufficiently. In turn, we propose a {\\em practical optimistic-exploration algorithm} (\\alg), which enlarges the input space with {\\em hallucinated} inputs that can exert as much control as the {\\em epistemic} uncertainty in the model affords. We analyze this setting and construct a general regret bound for well-calibrated models, which is provably sublinear in the case of Gaussian Process models. Based on this theoretical foundation, we show how optimistic exploration can be easily combined with state-of-the-art reinforcement learning algorithms and different probabilistic models. Our experiments demonstrate that optimistic exploration significantly speeds up learning when there are penalties on actions, a setting that is notoriously difficult for existing model-based reinforcement learning algorithms.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "12646136",
                        "name": "Sebastian Curi"
                    },
                    {
                        "authorId": "2064772",
                        "name": "Felix Berkenkamp"
                    },
                    {
                        "authorId": "153243248",
                        "name": "A. Krause"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2019), PILCO (Deisenroth and Rasmussen 2011), MBPO (Janner et al. 2019)) sheds light on the importance of our acquisition functions for safe exploration.",
                "We use PlaNet and MBPO as generic model-based deep reinforcement learning algorithm that use neural network dynamic models, i.e., we do not anticipate a considerably higher sample efficiency from other methods with neural network dynamic models.",
                "Learner Safety gym car Safety gym point\nLearning Evaluation Learning Evaluation\nSamples TC TV TC Samples TC TV TC\nTRPO 1000 202 3.7 5.6 1000 26.8 8.6 13 PPO 1000 205 7.4 11 1000 9 6.1 9.2 PlaNet 100 3.5 3.2 4.9 100 2.6 7.7 16 MBPO 90 3.4 4.3 5.6 100 3.2 6.9 21 PPILCO 2 0.47 7.4 11 8 0.31 5.4 8.9 PlaNet w RS 100 3.4 3.1 3.9 100 2.5 7.9 13 CPO 1000 49 4.7 4.4 1000 14 1.1 5 STRPO 1000 64 2.0 2.1 1000 17 2.1 2.7 SPPO 1000 66 1.7 2.4 1000 10 1 1.8 SAMBA (w/o\nactive learning)\n0.8 0.13 1.4 4.8 0.7 0.56 1.8 2.5\nSAMBA 0.6 0.01 1.3 2.2 0.5 0.04 1.3 1.5\n1 3\nSafety Gym Car and Safety Gym Point environments, respectively.",
                "2019), PILCO (Deisenroth and Rasmussen 2011), MBPO (Janner et\u00a0al."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f5a23ab09df4b80bf32aebb531b8eff73436637e",
                "externalIds": {
                    "ArXiv": "2006.09436",
                    "DBLP": "journals/corr/abs-2006-09436",
                    "MAG": "3036293943",
                    "DOI": "10.1007/s10994-021-06103-6",
                    "CorpusId": 219720955
                },
                "corpusId": 219720955,
                "publicationVenue": {
                    "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
                    "name": "Machine-mediated learning",
                    "type": "journal",
                    "alternate_names": [
                        "Mach learn",
                        "Machine Learning",
                        "Mach Learn"
                    ],
                    "issn": "0732-6718",
                    "alternate_issns": [
                        "0885-6125"
                    ],
                    "url": "http://www.springer.com/computer/artificial/journal/10994",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10994",
                        "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f5a23ab09df4b80bf32aebb531b8eff73436637e",
                "title": "SAMBA: safe model-based & active reinforcement learning",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1413805313",
                        "name": "A. Cowen-Rivers"
                    },
                    {
                        "authorId": "1751630928",
                        "name": "Daniel Palenicek"
                    },
                    {
                        "authorId": "12887111",
                        "name": "V. Moens"
                    },
                    {
                        "authorId": "2112604928",
                        "name": "Mohammed Abdullah"
                    },
                    {
                        "authorId": "144424020",
                        "name": "Aivar Sootla"
                    },
                    {
                        "authorId": "48094081",
                        "name": "Jun Wang"
                    },
                    {
                        "authorId": "1398842047",
                        "name": "Haitham Bou-Ammar"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018), and model-based (Janner et al., 2019; Williams et al., 2015) algorithms.",
                "Our method is applicable to any maximum entropy RL algorithm, including on-policy (Song et al., 2019), off-policy (Abdolmaleki et al., 2018; Haarnoja et al., 2018), and model-based (Janner et al., 2019; Williams et al., 2015) algorithms.",
                "Finally, we compared against two model-based RL methods: MBPO (Janner et al., 2019) and PETS (Chua et al.",
                "Algorithms for model-based RL (e.g., (Polydoros & Nalpantidis, 2017; Sutton, 1991; Janner et al., 2019; Deisenroth & Rasmussen, 2011; Wang et al., 2019; Williams et al., 2015; Hafner et al., 2018; Chua et al., 2018; Finn & Levine, 2017) and off-policy RL (e.g., (Munos et al., 2016; Fujimoto et al.,\u2026",
                "Finally, we compared against two model-based RL methods: MBPO (Janner et al., 2019) and PETS (Chua et al., 2018)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ae712addbdd969c3d1f207fcaec1ce9f8b8741e3",
                "externalIds": {
                    "DBLP": "conf/iclr/EysenbachCALS21",
                    "ArXiv": "2006.13916",
                    "MAG": "3037655992",
                    "CorpusId": 220041969
                },
                "corpusId": 220041969,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ae712addbdd969c3d1f207fcaec1ce9f8b8741e3",
                "title": "Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers",
                "abstract": "We propose a simple, practical, and intuitive approach for domain adaptation in reinforcement learning. Our approach stems from the idea that the agent's experience in the source domain should look similar to its experience in the target domain. Building off of a probabilistic view of RL, we formally show that we can achieve this goal by compensating for the difference in dynamics by modifying the reward function. This modified reward function is simple to estimate by learning auxiliary classifiers that distinguish source-domain transitions from target-domain transitions. Intuitively, the modified reward function penalizes the agent for visiting states and taking actions in the source domain which are not possible in the target domain. Said another way, the agent is penalized for transitions that would indicate that the agent is interacting with the source domain, rather than the target domain. Our approach is applicable to domains with continuous states and actions and does not require learning an explicit model of the dynamics. On discrete and continuous control tasks, we illustrate the mechanics of our approach and demonstrate its scalability to high-dimensional tasks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "8140754",
                        "name": "Benjamin Eysenbach"
                    },
                    {
                        "authorId": "1753866191",
                        "name": "Swapnil Asawa"
                    },
                    {
                        "authorId": "143965735",
                        "name": "Shreyas Chaudhari"
                    },
                    {
                        "authorId": "145124475",
                        "name": "R. Salakhutdinov"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This resembles methods that combine model learning with model-free RL in single-tasks settings [32, 12].",
                "end end When using data generated from a learned model to train a policy, the model\u2019s predicted trajectory often diverges from the real dynamics after a large number of time steps, due to accumulated error [12]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "41a5c7271635f66aa1fa3497e28d6791db27b73b",
                "externalIds": {
                    "ArXiv": "2006.07178",
                    "DBLP": "journals/corr/abs-2006-07178",
                    "MAG": "3035216917",
                    "CorpusId": 219635913
                },
                "corpusId": 219635913,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/41a5c7271635f66aa1fa3497e28d6791db27b73b",
                "title": "Meta-Reinforcement Learning Robust to Distributional Shift via Model Identification and Experience Relabeling",
                "abstract": "Reinforcement learning algorithms can acquire policies for complex tasks autonomously. However, the number of samples required to learn a diverse set of skills can be prohibitively large. While meta-reinforcement learning methods have enabled agents to leverage prior experience to adapt quickly to new tasks, their performance depends crucially on how close the new task is to the previously experienced tasks. Current approaches are either not able to extrapolate well, or can do so at the expense of requiring extremely large amounts of data for on-policy meta-training. In this work, we present model identification and experience relabeling (MIER), a meta-reinforcement learning algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time. Our method is based on a simple insight: we recognize that dynamics models can be adapted efficiently and consistently with off-policy data, more easily than policies and value functions. These dynamics models can then be used to continue training policies and value functions for out-of-distribution tasks without using meta-reinforcement learning at all, by generating synthetic experience for the new task.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35509365",
                        "name": "Russell Mendonca"
                    },
                    {
                        "authorId": "3468192",
                        "name": "Xinyang Geng"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "cbe9be3a9731c13dd18fc7bdfaf8dcedfe7a5544",
                "externalIds": {
                    "MAG": "3035700320",
                    "ArXiv": "2006.05990",
                    "DBLP": "journals/corr/abs-2006-05990",
                    "CorpusId": 219558790
                },
                "corpusId": 219558790,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cbe9be3a9731c13dd18fc7bdfaf8dcedfe7a5544",
                "title": "What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study",
                "abstract": "In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2206490",
                        "name": "Marcin Andrychowicz"
                    },
                    {
                        "authorId": "150918315",
                        "name": "Anton Raichuk"
                    },
                    {
                        "authorId": "2067024592",
                        "name": "Piotr Sta'nczyk"
                    },
                    {
                        "authorId": "1741487247",
                        "name": "Manu Orsini"
                    },
                    {
                        "authorId": "35022714",
                        "name": "Sertan Girgin"
                    },
                    {
                        "authorId": "52153018",
                        "name": "Rapha\u00ebl Marinier"
                    },
                    {
                        "authorId": "122562941",
                        "name": "L'eonard Hussenot"
                    },
                    {
                        "authorId": "1737555",
                        "name": "M. Geist"
                    },
                    {
                        "authorId": "1721354",
                        "name": "O. Pietquin"
                    },
                    {
                        "authorId": "145605490",
                        "name": "Marcin Michalski"
                    },
                    {
                        "authorId": "1802148",
                        "name": "S. Gelly"
                    },
                    {
                        "authorId": "1936951",
                        "name": "Olivier Bachem"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Using the same control tasks, we also compare VMBPO with several state-of-the-art model-based and model-free RL algorithms, including model-based policy optimization (MBPO) [Janner et al., 2019] and maximum a posteriori policy optimization (MPO) [Abdolmaleki et al.",
                "Model-based RL algorithms address the data efficiency issue of the model-free methods by learning a model, and combining model-generated data with those collected from interaction with the real system [Sutton, 1990; Janner et al., 2019].",
                "We compare VMBPO with five baselines, two popular model-free algorithms: MPO [Abdolmaleki et al., 2018] and SAC [Haarnoja et al., 2018], and three recent model-based algorithms: MBPO [Janner et al., 2019], PETS [Chua et al., 2019], and STEVE [Buckman et al., 2018].",
                "Model-based RL algorithms address the data efficiency issue of the model-free methods by learning a model, and com-\nbining model-generated data with those collected from interaction with the real system [Sutton, 1990; Janner et al., 2019].",
                "\u2026tasks, we also compare VMBPO with several state-of-the-art model-based and model-free RL algorithms, including model-based policy optimization (MBPO) [Janner et al., 2019] and maximum a posteriori policy optimization (MPO) [Abdolmaleki et al., 2018], and show its sample efficiency and performance.",
                ", 2018], and three recent model-based algorithms: MBPO [Janner et al., 2019], PETS [Chua et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f60aa6f42eeed55c8f514d44b1c4fd0625c5d22b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-05443",
                    "ArXiv": "2006.05443",
                    "MAG": "3034214864",
                    "DOI": "10.24963/ijcai.2021/316",
                    "CorpusId": 219559132
                },
                "corpusId": 219559132,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f60aa6f42eeed55c8f514d44b1c4fd0625c5d22b",
                "title": "Variational Model-based Policy Optimization",
                "abstract": "Model-based reinforcement learning (RL) algorithms allow us to combine model-generated data with those collected from interaction with the real system in order to alleviate the data efficiency problem in RL. However, designing such algorithms is often challenging because the bias in simulated data may overshadow the ease of data generation. A potential solution to this challenge is to jointly learn and improve model and policy using a universal objective function. In this paper, we leverage the connection between RL and probabilistic inference, and formulate such an objective function as a variational lower-bound of a log-likelihood. This allows us to use expectation maximization (EM) and iteratively fix a baseline policy and learn a variational distribution, consisting of a model and a policy (E-step), followed by improving the baseline policy given the learned variational distribution (M-step). We propose model-based and model-free policy iteration (actor-critic) style algorithms for the E-step and show how the variational distribution learned by them can be used to optimize the M-step in a fully model-based fashion. Our experiments on a number of continuous control tasks show that our model-based (E-step) algorithm, called variational model-based policy optimization (VMBPO), is more sample-efficient and robust to hyper-parameter tuning than its model-free (E-step) counterpart. Using the same control tasks, we also compare VMBPO with several state-of-the-art model-based and model-free RL algorithms and show its sample efficiency and performance.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1819830",
                        "name": "Yinlam Chow"
                    },
                    {
                        "authorId": "30978802",
                        "name": "Brandon Cui"
                    },
                    {
                        "authorId": "2060597867",
                        "name": "M. Ryu"
                    },
                    {
                        "authorId": "1678622",
                        "name": "M. Ghavamzadeh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based reinforcement learning (MBRL) (Janner et al., 2019; Buckman et al., 2018; Xu et al., 2018; Chua et al., 2018) shows competitive performance compared with best model-free reinforcement learning (MFRL) algorithms (Schulman et al.",
                "\u2026benchmarks (Todorov et al., 2012) and the experimental results show that MEMR matches asymptotic performance and sample efficiency with MBPO (Janner et al., 2019) while significantly reduces the number of policy updates and model rollouts, which leads to faster learning speed. ar X iv :2 00\u2026",
                "We compare our method with the state-of-the-art model-based method, MBPO (Janner et al., 2019).",
                "It indicates that much computation power is wasted in (Janner et al., 2019) on less informative model rollouts that barely help the learning of the value functions in SAC.",
                "\u20262018) develops a theoretical framework that provides monotonic improvement of the to a local maximum of the expected reward for MBRL. Model-based policy optimization (MBPO) (Janner et al., 2019) achieves state-of-the-art sample efficiency and matches the asymptotic performance of MFRL approaches.",
                "In MBPO, learned dynamics model is used to generate branched model rollouts with short horizons (Janner et al., 2019).",
                "Recently, (Janner et al., 2019) proposed Model-based Policy Optimization (MBPO), including a theoretical framework that encourages short-horizon model usage based on an optimistic assumption of a bounded model generalization error given policy shift.",
                "Model-based policy optimization (MBPO) (Janner et al., 2019) achieves state-of-the-art sample efficiency and matches the asymptotic performance of MFRL approaches.",
                "\u2026(MBRL) learns a dynamics model and directly perform model predictive control (MPC) (Nagabandi et al., 2017; Chua et al., 2018; Deisenroth & Rasmussen, 2011) or derives the policy using model generated rollouts (Kurutach et al., 2018; Janner et al., 2019; Buckman et al., 2018; Xu et al., 2018).",
                "Uniform sampling of true states 1 to generate model rollouts is adopted in MBPO (Janner et al., 2019).",
                "Our approach combines (Janner et al., 2019) and (Sutton, 1991) by proposing an non-trivial sampling approach to significantly reduce the number of policy updates and model rollouts that obtain asymptotic performance.",
                "Although (Janner et al., 2019) presented theoretical analysis to bound the policy performance trained using model generate rollouts, the over exploitation of model generalization can\u2019t be eliminated.",
                ", 2012) and the experimental results show that MEMR matches asymptotic performance and sample efficiency with MBPO (Janner et al., 2019) while significantly reduces the number of policy updates and model rollouts, which leads to faster learning speed.",
                "Model-based reinforcement learning (MBRL) (Janner et al., 2019; Buckman et al., 2018; Xu et al., 2018; Chua et al., 2018) shows competitive performance compared with best model-free reinforcement learning (MFRL) algorithms (Schulman et al., 2017; 2015; Mnih et al., 2013; Haarnoja et al., 2018a;b)\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "174bb06082ee8bcdc2c40402d194acc67e9c5791",
                "externalIds": {
                    "ArXiv": "2006.04802",
                    "DBLP": "journals/corr/abs-2006-04802",
                    "MAG": "3034520425",
                    "CorpusId": 219558581
                },
                "corpusId": 219558581,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/174bb06082ee8bcdc2c40402d194acc67e9c5791",
                "title": "Maximum Entropy Model Rollouts: Fast Model Based Policy Optimization without Compounding Errors",
                "abstract": "Model usage is the central challenge of model-based reinforcement learning. Although dynamics model based on deep neural networks provide good generalization for single step prediction, such ability is over exploited when it is used to predict long horizon trajectories due to compounding errors. In this work, we propose a Dyna-style model-based reinforcement learning algorithm, which we called Maximum Entropy Model Rollouts (MEMR). To eliminate the compounding errors, we only use our model to generate single-step rollouts. Furthermore, we propose to generate \\emph{diverse} model rollouts by non-uniform sampling of the environment states such that the entropy of the model rollouts is maximized. We mathematically derived the maximum entropy sampling criteria for one data case under Gaussian prior. To accomplish this criteria, we propose to utilize a prioritized experience replay. Our preliminary experiments in challenging locomotion benchmarks show that our approach achieves the same sample efficiency of the best model-based algorithms, matches the asymptotic performance of the best model-free algorithms, and significantly reduces the computation requirements of other model-based methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2115812350",
                        "name": "Chi Zhang"
                    },
                    {
                        "authorId": "2873546",
                        "name": "S. Kuppannagari"
                    },
                    {
                        "authorId": "1728271",
                        "name": "V. Prasanna"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(5) This bound guarantees the improvement under the true returns as long as the improvement under the model returns increases by more than the slack in the bound due to m, \u03c0 [22, 30].",
                "[22], we denote the generalization error of a dynamics model on the state distribution under the true behavior policy as m = maxt Es\u223cd\u03c0b t DTV (p(st+1|st, at)||p\u03c6(st+1|st, at)), where DTV represents the total variation distance between true dynamics p and learned model p\u03c6.",
                "[22] as, \u03b7[\u03c0] \u2265 \u03b7\u0302[\u03c0]\u2212 [ 2\u03b3rmax( m + 2 \u03c0) (1\u2212 \u03b3)2 + 4rmax \u03c0 (1\u2212 \u03b3) ] .",
                "A variety of remedies have been proposed to relieve the problem of model bias, such as the use of multiple dynamics models as an ensemble [6, 28, 22], meta-learning [7], energy-based model regularizer [2], and explicit reward penalty for unknown state [25, 57]."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "79ebde314ab90d066cee3b82193ef05666323394",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-03647",
                    "MAG": "3034084488",
                    "ArXiv": "2006.03647",
                    "CorpusId": 219530969
                },
                "corpusId": 219530969,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/79ebde314ab90d066cee3b82193ef05666323394",
                "title": "Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization",
                "abstract": "Most reinforcement learning (RL) algorithms assume online access to the environment, in which one may readily interleave updates to the policy with experience collection using that policy. However, in many real-world applications such as health, education, dialogue agents, and robotics, the cost or potential risk of deploying a new data-collection policy is high, to the point that it can become prohibitive to update the data-collection policy more than a few times during learning. With this view, we propose a novel concept of deployment efficiency, measuring the number of distinct data-collection policies that are used during policy learning. We observe that naively applying existing model-free offline RL algorithms recursively does not lead to a practical deployment-efficient and sample-efficient algorithm. We propose a novel model-based algorithm, Behavior-Regularized Model-ENsemble (BREMEN) that can effectively optimize a policy offline using 10-20 times fewer data than prior works. Furthermore, the recursive application of BREMEN is able to achieve impressive deployment efficiency while maintaining the same or better sample efficiency, learning successful policies from scratch on simulated robotic environments with only 5-10 deployments, compared to typical values of hundreds to millions in standard RL baselines. Codes and pre-trained models are available at this https URL .",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145930468",
                        "name": "T. Matsushima"
                    },
                    {
                        "authorId": "2052903664",
                        "name": "Hiroki Furuta"
                    },
                    {
                        "authorId": "49484314",
                        "name": "Y. Matsuo"
                    },
                    {
                        "authorId": "7624658",
                        "name": "Ofir Nachum"
                    },
                    {
                        "authorId": "2046135",
                        "name": "S. Gu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Then, in Section 5, we conduct a theoretical analysis to justify the use of branched rollouts, which is a promising model-based rollout method proposed by Janner et al. (2019), in the meta-RL setting.",
                "We refine analyses in Janner et al. (2019) by considering multiple-model-based rollout factors (Appendix A.1), and extend our analyses results into a meta-RL setting (Section 5).",
                "Branched rollouts (Janner et al., 2019) Branched rollouts are Dyna-style rollouts (Sutton, 1991), in which model-based rollouts are run as being branched from real trajectories.",
                "Existing works have focused on the performance bound of model-based RL (Feinberg et al., 2018; Henaff, 2019; Janner et al., 2019; Luo et al., 2018; Rajeswaran et al., 2020), while ignoring model-based meta-RL.",
                "Specifically, we extend the branched rollout defined originally in the state-action space (Janner et al., 2019) to a branched rollout defined in the history-action space (Figure 3).",
                "To provide our theorem, we extend the notion and theorem of the branched rollout proposed in Janner et al. (2019) into the meta-RL setting.",
                "In existing Dyna-style RL methods (e.g. Janner et al. (2019); Shen et al. (2020); Yu et al. (2020)), this mixture ratio is fixed during the training phase.",
                "The branched rollout (Janner et al., 2019) is a kind of Dyna-style rollouts (Sutton, 1991), in which k-step model-based rollouts are run as being branched from real trajectories.",
                "The proof of Theorem 1 is given in Appendix A.2, where we extend the result of Janner et al. (2019) to the meta-RL setting.",
                "A theorem for the performance guarantee of the branched rollout in MDPs are provided as Theorem 4.2 in Janner et al. (2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "14a44d4555bae15c01b1ed48fb3e1d75185573e9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-02608",
                    "MAG": "3033097743",
                    "ArXiv": "2006.02608",
                    "CorpusId": 219305249
                },
                "corpusId": 219305249,
                "publicationVenue": {
                    "id": "2486528b-036c-4f3c-953f-c574eb381d12",
                    "name": "Asian Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Mach Learn",
                        "ACML"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=40"
                },
                "url": "https://www.semanticscholar.org/paper/14a44d4555bae15c01b1ed48fb3e1d75185573e9",
                "title": "Meta-Model-Based Meta-Policy Optimization",
                "abstract": "Model-based reinforcement learning (MBRL) has been applied to meta-learning settings and demonstrated its high sample efficiency. However, in previous MBRL for meta-learning settings, policies are optimized via rollouts that fully rely on a predictive model for an environment, and thus its performance in a real environment tends to degrade when the predictive model is inaccurate. In this paper, we prove that the performance degradation can be suppressed by using branched meta-rollouts. Based on this theoretical analysis, we propose meta-model-based meta-policy optimization (M3PO), in which the branched meta-rollouts are used for policy optimization. We demonstrate that M3PO outperforms existing meta reinforcement learning methods in continuous-control benchmarks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3027595",
                        "name": "Takuya Hiraoka"
                    },
                    {
                        "authorId": "22312167",
                        "name": "Takahisa Imagawa"
                    },
                    {
                        "authorId": "1801873",
                        "name": "Voot Tangkaratt"
                    },
                    {
                        "authorId": "40229316",
                        "name": "Takayuki Osa"
                    },
                    {
                        "authorId": "2058274526",
                        "name": "Takashi Onishi"
                    },
                    {
                        "authorId": "143946906",
                        "name": "Yoshimasa Tsuruoka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As a proof-of-concept experiment, we evaluate two state-of-the-art off-policy model-based and model-free algorithms, MBPO [28] and SAC [26], in Figure 1.",
                "While prior approaches have used these models to select actions using planning [66, 16, 53, 50, 58], we choose to build upon Dyna-style approaches that optimize for a policy [63, 65, 71, 31, 25, 27, 43], specifically MBPO [28].",
                "To approach this question, we first hypothesize that model-based RL methods [63, 11, 41, 37, 28, 43] make a natural choice for enabling generalization, for a number of reasons.",
                "Figure 1: Comparison between vanilla model-based RL (MBPO [28]) with or without model ensembles and vanilla model-free RL (SAC [26]) on two offline RL tasks: one from the D4RL benchmark [17] and one that demands out-of-distribution generalization.",
                "We now summarize model-based policy optimization (MBPO) [28], which we build on in this work."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7490d73dc5ba02204407bb6ef630d2e8ec47bb4f",
                "externalIds": {
                    "MAG": "3101192004",
                    "DBLP": "conf/nips/YuTYEZLFM20",
                    "ArXiv": "2005.13239",
                    "CorpusId": 218900501
                },
                "corpusId": 218900501,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7490d73dc5ba02204407bb6ef630d2e8ec47bb4f",
                "title": "MOPO: Model-based Offline Policy Optimization",
                "abstract": "Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a batch of previously collected data. This problem setting is compelling, because it offers the promise of utilizing large, diverse, previously collected datasets to acquire policies without any costly or dangerous active exploration, but it is also exceptionally difficult, due to the distributional shift between the offline training data and the learned policy. While there has been significant progress in model-free offline RL, the most successful prior methods constrain the policy to the support of the data, precluding generalization to new states. In this paper, we observe that an existing model-based RL algorithm on its own already produces significant gains in the offline setting, as compared to model-free approaches, despite not being designed for this setting. However, although many standard model-based RL methods already estimate the uncertainty of their model, they do not by themselves provide a mechanism to avoid the issues associated with distributional shift in the offline setting. We therefore propose to modify existing model-based RL methods to address these issues by casting offline model-based RL into a penalized MDP framework. We theoretically show that, by using this penalized MDP, we are maximizing a lower bound of the return in the true MDP. Based on our theoretical results, we propose a new model-based offline RL algorithm that applies the variance of a Lipschitz-regularized model as a penalty to the reward function. We find that this algorithm outperforms both standard model-based RL methods and existing state-of-the-art model-free offline RL approaches on existing offline RL benchmarks, as well as two challenging continuous control tasks that require generalizing from data collected for a different task.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "10909315",
                        "name": "Tianhe Yu"
                    },
                    {
                        "authorId": "8234443",
                        "name": "G. Thomas"
                    },
                    {
                        "authorId": "3469209",
                        "name": "Lantao Yu"
                    },
                    {
                        "authorId": "2490652",
                        "name": "Stefano Ermon"
                    },
                    {
                        "authorId": "145085305",
                        "name": "James Y. Zou"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    },
                    {
                        "authorId": "1901958",
                        "name": "Tengyu Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Complementary work (Janner et al., 2019) shows that simulating one-step transitions provides a strong baseline with respect to partial or complete policy rollouts with a learned model, and PPO mantains its monotonic improvement property."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ba408d1e603bf90c73cfbf69231355568005be8b",
                "externalIds": {
                    "MAG": "3024248582",
                    "DBLP": "journals/corr/abs-2005-08006",
                    "ArXiv": "2005.08006",
                    "DOI": "10.1016/J.ENERGY.2021.121035",
                    "CorpusId": 218673656
                },
                "corpusId": 218673656,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ba408d1e603bf90c73cfbf69231355568005be8b",
                "title": "Lifelong Control of Off-grid Microgrid with Model Based Reinforcement Learning",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51116364",
                        "name": "Simone Totaro"
                    },
                    {
                        "authorId": "73437592",
                        "name": "Ioannis Boukas"
                    },
                    {
                        "authorId": "143808510",
                        "name": "Anders Jonsson"
                    },
                    {
                        "authorId": "50193670",
                        "name": "Bertrand Corn'elusse"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such a dynamics model can then be used for control by planning (Atkeson & Santamaria, 1997; Lenz et al., 2015; Finn & Levine, 2017), or for improving the data-effciency of model-free RL methods (Sutton, 1990; Gu et al., 2016; Janner et al., 2019).",
                ", 2015; Finn & Levine, 2017), or for improving the data-effciency of model-free RL methods (Sutton, 1990; Gu et al., 2016; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "16ce156a802e43d34929f0b8d32b93db7e852690",
                "externalIds": {
                    "DBLP": "conf/icml/LeeSLLS20",
                    "ArXiv": "2005.06800",
                    "MAG": "3025225624",
                    "CorpusId": 218630003
                },
                "corpusId": 218630003,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/16ce156a802e43d34929f0b8d32b93db7e852690",
                "title": "Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (RL) enjoys several benefits, such as data-efficiency and planning, by learning a model of the environment's dynamics. However, learning a global model that can generalize across different dynamics is a challenging task. To tackle this problem, we decompose the task of learning a global dynamics model into two stages: (a) learning a context latent vector that captures the local dynamics, then (b) predicting the next state conditioned on it. In order to encode dynamics-specific information into the context latent vector, we introduce a novel loss function that encourages the context latent vector to be useful for predicting both forward and backward dynamics. The proposed method achieves superior generalization ability across various simulated robotics and control tasks, compared to existing RL schemes.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3436470",
                        "name": "Kimin Lee"
                    },
                    {
                        "authorId": "2067714176",
                        "name": "Younggyo Seo"
                    },
                    {
                        "authorId": "2117177611",
                        "name": "Seunghyun Lee"
                    },
                    {
                        "authorId": "1697141",
                        "name": "Honglak Lee"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As a result, MBRL algorithms have been highly sample efficient for online RL [28, 29].",
                "As a result, planning using a learned model without any safeguards against model inaccuracy can result in \u201cmodel exploitation\u201d [30, 31, 29, 28], yielding poor results [32]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9",
                "externalIds": {
                    "DBLP": "conf/nips/KidambiRNJ20",
                    "ArXiv": "2005.05951",
                    "MAG": "3025606523",
                    "CorpusId": 218595964
                },
                "corpusId": 218595964,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/309c2c5ee60e725244da09180f913cd8d4b8d4e9",
                "title": "MOReL : Model-Based Offline Reinforcement Learning",
                "abstract": "In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. The ability to train RL policies offline can greatly expand the applicability of RL, its data efficiency, and its experimental velocity. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; and (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP. This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied offline RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g. generative modeling, uncertainty estimation, planning etc.) to directly translate into advances for offline RL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1891978",
                        "name": "Rahul Kidambi"
                    },
                    {
                        "authorId": "19275599",
                        "name": "A. Rajeswaran"
                    },
                    {
                        "authorId": "1751626",
                        "name": "Praneeth Netrapalli"
                    },
                    {
                        "authorId": "1680188",
                        "name": "T. Joachims"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3ccf1627d4771a3e8ee746517ca7a0131d2315cb",
                "externalIds": {
                    "ArXiv": "2005.05440",
                    "DBLP": "journals/corr/abs-2005-05440",
                    "MAG": "3025864009",
                    "DOI": "10.1016/J.NEUCOM.2021.04.015",
                    "CorpusId": 218595777
                },
                "corpusId": 218595777,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3ccf1627d4771a3e8ee746517ca7a0131d2315cb",
                "title": "Delay-Aware Model-Based Reinforcement Learning for Continuous Control",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2108425121",
                        "name": "Baiming Chen"
                    },
                    {
                        "authorId": "1775497",
                        "name": "Mengdi Xu"
                    },
                    {
                        "authorId": "2154884750",
                        "name": "Liang Li"
                    },
                    {
                        "authorId": "47783130",
                        "name": "Ding Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following Janner et al. (2019), if we assume that the total variation distance (TVD) between the learned model T\u03c8 and true model T is bounded by m = maxt Ed\u03c0t DTV (T\u03c8(st+1|st, at)\u2016T (st+1|st, at)), and the TVD between \u03c0 and \u03c0\u03b2 is likewise bounded on sampled states by \u03c0, then the true policy value\u2026",
                "\u2026low-dimensional MDPs, such uncertainty estimates can be produced by means of Bayesian models such as Gaussian processes (Deisenroth and Rasmussen, 2011), while for higher-dimensional problems, Bayesian neural networks and bootstrap ensembles can be utilized (Chua et al., 2018; Janner et al., 2019).",
                "Janner et al. (2019) also argue that a modified model-based RL procedure that resembles Dyna Sutton (1991), where only short-horizon rollouts from the model are generated by \u201cbranching\u201d off of states seen in the data, can mitigate this accumulation of error.",
                "\u2026Q-learning and one-step predictions via the model from previously seen states (Sutton, 1991), while a variety of recently proposed algorithms employ synthetic model-based rollouts with policy gradients (Parmas et al., 2019; Kaiser et al., 2019a) and actor-critic algorithms (Janner et al., 2019).",
                ", 2019a) and actor-critic algorithms (Janner et al., 2019).",
                "Hybrid methods that combine model-based and model-free learning, for example by utilizing short rollouts (Sutton, 1991; Janner et al., 2019) or avoiding prediction of full observations (Dosovitskiy and Koltun, 2016; Oh et al.",
                "Many such methods have been known to exhibit excellent performance in conventional off-policy settings, where additional data collection is allowed, but prior data is also utilized (Sutton, 1991; Watter et al., 2015; Zhang et al., 2018; Hafner et al., 2018; Janner et al., 2019).",
                "Hybrid methods that combine model-based and model-free learning, for example by utilizing short rollouts (Sutton, 1991; Janner et al., 2019) or avoiding prediction of full observations (Dosovitskiy and Koltun, 2016; Oh et al., 2017; Kahn et al., 2020) offer some promise in this area.",
                "Theoretical analysis of model-based policy learning can provide bounds on the error incurred from the distributional shift due to the divergence between the learned policy \u03c0(a|s) and the behavior policy \u03c0\u03b2(a|s) (Sun et al., 2018b; Luo et al., 2018; Janner et al., 2019).",
                "In low-dimensional MDPs, such uncertainty estimates can be produced by means of Bayesian models such as Gaussian processes (Deisenroth and Rasmussen, 2011), while for higher-dimensional problems, Bayesian neural networks and bootstrap ensembles can be utilized (Chua et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-01643",
                    "ArXiv": "2005.01643",
                    "MAG": "3022566517",
                    "CorpusId": 218486979
                },
                "corpusId": 218486979,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5e7bc93622416f14e6948a500278bfbe58cd3890",
                "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
                "abstract": "In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "1488785534",
                        "name": "Aviral Kumar"
                    },
                    {
                        "authorId": "145499435",
                        "name": "G. Tucker"
                    },
                    {
                        "authorId": "2550764",
                        "name": "Justin Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this case, the policy is prone to exploit regions where the model is inaccurate (Janner et al., 2019).",
                "\u2026work on model-based RL for sample efficient control (Deisenroth & Rasmussen, 2011; Kurutach et al., 2018; Peng et al., 2018; Kaiser et al., 2019; Janner et al., 2019), with the key difference that the state transition function is known and the reward function is unknown in our work, whereas\u2026",
                "DyNA PPO is related to existing work on model-based RL for sample efficient control (Deisenroth & Rasmussen, 2011; Kurutach et al., 2018; Peng et al., 2018; Kaiser et al., 2019; Janner et al., 2019), with the key difference that the state transition function is known and the reward function is unknown in our work, whereas most existing model-based RL approaches seek to model the state-transition function and consider the reward function as known.",
                "Janner et al. (2019) investigate conditions in which an estimate of model generalization (their analysis uses validation accuracy) could justify model usage in such model-based policy optimization settings.",
                "By ignoring the model if it is inaccurate, we aim to prevent the policy from exploiting deficiencies of the model (Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "b8be225a955ba3e6f6a8cbdc8b5c8547430be1c0",
                "externalIds": {
                    "MAG": "2996314716",
                    "DBLP": "conf/iclr/AngermullerDBDM20",
                    "CorpusId": 210847328
                },
                "corpusId": 210847328,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b8be225a955ba3e6f6a8cbdc8b5c8547430be1c0",
                "title": "Model-based reinforcement learning for biological sequence design",
                "abstract": "The ability to design biological structures such as DNA or proteins would have considerable medical and industrial impact. Doing so presents a challenging black-box optimization problem characterized by the large-batch, low round setting due to the need for labor-intensive wet lab evaluations. In response, we propose using reinforcement learning (RL) based on proximal-policy optimization (PPO) for biological sequence design. RL provides a flexible framework for optimization generative sequence models to achieve specific criteria, such as diversity among the high-quality sequences discovered. We propose a model-based variant of PPO, DyNA-PPO, to improve sample efficiency, where the policy for a new round is trained offline using a simulator fit on functional measurements from prior rounds. To accommodate the growing number of observations across rounds, the simulator model is automatically selected at each round from a pool of diverse models of varying capacity. On the tasks of designing DNA transcription factor binding sites, designing antimicrobial proteins, and optimizing the energy of Ising models based on protein structure, we find that DyNA-PPO performs significantly better than existing methods in settings in which modeling is feasible, while still not performing worse in situations in which a reliable model cannot be learned.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48765757",
                        "name": "Christof Angermueller"
                    },
                    {
                        "authorId": "35363891",
                        "name": "David Dohan"
                    },
                    {
                        "authorId": "2636941",
                        "name": "David Belanger"
                    },
                    {
                        "authorId": "71051924",
                        "name": "Ramya Deshpande"
                    },
                    {
                        "authorId": "2056418611",
                        "name": "Kevin Murphy"
                    },
                    {
                        "authorId": "2654847",
                        "name": "Lucy J. Colwell"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[9] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.",
                "Specifically, we compare against the modelfree soft actor-critic (SAC) [6] as well as two state-of-the-art model-based baselines: model-based policy-optimization (MBPO) [9] and stochastic ensemble value expansion (STEVE) [1].",
                "Then, model-based methods use the model to derive controllers from it, either parametric [14, 1, 9] or non-parametric [17, 2].",
                "We train both Q functions by minimizing the Bellman error (Section 2): JQ(\u03c8) = E[(Q\u03c8(st, at)\u2212 (r(st, at) + \u03b3Q\u03c8(st+1, at+1)))(2)] Similar to [9], we minimize the Bellman residual on states previously visited and imagined states obtained from unrolling the learned model.",
                "The main contribution of this work is a model-based method that significantly reduces the sample complexity compared to state-of-the-art model-based algorithms [9, 1]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c37c2ded09f0e5ac181cdeebb141ec7c8b4641d6",
                "externalIds": {
                    "ArXiv": "2005.08068",
                    "DBLP": "conf/iclr/ClaveraFA20",
                    "MAG": "2996449210",
                    "CorpusId": 213529244
                },
                "corpusId": 213529244,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c37c2ded09f0e5ac181cdeebb141ec7c8b4641d6",
                "title": "Model-Augmented Actor-Critic: Backpropagating through Paths",
                "abstract": "Current model-based reinforcement learning approaches use the model simply as a learned black-box simulator to augment the data for policy optimization or value function learning. In this paper, we show how to make more effective use of the model by exploiting its differentiability. We construct a policy optimization algorithm that uses the pathwise derivative of the learned model and policy across future timesteps. Instabilities of learning across many timesteps are prevented by using a terminal value function, learning the policy in an actor-critic fashion. Furthermore, we present a derivation on the monotonic improvement of our objective in terms of the gradient error in the model and value function. We show that our approach (i) is consistently more sample efficient than existing state-of-the-art model-based algorithms, (ii) matches the asymptotic performance of model-free algorithms, and (iii) scales to long horizons, a regime where typically past model-based approaches have struggled.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "15593386",
                        "name": "I. Clavera"
                    },
                    {
                        "authorId": "2117786875",
                        "name": "Yao Fu"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "in a differentiable form, it needs to be substituted with an approximate model p\u0302, as commonly done in model-based RL [7, 9, 20].",
                "While it is often hard to determine under which circumstances the addition of an approximate learned model to a model-free algorithm is beneficial [20], we have shown that model-based techniques such as MAGE\u2019s gradient-learning procedure, can unlock novel learning modalities, inaccessible for model-free algorithms.",
                "It resembles 1-step horizon Model-based Policy Optimization (MBPO [20]), but uses a deterministic policy optimized by TD3.",
                "Our algorithm, which learns a Q-function from model-generated data but only optimizes the policy by using real data, is related to the approaches that compute the policy gradient by using a model-based value function together with trajectories sampled in the environment [1, 10, 19, 20].",
                "In practice, we leverage an ensemble of models, which has been shown to improve performance in a variety of contexts [7, 20, 23]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "669aecc2cc30f7d80aa7c4c235991559129f7d19",
                "externalIds": {
                    "MAG": "3104792380",
                    "DBLP": "conf/nips/DOroJ20",
                    "ArXiv": "2004.14309",
                    "CorpusId": 216641655
                },
                "corpusId": 216641655,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/669aecc2cc30f7d80aa7c4c235991559129f7d19",
                "title": "How to Learn a Useful Critic? Model-based Action-Gradient-Estimator Policy Optimization",
                "abstract": "Deterministic-policy actor-critic algorithms for continuous control improve the actor by plugging its actions into the critic and ascending the action-value gradient, which is obtained by chaining the actor's Jacobian matrix with the gradient of the critic w.r.t. input actions. However, instead of gradients, the critic is, typically, only trained to accurately predict expected returns, which, on their own, are useless for policy optimization. In this paper, we propose MAGE, a model-based actor-critic algorithm, grounded in the theory of policy gradients, which explicitly learns the action-value gradient. MAGE backpropagates through the learned dynamics to compute gradient targets in temporal difference learning, leading to a critic tailored for policy improvement. On a set of MuJoCo continuous-control tasks, we demonstrate the efficiency of the algorithm with respect to model-free and model-based state-of-the-art baselines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1410392725",
                        "name": "P. D'Oro"
                    },
                    {
                        "authorId": "2146303",
                        "name": "Wojciech Ja\u015bkowski"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One may compare their methods to MBPO method (Janner et al. 2019) that also uses offline data while our method focuses on online learning."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "35763a64b605f37b6de74c776c57dc964416eb18",
                "externalIds": {
                    "MAG": "3021788576",
                    "DBLP": "journals/corr/abs-2004-13657",
                    "ArXiv": "2004.13657",
                    "CorpusId": 216562568
                },
                "corpusId": 216562568,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/35763a64b605f37b6de74c776c57dc964416eb18",
                "title": "Sample-Efficient Model-based Actor-Critic for an Interactive Dialogue Task",
                "abstract": "Human-computer interactive systems that rely on machine learning are becoming paramount to the lives of millions of people who use digital assistants on a daily basis. Yet, further advances are limited by the availability of data and the cost of acquiring new samples. One way to address this problem is by improving the sample efficiency of current approaches. As a solution path, we present a model-based reinforcement learning algorithm for an interactive dialogue task. We build on commonly used actor-critic methods, adding an environment model and planner that augments a learning agent to learn the model of the environment dynamics. Our results show that, on a simulation that mimics the interactive task, our algorithm requires 70 times fewer samples, compared to the baseline of commonly used model-free algorithm, and demonstrates 2~times better performance asymptotically. Moreover, we introduce a novel contribution of computing a soft planner policy and further updating a model-free policy yielding a less computationally expensive model-free agent as good as the model-based one. This model-based architecture serves as a foundation that can be extended to other human-computer interactive tasks allowing further advances in this direction.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1660812666",
                        "name": "Katya Kudashkina"
                    },
                    {
                        "authorId": "46660962",
                        "name": "Valliappa Chockalingam"
                    },
                    {
                        "authorId": "144639556",
                        "name": "Graham W. Taylor"
                    },
                    {
                        "authorId": "143913104",
                        "name": "Michael H. Bowling"
                    }
                ]
            }
        },
        {
            "contexts": [
                "State-of-the-art algorithms do not include steps to filter data in search of generalization and maximum sample efficiency [15], [16] \u2013 but such a direct training process can result in numerical instability in prediction and low effectiveness in terms of maximizing",
                "While state-of-the-art MBRL algorithms showcase strong asymptotic performance [15], [16], the computational requirements include substantial data storage to form the model and low-frequency control \u2013 even on a graphics processing unit."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cad288b72b34d57944acdcee723a9bac259f3312",
                "externalIds": {
                    "MAG": "3022437055",
                    "DBLP": "journals/corr/abs-2004-13194",
                    "ArXiv": "2004.13194",
                    "DOI": "10.1109/MARSS49294.2020.9307921",
                    "CorpusId": 216562426
                },
                "corpusId": 216562426,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cad288b72b34d57944acdcee723a9bac259f3312",
                "title": "Learning for Microrobot Exploration: Model-based Locomotion, Sparse-robust Navigation, and Low-power Deep Classification",
                "abstract": "Building intelligent autonomous systems at any scale is challenging. The sensing and computation constraints of a microrobot platform make the problems harder. We present improvements to learning-based methods for on-board learning of locomotion, classification, and navigation of microrobots. We show how simulated locomotion can be achieved with model-based reinforcement learning via on-board sensor data distilled into control. Next, we introduce a sparse, linear detector and a Dynamic Thresholding method to FAST Visual Odometry for improved navigation in the noisy regime of mm scale imagery. We end with a new image classifier capable of classification with fewer than one million multiply-and-accumulate (MAC) operations by combining fast downsampling, efficient layer structures and hard activation functions. These are promising steps toward using state-of-the-art algorithms in the power-limited world of edge-intelligence and microrobots.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2052363815",
                        "name": "Nathan Lambert"
                    },
                    {
                        "authorId": "1660861822",
                        "name": "Farhan Toddywala"
                    },
                    {
                        "authorId": "47188386",
                        "name": "B. Liao"
                    },
                    {
                        "authorId": "2064545858",
                        "name": "Eric Zhu"
                    },
                    {
                        "authorId": "2115973150",
                        "name": "Lydia Lee"
                    },
                    {
                        "authorId": "143648343",
                        "name": "K. Pister"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ef\ufb01ciency. Using these imaginary rollouts improves model-free reinforcement learning effectively if the model\u2019s predictions are accurate, and deteriorates the performance when the model is inaccurate [26, 11]. To address this problem, the model-based value expansion method [12] controls the depth of imagination to improve the performance of the model-based method by keeping the model-generated data accura"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "47a7b24fb4ae5f361047bfc2cbf67acfdc6f5d00",
                "externalIds": {
                    "ArXiv": "2004.08648",
                    "DBLP": "journals/corr/abs-2004-08648",
                    "MAG": "3106309118",
                    "DOI": "10.1109/TransAI49837.2020.00009",
                    "CorpusId": 215828084
                },
                "corpusId": 215828084,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/47a7b24fb4ae5f361047bfc2cbf67acfdc6f5d00",
                "title": "Modeling Survival in model-based Reinforcement Learning",
                "abstract": "Although recent model-free reinforcement learning algorithms have been shown to be capable of mastering complicated decision-making tasks, the sample complexity of these methods has remained a hurdle to utilizing them in many real-world applications. In this regard, model-based reinforcement learning proposes some remedies. Yet, inherently, model-based methods are more computationally expensive and susceptible to sub-optimality. One reason is that model-generated data are always less accurate than real data, and this often leads to inaccurate transition and reward function models. With the aim to mitigate this problem, this work presents the notion of survival by discussing cases in which the agent\u2019s goal is to survive and its analogy to maximizing the expected rewards. To that end, a substitute model for the reward function approximator is introduced that learns to avoid terminal states rather than to maximize accumulated rewards from safe states. Focusing on terminal states, as a small fraction of state-space, reduces the training effort drastically. Next, a model-based reinforcement learning method is proposed (Survive) to train an agent to avoid dangerous states through a safety map model built upon temporal credit assignment in the vicinity of terminal states. Finally, the performance of the presented algorithm is investigated, along with a comparison between the proposed and current methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "84086810",
                        "name": "Saeed Moazami"
                    },
                    {
                        "authorId": "2048982",
                        "name": "P. Doerschuk"
                    }
                ]
            }
        },
        {
            "contexts": [
                "PAL is also twice as efficient as MBPO (Janner et al., 2019), a state of the art hybrid model-based and model-free algorithm.",
                "For baselines, we consider MBPO (Janner et al., 2019), PETS (Chua et al., 2018), STEVE (Buckman et al., 2018), SLBO (Xu et al., 2018), and SAC (Haarnoja et al., 2018).",
                "Dyna (Sutton, 1990) and MBPO (Janner et al., 2019) use a learned model to provide additional learning targets for an actor-critic algorithm through short-horizon synthetic trajectories."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "325488b29a33ba8e3f061dda9aed7b2df52f0447",
                "externalIds": {
                    "ArXiv": "2004.07804",
                    "MAG": "3017367030",
                    "DBLP": "journals/corr/abs-2004-07804",
                    "CorpusId": 215786389
                },
                "corpusId": 215786389,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/325488b29a33ba8e3f061dda9aed7b2df52f0447",
                "title": "A Game Theoretic Framework for Model Based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (MBRL) has recently gained immense interest due to its potential for sample efficiency and ability to incorporate off-policy data. However, designing stable and efficient MBRL algorithms using rich function approximators have remained challenging. To help expose the practical challenges in MBRL and simplify algorithm design from the lens of abstraction, we develop a new framework that casts MBRL as a game between: (1) a policy player, which attempts to maximize rewards under the learned model; (2) a model player, which attempts to fit the real-world data collected by the policy player. For algorithm development, we construct a Stackelberg game between the two players, and show that it can be solved with approximate bi-level optimization. This gives rise to two natural families of algorithms for MBRL based on which player is chosen as the leader in the Stackelberg game. Together, they encapsulate, unify, and generalize many previous MBRL algorithms. Furthermore, our framework is consistent with and provides a clear basis for heuristics known to be important in practice from prior works. Finally, through experiments we validate that our proposed algorithms are highly sample efficient, match the asymptotic performance of model-free policy gradient, and scale gracefully to high-dimensional tasks like dexterous hand manipulation.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "19275599",
                        "name": "A. Rajeswaran"
                    },
                    {
                        "authorId": "2080746",
                        "name": "Igor Mordatch"
                    },
                    {
                        "authorId": "2109446216",
                        "name": "Vikash Kumar"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", [34], [35] for recent developments in the field."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8974e8d8828a86c9454668b0246fd34e6f1f8f0f",
                "externalIds": {
                    "ArXiv": "2003.08099",
                    "DBLP": "journals/corr/abs-2003-08099",
                    "MAG": "3011994177",
                    "DOI": "10.1109/TNNLS.2020.3016906",
                    "CorpusId": 212747951,
                    "PubMed": "32870801"
                },
                "corpusId": 212747951,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8974e8d8828a86c9454668b0246fd34e6f1f8f0f",
                "title": "A Hybrid Learning Method for System Identification and Optimal Control",
                "abstract": "We present a three-step method to perform system identification and optimal control of nonlinear systems. Our approach is mainly data-driven and does not require active excitation of the system to perform system identification. In particular, it is designed for systems for which only historical data under closed-loop control are available and where historical control commands exhibit low variability. In the first step, simple simulation models of the system are built and run under various conditions. In the second step, a neural network architecture is extensively trained on the simulation outputs to learn the system physics and retrained with historical data from the real system with stopping rules. These constraints avoid overfitting that arises by fitting closed-loop controlled systems. By doing so, we obtain one (or many) system model(s), represented by this architecture, whose behavior can be chosen to match more or less the real system. Finally, state-of-the-art reinforcement learning with a variant of domain randomization and distributed learning is used for optimal control of the system. We first illustrate the model identification strategy with a simple example, the pendulum with external torque. We then apply our method to model and optimize the control of a large building facility located in Switzerland. Simulation results demonstrate that this approach generates stable functional controllers that outperform on comfort and energy benchmark rule-based controllers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "72823264",
                        "name": "B. Schubnel"
                    },
                    {
                        "authorId": "29322806",
                        "name": "R. Carrillo"
                    },
                    {
                        "authorId": "1993426",
                        "name": "P. Alet"
                    },
                    {
                        "authorId": "133607451",
                        "name": "A. Hutter"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, model-based reinforcement learning has been shown to be more sample efficient in other domains [11], so it may be a viable direction to take in future work, especially in situations where the reward function is very expensive to evaluate."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ec8fc03ed1d5a9f42e3403ff8c725bd4c4138ac9",
                "externalIds": {
                    "DBLP": "conf/ispd/GoldieM20",
                    "ArXiv": "2003.08445",
                    "MAG": "3010672652",
                    "DOI": "10.1145/3372780.3378174",
                    "CorpusId": 213005486
                },
                "corpusId": 213005486,
                "publicationVenue": {
                    "id": "37534224-b04a-4979-b776-ed15df964f13",
                    "name": "ACM International Symposium on Physical Design",
                    "type": "conference",
                    "alternate_names": [
                        "ACM Int Symp Phys Des",
                        "Int Symp Phys Des",
                        "International Symposium on Physical Design",
                        "ISPD"
                    ],
                    "url": "http://www.ispd.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ec8fc03ed1d5a9f42e3403ff8c725bd4c4138ac9",
                "title": "Placement Optimization with Deep Reinforcement Learning",
                "abstract": "Placement Optimization is an important problem in systems and chip design, which consists of mapping the nodes of a graph onto a limited set of resources to optimize for an objective, subject to constraints. In this paper, we start by motivating reinforcement learning as a solution to the placement problem. We then give an overview of what deep reinforcement learning is. We next formulate the placement problem as a reinforcement learning problem, and show how this problem can be solved with policy gradient optimization. Finally, we describe lessons we have learned from training deep reinforcement learning policies across a variety of placement optimization problems.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46684455",
                        "name": "Anna Goldie"
                    },
                    {
                        "authorId": "1861312",
                        "name": "Azalia Mirhoseini"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We chose two function approximators for the learned residual dynamics to account for model learning approaches that use global function approximators such as neural networks (NN) [14], and local function approximators such as K-nearest neighbor regression (KNN) [25, 16]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "143210ec5b56098e101a785ed4402ba04a2b7159",
                "externalIds": {
                    "MAG": "3011473985",
                    "DBLP": "conf/rss/VemulaOBL20",
                    "ArXiv": "2003.04394",
                    "DOI": "10.15607/rss.2020.xvi.001",
                    "CorpusId": 212646198
                },
                "corpusId": 212646198,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/143210ec5b56098e101a785ed4402ba04a2b7159",
                "title": "Planning and Execution using Inaccurate Models with Provable Guarantees",
                "abstract": "Models used in modern planning problems to simulate outcomes of real world action executions are becoming increasingly complex, ranging from simulators that do physics-based reasoning to precomputed analytical motion primitives. However, robots operating in the real world often face situations not modeled by these models before execution. This imperfect modeling can lead to highly suboptimal or even incomplete behavior during execution. In this paper, we propose CMAX an approach for interleaving planning and execution. CMAX adapts its planning strategy online during real-world execution to account for any discrepancies in dynamics during planning, without requiring updates to the dynamics of the model. This is achieved by biasing the planner away from transitions whose dynamics are discovered to be inaccurately modeled, thereby leading to robot behavior that tries to complete the task despite having an inaccurate model. We provide provable guarantees on the completeness and efficiency of the proposed planning and execution framework under specific assumptions on the model, for both small and large state spaces. Our approach CMAX is shown to be efficient empirically in simulated robotic tasks including 4D planar pushing, and in real robotic experiments using PR2 involving a 3D pick-and-place task where the mass of the object is incorrectly modeled, and a 7D arm planning task where one of the joints is not operational leading to discrepancy in dynamics. The video of our physical robot experiments can be found at this https URL",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2387189",
                        "name": "Anirudh Vemula"
                    },
                    {
                        "authorId": "50276096",
                        "name": "Yash Oza"
                    },
                    {
                        "authorId": "1756566",
                        "name": "J. Bagnell"
                    },
                    {
                        "authorId": "145371551",
                        "name": "M. Likhachev"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[12] propose a monotonic model-based policy optimization (MBPO) to provide a performance guarantee."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "92cfd6553c742037a97338bbbcd9bfa0031ccf92",
                "externalIds": {
                    "MAG": "3008009191",
                    "ArXiv": "2002.11573",
                    "DBLP": "journals/corr/abs-2002-11573",
                    "CorpusId": 211506028
                },
                "corpusId": 211506028,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/92cfd6553c742037a97338bbbcd9bfa0031ccf92",
                "title": "Efficient reinforcement learning control for continuum robots based on Inexplicit Prior Knowledge",
                "abstract": "Compared to rigid robots that are often studied in reinforcement learning, the physical characteristics of some sophisticated robots such as software or continuum are more complicated. Moreover, recent reinforcement learning methods are data-inefficient and can not be directly deployed to the robot without simulation. In this paper, we propose an efficient reinforcement learning method based on inexplicit prior knowledge in response to such problems. The method is firstly corroborated by simulation and employed directly in the real world. By using our method, we can achieve visual active tracking and distance maintenance of a tendon-driven robot which will be critical in minimally-invasive procedures.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "10104472",
                        "name": "Junjia Liu"
                    },
                    {
                        "authorId": "1506971420",
                        "name": "Jiaying Shou"
                    },
                    {
                        "authorId": "2068055826",
                        "name": "Zhuang Fu"
                    },
                    {
                        "authorId": "2546704",
                        "name": "Hangfei Zhou"
                    },
                    {
                        "authorId": "35662013",
                        "name": "Rongli Xie"
                    },
                    {
                        "authorId": null,
                        "name": "Jun Zhang"
                    },
                    {
                        "authorId": "144992502",
                        "name": "Jian Fei"
                    },
                    {
                        "authorId": "46316793",
                        "name": "Yanna Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018) and data-efficient reinforcement learning algorithms (Nagabandi et al., 2018; Janner et al., 2019).",
                "Accurate transition models for macroscopic physical systems are critical components in control systems (Lenz et al., 2015; Kamthe and Deisenroth, 2017; Chua et al., 2018) and data-efficient reinforcement learning algorithms (Nagabandi et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "563ca4cda06665f4b90f8fce9bcb28c02e6872b9",
                "externalIds": {
                    "ArXiv": "2002.12880",
                    "DBLP": "journals/corr/abs-2002-12880",
                    "MAG": "3008803199",
                    "CorpusId": 211572990
                },
                "corpusId": 211572990,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/563ca4cda06665f4b90f8fce9bcb28c02e6872b9",
                "title": "Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data",
                "abstract": "The translation equivariance of convolutional layers enables convolutional neural networks to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data. We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51007156",
                        "name": "Marc Finzi"
                    },
                    {
                        "authorId": "2067201658",
                        "name": "S. Stanton"
                    },
                    {
                        "authorId": "7991830",
                        "name": "Pavel Izmailov"
                    },
                    {
                        "authorId": "145771261",
                        "name": "A. Wilson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 Model-based RL: Neural-network parameterized policy and ensemble of dynamic models trained using the algorithm,\nMBPO [34].",
                "For model-based baselines, we consider model-based policy optimization (MBPO) [34] and the demonstrator MPC.",
                "Suboptimality was analysed in [45] for MPC and in [34] for policies.",
                "2For all our experiments, training datapoints: PPO: 4\u00d7106, SAC: 4\u00d7106, MBPO: 2.4\u00d7 105, NLMPC: 104 (random) + 104 (demonstrations).",
                "\u2022 Model-based RL: Neural-network parameterized policy and ensemble of dynamic models trained using the algorithm, MBPO [34]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0237a5b7c5fa15f8c47dd8e68fd396ea763c5b01",
                "externalIds": {
                    "ArXiv": "2002.10451",
                    "CorpusId": 235313970
                },
                "corpusId": 235313970,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0237a5b7c5fa15f8c47dd8e68fd396ea763c5b01",
                "title": "Neural Lyapunov Model Predictive Control: Learning Safe Global Controllers from Sub-optimal Examples",
                "abstract": "\u2014With a growing interest in data-driven control techniques, Model Predictive Control (MPC) provides an op-portunity to exploit the surplus of data reliably, particularly while taking safety and stability into account. In many real-world and industrial applications, it is typical to have an existing control strategy, for instance, execution from a human operator. The objective of this work is to improve upon this unknown, safe but suboptimal policy by learning a new controller that retains safety and stability. Learning how to be safe is achieved directly from data and from a knowledge of the system constraints. The proposed algorithm alternatively learns the terminal cost and updates the MPC parameters according to a stability metric. The terminal cost is constructed as a Lyapunov function neural network with the aim of recovering or extending the stable region of the initial demonstrator using a short prediction horizon. Theorems that characterize the stability and performance of the learned MPC in the bearing of model uncertainties and sub-optimality due to function approximation are presented. The ef\ufb01cacy of the proposed algorithm is demonstrated on non-linear continuous control tasks with soft constraints. The proposed approach can improve upon the initial demonstrator also in practice and achieve better stability than popular reinforcement learning baselines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2061780867",
                        "name": "Mayank Mittal"
                    },
                    {
                        "authorId": "2066311",
                        "name": "Marco Gallieri"
                    },
                    {
                        "authorId": "49086703",
                        "name": "A. Quaglino"
                    },
                    {
                        "authorId": "2788694",
                        "name": "Seyed Sina Mirrazavi Salehian"
                    },
                    {
                        "authorId": "2065083612",
                        "name": "Jan Koutn'ik"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "48510fa42e636b9b144164367794d734e9cc82f8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2002-10451",
                    "MAG": "3007000796",
                    "CorpusId": 211296627
                },
                "corpusId": 211296627,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/48510fa42e636b9b144164367794d734e9cc82f8",
                "title": "Neural Lyapunov Model Predictive Control",
                "abstract": "This paper presents Neural Lyapunov MPC, an algorithm to alternately train a Lyapunov neural network and a stabilising constrained Model Predictive Controller (MPC), given a neural network model of the system dynamics. This extends recent works on Lyapunov networks to be able to train solely from expert demonstrations of one-step transitions. The learned Lyapunov network is used as the value function for the MPC in order to guarantee stability and extend the stable region. Formal results are presented on the existence of a set of MPC parameters, such as discount factors, that guarantees stability with a horizon as short as one. Robustness margins are also discussed and existing performance bounds on value function MPC are extended to the case of imperfect models. The approach is tested on unstable non-linear continuous control tasks with hard constraints. Results demonstrate that, when a neural network trained on short sequences is used for predictions, a one-step horizon Neural Lyapunov MPC can successfully reproduce the expert behaviour and significantly outperform longer horizon MPCs.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46880492",
                        "name": "Mayank K. Mittal"
                    },
                    {
                        "authorId": "2066311",
                        "name": "Marco Gallieri"
                    },
                    {
                        "authorId": "49086703",
                        "name": "A. Quaglino"
                    },
                    {
                        "authorId": "2788694",
                        "name": "Seyed Sina Mirrazavi Salehian"
                    },
                    {
                        "authorId": "2065083612",
                        "name": "Jan Koutn'ik"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This sharing can be potentially approached by Hindsight Experience Replay [6] or model-based RL [18, 32, 54]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "003987bfff295e76946bf430376af4fe3d466cb4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2002-08550",
                    "ArXiv": "2002.08550",
                    "MAG": "3007553593",
                    "CorpusId": 211205073
                },
                "corpusId": 211205073,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/003987bfff295e76946bf430376af4fe3d466cb4",
                "title": "Learning to Walk in the Real World with Minimal Human Effort",
                "abstract": "Reliable and stable locomotion has been one of the most fundamental challenges for legged robots. Deep reinforcement learning (deep RL) has emerged as a promising method for developing such control policies autonomously. In this paper, we develop a system for learning legged locomotion policies with deep RL in the real world with minimal human effort. The key difficulties for on-robot learning systems are automatic data collection and safety. We overcome these two challenges by developing a multi-task learning procedure, an automatic reset controller, and a safety-constrained RL framework. We tested our system on the task of learning to walk on three different terrains: flat ground, a soft mattress, and a doormat with crevices. Our system can automatically and efficiently learn locomotion skills on a Minitaur robot with little human intervention.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2248552",
                        "name": "Sehoon Ha"
                    },
                    {
                        "authorId": "1796652",
                        "name": "P. Xu"
                    },
                    {
                        "authorId": "2093186792",
                        "name": "Zhenyu Tan"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "1739176520",
                        "name": "Jie Tan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Within MBRL, commonly explored methods include actionconditional, next-step models (Oh et al., 2015; Ha & Schmidhuber, 2018; Chiappa et al., 2017; Schmidhuber, 2010; Xie et al., 2016; Deisenroth & Rasmussen, 2011; Lin & Mitchell, 1992; Li et al., 2015; Diuk et al., 2008; Igl et al., 2018; Ebert et al., 2018; Kaiser et al., 2019; Janner et al., 2019).",
                "\u2026next-step models (Oh et al., 2015; Ha & Schmidhuber, 2018; Chiappa et al., 2017; Schmidhuber, 2010; Xie et al., 2016; Deisenroth & Rasmussen, 2011; Lin & Mitchell, 1992; Li et al., 2015; Diuk et al., 2008; Igl et al., 2018; Ebert et al., 2018; Kaiser et al., 2019; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9d55314573ec254569df35ecc4cc8d464431f2cc",
                "externalIds": {
                    "MAG": "2996117162",
                    "ArXiv": "2002.02836",
                    "DBLP": "journals/corr/abs-2002-02836",
                    "CorpusId": 211066569
                },
                "corpusId": 211066569,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9d55314573ec254569df35ecc4cc8d464431f2cc",
                "title": "Causally Correct Partial Models for Reinforcement Learning",
                "abstract": "In reinforcement learning, we can learn a model of future observations and rewards, and use it to plan the agent's next actions. However, jointly modeling future observations can be computationally expensive or even intractable if the observations are high-dimensional (e.g. images). For this reason, previous works have considered partial models, which model only part of the observation. In this paper, we show that partial models can be causally incorrect: they are confounded by the observations they don't model, and can therefore lead to incorrect planning. To address this, we introduce a general family of partial models that are provably causally correct, yet remain fast because they do not need to fully model future observations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1748523",
                        "name": "Danilo Jimenez Rezende"
                    },
                    {
                        "authorId": "1841008",
                        "name": "Ivo Danihelka"
                    },
                    {
                        "authorId": "3065681",
                        "name": "G. Papamakarios"
                    },
                    {
                        "authorId": "145604319",
                        "name": "Nan Rosemary Ke"
                    },
                    {
                        "authorId": "35076395",
                        "name": "Ray Jiang"
                    },
                    {
                        "authorId": "143947744",
                        "name": "T. Weber"
                    },
                    {
                        "authorId": "144717963",
                        "name": "Karol Gregor"
                    },
                    {
                        "authorId": "20896818",
                        "name": "Hamza Merzic"
                    },
                    {
                        "authorId": "47963165",
                        "name": "Fabio Viola"
                    },
                    {
                        "authorId": "2116439278",
                        "name": "Jane X. Wang"
                    },
                    {
                        "authorId": "37955812",
                        "name": "Jovana Mitrovic"
                    },
                    {
                        "authorId": "143923544",
                        "name": "F. Besse"
                    },
                    {
                        "authorId": "2460849",
                        "name": "Ioannis Antonoglou"
                    },
                    {
                        "authorId": "1981334",
                        "name": "Lars Buesing"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d99028fbc40e0bb11b2c8e5a0d844d2ecfdeba4a",
                "externalIds": {
                    "DBLP": "conf/icml/BallPPCR20",
                    "MAG": "3005127431",
                    "ArXiv": "2002.02693",
                    "CorpusId": 211066218
                },
                "corpusId": 211066218,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d99028fbc40e0bb11b2c8e5a0d844d2ecfdeba4a",
                "title": "Ready Policy One: World Building Through Active Learning",
                "abstract": "Model-Based Reinforcement Learning (MBRL) offers a promising direction for sample efficient learning, often achieving state of the art results for continuous control tasks. However, many existing MBRL methods rely on combining greedy policies with exploration heuristics, and even those which utilize principled exploration bonuses construct dual objectives in an ad hoc fashion. In this paper we introduce Ready Policy One (RP1), a framework that views MBRL as an active learning problem, where we aim to improve the world model in the fewest samples possible. RP1 achieves this by utilizing a hybrid objective function, which crucially adapts during optimization, allowing the algorithm to trade off reward v.s. exploration at different stages of learning. In addition, we introduce a principled mechanism to terminate sample collection once we have a rich enough trajectory batch to improve the model. We rigorously evaluate our method on a variety of continuous control tasks, and demonstrate statistically significant gains over existing approaches.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2053179501",
                        "name": "Philip J. Ball"
                    },
                    {
                        "authorId": "1410302742",
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "authorId": "3124110",
                        "name": "Aldo Pacchiano"
                    },
                    {
                        "authorId": "1805203",
                        "name": "K. Choromanski"
                    },
                    {
                        "authorId": "145029236",
                        "name": "S. Roberts"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, these effects should be quantified in other MBRL algorithms such as MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019)."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ecbdc83a1aa8d196943d8997300be871b1c7c2dc",
                "externalIds": {
                    "MAG": "3107736070",
                    "ArXiv": "2002.04523",
                    "DBLP": "journals/corr/abs-2002-04523",
                    "CorpusId": 210155681
                },
                "corpusId": 210155681,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ecbdc83a1aa8d196943d8997300be871b1c7c2dc",
                "title": "Objective Mismatch in Model-based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, with little development of the general framework. In this paper, we identify a fundamental issue of the standard MBRL framework -- what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model w.r.t.~the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood of one-step ahead predictions is not always correlated with control performance. This observation highlights a critical limitation in the MBRL framework which will require further research to be fully understood and addressed. We propose an initial method to mitigate the mismatch issue by re-weighting dynamics model training. Building on it, we conclude with a discussion about other potential directions of research for addressing this issue.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2052363815",
                        "name": "Nathan Lambert"
                    },
                    {
                        "authorId": "1773498",
                        "name": "Brandon Amos"
                    },
                    {
                        "authorId": "2705825",
                        "name": "Omry Yadan"
                    },
                    {
                        "authorId": "35159852",
                        "name": "R. Calandra"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a34f3312c8ee4ab1ce3c9e1a76f81f942a05b7b8",
                "externalIds": {
                    "MAG": "2999107413",
                    "ArXiv": "2001.04515",
                    "DBLP": "journals/corr/abs-2001-04515",
                    "DOI": "10.1111/rssb.12465",
                    "CorpusId": 210473701
                },
                "corpusId": 210473701,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a34f3312c8ee4ab1ce3c9e1a76f81f942a05b7b8",
                "title": "Statistical inference of the value function for reinforcement learning in infinite\u2010horizon settings",
                "abstract": "Reinforcement learning is a general technique that allows an agent to learn an optimal policy and interact with an environment in sequential decision\u2010making problems. The goodness of a policy is measured by its value function starting from some initial state. The focus of this paper was to construct confidence intervals (CIs) for a policy\u2019s value in infinite horizon settings where the number of decision points diverges to infinity. We propose to model the action\u2010value state function (Q\u2010function) associated with a policy based on series/sieve method to derive its confidence interval. When the target policy depends on the observed data as well, we propose a SequentiAl Value Evaluation (SAVE) method to recursively update the estimated policy and its value estimator. As long as either the number of trajectories or the number of decision points diverges to infinity, we show that the proposed CI achieves nominal coverage even in cases where the optimal policy is not unique. Simulation studies are conducted to back up our theoretical findings. We apply the proposed method to a dataset from mobile health studies and find that reinforcement learning algorithms could help improve patient\u2019s health status. A Python implementation of the proposed procedure is available at https://github.com/shengzhang37/SAVE.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "6086301",
                        "name": "C. Shi"
                    },
                    {
                        "authorId": "122200570",
                        "name": "Shengyao Zhang"
                    },
                    {
                        "authorId": "2107350706",
                        "name": "W. Lu"
                    },
                    {
                        "authorId": "145401368",
                        "name": "R. Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, how to efficiently leverage imperfect models [34], and how to maximize the joint benefit by combining policy learning and motion planning (trajectory optimization) [31, 35], where a policy has the advantage of execution coherence and fast deployment while the trajectory planning has the competence of adaption to unseen or future situations."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "41c3bf144244859a489206ef0b1cac4ad278be48",
                "externalIds": {
                    "MAG": "2997158951",
                    "DBLP": "journals/corr/abs-1912-12970",
                    "ArXiv": "1912.12970",
                    "CorpusId": 209516172
                },
                "corpusId": 209516172,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/41c3bf144244859a489206ef0b1cac4ad278be48",
                "title": "Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework",
                "abstract": "This paper develops a Pontryagin differentiable programming (PDP) methodology, which establishes a unified framework to solve a broad class of learning and control tasks. The PDP methodology distinguishes from existing methods by two novel techniques: first, we differentiate the Pontryagin's Maximum Principle, and this allows us to obtain analytical gradient of a trajectory with respect to a tunable parameter of a system, thus enabling end-to-end learning of system dynamics, policy, or/and control objective function; and second, we propose an auxiliary control system in backward pass of the PDP framework, and show that the output of the auxiliary control system is exactly the gradient of the system trajectory with respect to the parameter, which can be iteratively obtained using control tools. We investigate three learning modes of the PDP: inverse reinforcement learning, system identification, and control/planning, respectively. We demonstrate the capability of the PDP in each learning mode using various high-dimensional systems, including multilink robot arm, 6-DoF maneuvering UAV, and 6-DoF rocket powered landing.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "9433338",
                        "name": "Wanxin Jin"
                    },
                    {
                        "authorId": "50218397",
                        "name": "Zhaoran Wang"
                    },
                    {
                        "authorId": "150358650",
                        "name": "Zhuoran Yang"
                    },
                    {
                        "authorId": "1891624",
                        "name": "S. Mou"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "59ae7276c90c0c84fb3aeffda0e04d50375f19c5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1912-10600",
                    "ArXiv": "1912.10600",
                    "MAG": "2996357468",
                    "DOI": "10.1002/int.22466",
                    "CorpusId": 209444957
                },
                "corpusId": 209444957,
                "publicationVenue": {
                    "id": "05528bac-d212-46a6-9c84-314d4bd77368",
                    "name": "International Journal of Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Intell Syst"
                    ],
                    "issn": "0884-8173",
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/36062",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/1098111X"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/59ae7276c90c0c84fb3aeffda0e04d50375f19c5",
                "title": "Direct and indirect reinforcement learning",
                "abstract": "Reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision\u2010making and control tasks. In this paper, we classify RL into direct and indirect RL according to how they seek the optimal policy of the Markov decision process problem. The former solves the optimal policy by directly maximizing an objective function using gradient descent methods, in which the objective function is usually the expectation of accumulative future rewards. The latter indirectly finds the optimal policy by solving the Bellman equation, which is the sufficient and necessary condition from Bellman's principle of optimality. We study policy gradient (PG) forms of direct and indirect RL and show that both of them can derive the actor\u2013critic architecture and can be unified into a PG with the approximate value function and the stationary state distribution, revealing the equivalence of direct and indirect RL. We employ a Gridworld task to verify the influence of different forms of PG, suggesting their differences and relationships experimentally. Finally, we classify current mainstream RL algorithms using the direct and indirect taxonomy, together with other ones, including value\u2010based and policy\u2010based, model\u2010based and model\u2010free.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "50463545",
                        "name": "Yang Guan"
                    },
                    {
                        "authorId": "2023891",
                        "name": "S. Li"
                    },
                    {
                        "authorId": "23637596",
                        "name": "Jingliang Duan"
                    },
                    {
                        "authorId": "2046898622",
                        "name": "Jie Li"
                    },
                    {
                        "authorId": "3649406",
                        "name": "Yangang Ren"
                    },
                    {
                        "authorId": "117675219",
                        "name": "B. Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While several existing actor-critic algorithms either use model-based estimates [Janner et al., 2019] or use IS corrections and truncations [Wang et al., 2017], we propose a novel approach towards extending doubly robust estimators, based on a combination of direct model-based approach and\u2026",
                "While several existing actor-critic algorithms either use model-based estimates [Janner et al., 2019] or use IS corrections and truncations [Wang et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7a7e1f9bdb51dd86fa474a4e722cb5efc3deb741",
                "externalIds": {
                    "ArXiv": "1912.05109",
                    "DBLP": "journals/corr/abs-1912-05109",
                    "MAG": "2995031981",
                    "CorpusId": 209202886
                },
                "corpusId": 209202886,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7a7e1f9bdb51dd86fa474a4e722cb5efc3deb741",
                "title": "Doubly Robust Off-Policy Actor-Critic Algorithms for Reinforcement Learning",
                "abstract": "We study the problem of off-policy critic evaluation in several variants of value-based off-policy actor-critic algorithms. Off-policy actor-critic algorithms require an off-policy critic evaluation step, to estimate the value of the new policy after every policy gradient update. Despite enormous success of off-policy policy gradients on control tasks, existing general methods suffer from high variance and instability, partly because the policy improvement depends on gradient of the estimated value function. In this work, we present a new way of off-policy policy evaluation in actor-critic, based on the doubly robust estimators. We extend the doubly robust estimator from off-policy policy evaluation (OPE) to actor-critic algorithms that consist of a reward estimator performance model. We find that doubly robust estimation of the critic can significantly improve performance in continuous control tasks. Furthermore, in cases where the reward function is stochastic that can lead to high variance, doubly robust critic estimation can improve performance under corrupted, stochastic reward signals, indicating its usefulness for robust and safe reinforcement learning.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "18014232",
                        "name": "Riashat Islam"
                    },
                    {
                        "authorId": "1411436226",
                        "name": "Raihan Seraj"
                    },
                    {
                        "authorId": "30625371",
                        "name": "Samin Yeasar Arnob"
                    },
                    {
                        "authorId": "144368601",
                        "name": "Doina Precup"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, prior works have learned such predictive models from interaction data alone [24,23,28,16,68]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "124ec2c95bc0fe417be2bb0d0701f88583d6a16a",
                "externalIds": {
                    "MAG": "2998259156",
                    "DBLP": "conf/eccv/SchmeckpeperXRT20",
                    "ArXiv": "1912.12773",
                    "DOI": "10.1007/978-3-030-58565-5_42",
                    "CorpusId": 209515451
                },
                "corpusId": 209515451,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/124ec2c95bc0fe417be2bb0d0701f88583d6a16a",
                "title": "Learning Predictive Models From Observation and Interaction",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "88726258",
                        "name": "Karl Schmeckpeper"
                    },
                    {
                        "authorId": "14484808",
                        "name": "A. Xie"
                    },
                    {
                        "authorId": "40900227",
                        "name": "Oleh Rybkin"
                    },
                    {
                        "authorId": "71692259",
                        "name": "Stephen Tian"
                    },
                    {
                        "authorId": "1751586",
                        "name": "Kostas Daniilidis"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based algorithms presented in [2, 3, 4, 5] achieve the same asymptotic performance as model-free algorithms while requiring an order of magnitude less data.",
                "Current model-based RL algorithms generally fall into one of three categories: Dyna-style algorithms, where the model is used to create imaginary experience for a model-free algorithm [10, 4, 7, 2, 11, 12, 5]; model predictive control (MPC) algorithms, where the model is used for planning at each time-step [13, 3]; and policy search with backpropagation-through-time approaches, which exploit the model derivatives [14, 15, 16, 17]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0337961db56cfeb1b73169b45b7dff1541fc5566",
                "externalIds": {
                    "DBLP": "conf/corl/ZhangCTA19",
                    "ArXiv": "1910.12453",
                    "MAG": "2981547136",
                    "CorpusId": 204904857
                },
                "corpusId": 204904857,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0337961db56cfeb1b73169b45b7dff1541fc5566",
                "title": "Asynchronous Methods for Model-Based Reinforcement Learning",
                "abstract": "Significant progress has been made in the area of model-based reinforcement learning. State-of-the-art algorithms are now able to match the asymptotic performance of model-free methods while being significantly more data efficient. However, this success has come at a price: state-of-the-art model-based methods require significant computation interleaved with data collection, resulting in run times that take days, even if the amount of agent interaction might be just hours or even minutes. When considering the goal of learning in real-time on real robots, this means these state-of-the-art model-based algorithms still remain impractical. In this work, we propose an asynchronous framework for model-based reinforcement learning methods that brings down the run time of these algorithms to be just the data collection time. We evaluate our asynchronous framework on a range of standard MuJoCo benchmarks. We also evaluate our asynchronous framework on three real-world robotic manipulation tasks. We show how asynchronous learning not only speeds up learning w.r.t wall-clock time through parallelization, but also further reduces the sample complexity of model-based approaches by means of improving the exploration and by means of effectively avoiding the policy overfitting to the deficiencies of learned dynamics models.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "49890268",
                        "name": "Yunzhi Zhang"
                    },
                    {
                        "authorId": "15593386",
                        "name": "I. Clavera"
                    },
                    {
                        "authorId": "2064087301",
                        "name": "Bo-Yu Tsai"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "model-based policy optimization algorithms which learn policies orQ-functions, parameterized by neural networks, on the estimated dynamics, using off-the-shelf model-free algorithms or their variants (Luo et al., 2019; Janner et al., 2019; Kaiser et al., 2019; Kurutach et al., 2018; Feinberg et al., 2018; Buckman et al., 2018), and 2.",
                ", 2019), and MBPO (Janner et al., 2019) with various architecture size.",
                ", 2019), which is called MBSAC, see Appendix A for details; (c) MBPO (Janner et al., 2019), the previous state-of-the-art model-based RL algorithm.",
                "\u2026model-free RL algorithm; (b) a computationally efficient variant of MBPO (Janner et al., 2019) that we developed using ideas from SLBO (Luo et al., 2019), which is called MBSAC, see Appendix A for details; (c) MBPO (Janner et al., 2019), the previous state-of-the-art model-based RL algorithm.",
                "These algorithms include both model-free algorithms such as DQN (Mnih et al., 2015) and SAC (Haarnoja et al., 2018), and model-based policy optimization algorithms such as SLBO (Luo et al., 2019) and MBPO (Janner et al., 2019).",
                ", 2018), the state-of-the-art model-free RL algorithm; (b) a computationally efficient variant of MBPO (Janner et al., 2019) that we developed using ideas from SLBO (Luo et al.",
                "We verify this claim on the randomly generated MDPs discussed in Section 5.1, by running DQN (Mnih et al., 2015), SLBO (Luo et al., 2019), and MBPO (Janner et al., 2019) with various architecture size.",
                "\u2026parameterized by neural networks, on the estimated dynamics, using off-the-shelf model-free algorithms or their variants (Luo et al., 2019; Janner et al., 2019; Kaiser et al., 2019; Kurutach et al., 2018; Feinberg et al., 2018; Buckman et al., 2018), and 2. model-based planning\u2026",
                "\u2026of three algorithms: (a)\nSAC (Haarnoja et al., 2018), the state-of-the-art model-free RL algorithm; (b) a computationally efficient variant of MBPO (Janner et al., 2019) that we developed using ideas from SLBO (Luo et al., 2019), which is called MBSAC, see Appendix A for details; (c) MBPO (Janner\u2026",
                "MBPO (Janner et al., 2019), STEVE (Buckman et al., 2018), and MVE (Feinberg et al., 2018) are model-based Q-learning-based policy optimization algorithms, which can be viewed as modern extensions and improvements of the early model-based Qlearning framework, Dyna (Sutton, 1990)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9eba601720dc5d9070875468bfd287a646d191f2",
                "externalIds": {
                    "DBLP": "conf/icml/DongLYFM20",
                    "MAG": "3034514558",
                    "CorpusId": 219687279
                },
                "corpusId": 219687279,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9eba601720dc5d9070875468bfd287a646d191f2",
                "title": "On the Expressivity of Neural Networks for Deep Reinforcement Learning",
                "abstract": "We compare the model-free reinforcement learning with the model-based approaches through the lens of the expressive power of neural networks for policies, $Q$-functions, and dynamics. We show, theoretically and empirically, that even for one-dimensional continuous state space, there are many MDPs whose optimal $Q$-functions and policies are much more complex than the dynamics. We hypothesize many real-world MDPs also have a similar property. For these MDPs, model-based planning is a favorable algorithm, because the resulting policies can approximate the optimal policy significantly better than a neural network parameterization can, and model-free or model-based policy optimization rely on policy parameterization. Motivated by the theory, we apply a simple multi-step model-based bootstrapping planner (BOOTS) to bootstrap a weak $Q$-function into a stronger policy. Empirical results show that applying BOOTS on top of model-based or model-free policy optimization algorithms at the test time improves the performance on MuJoCo benchmark tasks.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "66742509",
                        "name": "Kefan Dong"
                    },
                    {
                        "authorId": "1491625903",
                        "name": "Yuping Luo"
                    },
                    {
                        "authorId": "1901958",
                        "name": "Tengyu Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another direction is to use multi-step greedy in model-based RL\n(e.g., Kumar et al., 2016; Talvitie, 2017; Luo et al., 2018; Janner et al., 2019) and solve the surrogate decision problem with an approximate model.",
                "Another direction is to use multi-step greedy in model-based RL (e.g., Kumar et al., 2016; Talvitie, 2017; Luo et al., 2018; Janner et al., 2019) and solve the surrogate decision problem with an approximate model."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bb4999ad9bb6d24a9a9dd863fad312c5b0018fa9",
                "externalIds": {
                    "MAG": "3040891685",
                    "DBLP": "conf/icml/TomarEG20",
                    "CorpusId": 220496743
                },
                "corpusId": 220496743,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bb4999ad9bb6d24a9a9dd863fad312c5b0018fa9",
                "title": "Multi-step Greedy Reinforcement Learning Algorithms",
                "abstract": "Multi-step greedy policies have been extensively used in model-based reinforcement learning (RL), both when a model of the environment is available (e.g.,~in the game of Go) and when it is learned. In this paper, we explore their benefits in model-free RL, when employed using multi-step dynamic programming algorithms: $\\kappa$-Policy Iteration ($\\kappa$-PI) and $\\kappa$-Value Iteration ($\\kappa$-VI). These methods iteratively compute the next policy ($\\kappa$-PI) and value function ($\\kappa$-VI) by solving a surrogate decision problem with a shaped reward and a smaller discount factor. We derive model-free RL algorithms based on $\\kappa$-PI and $\\kappa$-VI in which the surrogate problem can be solved by any discrete or continuous action RL method, such as DQN and TRPO. We identify the importance of a hyper-parameter that controls the extent to which the surrogate problem is solved and suggest a way to set this parameter. When evaluated on a range of Atari and MuJoCo benchmark tasks, our results indicate that for the right range of $\\kappa$, our algorithms outperform DQN and TRPO. This shows that our multi-step greedy algorithms are general enough to be applied over any existing RL algorithm and can significantly improve its performance.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "119439163",
                        "name": "Manan Tomar"
                    },
                    {
                        "authorId": "27098848",
                        "name": "Yonathan Efroni"
                    },
                    {
                        "authorId": "1678622",
                        "name": "M. Ghavamzadeh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another direction the RL and control has been pursuing is on the combination of modelbased and model-free methods (Bansal et al., 2017; Okada et al., 2017; Jonschkowski et al., 2018; Pereira et al., 2018; Amos et al., 2018; Okada & Taniguchi, 2019; Janner et al., 2019; Pong et al., 2018)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4d5f904f923e5e031fb500a9e9ef7699ea9283de",
                "externalIds": {
                    "DBLP": "conf/icml/AmosY20",
                    "ArXiv": "1909.12830",
                    "MAG": "3034999548",
                    "CorpusId": 203591671
                },
                "corpusId": 203591671,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4d5f904f923e5e031fb500a9e9ef7699ea9283de",
                "title": "The Differentiable Cross-Entropy Method",
                "abstract": "We study the cross-entropy method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant that enables us to differentiate the output of CEM with respect to the objective function's parameters. In the machine learning setting this brings CEM inside of the end-to-end learning pipeline where this has otherwise been impossible. We show applications in a synthetic energy-based structured prediction task and in non-convex continuous control. In the control setting we show how to embed optimal action sequences into a lower-dimensional space. DCEM enables us to fine-tune CEM-based controllers with policy optimization.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1773498",
                        "name": "Brandon Amos"
                    },
                    {
                        "authorId": "13759615",
                        "name": "Denis Yarats"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, we employ the the ensemble of policies trick which could also improve other off-policy policy-gradient based methods such as MBPO, SAC, and PPO.",
                ", 2018) and model-based (Janner et al., 2019) baselines on continuous control benchmarks Hopper-v3 and HalfCheetah-v3 Each environments is evaluated with the canonical 1000-step variant of the task.",
                "We evaluate Neural-PSRL on the popular and widely studied MuJoCo continuous control tasks (Todorov et al., 2012) of HalfCheetahv3 and Hopper-v3 (Erez et al., 2012), comparing our method to Model Based Policy Optimization (MBPO) (Janner et al., 2019).",
                "For each run we averaged the last 50 (for Neural-PSRL and MBPO) and last 100 (for MBPO) returns, and take the average and standard deviation of these five values to report both mean and 95% confidence intervals.",
                "In Janner et al. (2019) this issue is addressed and a method for optimal Trajectory length is developed.",
                "Like in MBPO, we use the canonical 1000-step horizon with early termination versions of both tasks, and assume knowledge of the termination criteria, and for the sake of simplicity also assume knowledge of the reward distribution instead of learning it.",
                "For the results in table 1, we ran Neural-PSRL and MBPO for 400 epochs (so 400K steps), and used the 3000 epoch (so 3M step) runs discussed above for SAC.",
                "Further, in keeping with Janner et al. (2019) we assume knowledge of the terminal conditions.",
                ", 2012), comparing our method to Model Based Policy Optimization (MBPO) (Janner et al., 2019).",
                "Since MBPO clearly outperforms by a wide margin other reinforcement learning methods such as PPO (Schulman et al., 2017) and PETS (Chua et al., 2018), we compare only to the model-based MBPO and the model-free SAC.",
                "To the best of our knowledge, MBPO is the current state-of-the-art\non these tasks when the number of environment evaluations allowed is restricted (\u2264 500K steps)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "e52b0f99c0170506c938ca4a967f4790594fc504",
                "externalIds": {
                    "MAG": "3006543166",
                    "CorpusId": 212425534
                },
                "corpusId": 212425534,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e52b0f99c0170506c938ca4a967f4790594fc504",
                "title": "Posterior Sampling: Make Reinforcement Learning Sample Efficient Again",
                "abstract": "Machine learning thrives on leveraging structure in data, and many breakthroughs (e.g. convolutional networks) have been made by designing algorithms which exploit the underlying structure of a distribution. Reinforcement Learning agents interact with worlds that are similarly full of structure. For example, no sequence of actions an agent takes will ever cause the laws of physics to change, therefore an agent which learns to generalize such laws through time and space will have an advantage. Sample efficient reinforcement learning can be accomplished when assuming that the world has structure and designing learning algorithms which exploit this assumption without knowing the actual structure beforehand. Posterior Sampling for Reinforcement Learning (PSRL) (Strens, 2000) is such a method which assumes structure in the world and exploits it for learning. A PSLR learning agent first samples models of the environment which conform to both prior assumptions on the world\u2019s structure and past observations and then interacts with the true environment using a policy guided by the sampled model of the environment. While PSRL delivers theoretical Bayesian regret bounds, there are many open issues which must be addressed before PSRL can be applied to current benchmark continuous reinforcement reinforcement tasks. In this work, we identify these issues and find practical solutions to them leading to a novel algorithm we call Neural-PSRL1. We validate the algorithm\u2019s effectiveness by achieving state-of-the-art results in the HalfCheetah-v3 and Hopper-v3 domains.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "49972304",
                        "name": "C. Seward"
                    },
                    {
                        "authorId": "35959543",
                        "name": "Urs M. Bergmann"
                    },
                    {
                        "authorId": "2742129",
                        "name": "Roland Vollgraf"
                    },
                    {
                        "authorId": "3308557",
                        "name": "S. Hochreiter"
                    }
                ]
            }
        },
        {
            "contexts": [
                "0 50000 100000 150000 Number of datapoints\n\u22120.2\n\u22120.1\n0.0\nT as\nk R\new ar\nd\nBaoding Balls\nMBPO PETS Nagabandi et. al SAC NPG PDDM (Ours)\nFigure 8: PDDM outperforms prior model-based and modelfree methods.",
                "al [20] learns a deterministic neural network model, combined with a random shooting MPC controller; PETS [27] combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation; NPG [33] is a model-free natural policy gradient method, and has been used in prior work on learning manipulation skills [4]; SAC [34] is an off-policy model-free RL algorithm; MBPO [35] is a recent hybrid approach that uses data from its model to accelerate policy learning.",
                "In this section, we compare our method to the following state-of-the-art model-based and model-free RL algorithms: Nagabandi et. al [20] learns a deterministic neural network model, combined with a random shooting MPC controller; PETS [27] combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation; NPG [33] is a model-free natural policy gradient method, and has been used in prior work on learning manipulation skills [4]; SAC [34] is an off-policy model-free RL algorithm; MBPO [35] is a recent hybrid approach that uses data from its model to accelerate policy learning."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7a450675968d31b8363e21fb5d5b72474c128076",
                "externalIds": {
                    "ArXiv": "1909.11652",
                    "DBLP": "journals/corr/abs-1909-11652",
                    "MAG": "2975909688",
                    "CorpusId": 202750286
                },
                "corpusId": 202750286,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7a450675968d31b8363e21fb5d5b72474c128076",
                "title": "Deep Dynamics Models for Learning Dexterous Manipulation",
                "abstract": "Dexterous multi-fingered hands can provide robots with the ability to flexibly perform a wide range of manipulation skills. However, many of the more complex behaviors are also notoriously difficult to control: Performing in-hand object manipulation, executing finger gaits to move objects, and exhibiting precise fine motor skills such as writing, all require finely balancing contact forces, breaking and reestablishing contacts repeatedly, and maintaining control of unactuated objects. Learning-based techniques provide the appealing possibility of acquiring these skills directly from data, but current learning approaches either require large amounts of data and produce task-specific policies, or they have not yet been shown to scale up to more complex and realistic tasks requiring fine motor skills. In this work, we demonstrate that our method of online planning with deep dynamics models (PDDM) addresses both of these limitations; we show that improvements in learned dynamics models, together with improvements in online model-predictive control, can indeed enable efficient and effective learning of flexible contact-rich dexterous manipulation skills -- and that too, on a 24-DoF anthropomorphic hand in the real world, using just 4 hours of purely real-world data to learn to simultaneously coordinate multiple free-floating objects. Videos can be found at this https URL",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "3195183",
                        "name": "Anusha Nagabandi"
                    },
                    {
                        "authorId": "70162540",
                        "name": "K. Konolige"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "2109446216",
                        "name": "Vikash Kumar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most previous works of MBRL adopt supervised learning with `2-based errors [Clavera et al., 2018; Kurutach et al., 2018; Luo et al., 2019] or maximum likelihood [Janner et al., 2019], to obtain an environment model that synthesizes real transitions.",
                ", 2019] or maximum likelihood [Janner et al., 2019], to obtain an environment model that synthesizes real transitions."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "2a3173d38a6e2dd613f1ebbd01d1fe485fc4a70d",
                "externalIds": {
                    "ArXiv": "1909.11821",
                    "DBLP": "journals/corr/abs-1909-11821",
                    "MAG": "2976281930",
                    "CorpusId": 202889287
                },
                "corpusId": 202889287,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2a3173d38a6e2dd613f1ebbd01d1fe485fc4a70d",
                "title": "Model Imitation for Model-Based Reinforcement Learning",
                "abstract": "Model-based reinforcement learning (MBRL) aims to learn a dynamic model to reduce the number of interactions with real-world environments. However, due to estimation error, rollouts in the learned model, especially those of long horizon, fail to match the ones in real-world environments. This mismatching has seriously impacted the sample complexity of MBRL. The phenomenon can be attributed to the fact that previous works employ supervised learning to learn the one-step transition models, which has inherent difficulty ensuring the matching of distributions from multi-step rollouts. Based on the claim, we propose to learn the synthesized model by matching the distributions of multi-step rollouts sampled from the synthesized model and the real ones via WGAN. We theoretically show that matching the two can minimize the difference of cumulative rewards between the real transition and the learned one. Our experiments also show that the proposed model imitation method outperforms the state-of-the-art in terms of sample complexity and average return.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "31609618",
                        "name": "Yueh-Hua Wu"
                    },
                    {
                        "authorId": "32037089",
                        "name": "Ting-Han Fan"
                    },
                    {
                        "authorId": "1693135",
                        "name": "P. Ramadge"
                    },
                    {
                        "authorId": "2093560213",
                        "name": "H. Su"
                    }
                ]
            }
        },
        {
            "contexts": [
                "L G\n] 1\n4 O\nct 2\n01 9\nIn this work, we derive and empirically validate model-free deep RL (DRL) implementations of \u03ba-PI and \u03ba-VI.",
                "Furthermore, and although in this work we focused on model-free DRL, it is arguably more natural to use multi-step DP in model-based DRL (e.g.,Kumar et al., 2016; Talvitie, 2017; Luo et al., 2018; Janner et al., 2019).",
                "1), we use standard policy evaluation deep RL (DRL) algorithms."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "56e6485a960d0ef54d965de3b7060fac67c912d9",
                "externalIds": {
                    "MAG": "2978694579",
                    "DBLP": "journals/corr/abs-1910-02919",
                    "ArXiv": "1910.02919",
                    "CorpusId": 203836125
                },
                "corpusId": 203836125,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/56e6485a960d0ef54d965de3b7060fac67c912d9",
                "title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning",
                "abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "119439163",
                        "name": "Manan Tomar"
                    },
                    {
                        "authorId": "27098848",
                        "name": "Yonathan Efroni"
                    },
                    {
                        "authorId": "1678622",
                        "name": "M. Ghavamzadeh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[7, 15, 18] utilize model ensembles to reduce model overfitting for policy learning."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1690d6db86ed751bf7fb29fb768d1418ba579abc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1912-01188",
                    "ArXiv": "1912.01188",
                    "MAG": "2991934429",
                    "CorpusId": 208547857
                },
                "corpusId": 208547857,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1690d6db86ed751bf7fb29fb768d1418ba579abc",
                "title": "Adaptive Online Planning for Continual Lifelong Learning",
                "abstract": "We study learning control in an online reset-free lifelong learning scenario, where mistakes can compound catastrophically into the future and the underlying dynamics of the environment may change. Traditional model-free policy learning methods have achieved successes in difficult tasks due to their broad flexibility, but struggle in this setting, as they can activate failure modes early in their lifetimes which are difficult to recover from and face performance degradation as dynamics change. On the other hand, model-based planning methods learn and adapt quickly, but require prohibitive levels of computational resources. We present a new algorithm, Adaptive Online Planning (AOP), that achieves strong performance in this setting by combining model-based planning with model-free learning. By approximating the uncertainty of the model-free components and the planner performance, AOP is able to call upon more extensive planning only when necessary, leading to reduced computation times, while still gracefully adapting behaviors in the face of unpredictable changes in the world -- even when traditional RL fails.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2070275468",
                        "name": "Kevin Lu"
                    },
                    {
                        "authorId": "2080746",
                        "name": "Igor Mordatch"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "dbd0acabcad0392020f7e6572eacc84f2a7efc74",
                "externalIds": {
                    "MAG": "2996191627",
                    "CorpusId": 208192232
                },
                "corpusId": 208192232,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dbd0acabcad0392020f7e6572eacc84f2a7efc74",
                "title": "Policy Optimization by Local Improvement through Search",
                "abstract": "Imitation learning has emerged as a powerful strategy for learning initial policies that can be refined with reinforcement learning techniques. Most strategies in imitation learning, however, rely on per-step supervision either from expert demonstrations, referred to as behavioral cloning (Pomerleau, 1989; 1991) or from interactive expert policy queries such as DAgger (Ross et al., 2011). These strategies differ on the state distribution at which the expert actions are collected \u2013 the former using the state distribution of the expert, the latter using the state distribution of the policy being trained. However, the learning signal in both cases arises from the expert actions. On the other end of the spectrum, approaches rooted in Policy Iteration, such as Dual Policy Iteration (Sun et al., 2018b) do not choose next step actions based on an expert, but instead use planning or search over the policy to choose an action distribution to train towards. However, this can be computationally expensive, and can also end up training the policy on a state distribution that is far from the current policy\u2019s induced distribution. In this paper, we propose an algorithm that finds a middle ground by using Monte Carlo Tree Search (MCTS) (Kocsis & Szepesv\u00e1ri, 2006) to perform local trajectory improvement over rollouts from the policy. We provide theoretical justification for both the proposed local trajectory search algorithm and for our use of MCTS as a local policy improvement operator. We also show empirically that our method (Policy Optimization by Local Improvement through Search or POLISh) is much faster than methods that plan globally, speeding up training by a factor of up to 14 in wall clock time. Furthermore, the resulting policy outperforms strong baselines in both reinforcement learning and imitation learning.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2319190",
                        "name": "Jialin Song"
                    },
                    {
                        "authorId": "24272604",
                        "name": "J. Jiang"
                    },
                    {
                        "authorId": "2112229",
                        "name": "A. Yazdanbakhsh"
                    },
                    {
                        "authorId": "2714003",
                        "name": "Ebrahim M. Songhori"
                    },
                    {
                        "authorId": "46684455",
                        "name": "Anna Goldie"
                    },
                    {
                        "authorId": "3111912",
                        "name": "N. Jaitly"
                    },
                    {
                        "authorId": "1861312",
                        "name": "Azalia Mirhoseini"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several methods incorporate the model uncertainty into policy updates, by using Gaussian processes and moment matching approximations (Deisenroth and Rasmussen 2011), Bayesian neural networks (Gal, McAllister, and Rasmussen 2016) or ensembles of forward models (Chua et al. 2018; Kurutach et al. 2018; Janner et al. 2019; Buckman et al. 2018).",
                "\u2026into policy updates, by using Gaussian processes and moment matching approximations (Deisenroth and Rasmussen 2011), Bayesian neural networks (Gal, McAllister, and Rasmussen 2016) or ensembles of forward models (Chua et al. 2018; Kurutach et al. 2018; Janner et al. 2019; Buckman et al. 2018)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3f9f572ec86ad4d2181233f09ea82bb6d4d35fd9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1909-04115",
                    "MAG": "2998066995",
                    "ArXiv": "1909.04115",
                    "DOI": "10.1609/AAAI.V34I04.5791",
                    "CorpusId": 202542702
                },
                "corpusId": 202542702,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3f9f572ec86ad4d2181233f09ea82bb6d4d35fd9",
                "title": "Gradient-Aware Model-based Policy Search",
                "abstract": "Traditional model-based reinforcement learning approaches learn a model of the environment dynamics without explicitly considering how it will be used by the agent. In the presence of misspecified model classes, this can lead to poor estimates, as some relevant available information is ignored. In this paper, we introduce a novel model-based policy search approach that exploits the knowledge of the current agent policy to learn an approximate transition model, focusing on the portions of the environment that are most relevant for policy improvement. We leverage a weighting scheme, derived from the minimization of the error on the model-based policy gradient estimator, in order to define a suitable objective function that is optimized for learning the approximate transition model. Then, we integrate this procedure into a batch policy improvement algorithm, named Gradient-Aware Model-based Policy Search (GAMPS), which iteratively learns a transition model and uses it, together with the collected trajectories, to compute the new policy parameters. Finally, we empirically validate GAMPS on benchmark domains analyzing and discussing its properties.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1410392725",
                        "name": "P. D'Oro"
                    },
                    {
                        "authorId": "24717227",
                        "name": "Alberto Maria Metelli"
                    },
                    {
                        "authorId": "46188113",
                        "name": "Andrea Tirinzoni"
                    },
                    {
                        "authorId": "145388375",
                        "name": "M. Papini"
                    },
                    {
                        "authorId": "1792167",
                        "name": "Marcello Restelli"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1e32e45c17132060b2c808824c9f44f555be1cc4",
                "externalIds": {
                    "MAG": "2997633275",
                    "DBLP": "conf/sgai/AndersenGG19",
                    "ArXiv": "1907.11971",
                    "DOI": "10.1007/978-3-030-34885-4_3",
                    "CorpusId": 198968376
                },
                "corpusId": 198968376,
                "publicationVenue": {
                    "id": "f3bfa077-18b6-4ca9-a559-a809b86a837f",
                    "name": "SGAI Conferences",
                    "type": "conference",
                    "alternate_names": [
                        "SGAI",
                        "SGAI Conf"
                    ],
                    "url": "http://www.bcs-sgai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1e32e45c17132060b2c808824c9f44f555be1cc4",
                "title": "Towards Model-based Reinforcement Learning for Industry-near Environments",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "33999307",
                        "name": "Per-Arne Andersen"
                    },
                    {
                        "authorId": "1833672",
                        "name": "M. G. Olsen"
                    },
                    {
                        "authorId": "2493161",
                        "name": "Ole-Christoffer Granmo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this paper, we propose a model-based learning [13, 14, 15] framework that significantly improves sample efficiency and task generalization compared to model-free methods."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d078f720a7fb0e1961a17ea967332599e6d2b692",
                "externalIds": {
                    "DBLP": "conf/corl/YangCIZTS19",
                    "ArXiv": "1907.03613",
                    "MAG": "2954829582",
                    "CorpusId": 195833719
                },
                "corpusId": 195833719,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d078f720a7fb0e1961a17ea967332599e6d2b692",
                "title": "Data Efficient Reinforcement Learning for Legged Robots",
                "abstract": "We present a model-based framework for robot locomotion that achieves walking based on only 4.5 minutes (45,000 control steps) of data collected on a quadruped robot. To accurately model the robot's dynamics over a long horizon, we introduce a loss function that tracks the model's prediction over multiple timesteps. We adapt model predictive control to account for planning latency, which allows the learned model to be used for real time control. Additionally, to ensure safe exploration during model learning, we embed prior knowledge of leg trajectories into the action space. The resulting system achieves fast and robust locomotion. Unlike model-free methods, which optimize for a particular task, our planner can use the same learned dynamics for various tasks, simply by changing the reward function. To the best of our knowledge, our approach is more than an order of magnitude more sample efficient than current model-free methods.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2108795581",
                        "name": "Yuxiang Yang"
                    },
                    {
                        "authorId": "2758571",
                        "name": "Ken Caluwaerts"
                    },
                    {
                        "authorId": "2106754",
                        "name": "Atil Iscen"
                    },
                    {
                        "authorId": "28292148",
                        "name": "Tingnan Zhang"
                    },
                    {
                        "authorId": "1739176520",
                        "name": "Jie Tan"
                    },
                    {
                        "authorId": "1808676",
                        "name": "Vikas Sindhwani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this work, our predictive model serves to accelerate task learning by separately addressing representation learning, in contrast to existing model-based RL approaches, which use predictive models either for generating cheap synthetic experience [51, 22, 32] or for planning into the future [11, 13, 46, 9, 55, 26].",
                "[32] M."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "69d1ee8a99f55e9228f33fdb3a0339541ad1201c",
                "externalIds": {
                    "ArXiv": "1907.00953",
                    "DBLP": "journals/corr/abs-1907-00953",
                    "MAG": "2954974210",
                    "CorpusId": 195767454
                },
                "corpusId": 195767454,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/69d1ee8a99f55e9228f33fdb3a0339541ad1201c",
                "title": "Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model",
                "abstract": "Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional observation spaces present a number of challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model's learned latent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "49250083",
                        "name": "Alex X. Lee"
                    },
                    {
                        "authorId": "3195183",
                        "name": "Anusha Nagabandi"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "db1614b97aec1e0ead9554a038aa0f8dd9a26e30",
                "externalIds": {
                    "MAG": "2996347495",
                    "DBLP": "journals/corr/abs-1906-08649",
                    "ArXiv": "1906.08649",
                    "CorpusId": 195218755
                },
                "corpusId": 195218755,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/db1614b97aec1e0ead9554a038aa0f8dd9a26e30",
                "title": "Exploring Model-based Planning with Policy Networks",
                "abstract": "Model-based reinforcement learning (MBRL) with model-predictive control or online planning has shown great potential for locomotion control tasks in terms of both sample efficiency and asymptotic performance. Despite their initial successes, the existing planning methods search from candidate sequences randomly generated in the action space, which is inefficient in complex high-dimensional environments. In this paper, we propose a novel MBRL algorithm, model-based policy planning (POPLIN), that combines policy networks with online planning. More specifically, we formulate action planning at each time-step as an optimization problem using neural networks. We experiment with both optimization w.r.t. the action sequences initialized from the policy network, and also online optimization directly w.r.t. the parameters of the policy network. We show that POPLIN obtains state-of-the-art performance in the MuJoCo benchmarking environments, being about 3x more sample efficient than the state-of-the-art algorithms, such as PETS, TD3 and SAC. To explain the effectiveness of our algorithm, we show that the optimization surface in parameter space is smoother than in action space. Further more, we found the distilled policy network can be effectively applied without the expansive model predictive control during test time for some environments such as Cheetah. Code is released in this https URL.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "3428549",
                        "name": "Tingwu Wang"
                    },
                    {
                        "authorId": "2503659",
                        "name": "Jimmy Ba"
                    }
                ]
            }
        },
        {
            "contexts": [
                "3: for N epochs do 4: Train model E\u03b8 on Denv via maximum likelihood 5: Unroll M trajectories int he model under \u03c0\u03c8; add to Dmodel 6: Take action in environment according to \u03c0\u03c8; add to Denv 7: for G gradient updates do 8: Calculate normal policy loss L(\u03c0\u03c8,Dmodel) as in MBPO (Janner et al., 2019) 9: Sample \u3008st, at, st+1, rt\u3009 uniformly from Dmodel 10: Rollout \u03c0 starting from st under E\u03b8 for Ttrain steps and compute the total reward R 11: Compute the worst-case reward Rmin using Algorithm 2 over horizon Ttrain.",
                "We implement CAROL on top of the MBPO (Janner et al., 2019) model-based RL algorithm using the implementation from Pineda et al. (2021).",
                "We compare CAROL with the following methods: (1) MBPO (Janner et al., 2019), our base algorithm for policy optimization.",
                "During exploration, our algorithm learns a model of the environment using an existing model-based policy optimization algorithm (Janner et al., 2019).",
                "\u20266: Take action in environment according to \u03c0\u03c8; add to Denv 7: for G gradient updates do 8: Calculate normal policy loss Lnormal(\u03c0\u03c8,Dmodel) as in MBPO (Janner et al., 2019) 9: Sample \u3008st, at, st+1, rt\u3009 uniformly from Dmodel 10: Rollout \u03c0 starting from st under E\u03b8 for Ttrain steps and compute\u2026",
                "We implement CAROL on top of the MBPO (Janner et al., 2019) model-based RL algorithm using the implementation from Pineda et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f07536cd82bc575d8ed5a23c7c3a56339e010722",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-11374",
                    "DOI": "10.48550/arXiv.2301.11374",
                    "CorpusId": 256358782
                },
                "corpusId": 256358782,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f07536cd82bc575d8ed5a23c7c3a56339e010722",
                "title": "Policy Optimization with Robustness Certificates",
                "abstract": "We present a policy optimization framework in which the learned policy comes with a machine-checkable certi\ufb01cate of adversarial robustness . Our approach, called C AROL , learns a model of the environment. In each learning iteration, it uses the current version of this model and an external abstract interpreter to construct a differentiable signal for provable robustness. This signal is used to guide policy learning, and the abstract interpretation used to construct it directly leads to the robustness certi\ufb01cate returned at convergence. We give a theoretical analysis that bounds the worst-case accumulative reward of C AROL . We also experimentally evaluate C AROL on four MuJoCo environments. On these tasks, which involve continuous state and action spaces, C AROL learns certi\ufb01ed policies that have performance comparable to the (non-certi\ufb01ed) policies learned using state-of-the-art robust RL methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46962421",
                        "name": "Chenxi Yang"
                    },
                    {
                        "authorId": "2064997780",
                        "name": "Greg Anderson"
                    },
                    {
                        "authorId": "35865989",
                        "name": "Swarat Chaudhuri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We train\nan ensemble of seven such neural networks by following prior work [Janner et al., 2019].",
                "It has been revealed that directly applying model-based online RL methods like MBPO [Janner et al., 2019] fails on offline datasets [Yu et al.",
                "The practical implementation of TATU can be generally divided into three steps: Training Dynamics Models: Following prior work [Janner et al., 2019], we train the dynamics model P\u0302 (\u00b7|s, a) with a neural network p\u0302\u03c8(s\u2032|s, a) parameterized by \u03c8 that produces a Gaussian distribution over the next\u2026",
                "It has been revealed that directly applying model-based online RL methods like MBPO [Janner et al., 2019] fails on offline datasets [Yu et al., 2020].",
                "The practical implementation of TATU can be generally divided into three steps: Training Dynamics Models: Following prior work [Janner et al., 2019], we train the dynamics model P\u0302 (\u00b7|s, a) with a neural network p\u0302\u03c8(s|s, a) parameterized by \u03c8 that produces a Gaussian distribution over the next state, i."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3ed097d7351c32bb971a00d981281b5a9c28facd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-04660",
                    "DOI": "10.48550/arXiv.2304.04660",
                    "CorpusId": 258048479
                },
                "corpusId": 258048479,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3ed097d7351c32bb971a00d981281b5a9c28facd",
                "title": "Uncertainty-driven Trajectory Truncation for Model-based Offline Reinforcement Learning",
                "abstract": "Equipped with the trained environmental dynamics, model-based of\ufb02ine reinforcement learning (RL) algorithms can often successfully learn good poli-cies from \ufb01xed-sized datasets, even some datasets with poor quality. Unfortunately, however, it can not be guaranteed that the generated samples from the trained dynamics model are reliable (e.g., some synthetic samples may lie outside of the support region of the static dataset). To address this issue, we propose T r a jectory T runcation with U ncertainty (TATU), which adaptively truncates the synthetic trajectory if the accumulated uncertainty along the trajectory is too large. We theoretically show the performance bound of TATU to justify its bene-\ufb01ts. To empirically show the advantages of TATU, we \ufb01rst combine it with two classical model-based of\ufb02ine RL algorithms, MOPO and COMBO. Furthermore, we integrate TATU with several off-the-shelf model-free of\ufb02ine RL algorithms, e.g., BCQ. Experimental results on the D4RL benchmark show that TATU signi\ufb01cantly improves their performance, often by a large margin.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155570419",
                        "name": "Junjie Zhang"
                    },
                    {
                        "authorId": "2008151131",
                        "name": "Jiafei Lyu"
                    },
                    {
                        "authorId": "2125106047",
                        "name": "Xiaoteng Ma"
                    },
                    {
                        "authorId": "30411824",
                        "name": "Jiangpeng Yan"
                    },
                    {
                        "authorId": "2146157882",
                        "name": "Jun Yang"
                    },
                    {
                        "authorId": "2187301367",
                        "name": "Le Wan"
                    },
                    {
                        "authorId": "2116523082",
                        "name": "Xiu Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026gradient targets in temporal difference learning by backpropagating through the learned dynamics; (v) Model-Based Policy Optimization (MBPO) method (Janner et al., 2019b), which shows that using short model-generated rollouts branched from real data could benefit model-based algorithms; (vi)\u2026",
                "All network structure, including model, critic, and policy are the same as MAAC (Clavera et al., 2019) and MBPO (Janner et al., 2019b).",
                "With several key innovations (Janner et al., 2019a; Clavera et al., 2019), model-based RL algorithms have shown outstanding data efficiency and performance compared to their model-free counterparts, which make it possible to be applied in real-world physical systems when data collection is arduous\u2026",
                "In our method, for a fair comparison, except the D3P planning, we keep the model learning , policy learning, and Q-function learning to be the same as prior work (Janner et al., 2019b; Clavera et al., 2019).",
                "Therefore, the sample efficiency of our method is comparable with MBPO and MAAC which also used the same state augmentation strategy.",
                "One is to serve the learned model as a black-box simulator to generate the data (Janner et al., 2019b; Yu et al., 2020; Lee et al., 2020).",
                "Most of the model-based RL methods make the decision by using the learned policy solely (Janner et al., 2019b; Yu et al., 2020; Clavera et al., 2019; Hafner et al., 2021).",
                "Janner et al. (2019b) is a representing work of this line.",
                "When doing planning and rollout with the learned model to generate fake data, we follow the method used by Janner et al. (2019a); Clavera et al. (2019) to truncate the trajectory and use Q-function to approximate the return after the truncation.",
                "Noting that our planner is built upon the framework of MBPO and MAAC.",
                "The detailed hyper-parameters are summarized in Table 1, and refer to Janner et al. (2019b); Clavera et al. (2019) for more details.",
                "Model-based reinforcement learning (RL) (Janner et al., 2019a; Yu et al., 2020; Schrittwieser et al., 2020; Hafner et al., 2021) has shown its promise to be a general-purpose tool for solving sequential decision-making problems.",
                "Following Janner et al. (2019a) and Clavera et al. (2019), we use parametric neural networks to approximate the transition function, reward function, policy function and Q-value function with the following objective function to be optimized Jf (\u03c8) = E [ log f(xt+1|xt, at) ] , Jr(\u03c9) = E [ log\u2026",
                "To show the effectiveness of our algorithm, we compare our method on six classical continuous control tasks against the following state-of-the-art model-free and model-based RL algorithms: (i) Soft Actor-Critic (SAC) (Haarnoja et al., 2018), a popular off-policy actor-critic RL algorithm based on maximum entropy RL framework; (ii) SVG(1) (Heess et al., 2015a), which first uses dynamics derivatives in model-based RL; (iii) STochastic Ensemble Value Expansion (STEVE) method (Buckman et al., 2018), which utilizes the learned models only when the uncertainty of the learned model is not too high; (iv) Model-based Action-Gradient-Estimator policy optimization (MAGE) method (D\u2019Oro & Jas\u0301kowski, 2020), which computes gradient targets in temporal difference learning by backpropagating through the learned dynamics; (v) Model-Based Policy Optimization (MBPO) method (Janner et al., 2019b), which shows that using short model-generated rollouts branched from real data could benefit model-based algorithms; (vi) Model-Augmented Actor-Critic (MAAC) (Clavera et al., 2019) method, which exploits the learned model by computing the analytic gradient of the returns with respect to the policy.",
                "In the first class, the models play an auxiliary role to only affect the decision-making by helping the policy learning (Janner et al., 2019b; Clavera et al., 2019).",
                "The results are consistent with prior work Janner et al. (2019b); Clavera et al. (2019)."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "42557a01daed167f328e025a9c8fea1889137abc",
                "externalIds": {
                    "CorpusId": 259861487
                },
                "corpusId": 259861487,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/42557a01daed167f328e025a9c8fea1889137abc",
                "title": "M AKING B ETTER D ECISION BY D IRECTLY P LANNING IN C ONTINUOUS C ONTROL",
                "abstract": "By properly utilizing the learned environment model, model-based reinforcement learning methods can improve the sample efficiency for decision-making problems. Beyond using the learned environment model to train a policy, the success of MCTS-based methods shows that directly incorporating the learned environment model as a planner to make decisions might be more effective. However, when action space is of high dimension and continuous, directly planning according to the learned model is costly and non-trivial. Because of two challenges: (1) the infinite number of candidate actions and (2) the temporal dependency between actions in different timesteps. To address these challenges, inspired by Differential Dynamic Programming (DDP) in optimal control theory, we design a novel Policy Optimization with Model Planning (POMP) algorithm, which incorporates a carefully designed Deep Differential Dynamic Programming (D3P) planner into the model-based RL framework. In D3P planner, (1) to effectively plan in the continuous action space, we construct a locally quadratic programming problem that uses a gradient-based optimization process to replace search. (2) To take the temporal dependency of actions at different timesteps into account, we leverage the updated and latest actions of previous timesteps (i.e., step 1 , \u00b7 \u00b7 \u00b7 , h \u2212 1 ) to update the action of the current step (i.e., step h ), instead of updating all actions simultaneously. We theoretically prove the convergence rate for our D3P planner and analyze the effect of the feedback term. In practice, to effectively apply the neural network based",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "151068900",
                        "name": "Jinhua Zhu"
                    },
                    {
                        "authorId": "2144333734",
                        "name": "Yue Wang"
                    },
                    {
                        "authorId": "47767550",
                        "name": "Lijun Wu"
                    },
                    {
                        "authorId": "143826491",
                        "name": "Tao Qin"
                    },
                    {
                        "authorId": "38272296",
                        "name": "Wen-gang Zhou"
                    },
                    {
                        "authorId": "2110264337",
                        "name": "Tie-Yan Liu"
                    },
                    {
                        "authorId": "2144406784",
                        "name": "Houqiang Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite this, prevalent MBRL algorithms (Kurutach et al., 2018; Chua et al., 2018; Janner et al., 2019; Eysenbach et al., 2022) often struggle to balance exploration and exploitation, leading to poor performance when\n*Work done in Google Brain (now Google DeepMind).",
                "For the baseline methods, we consider a range of modelbased methods including SLBO (Luo et al., 2019), PETS (Chua et al., 2018), and MBPO (Janner et al., 2019), as well as a model-free approach, SAC (Haarnoja et al., 2018).",
                "These tasks are taken from the official Github repository of MBPO (Janner et al., 2019), https://github.com/jannerm/mbpo.",
                "To put this method into practice, we amalgamate deep ensembles (Lakshminarayanan et al., 2017) and Model-based Policy Optimization (MBPO) (Janner et al., 2019).",
                "In terms of the hyperparameters, our choice of them are mostly the same as the ones adopted in MBPO (Janner et al., 2019) and Pineda et al. (2021) for Ant, Halfcheetah, Hopper, Walker2D and Cartpole-swingup, and Eysenbach et al. (2022) for Window-open-v2, which are sufficiently optimized by the\u2026",
                "Subsequently, we present a pragmatic version of the algorithm based deep ensembles (Lakshminarayanan et al., 2017) and MBPO (Janner et al., 2019).",
                "As a mitigation strategy, Kurutach et al. (2018); Luo et al. (2019); Janner et al. (2019) propose training a policy on top of the model to amortize the planning cost.",
                "\u2026PETS and ME-TRPO\nPopular model-based reinforcement learning algorithms such as ME-TRPO (Kurutach et al., 2018), PETs (Chua et al., 2018) and MBPO (Janner et al., 2019) typically repeat the following three steps: 1) train a dynamics model (or an ensemble of models) q(M|DE); 2) train/extract a\u2026",
                "This approach aligns with ME-TRPO (Kurutach et al., 2018), PETS (Chua et al., 2018), and MBPO (Janner et al., 2019), with the additional facet of modeling the uncertainty over policies, i.e., q(\u03c0|M,DE), as well as dynamics, i.e., q(M|DE).",
                "The most widespread technique for model learning involves the use of the L2 loss for one-step transitions (Kurutach et al., 2018; Luo et al., 2019; Chua et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "0fc3b453f796fedc93f4c667e71e73838b134505",
                "externalIds": {
                    "CorpusId": 260515025
                },
                "corpusId": 260515025,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0fc3b453f796fedc93f4c667e71e73838b134505",
                "title": "Model-based Policy Optimization under Approximate Bayesian Inference",
                "abstract": "Model-based reinforcement learning algorithms (MBRL) present an exceptional potential to enhance sample efficiency within the realm of online reinforcement learning (RL). Nevertheless, a substantial proportion of prevalent MBRL algorithms fail to adequately address the dichotomy of exploration and exploitation. Posterior sampling reinforcement learning (PSRL) emerges as an innovative strategy adept at balancing exploration and exploitation, albeit its theoretical assurances are contingent upon exact inference. In this paper, we show that adopting the same methodology as in exact PSRL can be suboptimal under approximate inference. Motivated by the analysis, we propose an improved factorization for the posterior distribution of polices by removing the conditional independence between the policy and data given the model. By adopting such a posterior factorization, we further propose a general algorithmic framework for PSRL under approximate inference and a practical instantiation of it. Empirically, our algorithm can surpass baseline methods by a significant margin on both dense rewards and sparse rewards tasks from the Deepmind control suite, OpenAI Gym and Metaworld benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144093796",
                        "name": "Chaoqi Wang"
                    },
                    {
                        "authorId": "50580847",
                        "name": "Yuxin Chen"
                    },
                    {
                        "authorId": "2056418611",
                        "name": "Kevin Murphy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following MBPO, we learn an ensemble of N dynamics models {T\u0302 i\u03b8 = N (\u00b5i\u03b8,\u03a3i\u03b8)}Ni=1, each of which is a neural network that outputs a Gaussian distribution over the next state and reward and is trained independently via maximum likelihood.",
                "MBPO utilizes a standard actorcritic RL algorithm but uses an augmented dataset D \u222a Dmodel to train the policy, where Dmodel is synthetic data generated by performing h-step rollouts in M\u0302 starting from states in D.",
                "We are now ready to present our overall approach in Algorithm 1, which is built upon an off-the-shelf model-based off-policy online RL algorithm, model-based policy optimization (MBPO) (Janner et al., 2019).",
                "Similarly, model-based offline RL also needs to incorporate conservatism due to inevitable model errors (Xu et al., 2020; Janner et al., 2019; Luo et al., 2019).",
                "In line with some existing work, we choose model-based policy optimization (MBPO) (Janner et al., 2019) to learn the optimal policy for M\u0302.",
                "We train an ensemble of 7 such dynamics models following (Janner et al., 2019; Yu et al., 2020) and pick the best 5 models based on the validation prediction error on a held-out set that contains 1000 transitions in the offline dataset D.",
                "We focus on Dyna-style modelbased RL (Janner et al., 2019; Lin et al., 2022)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "25db1b77bc330476c3cf6ce43236404c578b4372",
                "externalIds": {
                    "DBLP": "conf/icml/SunZJLYY23",
                    "CorpusId": 260846642
                },
                "corpusId": 260846642,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/25db1b77bc330476c3cf6ce43236404c578b4372",
                "title": "Model-Bellman Inconsistency for Model-based Offline Reinforcement Learning",
                "abstract": "For offline reinforcement learning (RL), model-based methods are expected to be data-efficient as they incorporate dynamics models to generate more data. However, due to inevitable model errors, straightforwardly learning a policy in the model typically fails in the offline setting. Previous studies have incorporated conservatism to prevent out-of-distribution exploration. For example, MOPO penalizes rewards through uncertainty measures from predicting the next states, which we have discovered are loose bounds of the ideal uncertainty, i.e., the Bellman error. In this work, we propose MO del-B ellman I nconsistency penalized off L in E Policy Optimization (MOBILE), a novel uncertainty-driven offline RL algorithm. MOBILE conducts uncertainty quantification through the inconsistency of Bell-man estimations under an ensemble of learned dynamics models, which can be a better approx-imator to the true Bellman error, and penalizes the Bellman estimation based on this uncertainty. Empirically we have verified that our proposed uncertainty quantification can be significantly closer to the true Bellman error than the compared meth-ods. Consequently, MOBILE outperforms prior offline RL approaches on most tasks of D4RL and NeoRL benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "120738487",
                        "name": "Yihao Sun"
                    },
                    {
                        "authorId": "2000940184",
                        "name": "Jiajin Zhang"
                    },
                    {
                        "authorId": "2167317595",
                        "name": "Chengxing Jia"
                    },
                    {
                        "authorId": "2175115642",
                        "name": "Hao-Chu Lin"
                    },
                    {
                        "authorId": "2232411195",
                        "name": "Junyin Ye"
                    },
                    {
                        "authorId": "2198583888",
                        "name": "Yangze Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To this end, we propose our AMPO (Adaptation augmented Model-based Policy Optimization) framework upon the existing MBPO (Janner et al., 2019) method with two variants, dubbed as FAMPO and IAMPO, respectively.",
                "Under this taxonomy, our approaches are Dyna-style methods that are inspired by the recent MBPO (Janner et al., 2019) algorithm.",
                "We demonstrate the detailed process of IAMPO upon the MBPO (Janner et al., 2019) backbone in Algorithm 2.",
                "Therefore, it is necessary in MBRL to derive an upper bound of the discrepancy between the expected return in the real environment \u03b7T (\u03c0) and the expected return in the model \u03b7T\u0302 (\u03c0) with the same policy \u03c0 in the following form (Luo et al., 2018; Janner et al., 2019): \u03b7T\u0302 (\u03c0)\u2212 \u03b7T (\u03c0) \u2264 C.",
                "In practice, following MBPO (Janner et al., 2019), we use an ensemble of probabilistic networks to represent the model and train the model ensemble via maximum likelihood.",
                "For example, MBPO (Janner et al., 2019) used the model to generate short",
                "3 Model-based Policy Optimization We briefly summarize the model-based policy optimization (MBPO) (Janner et al., 2019) algorithm, on top of which we build our algorithm.",
                "For model-based methods, we compare to MBPO (Janner et al., 2019), PETS (Chua et al.",
                "On the other hand, for model usage, delicate rollout schemes (Janner et al., 2019; Pan et al., 2020) have been adopted to stop the model rollouts before the simulated data deviate too much from the real distribution.",
                "Thereafter, in this paper, we adopt MBPO (Janner et al., 2019) as our baseline backbone framework due to its remarkable success in practice."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "41e3b2b16ca00229b873328827cf69a707760e8a",
                "externalIds": {
                    "CorpusId": 261043114
                },
                "corpusId": 261043114,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/41e3b2b16ca00229b873328827cf69a707760e8a",
                "title": "Adaptation Augmented Model-based Policy Optimization",
                "abstract": "Compared to model-free reinforcement learning (RL), model-based RL is often more sample e\ufb03cient by leveraging a learned dynamics model to help decision making. However, the learned model is usually not perfectly accurate and the error will compound in multi-step predictions, which can lead to poor asymptotic performance. In this paper, we \ufb01rst derive an upper bound of the return discrepancy between the real dynamics and the learned model, which reveals the fundamental problem of distribution shift between simulated data and real data. Inspired by the theoretical analysis, we propose an adaptation augmented model-based policy optimization (AMPO) framework to address the distribution shift problem from the perspectives of feature learning and instance re-weighting, respectively. Speci\ufb01cally, the feature-based variant, namely FAMPO, introduces unsupervised model adaptation to minimize the integral probability metric (IPM) between feature distributions from real and simulated data, while the instance-based variant, termed as IAMPO, utilizes importance sampling to re-weight the real samples used to train the model. Besides model learning, we also investigate how to improve policy optimization in the model usage phase by selecting simulated samples with di\ufb00erent probability according to their uncertainty. Extensive experiments on challenging continuous control tasks show that FAMPO and IAMPO, coupled with our model usage technique, achieves superior performance against baselines, which demonstrates the e\ufb00ectiveness of the proposed methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "143808691",
                        "name": "Jian Shen"
                    },
                    {
                        "authorId": "2054235888",
                        "name": "Hang Lai"
                    },
                    {
                        "authorId": "2232669157",
                        "name": "\u2020. MinghuanLiu"
                    },
                    {
                        "authorId": "2232668601",
                        "name": "\u2021. HanZhao"
                    },
                    {
                        "authorId": "2232746445",
                        "name": "Yong Yu"
                    },
                    {
                        "authorId": "2232668280",
                        "name": "\u2020. WeinanZhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e8cd8e1ee73ce6059c1a08acd5cc088fee2e7a29",
                "externalIds": {
                    "CorpusId": 256615169
                },
                "corpusId": 256615169,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e8cd8e1ee73ce6059c1a08acd5cc088fee2e7a29",
                "title": "Stock Trading Optimization through Model-based Reinforcement Learning with Normalizing Flows (preprint)",
                "abstract": "With the fast development of quantitative portfolio optimization in financial engineering, lots of promising algorithmic trading strategies have shown competitive advantages in recent years. However, the environment from real financial markets is complex and hard to be fully simulated, considering non-stationarity of the stock data, unpredictable hidden causal factors and so on. Fortunately, difference of stock prices is often stationary series, and the internal relationship between difference of stocks can be linked to the decision-making process, then the portfolio should be able to achieve better performance. In this paper, we demonstrate normalizing flows is adopted to simulated high-dimensional joint probability of the complex trading environment, and develop a novel model based reinforcement learning framework to better understand the intrinsic mechanisms of quantitative online trading. Second, we experiment various stocks from three different financial markets (Dow, NASDAQ and S&P 500) and show that among these three financial markets, Dow gets the best performance results on various evaluation metrics under our back-testing system. Especially, our proposed method even resists big drop (less maximum drawdown) during COVID-19 pandemic period when the financial market got unpredictable crisis. All these results are comparatively better than modeling the state transition dynamics with independent Gaussian Processes. Third, we utilize a causal analysis method to study the causal relationship among different stocks of the environment. Further, by visualizing high dimensional state transition data comparisons from real and virtual buffer with t-SNE, we uncover some effective patterns of bet",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2151675056",
                        "name": "Huifang Huang"
                    },
                    {
                        "authorId": "2072687794",
                        "name": "Ting Gao"
                    },
                    {
                        "authorId": "2118111202",
                        "name": "Pengbo Li"
                    },
                    {
                        "authorId": "2157959267",
                        "name": "Jinqiu Guo"
                    },
                    {
                        "authorId": "40075735",
                        "name": "P. Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-free deep RL algorithms such as SAC (Haarnoja et al., 2018), TD3 (Fujimoto et al., 2018), DDPG (Lillicrap et al., 2015) and model-based deep RL algorithms such as Dreamer (Hafner et al., 2023), TD-MPC (Hansen et al., 2022), LOOP (Sikchi et al., 2022b), MuZero (Schrittwieser et al., 2020) and MBPO (Janner et al., 2019) exemplify this class of successful algorithms.",
                ", 2020) and MBPO (Janner et al., 2019) exemplify this class of successful algorithms.",
                "\u2026et al., 2018), DDPG (Lillicrap et al., 2015) and model-based deep RL algorithms such as Dreamer (Hafner et al., 2023), TD-MPC (Hansen et al., 2022), LOOP (Sikchi et al., 2022b), MuZero (Schrittwieser et al., 2020) and MBPO (Janner et al., 2019) exemplify this class of successful algorithms."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "90050ce8752719200760e33dc07bb812c38d33bb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-08560",
                    "DOI": "10.48550/arXiv.2302.08560",
                    "CorpusId": 257020073
                },
                "corpusId": 257020073,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/90050ce8752719200760e33dc07bb812c38d33bb",
                "title": "Imitation from Arbitrary Experience: A Dual Unification of Reinforcement and Imitation Learning Methods",
                "abstract": "It is well known that Reinforcement Learning (RL) can be formulated as a convex program with linear constraints. The dual form of this formulation is unconstrained, which we refer to as dual RL, and can leverage preexisting tools from convex optimization to improve the learning performance of RL agents. We show that several state-of-the-art deep RL algorithms (in online, of\ufb02ine, and imitation settings) can be viewed as dual RL approaches in a uni\ufb01ed framework. This uni\ufb01cation calls for the methods to be studied on common ground, so as to identify the components that actually contribute to the success of these methods. Our uni\ufb01cation also reveals that prior off-policy imitation learning methods in the dual space are based on an unrealistic coverage assumption and are restricted to matching a particular f -divergence. We propose a new method using a simple modi\ufb01cation to the dual framework that allows for imitation learning with arbitrary off-policy data to obtain near-expert performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51521430",
                        "name": "Harshit S. Sikchi"
                    },
                    {
                        "authorId": "2111672235",
                        "name": "Amy Zhang"
                    },
                    {
                        "authorId": "2791038",
                        "name": "S. Niekum"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2021) or can be used to generate additional, fictitious data for the agent to train on (Kurutach et al., 2018; Janner et al., 2019).",
                "These models can either be used for better estimates of the value function (Feinberg et al., 2018; Amos et al., 2021) or can be used to generate additional, fictitious data for the agent to train on (Kurutach et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3b24a42bff5671ed82208488c0f96d0078124648",
                "externalIds": {
                    "DBLP": "conf/l4dc/CharABBCCEMRKS23",
                    "CorpusId": 259178304
                },
                "corpusId": 259178304,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3b24a42bff5671ed82208488c0f96d0078124648",
                "title": "Offline Model-Based Reinforcement Learning for Tokamak Control",
                "abstract": "Control for tokamaks, the leading candidate technology for nuclear fusion, is an important pursuit since the realization of nuclear fusion as an energy source would result in virtually unlimited clean energy. However, control of these devices remains a challenging problem due to complex, non-linear dynamics. At the same time, there is promise in learning controllers for difficult problems thanks to recent algorithmic developments in reinforcement learning. Because every run (or shot) of the tokamak is extremely expensive, in this work, we investigated learning a controller from logged data before testing it on a tokamak. In particular, we used 18 years of data from the DIII-D device in order to learn a controller for the neutral beams that targets specified \u03b2N (normalized ratio of plasma pressure to magnetic pressure) and rotation quantities. This was done by using the data to first learn a dynamics model, and then by using this model as a simulator to generate experience to train a controller via reinforcement learning. During a control session on DIII-D, we tested both the ability for our dynamics model to design feedforward trajectories and the controller\u2019s ability to do feedback control to achieve specified targets. This work marks some of the first steps in doing reinforcement learning for tokamak control through historical data alone.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2306889",
                        "name": "I. Char"
                    },
                    {
                        "authorId": "94396899",
                        "name": "J. Abbate"
                    },
                    {
                        "authorId": "15557300",
                        "name": "L. Bardoczi"
                    },
                    {
                        "authorId": "39062485",
                        "name": "M. Boyer"
                    },
                    {
                        "authorId": "1387920173",
                        "name": "Youngseog Chung"
                    },
                    {
                        "authorId": "1581200399",
                        "name": "R. Conlin"
                    },
                    {
                        "authorId": "32524546",
                        "name": "K. Erickson"
                    },
                    {
                        "authorId": "49061544",
                        "name": "Viraj Mehta"
                    },
                    {
                        "authorId": "2164469485",
                        "name": "Nathan J. Richner"
                    },
                    {
                        "authorId": "4372060",
                        "name": "E. Kolemen"
                    },
                    {
                        "authorId": "2113527506",
                        "name": "J. G. Schneider"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In fact, we also tried two different MBRL methods, namely SVG(H)-SAC [Amos et al. (2020)] and MBPO [Janner et al. (2019)] and found that their performance were not on par with our modified Dreamer."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a7dd9a93164779a3e69bac767a902f1998ebd7b6",
                "externalIds": {
                    "DBLP": "conf/l4dc/NimaraMOWH23",
                    "CorpusId": 259178480
                },
                "corpusId": 259178480,
                "publicationVenue": {
                    "id": "20e3e0c7-fed4-4b57-a89c-0c9956afb80b",
                    "name": "Conference on Learning for Dynamics & Control",
                    "type": "conference",
                    "alternate_names": [
                        "L4DC",
                        "Conf Learn Dyn  Control"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a7dd9a93164779a3e69bac767a902f1998ebd7b6",
                "title": "Model-Based Reinforcement Learning for Cavity Filter Tuning",
                "abstract": "The ongoing development of telecommunication systems like 5G has led to an increase in demand of well calibrated base transceiver station (BTS) components. A pivotal component of every BTS is cavity filters, which provide a sharp frequency characteristic to select a particular band of interest and reject the rest. Unfortunately, their characteristics in combination with manufacturing tolerances make them difficult for mass production and often lead to costly manual post-production fine tuning. To address this, numerous approaches have been proposed to automate the tuning process. One particularly promising one, that has emerged in the past few years, is to use model free reinforcement learning (MFRL); however, the agents are not sample efficient. This poses a serious bottleneck, as utilising complex simulators or training with real filters is prohibitively time demanding. This work advocates for the usage of model based reinforcement learning (MBRL) and showcases how its utilisation can significantly decrease sample complexity, while maintaining similar levels of success rate. More specifically, we propose an improvement over a state-of-theart (SoTA) MBRL algorithm, namely the Dreamer algorithm. This improvement can serve as a template for applications in other similar, high-dimensional non-image data problems. We carry experiments on two complex filter types, and show that our novel modification on the Dreamer architecture reduces sample complexity by a factor of 4 and 10, respectively. Our findings pioneer the usage of MBRL which paves the way for utilising more precise and accurate simulators which was previously prohibitively time demanding.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220159393",
                        "name": "Doumitrou Daniil Nimara"
                    },
                    {
                        "authorId": "1402778128",
                        "name": "Mohammadreza Malek-Mohammadi"
                    },
                    {
                        "authorId": "66326390",
                        "name": "Petter \u00d6gren"
                    },
                    {
                        "authorId": "3217488",
                        "name": "Jieqiang Wei"
                    },
                    {
                        "authorId": "2220163493",
                        "name": "Vincent Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These approaches consider an MBPO-style approach (Janner et al., 2019) in the latent space of a variational model.",
                "A line of prior works (Yu et al., 2020; 2021c; Cang et al., 2021) use MBPO-style optimization (Janner et al., 2019), which mixes real and model-generated data in a replay buffer used for policy training.",
                ", 2021) use MBPO-style optimization (Janner et al., 2019), which mixes real and model-generated data in a replay buffer used for policy training."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c79d69253e42ad15c684faa3d61097b35c2dd8ed",
                "externalIds": {
                    "CorpusId": 259311329
                },
                "corpusId": 259311329,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c79d69253e42ad15c684faa3d61097b35c2dd8ed",
                "title": "MOTO: Offline to Online Fine-tuning for Model-Based Reinforcement Learning",
                "abstract": "We study the problem of offline-to-online reinforcement learning from high-dimensional pixel observations. While recent model-free approaches successfully use offline pre-training with online fine-tuning to either improve the performance of the data-collection policy or adapt to novel tasks, model-based approaches still remain underutilized in this setting. In this work, we argue that existing methods for high-dimensional model-based offline RL are not suitable for offlineto-online fine-tuning due to issues with representation learning shifts, off-dynamics data, and non-stationary rewards. We propose a simple onpolicy model-based method with adaptive behavior regularization. In our simulation experiments, we find that our approach successfully solves longhorizon robot manipulation tasks completely from images by using a combination of offline data and online interactions.",
                "year": 2023,
                "authors": []
            }
        },
        {
            "contexts": [
                "Although learning P has many advantages since this model is very general and can be used for other problems with the same dynamics, it is a difficult problem since inaccuracies can propagate during the search making long sampled trajectories unusable [42]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f1aa241b7d85d80c556a9b2892f82a330feef84e",
                "externalIds": {
                    "CorpusId": 259837728
                },
                "corpusId": 259837728,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f1aa241b7d85d80c556a9b2892f82a330feef84e",
                "title": "The Principle of Value Equivalence for Policy Gradient Search",
                "abstract": "Planning algorithms have shown impressive performance in many domains such as chess and Go. In particular, Monte Carlo Tree Search (MCTS) is used, which builds a search tree toward more promising nodes, while maintaining node statistics to estimate the value of each node. However, these statistics are inaccurate for a small number of visits and the memory requirements for the tree can become unsustainable for large action spaces. A solution to this problem is presented by Policy Gradient Search (PGS). This search method is not based on node statistics but rather learns a simulation policy during the search that guides it toward higher-value nodes. No search tree is needed, which reduces memory requirements. Still, this method suffers from several problems, such as high variance in its value estimates, and is limited to tasks where a perfect simulator of the environment is available. These issues prevent the use of the method in real-world tasks e.g. in logistics or medicine. In this thesis, we address these problems by proposing a new algorithm: Value Equivalence for Policy Gradient Search (VE-PGS), which uses a modified search of PGS on a value equivalent (VE) model. VE models are learned models that only focus on those parts of the environment that are relevant to the search. Furthermore, we propose several changes to PGS to address its issues, which are then incorporated into VE-PGS. We test our new method in the two-player board game Hex, while we verify the effectiveness of our changes in an ablation study. Our results show that our method is not only able to plan effectively on a learned VE model but that it is also competitive with state-of-the-art approaches such as MuZero and requires much less memory. These properties make the method suitable for complex tasks with unknown dynamics, where the use of a search tree is infeasible due to high branching factors or memory requirements.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143214066",
                        "name": "J. Brosseit"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(which has been increasingly studied in the context of deep RL [11]) is to use a model-based approach to accelerate learning."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d775b8c4e245ee9b11de22704e9c3bc1eb9be809",
                "externalIds": {
                    "DBLP": "conf/atal/Alegre23",
                    "DOI": "10.5555/3545946.3599141",
                    "CorpusId": 258845393
                },
                "corpusId": 258845393,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d775b8c4e245ee9b11de22704e9c3bc1eb9be809",
                "title": "Towards Sample-Efficient Multi-Objective Reinforcement Learning",
                "abstract": "In sequential decision-making problems, the objective that a reinforcement learning agent seeks to optimize is often modeled via a reward function. However, in real-world problems, agents often have to optimize multiple (possibly conflicting) objectives. This setting is known as multi-objective reinforcement learning (MORL). In MORL, the goal of the agent is not to learn a single policy, but a set of policies, each of which specialized in optimizing a single objective or a combination of objectives. In my Ph.D., I investigate methods that allow the agent to learn a carefully-constructed set of policies that can be combined to solve challenging MORL problems in a sample-efficient manner. In this paper, I present a brief overview of my work on this topic and focus on two main contributions: (i) a novel algorithm for optimal policy transfer based on theoretical equivalences between successor features and MORL; and (ii) a novel MORL algorithm based on generalized policy improvement that learns a set of policies that is guaranteed to contain an optimal policy for any possible agent\u2019s preferences over objectives.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1411722140",
                        "name": "L. N. Alegre"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To better train fM , we use D similar to [12, 22] to collect data as the training set during the interaction of agents with the environment.",
                "To train an accurate Message Estimator, we seek inspiration from model-based reinforcement learning methods [12, 22]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "87059a160f1ca142106ca490dd5b232d48a7cecf",
                "externalIds": {
                    "DBLP": "conf/atal/HanDW23",
                    "DOI": "10.5555/3545946.3598669",
                    "CorpusId": 258845335
                },
                "corpusId": 258845335,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/87059a160f1ca142106ca490dd5b232d48a7cecf",
                "title": "Model-based Sparse Communication in Multi-agent Reinforcement Learning",
                "abstract": "Learning to communicate efficiently is central to multi-agent reinforcement learning (MARL). Existing methods often require agents to exchange messages intensively, which abuses communication channels and leads to high communication overhead. Only a few methods target on learning sparse communication, but they allow limited information to be shared, which affects the efficiency of policy learning. In this work, we propose model-based communication (MBC), a learning framework with a decentralized communication scheduling process. The MBC framework enables multiple agents to make decisions with sparse communication. In particular, the MBC framework introduces a model-based message estimator to estimate the up-to-date global messages using past local data. A decentralized message scheduling mechanism is also proposed to determine whether a message shall be sent based on the estimation. We eval-uated our method in a variety of mixed cooperative-competitive environments. The experiment results show that the MBC method shows better performance and lower channel overhead than the state-of-art baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218232881",
                        "name": "Shuai Han"
                    },
                    {
                        "authorId": "1707738",
                        "name": "M. Dastani"
                    },
                    {
                        "authorId": "2109511021",
                        "name": "Shihan Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026Kumar et al., 2019b; Zhang et al., 2021; Kostrikov et al., 2021a); ii) enforcing conservative estimates of future rewards (Kumar et al., 2020; Yu et al., 2021; Cheng et al., 2022); and iii) model-based methods that estimate the uncertainty via ensembles (Janner et al., 2019; Kidambi et al., 2020).",
                ", 2022); and iii) model-based methods that estimate the uncertainty via ensembles (Janner et al., 2019; Kidambi et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "cc40dee43196740ced9cfe91a87ba50eded12c4e",
                "externalIds": {
                    "CorpusId": 259926635
                },
                "corpusId": 259926635,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cc40dee43196740ced9cfe91a87ba50eded12c4e",
                "title": "BENCHMARKING OFFLINE REINFORCEMENT LEARN-",
                "abstract": "Learning policies from previously recorded data is a promising direction for realworld robotics tasks, as online learning is often infeasible. Dexterous manipulation in particular remains an open problem in its general form. The combination of offline reinforcement learning with large diverse datasets, however, has the potential to lead to a breakthrough in this challenging domain analogously to the rapid progress made in supervised learning in recent years. To coordinate the efforts of the research community toward tackling this problem, we propose a benchmark including: i) a large collection of data for offline learning from a dexterous manipulation platform on two tasks, obtained with capable RL agents trained in simulation; ii) the option to execute learned policies on a real-world robotic system and a simulation for efficient debugging. We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets and provide a reproducible experimental setup for offline reinforcement learning on real systems. Visit https://sites.google.com/view/ benchmarking-offline-rl-real for more details.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46773173",
                        "name": "Sebastian Blaes"
                    },
                    {
                        "authorId": "2662064",
                        "name": "Pavel Kolev"
                    },
                    {
                        "authorId": "47804478",
                        "name": "F. Widmaier"
                    },
                    {
                        "authorId": "36661824",
                        "name": "Manuel W\u00fcthrich"
                    },
                    {
                        "authorId": "153125952",
                        "name": "Stefan Bauer"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    },
                    {
                        "authorId": "144247521",
                        "name": "G. Martius"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One key challenge to RL is the distribution shift due to the difference between the learned policy and the behavior policy (Lagoudakis & Parr, 2003; Lange et al., 2012; Schulman et al., 2015; Sun et al., 2018; Janner et al., 2019).",
                "Related Work One key challenge to RL is the distribution shift due to the difference between the learned policy and the behavior policy (Lagoudakis & Parr, 2003; Lange et al., 2012; Schulman et al., 2015; Sun et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b36a202e106f45b5afe59deb3cf61898fcd3b0f9",
                "externalIds": {
                    "DBLP": "conf/icml/SuiHZ023",
                    "CorpusId": 260827965
                },
                "corpusId": 260827965,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b36a202e106f45b5afe59deb3cf61898fcd3b0f9",
                "title": "Adversarial Learning of Distributional Reinforcement Learning",
                "abstract": "Reinforcement learning (RL) has made significant advancements in artificial intelligence. However, its real-world applications are limited due to differences between simulated environments and the actual world. Consequently, it is crucial to systematically analyze how each component of the RL system can affect the final model performance. In this study, we propose an adversarial learning framework for distributional reinforcement learning, which adopts the concept of influence measure from the statistics community. This framework enables us to detect performance loss caused by either the internal policy structure or the ex-ternal state observation. The proposed influence measure is based on information geometry and has desirable properties of invariance. We demonstrate that the influence measure is useful for three diagnostic tasks: identifying fragile states in trajectories, determining the instability of the policy architecture, and pinpointing anomalously sensitive policy parameters.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2229444231",
                        "name": "Yang Sui"
                    },
                    {
                        "authorId": "2145952181",
                        "name": "Yukun Huang"
                    },
                    {
                        "authorId": "1505822711",
                        "name": "Hongtu Zhu"
                    },
                    {
                        "authorId": "144315726",
                        "name": "Fan Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our work is also related to the model-based RL literature, where a predictive model is learned from data for policy optimization (Janner et al., 2019; Feinberg et al., 2018; Zhang, 2022; Amos et al., 2021) or planning (Wang & Ba, 2019; Schrittwieser et al.",
                "For MBPO (Janner et al., 2019), we use neural network (NN) models that are trained by minimizing the mean squared error."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "25e197a2ce7bf803a8bca007ac42f4c5c1862a2b",
                "externalIds": {
                    "DBLP": "conf/icml/ZhangJW23",
                    "CorpusId": 260957144
                },
                "corpusId": 260957144,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/25e197a2ce7bf803a8bca007ac42f4c5c1862a2b",
                "title": "Adaptive Barrier Smoothing for First-Order Policy Gradient with Contact Dynamics",
                "abstract": "Differentiable physics-based simulators have witnessed remarkable success in robot learning involving contact dynamics, benefiting from their improved accuracy and efficiency in solving the underlying complementarity problem. However, when utilizing the First-Order Policy Gradient (FOPG) method, our theory indicates that the complementarity-based systems suffer from stiffness, leading to an explosion in the gradient variance of FOPG. As a result, optimization becomes challenging due to chaotic and non-smooth loss landscapes. To tackle this issue, we propose a novel approach called Adaptive Barrier Smooth-ing (ABS), which introduces a class of soft-ened complementarity systems that correspond to barrier-smoothed objectives. With a contact-aware adaptive central-path parameter, ABS reduces the FOPG gradient variance while controlling the gradient bias. We justify the adaptive design by analyzing the roots of the system\u2019s stiffness. Additionally, we establish the convergence of FOPG and show that ABS achieves a reasonable trade-off between the gradient variance and bias by providing their upper bounds. Moreover, we present a variant of FOPG based on complementarity modeling that efficiently fits the contact dynamics by learning the physical parameters. Experimental results on various robotic tasks are provided to support our theory and method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145522248",
                        "name": "Shenao Zhang"
                    },
                    {
                        "authorId": "9433338",
                        "name": "Wanxin Jin"
                    },
                    {
                        "authorId": "50218397",
                        "name": "Zhaoran Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Future work could investigate improving sample efficiency through offline datasets [36], model-based reinforcement learning [37, 38], or better representation learning methods [39, 40]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d02a2d206c281dbdbb41b8ccee8312c39762ec5a",
                "externalIds": {
                    "CorpusId": 261053722
                },
                "corpusId": 261053722,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d02a2d206c281dbdbb41b8ccee8312c39762ec5a",
                "title": "Your Value Function is a Control Barrier Function",
                "abstract": "Guaranteeing safe behaviour of reinforcement learning (RL) policies poses significant challenges for safety-critical applications, despite RL\u2019s generality and scalability. To address this, we propose a new approach to apply verification methods from control theory to learned value functions. By analyzing task structures for safety preservation, we formalize original theorems that establish links between value functions and con-trol barrier functions. Further, we propose novel metrics for verifying value functions in safe con-trol tasks and practical implementation details to improve learning. Our work presents a novel method for certificate learning, which unlocks a diversity of verification techniques from control theory for RL policies, and marks a significant step towards a formal framework for the general, scalable, and verifiable design of RL-based con-trol systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219272080",
                        "name": "Daniel C.H. Tan"
                    },
                    {
                        "authorId": "2126051157",
                        "name": "Fernando Acero"
                    },
                    {
                        "authorId": "144722083",
                        "name": "Robert McCarthy"
                    },
                    {
                        "authorId": "1704541",
                        "name": "D. Kanoulas"
                    },
                    {
                        "authorId": "2216390361",
                        "name": "Zhibin Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based RL and RL with Representation Learning Model-based RL and RL with representation learning are two active research areas but without a clear general state-of-the-art (Bharadhwaj et al., 2022; Chebotar et al., 2017a; Deisenroth & Rasmussen, 2011; Eysenbach et al., 2021; Hafner et al., 2019; Janner et al., 2019; Kim et al., 2019; Laskin et al., 2021; Lee et al., 2020; Nagabandi et al., 2018; Trabucco et al., 2022; Watter et al., 2015).",
                ", 2015) and leverage discrete-time models (Deisenroth & Rasmussen, 2011; Gu et al., 2016; Janner et al., 2019; Mhammedi et al., 2020; Nagabandi et al., 2018)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "407c64249dc831b5f153f2b4703f793317a4ee87",
                "externalIds": {
                    "CorpusId": 261176534
                },
                "corpusId": 261176534,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/407c64249dc831b5f153f2b4703f793317a4ee87",
                "title": "Factor Learning Portfolio Optimization Informed by Continuous-Time Finance Models",
                "abstract": "We study financial portfolio optimization in the presence of unknown and uncontrolled system variables referred to as stochastic factors. Existing work falls into two distinct categories: (i) reinforcement learning employs end-to-end policy learning with flexible factor representation, but does not precisely model the dynamics of asset prices or factors; (ii) continuous-time finance methods, in contrast, take advantage of explicitly modeled dynamics but pre-specify, rather than learn, factor representation. We propose FaLPO (factor learning portfolio optimization), a framework that interpolates between these two approaches. Specifically, FaLPO hinges on deep policy gradient to learn a performant investment policy that takes advantage of flexible representation for stochastic factors. Meanwhile, FaLPO also incorporates continuous-time finance models when modeling the dynamics. It uses the optimal policy functional form derived from such models and optimizes an objective that combines policy learning and model calibration. We prove the convergence of FaLPO and provide performance guarantees via a finite-sample bound. On both synthetic and real-world portfolio optimization tasks, we observe that FaLPO outperforms five leading methods. Finally, we show that FaLPO can be extended to other decision-making problems with stochastic factors.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "29824845",
                        "name": "Sinong Geng"
                    },
                    {
                        "authorId": "2005660496",
                        "name": "Houssam Nassif"
                    },
                    {
                        "authorId": "3431263",
                        "name": "Zhaobin Kuang"
                    },
                    {
                        "authorId": "2163259872",
                        "name": "A. Reppen"
                    },
                    {
                        "authorId": "7402018",
                        "name": "R. Sircar"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "be1245fc8c5275e51a62147fe49e31b072f588e3",
                "externalIds": {
                    "DBLP": "journals/access/LiuLJZGS23",
                    "DOI": "10.1109/ACCESS.2023.3307573",
                    "CorpusId": 261139247
                },
                "corpusId": 261139247,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/be1245fc8c5275e51a62147fe49e31b072f588e3",
                "title": "Bidirectional Model-Based Policy Optimization Based on Adaptive Gaussian Noise and Improved Confidence Weights",
                "abstract": "Model-Based Reinforcement Learning (MBRL) has been gradually applied in the field of Robot Learning due to its excellent sample efficiency and asymptotic performance. However, for high-dimensional learning tasks in complex scenes, the exploration and stable training capabilities of the robot still need enhancement. In light of policy planning and policy optimization, we propose a bidirectional model-based policy optimization algorithm based on adaptive gaussian noise and improved confidence weights (BMPO-NW). The algorithm parameterizes bidirectional policy networks into noise networks by adding different adaptive Gaussian noises to the connection weights and biases. This can improve the randomness of policy search and induce efficient exploration for the robot. Simultaneously, the confidence weight of improved activation function is introduced into the Q-function update formula of SAC, which can reduce the error propagation problem of target Q-network, and enhance the robot\u2019s training stability. Finally, we implement the improved algorithm based on the framework of bidirectional model-based policy optimization algorithm (BMPO) to ensure asymptotic performance and sample efficiency. Experimental results in MuJoCo benchmark environments demonstrate that the learning speed of BMPO-NW is about 20% higher than baseline methods, the average reward is about 15% higher than other MBRL methods, and 50%-70% higher than MFRL methods, while the training process is more stable. Ablation experiments and different variant design experiments further verify the feasibility and robustness. The research results provide theoretical support for the conclusion of this paper and hold significant practical value for MBRL to help the robot realize applications in complex scenarios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46641573",
                        "name": "W. Liu"
                    },
                    {
                        "authorId": "2221337484",
                        "name": "Mengyuan Liu"
                    },
                    {
                        "authorId": "2057049757",
                        "name": "Baoying Jin"
                    },
                    {
                        "authorId": "2130386408",
                        "name": "Yixin Zhu"
                    },
                    {
                        "authorId": "2221279509",
                        "name": "Qi Gao"
                    },
                    {
                        "authorId": "2221324046",
                        "name": "Jiayang Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such a setting is designed to implement the DA-level policy which is commonly optimized by using RL algorithms [29, 49, 68, 86].",
                "Model-based RL [17, 49] has been developed and employed in policy optimization in dialogue act level where the dialogue policy is optimized to conduct a planning step."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "58b61f437ea8b4a9c56d6b7a6a47d7de9b053b0f",
                "externalIds": {
                    "DOI": "10.1561/116.00000132",
                    "CorpusId": 261564803
                },
                "corpusId": 261564803,
                "publicationVenue": {
                    "id": "d081062e-f8a7-4018-9072-ca99195f38d8",
                    "name": "APSIPA Transactions on Signal and Information Processing",
                    "alternate_names": [
                        "APSIPA Trans Signal Inf Process"
                    ],
                    "issn": "2048-7703",
                    "url": "https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information-processing",
                    "alternate_urls": [
                        "http://journals.cambridge.org/action/displayJournal?jid=SIP"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/58b61f437ea8b4a9c56d6b7a6a47d7de9b053b0f",
                "title": "Advances and Challenges in Multi-Domain Task-Oriented Dialogue Policy Optimization",
                "abstract": "Developing a successful dialogue policy for a multi-domain task-oriented dialogue (MDTD) system is a challenging task. Basically, a desirable dialogue policy acts as the decision-making agent who understands the user\u2019s intention to provide suitable responses within a short conversation. Furthermore, offering the precise answers to satisfy the user requirements makes the task even more challenging. This paper surveys recent advances in multi-domain task-oriented dialogue policy optimization and summarizes a number of solutions to policy learning. In particular, the case study on the task of travel assistance using the MDTD dataset based on MultiWOZ containing seven different domains is investigated. The dialogue policy optimization methods, categorized into dialogue act level and word level, are systematically presented. Moreover, this paper addresses a number of challenges and difficulties including the user simulator design and the dialogue policy evaluation which need to be resolved to further enhance the robustness and effectiveness in multi-domain dialogue policy representation. \u2217Corresponding author: Jen-Tzung Chien, jtchien@nycu.edu.tw. Received 22 May 2023; Revised 20 July 2023 ISSN 2048-7703; DOI 10.1561/116.00000132 \u00a9 2023 M. Rohmatillah and J.-T. Chien 2 Rohmatillah and Chien",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "81512331",
                        "name": "Mahdin Rohmatillah"
                    },
                    {
                        "authorId": "2238060080",
                        "name": "Jen-Tzung Chien"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7090d35c316afe869440ede6ad61fdeec85b8bc8",
                "externalIds": {
                    "CorpusId": 261685884
                },
                "corpusId": 261685884,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7090d35c316afe869440ede6ad61fdeec85b8bc8",
                "title": "Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models",
                "abstract": ": Long-horizon tasks, usually characterized by complex subtask depen-1 dencies, present a significant challenge in manipulation planning. Skill chaining 2 is a practical approach to solving unseen tasks by combining learned skill pri-3 ors. However, such methods are myopic if sequenced greedily and face scalability 4 issues with search-based planning strategy. To address these challenges, we in-5 troduce Generative Skill Chaining (GSC), a probabilistic framework that learns 6 skill-centric diffusion models and composes their learned distributions to generate 7 long-horizon plans during inference. GSC samples from all skill models in par-8 allel to efficiently solve unseen tasks while enforcing geometric constraints. We 9 evaluate the method on various long-horizon tasks and demonstrate its capability 10 in reasoning about action dependencies, constraint handling, and generalization, 11 along with its ability to replan in the face of perturbations. We show results in 12 simulation and on real robot to validate the efficiency and scalability of GSC, 13 highlighting its potential for advancing long-horizon task planning. More details 14 are available at: sites.google.com/view/generative-skill-chaining . 15",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1927638269",
                        "name": "Utkarsh Aashu Mishra"
                    },
                    {
                        "authorId": "104015219",
                        "name": "Shangjie Xue"
                    },
                    {
                        "authorId": "2243868887",
                        "name": "Yongxin Chen"
                    },
                    {
                        "authorId": "2243486191",
                        "name": "Danfei Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a6e6b5dfa8292be96a1bb8f8f602748eb04711b9",
                "externalIds": {
                    "DBLP": "conf/iclr/LiZZSDY23",
                    "CorpusId": 259951729
                },
                "corpusId": 259951729,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a6e6b5dfa8292be96a1bb8f8f602748eb04711b9",
                "title": "Mitigating Metastable Failures in Distributed Systems with Offline Reinforcement Learning",
                "abstract": "This paper introduces a load shedding mechanism that mitigates metastable failures through offline reinforcement learning (RL). Previous studies have heavily focused on heuristics that are reactive and limited in generalization, while online RL algorithms face challenges in accurately simulating system dynamics and acquiring data with sufficient coverage. In contrast, our algorithm leverages offline RL to learn directly from existing log data. Through extensive empirical experiments, we demonstrate that our algorithm outperforms rule-based methods and supervised learning algorithms in a proactive, adaptive, generalizable, and safe manner. Deployed in a Java compute service with diverse execution times and configurations, our algorithm exhibits faster reaction time and attains the Pareto frontier between throughput and tail latency.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2181747709",
                        "name": "Yueying Li"
                    },
                    {
                        "authorId": "1759658",
                        "name": "D. Zha"
                    },
                    {
                        "authorId": "1993655237",
                        "name": "Tianjun Zhang"
                    },
                    {
                        "authorId": "145691878",
                        "name": "G. Suh"
                    },
                    {
                        "authorId": "3234334",
                        "name": "Christina Delimitrou"
                    },
                    {
                        "authorId": "51129027",
                        "name": "Francis Y. Yan"
                    },
                    {
                        "authorId": "68973648",
                        "name": "Microsoft Research"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The code for building Ant and Humanoid environment is provided by Janner et al. (2019).1\nB.2.2 HYPERPARAMETER SETTINGS\n0 25k 50k 75k 100k Steps\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAv er\nag e\nRe tu\nrn\nHopper\nours(H=1) ours(H=2) ours(H",
                "Use the learned model to argument data: Janner et al. (2019) use the learned model to rollout the trajectories and use both the generated and the true trajectories to train the SAC algorithm.",
                "The learned model can be viewed as a black-box simulator and then used for training a model-free policy (Nagabandi et al., 2018; Luo et al., 2018; Kurutach et al., 2018; Wang et al., 2019; Janner et al., 2019; Pan et al., 2020).",
                "For MBPO, we directly use the reported number given by Janner et al. (2019)2; For SAC we use the codes and hyperparameters available3; For MAGE, we use the codes available4 and hyperparameters from the author.",
                "Recent work on model-based RL (Nagabandi et al., 2018; Luo et al., 2018; Kurutach et al., 2018; Wang et al., 2019; Janner et al., 2019; Pan et al., 2020), has shown the power of first learning the environment model and then using it to do the policy optimization.",
                "The third step is to perform k step model rollout on the prediction model M\u0303p\u03b8p with the current policy, where k is increased over time which proposed by Janner et al. (2019) to achieve better performance.",
                "Training of the prediction model is commonly through supervised learning, e.g., maximum likelihood with early stopping on a validation set (Janner et al., 2019; Clavera et al., 2019).",
                "The prediction model M\u0303p is trained following Janner et al. (2019) via maximum likelihood (Equation 2) To improve the ability of models to portray complex environment, we use a bootstrap ensemble of models {M\u0303\u03b81 , . . . , M\u0303\u03b8B} which is consistent with Janner et al. (2019); Clavera et al. (2019).",
                "Note that the Ant and Humanoid environments are truncated observations which is consistent with MBPO (Janner et al., 2019).",
                "For model-based methods, we select MBPO (Janner et al., 2019) and STEVE (Buckman et al., 2019) as our baseline which both use short-horizon model-based rollouts.",
                ", maximum likelihood with early stopping on a validation set (Janner et al., 2019; Clavera et al., 2019).",
                "To address this problem, a popular approach is to use interpolation between different horizon predictions (Buckman et al., 2019; Janner et al., 2019) and interpolating between model and real data (Kalweit & Boedecker, 2017).",
                "Recent work on model-based RL (Nagabandi et al., 2018; Luo et al., 2018; Kurutach et al., 2018; Wang et al., 2019; Janner et al., 2019; Pan et al., 2020), has shown the power of first learning the environment model and then use it to do the policy optimization.",
                "\u2026as a general-purpose tool for learning complex policies (Mnih et al., 2015; Lillicrap et al., 2016; Haarnoja et al., 2018), has the problem of low-efficiency (Janner et al., 2019), which limits the application in real-world physical systems where data collection can be an arduous process.",
                "For model-based methods, we select MBPO (Janner et al., 2019) and STEVE (Buckman et al."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "64759b31b83b12ee52468ea16a4bafc5572bb8e1",
                "externalIds": {
                    "CorpusId": 251735782
                },
                "corpusId": 251735782,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/64759b31b83b12ee52468ea16a4bafc5572bb8e1",
                "title": "G RADIENT I NFORMATION M ATTERS IN P OLICY O PTI MIZATION BY B ACK - PROPAGATING THROUGH M ODEL",
                "abstract": "a the model to its differentiability. the gradient of the learned environment model when calculating the policy gradient. However, since the error of gradient is in the model learning phase, there is no for the model\u2019s accuracy. To this problem, we first analyze the convergence rate for the policy optimization methods when the policy gradient is calculated using the learned environment model. The theoretical results show that the model gradient error matters in the policy optimization phrase. Then we propose a two-model-based learning method to control the prediction error and the gradient error. We separate the different roles of these two models at the model learning phase and coordinate them at the policy optimization phase. After proposing the method, we introduce the directional derivative policy optimization (DDPPO) as a implementation to find the optimal policy. we the on",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182417668",
                        "name": "Chongchong Li"
                    },
                    {
                        "authorId": null,
                        "name": "Yue Wang"
                    },
                    {
                        "authorId": "2154939268",
                        "name": "Wei Chen"
                    },
                    {
                        "authorId": "2145527647",
                        "name": "Yuting Liu"
                    },
                    {
                        "authorId": "11958025",
                        "name": "Zhirui Ma"
                    },
                    {
                        "authorId": "2110264835",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The policy being trained learns to take advantage of model errors when optimising the reward, leading to poor performance in the true environment (Cang et al., 2021; Clavera et al., 2018; Janner et al., 2019; Levine et al., 2020; Rajeswaran et al., 2016).",
                "MBPO and MOPO use short rollouts from ID starting locations in an explicit attempt to limit distributional shift, therefore improved in-distribution performance may be beneficial for finding locally optimal policies.",
                "Appendix B\nAdditional Experiments\nB.1 Individual Demonstrator MBPO Policies\nTable B.1 shows the results for training policies on individual demonstrators using rollout length h = 5.0 and MOPO penalty coefficient \u03bb = 0.",
                "B.1 Individual Demonstrator MBPO Policies . . . . . . . . . . . . . . . . . . . 72",
                "This may inhibit learning entirely, or may be exhibited as model exploitation, whereby the learned policy obtains higher reward using the dynamics model than it does when deployed to the real environment (Cang et al., 2021; Clavera et al., 2018; Janner et al., 2019).",
                "Yu et al. (2020) subsequently adapted MBPO into the offline method MOPO.",
                "When the MOPO penalty coefficient is removed, we are left with the MBPO algorithm (Janner et al., 2019).",
                "When compared to a range of SOTA algorithms, MOPO outperforms model-free methods and MBPO on all but the medium dataset.",
                "In an attempt to limit distributional shift, previous works (Janner et al., 2019; Yu et al., 2020) sample rollout starting locations from the same dataset used to train the dynamics model, and use horizons of at most five steps.",
                "Methods are grouped based on how they utilise the collected rollout data: learning a dynamics model, learning a trajectory distribution, or using the data directly in model-free approaches, which learn direct mappings from states to actions (Janner et al., 2019).",
                "The dynamics model training process from PETS is further used in the online method MBPO (Janner et al., 2019) to train policies."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9b8444f3fba46f861740808fc20bf90ae791b478",
                "externalIds": {
                    "CorpusId": 259327160
                },
                "corpusId": 259327160,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9b8444f3fba46f861740808fc20bf90ae791b478",
                "title": "Domain Generalisation for Robust Model-Based Offline Reinforcement Learning",
                "abstract": "In many industries it is too dangerous or expensive to learn via online interaction with the real-world. The goal of offline reinforcement learning (RL) is therefore to learn a policy solely from collected data. Using this data, offline model-based RL (MBRL) methods employ supervised learning to train a model of the world (a dynamics model), against which a learning agent can safely interact. In our work, we define domain generalisation with reference to the logging/behavioural policies employed during data collection, such as when different human demonstrators have performed a given task, each following their own decision process. Diverse demonstrators give rise to varied data distributions, and so domain-generalisation techniques can, in theory, be used to learn dynamics models that are robust to distributional shifts encountered during policy training. To this end, we apply Risk Extrapolation (REx) (Krueger et al., 2020) to the process of training dynamics models. As a proxy for data collected from real-world demonstrators, we train policies using online RL methods and use these to generate datasets. We demonstrate that models trained with REx exhibit improved domain generalisation performance, and achieve greater equality of risks across out-of-distribution domains. Further, we find that these models enable superior policies to be learned in the offline MBRL setting, and increase the stability of the policy learning process.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192607763",
                        "name": "Alan Clark"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Janner et al. (2019) use fully connected neural networks with four hidden layers and 200 neurons per layer.",
                "\u2026models and losses for future work but highlight that significant performance might be gained by finding better tradeoffs than those discussed in Janner et al. (2019), especially when comparing deterministic and probabilistic models (compare Appendix F) as well as value-aware and value-agnostic\u2026",
                "On the Ant-v2, we see small performance improvements above the results reported by Janner et al. (2019) and Pineda et al. (2021).",
                "To assure a fair comparison we used the hyperparameters provided by Janner et al. (2019) for all experiments with our approach and the NLL loss function used for the baseline.",
                "As a deep learning based Dyna algorithm, we chose Model-based Policy Optimization (MBPO) (Janner et al., 2019) and ran all of our experiments using the implementation provided by Pineda et al.",
                "As a deep learning based Dyna algorithm, we chose Model-based Policy Optimization (MBPO) (Janner et al., 2019) and ran all of our experiments using the implementation provided by Pineda et al. (2021).",
                "One of the core problems of model-based policy learning methods however is that the accuracy of the model directly influences the quality of the learned policy or plan (Schneider, 1997; Kearns & Singh, 2002; Ross & Bagnell, 2012; Talvitie, 2017; Luo et al., 2019; Janner et al., 2019).",
                "F.1 ABLATION EXPERIMENT WITH DETERMINISTIC MODELS\nIn our experiments in the Hopper domain, we used probabilistic models following Janner et al. (2019)."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "70311b37ffb14a04e7c39a298ee39cf609725125",
                "externalIds": {
                    "CorpusId": 248541089
                },
                "corpusId": 248541089,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/70311b37ffb14a04e7c39a298ee39cf609725125",
                "title": "V ALUE G RADIENT WEIGHTED M ODEL -B ASED R EINFORCEMENT L EARNING",
                "abstract": "Model-based reinforcement learning (MBRL) is a sample ef\ufb01cient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely \ufb01tted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition suggests that value-aware model learning would \ufb01x this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-Gradient weighted Model loss (VaGraM), a novel method for value-aware model learning which improves the performance of MBRL in challenging settings, such as small model capacity and the presence of distracting state dimensions. We analyze both MLE and value-aware approaches and demonstrate how they fail to account for exploration and the behavior of function approximation when learning value-aware models and highlight the additional goals that must be met to stabilize optimization in the deep learning setting. To achieve this, we leverage the gradient of the empirical value function as a measure of the sensitivity of the RL algorithm to model errors. We verify our analysis by showing that our loss function is able to achieve high returns on the Mujoco benchmark suite while being more robust than maximum likelihood based approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1387979639",
                        "name": "C. Voelcker"
                    },
                    {
                        "authorId": "2161339569",
                        "name": "Victor Liao"
                    },
                    {
                        "authorId": "2054554660",
                        "name": "Animesh Garg"
                    },
                    {
                        "authorId": "5689899",
                        "name": "Amir-massoud Farahmand"
                    }
                ]
            }
        },
        {
            "contexts": [
                "4 in [1], setting \u03b5 \u03c0 \u2264 \u03b5\u03c0 and all other errors set to 0.",
                "4 in [1], setting \u03b5 m \u2264 \u03b5m and all other errors set to 0.",
                "5 Proof for Returns Estimation Error Upper Bound In this section, we prove the upper bound of the value estimation error for MBRL under the IDM framework based on Janner\u2019s work [1].",
                "Next, we analyze the IDM framework based on Janner\u2019s work [1]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a32e1a4b1c1c3cc335458f5a8fcf8f268bd91f5e",
                "externalIds": {
                    "CorpusId": 249305692
                },
                "corpusId": 249305692,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a32e1a4b1c1c3cc335458f5a8fcf8f268bd91f5e",
                "title": "Model-Based Reinforcement Learning via Imagination with Derived Memory",
                "abstract": "The representation model aims to infer approximate state posteriors from past observations and actions, where q (st | zt\u22121, at\u22121, ot) is a diagonal Gaussian with mean and variance parameterized by a convolutional neural network (CNN) followed by a fully connected neural (FC) network. In order to enable accurate long-term predictions, the transition model is designed with both stochastic and deterministic paths. The latent state is split into a stochastic state st and a deterministic hidden state ht, where the stochastic state st is Gaussian with mean and variance parameterized by a fully connected neural network. The transition model f (ht\u22121, st\u22121, at\u22121) is implemented as a recurrent neural network (RNN). The observation model is Gaussian with mean parameterized by a transposed convolutional neural network and identity covariance. The reward model q(rt|zt) is a scalar Gaussian with mean parameterized by a fully connected (FC) neural network and unit variance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1675357512",
                        "name": "Yao Mu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They are also utilized as a component of MbRL (Janner et al., 2019).",
                "First, MaPER dramatically increases sample efficiency of state-of-the-art algorithms: SAC, TD3, Rainbow, and MBPO since it largely alleviates the underestimation or overestimation of the value with the conventional Q-learning.",
                "To this end, we consider components in Model-based RL (MbRL).",
                "MbRL. Figure 3 shows the learning curves of the MBPO with and without MaPER on the MuJoCo environments.",
                "Due to its ability to generate transitions, MbRL\u2019s sample efficiency is remarkable on certain tasks although much larger computing costs are generally needed.",
                "MBPO (Janner et al., 2019)) methods with negligible memory or computational overhead.",
                "We validate the effectiveness of MaPER with the following algorithms: Soft Actor-Critic (SAC) (Haarnoja et al., 2018a), Twin Delayed Deep Deterministic policy gradient (TD3), Rainbow (Hessel et al., 2018), and Model-based Policy Optimization (MBPO) (Janner et al., 2019).",
                "Although our approach is fundamentally different from Model-based RL (MbRL) approaches which generate virtual experiences to train agents by planning, we briefly introduce some of them because we want to verify our method in MbRL.",
                "Thus far, various MbRL methods (Kurutach et al., 2018; Luo et al., 2018; Clavera et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020; Clavera et al., 2020; Silver et al., 2017; Schrittwieser et al., 2020; Hafner et al., 2020a; Kaiser et al., 2019) have been proposed, but the common strategy\u2026",
                "\u2022 MaPER can be seamlessly integrated into any modern off-policy RL frameworks with critic networks, including both MfRL (e.g. SAC (Haarnoja et al., 2018a), TD3 (Fujimoto et al., 2018)\nand Rainbow (van Hasselt et al., 2019)) and MbRL (e.g. MBPO (Janner et al., 2019)) methods with negligible memory or computational overhead.",
                "We observe that MBPO with MaPER consistently outperforms the vanilla MBPO.",
                ", 2018), and Model-based Policy Optimization (MBPO) (Janner et al., 2019).",
                "Thus far, various MbRL methods (Kurutach et al., 2018; Luo et al., 2018; Clavera et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020; Clavera et al., 2020; Silver et al., 2017; Schrittwieser et al., 2020; Hafner et al., 2020a; Kaiser et al., 2019) have been proposed, but the common strategy across them is to first learn the environment models and use them to generate fictitious experiences for learning a policy.",
                "\u2026integrated into any modern off-policy RL frameworks with critic networks, including both MfRL (e.g. SAC (Haarnoja et al., 2018a), TD3 (Fujimoto et al., 2018)\nand Rainbow (van Hasselt et al., 2019)) and MbRL (e.g. MBPO (Janner et al., 2019)) methods with negligible memory or computational overhead.",
                "These impressive results with MBPO further show MaPER\u2019s versatility and effectiveness."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "52ac0747d42315bb85195a1878fd747a0679b600",
                "externalIds": {
                    "CorpusId": 251745710
                },
                "corpusId": 251745710,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/52ac0747d42315bb85195a1878fd747a0679b600",
                "title": "M ODEL - AUGMENTED P RIORITIZED E XPERIENCE R EPLAY",
                "abstract": "Experience replay is an essential component in off-policy model-free reinforcement learning (MfRL). Due",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1564629398",
                        "name": "Youngmin Oh"
                    },
                    {
                        "authorId": "2111247368",
                        "name": "J. Shin"
                    },
                    {
                        "authorId": "1720494",
                        "name": "Eunho Yang"
                    },
                    {
                        "authorId": "2110796623",
                        "name": "S. Hwang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "via measures of ensemble-member disagreement (Janner et al., 2019).",
                "Model-based approaches to ORL have commonly used ensembles as an attempt to quantify uncertainty, e.g. via measures of ensemble-member disagreement (Janner et al., 2019).",
                "\u2026systems to support model-based RL. Examples from online RL include Clavera et al. (2018); Kurutach et al. (2018), which learn one-step observation-based dynamics along with extensions to ensembles Deisenroth & Rasmussen (2011); Chua et al. (2018); Janner et al. (2019); Nagabandi et al. (2020).",
                "In this work, we use the same FF base-model architecture and training details as MBPO (Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9d969d6867c88e81d5fa023ca897341a23cb7ceb",
                "externalIds": {
                    "CorpusId": 253203523
                },
                "corpusId": 253203523,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9d969d6867c88e81d5fa023ca897341a23cb7ceb",
                "title": "B ENCHMARKS AND B ASELINES",
                "abstract": "Decision makers often wish to use offline historical data to compare sequentialaction policies at various world states. Importantly, computational tools should produce confidence values for such offline policy comparison (OPC) to account for statistical variance and limited data coverage. Nevertheless, there is little work that directly evaluates the quality of confidence values for OPC. In this work, we address this issue by creating benchmarks for OPC with Confidence (OPCC), derived by adding sets of policy comparison queries to datasets from offline reinforcement learning. In addition, we present an empirical evaluation of the risk versus coverage trade-off for a class of model-based baselines. In particular, the baselines learn ensembles of dynamics models, which are used in various ways to produce simulations for answering queries with confidence values. While our results suggest advantages for certain baseline variations, there appears to be significant room for improvement in future work.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [
                "Thus, naive policy optimisation on a learnt model in the offline setting can result in model exploitation (Janner et al., 2019; Kurutach et al., 2018; Rajeswaran et al., 2020), i.",
                "Following previous works (Kidambi et al., 2020; Yu et al., 2021; 2020), we use model-based policy optimisation (MBPO) (Janner et al., 2019) to learn the optimal policy for M\u0302 .",
                "Thus, naive policy optimisation on a learnt model in the offline setting can result in model exploitation (Janner et al., 2019; Kurutach et al., 2018; Rajeswaran et al., 2020), i.e. the policy chooses actions that the model erroneously predicts will lead to high reward.",
                "Our approach is based on Model-Based Policy Optimisation (Janner et al., 2019).",
                "To generate the synthetic data, MBPO performs k-step rollouts in M\u0302 starting from states s \u2208 D, and adds this data to D\u0302.",
                "MBPO utilises a standard off-policy actor-critic RL algorithm.",
                ", 2021; 2020), we use model-based policy optimisation (MBPO) (Janner et al., 2019) to learn the optimal policy for M\u0302 .",
                "The dashed red line indicates the Q-values from running MBPO (Janner et al., 2019).",
                "However, sampling full length trajectories is not desirable in model-based methods due to compounding modelling error (Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "1558c6b5796ebcafc6047020334aa82d3dfa5f94",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-00124",
                    "DOI": "10.48550/arXiv.2212.00124",
                    "CorpusId": 254125708
                },
                "corpusId": 254125708,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1558c6b5796ebcafc6047020334aa82d3dfa5f94",
                "title": "One Risk to Rule Them All: A Risk-Sensitive Perspective on Model-Based Offline Reinforcement Learning",
                "abstract": "Of\ufb02ine reinforcement learning (RL) is suitable for safety-critical domains where online exploration is too costly or dangerous. In safety-critical settings, decision-making should take into consideration the risk of catastrophic outcomes. In other words, decision-making should be risk-sensitive . Previous works on risk in of\ufb02ine RL combine together of\ufb02ine RL techniques, to avoid distributional shift, with risk-sensitive RL algorithms, to achieve risk-sensitivity. In this work, we propose risk-sensitivity as a mechanism to jointly address both of these issues. Our model-based approach is risk-averse to both epistemic and aleatoric uncertainty. Risk-aversion to epistemic uncertainty prevents distributional shift, as areas not covered by the dataset have high epistemic uncertainty. Risk-aversion to aleatoric uncertainty discourages actions that may result in poor outcomes due to environment stochasticity. Our experiments show that our algorithm achieves competitive performance on deterministic benchmarks, and outperforms existing approaches for risk-sensitive objectives in stochastic domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51300315",
                        "name": "Marc Rigter"
                    },
                    {
                        "authorId": "145350537",
                        "name": "Bruno Lacerda"
                    },
                    {
                        "authorId": "2072387078",
                        "name": "N. Hawes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "If we restrict ourselves to states and actions in the empirical dataset (EMP) or short-horizon rollouts that start in the empirical state-action distribution (DYNA), as is typical in Dyna-style approaches (Sutton and Barto, 2018; Janner et al., 2019), we limit ourselves to a small neighborhood of the empirical state-action distribution.",
                ", 2008), or as training data for the agent\u2019s policy and value functions (Sutton, 1991; Janner et al., 2019).",
                "\u2026actions in the empirical dataset (EMP) or short-horizon rollouts that start in the empirical state-action distribution (DYNA), as is typical in Dyna-style approaches (Sutton and Barto, 2018; Janner et al., 2019), we limit ourselves to a small neighborhood of the empirical state-action distribution.",
                "The model is \u201crolled out\u201d to generate \u201cimagined\u201d trajectories, which are used either for direct planning (De Boer et al., 2005; Chaslot et al., 2008), or as training data for the agent\u2019s policy and value functions (Sutton, 1991; Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "227062c151fb48f222f6c3b946a1618a46c01660",
                "externalIds": {
                    "CorpusId": 250385009
                },
                "corpusId": 250385009,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/227062c151fb48f222f6c3b946a1618a46c01660",
                "title": "M O C O DA: Model-based Counterfactual Data Augmentation",
                "abstract": "The number of states in a dynamic process is exponential in the number of objects, making reinforcement learning (RL) difficult in complex, multi-object domains. For agents to scale to the real world, they will need to react to and reason about unseen combinations of objects. We argue that the ability to recognize and use local factorization in transition dynamics is a key element in unlocking the power of multi-object reasoning. To this end, we show that (1) known local structure in the environment transitions is sufficient for an exponential reduction in the sample complexity of training a dynamics model, and (2) a locally factored dynamics model provably generalizes out-of-distribution to unseen states and actions. Knowing the local structure also allows us to pre-dict which unseen states and actions this dynamics model will generalize to. We propose to leverage these observations in a novel Model-based Counterfactual Data Augmentation (M O C O DA) framework. M O C O DA applies a learned locally factored dynamics model to an augmented distribution of states and actions to generate counterfactual transitions for RL. M O C O DA works with a broader set of local structures than prior work and allows for direct control over the augmented training distribution. We show that M O C O DA enables RL agents to learn policies that generalize to unseen states and actions. We use M O C O DA to train an offline RL agent to solve an out-of-distribution robotics manipulation task on which standard offline RL algorithms fail. 1",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32305445",
                        "name": "Silviu Pitis"
                    },
                    {
                        "authorId": "3422145",
                        "name": "Elliot Creager"
                    },
                    {
                        "authorId": "49686756",
                        "name": "Ajay Mandlekar"
                    },
                    {
                        "authorId": "2054554660",
                        "name": "Animesh Garg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To alleviate the issue of compounding model error, we adopt a branching strategy [16], [12] by replacing few long-horizon rollouts with many short-horizon rollouts to reduce compounding error in model-generated rollouts.",
                "While the success of model-based RL (MB-RL) has been witnessed in many single agent RL tasks [10], [11], [12], the understanding of its MARL counterpart is still limited.",
                "To reduce the negative effect of model error, we adopt a branched rollout scheme proposed in [12], [16].",
                "Due to low data efficiency, model-based methods are widely studied as a promising approach for improving sample efficiency [22], [8], [11], [12]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "69d6ca9250a63a63d94cbef3a44545d425cf605b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-06559",
                    "DOI": "10.48550/arXiv.2207.06559",
                    "CorpusId": 250526270
                },
                "corpusId": 250526270,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/69d6ca9250a63a63d94cbef3a44545d425cf605b",
                "title": "Fully Decentralized Model-based Policy Optimization for Networked Systems",
                "abstract": "\u2014Reinforcement learning algorithms require a large amount of samples; this often limits their real-world applications on even simple tasks. Such a challenge is more outstanding in multi-agent tasks, as each step of operation is more costly requiring communications or shifting or resources. This work aims to improve data ef\ufb01ciency of multi-agent control by model-based learning. We consider networked systems where agents are cooperative and communicate only locally with their neighbors, and propose the decentralized model-based policy optimization framework (DMPO). In our method, each agent learns a dynamic model to predict future states and broadcast their predictions by communication, and then the policies are trained under the model rollouts. To alleviate the bias of model- generated data, we restrain the model usage for generating myopic rollouts, thus reducing the compounding error of model generation. To pertain the independence of policy update, we introduce extended value function and theoretically prove that the resulting policy gradient is a close approximation to true policy gradients. We evaluate our algorithm on several benchmarks for intelligent transportation systems, which are connected autonomous vehicle control tasks (Flow and CACC) and adaptive traf\ufb01c signal control (ATSC). Empirically results show that our method achieves superior data ef\ufb01ciency and matches the performance of model-free methods using true models.Thesource code of our algorithm and baselines can be found at https://github.com/CDM1619/MARL-Algorithms .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390662136",
                        "name": "Yali Du"
                    },
                    {
                        "authorId": "2121455365",
                        "name": "Chengdong Ma"
                    },
                    {
                        "authorId": null,
                        "name": "Yuchen Liu"
                    },
                    {
                        "authorId": "2167032295",
                        "name": "Runji Lin"
                    },
                    {
                        "authorId": "2113412435",
                        "name": "Hao Dong"
                    },
                    {
                        "authorId": "48094081",
                        "name": "Jun Wang"
                    },
                    {
                        "authorId": "47796324",
                        "name": "Yaodong Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In order to estimate the model uncertainty accurately and alleviate the model exploitation problem, we follow previous works (Janner et al., 2019; Yu et al., 2020; Kidambi et al., 2020; Rafailov et al., 2021; Yu et al., 2021) and construct a bootstrap ensemble of K environment models {M\u0302}i=1.",
                "However, RL algorithms developed for the online/interactive setting usually perform poorly in the offline setting (Fujimoto et al., 2019; Janner et al., 2019) due to the data distribution shift caused by (1) the difference between the policy-in-training and the behavior policies used to collect the data; and (2) the difference between the realistic environment in which we will deploy the policy and the environments used to collect the data.",
                "However, RL algorithms developed for the online/interactive setting usually perform poorly in the offline setting (Fujimoto et al., 2019; Janner et al., 2019) due to the data distribution shift caused by (1) the difference between the policy-in-training and the behavior policies used to collect the\u2026",
                "Prior work (Ha & Schmidhuber, 2018; Janner et al., 2019; Yu et al., 2020) demonstrated that the method effectively alleviates the model exploitation issue (Levine et al., 2020), especially in the offline setting.",
                "In order to estimate the model uncertainty accurately and alleviate the model exploitation problem, we follow previous works (Janner et al., 2019; Yu et al., 2020; Kidambi et al., 2020; Rafailov et al., 2021; Yu et al., 2021) and construct a bootstrap ensemble of K environment models {M\u0302i}Ki=1."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1e650cb12aaad371a49b0c8c4514e1b988a5178c",
                "externalIds": {
                    "DBLP": "conf/iclr/YangJZMS22",
                    "CorpusId": 251649208
                },
                "corpusId": 251649208,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1e650cb12aaad371a49b0c8c4514e1b988a5178c",
                "title": "Pareto Policy Pool for Model-based Offline Reinforcement Learning",
                "abstract": "we apply a subset of datasets, including three environments (halfcheetah, hopper, and walker2d) and \ufb01ve dataset types (random, medium, medium-replay, expert, and medium-expert), to yield a total of 15 benchmark problems, in which random contains 1M samples from a random policy, medium contains 1M samples from a policy trained to approximately 1/3 of the performance of the expert, expert contains 1M samples from a policy trained to the performance of the expert, medium-replay contains the whole replay buffer of a policy trained up to the performance of the medium agent, and medium-expert contains a 50-50 split of medium and expert dataset (2M samples).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108568169",
                        "name": "Yijun Yang"
                    },
                    {
                        "authorId": "2118240389",
                        "name": "J. Jiang"
                    },
                    {
                        "authorId": "2144115714",
                        "name": "Tianyi Zhou"
                    },
                    {
                        "authorId": "2115889693",
                        "name": "Jie Ma"
                    },
                    {
                        "authorId": "2051687296",
                        "name": "Yuhui Shi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based algorithms rely on an approximation of the environment\u2019s dynamics (Sutton 1991; Janner et al. 2019).",
                "Modelling the environment dynamics as a Gaussian distribution is common for continuous state-space applications (Janner et al. 2019; Yu et al. 2020; Kidambi et al. 2020; Yu et al. 2021).",
                "In the online setting, they tend to improve sample efficiency (Kalweit and Boedecker 2017; Janner et al. 2019; Feinberg et al. 2018; Buckman et al. 2018; Chua et al. 2018).",
                "Model-based RL approaches typically use such dynamics\u2019 models conditioned on the action as well as the state to make predictions (Janner et al. 2019; Yu et al. 2020; Kidambi et al. 2020; Argenson and Dulac-Arnold 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3f14defdaf32541312c1378ac4c1dcf1e685789b",
                "externalIds": {
                    "CorpusId": 253180619
                },
                "corpusId": 253180619,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3f14defdaf32541312c1378ac4c1dcf1e685789b",
                "title": "Squeezing more value out of your historical data: data-augmented behavioural cloning as launchpad for reinforcement learning",
                "abstract": "In many real-world applications collecting large, high-quality datasets may be too costly or impractical. Offline reinforcement learning (RL) aims to infer an optimal decision-making policy from a fixed set of data. Getting the most information from this dataset is then vital for good performance. We pro- pose a model-based data augmentation strategy, Trajectory Stitching (TS), to improve the quality of sub-optimal trajec- tories. TS introduces unseen actions joining previously disconnected states: using a probabilistic notion of state reach- ability, it effectively \u2018stitches\u2019 together parts of the historical demonstrations to generate new, higher quality ones. A stitch- ing event consists of a transition between a pair of observed states through a synthetic and highly probable action. New ac- tions are introduced only when they are expected to be benefi-cial, according to an estimated state-value function. We show that using supervised learning, behavioural cloning (BC), to extract a decision-making policy from the new TS dataset, leads to improvements over the behaviour-cloned policy from the original dataset. Improving over the BC policy could then be used as a launchpad for online RL through planning and demonstration-guided RL.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [
                "MOPO extends MBPO (Janner et al., 2019), which combines the soft-actor critic (SAC) policy gradient algorithm (Haarnoja et al.",
                "MOPO extends MBPO (Janner et al., 2019), which combines the soft-actor critic (SAC) policy gradient algorithm (Haarnoja et al., 2018a;b) with using learned dynamics models to generate short roll-outs.",
                "\u2026shift, can result in model exploitation: the policy being trained learns to take advantage of model errors when optimizing the reward, leading to poor performance in the real environment (Cang et al., 2021; Rajeswaran et al., 2016; Janner et al., 2019; Clavera et al., 2018; Levine et al., 2020).",
                "Errors made by the learned environment models, such as those arising from poor generalization performance under distributional shift, can result in model exploitation: the policy being trained learns to take advantage of model errors when optimizing the reward, leading to poor performance in the real environment (Cang et al., 2021; Rajeswaran et al., 2016; Janner et al., 2019; Clavera et al., 2018; Levine et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "6e33c58326ad8e6f0b9719fe398771878383d558",
                "externalIds": {
                    "CorpusId": 253181117
                },
                "corpusId": 253181117,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6e33c58326ad8e6f0b9719fe398771878383d558",
                "title": "D OMAIN G ENERALIZATION FOR R OBUST M ODEL -B ASED O FFLINE RL",
                "abstract": "Existing offline reinforcement learning (RL) algorithms typically assume that training data is either: 1) generated by a known policy, or 2) of entirely unknown origin. We consider multi-demonstrator offline RL, a middle ground where we know which demonstrators generated each dataset, but make no assumptions about the underlying policies of the demonstrators. This is the most natural setting when collecting data from multiple human operators, yet remains unexplored. Since different demonstrators induce different data distributions, we show that this can be naturally framed as a domain generalization problem, with each demonstrator corresponding to a different domain. Specifically, we propose Domain-Invariant Model-based Offline RL (DIMORL), where we apply Risk Extrapolation (REx) (Krueger et al., 2020) to the process of learning dynamics and rewards models. Our results show that models trained with REx exhibit improved domain generalization performance when compared with the natural baseline of pooling all demonstrators\u2019 data. We observe that the resulting models frequently enable the learning of superior policies in the offline model-based RL setting, can improve the stability of the policy learning process, and potentially increase exploration.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [
                "Model-Based Policy Optimization (MBPO) [10].",
                "MBPO is only allowed to train for 100,000 timesteps after which its training is frozen due to its training being very computationally expensive (compared to SAC).",
                "The results show that APEL and SSBAS correctly select MBPO as the best-performing learner and do not switch to SAC since it never (in the 10 million timesteps) starts outperforming MBPO.",
                "We also run APEL and SSBAS on the Mujoco Half Cheetah domain with a SAC base learner and a MBPO base learner.",
                "The learning mechanisms used by RL learners can be broadly divided into four classes: (1) algorithms that learn (action) value functions in order to determine a policy, such as Qlearning [5] and DQN [6], (2) algorithms that learn policies directly through policy gradients, such as REINFORCE [7], (3) algorithms that combine the two through actor-critic methods, such as Soft Actor-Critic (SAC) [8] and Actor-Critic with Experience Replay (ACER) [9], and (4) algorithms that learn a model of the world and then use that model to learn a policy, e.g. Model-Based Policy Optimization (MBPO) [10].",
                "The authors\u2019 implementation of the MBPO algorithm is used with their hyperparameters and the stable baselines [21] implementation is used for the SAC algorithm."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "432c47c0b3e4584508fbd224eb2da4d76828c306",
                "externalIds": {
                    "DOI": "10.1051/matecconf/202237007008",
                    "CorpusId": 254332813
                },
                "corpusId": 254332813,
                "publicationVenue": {
                    "id": "b877cd38-7f65-4f61-aa13-3413da5de430",
                    "name": "MATEC Web of Conferences",
                    "alternate_names": [
                        "MATEC Web Conf"
                    ],
                    "issn": "2261-236X",
                    "url": "https://web.archive.org/web/*/http:/www.matec-conferences.org/",
                    "alternate_urls": [
                        "https://www.matec-conferences.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/432c47c0b3e4584508fbd224eb2da4d76828c306",
                "title": "Improving Reinforcement Learning with Ensembles of Different Learners",
                "abstract": "Different reinforcement learning (RL) methods exist to address the problem of combining multiple different learners to generate a superior learner. These existing methods usually assume that each learner uses the same algorithm and/or state representation. We propose an ensemble learner that combines a set of base learners and leverages the strengths of the different base learners online. We demonstrate the proposed ensemble learner\u2019s ability to combine the strengths of multiple base learners and adapt to changes in base learner performance on various domains, including the Atari Breakout domain.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1585231703",
                        "name": "Gerrie Crafford"
                    },
                    {
                        "authorId": "2831294",
                        "name": "Benjamin Rosman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We select two commonly adopted dyna-style MBRL algorithms \u2013 SLBO [6] and MBPO [7] as a foundation for evaluating value-aware approaches in continuous control.",
                "3 Evaluation on continuous control environments for value aware methods and baselines with MBPO [7], without tuning existing parameters, over 5 random seeds.",
                "Next, we describe our algorithm with which we find positive results in conjunction with the SLBO algorithm [6] on Swimmer-v1, Hopper-v1 and Ant-v1 and in conjunction with the MBPO algorithm [7] on Walker-v2 and HalfCheetah-v2.",
                "While several such methods have focused on how to better utilize a learned parametrized model [6, 7, 20], the choice of objective for model learning \u2013 specifically the learning of a dynamics model for predicting state transitions \u2013 has largely been overlooked.",
                "We empirically test our proposed algorithm and novel upper bound on two recent dynastyle MBRL algorithms \u2013 SLBO [6] and MBPO [7].",
                "KL [66], total-variation [7] or Wasserstein distances [67].",
                "More recent research in MBRL has focused on efforts to overcome these shortcomings \u2013 including optimizing for auxiliary objectives [54, 55, 56], augmenting model-learning with exploration strategies [7, 57], meta-learning to closely intertwine the two objectives [58] and introducing inductive biases to the model-learning objective [59].",
                "Second, we evaluate Algorithm 2 together with our proposed and a prior value aware objective on several continuous control tasks, with two recent dyna-style MBRL algorithms \u2013 SLBO [6] and MBPO [7].",
                "3: Evaluation on continuous control environments for value aware methods and baselines with MBPO [7], without tuning existing parameters, over 5 random seeds."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "111ada0d13076516770657cf17c119ecf3379dfb",
                "externalIds": {
                    "CorpusId": 263306689
                },
                "corpusId": 263306689,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/111ada0d13076516770657cf17c119ecf3379dfb",
                "title": "LEVERAGING VALUE-AWARENESS FOR ONLINE AND OFFLINE MODEL-BASED REINFORCEMENT LEARNING",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "20809363",
                        "name": "Nirbhay Modhe"
                    },
                    {
                        "authorId": "1746610",
                        "name": "Dhruv Batra"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ba23164f2daaa43771454184a8ae9aa83b8a687d",
                "externalIds": {
                    "MAG": "3201970852",
                    "DOI": "10.1016/j.ijepes.2021.107625",
                    "CorpusId": 244225763
                },
                "corpusId": 244225763,
                "publicationVenue": {
                    "id": "d7813d52-7e6c-4c33-a5a8-143c7afa7892",
                    "name": "International Journal of Electrical Power & Energy Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Electr Power  Energy Syst"
                    ],
                    "issn": "0142-0615",
                    "url": "http://www.elsevier.com/locate/issn/01420615",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/international-journal-of-electrical-power-and-energy-systems",
                        "http://www.sciencedirect.com/science/journal/01420615"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ba23164f2daaa43771454184a8ae9aa83b8a687d",
                "title": "Model-based deep reinforcement learning for wind energy bidding",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31026233",
                        "name": "Manassakan Sanayha"
                    },
                    {
                        "authorId": "1715851",
                        "name": "P. Vateekul"
                    }
                ]
            }
        },
        {
            "contexts": [
                "35th Conference on Neural Information Processing Systems (NeurIPS 2021).\ndata (e.g. ME-TRPO [6], MBPO [7]) to improve policy optimization.",
                "ME-TRPO [6], MBPO [7]) to improve policy optimization."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4ac07c52de7d7ad1a98d346ce5a2c0c1dc6b8cf2",
                "externalIds": {
                    "CorpusId": 246995329
                },
                "corpusId": 246995329,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4ac07c52de7d7ad1a98d346ce5a2c0c1dc6b8cf2",
                "title": "Model-Based Reinforcement Learning via Imagination with Derived Memory",
                "abstract": "Model-based reinforcement learning aims to improve the sample efficiency of policy learning by modelling the dynamics of the environment. Recently, the latent dynamics model has been further developed to enable fast planning in a compact space. It summarizes the high-dimensional experiences of an agent, which mimics the memory function of humans. Learning policies via imagination with the latent model shows great potential for solving complex tasks. However, only considering memories from the true experiences in the process of imagination could limit its advantages. Inspired by the memory prosthesis proposed by neuroscientists, we present a novel model-based reinforcement learning framework called Imagining with Derived Memory (IDM). It enables the agent to learn policy from enriched diverse imagination with prediction-reliability weight, thus improving sample efficiency and policy robustness. Experiments on various high-dimensional visual control tasks in the DMControl benchmark demonstrate that IDM outperforms previous state-of-the-art methods in terms of policy robustness and further improves the sample efficiency of the model-based method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1675357512",
                        "name": "Yao Mu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are also uncertainty-based and model-based methods that regularize the value function or policy with epistemic uncertainty estimated from model or value function [Janner et al., 2019; Yu et al., 2020; Uehara and Sun, 2021; Wu et al., 2021; Zhan et al., 2022]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4ff099b6fa07017bb065669028957c9ff36041ab",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-11027",
                    "DOI": "10.48550/arXiv.2205.11027",
                    "CorpusId": 248987055
                },
                "corpusId": 248987055,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4ff099b6fa07017bb065669028957c9ff36041ab",
                "title": "Distance-Sensitive Offline Reinforcement Learning",
                "abstract": "In of\ufb02ine reinforcement learning (RL), one detrimental issue to policy learning is the error accumulation of deep Q function in out-of-distribution (OOD) areas. Unfortunately, existing of\ufb02ine RL methods are often over-conservative, inevitably hurting generalization performance outside data distribution. In our study, one interesting observation is that deep Q functions approximate well inside the convex hull of training data. Inspired by this, we propose a new method, DOGE (Distance-sensitive Of\ufb02ine RL with better GEneralization) . DOGE marries dataset geometry with deep function approximators in of\ufb02ine RL, and enables exploitation in generalizable OOD areas rather than strictly constraining policy within data distribution. Speci\ufb01cally, DOGE trains a state-conditioned distance function that can be readily plugged into standard actor-critic methods as a policy constraint. Simple yet elegant, our algorithm enjoys better generalization compared to state-of-the-art methods on D4RL benchmarks. Theoretical analysis demonstrates the superiority of our approach to existing methods that are solely based on data distribution or support constraints.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2136086524",
                        "name": "Jianxiong Li"
                    },
                    {
                        "authorId": "3415564",
                        "name": "Xianyuan Zhan"
                    },
                    {
                        "authorId": "49507262",
                        "name": "Haoran Xu"
                    },
                    {
                        "authorId": "2144104090",
                        "name": "Xiangyu Zhu"
                    },
                    {
                        "authorId": "46700348",
                        "name": "Jingjing Liu"
                    },
                    {
                        "authorId": "2142985939",
                        "name": "Ya-Qin Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e0bfdcf3f4a4cd282e2916855c1fb02958d52738",
                "externalIds": {
                    "CorpusId": 249039402
                },
                "corpusId": 249039402,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e0bfdcf3f4a4cd282e2916855c1fb02958d52738",
                "title": "Synthesizing and Simulating Volumetric Meshes from Vision-based Tactile Imprints",
                "abstract": "Vision-based tactile sensors typically employ a deformable elastomer and a camera to provide high-resolution contact images. This work focuses on learning to simulate and synthesize the volumetric mesh of the elastomer based on the image imprints acquired from tactile sensors. Obtaining accurate volumetric meshes for the elastomer can provide direct contact information and bene\ufb01t robotic grasping and manipulation. Our method [1] proposes a train-then-adapt way to leverage synthetic image-mesh pairs and real-world images from \ufb01nite element methods (FEM) and physical sensors. Our approach can accurately reconstruct the deformation of the real-world tactile sensor elastomer in various domains. While the proposed learning approaches have shown to produce solutions, we discuss some limitations and challenges for viable real-world applications. Abstract \u2014Vision-based tactile sensors typically employ a deformable elastomer and a camera to provide high-resolution contact images. This work focuses on learning to simulate and synthesize the volumetric mesh of the elastomer based on the image imprints acquired from tactile sensors. Obtaining accurate volumetric meshes for the elastomer can provide direct contact information and benefit robotic grasping and manipulation. Our method [1] proposes a train-then-adapt way to leverage synthetic image-mesh pairs and real-world images from finite element methods (FEM) and physical sensors. Our approach can accurately reconstruct the deformation of the real-world tactile sensor elastomer in various domains. While the proposed learning approaches have shown to produce solutions, we discuss some limitations and challenges for viable real-world applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8362363",
                        "name": "Xinghao Zhu"
                    },
                    {
                        "authorId": "1741915",
                        "name": "Siddarth Jain"
                    },
                    {
                        "authorId": "1680165",
                        "name": "M. Tomizuka"
                    },
                    {
                        "authorId": "1751880",
                        "name": "J. Baar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works have proposed solutions to deal with this [24, 36, 40]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8165b4c0b47576b34591473e951ba6ffb2aba9a7",
                "externalIds": {
                    "DBLP": "conf/atal/CarrenoNPP22",
                    "DOI": "10.5555/3535850.3535876",
                    "CorpusId": 248700412
                },
                "corpusId": 248700412,
                "publicationVenue": {
                    "id": "6c3a9833-5fac-49d3-b7e7-64910bd40b4e",
                    "name": "Adaptive Agents and Multi-Agent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Adapt Agent Multi-agent Syst",
                        "International Joint Conference on Autonomous Agents & Multiagent Systems",
                        "Adapt Agent Multi-agents Syst",
                        "AAMAS",
                        "Adaptive Agents and Multi-Agents Systems",
                        "Int Jt Conf Auton Agent  Multiagent Syst"
                    ],
                    "url": "http://www.ifaamas.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8165b4c0b47576b34591473e951ba6ffb2aba9a7",
                "title": "Planning, Execution, and Adaptation for Multi-Robot Systems using Probabilistic and Temporal Planning",
                "abstract": "Planning for multi-robot coordination during long horizonmissions in complex environments need to consider resources, temporal constraints, and uncertainty. This could be computationally expensive and impractical for online planning and execution. We propose a decoupled framework to address this. At the high-level, we plan for multi-robot missions that require coordination amongst robots considering temporal and numeric constraints. The temporal plan is decomposed into low-level plans for individual robots. At the lowlevel, we perform online learning and adaptation due to unexpected probabilistic outcomes to achieve mission goals. Our framework learns over time to improve the performance by (1) updating the learned domain model to reduce model prediction errors and (2) constraining the robot\u2019s capabilities which in turn improves goal allocation. The approach provides a solution to planning problems that require long-term robot operability. We demonstrate the performance of our approach via experiments involving a fleet of heterogeneous robots.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151259675",
                        "name": "Yaniel Carreno"
                    },
                    {
                        "authorId": "100485748",
                        "name": "Jun Hao Alvin Ng"
                    },
                    {
                        "authorId": "1711690",
                        "name": "Y. P\u00e9tillot"
                    },
                    {
                        "authorId": "35407423",
                        "name": "Ronald P. A. Petrick"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "78d0aa99bc9cefea2658f014b76f4f22ce3a2172",
                "externalIds": {
                    "DOI": "10.11606/d.45.2022.tde-28062022-123656",
                    "CorpusId": 250136067
                },
                "corpusId": 250136067,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/78d0aa99bc9cefea2658f014b76f4f22ce3a2172",
                "title": "Model-based policy gradients: an empirical study on linear quadratic environments",
                "abstract": "\u00c2ngelo Greg\u00f3rio Lovatto. Model-Based Policy Gradients: An empirical study on linear quadratic environments. Thesis (Master\u2019s). Institute of Mathematics and Statistics, University of S\u00e3o Paulo, S\u00e3o Paulo, 2022. Stochastic Value Gradient (SVG) methods underlie many recent achievements of model-based Reinforcement Learning (RL) agents in continuous state-action spaces. Such methods use data collected by exploration in the environment to produce a model of its dynamics, which is then used to approximate the gradient of the objective function w.r.t. the agent\u2019s parameters. Despite the practical signi cance of these methods, many algorithm design choices still lack rigorous theoretical or empirical justi cation. Instead, most works rely heavily on benchmark-centric evaluation methods, which confound the contributions of several components of an RL agent\u2019s design to the nal performance. In this work, we propose a negrained analysis of core algorithmic components of SVGs, including: the gradient estimator formula, model learning and value function approximation. We implement a con gurable benchmark environment based on the Linear Quadratic Gaussian (LQG) regulator, allowing us to compute the ground-truth SVG and compare it with learning approaches. We conduct our analysis on a range of LQG environments, evaluating the impact of each algorithmic component in prediction and control tasks. Our results show that a widely used gradient estimator induces a favorable bias-variance trade-o , using a biased expectation that yields better gradient estimates in smaller sample regimes than the unbiased expression for the gradient. On model learning, we show that over tting to on-policy data may occur, leading to accurate state predictions but inaccurate gradients, highlighting the importance of exploration even in stochastic environments. We also show that value function approximation can be more unstable than model learning, even in simple linear environments. Finally, we evaluate performance when using the model for direct gradient estimation vs. for value function approximation, concluding that the former is more e ective for both prediction and control.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1452337990",
                        "name": "\u00c2. G. Lovatto"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[23] use forward and backward models in model-based policy optimization [19], a model based actor-critic method, in order to reduce accumulative model error while maintaining a similar update depth."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4f7db3687405c2041ee422822568a4827c7d8f8a",
                "externalIds": {
                    "CorpusId": 251492078
                },
                "corpusId": 251492078,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4f7db3687405c2041ee422822568a4827c7d8f8a",
                "title": "Monte Carlo Tree Search in the Presence of Model Uncertainty",
                "abstract": "Monte Carlo Tree Search (MCTS) is a extremely successful search-based framework for decision making. With an accurate simulator of the environment\u2019s dynamics, it can achieve great performance in many games and non-games applications. However, without a perfect simulator, the performance degradation is so high that it can make the framework almost useless. Therefore, we propose two methods to improve the performance of MCTS in such a scenario: Deep Q-Network MCTS (DQMCTS) and Uncertainty Adapted MCTS (UAMCTS). In the former, we use the model-free algorithm DQN to evaluate the leaf nodes in the search tree. Although this approach shows promising improvement over baseline MCTS, our results show that there is still more room for improvement. In UAMCTS, we take a more fundamental approach and change the behavior of MCTS\u2019s components to directly take the model incorrectness into account. Our results show that with an accurate measure of model incorrectness, UAMCTS can achieve the performance of MCTS with a perfect simulator in some cases. Even with a poor measure of model error, UAMCTS can still outperform plain MCTS with an imperfect simulator.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2044295861",
                        "name": "Kiarash Aghakasiri"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a872ba82fcdd25bfc939aa9b34b4acbb0797fee7",
                "externalIds": {
                    "DOI": "10.1016/j.ifacol.2022.08.061",
                    "CorpusId": 252274619
                },
                "corpusId": 252274619,
                "publicationVenue": {
                    "id": "af98f1eb-affb-4b55-b8ff-1964b29cf894",
                    "name": "IFAC-PapersOnLine",
                    "type": "journal",
                    "issn": "2405-8963",
                    "url": "https://www.journals.elsevier.com/ifac-papersonline/",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/24058963",
                        "https://www.journals.elsevier.com/ifac-papersonline"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a872ba82fcdd25bfc939aa9b34b4acbb0797fee7",
                "title": "Quantum reinforcement learning method and application based on value function",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2185020629",
                        "name": "Yi-Pei Liu"
                    },
                    {
                        "authorId": "46355830",
                        "name": "Qing-Shan Jia"
                    },
                    {
                        "authorId": "2108600798",
                        "name": "Xu Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Dynamics models combined with powerful search methods have led to impressive results on a wide variety of tasks such as Atari (Schrittwieser et al., 2020) and continuous control (Hafner et al., 2019a; Janner et al., 2019; Sikchi et al., 2021; Lowrey et al., 2018)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6942045726eb5ff6b51bfe79519987ebd9c5785a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-12016",
                    "DOI": "10.48550/arXiv.2209.12016",
                    "CorpusId": 252532051
                },
                "corpusId": 252532051,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6942045726eb5ff6b51bfe79519987ebd9c5785a",
                "title": "Unsupervised Model-based Pre-training for Data-efficient Control from Pixels",
                "abstract": "Controlling arti\ufb01cial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed in this but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, whether current unsupervised strategies improve generalization capabilities is still unclear, especially in visual control settings. In this work, we design an effective unsupervised RL strategy for data-ef\ufb01cient visual control. First, we show that world models pre-trained with data collected using unsupervised RL can facilitate adaptation for future tasks. Then, we analyze several design choices to adapt ef\ufb01ciently, effectively reusing the agents\u2019 pre-trained components, and learning and planning in imagination, with our hybrid planner, which we dub Dyna-MPC. By combining the \ufb01ndings of a large-scale empirical study, we establish an approach that strongly improves performance on the Unsupervised RL Benchmark, requiring 20 \u00d7 less data to match the performance of supervised methods. The approach also demonstrates robust performance on the Real-Word RL benchmark, hinting that the approach generalizes to noisy environments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1818842",
                        "name": "Sai Rajeswar"
                    },
                    {
                        "authorId": "2098445287",
                        "name": "Pietro Mazzaglia"
                    },
                    {
                        "authorId": "2413244",
                        "name": "Tim Verbelen"
                    },
                    {
                        "authorId": "2064234172",
                        "name": "Alexandre Pich'e"
                    },
                    {
                        "authorId": "1733741",
                        "name": "B. Dhoedt"
                    },
                    {
                        "authorId": "2058336670",
                        "name": "Aaron C. Courville"
                    },
                    {
                        "authorId": "8651990",
                        "name": "Alexandre Lacoste"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our work is also related to model-based RL methods which jointly learn dynamics and reward models to guide planning [45\u201347], policy search [48, 49], or combine both [50, 51]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "09fc037f43fa3fbe7792ad801e71c7e0bd92a386",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-12250",
                    "DOI": "10.48550/arXiv.2210.12250",
                    "CorpusId": 253098434
                },
                "corpusId": 253098434,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/09fc037f43fa3fbe7792ad801e71c7e0bd92a386",
                "title": "TAPS: Task-Agnostic Policy Sequencing",
                "abstract": "\u2014Advances in robotic skill acquisition have made it possible to build general-purpose libraries of primitive skills for downstream manipulation tasks. However, naively executing these learned primitives one after the other is unlikely to succeed without accounting for dependencies between actions prevalent in long-horizon plans. We present Task-Agnostic Policy Sequencing (TAPS), a scalable framework for training manipulation primitives and coordinating their geometric dependencies at plan-time to ef\ufb01ciently solve long-horizon tasks never seen by any primitive during training. Based on the notion that Q-functions encode a measure of action feasibility, we formulate motion planning as a maximization problem over the expected success of each individual primitive in the plan, which we estimate by the product of their Q-values. Our experiments indicate that this objective function approximates ground truth plan feasibility and, when used as a planning objective, reduces myopic behavior and thereby promotes task success. We further demonstrate how TAPS can be used for task and motion planning by estimating the geometric feasibility of candidate action sequences provided by a task planner. We evaluate our approach in simulation and on a real robot.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1742185866",
                        "name": "Christopher Agia"
                    },
                    {
                        "authorId": "2178889",
                        "name": "Toki Migimatsu"
                    },
                    {
                        "authorId": "3045089",
                        "name": "Jiajun Wu"
                    },
                    {
                        "authorId": "1775407",
                        "name": "J. Bohg"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0f75eaaa4d900320d13f7b902ec533bc8b4eac2c",
                "externalIds": {
                    "CorpusId": 253113185
                },
                "corpusId": 253113185,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0f75eaaa4d900320d13f7b902ec533bc8b4eac2c",
                "title": "Lyapunov Robust Constrained-MDPs for Sim2Real Transfer Learning",
                "abstract": "Safety and robustness are two desired properties for any reinforcement learning algorithm. Constrained Markov Decision Processes (CMDPs) can handle additional safety constraints and Robust Markov Decision Processes (RMDPs) can perform well under model uncertainties. In this chapter, we propose to unify these two frameworks resulting in Robust Constrained MDPs (RCMDPs). The motivation is to develop a framework that can satisfy safety constraints while also simultaneously o\ufb00er robustness to model uncertainties. We develop the RCMDP objective, derive gradient update formula to optimize this objective and then propose policy gradient based algorithms. We also independently propose Lyapunov-based reward shaping for RCMDPs, yielding better stability and convergence properties. Abstract Safety and robustness are two desired properties for any reinforcement learning algorithm. Constrained Markov Decision Processes (CMDPs) can handle additional safety constraints and Robust Markov Decision Processes (RMDPs) can perform well under model uncertainties. In this chapter, we propose to unify these two frameworks resulting in Robust Constrained MDPs (RCMDPs). The motivation is to develop a framework that can satisfy safety constraints while also simultaneously o\ufb00er robustness to model uncertainties. We develop the RCMDP objective, derive gradient update formula to optimize this objective and then propose policy gradient based algorithms. We also independently propose Lyapunov-based reward shaping for RCMDPs, yielding better stability and convergence properties.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "10766679",
                        "name": "R. Russel"
                    },
                    {
                        "authorId": "1730046",
                        "name": "M. Benosman"
                    },
                    {
                        "authorId": "1751880",
                        "name": "J. Baar"
                    },
                    {
                        "authorId": "102147358",
                        "name": "Radu Corcodel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026have been proposed as the underlying cause with comprehensive analyses, and several mitigation strategies, such as model-based data augmentation (Janner et al., 2019a), the use of ensembles (Chen et al., 2021), network regularizations (Hiraoka et al., 2021), and periodically reseting the RL\u2026",
                "\u2026it is important to develop sample-efficient deep RL algorithms, that can learn efficiently even with limited amount of experience, and devising such efficient RL algorithm has been a important thread of research in recent years (Janner et al., 2019b; Chen et al., 2021; Hiraoka et al., 2021).",
                "However, na\u0131\u0308vely doing this can lead to worse performance (e.g., on DMC tasks (Nikishin et al., 2022) and on MuJoCo gym tasks (Janner et al., 2019b))."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "19081eebe18d7aa45073931d0a316204567f396e",
                "externalIds": {
                    "CorpusId": 253180402
                },
                "corpusId": 253180402,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/19081eebe18d7aa45073931d0a316204567f396e",
                "title": "E FFICIENT D EEP R EINFORCEMENT L EARNING R EQUIRES R EGULATING S TATISTICAL O VERFITTING authors",
                "abstract": "Deep reinforcement learning algorithms that learn policies by trial-and-error must learn from limited amounts of data collected by actively interacting with the environment. While many prior works have shown that proper regularization techniques are crucial for enabling data-efficient RL, a general understanding of the bottlenecks in data-efficient RL has remained unclear. Consequently, it has been difficult to devise a universal technique that works well across all domains. In this paper, we attempt to understand the primary bottleneck in sample-efficient deep RL by examining several potential hypotheses such as non-stationarity, excessive action distribution shift, and overfitting. We perform thorough empirical analysis on state-based DeepMind control suite (DMC) tasks in a controlled and system-atic way to show that statistical overfitting on the temporal-difference (TD) error is the main culprit that severely affects the performance of deep RL algorithms, and prior methods that lead to good performance do in fact, control the amount of statistical overfitting. This observation gives us a robust principle for making deep RL efficient: we can hill-climb on a notion of validation temporal-difference error by utilizing any form of regularization techniques from supervised learning. We show that a simple online model selection method that targets the statistical overfitting issue is effective across state-based DMC and Gym tasks.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [
                "Accelerate learning via model-based planning For sequential decision making problems, model-based planning is a powerful approach to improve sample efficiency and has achieved great success in applied domains such as game playing [79\u201381] and continuous control [82,83]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bafbb3c535d9ee0fbffaad266f732a3892f53b4e",
                "externalIds": {
                    "DOI": "10.20517/ir.2022.20",
                    "CorpusId": 252442857
                },
                "corpusId": 252442857,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bafbb3c535d9ee0fbffaad266f732a3892f53b4e",
                "title": "Deep reinforcement learning for real-world quadrupedal locomotion: a comprehensive review",
                "abstract": "Building controllers for legged robots with agility and intelligence has been one of the typical challenges in the pursuit of artificial intelligence (AI). As an important part of the AI field, deep reinforcement learning (DRL) can realize sequential decision making without physical modeling through end-to-end learning and has achieved a series of major breakthroughs in quadrupedal locomotion research. In this review article, we systematically organize and summarize relevant important literature, covering DRL algorithms from problem setting to advanced learning methods. These algorithms alleviate the specific problems encountered in the practical application of robots to a certain extent. We first elaborate on the general development trend in this field from several aspects, such as the DRL algorithms, simulation environments, and hardware platforms. Moreover, core components in the algorithm design, such as state and action spaces, reward functions, and solutions to reality gap problems, are highlighted and summarized. We further discuss open problems and propose promising future research directions to discover new areas of research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155343887",
                        "name": "Hongyin Zhang"
                    },
                    {
                        "authorId": "47648453",
                        "name": "Li He"
                    },
                    {
                        "authorId": "2144293191",
                        "name": "Donglin Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As for the multi-joint dynamics with contact (MuJoCo) benchmark, advanced MBRL algorithms have been able to optimize the reward function [50], [51], [52]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0d28e74991edaa2a64f9d55cce180d60e30cee46",
                "externalIds": {
                    "DBLP": "journals/access/SanayhaV22",
                    "DOI": "10.1109/ACCESS.2022.3224460",
                    "CorpusId": 253950724
                },
                "corpusId": 253950724,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0d28e74991edaa2a64f9d55cce180d60e30cee46",
                "title": "Model-Based Approach on Multi-Agent Deep Reinforcement Learning With Multiple Clusters for Peer-To-Peer Energy Trading",
                "abstract": "Peer-to-peer (P2P) energy trading system has the ability to completely revolutionize the current household energy system by sharing energy among residents. As the number of customers employing distributed energy resources (DERs) such as solar rooftops increase, innovation in the double auction market (DA) system is becoming more significant. In this paper, a novel model-based, multi-agent asynchronous advantage actor-centralized-critic with communication (MB-A3C3) approach is carried out. Previous studies are limited since they suffer from unpredictable behavior in renewable energy resources and a large number of prosumers in the peer-to-peer market. As for the model-based strategy, we forecast the trading price and trading quantity in the daily energy trading system in order to overcome unpredictable issues. For the large number of prosumers, the multi-agent and multithreading RL has been chosen as our backbone since the prosumers\u2019 behavior can be diverse; time-series clustering is introduced based on their daily trading behavior. With its environmental model and multi-threaded mechanism, MB-A3C3 is seen to be most efficient in carrying out tasks regards time and precision. The model is conducted on a large scale real-world hourly 2012\u20132013 dataset of 300 households in Sydney having rooftop solar systems installed in New South Wales (NSW), Australia. Results reveal that the MB-A3C3 approach outperforms other reinforcement learning methods (MADDPG and A3C3), producing lower community energy bills for 300 households. When internal trade (trading among houses) increased and external trade (trading to the grid) decreased, our multiple agent RL (MB-A3C3) significantly lowered energy bills by 17%. In closing the gap between the real-world and theoretical problems, the algorithms herein aid in reducing customers\u2019 electricity bills.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31026233",
                        "name": "Manassakan Sanayha"
                    },
                    {
                        "authorId": "1715851",
                        "name": "P. Vateekul"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[15] provided theoretical analysis on how to decide the simulation horizon."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "39ae24156b94a9ef91b4208c709a3c744896f611",
                "externalIds": {
                    "DBLP": "conf/corl/HuW22",
                    "CorpusId": 257432651
                },
                "corpusId": 257432651,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/39ae24156b94a9ef91b4208c709a3c744896f611",
                "title": "Solving Complex Manipulation Tasks with Model-Assisted Model-Free Reinforcement Learning",
                "abstract": "In this paper, we propose a novel deep reinforcement learning approach for improving the sample efficiency of a model-free actor-critic method by using a learned model to encourage exploration. The basic idea consists in generating imaginary transitions with noisy actions, which can be used to update the critic. To counteract the model bias, we introduce a high initialization for the critic and two filters for the imaginary transitions. Finally, we evaluate our approach with the TD3 algorithm on different robotic tasks and demonstrate that it achieves a better performance with higher sample efficiency than several other model-based and model-free methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2211084679",
                        "name": "Jianshu Hu"
                    },
                    {
                        "authorId": "145822500",
                        "name": "P. Weng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "3), an alternative idea could be to learn a model of the environment itself to simulate the realistic feedback during the control-policy learning (Janner et al. 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5ba117627b6ceb2ddf418ed9b300898a1fd6a0aa",
                "externalIds": {
                    "CorpusId": 259282105
                },
                "corpusId": 259282105,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5ba117627b6ceb2ddf418ed9b300898a1fd6a0aa",
                "title": "Constrained Reinforcement Learning for Autonomous Farming: Challenges and Opportunities",
                "abstract": "Reinforcement learning (RL) is a machine learning technique to optimize policies to execute the most desirable sequence of actions in a complex environment. In particular, the optimal set of parameters is automatically learned based on observations while continuously interacting with environments. While this powerful approach has shown great success in various applications (e",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108081014",
                        "name": "Yongshuai Liu"
                    },
                    {
                        "authorId": "3041170",
                        "name": "Taeyeong Choi"
                    },
                    {
                        "authorId": "89121677",
                        "name": "Xin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some model-based RL algorithms use the model just to generate additional data and update the policy using a model-free algorithm (Sutton, 1991; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7fecadaa8deb107fbf20dfd3290d3917cc24891b",
                "externalIds": {
                    "CorpusId": 259305520
                },
                "corpusId": 259305520,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7fecadaa8deb107fbf20dfd3290d3917cc24891b",
                "title": "Lagrangian Model Based Reinforcement Learning",
                "abstract": "We are interested in reinforcement learning (RL) for physical systems. One of the drawbacks of traditional RL algorithms has been their poor sample efficiency. One approach to improve it is model-based RL. In our algorithm, we learn a model of the environment, essentially its transition dynamics and reward function, use it to generate imaginary trajectories and backpropagate through them to update the policy, exploiting the differentiability of the model. Intuitively, learning more accurate models should lead to better performance. Recently, there has been growing interest in developing better deep neural network based dynamics models for physical systems, through better inductive biases. We focus on systems undergoing rigid body motion. We compare two versions of our model-based RL algorithm, one which uses a standard deep neural network based dynamics model and the other which uses a Lagrangian Neural Network based dynamics model, which utilizes the structure of the underlying physics. We find that, in environments that are not sensitive to initial conditions, both versions achieve similar average-return, while the physics-informed version achieves better sample efficiency. Whereas, in environments that are sensitive to initial conditions, the physics-informed version achieves significantly better average-return and sample efficiency. In these latter environments, our physics-informed model-based RL approach achieves better average-return than Soft Actor-Critic, a state-of-the-art model-free RL algorithm.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153519852",
                        "name": "Adithya Ramesh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Techniques from MBRL need to mitigate model error either by learning a global, task-conditioned policy during model learning [45] (which then cannot be adapted to novel tasks on-the-fly), or by re-planning at every timestep to avoid error accumulation [15].",
                "This approach, however, requires models with sufficient long-term stability and strong generalization, which commonly applied blackbox model architectures such as CNNs often lack [45, 69, 72, 4]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "eaf3be557e3e9179ee9fa6b167426ff8f9dc1af1",
                "externalIds": {
                    "DBLP": "conf/nips/AllenLSSBHP22",
                    "CorpusId": 258509535
                },
                "corpusId": 258509535,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/eaf3be557e3e9179ee9fa6b167426ff8f9dc1af1",
                "title": "Inverse Design for Fluid-Structure Interactions using Graph Network Simulators",
                "abstract": "Designing physical artifacts that serve a purpose\u2014such as tools and other functional structures\u2014is central to engineering as well as everyday human behavior. Though automating design using machine learning has tremendous promise, existing methods are often limited by the task-dependent distributions they were exposed to during training. Here we showcase a task-agnostic approach to inverse design, by combining general-purpose graph network simulators with gradient-based design optimization. This constitutes a simple, fast, and reusable approach that solves high-dimensional problems with complex physical dynamics, including designing surfaces and tools to manipulate fluid flows and optimizing the shape of an airfoil to minimize drag. This framework produces high-quality designs by propagating gradients through trajectories of hundreds of steps, even when using models that were pre-trained for single-step predictions on data substantially different from the design tasks. In our fluid manipulation tasks, the resulting designs outperformed those found by sampling-based optimization techniques. In airfoil design, they matched the quality of those obtained with a specialized solver. Our results suggest that despite some remaining challenges, machine learning-based simulators are maturing to the point where they can support general-purpose design optimization across a variety of fluid-structure interaction domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145254624",
                        "name": "Kelsey R. Allen"
                    },
                    {
                        "authorId": "1414739741",
                        "name": "Tatiana Lopez-Guevara"
                    },
                    {
                        "authorId": "2238306103",
                        "name": "Kimberly L. Stachenfeld"
                    },
                    {
                        "authorId": "1398105826",
                        "name": "Alvaro Sanchez-Gonzalez"
                    },
                    {
                        "authorId": "2019153",
                        "name": "P. Battaglia"
                    },
                    {
                        "authorId": "2158860",
                        "name": "Jessica B. Hamrick"
                    },
                    {
                        "authorId": "2054956",
                        "name": "T. Pfaff"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our agent exhibited a different type of \u201ccausal confusion\u201d similar to the model exploitation phenomena in reinforcement learning [8], where the cause of an action is attributed to a model with"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e5c181d1517055f9fbae41c3517cf85a42aca677",
                "externalIds": {
                    "DBLP": "conf/iwai2/WeiGMMESO22",
                    "DOI": "10.1007/978-3-031-28719-0_9",
                    "CorpusId": 258559101
                },
                "corpusId": 258559101,
                "publicationVenue": {
                    "id": "56d76a13-b185-4c2e-a114-f98701176f71",
                    "name": "International Workshop on Affective Interactions",
                    "type": "conference",
                    "alternate_names": [
                        "Int Workshop Affect Interact",
                        "IWAI"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e5c181d1517055f9fbae41c3517cf85a42aca677",
                "title": "World Model Learning from Demonstrations with Active Inference: Application to Driving Behavior",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145064542",
                        "name": "Ran Wei"
                    },
                    {
                        "authorId": "2140833946",
                        "name": "Alfredo Garcia"
                    },
                    {
                        "authorId": "144808726",
                        "name": "Anthony D. McDonald"
                    },
                    {
                        "authorId": "3189420",
                        "name": "G. Markkula"
                    },
                    {
                        "authorId": "26908946",
                        "name": "J. Engstr\u00f6m"
                    },
                    {
                        "authorId": "2057102",
                        "name": "Isaac Supeene"
                    },
                    {
                        "authorId": "1389853689",
                        "name": "Matthew O'Kelly"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DROP MOPO MBPO BC SAC-off BEAR BRAC-v AWR CQL\nrandom hopp.",
                "By comparing to MBPO, the effectiveness of reward penalty can be investigated while the comparison with MOPO helps reveal the potential of density ratio as the reward penalty.",
                "By incorporating the ratio estimation and policy regularization into an effective model-based method MBPO [13], we obtain our algorithm DROP.",
                "For model-based baselines, MBPO [13] and MOPO [42] are taken into comparison.",
                "14: end for\nBy incorporating the ratio estimation and policy regularization into an effective model-based method MBPO [13], we obtain our algorithm DROP.",
                "Following previous works [4, 13], we randomly choose a state from offline data Db and use the current policy \u03c0\u03c6 to perform h-step rollouts on the model ensemble."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "38453770ee3e54550aefcd6c4ccd99448dd21441",
                "externalIds": {
                    "DBLP": "conf/pkdd/ShenCZYZY21",
                    "DOI": "10.1007/978-3-030-86486-6_11",
                    "CorpusId": 236322275
                },
                "corpusId": 236322275,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/38453770ee3e54550aefcd6c4ccd99448dd21441",
                "title": "Model-Based Offline Policy Optimization with Distribution Correcting Regularization",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115732606",
                        "name": "Jian Shen"
                    },
                    {
                        "authorId": "2389257",
                        "name": "Mingcheng Chen"
                    },
                    {
                        "authorId": "2116706397",
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "authorId": "2149232039",
                        "name": "Zhengyu Yang"
                    },
                    {
                        "authorId": "2108309275",
                        "name": "Weinan Zhang"
                    },
                    {
                        "authorId": "2111510213",
                        "name": "Yong Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(5)\nThis bound guarantees the improvement under the true returns as long as the improvement under the model returns increases by more than the slack in the bound due to m, \u03c0 (Janner et al., 2019; Levine et al., 2020).",
                "This bound guarantees the improvement under the true returns as long as the improvement under the model returns increases by more than the slack in the bound due to m, \u03c0 (Janner et al., 2019; Levine et al., 2020).",
                "Following the notation of Janner et al. (2019), we denote the generalization error of a dynamics model on the state distribution under the true behavior policy as m = maxt Es\u223cd\u03c0bt DTV (p(st+1|st, at)||p\u03c6(st+1|st, at)), where DTV represents the total variation distance between true dynamics p and\u2026",
                "\u2026proposed to relieve the issue of model bias, such as the use of multiple dynamics models as an ensemble (Chua et al., 2018; Kurutach\net al., 2018; Janner et al., 2019), meta-learning (Clavera et al., 2018), energy-based regularizer (Boney et al., 2019), game-theoretic framework (Rajeswaran et\u2026",
                "A bound relating the true returns \u03b7[\u03c0] and the model returns \u03b7\u0302[\u03c0] on the target policy is given in Janner et al. (2019) as,\n\u03b7[\u03c0] \u2265 \u03b7\u0302[\u03c0]\u2212 [ 2\u03b3rmax( m + 2 \u03c0)\n(1\u2212 \u03b3)2 + 4rmax \u03c0 (1\u2212 \u03b3)\n] ."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f3617745312c9ec1d3cb4c0a0cd724017485e7bc",
                "externalIds": {
                    "CorpusId": 249569149
                },
                "corpusId": 249569149,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f3617745312c9ec1d3cb4c0a0cd724017485e7bc",
                "title": "ING VIA MODEL-BASED OFFLINE OPTIMIZATION",
                "abstract": "Most reinforcement learning (RL) algorithms assume online access to the environment, in which one may readily interleave updates to the policy with experience collection using that policy. However, in many real-world applications such as health, education, dialogue agents, and robotics, the cost or potential risk of deploying a new data-collection policy is high, to the point that it can become prohibitive to update the data-collection policy more than a few times during learning. With this view, we propose a novel concept of deployment efficiency, measuring the number of distinct data-collection policies that are used during policy learning. We observe that na\u00efvely applying existing model-free offline RL algorithms recursively does not lead to a practical deployment-efficient and sample-efficient algorithm. We propose a novel model-based algorithm, Behavior-Regularized Model-ENsemble (BREMEN), that not only performs better than or comparably as the state-of-the-art dynamic-programming-based and concurrently-proposed model-based offline approaches on existing benchmarks, but can also effectively optimize a policy offline using 10-20 times fewer data than prior works. Furthermore, the recursive application of BREMEN achieves impressive deployment efficiency while maintaining the same or better sample efficiency, learning successful policies from scratch on simulated robotic environments with only 5-10 deployments, compared to typical values of hundreds to millions in standard RL baselines. 1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145930468",
                        "name": "T. Matsushima"
                    },
                    {
                        "authorId": "2052903664",
                        "name": "Hiroki Furuta"
                    },
                    {
                        "authorId": "2153732825",
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "authorId": "7624658",
                        "name": "Ofir Nachum"
                    },
                    {
                        "authorId": "2046135",
                        "name": "S. Gu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As the baseline of our framework is built upon MBPO implementation, we derive the \u201csame hyperparameters\u201d for our experiments and all the baseline algorithms.",
                "Further, the number of interactions with the true environment for outer loop policy were kept constant to 1000 for each epoch, same as MoPAC and MBPO.",
                "Our rewards in Ant-v2 were comparable with MoPAC but still significantly better than MBPO.",
                "This will be compared with the existing approaches MoPAC (Morgan et al., 2021) and MBPO (Janner et al., 2019) on the benchmark MuJoCo control environments.",
                "Several experiments were conducted on the MuJoCo (Todorov et al., 2012) continuous control tasks with the OpenAI-Gym benchmark and the performance was compared with recent related works MoPAC (Morgan et al., 2021) and MBPO (Janner et al., 2019).",
                "Model Based Policy Optimisation (MBPO) (Janner et al., 2019) introduced the outer loop policy to collect transition to train approximate model and sample over it to train the policy.",
                "We consider p\u03c6 as the discounted state-action visitation corresponding to f\u03c6 (similarly p for f ) and superscript h to resemble the notations of (Janner et al., 2019).",
                ", 2021) and MBPO (Janner et al., 2019) on the benchmark MuJoCo control environments.",
                "Now, we will derive the bounds on the performance improvement in a similar way as demonstrated in (Janner et al., 2019) and (Morgan et al., 2021), however with consideration and assumptions related to the convexity of the losses.",
                "Let the total variation distance between them be bounded by f (see (Janner et al., 2019)).",
                "Now, to realize the maximum improvement in the approximated MDP while using the policy parameters (\u03b7\u0303t), obtained from the shift model, we use a formulation motivated by the bound formulated in Lemma B.3 in (Janner et al., 2019).",
                "Then the difference in the returns between the real and the approximated model (Jr and J respectively) is\nJ (xt, \u03b7\u0303t)\u2212 Jr (xt, \u03b7\u0303t)\u22642cmax (H\u22121)\u03b3H+1\u2212H\u03b3H + \u03b3\n(1\u2212 \u03b3)2 f\n+ \u03b3H2 VmaxH f , Rf,H\nusing Lemma B.3 in (Janner et al., 2019).",
                "We would like to emphasize that our final rewards are eventually the same as achieved by MoPAC and MBPO, however the progress rate is faster for all our experiments with lesser true environment interactions.",
                "\u2026ut,h)) c(xt,h, ut,h)\n+ \u03b3H (pH\u03c6 (xt,H , ut,H)\u2212 pH(xt,H , ut,H))V\u03b6(xt,H)\n\u22642 cmax H\u22121\u2211 h=0 \u03b3h h f + \u03b3 H2 VmaxH f\n=2 cmax (H \u2212 1)\u03b3H+1 \u2212H\u03b3H + \u03b3\n(1\u2212 \u03b3)2 f + \u03b3\nH2 VmaxH f\nwhere, |(ph(x, u) \u2212 ph\u03c6(x, u))| \u2264 h f is inherited from Lemma B.2 in (Janner et al., 2019), the uncertainty in dynamics approximation."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "8c41cf8ff2005d99ed9f440548685a1be7c56ca2",
                "externalIds": {
                    "CorpusId": 252377930
                },
                "corpusId": 252377930,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8c41cf8ff2005d99ed9f440548685a1be7c56ca2",
                "title": "D YNAMIC M IRROR D ESCENT BASED M ODEL P REDICTIVE C ONTROL FOR A CCELERATING R OBOT L EARNING",
                "abstract": "(CEM) Our experiments show",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1927638269",
                        "name": "Utkarsh Aashu Mishra"
                    },
                    {
                        "authorId": "2134899829",
                        "name": "Soumya R. Samineni"
                    },
                    {
                        "authorId": "2000932039",
                        "name": "Prakhar Goel"
                    },
                    {
                        "authorId": "2143195117",
                        "name": "Chandravaran Kunjeti"
                    },
                    {
                        "authorId": "92468070",
                        "name": "Himanshu Lodha"
                    },
                    {
                        "authorId": "2151564277",
                        "name": "Aman Singh"
                    },
                    {
                        "authorId": "66660719",
                        "name": "Aditya Sagi"
                    },
                    {
                        "authorId": "2185482270",
                        "name": "Shalabh Bhatnagar"
                    },
                    {
                        "authorId": "2767923",
                        "name": "Shishir Kolathaya"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018) using samples from both datasets D \u222a D\u0302 similar to MBPO (Janner et al., 2019).",
                "Implementation details Following the recent practice (Chua et al., 2018; Janner et al., 2019; Yu et al., 2020), we model the dynamics (T\u0302 , R\u0302) using a bootstrap ensemble of neural networks.",
                "(9) can be optimized using various model-based RL algorithms, e.g. with planning (Chua et al., 2018) or using a model-free learner (Janner et al., 2019).",
                ", 2018) or using a model-free learner (Janner et al., 2019).",
                "We store the generated experiences to a separate dataset D\u0302 and update the policy \u03c0 with IPM-regularized soft actor-critic (SAC) (Haarnoja et al., 2018) using samples from both datasets D \u222a D\u0302 similar to MBPO (Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "61ce91bd0a925bb24c4d236fd14a5f27c3f5b43f",
                "externalIds": {
                    "DBLP": "conf/iclr/00010K21",
                    "CorpusId": 232319243
                },
                "corpusId": 232319243,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/61ce91bd0a925bb24c4d236fd14a5f27c3f5b43f",
                "title": "Representation Balancing Offline Model-based Reinforcement Learning",
                "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2112001925",
                        "name": "Byung-Jun Lee"
                    },
                    {
                        "authorId": "38726140",
                        "name": "Jongmin Lee"
                    },
                    {
                        "authorId": "2110340321",
                        "name": "Kee-Eung Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2021], branched model rollouts [Janner et al., 2019] and exploration strategies [Kidambi et al.",
                "\u2026optimizing for auxiliary objectives [Lee et al., 2020, Nair et al., 2020, Tomar et al., 2021], augmenting modellearning with exploration strategies [Janner et al., 2019, Kidambi et al., 2020], meta-learning to closely intertwine the two objectives [Nagabandi et al., 2018] and introducing\u2026",
                "Further, while we do compare with a competitive MLE-based baseline [Luo et al., 2018], the current state of the art MBRL methods achieve significantly higher performance on the continuous control MuJoCo environments than seen in this paper (e.g. [Janner et al., 2019]).",
                "However, these methods rely on innovations in directions independent of the use of MLE, e.g. using auxiliary objectives [Lee et al., 2020, Nair et al., 2020, Tomar et al., 2021], branched model rollouts [Janner et al., 2019] and exploration strategies [Kidambi et al., 2020]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3325845c5287d09af575b7b2fb6b39c3a9a2b5bb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-14080",
                    "CorpusId": 235658870
                },
                "corpusId": 235658870,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3325845c5287d09af575b7b2fb6b39c3a9a2b5bb",
                "title": "Model-Advantage Optimization for Model-Based Reinforcement Learning",
                "abstract": "Model-based Reinforcement Learning (MBRL) algorithms have been traditionally designed with the goal of learning accurate dynamics of the environment. This introduces a mismatch between the objectives of model-learning and the overall learning problem of finding an optimal policy. Value-aware model learning, an alternative model-learning paradigm to maximum likelihood, proposes to inform model-learning through the value function of the learnt policy. While this paradigm is theoretically sound, it does not scale beyond toy settings. In this work, we propose a novel value-aware objective that is an upper bound on the absolute performance difference of a policy across two models. Further, we propose a general purpose algorithm that modifies the standard MBRL pipeline \u2013 enabling learning with value aware objectives. Our proposed objective, in conjunction with this algorithm, is the first successful instantiation of value-aware MBRL on challenging continuous control environments, outperforming previous value-aware objectives and with competitive performance w.r.t. MLE-based MBRL approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "20809363",
                        "name": "Nirbhay Modhe"
                    },
                    {
                        "authorId": "2074872694",
                        "name": "Harish Kamath"
                    },
                    {
                        "authorId": "1746610",
                        "name": "Dhruv Batra"
                    },
                    {
                        "authorId": "51043791",
                        "name": "A. Kalyan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "During policy optimization in RL, by updating the current policy \u03c0 to a new policy \u03c0\u0303, Schulman et al. (2015) prove that:\n\u03b7(\u03c0\u0303|p) \u2265 L\u03c0(\u03c0\u0303|p)\u2212 2\u03bb\u03b3\n(1\u2212 \u03b3)2 \u03b2 2, (1)\nL\u03c0(\u03c0\u0303|p) = \u03b7(\u03c0|p) + 1 1\u2212 \u03b3Es\u223cP\u03c0,a\u223c\u03c0 [ \u03c0\u0303(a|s) \u03c0(a|s)A\u03c0(s, a) ] ,\nwhere \u03bb = maxs |Ea\u223c\u03c0(a|s)[A\u03c0(s, a)]| is the maximum expected advantage following current policy \u03c0, and \u03b2 = maxs DTV (\u03c0(\u00b7|s)\u2016\u03c0\u0303(\u00b7|s)) is the maximum total variation (TV) distance between \u03c0 and \u03c0\u0303.",
                "In the context of model-based RL, Janner et al. (2019); Luo et al. (2019) formulate the lower bound for a certain policy\u2019s performance on true environment in terms of the performance on the learned model.",
                "For monotonic policy optimization in RL, Schulman et al. (2015) propose to optimize a constrained surrogate objective, which can guarantee the performance improvement of updated policy.",
                "In standard RL, environment parameter p is fixed without any model discrepancy.",
                "In this paper, we focus on the generalization issue in RL, and aim to mitigate the model discrepancy of the transition dynamics between source and target environments."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "fc842c477e38dc645b0b14958756bce713791c20",
                "externalIds": {
                    "DBLP": "conf/icml/JiangLDZX21",
                    "CorpusId": 235826018
                },
                "corpusId": 235826018,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fc842c477e38dc645b0b14958756bce713791c20",
                "title": "Monotonic Robust Policy Optimization with Model Discrepancy",
                "abstract": "State-of-the-art deep reinforcement learning (DRL) algorithms tend to over\ufb01t due to the model discrepancy between source and target environments. Though applying domain randomization during training can improve the average performance by randomly generating a suf\ufb01cient diversity of environments in simulator, the worst-case environment is still neglected without any performance guarantee. Since the average and worst-case performance are both important for generalization in RL, in this paper, we propose a policy optimization approach for concurrently improving the policy\u2019s performance in the average and worst-case environment. We theoretically derive a lower bound for the worst-case performance of a given policy by relating it to the expected performance. Guided by this lower bound, we formulate an optimization problem to jointly optimize the policy and sampling distribution, and prove that by iteratively solving it the worst-case performance is monotonically improved. We then develop a practical algorithm, named monotonic robust policy optimization (MRPO). Experimental evaluations in several robot control tasks demonstrate that MRPO can generally improve both the average and worst-case performance in the source environments for training, and facilitate in all cases the learned policy with a better generalization capability in some unseen testing environments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107328900",
                        "name": "Y. Jiang"
                    },
                    {
                        "authorId": "144535686",
                        "name": "Chenglin Li"
                    },
                    {
                        "authorId": "3207464",
                        "name": "Wenrui Dai"
                    },
                    {
                        "authorId": "38871632",
                        "name": "Junni Zou"
                    },
                    {
                        "authorId": "144045763",
                        "name": "H. Xiong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Figure 1 illustrates the last idea, which has received much attention recently (Chua et al., 2018; Janner et al., 2019).",
                "The follow-up work (Janner et al., 2019) extended PETS with policy learning.",
                "\u2026et al., 2017; Ruiz et al., 2019), iii) randomizing trajectory synthesis (Andrychowicz et al., 2017; Lu et al., 2019), iv) selecting more diverse data contributors (Stasaski et al., 2020), and v) sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",
                "Different from (Janner et al., 2019), our method is designed for dialogue agents\u2019 discrete action space.",
                "In our method, each training trajectory has an overlap much larger than (Janner et al., 2019) has with the expert trajectory.",
                ", 2020), and sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",
                "\u2026parameters (Tobin et al., 2017; Ruiz et al., 2019), randomizing trajectory synthesis (Andrychowicz et al., 2017; Lu et al., 2019), selecting more diverse data contributors (Stasaski et al., 2020), and sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019).",
                ", 2020), and v) sampling trajectories from a diverse set of environments (Chua et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "66cb883bfba8600740a043c8c119ef541ec07b73",
                "externalIds": {
                    "DBLP": "conf/acl/TangKY21",
                    "ACL": "2021.findings-acl.163",
                    "DOI": "10.18653/v1/2021.findings-acl.163",
                    "CorpusId": 236478147
                },
                "corpusId": 236478147,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/66cb883bfba8600740a043c8c119ef541ec07b73",
                "title": "High-Quality Dialogue Diversification by Intermittent Short Extension Ensembles",
                "abstract": "Many task-oriented dialogue systems use deep reinforcement learning (DRL) to learn policies that respond to the user appropriately and complete the tasks successfully. Training DRL agents with diverse dialogue trajectories prepare them well for rare user requests and unseen situations. One effective diversification method is to let the agent interact with a diverse set of learned user models. However, trajectories created by these artificial user models may contain generation errors, which can quickly propagate into the agent\u2019s policy. It is thus important to control the quality of the diversification and resist the noise. In this paper, we propose a novel dialogue diversification method for task-oriented dialogue systems trained in simulators. Our method, Intermittent Short Extension Ensemble (I-SEE),1 constrains the intensity to interact with an ensemble of diverse user models and effectively controls the quality of the diversification. Evaluations on the Multiwoz dataset show that ISEE successfully boosts the performance of several state-of-the-art DRL dialogue agents.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47347567",
                        "name": "Zhiwen Tang"
                    },
                    {
                        "authorId": "50359017",
                        "name": "Hrishikesh Kulkarni"
                    },
                    {
                        "authorId": "2109540870",
                        "name": "Grace Hui Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020) and MBPO (Janner et al., 2019), as well as the standard baselines of SAC (Haarnoja et al.",
                "We compared\nGELATO and its variants to contemporary model-based offline RL approaches; namely, MOPO (Yu et al., 2020) and MBPO (Janner et al., 2019), as well as the standard baselines of SAC (Haarnoja et al., 2018) and imitation (behavioral cloning)."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "35036e009fa42c38741a1155853e1fdb47ce583b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-11327",
                    "CorpusId": 232014494
                },
                "corpusId": 232014494,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/35036e009fa42c38741a1155853e1fdb47ce583b",
                "title": "GELATO: Geometrically Enriched Latent Model for Offline Reinforcement Learning",
                "abstract": "Offline reinforcement learning approaches can generally be divided to proximal and uncertaintyaware methods. In this work, we demonstrate the benefit of combining the two in a latent variational model. We impose a latent representation of states and actions and leverage its intrinsic Riemannian geometry to measure distance of latent samples to the data. Our proposed metrics measure both the quality of out of distribution samples as well as the discrepancy of examples in the data. We integrate our metrics in a model-based offline optimization framework, in which proximity and uncertainty can be carefully controlled. We illustrate the geodesics on a simple grid-like environment, depicting its natural inherent topology. Finally, we analyze our approach and improve upon contemporary offline RL benchmarks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "29978064",
                        "name": "Guy Tennenholtz"
                    },
                    {
                        "authorId": "35712547",
                        "name": "Nir Baram"
                    },
                    {
                        "authorId": "1712535",
                        "name": "Shie Mannor"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f6264b11ec0dd9133f1d88a5288a3267a93182f8",
                "externalIds": {
                    "DBLP": "conf/iclr/AndrychowiczRSO21",
                    "CorpusId": 233340556
                },
                "corpusId": 233340556,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f6264b11ec0dd9133f1d88a5288a3267a93182f8",
                "title": "What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study",
                "abstract": "In recent years, reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous lowand high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [27]. As a step towards filling that gap, we implement >50 such \u201cchoices\u201d in a unified on-policy deep actor-critic framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250\u2019000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for the training of on-policy deep actor-critic RL agents.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150918315",
                        "name": "Anton Raichuk"
                    },
                    {
                        "authorId": "2067024583",
                        "name": "P. Sta\u0144czyk"
                    },
                    {
                        "authorId": "1741487247",
                        "name": "Manu Orsini"
                    },
                    {
                        "authorId": "35022714",
                        "name": "Sertan Girgin"
                    },
                    {
                        "authorId": "52153018",
                        "name": "Rapha\u00ebl Marinier"
                    },
                    {
                        "authorId": "122562941",
                        "name": "L'eonard Hussenot"
                    },
                    {
                        "authorId": "1737555",
                        "name": "M. Geist"
                    },
                    {
                        "authorId": "1721354",
                        "name": "O. Pietquin"
                    },
                    {
                        "authorId": "145605490",
                        "name": "Marcin Michalski"
                    },
                    {
                        "authorId": "1802148",
                        "name": "S. Gelly"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These benchmarks have been extensively studied by previous methods, including on-policy [18] [19], off-policy [20] [2] and model-based [21] [22] ones."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1d8dd65c460d91eff5fbf0b1498e88eb398d583a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-07253",
                    "CorpusId": 234742442
                },
                "corpusId": 234742442,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1d8dd65c460d91eff5fbf0b1498e88eb398d583a",
                "title": "Regret Minimization Experience Replay",
                "abstract": "In reinforcement learning, experience replay stores past samples for further reuse. Prioritized sampling is a promising technique to better utilize these samples. Previous criteria of prioritization include TD error, recentness and corrective feedback, which are mostly heuristically designed. In this work, we start from the regret minimization objective, and obtain an optimal prioritization strategy for Bellman update that can directly maximize the return of the policy. The theory suggests that data with higher hindsight TD error, better on-policiness and more accurate Q value should be assigned with higher weights during sampling. Thus most previous criteria only consider this strategy partially. We not only provide theoretical justifications for previous criteria, but also propose two new methods to compute the prioritization weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT exploits the temporal ordering of states. Both methods outperform previous prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo, Atari and Meta-World.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Zhenghai Xue"
                    },
                    {
                        "authorId": "2108707316",
                        "name": "Xu-Hui Liu"
                    },
                    {
                        "authorId": "1432234123",
                        "name": "Jing-Cheng Pang"
                    },
                    {
                        "authorId": "2119326931",
                        "name": "Shengyi Jiang"
                    },
                    {
                        "authorId": "2152479603",
                        "name": "Feng Xu"
                    },
                    {
                        "authorId": "2152847241",
                        "name": "Yang Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since almost all of the existing learning-based methods apply imitation learning to learn the simulator where GAIL is utilized in [36,25,24,31] and BC is adopted in most MBRL methods [1,6], we take GAIL and BC as baselines.",
                "2) BC-based methods have good performance because of the alleviation of the compounding error due to short rollout length [6] while utilizing p\u03c6 to collect data."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d4820ce9898c4983e380cb493c9e80c98c310525",
                "externalIds": {
                    "DBLP": "conf/pkdd/ZhangYSLHZTL21",
                    "DOI": "10.1007/978-3-030-86486-6_7",
                    "CorpusId": 236213962
                },
                "corpusId": 236213962,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d4820ce9898c4983e380cb493c9e80c98c310525",
                "title": "Learning to Build High-Fidelity and Robust Environment Models",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108309275",
                        "name": "Weinan Zhang"
                    },
                    {
                        "authorId": "2149232039",
                        "name": "Zhengyu Yang"
                    },
                    {
                        "authorId": "2115732606",
                        "name": "Jian Shen"
                    },
                    {
                        "authorId": "2112276540",
                        "name": "Minghuan Liu"
                    },
                    {
                        "authorId": "49866668",
                        "name": "Yimin Huang"
                    },
                    {
                        "authorId": "102356716",
                        "name": "Xingzhi Zhang"
                    },
                    {
                        "authorId": "2824766",
                        "name": "Ruiming Tang"
                    },
                    {
                        "authorId": "7718952",
                        "name": "Zhenguo Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The second type of method begins from the perspective of learning policies in the environment, makes the most efficient use of the dynamic models, represented by model-based policy planning (POPLIN) [15], stochastic lower bound optimization (SLBO) [16], and model-based policy optimization (MBPO) [17], and draws upon theories to determine how to correctly make choices in the interaction between the real environment and the model."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8dfb33f467c861e1d63f1fcd740cd942b248ba2d",
                "externalIds": {
                    "DBLP": "journals/wicomm/ZhangW21b",
                    "DOI": "10.1155/2021/4238125",
                    "CorpusId": 237100575
                },
                "corpusId": 237100575,
                "publicationVenue": {
                    "id": "501c1070-b5d2-4ff0-ad6f-8769a0a1e13f",
                    "name": "Wireless Communications and Mobile Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Wirel Commun Mob Comput"
                    ],
                    "issn": "1530-8669",
                    "url": "https://www.hindawi.com/journals/wcmc/",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/15308677",
                        "http://www.interscience.wiley.com/jpages/1530-8669/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8dfb33f467c861e1d63f1fcd740cd942b248ba2d",
                "title": "A Method of Offline Reinforcement Learning Virtual Reality Satellite Attitude Control Based on Generative Adversarial Network",
                "abstract": "Virtual reality satellites give people an immersive experience of exploring space. The intelligent attitude control method using reinforcement learning to achieve multiaxis synchronous control is one of the important tasks of virtual reality satellites. In real-world systems, methods based on reinforcement learning face safety issues during exploration, unknown actuator delays, and noise in the raw sensor data. To improve the sample efficiency and avoid safety issues during exploration, this paper proposes a new offline reinforcement learning method to make full use of samples. This method learns a policy set with imitation learning and a policy selector using a generative adversarial network (GAN). The performance of the proposed method was verified in a real-world system (reaction-wheel-based inverted pendulum). The results showed that the agent trained with our method reached and maintained a stable goal state in 10,000 steps, whereas the behavior cloning method only remained stable for 500 steps.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2151809806",
                        "name": "Jian Zhang"
                    },
                    {
                        "authorId": "3066431",
                        "name": "Fengge Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c63ebb1d4f2df563daba43ac7ad500b4c6977268",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-05549",
                    "CorpusId": 237491907
                },
                "corpusId": 237491907,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c63ebb1d4f2df563daba43ac7ad500b4c6977268",
                "title": "Federated Ensemble Model-based Reinforcement Learning",
                "abstract": "\u2014Federated learning (FL) is a privacy-preserving machine learning paradigm that enables collaborative training among geographically distributed and heterogeneous users with- out gathering their data. Extending FL beyond the conventional supervised learning paradigm, federated Reinforcement Learning (RL) was proposed to handle sequential decision-making problems for various privacy-sensitive applications such as autonomous driving. However, the existing federated RL algorithms directly combine model-free RL with FL, and thus generally have high sample complexity and lack theoretical guarantees. To address the above challenges, we propose a new federated RL algorithm that incorporates model-based RL and ensemble knowledge distillation into FL. Speci\ufb01cally, we utilise FL and knowledge distillation to create an ensemble of dynamics models from clients, and then train the policy by solely using the ensemble model without interacting with the real environment. Furthermore, we theoretically prove that the monotonic improvement of the proposed algorithm is guaranteed. Extensive experimental results demonstrate that our algorithm obtains signi\ufb01cantly higher sample ef\ufb01ciency compared to federated model-free RL algorithms in the challenging continuous control benchmark environments. The results also show the impact of non-IID client data and local update steps on the performance of federated RL, validating the insights obtained from our theoretical analysis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143718761",
                        "name": "Jin Wang"
                    },
                    {
                        "authorId": "31703425",
                        "name": "Jia Hu"
                    },
                    {
                        "authorId": "1581440302",
                        "name": "Jed Mills"
                    },
                    {
                        "authorId": "145896559",
                        "name": "G. Min"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thus, another important direction for future work would be to reduce sample complexity so as to increase the feasibility of real robot training, perhaps achievable via a model-based reinforcement learning approach [10, 11]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "30b1e0a54817d77251e234270f6b45c700e46d43",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-15233",
                    "CorpusId": 238227175
                },
                "corpusId": 238227175,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/30b1e0a54817d77251e234270f6b45c700e46d43",
                "title": "Real Robot Challenge using Deep Reinforcement Learning",
                "abstract": "This paper details our winning submission to Phase 1 of the 2021 Real Robot Challenge, a challenge in which a three fingered robot must carry a cube along specified goal trajectories. To solve Phase 1, we use a pure reinforcement learning approach which requires minimal expert knowledge of the robotic system or of robotic grasping in general. A sparse goal-based reward is employed in conjunction with Hindsight Experience Replay to teach the control policy to move the cube to the desired x and y coordinates. Simultaneously, a dense distance-based reward is employed to teach the policy to lift the cube to the desired z coordinate. The policy is trained in simulation with domain randomisation before being transferred to the real robot for evaluation. Although performance tends to worsen after this transfer, our best trained policy can successfully lift the real cube along goal trajectories via the use of an effective pinching grasp. Our approach outperforms all other submissions, including those leveraging more traditional robotic control techniques, and is the first learning-based approach to solve this challenge. (a) Pushing (b) Cradling (c) Pinching Figure 1: The various manipulation strategies learned by our approach. 1 Real Robot Challenge Phase 1 Dexterous robotic manipulation is applicable in various industrial and domestic settings. However, current state-of-the-art control strategies generally struggle when faced with unstructured tasks which require high degrees of dexterity. Data-driven learning methods are promising for these challenging manipulation tasks, yet related research has been limited by the costly nature of real robot experimentation. In light of these issues, the Real Robot Challenge (RRC) aims to advance the state-of-the-art in robotic manipulation by providing participants with remote access to well maintained robotic platforms, allowing for cheap and easy real robot experimentation. To further support easy experimentation, users are also provided a simulated recreation of the robotic setup. After an initial RRC qualifying phase, successful participants are entered into Phase 1 where they must solve the challenging \u2018Move Cube on Trajectory\u2019 task. In this task, a cube must be carried along a goal trajectory, which specifies the coordinates at which the cube should be positioned at each time-step, using the provided TriFinger robotic platform [3]. For final Phase 1 evaluation, participants submit their developed control policy and receive a score based on how closely it can bring the cube to a number of randomly sampled goal trajectories. \u2217University College Dublin, \u2020Dublin City University, \u2021 SFI Insight Centre for Data Analytics, Ireland 1Videos of our policies in action can be found at https://www.youtube.com/playlist?list= PLLJoWXUn8XplFszi16-VZMTDBhMQFuc5o. 1 ar X iv :2 10 9. 15 23 3v 1 [ cs .R O ] 3 0 Se p 20 21 \u2018Move Cube on Trajectory\u2019 requires a dexterous policy that can adapt to the various different cube and goal positions that may encountered during a single evaluation run. In the 2020 Real Robot Challenge, solutions to this task consisted of structured policies which relied heavily on inductive biases and task specific engineering [5, 6]. We take an alternative approach by formulating the task as a pure reinforcement learning (RL) problem, using RL to learn our control policy entirely in simulation before transferring it to the real robot for final evaluation. Upon this evaluation, our learned policy outperformed all other competing submissions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Robert McCarthy"
                    },
                    {
                        "authorId": "2065325865",
                        "name": "Francisco Rold\u00e1n S\u00e1nchez"
                    },
                    {
                        "authorId": "145470864",
                        "name": "Kevin McGuinness"
                    },
                    {
                        "authorId": "98536322",
                        "name": "N. O\u2019Connor"
                    },
                    {
                        "authorId": "144869332",
                        "name": "S. Redmond"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "8b3fb7ec4d54dd1dd8e775bf98491bb3e1f71eab",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-05415",
                    "CorpusId": 238582980
                },
                "corpusId": 238582980,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8b3fb7ec4d54dd1dd8e775bf98491bb3e1f71eab",
                "title": "Safe Model-Based Reinforcement Learning Using Robust Control Barrier Functions",
                "abstract": "Reinforcement Learning (RL) is effective in many scenarios. However, it typically requires the exploration of a sufficiently large number of state-action pairs, some of which may be unsafe. Consequently, its application to safety-critical systems remains a challenge. Towards this end, an increasingly common approach to address safety involves the addition of a safety layer that projects the RL actions onto a safe set of actions. In turn, a challenge for such frameworks is how to effectively couple RL with the safety layer to improve the learning performance. In the context of leveraging control barrier functions for safe RL training, prior work focuses on a restricted class of barrier functions and utilizes an auxiliary neural net to account for the effects of the safety layer which inherently results in an approximation. In this paper, we frame safety as a differentiable robust-control-barrier-function layer in a model-based RL framework. As such, this approach both ensures safety and effectively guides exploration during training resulting in increased sample efficiency as demonstrated in the experiments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153025282",
                        "name": "Y. Emam"
                    },
                    {
                        "authorId": "2226508",
                        "name": "Paul Glotfelter"
                    },
                    {
                        "authorId": "145276578",
                        "name": "Z. Kira"
                    },
                    {
                        "authorId": "152599370",
                        "name": "M. Egerstedt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Also, in the context of accelerating and generalizing skill-learning, the combination of learning and planning has received increasing attention [20, 37, 40, 41]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5183d9d4d004f121007e1ce6c6d66a8e4e010d30",
                "externalIds": {
                    "DBLP": "conf/corl/FunkCB021",
                    "CorpusId": 243798795
                },
                "corpusId": 243798795,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5183d9d4d004f121007e1ce6c6d66a8e4e010d30",
                "title": "Learn2Assemble with Structured Representations and Search for Robotic Architectural Construction",
                "abstract": ": Autonomous robotic assembly requires a well-orchestrated sequence of high-level actions and smooth manipulation executions. Learning to assemble complex 3D structures remains a challenging problem that requires drawing connections between target designs and building blocks, and creating valid assembly sequences considering structural stability and feasibility. To address the combinatorial complexity of the assembly tasks, we propose a multi-head attention graph representation that can be trained with reinforcement learning (RL) to encode the spatial relations and provide meaningful assembly actions. Combining structured representations with model-free RL and Monte-Carlo planning allows agents to operate with various target shapes and building block types. We design a hierarchical control framework that learns to sequence the building blocks to construct arbitrary 3D designs and ensures their feasibility, as we plan the geometric execution with the robot-in-the-loop. We demonstrate the \ufb02exibility of the proposed structured representation and our algorithmic solution in a series of simulated 3D assembly tasks with robotic evaluation, which showcases our method\u2019s ability to learn to construct stable structures with a large number of building blocks. Code and videos are available at: https://sites.google.com/view/learn2assemble",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34897453",
                        "name": "Niklas Funk"
                    },
                    {
                        "authorId": "1989757",
                        "name": "G. Chalvatzaki"
                    },
                    {
                        "authorId": "29505409",
                        "name": "B. Belousov"
                    },
                    {
                        "authorId": "144668016",
                        "name": "Jan Peters"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2d3eb957615329c45927be5ee30afd68e7da4c73",
                "externalIds": {
                    "CorpusId": 244037287
                },
                "corpusId": 244037287,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2d3eb957615329c45927be5ee30afd68e7da4c73",
                "title": "Robust Reinforcement Learning for Shifting Dynamics During Deployment",
                "abstract": "While high-return policies can be learned on a wide range of systems through reinforcement learning, actual deployment of the resulting policies is often hindered by their sensitivity to future changes in the environment. Adversarial training has shown some promise in producing policies that retain better performance under environment shifts, but existing approaches only consider robustness to specific kinds of perturbations that must be specified a priori. As possible changes in future dynamics are typically unknown in practice, we instead seek a policy that is robust to a variety of realistic changes only encountered at test-time. Towards this goal, we propose a new adversarial variant of soft actor-critic, which produces policies on Mujoco continuous control tasks that are simultaneously more robust across various environment shifts, such as changes to friction and body mass.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2067201658",
                        "name": "S. Stanton"
                    }
                ]
            }
        },
        {
            "contexts": [
                "35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\ndata (e.g. ME-TRPO [6], MBPO [7]) to improve policy optimization.",
                "ME-TRPO [6], MBPO [7]) to improve policy optimization."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "824463653eb758014679de144e7da304150df4cc",
                "externalIds": {
                    "CorpusId": 244895221
                },
                "corpusId": 244895221,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/824463653eb758014679de144e7da304150df4cc",
                "title": "Model-Based Reinforcement Learning via Imagination with Derived Memory",
                "abstract": "Model-based reinforcement learning aims to improve the sample efficiency of policy learning by modelling the dynamics of the environment. Recently, the latent dynamics model has been further developed to enable fast planning in a compact space. It summarizes the high-dimensional experiences of an agent, which mimics the memory function of humans. Learning policies via imagination with the latent model shows great potential for solving complex tasks. However, only considering memories from the true experiences in the process of imagination could limit its advantages. Inspired by the memory prosthesis proposed by neuroscientists, we present a novel model-based reinforcement learning framework called Imagining with Derived Memory (IDM). It enables the agent to learn policy from enriched diverse imagination with prediction-reliability weight, thus improving sample efficiency and policy robustness. Experiments on various high-dimensional visual control tasks in the DMControl benchmark demonstrate that IDM outperforms previous state-of-the-art methods in terms of policy robustness and further improves the sample efficiency of the model-based method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1675357512",
                        "name": "Yao Mu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "31d51341f40a7a3181d39e51e967c35ff1a40a61",
                "externalIds": {
                    "DBLP": "conf/nips/HishinumaS21",
                    "CorpusId": 245011233
                },
                "corpusId": 245011233,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/31d51341f40a7a3181d39e51e967c35ff1a40a61",
                "title": "Weighted model estimation for offline model-based reinforcement learning",
                "abstract": "This paper discusses model estimation in offline model-based reinforcement learning (MBRL), which is important for subsequent policy improvement using an estimated model. From the viewpoint of covariate shift, a natural idea is model estimation weighted by the ratio of the state-action distributions of offline data and real future data. However, estimating such a natural weight is one of the main challenges for off-policy evaluation, which is not easy to use. As an artificial alternative, this paper considers weighting with the state-action distribution ratio of offline data and simulated future data, which can be estimated relatively easily by standard density ratio estimation techniques for supervised learning. Based on the artificial weight, this paper defines a loss function for offline MBRL and presents an algorithm to optimize it. Weighting with the artificial weight is justified as evaluating an upper bound of the policy evaluation error. Numerical experiments demonstrate the effectiveness of weighting with the artificial weight.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2393431",
                        "name": "T. Hishinuma"
                    },
                    {
                        "authorId": "3188792",
                        "name": "K. Senda"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model-based reinforcement learning is one way to do this, and provides a framework for learning a policy from just a reward signal by optimizing for the control policy that takes actions which maximize the expected discounted return [Janner et al. 2019]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "aaac91a5ebef1ec06f15434fa1a3e15c4ed82d12",
                "externalIds": {
                    "CorpusId": 245016840
                },
                "corpusId": 245016840,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/aaac91a5ebef1ec06f15434fa1a3e15c4ed82d12",
                "title": "SuperTrack: Motion Tracking for Physically Simulated Characters using Supervised Learning",
                "abstract": "In this paper we show how the task of motion tracking for physically simulated characters can be solved using supervised learning and optimizing a policy directly via back-propagation. To achieve this we make use of a world model trained to approximate a specific subset of the environment\u2019s transition function, effectively acting as a differentiable physics simulator through which the policy can be optimized to minimize the tracking error. Compared to popular model-free methods of physically simulated character control which primarily make use of Proximal Policy Optimization (PPO) we find direct optimization of the policy via our approach consistently achieves a higher quality of control in a shorter training time, with a reduced sensitivity to the rate of experience gathering, dataset size, and distribution.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51903917",
                        "name": "Levi Fussell"
                    },
                    {
                        "authorId": "1405689021",
                        "name": "Kevin Bergamin"
                    },
                    {
                        "authorId": "2144287755",
                        "name": "Ubisoft La Forge"
                    },
                    {
                        "authorId": "1484131432",
                        "name": "Daniel Holden"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, model-based RL is an active area of research, and recently developed model-based approaches have been successful on many continuous control benchmarks (Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "dbbf0fb940b3edc3c5ea62fa97cea3a966ef4dce",
                "externalIds": {
                    "CorpusId": 245139788
                },
                "corpusId": 245139788,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dbbf0fb940b3edc3c5ea62fa97cea3a966ef4dce",
                "title": "OFF-POLICY REINFORCEMENT LEARNING FOR BIPEDAL ROBOT LOCOMOTION",
                "abstract": null,
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "68624326620bb2c028f162074070229791d98d14",
                "externalIds": {
                    "CorpusId": 247863454
                },
                "corpusId": 247863454,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/68624326620bb2c028f162074070229791d98d14",
                "title": "Importance of Representation Learning for Off-Policy Fitted Q-Evaluation",
                "abstract": "The goal of of\ufb02ine policy evaluation (OPE) is to evaluate target policies based on logged data with a possibly much different distribution. One of the most popular empirical approaches to OPE is \ufb01tted Q-evaluation (FQE). With linear function approximation, several works have found that FQE (and other OPE methods) exhibit exponential error ampli\ufb01cation in the problem horizon, except under very strong assumptions. Given the empirical success of deep FQE, in this work we examine the effect of implicit regularization through deep architectures and loss functions on the divergence and performance of FQE. We \ufb01nd that divergence does occur with simple feed-forward architectures, but can be mitigated using various architectures and algorithmic techniques, such as ResNet architectures, learning a shared representation between multiple target policies, and hypermodels. Our results suggest interesting directions for future work, including analyzing the effect of architecture on stability of \ufb01xed-point updates which are ubiquitous in modern reinforcement learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108403552",
                        "name": "Xian Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ac5e418df11919e118a4006b092bfbbbf87b9b42",
                "externalIds": {
                    "CorpusId": 247974193
                },
                "corpusId": 247974193,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ac5e418df11919e118a4006b092bfbbbf87b9b42",
                "title": "Task-Independent Causal State Abstraction",
                "abstract": "Learning dynamics models accurately and learning policies sample-ef\ufb01ciently are two important challenges for Model-Based Reinforcement Learning ( MBRL ). Regarding dynamics accuracy, in contrast to the sparse dynamics exhibited in many real world environments, most MBRL methods learn a dense dynamics model which is vulnerable to spurious correlations and therefore generalizes poorly to unseen states. Meanwhile, existing state abstractions can improve sample ef\ufb01ciency, but their dependence on speci\ufb01c reward functions constrains their applications to limited tasks. In this paper, we introduce a novel state abstraction called Task-Independent Causal State Abstraction ( TICSA ). Exploiting sparsity exhibited in the real world, the proposed method \ufb01rst learns a causal dynamics model that generalizes to unexplored states. A state abstraction can then be derived from the learned dynamics, which not only improves sample ef\ufb01ciency but also applies to many tasks. Using a simulated manipulation environment and two different tasks, we observe that both the dynamics model and policies learned by the proposed method generalize well to unseen states and that TICSA also improves sample ef\ufb01ciency compared to learning without state abstraction.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2881858",
                        "name": "Zizhao Wang"
                    },
                    {
                        "authorId": "2118724825",
                        "name": "Xuesu Xiao"
                    },
                    {
                        "authorId": "2117748",
                        "name": "Yuke Zhu"
                    },
                    {
                        "authorId": "144848112",
                        "name": "P. Stone"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Alternatively, for model-based RL (e.g., (Janner et al. 2019)), the pipeline is also a linear structure with an additional module which corresponds to the dynamics model (see Figures 2(b) and 2(d), respectively).",
                ", (Janner et al. 2019)), the pipeline is also a linear structure with an additional module which corresponds to the dynamics model (see Figures 2(b) and 2(d), respectively)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "dc457bd09b5425e576edacd5f0bebfe6c7420df2",
                "externalIds": {
                    "CorpusId": 252285658
                },
                "corpusId": 252285658,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dc457bd09b5425e576edacd5f0bebfe6c7420df2",
                "title": "Automated Decision Optimization with Reinforcement Learning",
                "abstract": "Sequential decision making under uncertainty has many prac- tical real-world applications, ranging from supply-chain management to autonomous driving. Reinforcement learning has recently emerged as a promising technique for solving these kinds of problems. However, reinforcement learning algo- rithms are known to be highly sensitive to their internal hyperparameters which require significant expert manual effort for tuning their values. This in turn limits the widespread ap-plicability of reinforcement learning based solutions in prac- tice. In this paper, we introduce a new automated decision optimization system called AutoDO for end-to-end solving of sequential decision making problems. More specifically, the system not only automatically selects the best reinforcement learning algorithm but it also finds the best configuration of its hyperparameters using a search strategy based on limited discrepancy search coupled with Bayesian optimization. Fur-thermore, our system supports several different flavours of reinforcement learning including online and offline as well as model-free and model-based algorithms. We experiment with classical control benchmarks as well as a more realistic inventory management problem. Our results show that the pro- posed system is competitive and often outperforms existing state-of-the-art in terms of the quality of the solutions found.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2808565",
                        "name": "Radu Marinescu"
                    },
                    {
                        "authorId": "22218771",
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "authorId": "2059482494",
                        "name": "Long Vu"
                    },
                    {
                        "authorId": "2625510",
                        "name": "Paulito Palmes"
                    },
                    {
                        "authorId": "1866078",
                        "name": "Todd W. Mummert"
                    },
                    {
                        "authorId": "2075248346",
                        "name": "Peter Kirchner"
                    },
                    {
                        "authorId": "1762215",
                        "name": "D. Subramanian"
                    },
                    {
                        "authorId": "2944292",
                        "name": "P. Ram"
                    },
                    {
                        "authorId": "1744675",
                        "name": "Djallel Bouneffouf"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020) and MBPO (Janner et al., 2019), as well as the standard baselines of SAC (Haarnoja et al.",
                "We compared GELATO and its variants to contemporary model-based offline RL approaches; namely, MOPO (Yu et al., 2020) and MBPO (Janner et al., 2019), as well as the standard baselines of SAC (Haarnoja et al., 2018) and imitation (behavioral cloning)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c37190f3e03017e5577f68cf4a43f72b3dbb1245",
                "externalIds": {
                    "CorpusId": 252376604
                },
                "corpusId": 252376604,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c37190f3e03017e5577f68cf4a43f72b3dbb1245",
                "title": "Latent Geodesics of Model Dynamics for Of\ufb02ine Reinforcement Learning",
                "abstract": "Model-based of\ufb02ine reinforcement learning approaches generally rely on bounds of model error. While contemporary methods achieve such bounds through an ensemble of models, we propose to estimate them using a data-driven latent metric. Particularly, we build upon recent advances in Riemannian geometry of generative models to construct a latent metric of an encoder-decoder based forward model. Our proposed metric measures both the quality of out of distribution samples as well as the discrepancy of examples in the data. We show that our metric can be viewed as a combination of two metrics, one relating to proximity and the other to epistemic uncertainty. Finally, we leverage our metric in a pessimistic model-based framework, showing a signi\ufb01cant improvement upon contemporary model-based of\ufb02ine reinforcement learning benchmarks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "29978064",
                        "name": "Guy Tennenholtz"
                    },
                    {
                        "authorId": "35712547",
                        "name": "Nir Baram"
                    },
                    {
                        "authorId": "1712535",
                        "name": "Shie Mannor"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9d325790fc741a85b79a640365d2f4199cffab9a",
                "externalIds": {
                    "CorpusId": 252378008
                },
                "corpusId": 252378008,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9d325790fc741a85b79a640365d2f4199cffab9a",
                "title": "Discrete Uncertainty Quanti\ufb01cation Approach for Of\ufb02ine RL",
                "abstract": "In many Reinforcement Learning tasks, the classical online interaction of the learning agent with the environment is impractical, either because such interaction is expensive or dangerous. In these cases, previous gathered data can be used, arising what is typically called Of\ufb02ine Reinforcement Learning. However, this type of learning faces a large number of challenges, mostly derived from the fact that exploration/exploitation trade-off is overshadowed. Instead, the historical data is usually biased by the way it was obtained, typically, a sub-optimal controller, producing a distributional shift from historical data and the one required to learn the optimal policy. Speci\ufb01cally, in this paper we present a new approach to deal with the uncertainty risen by the absence or sparse presence of some states in the data. Our approach is based on shaping the reward signal of the environment to ensure the task is solved. We present the approach and show that combining it with classic online RL methods make them perform as good as state of the art of\ufb02ine RL algorithms such as CQL and BCQ. Finally, we show that using our method on top of established of\ufb02ine learning algorithms can improve them signi\ufb01cantly.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "102966487",
                        "name": "J. Corrochano"
                    },
                    {
                        "authorId": "2110194270",
                        "name": "Javier Garc\u00eda"
                    },
                    {
                        "authorId": "121424788",
                        "name": "Rub\u00e9n Majadas"
                    },
                    {
                        "authorId": "1402943547",
                        "name": "C. Ib\u00e1\u00f1ez-Llano"
                    },
                    {
                        "authorId": "2185507147",
                        "name": "Sergio P\u00e9rez"
                    },
                    {
                        "authorId": "143901279",
                        "name": "F. Fern\u00e1ndez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Prior works 65 have shown remarkable performance to train deep visuomotor skills for a single task in low-data 66 regimes with imitation learning [11, 12, 13, 14, 15, 16], reinforcement learning [17, 18], and model 67 predictive control [19, 20]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "660a733aef2e7bc9be91925e1292416a5353a97a",
                "externalIds": {
                    "CorpusId": 260430409
                },
                "corpusId": 260430409,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/660a733aef2e7bc9be91925e1292416a5353a97a",
                "title": "Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets",
                "abstract": "Robot learning holds the promise of learning flexible policies that gen1 eralize across a wide range of scenarios. However, such generalization requires 2 sufficiently broad datasets of the particular task of interest, which is often pro3 hibitively expensive to collect. On the other hand, there often exist large amounts 4 of past data from related tasks collected in other contexts. In this work, we study 5 how we can leverage such past data to broaden robot generalization in a number 6 of settings. This includes broadening the generalization of a new target task for 7 which we only have narrow data, and generalizing a task to a new target domain 8 from which we have little or no data. To this end, we collect a diverse \u2018bridge\u2019 9 dataset of 4,700 human demonstrations of a robot performing 33 common kitchen 10 tasks across 3 toy kitchens and 3 toy sinks with varying lighting, robot positions, 11 and backgrounds. We train policies via task-conditioned behavior cloning directly 12 from RGB images on this data mixed with narrow data from the target task or do14 13 main. Our experiments find that leveraging the diverse bridge dataset often leads 14 to a 2x improvement in success rate when evaluated on challenging scenarios that 15 demand generalization, even if the user\u2019s domain is substantially different, both 16 visually and functionally, from the bridge data. These results suggest that reusing 17 diverse offline bridge datasets, including our open-source dataset, may pave the 18 way for broader robot generalization moving forward without requiring extensive 19 datasets to be re-collected for each new project. 20",
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [
                "Our approach is similar to MBPO [23] with 1-step rollouts, except we don\u2019t store virtual experiences in the replay buffer and instead sample fresh ones in every critic update.",
                "MBPO extends the Soft Actor-Critic (SAC) algorithm [19] (an off-policy, model-free AC method) by generating short model-based rollouts branched from real experiences that are then mixed together to augment the experience replay buffer.",
                "Recently, there has been a growing interest in extending AC methods by progressively learning and using a model of the environment dynamics [7, 8, 10, 15, 23].",
                "MBPO is successful in increasing the sample-efficiency of SAC, but learns its model via maximum likelihood, ignoring performance on the end task.",
                "One of the most successful instantiations of this paradigm in deep RL is the Model-Based Policy Optimization (MBPO) algorithm [23]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3ca1b6658ab1b45f47d8587b333db64ccfb0efb8",
                "externalIds": {
                    "DBLP": "conf/icbinb/LovattoBMB20",
                    "CorpusId": 232274062
                },
                "corpusId": 232274062,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3ca1b6658ab1b45f47d8587b333db64ccfb0efb8",
                "title": "Decision-Aware Model Learning for Actor-Critic Methods: When Theory Does Not Meet Practice",
                "abstract": "Actor-Critic methods are a prominent class of modern reinforcement learning algorithms based on the classic Policy Iteration procedure. Despite many successful cases, Actor-Critic methods tend to require a gigantic number of experiences and can be very unstable. Recent approaches have advocated learning and using a world model to improve sample ef\ufb01ciency and reduce reliance on the value function estimate. However, learning an accurate dynamics model of the world remains challenging, often requiring computationally costly and data-hungry models. More recent work has shown that learning an everywhere accurate model is unnecessary and often detrimental to the overall task; instead, the agent should improve the world model on task-critical regions. For example, in Iterative Value-Aware Model Learning, the authors extend model-based value iteration by incorporating the value function (estimate) into the model loss function, showing the novel model objective re\ufb02ects improved performance in the end task. Therefore, it seems natural to expect that model-based Actor-Critic methods can bene\ufb01t equally from learning value-aware models, improving overall task performance, or reducing the need for large, expensive models. However, we show empirically that combining Actor-Critic and value-aware model learning can be quite dif\ufb01cult and that naive approaches such as maximum likelihood estimation often achieve superior performance with less computational cost. Our results suggest that, despite theoretical guarantees, learning a value-aware model in continuous domains does not ensure better performance on the overall task.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1452337990",
                        "name": "\u00c2. G. Lovatto"
                    },
                    {
                        "authorId": "8858887",
                        "name": "T. P. Bueno"
                    },
                    {
                        "authorId": "3358691",
                        "name": "D. Mau\u00e1"
                    },
                    {
                        "authorId": "143844870",
                        "name": "L. N. Barros"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use a probabilistic neural network (Nix and Weigend, 1994), which has been shown to be effective as an ensemble in model-based RL (Chua et al., 2018; Janner et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4c8eaa1d1df4344e0b9dd9cc7af1e5ea633fdb4e",
                "externalIds": {
                    "CorpusId": 229330926
                },
                "corpusId": 229330926,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4c8eaa1d1df4344e0b9dd9cc7af1e5ea633fdb4e",
                "title": "UNCLEAR: A Straightforward Method for Continual Reinforcement Learning",
                "abstract": "We present UNCLEAR, a new approach for continual reinforcement learning in the face of tasks which catastrophically interfere. At the heart of UNCLEAR is a value function, which is parameterized by a neural network feature extractor shared across all tasks, and a set of linear heads, each specializing on a single task. We are then able to train one policy with respect to each head. Forgetting is prevented using simple regularization of the feature extraction layers of the policy and Q-functions. Drawing inspiration from online learning, we introduce a novel means to select policies at test time, allowing us to automatically select the right policy for an unknown task. We show in a simple experiment that our approach is able to achieve close to optimal performance when training sequentially on orthogonal tasks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2066244799",
                        "name": "Samuel Kessler"
                    },
                    {
                        "authorId": "1410302742",
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "authorId": "2053179501",
                        "name": "Philip J. Ball"
                    },
                    {
                        "authorId": "8106073",
                        "name": "S. Zohren"
                    },
                    {
                        "authorId": "143841496",
                        "name": "Stephen J. Roberts"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Janner et al.\n(2019); Buckman et al. (2018) uses ensembles to reduce model bias.",
                "Janner et al. (2019), proposed short model-generated branched rollouts starting from data collected on \u201creal environment\u201d."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1c2798c03a118a0624262581a5638e6c57eaf452",
                "externalIds": {
                    "CorpusId": 237249648
                },
                "corpusId": 237249648,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1c2798c03a118a0624262581a5638e6c57eaf452",
                "title": "Trust, but verify: model-based exploration in sparse reward environments",
                "abstract": "We propose trust-but-verify (TBV) mechanism, a new method which uses model uncertainty estimates to guide exploration. The mechanism augments graph search planning algorithms with the capacity to deal with learned model\u2019s imperfections. We identify certain type of frequent model errors, which we dub false loops, and which are particularly dangerous for graph search algorithms in discrete environments. These errors impose falsely pessimistic expectations and thus hinder exploration. We confirm this experimentally and show that TBV can effectively alleviate them. TBV combined with MCTS or Best First Search forms an effective model-based reinforcement learning solution, which is able to robustly solve sparse reward problems.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "30770224",
                        "name": "K. Czechowski"
                    },
                    {
                        "authorId": "2098510471",
                        "name": "Tomasz Odrzyg\u00f3\u017ad\u017a"
                    },
                    {
                        "authorId": "2123967118",
                        "name": "Micha\u0142 Izworski"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Research in deep reinforcement learning has observed that model-based algorithms have superior empirical sample-efficiency over model-free approaches [24] and thus has been preferred for applications where data collection is expensive or impossible such as offline reinforcement learning [25, 26]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1558690d952997d0b83834e04fa382545069ed15",
                "externalIds": {
                    "CorpusId": 243831565
                },
                "corpusId": 243831565,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1558690d952997d0b83834e04fa382545069ed15",
                "title": "Temporal Difference Learning Is Not All You Need",
                "abstract": "For decades, temporal-difference learning has emerged as the dominant computational framework for understanding dopamine\u2019s role in decision making in biological systems. Under this framework, dopamine response signals a reward prediction error for a scalar value prediction under a state, and drives learning in a model-free decision making paradigm. However, recent empirical evidence from biological systems have discovered discrepancies from the predictions made by TD learning. In this paper, we review works which reveal the role of dopamine in probabilistic inference and in model-based planning, which the classical scalar-value model-free TD learning perspective does not capture.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "9091886",
                        "name": "Huy Ha"
                    },
                    {
                        "authorId": "2107015962",
                        "name": "Sian Lee Kitt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, the effects of model mismatch should be quantified in other state of the art algorithms in MBRL such as MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ad70dfb41b903ca4dbaa6532cfad779ac5a68fb0",
                "externalIds": {
                    "CorpusId": 209475322
                },
                "corpusId": 209475322,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ad70dfb41b903ca4dbaa6532cfad779ac5a68fb0",
                "title": "Dynamics f \" Policy \u03c0 \" ( x ) Environment State Transitions RewardTrajectories Training : Maximum Likelihood Objective Mismatch Control Interacts Responses",
                "abstract": "Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, leaving the general framework virtually unchanged since its conception. In this paper, we identify a fundamental issue of the standard MBRL framework \u2013 what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood of the one-step ahead prediction is not always correlated with downstream control performance. This observation highlights a critical flaw in the current MBRL framework which will require further research to be fully understood and addressed. We propose an initial method to mitigate the mismatch issue by reweighting dynamics model training. Building on it, we conclude with a discussion about other potential directions of future research for addressing this issue.",
                "year": 2019,
                "authors": []
            }
        },
        {
            "contexts": [
                "Recently, MBPO [20] showed that they can train a model of Mujoco environments that is accurate enough for nearly 200-step rollouts in terms of accumulated rewards.",
                "To achieve these goals, we can use a trained model of the environment similar to model-based reinforcement learning approaches [20, 23, 6], instead of interacting directly with the environment in MCTS. Recently, MBPO [20] showed that they can train a model of Mujoco environments that is accurate enough for nearly 200-step rollouts in terms of accumulated rewards.",
                "To achieve these goals, we can use a trained model of the environment similar to model-based reinforcement learning approaches [20, 23, 6], instead of interacting directly with the environment in MCTS."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "0f38c4e251e8345f2dc412dfc50bdb9a3edcbbf4",
                "externalIds": {
                    "CorpusId": 209480513
                },
                "corpusId": 209480513,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0f38c4e251e8345f2dc412dfc50bdb9a3edcbbf4",
                "title": "TPO: TREE SEARCH POLICY OPTIMIZATION",
                "abstract": "Monte Carlo Tree Search (MCTS) has achieved impressive results on a range of discrete environments, such as Go, Mario and Arcade games, but it has not yet fulfilled its true potential in continuous domains. In this work, we introduce TPO, a tree search based policy optimization method for continuous environments. TPO takes a hybrid approach to policy optimization. Building the MCTS tree in a continuous action space and updating the policy gradient using off-policy MCTS trajectories are non-trivial. To overcome these challenges, we propose limiting tree search branching factor by drawing only a few action samples from the policy distribution and defining a new loss function based on the trajectories\u2019 mean and standard deviations. Our approach led to some non-intuitive findings. MCTS training generally requires a large number of samples and simulations. However, we observed that bootstrapping tree search with a pre-trained policy allows us to achieve high quality results with a low MCTS branching factor and few simulations. Without the proposed policy bootstrapping, continuous MCTS would require a much larger branching factor and simulation count, rendering it prohibitively expensive. In our experiments, we use PPO as our baseline policy optimization algorithm. TPO significantly improves the policy on nearly all the environments. For example, in complex environments such as Humanoid with a 17 dimensional action space, we achieve a 2.5\u00d7 improvement over the baseline algorithm.",
                "year": 2019,
                "authors": []
            }
        },
        {
            "contexts": [
                "The authors of the paper [1] first formulate and analyze a general implementation for MBPO with monotonic improvement at each step which uses a predictive model to optimize policy and utilizes the policy to collect data and train the model."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3786e5cfc6fcbe82727422f2163506b36f63e39a",
                "externalIds": {
                    "CorpusId": 250582044
                },
                "corpusId": 250582044,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3786e5cfc6fcbe82727422f2163506b36f63e39a",
                "title": "[Re] When to Trust Your Model: Model-Based Policy Optimization",
                "abstract": "Reinforcement learning algorithms generally belong to one of two categories: model-based techniques, which attempt to overcome the issue of a lack of prior knowledge by enabling the agent to construct a representation of its environment, and model-free techniques, which learn a direct mapping from states to actions. Model-free approaches are typically less practical because running a simulation is very time consuming or expensive, and model-based approaches tend to achieve lower asymptotic performance due to the error in model approximation. To design an effective model-based algorithm, Janner, M. et al [1]. studies the role of model usage in policy optimization and introduces a practical algorithm called model-based policy optimization (MBPO), which makes limited use of a predictive model to achieve pronounced improvements in performance comparing to other model-based approaches. The authors of the paper [1] first formulate and analyze a general implementation for MBPO with monotonic improvement at each step which uses a predictive model to optimize policy and utilizes the policy to collect data and train the model. Previous study shows it is difficult to justify model usage due to pessimistic bounds on model error and this paper finds a way to modify the pessimistic bounds which solves the problem. Based on the analysis, the authors propose a simple model-based reinforcement learning algorithm of using short model-generated rollouts branched from real data to improve model effectiveness. The experiments show this algorithm is faster than other state-ofthe-art model-based methods such as STEVE [2], and matches the performance of the best model-free methods like SAC [3]. In this reproducibility report, we study in detail the MBPO algorithm described in the paper (detailed in Section 3). Our work mainly focuses on the replication of their algorithm and the re-implementation of their predictive model but in a PyTorch version (detailed in Section 4). Lastly, we describe our experiments and provide insightful analysis of our results (detailed in Section 5). We provide the source code1 for generating the results and setting up the experiments.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2145527647",
                        "name": "Yuting Liu"
                    },
                    {
                        "authorId": "2144296591",
                        "name": "Jiahao Xu"
                    },
                    {
                        "authorId": "14079464",
                        "name": "Yi Pan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "259926435",
                "publicationVenue": null,
                "url": null,
                "title": "B ENCHMARKING O FFLINE R EINFORCEMENT L EARN - ING ON R EAL -R OBOT H ARDWARE",
                "abstract": null,
                "year": null,
                "authors": []
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "71140a8b477a7344907891b1822a708df46246c2",
                "externalIds": {
                    "CorpusId": 260378553
                },
                "corpusId": 260378553,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/71140a8b477a7344907891b1822a708df46246c2",
                "title": "The IISc Thesis Template A",
                "abstract": "Recent works in Reinforcement Learning (RL) combine model-free (Mf)-RL algorithms with model-based (Mb)-RL approaches to get the best from both: asymptotic performance of Mf-RL and high sample-e\ufb03ciency of Mb-RL. Inspired by these works, we propose a hierarchical framework that integrates online learning for the Mb-trajectory optimization with o\ufb00-policy methods for the Mf-RL. In particular, two loops are proposed, where the Dynamic Mirror Descent based Model Predictive Control (DMD-MPC) is used as the inner loop to obtain an optimal sequence of actions. These actions are in turn used to signi\ufb01cantly accelerate the outer loop Mf-RL. We show that our formulation is generic for a broad class of MPC based policies and objectives, and includes some of the well-known Mb-Mf approaches. Based on the frame work we de\ufb01ne two algorithms to increase sample e\ufb03ciency of O\ufb00 Policy RL and to guide end to end RL algorithms for online adaption respectively. Thus we \ufb01nally introduce two novel algorithms: D ynamic-Mirror D e scent Mo del Predictive RL ( DeMoRL ), which uses the method of elite fractions for the inner loop and Soft Actor-Critic (SAC) as the o\ufb00-policy RL for the outer loop and D ynamic-Mirror D e scent Mo del Predictive Layer ( DeMo Layer ), a special case of hierarchical framework which guides linear policies trained using Augmented Random Search(ARS). Our experiments show faster convergence of the proposed DeMo RL, and better or equal performance compared to other Mf-Mb approaches on benchmark MuJoCo control tasks. The DeMo Layer was tested on classical Cartpole and custom built Quadruped trained using Linear Policy Approach. The results shows that DeMo Layer signi\ufb01cantly increases performance of the Linear Policy in both the settings.",
                "year": null,
                "authors": [
                    {
                        "authorId": "2226453283",
                        "name": "Dr.Shishir Nadubettu"
                    },
                    {
                        "authorId": "2226454841",
                        "name": "Yadukumar Kolathaya Dr"
                    },
                    {
                        "authorId": "143683893",
                        "name": "S. Bhatnagar"
                    }
                ]
            }
        }
    ]
}