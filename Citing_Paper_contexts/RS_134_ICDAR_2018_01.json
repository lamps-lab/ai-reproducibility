{
    "offset": 0,
    "data": [
        {
            "contexts": [
                "While tables with simple structures and clean backgrounds can be recognized well [6, 12, 14, 15, 23, 24, 30, 35, 38, 41, 47], recognizing complicated table structures remains a challenging problem, which is primarily due to two main difficulties: 1) Firstly, tables in images vary widely in terms of structure and shape.",
                "Note that, there are fundamental differences between the representation in [8, 20, 36, 38, 46] and ours despite the representation is also termed as \"grid\".",
                "In the works of [8, 20, 27, 36, 38, 46], a table is depicted as a grid of regions.",
                "For instance, region-based methods [8, 20, 36, 38, 46] employ a split model to divide input table images into a grid of regions, and a merge model to combine over-split spanning cells."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3e797270bd42586b0e46ce597a5ef6d754465e43",
                "externalIds": {
                    "ArXiv": "2309.14962",
                    "CorpusId": 262825477
                },
                "corpusId": 262825477,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3e797270bd42586b0e46ce597a5ef6d754465e43",
                "title": "GridFormer: Towards Accurate Table Structure Recognition via Grid Prediction",
                "abstract": "All tables can be represented as grids. Based on this observation, we propose GridFormer, a novel approach for interpreting unconstrained table structures by predicting the vertex and edge of a grid. First, we propose a flexible table representation in the form of an MXN grid. In this representation, the vertexes and edges of the grid store the localization and adjacency information of the table. Then, we introduce a DETR-style table structure recognizer to efficiently predict this multi-objective information of the grid in a single shot. Specifically, given a set of learned row and column queries, the recognizer directly outputs the vertexes and edges information of the corresponding rows and columns. Extensive experiments on five challenging benchmarks which include wired, wireless, multi-merge-cell, oriented, and distorted tables demonstrate the competitive performance of our model over other methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "10344582",
                        "name": "Pengyuan Lyu"
                    },
                    {
                        "authorId": "47121217",
                        "name": "Weihong Ma"
                    },
                    {
                        "authorId": "2109798334",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "1631405123",
                        "name": "Yu Yu"
                    },
                    {
                        "authorId": "2248958848",
                        "name": "Chengquan Zhang"
                    },
                    {
                        "authorId": "2140371004",
                        "name": "Kun Yao"
                    },
                    {
                        "authorId": "2114922667",
                        "name": "Yang Xue"
                    },
                    {
                        "authorId": "1688516",
                        "name": "Jingdong Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Early works [37, 34] directly use visual features encoded by convolutional neural networks (CNN) [19] for layout units detection [36, 30, 35, 17], and have been proven to be effective.",
                "Recent deep learning works [37, 34] consider DLA as a classic visual object detection or segmentation problem and utilize convolutional neural networks (CNN) [19] to solve this task [36, 30, 35, 17]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7d96fa57d5c009c99f04913786ceaf68926d9acf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-14978",
                    "ArXiv": "2308.14978",
                    "DOI": "10.48550/arXiv.2308.14978",
                    "CorpusId": 261277255
                },
                "corpusId": 261277255,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7d96fa57d5c009c99f04913786ceaf68926d9acf",
                "title": "Vision Grid Transformer for Document Layout Analysis",
                "abstract": "Document pre-trained models and grid-based models have proven to be very effective on various tasks in Document AI. However, for the document layout analysis (DLA) task, existing document pre-trained models, even those pre-trained in a multi-modal fashion, usually rely on either textual features or visual features. Grid-based models for DLA are multi-modality but largely neglect the effect of pre-training. To fully leverage multi-modal information and exploit pre-training techniques to learn better representation for DLA, in this paper, we present VGT, a two-stream Vision Grid Transformer, in which Grid Transformer (GiT) is proposed and pre-trained for 2D token-level and segment-level semantic understanding. Furthermore, a new dataset named D$^4$LA, which is so far the most diverse and detailed manually-annotated benchmark for document layout analysis, is curated and released. Experiment results have illustrated that the proposed VGT model achieves new state-of-the-art results on DLA tasks, e.g. PubLayNet ($95.7\\%$$\\rightarrow$$96.2\\%$), DocBank ($79.6\\%$$\\rightarrow$$84.1\\%$), and D$^4$LA ($67.7\\%$$\\rightarrow$$68.8\\%$). The code and models as well as the D$^4$LA dataset will be made publicly available ~\\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2065329174",
                        "name": "Cheng Da"
                    },
                    {
                        "authorId": "7619267",
                        "name": "Chuwei Luo"
                    },
                    {
                        "authorId": "2114172383",
                        "name": "Qi Zheng"
                    },
                    {
                        "authorId": "2173739231",
                        "name": "Cong Yao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13de1c39dfe5733363ff1a8d4c6d577914f79618",
                "externalIds": {
                    "DBLP": "conf/ijcai/ShenGWQZLC23",
                    "DOI": "10.24963/ijcai.2023/152",
                    "CorpusId": 260853966
                },
                "corpusId": 260853966,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/13de1c39dfe5733363ff1a8d4c6d577914f79618",
                "title": "Divide Rows and Conquer Cells: Towards Structure Recognition for Large Tables",
                "abstract": "Recent advanced Table Structure Recognition (TSR) models adopt image-to-text solutions to parse table structure. These methods can be formulated as image caption problem, i.e., input a single-table image and output table structure description in a specific text format, e.g., HTML. With the impressive success of Transformer in text generation tasks, these methods use Transformer architecture to predict HTML table text in an autoregressive manner. However, tables always emerge with a large variety of shapes and sizes. Autoregressive models usually suffer from the error accumulation problem as the length of predicted text increases, which results in unsatisfactory performance for large tables. In this paper, we propose a novel image-to-text based TSR method that relieves error accumulation problems and improves performance noticeably. At the core of our method is a cascaded two-step decoder architecture with the former decoder predicting HTML table row tags non-autoregressively and the latter predicting HTML table cell tags of each row in a semi-autoregressive manner. Compared with existing methods that predict HTML text autoregressively, the superiority of our row-to-cell progressive table parsing is twofold: (1) it generates an HTML tag sequence with a vertical-and-horizontal two-step `scanning', which better fits the inherent 2D structure of image data, (2) it performs substantially better for large tables (long sequence prediction) since it alleviates error accumulation problem specific to autoregressive models. Extensive experiments demonstrate that our method achieves competitive performance on three public benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2230137426",
                        "name": "Huawen Shen"
                    },
                    {
                        "authorId": "2149397987",
                        "name": "Xiang Gao"
                    },
                    {
                        "authorId": "2111524263",
                        "name": "Jin Wei"
                    },
                    {
                        "authorId": "2065512194",
                        "name": "Liang Qiao"
                    },
                    {
                        "authorId": "49454803",
                        "name": "Yu Zhou"
                    },
                    {
                        "authorId": "2229647956",
                        "name": "Qiang Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "for tables [21,20,4,27] or structured diagrams [12,16,31,8]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4cc4dccbdba62ea19e4d13c2bd99f62f93ea5f9d",
                "externalIds": {
                    "ArXiv": "2307.10471",
                    "DBLP": "journals/corr/abs-2307-10471",
                    "DOI": "10.48550/arXiv.2307.10471",
                    "CorpusId": 259991612
                },
                "corpusId": 259991612,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4cc4dccbdba62ea19e4d13c2bd99f62f93ea5f9d",
                "title": "Classification of Visualization Types and Perspectives in Patents",
                "abstract": "Due to the swift growth of patent applications each year, information and multimedia retrieval approaches that facilitate patent exploration and retrieval are of utmost importance. Different types of visualizations (e.g., graphs, technical drawings) and perspectives (e.g., side view, perspective) are used to visualize details of innovations in patents. The classification of these images enables a more efficient search and allows for further analysis. So far, datasets for image type classification miss some important visualization types for patents. Furthermore, related work does not make use of recent deep learning approaches including transformers. In this paper, we adopt state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We extend the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. In addition, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Experimental results have demonstrated the feasibility of the proposed approaches. Source code, models, and dataset will be made publicly available.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2000247523",
                        "name": "J. Ghauri"
                    },
                    {
                        "authorId": "1404095219",
                        "name": "Eric M\u00fcller-Budack"
                    },
                    {
                        "authorId": "1738703",
                        "name": "R. Ewerth"
                    }
                ]
            }
        },
        {
            "contexts": [
                "former takes the document as an image input and frames entity detection as object detection or instance segmentation [40, 31]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "85e6c2d34304a01e3f7071cdc7200405c4af93d0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-07929",
                    "ArXiv": "2307.07929",
                    "DOI": "10.48550/arXiv.2307.07929",
                    "CorpusId": 259937511
                },
                "corpusId": 259937511,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/85e6c2d34304a01e3f7071cdc7200405c4af93d0",
                "title": "DocTr: Document Transformer for Structured Information Extraction in Documents",
                "abstract": "We present a new formulation for structured information extraction (SIE) from visually rich documents. It aims to address the limitations of existing IOB tagging or graph-based formulations, which are either overly reliant on the correct ordering of input text or struggle with decoding a complex graph. Instead, motivated by anchor-based object detectors in vision, we represent an entity as an anchor word and a bounding box, and represent entity linking as the association between anchor words. This is more robust to text ordering, and maintains a compact graph for entity linking. The formulation motivates us to introduce 1) a DOCument TRansformer (DocTr) that aims at detecting and associating entity bounding boxes in visually rich documents, and 2) a simple pre-training strategy that helps learn entity detection in the context of language. Evaluations on three SIE benchmarks show the effectiveness of the proposed formulation, and the overall approach outperforms existing solutions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145585312",
                        "name": "Haofu Liao"
                    },
                    {
                        "authorId": "2895705",
                        "name": "Aruni RoyChowdhury"
                    },
                    {
                        "authorId": "48624767",
                        "name": "Weijian Li"
                    },
                    {
                        "authorId": "2068427",
                        "name": "Ankan Bansal"
                    },
                    {
                        "authorId": "2108148342",
                        "name": "Yuting Zhang"
                    },
                    {
                        "authorId": "144035504",
                        "name": "Z. Tu"
                    },
                    {
                        "authorId": "1710219",
                        "name": "R. Satzoda"
                    },
                    {
                        "authorId": "1758550",
                        "name": "R. Manmatha"
                    },
                    {
                        "authorId": "48493294",
                        "name": "V. Mahadevan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, Sebastian Schreiber in 2017 [15] presented a deep learning-based solution called DeepDeSRT for table detection in document images and table structure recognition, using the concept of transfer learning and domain adaptation from a pre-trained model of Faster R-CNN [16]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "943a55e5a15b462837c9d317b0df9462982a00a0",
                "externalIds": {
                    "DBLP": "conf/ijcnn/DasGDSM23",
                    "DOI": "10.1109/IJCNN54540.2023.10192028",
                    "CorpusId": 260387207
                },
                "corpusId": 260387207,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/943a55e5a15b462837c9d317b0df9462982a00a0",
                "title": "\u201cFind the Table\u201d: A Contrastive Learning-based Approach with Faster RCNN for Establishing Tabular Entity Relationships",
                "abstract": "Financial industries rely on a variety of data to understand an organization's financial health and performance, and analysis of financial statements is used for decision-making and understanding business activities. Financial statements often have complex, unstructured formats, making it difficult to extract useful information for decision-making. Organizing and refining these data is crucial for effective analysis. Previous research in the field of table detection has primarily centred around object detection methods, with limited exploration of methods for cell-wise information extraction by identifying row and column entities. In order to address this research gap, this paper proposes a novel dataset called TERED (Tabular Entity Relationship Establishment Dataset) to train a model to identify relationships among elements in large financial tables, such as statements and balance sheets, using computer vision techniques which have yet to be fully explored in financial analysis. The dataset contains more than 10,000 tabular data in scanned image and pdf formats and is divided into 12 classes. We trained CLF-RCNN, a Contrastive Learning based Faster RCNN model which in turn is a state-of-the-art object detection model on this dataset and achieved an F1 score of 93 % for table detection and 74% for identifying tabular entities and relationships. Additionally, we introduced a new loss term influenced by contrastive learning that improves prediction performances with our developed algorithm to return the sequential order of the unordered predictions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226438701",
                        "name": "Sarmistha Das"
                    },
                    {
                        "authorId": "2190514830",
                        "name": "Tuhinangshu Gangopadhyay"
                    },
                    {
                        "authorId": "2226486078",
                        "name": "Atulya Deep"
                    },
                    {
                        "authorId": "145470045",
                        "name": "S. Saha"
                    },
                    {
                        "authorId": "104838863",
                        "name": "A. Maurya"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Subsequent research introduced end-to-end deep learning systems capable of not only detecting tables but also recognising their structure in document images, without the need for metadata or heuristics [23]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9255caa2757db1728190f22dabcb729b52ad0648",
                "externalIds": {
                    "ArXiv": "2306.07968",
                    "DBLP": "journals/corr/abs-2306-07968",
                    "DOI": "10.48550/arXiv.2306.07968",
                    "CorpusId": 259145157
                },
                "corpusId": 259145157,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9255caa2757db1728190f22dabcb729b52ad0648",
                "title": "arXiVeri: Automatic table verification with GPT",
                "abstract": "Without accurate transcription of numerical data in scientific documents, a scientist cannot draw accurate conclusions. Unfortunately, the process of copying numerical data from one paper to another is prone to human error. In this paper, we propose to meet this challenge through the novel task of automatic table verification (AutoTV), in which the objective is to verify the accuracy of numerical data in tables by cross-referencing cited sources. To support this task, we propose a new benchmark, arXiVeri, which comprises tabular data drawn from open-access academic papers on arXiv. We introduce metrics to evaluate the performance of a table verifier in two key areas: (i) table matching, which aims to identify the source table in a cited document that corresponds to a target table, and (ii) cell matching, which aims to locate shared cells between a target and source table and identify their row and column indices accurately. By leveraging the flexible capabilities of modern large language models (LLMs), we propose simple baselines for table verification. Our findings highlight the complexity of this task, even for state-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark will be made publicly available.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "104387308",
                        "name": "Gyungin Shin"
                    },
                    {
                        "authorId": "10096695",
                        "name": "Weidi Xie"
                    },
                    {
                        "authorId": "7641268",
                        "name": "Samuel Albanie"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4bcaef5baa082bf4cca420bbfca91c9508111267",
                "externalIds": {
                    "DOI": "10.1109/ICCES57224.2023.10192739",
                    "CorpusId": 260386295
                },
                "corpusId": 260386295,
                "publicationVenue": {
                    "id": "4a0a181c-3807-4883-8ac2-b0ddf5f46497",
                    "name": "International Conference on Communication and Electronics Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Commun Electron Syst",
                        "ICCES",
                        "Int Conf Comput Eng Syst",
                        "International Conference on Computer Engineering and Systems"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4bcaef5baa082bf4cca420bbfca91c9508111267",
                "title": "Table Detection & Data Extraction from Documents using Object Detection",
                "abstract": "Table detection and data extraction from documents is a crucial task in the field of computer vision and document analysis. The goal is to automatically detect tables in documents and extract relevant information from them. This can be achieved through object detection techniques, which use machine learning algorithms to identify objects in images and extract information from them. The process involves training a model on a dataset of table images, and then using the trained model to detect tables in new images. Once tables are detected, data extraction algorithms can be used to extract text and numerical information from the cells of the table. This information can then be used for a variety of applications, such as database creation, business intelligence, and data analysis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2227045096",
                        "name": "Mohammad Talha Khan"
                    },
                    {
                        "authorId": "9409896",
                        "name": "T. Jeyaprakash"
                    },
                    {
                        "authorId": "40964034",
                        "name": "D. chandar"
                    },
                    {
                        "authorId": "83337181",
                        "name": "V. Durga"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The experimental results of CascadeTabNet [4], TableDet [3], DeCNT [36], YOLOv3TD [33], DeepDeSRT [35], TableNet [45], GAN-TD [46], TableRadar [43] and NLPR-PAL [43] in Table 2, 3 and 4 are from study [3].",
                "There are many other studies discussing the TD problem, including DeepDeSRT [35], TableDet [3] and many others [36, 37] ."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "86badf878ebdc2b945fad1cf9d204268f62346e7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-19181",
                    "ArXiv": "2305.19181",
                    "DOI": "10.48550/arXiv.2305.19181",
                    "CorpusId": 258967592
                },
                "corpusId": 258967592,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/86badf878ebdc2b945fad1cf9d204268f62346e7",
                "title": "Table Detection for Visually Rich Document Images",
                "abstract": "Table Detection (TD) is a fundamental task towards visually rich document understanding. Current studies usually formulate the TD problem as an object detection problem, then leverage Intersection over Union (IoU) based metrics to evaluate the model performance and IoU-based loss functions to optimize the model. TD applications usually require the prediction results to cover all the table contents and avoid information loss. However, IoU and IoU-based loss functions cannot directly reflect the degree of information loss for the prediction results. Therefore, we propose to decouple IoU into a ground truth coverage term and a prediction coverage term, in which the former can be used to measure the information loss of the prediction results. Besides, tables in the documents are usually large, sparsely distributed, and have no overlaps because they are designed to summarize essential information to make it easy to read and interpret for human readers. Therefore, in this study, we use SparseR-CNN as the base model, and further improve the model by using Gaussian Noise Augmented Image Size region proposals and many-to-one label assignments. To demonstrate the effectiveness of proposed method and compare with state-of-the-art methods fairly, we conduct experiments and use IoU-based evaluation metrics to evaluate the model performance. The experimental results show that the proposed method can consistently outperform state-of-the-art methods under different IoU-based metric on a variety of datasets. We conduct further experiments to show the superiority of the proposed decoupled IoU for the TD applications by replacing the IoU-based loss functions and evaluation metrics with proposed decoupled IoU counterparts. The experimental results show that our proposed decoupled IoU loss can encourage the model to alleviate information loss.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2165556493",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "46661e9ffbad47e1c1064c781debc56fb4418c09",
                "externalIds": {
                    "DBLP": "conf/cniot/ZhangWBFLG23",
                    "DOI": "10.1145/3603781.3603873",
                    "CorpusId": 260204738
                },
                "corpusId": 260204738,
                "publicationVenue": {
                    "id": "6c1d471f-79a9-449a-b0b3-d5e78390b141",
                    "name": "International Conferences on Computing, Networks and Internet of Things",
                    "type": "conference",
                    "alternate_names": [
                        "CNIOT",
                        "Int Conf Comput Netw Internet Thing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/46661e9ffbad47e1c1064c781debc56fb4418c09",
                "title": "Knowledge Graph Construction from Tables in Chinese Electric Power PDF Documents",
                "abstract": "The PDF documents of the electric power standard contain a large number of tables, and the main purpose of these table layouts application is to show people more intuitive information. However, the data in the tables is not easily processed by computers and the value of the data is difficult to be exploited. In this paper, an approach is proposed to convert tabular data into RDF knowledge graphs for the power standard PDF documents. Firstly, a variety of table processing techniques are used to extract tabular data from PDF. Then tabular data is normalized and transformed into RDF data. Finally, an electricity domain ontology is constructed and the tabular data is mapped to ontology classes through table interpretation. Extensive experiments are conducted to obtain a structured RDF dataset related to electricity standards, which provides the necessary data support for intelligent knowledge services in the electricity industry.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152042478",
                        "name": "Rong Zhang"
                    },
                    {
                        "authorId": "2051700077",
                        "name": "Changlong Wang"
                    },
                    {
                        "authorId": "2128280400",
                        "name": "Siyun Bi"
                    },
                    {
                        "authorId": "5096878",
                        "name": "Qibin Fu"
                    },
                    {
                        "authorId": "2225339478",
                        "name": "Xingyu Li"
                    },
                    {
                        "authorId": "2190898322",
                        "name": "Tingting Gan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In order to provide a new state-of-the-art benchmark for table detection and table structure recognition, DeepDeSRT [40] utilizes a novel image transformation strategy to identify the visual features of the table structures and feed them into a fully convolution network with skip pooling."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f72376882f788a49c5943046d27becc0ad54c0ad",
                "externalIds": {
                    "DBLP": "conf/icdar/BanerjeeBLP23",
                    "ArXiv": "2305.04609",
                    "DOI": "10.48550/arXiv.2305.04609",
                    "CorpusId": 258556923
                },
                "corpusId": 258556923,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/f72376882f788a49c5943046d27becc0ad54c0ad",
                "title": "SwinDocSegmenter: An End-to-End Unified Domain Adaptive Transformer for Document Instance Segmentation",
                "abstract": "Instance-level segmentation of documents consists in assigning a class-aware and instance-aware label to each pixel of the image. It is a key step in document parsing for their understanding. In this paper, we present a unified transformer encoder-decoder architecture for en-to-end instance segmentation of complex layouts in document images. The method adapts a contrastive training with a mixed query selection for anchor initialization in the decoder. Later on, it performs a dot product between the obtained query embeddings and the pixel embedding map (coming from the encoder) for semantic reasoning. Extensive experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical Japanese (HJ), and TableBank demonstrate that our model with SwinL backbone achieves better segmentation performance than the existing state-of-the-art approaches with the average precision of \\textbf{93.72}, \\textbf{54.39}, \\textbf{84.65} and \\textbf{98.04} respectively under one billion parameters. The code is made publicly available at: \\href{https://github.com/ayanban011/SwinDocSegmenter}{github.com/ayanban011/SwinDocSegmenter}",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153585985",
                        "name": "Ayan Banerjee"
                    },
                    {
                        "authorId": "2150473007",
                        "name": "Sanket Biswas"
                    },
                    {
                        "authorId": "2215822671",
                        "name": "Josep Llad'os"
                    },
                    {
                        "authorId": "144167309",
                        "name": "U. Pal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, the problem of table structure recognition (TSR) is a lot more challenging and remains a very active area of research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22].",
                "Object-detection based methods [11,12,13,14,21] rely on tablestructure annotation using (overlapping) bounding boxes for training, and produce bounding-box predictions to define table cells, rows, and columns on a table image."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "88b556a6108b98cd0e12ae2945cc6597fff9101c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-03393",
                    "ArXiv": "2305.03393",
                    "DOI": "10.48550/arXiv.2305.03393",
                    "CorpusId": 258546918
                },
                "corpusId": 258546918,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/88b556a6108b98cd0e12ae2945cc6597fff9101c",
                "title": "Optimized Table Tokenization for Table Structure Recognition",
                "abstract": "Extracting tables from documents is a crucial task in any document conversion pipeline. Recently, transformer-based models have demonstrated that table-structure can be recognized with impressive accuracy using Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since the token representation of the table structure has a significant impact on the accuracy and run-time performance of any Im2Seq model, we investigate in this paper how table-structure representation can be optimised. We propose a new, optimised table-structure language (OTSL) with a minimized vocabulary and specific rules. The benefits of OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and shortens the sequence length to half of HTML on average. Consequently, model accuracy improves significantly, inference time is halved compared to HTML-based models, and the predicted table structures are always syntactically correct. This in turn eliminates most post-processing needs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1578242957",
                        "name": "Maksym Lysak"
                    },
                    {
                        "authorId": "73241238",
                        "name": "Ahmed Nassar"
                    },
                    {
                        "authorId": "2051177400",
                        "name": "Nikolaos Livathinos"
                    },
                    {
                        "authorId": "2065064783",
                        "name": "Christoph Auer"
                    },
                    {
                        "authorId": "1831851",
                        "name": "P. Staar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Deep learning approaches [16,17,18] do not rely on rules and can accurately generalize the problem.",
                "Later, more efficient single-stage object detectors like RetinaNet [58] and YOLO [59] and two-stage object detectors like Fast R-CNN [11], Faster R-CNN [12], Mask R-CNN [60], and Cascade Mask R-CNN [61] were applied for other document objects such as figures and formulas detection in document images [62,63,64,65,66,67,68,16,17,18].",
                "Recently, researchers employed statistical methods [28] and deep learning approaches to make the table detection systems more generalizable [16,29,30,31]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8f87ad8807a4b149c69bac1f169d9f28d13f8928",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-02769",
                    "ArXiv": "2305.02769",
                    "DOI": "10.48550/arXiv.2305.02769",
                    "CorpusId": 258480298
                },
                "corpusId": 258480298,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/8f87ad8807a4b149c69bac1f169d9f28d13f8928",
                "title": "Towards End-to-End Semi-Supervised Table Detection with Deformable Transformer",
                "abstract": "Table detection is the task of classifying and localizing table objects within document images. With the recent development in deep learning methods, we observe remarkable success in table detection. However, a significant amount of labeled data is required to train these models effectively. Many semi-supervised approaches are introduced to mitigate the need for a substantial amount of label data. These approaches use CNN-based detectors that rely on anchor proposals and post-processing stages such as NMS. To tackle these limitations, this paper presents a novel end-to-end semi-supervised table detection method that employs the deformable transformer for detecting table objects. We evaluate our semi-supervised method on PubLayNet, DocBank, ICADR-19 and TableBank datasets, and it achieves superior performance compared to previous methods. It outperforms the fully supervised method (Deformable transformer) by +3.4 points on 10\\% labels of TableBank-both dataset and the previous CNN-based semi-supervised approach (Soft Teacher) by +1.8 points on 10\\% labels of PubLayNet dataset. We hope this work opens new possibilities towards semi-supervised and unsupervised table detection methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2029681143",
                        "name": "Tahira Shehzadi"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are also many other studies discussing the TD problem, such as TableDet [7], DeCNT [32], DeepDeSRT [33], TableNet [34], and most of these studies follow the object detection formulation and utilize different types of object detection models that are mentioned above."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f35ae7f12b3713fae216688c358d8a528973d286",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-04833",
                    "ArXiv": "2305.04833",
                    "DOI": "10.48550/arXiv.2305.04833",
                    "CorpusId": 258557161
                },
                "corpusId": 258557161,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f35ae7f12b3713fae216688c358d8a528973d286",
                "title": "Revisiting Table Detection Datasets for Visually Rich Documents",
                "abstract": "Table Detection has become a fundamental task for visually rich document understanding with the surging number of electronic documents. There have been some open datasets widely used in many studies. However, popular available datasets have some inherent limitations, including the noisy and inconsistent samples, and the limit number of training samples, and the limit number of data-sources. These limitations make these datasets unreliable to evaluate the model performance and cannot reflect the actual capacity of models. Therefore, in this paper, we revisit some open datasets with high quality of annotations, identify and clean the noise, and align the annotation definitions of these datasets to merge a larger dataset, termed with Open-Tables. Moreover, to enrich the data sources, we propose a new dataset, termed with ICT-TD, using the PDF files of Information and communication technologies (ICT) commodities which is a different domain containing unique samples that hardly appear in open datasets. To ensure the label quality of the dataset, we annotated the dataset manually following the guidance of a domain expert. The proposed dataset has a larger intra-variance and smaller inter-variance, making it more challenging and can be a sample of actual cases in the business context. We built strong baselines using various state-of-the-art object detection models and also built the baselines in the cross-domain setting. Our experimental results show that the domain difference among existing open datasets are small, even they have different data-sources. Our proposed Open-tables and ICT-TD are more suitable for the cross domain setting, and can provide more reliable evaluation for model because of their high quality and consistent annotations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2165556493",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since the advent of deep learning and CNN-based approaches [53,55], the segmentation of document layouts has been reformulated as DOD."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "202860339bcaa37220f5b0ee6a3a2dbad68e9373",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00795",
                    "ArXiv": "2305.00795",
                    "DOI": "10.1007/978-3-031-41676-7_20",
                    "CorpusId": 258426656
                },
                "corpusId": 258426656,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/202860339bcaa37220f5b0ee6a3a2dbad68e9373",
                "title": "SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "13238892",
                        "name": "Subhajit Maity"
                    },
                    {
                        "authorId": "2150473007",
                        "name": "Sanket Biswas"
                    },
                    {
                        "authorId": "1816706542",
                        "name": "Siladittya Manna"
                    },
                    {
                        "authorId": "2153585985",
                        "name": "Ayan Banerjee"
                    },
                    {
                        "authorId": "2215822671",
                        "name": "Josep Llad'os"
                    },
                    {
                        "authorId": "1990389",
                        "name": "Saumik Bhattacharya"
                    },
                    {
                        "authorId": "144167309",
                        "name": "U. Pal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Researchers in [37,41,40] have proposed to detect row/column regions based on segmentation and off-the-shelf object detectors.",
                "For example, Faster R-CNN was adopted by [37,6,42,36,44], and YOLO was used in [11]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eca4dfc1703529bd87e1b4fea1fad1baa06e0ac9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00630",
                    "ArXiv": "2305.00630",
                    "DOI": "10.48550/arXiv.2305.00630",
                    "CorpusId": 258427145
                },
                "corpusId": 258427145,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/eca4dfc1703529bd87e1b4fea1fad1baa06e0ac9",
                "title": "TRACE: Table Reconstruction Aligned to Corner and Edges",
                "abstract": "A table is an object that captures structured and informative content within a document, and recognizing a table in an image is challenging due to the complexity and variety of table layouts. Many previous works typically adopt a two-stage approach; (1) Table detection(TD) localizes the table region in an image and (2) Table Structure Recognition(TSR) identifies row- and column-wise adjacency relations between the cells. The use of a two-stage approach often entails the consequences of error propagation between the modules and raises training and inference inefficiency. In this work, we analyze the natural characteristics of a table, where a table is composed of cells and each cell is made up of borders consisting of edges. We propose a novel method to reconstruct the table in a bottom-up manner. Through a simple process, the proposed method separates cell boundaries from low-level features, such as corners and edges, and localizes table positions by combining the cells. A simple design makes the model easier to train and requires less computation than previous two-stage methods. We achieve state-of-the-art performance on the ICDAR2013 table competition benchmark and Wired Table in the Wild(WTW) dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057310228",
                        "name": "Youngmin Baek"
                    },
                    {
                        "authorId": "2064808754",
                        "name": "Daehyun Nam"
                    },
                    {
                        "authorId": "10787779",
                        "name": "Jaeheung Surh"
                    },
                    {
                        "authorId": "2111068603",
                        "name": "Seung Shin"
                    },
                    {
                        "authorId": "2109603647",
                        "name": "Seonghyeon Kim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2fd312a5d5a259b585224aed1026b3c14be95c91",
                "externalIds": {
                    "DBLP": "journals/spic/Mishra23",
                    "DOI": "10.1016/j.image.2023.116986",
                    "CorpusId": 258600753
                },
                "corpusId": 258600753,
                "publicationVenue": {
                    "id": "5e329303-0790-4991-832f-dd8a737ec6fb",
                    "name": "Signal processing. Image communication",
                    "type": "journal",
                    "alternate_names": [
                        "Signal process Image commun",
                        "Signal Process Commun",
                        "Signal Processing-image Communication"
                    ],
                    "issn": "0923-5965",
                    "url": "https://www.journals.elsevier.com/signal-processing-image-communication"
                },
                "url": "https://www.semanticscholar.org/paper/2fd312a5d5a259b585224aed1026b3c14be95c91",
                "title": "Domain adaptive learning for document layout analysis and object detection using classifier alignment mechanism",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2068143981",
                        "name": "Prerna Mishra"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fe211819c3959eaa5382177016e8091465e84861",
                "externalIds": {
                    "DBLP": "journals/pr/MondalAJ23",
                    "DOI": "10.1016/j.patcog.2023.109698",
                    "CorpusId": 258826399
                },
                "corpusId": 258826399,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fe211819c3959eaa5382177016e8091465e84861",
                "title": "Dataset agnostic document object detection",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "2148629212",
                        "name": "Madhav Agarwal"
                    },
                    {
                        "authorId": "1694502",
                        "name": "C. V. Jawahar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[7,26] focused on extracting tabular data from documents by combining heuristic rules and convolutional networks to detect and recognize tabular data."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fa75fb586c46a30e742a0bd5748afaa875f30607",
                "externalIds": {
                    "ArXiv": "2304.14936",
                    "DBLP": "conf/icdar/LaatiriRTLVC23",
                    "DOI": "10.48550/arXiv.2304.14936",
                    "CorpusId": 258418006
                },
                "corpusId": 258418006,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/fa75fb586c46a30e742a0bd5748afaa875f30607",
                "title": "Information Redundancy and Biases in Public Document Information Extraction Benchmarks",
                "abstract": "Advances in the Visually-rich Document Understanding (VrDU) field and particularly the Key-Information Extraction (KIE) task are marked with the emergence of efficient Transformer-based approaches such as the LayoutLM models. Despite the good performance of KIE models when fine-tuned on public benchmarks, they still struggle to generalize on complex real-life use-cases lacking sufficient document annotations. Our research highlighted that KIE standard benchmarks such as SROIE and FUNSD contain significant similarity between training and testing documents and can be adjusted to better evaluate the generalization of models. In this work, we designed experiments to quantify the information redundancy in public benchmarks, revealing a 75% template replication in SROIE official test set and 16% in FUNSD. We also proposed resampling strategies to provide benchmarks more representative of the generalization ability of models. We showed that models not suited for document analysis struggle on the adjusted splits dropping on average 10,5% F1 score on SROIE and 3.5% on FUNSD compared to multi-modal models dropping only 7,5% F1 on SROIE and 0.5% F1 on FUNSD.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2215813745",
                        "name": "Seif Laatiri"
                    },
                    {
                        "authorId": "1637246846",
                        "name": "Pirashanth Ratnamogan"
                    },
                    {
                        "authorId": "2191525658",
                        "name": "Joel Tang"
                    },
                    {
                        "authorId": "2910364",
                        "name": "Laurent Lam"
                    },
                    {
                        "authorId": "2139705350",
                        "name": "William Vanhuffel"
                    },
                    {
                        "authorId": "2029670880",
                        "name": "Fabien Caspani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Earlier methods attempted to solve these two tasks separately [15].",
                "[15] 2017 DeepDeSRT ICDAR Marmot Marmot - - NR"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "50903f5f5e714e5746152b3e369c9329655e0018",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-10439",
                    "ArXiv": "2304.10439",
                    "DOI": "10.48550/arXiv.2304.10439",
                    "CorpusId": 258236507
                },
                "corpusId": 258236507,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/50903f5f5e714e5746152b3e369c9329655e0018",
                "title": "A Study on Reproducibility and Replicability of Table Structure Recognition Methods",
                "abstract": "Concerns about reproducibility in artificial intelligence (AI) have emerged, as researchers have reported unsuccessful attempts to directly reproduce published findings in the field. Replicability, the ability to affirm a finding using the same procedures on new data, has not been well studied. In this paper, we examine both reproducibility and replicability of a corpus of 16 papers on table structure recognition (TSR), an AI task aimed at identifying cell locations of tables in digital documents. We attempt to reproduce published results using codes and datasets provided by the original authors. We then examine replicability using a dataset similar to the original as well as a new dataset, GenTSR, consisting of 386 annotated tables extracted from scientific papers. Out of 16 papers studied, we reproduce results consistent with the original in only four. Two of the four papers are identified as replicable using the similar dataset under certain IoU values. No paper is identified as replicable using the new dataset. We offer observations on the causes of irreproducibility and irreplicability. All code and data are available on Codeocean at https://codeocean.com/capsule/6680116/tree.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "11359490",
                        "name": "Kehinde E. Ajayi"
                    },
                    {
                        "authorId": "2214808132",
                        "name": "Muntabhir Hasan Choudhury"
                    },
                    {
                        "authorId": "2528276",
                        "name": "S. Rajtmajer"
                    },
                    {
                        "authorId": "97569165",
                        "name": "Jian Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [1] first applied an FCN-based semantic segmentation method [28] to table structure extraction."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e0c97baa8e7b629a3d689e00aa21b94e2193766",
                "externalIds": {
                    "ArXiv": "2303.11615",
                    "DBLP": "journals/corr/abs-2303-11615",
                    "DOI": "10.48550/arXiv.2303.11615",
                    "CorpusId": 257636821
                },
                "corpusId": 257636821,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3e0c97baa8e7b629a3d689e00aa21b94e2193766",
                "title": "Robust Table Structure Recognition with Dynamic Queries Enhanced Detection Transformer",
                "abstract": "We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage dynamic queries enhanced DETR based separation line regression approach, named DQ-DETR, to predict separation lines from table images directly. Compared to Vallina DETR, we propose three improvements in DQ-DETR to make the two-stage DETR framework work efficiently and effectively for the separation line prediction task: 1) A new query design, named Dynamic Query, to decouple single line query into separable point queries which could intuitively improve the localization accuracy for regression tasks; 2) A dynamic queries based progressive line regression approach to progressively regressing points on the line which further enhances localization accuracy for distorted tables; 3) A prior-enhanced matching strategy to solve the slow convergence issue of DETR. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet, WTW and FinTabNet. Furthermore, we have validated the robustness and high localization accuracy of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2187553093",
                        "name": "Jiawei Wang"
                    },
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2181111465",
                        "name": "Mingze Li"
                    },
                    {
                        "authorId": "2145130513",
                        "name": "Zhengmao Sun"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In view of visual saliency, DeepDeSRT [38] and TableNet [31] are two rep-\nresentative works exploiting semantic segmentation to obtain table cell boundaries.",
                "With the fast-paced development of digital transformation, Table Structure Recognition (TSR) task, aiming at parsing table structure from a table image into machineinterpretable formats, which are often presented by both table cell physical coordinates [38, 31, 15, 41, 4, 34, 36, 45, 35, 23, 25] and their logical relationships [18, 47].",
                "In view of visual saliency, DeepDeSRT [38] and TableNet [31] are two rep-",
                "the filed, which can be mainly categorized into boundary extraction-based methods [38, 31, 15, 41, 25] and element relationship-based methods [23, 4, 34, 36] according to the component type leveraged.",
                "The first group are the single component-based methods, which rely on either boundary extraction [15, 25, 31, 38, 41] or element relationship [4, 23, 34, 36].",
                "To date, several pioneers works [18, 47, 38, 15, 41, 4, 34, 36, 45, 35, 23, 25] have achieved significant progress in"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2c267f43aca9bfb2f01e5400fca14545c4aa14b3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-09174",
                    "ArXiv": "2303.09174",
                    "DOI": "10.48550/arXiv.2303.09174",
                    "CorpusId": 257557431
                },
                "corpusId": 257557431,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c267f43aca9bfb2f01e5400fca14545c4aa14b3",
                "title": "Grab What You Need: Rethinking Complex Table Structure Recognition with Flexible Components Deliberation",
                "abstract": "Recently, Table Structure Recognition (TSR) task, aiming at identifying table structure into machine readable formats, has received increasing interest in the community. While impressive success, most single table component-based methods can not perform well on unregularized table cases distracted by not only complicated inner structure but also exterior capture distortion. In this paper, we raise it as Complex TSR problem, where the performance degeneration of existing methods is attributable to their inefficient component usage and redundant post-processing. To mitigate it, we shift our perspective from table component extraction towards the efficient multiple components leverage, which awaits further exploration in the field. Specifically, we propose a seminal method, termed GrabTab, equipped with newly proposed Component Deliberator. Thanks to its progressive deliberation mechanism, our GrabTab can flexibly accommodate to most complex tables with reasonable components selected but without complicated post-processing involved. Quantitative experimental results on public benchmarks demonstrate that our method significantly outperforms the state-of-the-arts, especially under more challenging scenes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216764712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2150155841",
                        "name": "Ming Gong"
                    },
                    {
                        "authorId": "47655556",
                        "name": "Bin Liu"
                    },
                    {
                        "authorId": "2164224877",
                        "name": "Yunfei Wu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "143900241",
                        "name": "Xing Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In recent years, inspired by the success of deep learning in various tasks, especially object detection and semantic segmentation, many deep learning-based methods (Raja et al., 2020; Schreiber et al., 2017) have been presented to recognize table structures.",
                "S. Schreiber et al. (Schreiber et al., 2017) proposed a two-fold system named DeepDeSRT that applies Faster RCNN (Ren et al., 2015) and FCN(Long et al., 2015) for both table detection and row/column segmentation.",
                "(Schreiber et al., 2017) proposed a two-fold system named DeepDeSRT that applies Faster RCNN (Ren et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "968b0d964e0639ae27a49b696cf11929313e9ff1",
                "externalIds": {
                    "ArXiv": "2303.08648",
                    "DBLP": "conf/visapp/LyT23",
                    "DOI": "10.5220/0011685000003417",
                    "CorpusId": 257360026
                },
                "corpusId": 257360026,
                "publicationVenue": {
                    "id": "3e53351c-7355-4159-a3f2-f3b03a3aa989",
                    "name": "VISIGRAPP",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Comput Vis Imaging Comput Graph Theory Appl",
                        "International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications"
                    ],
                    "issn": "2184-4321",
                    "alternate_issns": [
                        "2184-5921"
                    ],
                    "url": "http://www.visigrapp.org/"
                },
                "url": "https://www.semanticscholar.org/paper/968b0d964e0639ae27a49b696cf11929313e9ff1",
                "title": "An End-to-End Multi-Task Learning Model for Image-based Table Recognition",
                "abstract": "Image-based table recognition is a challenging task due to the diversity of table styles and the complexity of table structures. Most of the previous methods focus on a non-end-to-end approach which divides the problem into two separate sub-problems: table structure recognition; and cell-content recognition and then attempts to solve each sub-problem independently using two separate systems. In this paper, we propose an end-to-end multi-task learning model for image-based table recognition. The proposed model consists of one shared encoder, one shared decoder, and three separate decoders which are used for learning three sub-tasks of table recognition: table structure recognition, cell detection, and cell-content recognition. The whole system can be easily trained and inferred in an end-to-end approach. In the experiments, we evaluate the performance of the proposed model on two large-scale datasets: FinTabNet and PubTabNet. The experiment results show that the proposed model outperforms the state-of-the-art methods in all benchmark datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40325274",
                        "name": "N. Ly"
                    },
                    {
                        "authorId": "144752372",
                        "name": "A. Takasu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2021), many researchers only focused on the table structure recognition problem (Prasad et al., 2020; Raja et al., 2020; Schreiber et al., 2017).",
                "(Schreiber et al., 2017) proposed a two-fold system named DeepDeSRT that applies Faster RCNN (Ren et al.",
                "In recent years, motivated by the success of deep learning, especially in object detection and semantic segmentation, many deep learning-based methods (Prasad et al., 2020; Raja et al., 2020; Schreiber et al., 2017) have been proposed and proven to be powerful models for table structure recognition."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bc8fc30c2546979cfd7e1658b868fdbb02f9a22a",
                "externalIds": {
                    "ArXiv": "2303.07641",
                    "DBLP": "journals/corr/abs-2303-07641",
                    "DOI": "10.5220/0011682600003411",
                    "CorpusId": 257356700
                },
                "corpusId": 257356700,
                "publicationVenue": {
                    "id": "8ef5945c-5b25-4774-b55a-15cd5450f6e4",
                    "name": "International Conference on Pattern Recognition Applications and Methods",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Pattern Recognit Appl Method",
                        "ICPRAM"
                    ],
                    "url": "http://icpram.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bc8fc30c2546979cfd7e1658b868fdbb02f9a22a",
                "title": "Rethinking Image-based Table Recognition Using Weakly Supervised Methods",
                "abstract": "Most of the previous methods for table recognition rely on training datasets containing many richly annotated table images. Detailed table image annotation, e.g., cell or text bounding box annotation, however, is costly and often subjective. In this paper, we propose a weakly supervised model named WSTabNet for table recognition that relies only on HTML (or LaTeX) code-level annotations of table images. The proposed model consists of three main parts: an encoder for feature extraction, a structure decoder for generating table structure, and a cell decoder for predicting the content of each cell in the table. Our system is trained end-to-end by stochastic gradient descent algorithms, requiring only table images and their ground-truth HTML (or LaTeX) representations. To facilitate table recognition with deep learning, we create and release WikiTableSet, the largest publicly available image-based table recognition dataset built from Wikipedia. WikiTableSet contains nearly 4 million English table images, 590K Japanese table images, and 640k French table images with corresponding HTML representation and cell bounding boxes. The extensive experiments on WikiTableSet and two large-scale datasets: FinTabNet and PubTabNet demonstrate that the proposed weakly supervised model achieves better, or similar accuracies compared to the state-of-the-art models on all benchmark datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40325274",
                        "name": "N. Ly"
                    },
                    {
                        "authorId": "144752372",
                        "name": "A. Takasu"
                    },
                    {
                        "authorId": "2100790",
                        "name": "Phuc Nguyen"
                    },
                    {
                        "authorId": "2052440572",
                        "name": "H. Takeda"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, DeepDeSRT [40] and TableNet [32] adjusted FCN from the semantic segmentation to segment rows and columns."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5966cd3ac4460a97b27b4177a89c0d1041726881",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-06949",
                    "ArXiv": "2303.06949",
                    "DOI": "10.1109/CVPR52729.2023.01071",
                    "CorpusId": 257495840
                },
                "corpusId": 257495840,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5966cd3ac4460a97b27b4177a89c0d1041726881",
                "title": "Improving Table Structure Recognition with Visual-Alignment Sequential Coordinate Modeling",
                "abstract": "Table structure recognition aims to extract the logical and physical structure of unstructured table images into a machine-readable format. The latest end-to-end image-to-text approaches simultaneously predict the two structures by two decoders, where the prediction of the physical structure (the bounding boxes of the cells) is based on the representation of the logical structure. However, the previous methods struggle with imprecise bounding boxes as the logical representation lacks local visual information. To address this issue, we propose an end-to-end sequential modeling framework for table structure recognition called VAST. It contains a novel coordinate sequence decoder triggered by the representation of the non-empty cell from the logical structure decoder. In the coordinate sequence decoder, we model the bounding box coordinates as a language sequence, where the left, top, right and bottom coordinates are decoded sequentially to leverage the inter-coordinate dependency. Furthermore, we propose an auxiliary visual-alignment loss to enforce the logical representation of the non-empty cells to contain more local visual details, which helps produce better cell bounding boxes. Extensive experiments demonstrate that our proposed method can achieve state-of-the-art results in both logical and physical structure recognition. The ablation study also validates that the proposed coordinate sequence decoder and the visual-alignment loss are the keys to the success of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145769099",
                        "name": "Yongshuai Huang"
                    },
                    {
                        "authorId": "2054645836",
                        "name": "Ning Lu"
                    },
                    {
                        "authorId": "1679279",
                        "name": "Dapeng Chen"
                    },
                    {
                        "authorId": "2110470261",
                        "name": "Yibo Li"
                    },
                    {
                        "authorId": "2002678",
                        "name": "Zecheng Xie"
                    },
                    {
                        "authorId": "2901597",
                        "name": "Shenggao Zhu"
                    },
                    {
                        "authorId": "1777642",
                        "name": "Liangcai Gao"
                    },
                    {
                        "authorId": "2187312795",
                        "name": "Wei Peng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Limited by the training datasets [7, 2, 37, 36] used for TSR, most previous works [28, 24, 35, 26] focus on document images that are obtained from digital documents (e.",
                "Therefore, TSR as a precursor to contextual table understanding will be beneficial in a wide range of applications [27, 26].",
                "Distinct from previous segmentationbased methods [35, 26, 28, 20] in the \u201csplit\u201d stage, we aim to distinguish each table separation line and formulate table separation line detection as an instance segmentation task."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c78daabab3666d08d945098bc462f882b78803fd",
                "externalIds": {
                    "ArXiv": "2303.04384",
                    "DBLP": "journals/corr/abs-2303-04384",
                    "DOI": "10.48550/arXiv.2303.04384",
                    "CorpusId": 257405340
                },
                "corpusId": 257405340,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c78daabab3666d08d945098bc462f882b78803fd",
                "title": "SEMv2: Table Separation Line Detection Based on Conditional Convolution",
                "abstract": "Table structure recognition is an indispensable element for enabling machines to comprehend tables. Its primary purpose is to identify the internal structure of a table. Nevertheless, due to the complexity and diversity of their structure and style, it is highly challenging to parse the tabular data into a structured format that machines can comprehend. In this work, we adhere to the principle of the split-and-merge based methods and propose an accurate table structure recognizer, termed SEMv2 (SEM: Split, Embed and Merge). Unlike the previous works in the ``split'' stage, we aim to address the table separation line instance-level discrimination problem and introduce a table separation line detection strategy based on conditional convolution. Specifically, we design the ``split'' in a top-down manner that detects the table separation line instance first and then dynamically predicts the table separation line mask for each instance. The final table separation line shape can be accurately obtained by processing the table separation line mask in a row-wise/column-wise manner. To comprehensively evaluate the SEMv2, we also present a more challenging dataset for table structure recognition, dubbed iFLYTAB, which encompasses multiple style tables in various scenarios such as photos, scanned documents, etc. Extensive experiments on publicly available datasets (e.g. SciTSR, PubTabNet and iFLYTAB) demonstrate the efficacy of our proposed approach. The code and iFLYTAB dataset will be made publicly available upon acceptance of this paper.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2128660562",
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "authorId": "2067770685",
                        "name": "Pengfei Hu"
                    },
                    {
                        "authorId": "2143520841",
                        "name": "Jie Ma"
                    },
                    {
                        "authorId": "2149030934",
                        "name": "Jun Du"
                    },
                    {
                        "authorId": "39557762",
                        "name": "Jianshu Zhang"
                    },
                    {
                        "authorId": "2211065581",
                        "name": "Huihui Zhu"
                    },
                    {
                        "authorId": "2055464704",
                        "name": "Baocai Yin"
                    },
                    {
                        "authorId": "2185098372",
                        "name": "Bing Yin"
                    },
                    {
                        "authorId": "2108152462",
                        "name": "Cong Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Early works (Schreiber et al. 2017; Siddiqui et al. 2019) introduce segmentation or detection frameworks to locate and extract splitting lines of table rows and columns."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a90a77893708ae5070e2ed4572ca9e20837e69c6",
                "externalIds": {
                    "DBLP": "conf/aaai/XingGLBZLYY23",
                    "ArXiv": "2303.03730",
                    "DOI": "10.48550/arXiv.2303.03730",
                    "CorpusId": 257378294
                },
                "corpusId": 257378294,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a90a77893708ae5070e2ed4572ca9e20837e69c6",
                "title": "LORE: Logical Location Regression Network for Table Structure Recognition",
                "abstract": "Table structure recognition (TSR) aims at extracting tables in images into machine-understandable formats. Recent methods solve this problem by predicting the adjacency relations of detected cell boxes, or learning to generate the corresponding markup sequences from the table images. However, they either count on additional heuristic rules to recover the table structures, or require a huge amount of training data and time-consuming sequential decoders. In this paper, we propose an alternative paradigm. We model TSR as a logical location regression problem and propose a new TSR framework called LORE, standing for LOgical location REgression network, which for the first time combines logical location regression together with spatial location regression of table cells. Our proposed LORE is conceptually simpler, easier to train and more accurate than previous TSR models of other paradigms. Experiments on standard benchmarks demonstrate that LORE consistently outperforms prior arts. Code is available at https:// github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LORE-TSR.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2176894025",
                        "name": "Hangdi Xing"
                    },
                    {
                        "authorId": "2112256",
                        "name": "Feiyu Gao"
                    },
                    {
                        "authorId": "143799988",
                        "name": "Rujiao Long"
                    },
                    {
                        "authorId": "2064698184",
                        "name": "Jiajun Bu"
                    },
                    {
                        "authorId": "2114172383",
                        "name": "Qi Zheng"
                    },
                    {
                        "authorId": "2145730944",
                        "name": "Liangcheng Li"
                    },
                    {
                        "authorId": "2173739231",
                        "name": "Cong Yao"
                    },
                    {
                        "authorId": "2139424603",
                        "name": "Zhi Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[31] proposed DeepDeSRT that employs the Faster R-CNN model for table row and column detection followed by a semantic segmentation approach for TSR."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2b61d74b30df4034f6b3f8a2e9b8455a05a983cb",
                "externalIds": {
                    "DBLP": "journals/sncs/NamyslEBK23",
                    "DOI": "10.1007/s42979-022-01659-z",
                    "CorpusId": 257335296
                },
                "corpusId": 257335296,
                "publicationVenue": {
                    "id": "7a7dc89b-e1a6-44df-a496-46c330a87840",
                    "name": "SN Computer Science",
                    "type": "journal",
                    "alternate_names": [
                        "SN Comput Sci"
                    ],
                    "issn": "2661-8907",
                    "alternate_issns": [
                        "2662-995X"
                    ],
                    "url": "https://link.springer.com/journal/42979"
                },
                "url": "https://www.semanticscholar.org/paper/2b61d74b30df4034f6b3f8a2e9b8455a05a983cb",
                "title": "Flexible Hybrid Table Recognition and Semantic Interpretation System",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "134442417",
                        "name": "Marcin Namysl"
                    },
                    {
                        "authorId": "153486145",
                        "name": "Alexander M. Esser"
                    },
                    {
                        "authorId": "1699019",
                        "name": "Sven Behnke"
                    },
                    {
                        "authorId": "144306556",
                        "name": "J. K\u00f6hler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [18] split the competition dataset into a training set and a test set of just 34 samples.",
                "These developments have enabled significant advances in deep learning (DL) modeling for TE [18,16,25,21,13,12]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d943616a2b732c8b7cee18b44118db8d9a1af5e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-00716",
                    "ArXiv": "2303.00716",
                    "DOI": "10.48550/arXiv.2303.00716",
                    "CorpusId": 257255197
                },
                "corpusId": 257255197,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/d943616a2b732c8b7cee18b44118db8d9a1af5e8",
                "title": "Aligning benchmark datasets for table structure recognition",
                "abstract": "Benchmark datasets for table structure recognition (TSR) must be carefully processed to ensure they are annotated consistently. However, even if a dataset's annotations are self-consistent, there may be significant inconsistency across datasets, which can harm the performance of models trained and evaluated on them. In this work, we show that aligning these benchmarks$\\unicode{x2014}$removing both errors and inconsistency between them$\\unicode{x2014}$improves model performance significantly. We demonstrate this through a data-centric approach where we adopt one model architecture, the Table Transformer (TATR), that we hold fixed throughout. Baseline exact match accuracy for TATR evaluated on the ICDAR-2013 benchmark is 65% when trained on PubTables-1M, 42% when trained on FinTabNet, and 69% combined. After reducing annotation mistakes and inter-dataset inconsistency, performance of TATR evaluated on ICDAR-2013 increases substantially to 75% when trained on PubTables-1M, 65% when trained on FinTabNet, and 81% combined. We show through ablations over the modification steps that canonicalization of the table annotations has a significantly positive effect on performance, while other choices balance necessary trade-offs that arise when deciding a benchmark dataset's final composition. Overall we believe our work has significant implications for benchmark design for TSR and potentially other tasks as well. Dataset processing and training code will be released at https://github.com/microsoft/table-transformer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2128094499",
                        "name": "Robin Abraham"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4f5414f9b2873d6e4de1158695d7b1c6fe9653cb",
                "externalIds": {
                    "DOI": "10.1016/j.oregeorev.2023.105383",
                    "CorpusId": 257566076
                },
                "corpusId": 257566076,
                "publicationVenue": {
                    "id": "75e57f78-3c41-4803-8f91-bf6a220e9a66",
                    "name": "Ore Geology Reviews",
                    "type": "journal",
                    "alternate_names": [
                        "Ore Geol Rev"
                    ],
                    "issn": "0169-1368",
                    "url": "https://www.journals.elsevier.com/ore-geology-reviews",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/01691368"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4f5414f9b2873d6e4de1158695d7b1c6fe9653cb",
                "title": "Understanding table content for mineral exploration reports using deep learning and natural language processing",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210960579",
                        "name": "Jiahuizi Dong"
                    },
                    {
                        "authorId": "29847942",
                        "name": "Qinjun Qiu"
                    },
                    {
                        "authorId": "2153722665",
                        "name": "Zhong Xie"
                    },
                    {
                        "authorId": "1490932925",
                        "name": "K. Ma"
                    },
                    {
                        "authorId": "2056384890",
                        "name": "A. Hu"
                    },
                    {
                        "authorId": "2191360357",
                        "name": "Haitao Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0f4b62c2dcce219c92eb49ff0305864ea8eea15e",
                "externalIds": {
                    "DBLP": "journals/jimaging/KazdarMAAJA23",
                    "PubMedCentral": "10055874",
                    "DOI": "10.3390/jimaging9030062",
                    "CorpusId": 257479573,
                    "PubMed": "36976113"
                },
                "corpusId": 257479573,
                "publicationVenue": {
                    "id": "c0fc53c7-b0ed-487d-9191-1262c8322621",
                    "name": "Journal of Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "J Imaging"
                    ],
                    "issn": "2313-433X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-556372",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jimaging",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-556372"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0f4b62c2dcce219c92eb49ff0305864ea8eea15e",
                "title": "DCTable: A Dilated CNN with Optimizing Anchors for Accurate Table Detection",
                "abstract": "With the widespread use of deep learning in leading systems, it has become the mainstream in the table detection field. Some tables are difficult to detect because of the likely figure layout or the small size. As a solution to the underlined problem, we propose a novel method, called DCTable, to improve Faster R-CNN for table detection. DCTable came up to extract more discriminative features using a backbone with dilated convolutions in order to improve the quality of region proposals. Another main contribution of this paper is the anchors optimization using the Intersection over Union (IoU)-balanced loss to train the RPN and reduce the false positive rate. This is followed by a RoI Align layer, instead of the ROI pooling, to improve the accuracy during mapping table proposal candidates by eliminating the coarse misalignment and introducing the bilinear interpolation in mapping region proposal candidates. Training and testing on a public dataset showed the effectiveness of the algorithm and a considerable improvement of the F1-score on ICDAR 2017-Pod, ICDAR-2019, Marmot and RVL CDIP datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2185774122",
                        "name": "Takwa Kazdar"
                    },
                    {
                        "authorId": "51205936",
                        "name": "Wided Souid\u00e8ne Mseddi"
                    },
                    {
                        "authorId": "2629166",
                        "name": "M. Akhloufi"
                    },
                    {
                        "authorId": "2211336685",
                        "name": "Ala Agrebi"
                    },
                    {
                        "authorId": "2504338",
                        "name": "Marwa Jmal"
                    },
                    {
                        "authorId": "2066988903",
                        "name": "Rabah Attia"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "304b430fc030b1e1fc2f270e9c382e5c32812194",
                "externalIds": {
                    "ArXiv": "2302.11583",
                    "DBLP": "journals/corr/abs-2302-11583",
                    "DOI": "10.48550/arXiv.2302.11583",
                    "CorpusId": 257102474
                },
                "corpusId": 257102474,
                "publicationVenue": {
                    "id": "c79aa48a-5be0-4cc6-851b-549f1cf3df25",
                    "name": "International Journal on Digital Libraries",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Digit Libr"
                    ],
                    "issn": "1432-1300",
                    "url": "https://link.springer.com/journal/799"
                },
                "url": "https://www.semanticscholar.org/paper/304b430fc030b1e1fc2f270e9c382e5c32812194",
                "title": "The Digitization of Historical Astrophysical Literature with Highly-Localized Figures and Figure Captions",
                "abstract": "Scientific articles published prior to the\"age of digitization\"in the late 1990s contain figures which are\"trapped\"within their scanned pages. While progress to extract figures and their captions has been made, there is currently no robust method for this process. We present a YOLO-based method for use on scanned pages, after they have been processed with Optical Character Recognition (OCR), which uses both grayscale and OCR-features. We focus our efforts on translating the intersection-over-union (IOU) metric from the field of object detection to document layout analysis and quantify\"high localization\"levels as an IOU of 0.9. When applied to the astrophysics literature holdings of the NASA Astrophysics Data System (ADS), we find F1 scores of 90.9% (92.2%) for figures (figure captions) with the IOU cut-off of 0.9 which is a significant improvement over other state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145308380",
                        "name": "J. Naiman"
                    },
                    {
                        "authorId": "2246922800",
                        "name": "P. G. Williams"
                    },
                    {
                        "authorId": "2765398",
                        "name": "A. Goodman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4750b008727349147f4913a644f976168fa3e90b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-05658",
                    "ArXiv": "2302.05658",
                    "DOI": "10.48550/arXiv.2302.05658",
                    "CorpusId": 256827641
                },
                "corpusId": 256827641,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/4750b008727349147f4913a644f976168fa3e90b",
                "title": "DocILE Benchmark for Document Information Localization and Extraction",
                "abstract": "This paper introduces the DocILE benchmark with the largest dataset of business documents for the tasks of Key Information Localization and Extraction and Line Item Recognition. It contains 6.7k annotated business documents, 100k synthetically generated documents, and nearly~1M unlabeled documents for unsupervised pre-training. The dataset has been built with knowledge of domain- and task-specific aspects, resulting in the following key features: (i) annotations in 55 classes, which surpasses the granularity of previously published key information extraction datasets by a large margin; (ii) Line Item Recognition represents a highly practical information extraction task, where key information has to be assigned to items in a table; (iii) documents come from numerous layouts and the test set includes zero- and few-shot cases as well as layouts commonly seen in the training set. The benchmark comes with several baselines, including RoBERTa, LayoutLMv3 and DETR-based Table Transformer; applied to both tasks of the DocILE benchmark, with results shared in this paper, offering a quick starting point for future work. The dataset, baselines and supplementary material are available at https://github.com/rossumai/docile.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2203366733",
                        "name": "vStvep'an vSimsa"
                    },
                    {
                        "authorId": "2203364206",
                        "name": "Milan vSulc"
                    },
                    {
                        "authorId": "2205547051",
                        "name": "Michal Uvrivc'avr"
                    },
                    {
                        "authorId": "1699597533",
                        "name": "Yash J. Patel"
                    },
                    {
                        "authorId": "2924500",
                        "name": "Ahmed Hamdi"
                    },
                    {
                        "authorId": "2208709174",
                        "name": "Matvej Koci'an"
                    },
                    {
                        "authorId": "2203364204",
                        "name": "Maty'avs Skalick'y"
                    },
                    {
                        "authorId": "2000570582",
                        "name": "Jivr'i Matas"
                    },
                    {
                        "authorId": "2174737970",
                        "name": "Antoine Doucet"
                    },
                    {
                        "authorId": "1732746",
                        "name": "Micka\u00ebl Coustaty"
                    },
                    {
                        "authorId": "1694974",
                        "name": "Dimosthenis Karatzas"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6647af89d474405b4d0e9d1ee7bf9dd5fd8d4c09",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-10781",
                    "ArXiv": "2301.10781",
                    "DOI": "10.48550/arXiv.2301.10781",
                    "CorpusId": 256274582
                },
                "corpusId": 256274582,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6647af89d474405b4d0e9d1ee7bf9dd5fd8d4c09",
                "title": "Generalizability in Document Layout Analysis for Scientific Article Figure & Caption Extraction",
                "abstract": "The lack of generalizability -- in which a model trained on one dataset cannot provide accurate results for a different dataset -- is a known problem in the field of document layout analysis. Thus, when a model is used to locate important page objects in scientific literature such as figures, tables, captions, and math formulas, the model often cannot be applied successfully to new domains. While several solutions have been proposed, including newer and updated deep learning models, larger hand-annotated datasets, and the generation of large synthetic datasets, so far there is no\"magic bullet\"for translating a model trained on a particular domain or historical time period to a new field. Here we present our ongoing work in translating our document layout analysis model from the historical astrophysical literature to the larger corpus of scientific documents within the HathiTrust U.S. Federal Documents collection. We use this example as an avenue to highlight some of the problems with generalizability in the document layout analysis community and discuss several challenges and possible solutions to address these issues. All code for this work is available on The Reading Time Machine GitHub repository (https://github.com/ReadingTimeMachine/htrc_short_conf).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145308380",
                        "name": "J. Naiman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several deep learning-based methods and datasets from the literature (TabStructNet [17], TableNet [14], CDeCNet [1], UNLV [19], DeepDeSRT [18], TableBank [11], PubTabNet [23], ICDAR 2013 Table Competition [9], DeepFigures [20], PubLayNet [24]) explore solutions to table detection and table structure recognition tasks."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "380691f141c4392c6728fb3bf9be6c0d0ec9777b",
                "externalIds": {
                    "DBLP": "conf/comad/SripathyKS23",
                    "DOI": "10.1145/3570991.3571037",
                    "CorpusId": 255416707
                },
                "corpusId": 255416707,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/380691f141c4392c6728fb3bf9be6c0d0ec9777b",
                "title": "BUDDI Table Factory: A toolbox for generating synthetic documents with annotated tables and cells",
                "abstract": "Tables are the most convenient way to represent structured information in a document. Understanding the table structure is critical to understanding its contents. Several deep learning-based approaches from the literature have shown promising results in understanding table structures, but they require large amounts of annotated data. However, the availability of annotated datasets to train these methods are expensive, laborious, and very limited. Moreover, human-annotated data suffers from inconsistencies in table and cell annotations. We propose BUDDI Table Factory (BTF) for synthetically generating annotated documents with a wide range of variations in table structures. We propose a heuristics-based method to generate a variety of table structures from which we generate synthetic documents using LaTeX. We propose a computer vision-based approach to localize table and cell regions and automatically generate annotations in PASCAL VOC challenge format. We empirically illustrate the advantage of adding synthetic BTF documents with limited original documents to the model training, which can significantly improve the TEDS and IoU performance of the table structure recognition tasks in public and real-world healthcare datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2199253362",
                        "name": "Bharath Sripathy"
                    },
                    {
                        "authorId": "2176184223",
                        "name": "Harinath Krishnamoorthy"
                    },
                    {
                        "authorId": "35628253",
                        "name": "Sudarsun Santhiappan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [3] uses the fuzzy cognitive network (FCN) [18] to split the table structure based on semantic segmentation."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ca2fabed8604b296d713262794a427e5c4b51ffa",
                "externalIds": {
                    "DBLP": "journals/apin/WanZLZS23",
                    "DOI": "10.1007/s10489-022-04420-4",
                    "CorpusId": 255362168
                },
                "corpusId": 255362168,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ca2fabed8604b296d713262794a427e5c4b51ffa",
                "title": "Contextual transformer sequence-based recognition network for medical examination reports",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152912519",
                        "name": "Honglin Wan"
                    },
                    {
                        "authorId": "2199057485",
                        "name": "Zongfeng Zhong"
                    },
                    {
                        "authorId": "2263987",
                        "name": "Tianping Li"
                    },
                    {
                        "authorId": "2856513",
                        "name": "Huaxiang Zhang"
                    },
                    {
                        "authorId": "51299154",
                        "name": "Jiande Sun"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ae6e34d182fbc42bc99ba609369a3b7645d205cb",
                "externalIds": {
                    "DBLP": "journals/prl/WangXZJ23",
                    "DOI": "10.1016/j.patrec.2022.12.014",
                    "CorpusId": 255089449
                },
                "corpusId": 255089449,
                "publicationVenue": {
                    "id": "f35e3e87-9df4-497b-aa0d-bb8584197290",
                    "name": "Pattern Recognition Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit Lett"
                    ],
                    "issn": "0167-8655",
                    "url": "https://www.journals.elsevier.com/pattern-recognition-letters/",
                    "alternate_urls": [
                        "http://www.journals.elsevier.com/pattern-recognition-letters/",
                        "http://www.sciencedirect.com/science/journal/01678655"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ae6e34d182fbc42bc99ba609369a3b7645d205cb",
                "title": "Scene table structure recognition with segmentation collaboration and alignment",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109799388",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "46364544",
                        "name": "Yang Xue"
                    },
                    {
                        "authorId": "2118131879",
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "authorId": "144838978",
                        "name": "Lianwen Jin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e604f4ec9863e900fda0dd0a81b60362f220266c",
                "externalIds": {
                    "DBLP": "journals/tcad/FayaziCYBAD22",
                    "DOI": "10.1109/TCAD.2022.3158073",
                    "CorpusId": 247426766
                },
                "corpusId": 247426766,
                "publicationVenue": {
                    "id": "e86c30b0-c1dd-4f0e-be5e-22af711f7d5f",
                    "name": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Comput Des Integr Circuit Syst"
                    ],
                    "issn": "0278-0070",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=43",
                    "alternate_urls": [
                        "http://ieee-ceda.org/publications/tcad",
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=43"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e604f4ec9863e900fda0dd0a81b60362f220266c",
                "title": "FASCINET: A Fully Automated Single-Board Computer Generator Using Neural Networks",
                "abstract": "Designing single-board computers (SBCs) is becoming more challenging given the growing number of discrete components that are made available and the rate at which this number grows. Keeping track of all available components options, revisions, and functionalities is challenging for SBC designers who are striving for faster design cycles. Moreover, the procedure of deciding peripheral components, their values, and connections of an SBC is not only difficult because of various parameters that need to be considered but also is time consuming as there exist numerous components on a typical SBC nowadays. In this article, an SBC generator tool, FASCINET, is presented that uses a neural network (NN) model to design customized peripheral circuits for SBCs. The tool creates a large commercial off-the-shelf database (COTS DB) of existing components, efficiently searches through them, and selects optimal components for both main and peripheral components based on the user\u2019s requirements. Creating such a broad COTS DB requires processing abundant datasheets. A manual approach is time consuming, even if only a fraction of all available datasheets is considered. In order to automate this process, this article describes a novel NN-based approach for automatically categorizing datasheets and proposes an extraction technique for parsing relevant functional information from tables within. Our evaluation using a test set that contains over 770 000 components shows that the category of datasheets is identified correctly over 95% of the time. Additionally, the table extractor has a precision above 96%. Our proposed fully autonomous SBC design approach reduces the time for generating the schematic of an SBC to as little as 2 min. For validating the accuracy of our model, the netlists of 400 SBCs designed by FASCINET are compared to the human-designed versions. This evaluation shows that FASCINET is able to design SBCs that are identical to the manually designed ones except for minor differences.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "33935286",
                        "name": "Morteza Fayazi"
                    },
                    {
                        "authorId": "2087607913",
                        "name": "Zachary Colter"
                    },
                    {
                        "authorId": "7936000",
                        "name": "Z. Youbi"
                    },
                    {
                        "authorId": "27730985",
                        "name": "Javad Bagherzadeh"
                    },
                    {
                        "authorId": "35554623",
                        "name": "T. Ajayi"
                    },
                    {
                        "authorId": "1793651",
                        "name": "R. Dreslinski"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For table region detection, DeepDeSRT [34] built an end-to-end system based on deep learning for table detection in image documents, which can identify the row, column, and cell positions in detected tables without relying on any heuristics or metadata."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "12966e3481f135d5e0a682298047706eec1e4143",
                "externalIds": {
                    "DBLP": "conf/hpcc/TongSKN22",
                    "DOI": "10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00326",
                    "CorpusId": 257808822
                },
                "corpusId": 257808822,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/12966e3481f135d5e0a682298047706eec1e4143",
                "title": "CFCT: The cell function classification method for complex tables",
                "abstract": "Tables are one of the most common approaches to organizing, presenting and managing data. However, these tables are often designed to be user-friendly, making it easier for people to find and compare data. It is difficult for machines to understand the contents of tables automatically. Moreover, the existing works on table cell classification focus on relational tables, which ignore the other complex table data structures, and are poorly generalized. To address these issues, we propose a cell classification method for complex tables called CFCT, which considers both the textual content, stylistic features, and numerical content of cells. Moreover, CFCT uses the auto-encoder to extract the cell embeddings and introduces the adversarial training in the auto-encoder to obtain more representative cell embedding. Then, we utilize a transformer-based model to classify the functional roles of cells in complex tables, which can mine the structural information of the table data in a more parallel way. To demonstrate the effectiveness of our approach, we evaluate the performance of the CFCT on three datasets, DeEx, SAUS, and CIUS. The experimental results show that CFCT obtained a 3.6% improvement in terms of F1 score compared with the state-of-the-art deep learning method. The comprehensive experiments demonstrate that our CFCT is effective for the functional classification of cells in complex tables. More importantly, we find that adversarial training can fuse multiple table information to produce more accurate features.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2212857883",
                        "name": "Sainan Tong"
                    },
                    {
                        "authorId": "144986257",
                        "name": "Derong Shen"
                    },
                    {
                        "authorId": "1696338",
                        "name": "Yue Kou"
                    },
                    {
                        "authorId": "144856221",
                        "name": "Tiezheng Nie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[14] brought forward a novel deep learning based solution not only for table detection but also would recognize the structure of the tables as two separate parts for born- digital PDFs and scanned documents."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1279d1090eed92a8259086c3124853403e328554",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-14607",
                    "ArXiv": "2211.14607",
                    "DOI": "10.48550/arXiv.2211.14607",
                    "CorpusId": 254044385
                },
                "corpusId": 254044385,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1279d1090eed92a8259086c3124853403e328554",
                "title": "Sketch2FullStack: Generating Skeleton Code of Full Stack Website and Application from Sketch using Deep Learning and Computer Vision",
                "abstract": "\u2014For a full-stack web or app development, it requires a software \ufb01rm or more speci\ufb01cally a team of experienced developers to contribute a large portion of their time and resources to design the website and then convert it to code. As a result, the ef\ufb01ciency of the development team is signi\ufb01cantly reduced when it comes to converting UI wireframes and database schemas into an actual working system. It would save valuable resources and fasten the overall work\ufb02ow if the clients or developers can automate this process of converting the pre-made full-stack website design to get a partially working if not fully working code. In this paper, we present a novel approach of generating the skeleton code from sketched images using Deep Learning and Computer Vision approaches. The dataset for training are \ufb01rst-hand sketched images of low \ufb01delity wireframes, database schemas and class diagrams. The approach consists of three parts. First, the front-end or UI elements detection and extraction from custom-made UI wireframes. Second, individual database table creation from schema designs and lastly, creating a class \ufb01le from class diagrams.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192606835",
                        "name": "Somoy Subandhu Barua"
                    },
                    {
                        "authorId": "2192607050",
                        "name": "Imam Mohammad Zulkarnain"
                    },
                    {
                        "authorId": "2153931746",
                        "name": "Abhishek Roy"
                    },
                    {
                        "authorId": "2401685",
                        "name": "Md. Golam Rabiul Alam"
                    },
                    {
                        "authorId": "3241032",
                        "name": "Md. Zia Uddin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In DTE, there are three main cases: TD predicts bounding boxes of table candidates on document images (Shafait and Smith, 2010; Schreiber et al., 2017; Siddiqui et al., 2018; Paliwal et al., 2019; Prasad et al., 2020; Hashmi et al., 2021b; Zheng et al., 2021); TD searches for text lines signaling table candidates in plain-text (Hu et al.",
                "In this scenario, TSR usually deals with tables which are subareas of document images (Zuyev, 1997; Schreiber et al., 2017; Siddiqui et al., 2019a, 2019b; Zheng et al., 2021; Hashmi et al., 2021c), lines of plain-text (Kieninger, 1998; Ng et al.",
                "In DTE, there are three main cases: TD predicts bounding boxes of table candidates on document images (Shafait and Smith, 2010; Schreiber et al., 2017; Siddiqui et al., 2018; Paliwal et al., 2019; Prasad et al., 2020; Hashmi et al., 2021b; Zheng et al., 2021); TD searches for text lines signaling\u2026",
                "In this scenario, TSR usually deals with tables which are subareas of document images (Zuyev, 1997; Schreiber et al., 2017; Siddiqui et al., 2019a, 2019b; Zheng et al., 2021; Hashmi et al., 2021c), lines of plain-text (Kieninger, 1998; Ng et al., 1999; Hu et al., 2000), or text chunks and rulings\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "cf5f42b60003f9fc11d14a6eb126130411f861a4",
                "externalIds": {
                    "DBLP": "journals/widm/Shigarov23",
                    "DOI": "10.1002/widm.1482",
                    "CorpusId": 253823145
                },
                "corpusId": 253823145,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cf5f42b60003f9fc11d14a6eb126130411f861a4",
                "title": "Table understanding: Problem overview",
                "abstract": "Tables are probably the most natural way to represent relational data in various media and formats. They store a large number of valuable facts that could be utilized for question answering, knowledge base population, natural language generation, and other applications. However, many tables are not accompanied by semantics for the automatic interpretation of the information they present. Table Understanding (TU) aims at recovering the missing semantics that enables the extraction of facts from tables. This problem covers a range of issues from table detection in document images to semantic table interpretation with the help of external knowledge bases. To date, the TU research has been ongoing on for 30\u2009years. Nevertheless, there is no common point of view on the scope of TU; the terminology still needs agreement and unification. In recent years, science and technology have shown a rapidly increasing interest in TU. Nowadays, it is especially important to check the meaning of this research problem once again. This article gives a comprehensive characterization of the TU problem, including a description of its subproblems, tasks, subtasks, and applications. It also discusses the common limitations used in the existing problem statements and proposes some directions for further research that would help overcome the corresponding limitations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3355714",
                        "name": "A. Shigarov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "S Schreiber[20] ICDAR2013 Mask R-CNN Precision 97.",
                "As a result, the network misses out on critical visual cues that may aid in the detection and recognition of tables [20].",
                "Deep learning models are now widely used in many aspects of computer vision, including general table detection[20, 21, 22, 23, 24].",
                "S Schreiber[20] transfer learning methods + Faster R-CNN end-to-end strategy for detecting tables and table structures that is straightforward and efficient When compared to other state-of-the-art techniques, it is less accurate.",
                "[20] were the first to perform table detection and structure recognition using Faster RCNN.",
                "S Schreiber[20] ICDAR2013 Fully CNN Precision 95.",
                "Sebastian Schreiber et al. [20] were the first to perform table detection and structure recognition using Faster RCNN."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a81f504a1a101b9c6fe4a88957dd9ec450cf389d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-08469",
                    "ArXiv": "2211.08469",
                    "DOI": "10.48550/arXiv.2211.08469",
                    "CorpusId": 253553399
                },
                "corpusId": 253553399,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a81f504a1a101b9c6fe4a88957dd9ec450cf389d",
                "title": "Deep learning for table detection and structure recognition: A survey",
                "abstract": "Tables are everywhere, from scientific journals, papers, websites, and newspapers all the way to items we buy at the supermarket. Detecting them is thus of utmost importance to automatically understanding the content of a document. The performance of table detection has substantially increased thanks to the rapid development of deep learning networks. The goals of this survey are to provide a profound comprehension of the major developments in the field of Table Detection, offer insight into the different methodologies, and provide a systematic taxonomy of the different approaches. Furthermore, we provide an analysis of both classic and new applications in the field. Lastly, the datasets and source code of the existing models are organized to provide the reader with a compass on this vast literature. Finally, we go over the architecture of utilizing various object detection and table structure recognition methods to create an effective and efficient system, as well as a set of development trends to keep up with state-of-the-art algorithms and future research. We have also set up a public GitHub repository where we will be updating the most recent publications, open data, and source code. The GitHub repository is available at https://github.com/abdoelsayed2016/ table-detection-structure-recognition. Preprint submitted to Elsevier November 17, 2022 ar X iv :2 21 1. 08 46 9v 1 [ cs .C V ] 1 5 N ov 2 02 2",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "87034581",
                        "name": "M. Kasem"
                    },
                    {
                        "authorId": "115665360",
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "authorId": "2115948777",
                        "name": "Alexander Berendeyev"
                    },
                    {
                        "authorId": "2190955162",
                        "name": "Ebrahem Elkady"
                    },
                    {
                        "authorId": "2190955581",
                        "name": "Mahmoud Abdalla"
                    },
                    {
                        "authorId": "2163883464",
                        "name": "Mohamed Mahmoud"
                    },
                    {
                        "authorId": "2057870328",
                        "name": "Mohamed Hamada"
                    },
                    {
                        "authorId": "3253700",
                        "name": "D. Nurseitov"
                    },
                    {
                        "authorId": "1405340824",
                        "name": "I. Taj-Eddin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "299a74663a5230545ba9549b226d7116d1a31b43",
                "externalIds": {
                    "DOI": "10.23919/APSIPAASC55919.2022.9980172",
                    "CorpusId": 254930638
                },
                "corpusId": 254930638,
                "publicationVenue": {
                    "id": "5b924e1a-30f3-4275-bdb8-5a15517c0fde",
                    "name": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Asia-pacific Signal Inf Process Assoc Annu Summit Conf",
                        "APSIPA"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/299a74663a5230545ba9549b226d7116d1a31b43",
                "title": "Table Structure Recognition Based on Grid Shape Graph",
                "abstract": "Since tables in documents provide important information in compact form, table understanding has been an essential topic in document image processing. Researchers represented table structures in various formats for table understanding, such as simple grid structure, a graph with text/cell boxes as nodes, or a sequence of HTML tokens. However, these approaches have difficulties in handling regularities, e.g., global row and column information, and spanning cells simultaneously. In this paper, we propose a new table recognition method based on a grid shape graph and present grid localization and grid elements grouping networks. This approach is designed to exploit the grid structure and deal with spanning cells. To convert grid structure into cell structure, we only have to test adjacent pairs of grid elements, enabling efficient inference. In addition, we have discovered that predicting row/column-based relationships between grid elements improve cell-based connectivity estimation performance. We demonstrate the effectiveness of the proposed method through experiments on three benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115242015",
                        "name": "Eunji Lee"
                    },
                    {
                        "authorId": "151053767",
                        "name": "Junhyeong Kwon"
                    },
                    {
                        "authorId": "2109472075",
                        "name": "Haeyoon Yang"
                    },
                    {
                        "authorId": "2370571",
                        "name": "Jaewoo Park"
                    },
                    {
                        "authorId": "2144550201",
                        "name": "Soonyoung Lee"
                    },
                    {
                        "authorId": "2463454",
                        "name": "H. Koo"
                    },
                    {
                        "authorId": "1707645",
                        "name": "N. Cho"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This can further be split into a problem of detecting rows and columns and then combined at a later stage to obtain respective cells [5].",
                "[5] published an in-depth picture-based study program that examined the task of tabular structure extraction."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e35ef6f4cfdc2319a641735608562d3886fa9fdd",
                "externalIds": {
                    "DOI": "10.1109/ICCCIS56430.2022.10037664",
                    "CorpusId": 256743427
                },
                "corpusId": 256743427,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e35ef6f4cfdc2319a641735608562d3886fa9fdd",
                "title": "Using CoordConv for Tabular Data Detection and Structure Recognition",
                "abstract": "This paper explores the usage of CoordConv, the novel upgrade to general convolutional layers in the problem of Tabular Data Detection and Cell-Based Structure Recognition. CoordConv has been shown to provide considerably better results in the domain of Object Detection than its counterpart. The authors integrate it within the established Anchor optimization approach which leverages guided anchors to accomplish the task of recognizing rows and columns present in tabular data. In contrast to the majority of techniques implemented for Table Structure Recognition, the authors attempt to recognize the cells present in the tabular images instead of the rows and columns. They evaluate this method on the coveted ICDAR-19 dataset (International Conference on Document Analysis and Recognition - 2019) which comprises of scanned document images containing tabular regions and achieve results surpassing those of many popular techniques. They also apply this approach for the task of Table Detection and achieve results comparable to other established techniques.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2205330992",
                        "name": "Apoorva Ambulgekar"
                    },
                    {
                        "authorId": "2205330976",
                        "name": "Naman Lad"
                    },
                    {
                        "authorId": "2183415732",
                        "name": "Krunal Doshi"
                    },
                    {
                        "authorId": "2128946657",
                        "name": "Pranit Bari"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [11] is a typical top-down approach that uses a segmentation method to segment table columns and table rows directly, which can be implemented by Fully Convolution Networks."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "769935b79b9aa7c9e04406a1e2025005cd44b030",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-02128",
                    "ArXiv": "2211.02128",
                    "DOI": "10.1109/GLOBECOM48099.2022.10001041",
                    "CorpusId": 253370721
                },
                "corpusId": 253370721,
                "publicationVenue": {
                    "id": "b189dec0-41d0-4cea-a906-7c5186895904",
                    "name": "Global Communications Conference",
                    "type": "conference",
                    "alternate_names": [
                        "GLOBECOM",
                        "Glob Commun Conf"
                    ],
                    "url": "http://www.ieee-globecom.org/"
                },
                "url": "https://www.semanticscholar.org/paper/769935b79b9aa7c9e04406a1e2025005cd44b030",
                "title": "Efficient Information Sharing in ICT Supply Chain Social Network via Table Structure Recognition",
                "abstract": "The global Information and Communications Technology (ICT) supply chain is a complex network consisting of all types of participants. It is often formulated as a Social Network to discuss the supply chain network's relations, properties, and development in supply chain management. Information sharing plays a crucial role in improving the efficiency of the supply chain, and datasheets are the most common data format to describe e-component commodities in the ICT supply chain because of human readability. However, with the surging number of electronic documents, it has been far beyond the capacity of human readers, and it is also challenging to process tabular data automatically because of the complex table structures and heterogeneous layouts. Table Structure Recognition (TSR) aims to represent tables with complex structures in a machine-interpretable format so that the tabular data can be processed automatically. In this paper, we formulate TSR as an object detection problem and propose to generate an intuitive representation of a complex table structure to enable structuring of the tabular data related to the commodities. To cope with border-less and small layouts, we propose a cost-sensitive loss function by considering the detection difficulty of each class. Besides, we propose a novel anchor generation method using the character of tables that columns in a table should share an identical height, and rows in a table should share the same width. We implement our proposed method based on Faster-RCNN and achieve 94.79% on mean Average Precision (AP), and consistently improve more than 1.5% AP for different benchmark models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146517209",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "2130012695",
                        "name": "Yakup Akkaya"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "525c0df4767d10165f172bf5c2259b01840379b0",
                "externalIds": {
                    "DOI": "10.1016/j.nucengdes.2022.111975",
                    "CorpusId": 252634797
                },
                "corpusId": 252634797,
                "publicationVenue": {
                    "id": "26378f21-f2eb-464e-a5c1-6806942f4e7c",
                    "name": "Nuclear Engineering and Design",
                    "type": "journal",
                    "alternate_names": [
                        "Nucl Eng Des"
                    ],
                    "issn": "0029-5493",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/505661/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00295493",
                        "https://www.journals.elsevier.com/nuclear-engineering-and-design"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/525c0df4767d10165f172bf5c2259b01840379b0",
                "title": "Automatic recognition system for document digitization in nuclear power plants",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1753041373",
                        "name": "Elisa Ou"
                    },
                    {
                        "authorId": "2110188342",
                        "name": "Minhee Kim"
                    },
                    {
                        "authorId": "31888158",
                        "name": "Po-Ling Loh"
                    },
                    {
                        "authorId": "49973083",
                        "name": "Todd R. Allen"
                    },
                    {
                        "authorId": "51940545",
                        "name": "R. Agasie"
                    },
                    {
                        "authorId": "2478267",
                        "name": "Kaibo Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [24] is simple and effective end-to-end approach to detect tables but is not as accurate as compared to other state-of-the-art approaches [12]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bc8abf90ebc6d0e463bd0ad9952a437d5fea159b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-17246",
                    "ArXiv": "2210.17246",
                    "DOI": "10.1007/s10032-022-00420-9",
                    "CorpusId": 253193064
                },
                "corpusId": 253193064,
                "publicationVenue": {
                    "id": "46722266-4327-4a22-871d-5a9fe3c5ad0c",
                    "name": "International Journal on Document Analysis and Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Doc Anal Recognit"
                    ],
                    "issn": "1433-2825",
                    "url": "https://link.springer.com/journal/10032"
                },
                "url": "https://www.semanticscholar.org/paper/bc8abf90ebc6d0e463bd0ad9952a437d5fea159b",
                "title": "Tables to LaTeX: structure and content extraction from scientific tables",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "117769145",
                        "name": "Pratik Kayal"
                    },
                    {
                        "authorId": "2106415501",
                        "name": "Mrinal Anand"
                    },
                    {
                        "authorId": "73654757",
                        "name": "Harsh Desai"
                    },
                    {
                        "authorId": "145431050",
                        "name": "Mayank Singh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "19df2cdd53e03ad58d22fc4d16e2be5fa34c9c5a",
                "externalIds": {
                    "DBLP": "conf/assets/Schmitt-Koopmann22",
                    "DOI": "10.1145/3517428.3550407",
                    "CorpusId": 253064385
                },
                "corpusId": 253064385,
                "publicationVenue": {
                    "id": "6864fc0f-dd47-4798-a829-ac53dd78862f",
                    "name": "International ACM SIGACCESS Conference on Computers and Accessibility",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Computers and Accessibility",
                        "Conf Comput Access",
                        "Int ACM SIGACCESS Conf Comput Access",
                        "ASSETS"
                    ],
                    "url": "https://dl.acm.org/conference/assets"
                },
                "url": "https://www.semanticscholar.org/paper/19df2cdd53e03ad58d22fc4d16e2be5fa34c9c5a",
                "title": "Accessible PDFs: Applying Artificial Intelligence for Automated Remediation of STEM PDFs",
                "abstract": "People with visual impairments use assistive technology, e.g., screen readers, to navigate and read PDFs. However, such screen readers need extra information about the logical structure of the PDF, such as the reading order, header levels, and mathematical formulas, described in readable form to navigate the document in a meaningful way. This logical structure can be added to a PDF with tags. Creating tags for a PDF is time-consuming, and requires awareness and expert knowledge. Hence, most PDFs are left untagged, and as a result, they are poorly readable or unreadable for people who rely on screen readers. STEM documents are particularly problematic with their complex document structure and complicated mathematical formulae. These inaccessible PDFs present a major barrier for people with visual impairments wishing to pursue studies or careers in STEM fields, who cannot easily read studies and publications from their field. The goal of this Ph.D. is to apply artificial intelligence for document analysis to reasonably automate the remediation process of PDFs and present a solution for large mathematical formulae accessibility in PDFs. With these new methods, the Ph.D. research aims to lower barriers to creating accessible scientific PDFs, by reducing the time, effort, and expertise necessary to do so, ultimately facilitating greater access to scientific documents for people with visual impairments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2188574207",
                        "name": "Felix M. Schmitt-Koopmann"
                    },
                    {
                        "authorId": "2183442974",
                        "name": "Elaine M. Huang"
                    },
                    {
                        "authorId": "2934916",
                        "name": "Alireza Darvishy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The original dataset suffered from incorrect ground truth annotations, hence we list the corrected version of the dataset from [24], which has 1967 images, in Table 2.",
                "[24] also apply Faster R-CNN [13] for detecting tables in scanned document images."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4960e24f6cfab1afd36b7e89c5d88d107ba55863",
                "externalIds": {
                    "DOI": "10.3390/app122010578",
                    "CorpusId": 253075939
                },
                "corpusId": 253075939,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4960e24f6cfab1afd36b7e89c5d88d107ba55863",
                "title": "Rethinking Learnable Proposals for Graphical Object Detection in Scanned Document Images",
                "abstract": "In the age of deep learning, researchers have looked at domain adaptation under the pre-training and fine-tuning paradigm to leverage the gains in the natural image domain. These backbones and subsequent networks are designed for object detection in the natural image domain. They do not consider some of the critical characteristics of document images. Document images are sparse in contextual information, and the graphical page objects are logically clustered. This paper investigates the effectiveness of deep and robust backbones in the document image domain. Further, it explores the idea of learnable object proposals through Sparse R-CNN. This paper shows that simple domain adaptation of top-performing object detectors to the document image domain does not lead to better results. Furthermore, empirically showing that detectors based on dense object priors like Faster R-CNN, Mask R-CNN, and Cascade Mask R-CNN are perhaps not best suited for graphical page object detection. Detectors that reduce the number of object candidates while making them learnable are a step towards a better approach. We formulate and evaluate the Sparse R-CNN (SR-CNN) model on the IIIT-AR-13k, PubLayNet, and DocBank datasets and hope to inspire a rethinking of object proposals in the domain of graphical page object detection.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1576277442",
                        "name": "Sankalp Sinha"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1771057",
                        "name": "A. Pagani"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e73eb7777fc650405103658a0cf966d25118f32",
                "externalIds": {
                    "DOI": "10.1109/TSSA56819.2022.10063909",
                    "CorpusId": 257586166
                },
                "corpusId": 257586166,
                "publicationVenue": {
                    "id": "1eeaf3cb-1484-458d-b20f-72c0d39e4394",
                    "name": "International Conference on Telecommunication Systems, Services, and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "TSSA",
                        "Int Conf Telecommun Syst Serv Appl"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0e73eb7777fc650405103658a0cf966d25118f32",
                "title": "Table Information Extraction Using Data Augmentation on Deep Learning and Image Processing",
                "abstract": "Generally, the extraction of information in the table is done quickly if the table is within a document with a tabular structure. However, in the case of tables presented in the image document, steps are needed first to detect the table. Seeing a table in image documents becomes more complicated if the table to be seen does not have clear boundaries. This research focuses on extracting information from borderless tables in image documents. The study applies the Mask RCNN-FPN deep learning model to detect borderless tables using augmentation data. The use of data augmentation is expected to increase the accuracy of deep learning models even though there is only a small amount of training data available. The data augmentation technique proposed in this study is a fine-tuning method with CutMask augmentation data. For model formation and testing, this study uses the UNLV data set. This data set consists of scanned images of documents from various sources, including financial reports, journals, and different tabular research papers. The total amount of data used is 427 samples. After data augmentation, the amount of data used is 854 samples. The table detection model is based on the Mask RCNN created using Python. The testing parameters used for table detection quality are precise detection, partial detection, false detection, Precision, Recall, and F-Measure. The table's structure recognition rate is measured from the detection intersection value, rows, columns, and cells compared to ground truth. The test results show that using data augmentation with the CutMask technique can improve the performance of deep learning models to detect borderless tables. The use of image processing is shown to enhance table segmentation. However, the table structure recognition result does not offer a good result compared to the effects of other research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2211944553",
                        "name": "Izuardo Zulkarnain"
                    },
                    {
                        "authorId": "147946906",
                        "name": "Rin Rin Nurmalasari"
                    },
                    {
                        "authorId": "3273275",
                        "name": "F. N. Azizah"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar to previous works [8, 38, 46, 49, 51], the text content and structure information are obtained separately.",
                "The first group of methods [49, 51, 52] starts by detecting the row and columns of a"
            ],
            "intents": [
                "result",
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4ebf44ebb9634cb5c60f5dc7fab0c0f84aed0939",
                "externalIds": {
                    "DBLP": "conf/mm/LiL0LCNPL22",
                    "DOI": "10.1145/3503161.3547885",
                    "CorpusId": 252782335
                },
                "corpusId": 252782335,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4ebf44ebb9634cb5c60f5dc7fab0c0f84aed0939",
                "title": "End-to-End Compound Table Understanding with Multi-Modal Modeling",
                "abstract": "Table is a widely used data form in webpages, spreadsheets, or PDFs to organize and present structural data. Although studies on table structure recognition have been successfully used to convert image-based tables into digital structural formats, solving many real problems still relies on further understanding of the table, such as cell relationship extraction. The current datasets related to table understanding are all based on the digit format. To boost research development, we release a new benchmark named ComFinTab with rich annotations that support both table recognition and understanding tasks. Unlike previous datasets containing the basic tables, ComFinTab contains a large ratio of compound tables, which is much more challenging and requires methods using multiple information sources. Based on the dataset, we also propose a uniform, concise task form with the evaluation metric to better evaluate the model's performance on the table understanding task in compound tables. Finally, a framework named CTUNet is proposed to integrate the compromised visual, semantic, and position features with a graph attention network, which can solve the table recognition task and the challenging table understanding task as a whole. Experimental results compared with some previous advanced table understanding methods demonstrate the effectiveness of our proposed model. Code and dataset are available at \\urlhttps://github.com/hikopensource/DAVAR-Lab-OCR.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109614687",
                        "name": "Zaisheng Li"
                    },
                    {
                        "authorId": "2187429408",
                        "name": "Yi Li"
                    },
                    {
                        "authorId": "2187308758",
                        "name": "Qiao Liang"
                    },
                    {
                        "authorId": "2144902872",
                        "name": "Pengfei Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    },
                    {
                        "authorId": "1490934795",
                        "name": "Yi Niu"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    },
                    {
                        "authorId": "2116227961",
                        "name": "Xi Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[3] proposed DeepDeSRT, an end-to-end system for table understanding in document images and detecting PDF documents.",
                "Video Salient Document Detection (VSDD) is an essential task in several real-world applications, such as video document recognition [1], video document compression [2], video document captioning [3] and many more."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0837684d21a45db97413a34cd22d249a2da83d86",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-04447",
                    "ArXiv": "2301.04447",
                    "DOI": "10.1109/ICTAI56018.2022.00198",
                    "CorpusId": 255595482
                },
                "corpusId": 255595482,
                "publicationVenue": {
                    "id": "ba1e9488-a629-4abe-a0c2-ec2c79c91616",
                    "name": "IEEE International Conference on Tools with Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Tool Artif Intell",
                        "ICTAI",
                        "IEEE Int Conf Tool Artif Intell",
                        "International Conference on Tools with Artificial Intelligence"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1488"
                },
                "url": "https://www.semanticscholar.org/paper/0837684d21a45db97413a34cd22d249a2da83d86",
                "title": "VS-Net: Multiscale Spatiotemporal Features for Lightweight Video Salient Document Detection",
                "abstract": "Video Salient Document Detection (VSDD) is an essential task of practical computer vision, which aims to highlight visually salient document regions in video frames. Previous techniques for VSDD focus on learning features without considering the cooperation among and across the appearance and motion cues and thus fail to perform in practical scenarios. Moreover, most of the previous techniques demand high computational resources, which limits the usage of such systems in resource-constrained settings. To handle these issues, we propose VS-Net, which captures multi-scale spatiotemporal information with the help of dilated depth-wise separable convolution and Approximation Rank Pooling. VS-Net extracts the key features locally from each frame across embedding sub-spaces and forwards the features between adjacent and parallel nodes, enhancing model performance globally. Our model generates saliency maps considering both the background and foreground simultaneously, making it perform better in challenging scenarios. The immense experiments regulated on the benchmark MIDV-500 dataset show that the VS-Net model outperforms state-of-the-art approaches in both time and robustness measures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2199940380",
                        "name": "Hemraj Singh"
                    },
                    {
                        "authorId": "2618741",
                        "name": "Mridula Verma"
                    },
                    {
                        "authorId": "9327092",
                        "name": "Ramalingaswamy Cheruku"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a0f11b53aa27ada8144812a47727768a5fb412e9",
                "externalIds": {
                    "DBLP": "conf/ictai/LuoHGQ22",
                    "DOI": "10.1109/ICTAI56018.2022.00079",
                    "CorpusId": 258221050
                },
                "corpusId": 258221050,
                "publicationVenue": {
                    "id": "ba1e9488-a629-4abe-a0c2-ec2c79c91616",
                    "name": "IEEE International Conference on Tools with Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Tool Artif Intell",
                        "ICTAI",
                        "IEEE Int Conf Tool Artif Intell",
                        "International Conference on Tools with Artificial Intelligence"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1488"
                },
                "url": "https://www.semanticscholar.org/paper/a0f11b53aa27ada8144812a47727768a5fb412e9",
                "title": "GTRNet: a graph-based table reconstructed network",
                "abstract": "Tabular data, with an exceedingly effective data structure, can give us a more intuitive visual display. To under-stand well and make use of the spatial and logic dependencies of it, we propose an end-to-end, graph-based table reconstructed network, namely GTRNet, in this paper. Our model works differently from most existing models which treat tables as either a markup sequence problem or a graph structure of rows and columns. It can utilize a table as input and extract its features in the text, image and position coordinate to predict the dependencies of the text instances and well distinguish the spatial relationship to infer whether multiple text segments belong to the same merged cell. Optimized along with this network, we can then restore the structure of this table. Moreover, we also create a new Chinese benchmark dataset GraphTable for this task to tackle complex challenges on the table. The competitive results on ICDAR-2013, GraphTable, SciTSR and FinTab benchmarks further confirm the great effectiveness of GTRN et.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27684556",
                        "name": "Yuchen Luo"
                    },
                    {
                        "authorId": "1390799037",
                        "name": "Zheng Huang"
                    },
                    {
                        "authorId": "2117062832",
                        "name": "Jie Guo"
                    },
                    {
                        "authorId": "40453004",
                        "name": "Weidong Qiu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "the range of object detection using models like YOLO [32, 38, 47] to, more recently, Faster R-CNN [16, 34, 37, 43, 49] and Mask-CNN [2, 18, 26].",
                "Methods span\n5 makesense.ai 6 http://www.astroexplorer.org/\nthe range of object detection using models like YOLO [32, 38, 47] to, more recently, Faster R-CNN [16, 34, 37, 43, 49] and Mask-CNN [2, 18, 26]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b4b0f4c53c555cf0966a1333483c85ab46934336",
                "externalIds": {
                    "DBLP": "conf/ercimdl/NaimanWG21",
                    "ArXiv": "2209.04460",
                    "DOI": "10.1007/978-3-031-16802-4_5",
                    "CorpusId": 252199980
                },
                "corpusId": 252199980,
                "publicationVenue": {
                    "id": "14508953-a0fc-42e5-9a54-ab6f00373c45",
                    "name": "International Conference on Theory and Practice of Digital Libraries",
                    "type": "conference",
                    "alternate_names": [
                        "Theory Pract Digit Libr",
                        "International Conference Theory and Practice Digital Libraries",
                        "TPDL",
                        "Int Conf Theory Pract Digit Libr",
                        "Theory and Practice of Digital Libraries"
                    ],
                    "url": "http://www.tpdl.eu/"
                },
                "url": "https://www.semanticscholar.org/paper/b4b0f4c53c555cf0966a1333483c85ab46934336",
                "title": "Figure and Figure Caption Extraction for Mixed Raster and Vector PDFs: Digitization of Astronomical Literature with OCR Features",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065414161",
                        "name": "J. Naiman"
                    },
                    {
                        "authorId": "48588676",
                        "name": "P. Williams"
                    },
                    {
                        "authorId": "2765398",
                        "name": "A. Goodman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c18b3479a389ae4bc955338a862e6bc4f68d6ac1",
                "externalIds": {
                    "DOI": "10.3390/app12188969",
                    "CorpusId": 252171274
                },
                "corpusId": 252171274,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c18b3479a389ae4bc955338a862e6bc4f68d6ac1",
                "title": "Continual Learning for Table Detection in Document Images",
                "abstract": "The growing amount of data demands methods that can gradually learn from new samples. However, it is not trivial to continually train a network. Retraining a network with new data usually results in a phenomenon called \u201ccatastrophic forgetting\u201d. In a nutshell, the performance of the model on the previous data drops by learning from the new instances. This paper explores this issue in the table detection problem. While there are multiple datasets and sophisticated methods for table detection, the utilization of continual learning techniques in this domain has not been studied. We employed an effective technique called experience replay and performed extensive experiments on several datasets to investigate the effects of catastrophic forgetting. The results show that our proposed approach mitigates the performance drop by 15 percent. To the best of our knowledge, this is the first time that continual learning techniques have been adopted for table detection, and we hope this stands as a baseline for future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1752929623",
                        "name": "Mohammad Minouei"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "2757942",
                        "name": "M. Soheili"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, table detection and structure recognition is performed with a two-fold deep-learning-based system using Faster R-CNN in [12]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6727ee4a7b8440849fb810e9e11aaee3f85126a6",
                "externalIds": {
                    "DBLP": "conf/etfa/ChowdhuryAA22",
                    "DOI": "10.1109/ETFA52439.2022.9921455",
                    "CorpusId": 253123378
                },
                "corpusId": 253123378,
                "publicationVenue": {
                    "id": "22466e99-8720-4629-9169-4469e385e4d8",
                    "name": "IEEE International Conference on Emerging Technologies and Factory Automation",
                    "type": "conference",
                    "alternate_names": [
                        "Emerg Technol Fact Autom",
                        "IEEE Int Conf Emerg Technol Fact Autom",
                        "ETFA",
                        "Emerging Technologies and Factory Automation"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=937"
                },
                "url": "https://www.semanticscholar.org/paper/6727ee4a7b8440849fb810e9e11aaee3f85126a6",
                "title": "Towards Tabular Data Extraction From Richly-Structured Documents Using Supervised and Weakly-Supervised Learning",
                "abstract": "Tabular information extraction from richly structured documents is a challenging task, due to rich table and document structures. Supervised document table detection approaches include image classification and object localization methods, typically relying on manually annotated data which is often costly to acquire specially on domain specific dataset. Self-supervised learning is quickly closing the gap with supervised methods in computer vision research [1]. This paper investigates the impact of a self-supervised image classifier as the primary backbone in supervised object detection for document table detection. Furthermore, we study an approach for table structure recognition based on the pix2pix Generative Adversarial Networks (GAN) approach [2]. We propose these approaches as the basis of a machine learning pipeline for table detection and structure recognition. Our evaluation results on different publicly available datasets, as well as a domain specific dataset demonstrate the efficacy of the presented approaches towards tabular information extraction pipelines from richly structured documents.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1675904155",
                        "name": "A. Chowdhury"
                    },
                    {
                        "authorId": "2189543171",
                        "name": "Martin ben Ahmed"
                    },
                    {
                        "authorId": "2142735959",
                        "name": "Martin Atzmueller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More recent studies have thus focused mostly on developing deep CNN-based table detection and recognition approaches (see [2, 26, 9, 13, 27, 23] and references therein), leading to significant performance improvements.",
                "As addressed in Section 3.1, there are lots of state-of-theart methods for table detection, which include but not limited to YOLOv5 [14], DETR-R50 [3], deformable DETR [32], CascadeTabNet [26], DeCNT [28], DeepDeSRT [27], TableNet [23], CDeC-Net [2], and TableRadar [8].",
                "It was shown in [27] how to fine-tune a pre-trained faster R-CNN model.",
                "1, there are lots of state-of-theart methods for table detection, which include but not limited to YOLOv5 [14], DETR-R50 [3], deformable DETR [32], CascadeTabNet [26], DeCNT [28], DeepDeSRT [27], TableNet [23], CDeC-Net [2], and TableRadar [8]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "98d8ca558e6e7cf7657a7b9e30701b8b4aa500e6",
                "externalIds": {
                    "DBLP": "journals/kbs/KwonALS22",
                    "ArXiv": "2211.06648",
                    "DOI": "10.1016/j.knosys.2022.109946",
                    "CorpusId": 252651951
                },
                "corpusId": 252651951,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/98d8ca558e6e7cf7657a7b9e30701b8b4aa500e6",
                "title": "DATa: Domain Adaptation-Aided Deep Table Detection Using Visual-Lexical Representations",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2186749262",
                        "name": "Hyebin Kwon"
                    },
                    {
                        "authorId": "2186691027",
                        "name": "Joungbin An"
                    },
                    {
                        "authorId": "2232923040",
                        "name": "Dongwook Lee"
                    },
                    {
                        "authorId": "2186706815",
                        "name": "Won-Yong Shin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Popular table structure recognition methods include DeepDeSRT [7], ReS2Tim [15], DeepTabStR [16], etc.",
                "Unfortunately, the Component-based approaches such as DeepDeSRT [7], TableNet [8] and LGPMA [9] still suffer from boundary ambiguity problems in unlined tables and cannot achieve decent performance in complex scenarios such as tables with empty cells."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "92132fb36fb3e470464551210926f256a1f37280",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-14687",
                    "ArXiv": "2208.14687",
                    "DOI": "10.48550/arXiv.2208.14687",
                    "CorpusId": 251953555
                },
                "corpusId": 251953555,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/92132fb36fb3e470464551210926f256a1f37280",
                "title": "TRUST: An Accurate and End-to-End Table structure Recognizer Using Splitting-based Transformers",
                "abstract": "Table structure recognition is a crucial part of document image analysis domain. Its difficulty lies in the need to parse the physical coordinates and logical indices of each cell at the same time. However, the existing methods are difficult to achieve both these goals, especially when the table splitting lines are blurred or tilted. In this paper, we propose an accurate and end-to-end transformer-based table structure recognition method, referred to as TRUST. Transformers are suitable for table structure recognition because of their global computations, perfect memory, and parallel computation. By introducing novel Transformer-based Query-based Splitting Module and Vertex-based Merging Module, the table structure recognition problem is decoupled into two joint optimization sub-tasks: multi-oriented table row/column splitting and table grid merging. The Query-based Splitting Module learns strong context information from long dependencies via Transformer networks, accurately predicts the multi-oriented table row/column separators, and obtains the basic grids of the table accordingly. The Vertex-based Merging Module is capable of aggregating local contextual information between adjacent basic grids, providing the ability to merge basic girds that belong to the same spanning cell accurately. We conduct experiments on several popular benchmarks including PubTabNet and SynthTable, our method achieves new state-of-the-art results. In particular, TRUST runs at 10 FPS on PubTabNet, surpassing the previous methods by a large margin.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109309324",
                        "name": "Zengyuan Guo"
                    },
                    {
                        "authorId": "2117164666",
                        "name": "Yuecheng Yu"
                    },
                    {
                        "authorId": "25604699",
                        "name": "Pengyuan Lv"
                    },
                    {
                        "authorId": "1979323",
                        "name": "Chengquan Zhang"
                    },
                    {
                        "authorId": "2579920",
                        "name": "Haojie Li"
                    },
                    {
                        "authorId": "47196393",
                        "name": "Zhihui Wang"
                    },
                    {
                        "authorId": "2140371004",
                        "name": "Kun Yao"
                    },
                    {
                        "authorId": "2272123",
                        "name": "Jingtuo Liu"
                    },
                    {
                        "authorId": "2109534192",
                        "name": "Jingdong Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[17] uses a deep learning-based method to detect tables and identify table structure by detecting rows, columns, and table cells."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "559992b61675b0afd0024fbd2b659b46080a5325",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-09207",
                    "ArXiv": "2209.09207",
                    "DOI": "10.48550/arXiv.2209.09207",
                    "CorpusId": 252368143
                },
                "corpusId": 252368143,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/559992b61675b0afd0024fbd2b659b46080a5325",
                "title": "Table Detection in the Wild: A Novel Diverse Table Detection Dataset and Method",
                "abstract": "Recent deep learning approaches in table detection achieved outstanding performance and proved to be effective in identifying document layouts. Currently, available table detection benchmarks have many limitations, including the lack of samples diversity, simple table structure, the lack of training cases, and samples quality. In this paper, we introduce a diverse large-scale dataset for table detection with more than seven thousand samples containing a wide variety of table structures collected from many diverse sources. In addition to that, we also present baseline results using a convolutional neural network-based method to detect table structure in documents. Experimental results show the superiority of applying convolutional deep learning methods over classical computer vision-based methods. The introduction of this diverse table detection dataset will enable the community to develop high throughput deep learning methods for understanding document layout and tabular data processing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2372916",
                        "name": "Mrinal Haloi"
                    },
                    {
                        "authorId": "144675956",
                        "name": "Shashank Shekhar"
                    },
                    {
                        "authorId": "2105149533",
                        "name": "Nikhil Fande"
                    },
                    {
                        "authorId": "2185409811",
                        "name": "Siddhant Swaroop Dash"
                    },
                    {
                        "authorId": "150114927",
                        "name": "G. Sanjay"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ICDAR2013 UNLV Method Prec Recall F1 Prec Recall F1\nTabbyPDF 78.90 84.50 81.60 \u2212 \u2212 \u2212 GraphTSR 81.90 85.50 83.70 \u2212 \u2212 \u2212 DeepDeSRT 57.30 56.40 56.80 \u2212 \u2212 \u2212 CATT-Net 94.10 90.70 92.30 86.28 88.31 87.24 Ours 92.85 93.29 93.04 92.66 86.78 89.52",
                "For the TSR task, we list TabbyPDF [19], GraphTSR [7], DeepDeSRT [8] and CATT-Net [17] as benchmark models.",
                "The performance scores of TabbyPDF, GraphTSR, and DeepDeSRT on the ICDAR2013 dataset come from the study [7].",
                "In contrast, some work, such as DeepDeSRT [8], CascadeTabNet [9] and TableDet [10], using top-down approaches would define the structural recognition as object detection or segmentation problem, often together with table detection problem."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "551c36552b2d5af8493963ac38ee288f3cdb34ed",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-06031",
                    "ArXiv": "2208.06031",
                    "DOI": "10.1109/GLOBECOM48099.2022.10001253",
                    "CorpusId": 251554763
                },
                "corpusId": 251554763,
                "publicationVenue": {
                    "id": "b189dec0-41d0-4cea-a906-7c5186895904",
                    "name": "Global Communications Conference",
                    "type": "conference",
                    "alternate_names": [
                        "GLOBECOM",
                        "Glob Commun Conf"
                    ],
                    "url": "http://www.ieee-globecom.org/"
                },
                "url": "https://www.semanticscholar.org/paper/551c36552b2d5af8493963ac38ee288f3cdb34ed",
                "title": "Handling big tabular data of ICT supply chains: a multi-task, machine-interpretable approach",
                "abstract": "Due to the characteristics of Information and Communications Technology (ICT) products, the critical information of ICT devices is often summarized in big tabular data shared across supply chains. Therefore, it is critical to automatically interpret tabular structures with the surging amount of electronic assets. To transform the tabular data in electronic documents into a machine-interpretable format and provide layout and semantic information for information extraction and interpretation, we define a Table Structure Recognition (TSR) task and a Table Cell Type Classification (CTC) task. We use a graph to represent complex table structures for the TSR task. Meanwhile, table cells are categorized into three groups based on their functional roles for the CTC task, namely Header, Attribute, and Data. Subsequently, we propose a multi-task model to solve the defined two tasks simultaneously by using the text modal and image modal features. Our experimental results show that our proposed method can outperform state-of-the-art methods on ICDAR2013 and UNLV datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146517209",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[42] presents a data-driven system and does not need any heuristics or metadata to detect and recognize tabular structures on the ICDAR 2013 dataset [19]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ed28ec8d1e1b98bf348b6f66f7112643faf06d5b",
                "externalIds": {
                    "DBLP": "journals/tweb/HaiderY23",
                    "DOI": "10.1145/3555349",
                    "CorpusId": 251407565
                },
                "corpusId": 251407565,
                "publicationVenue": {
                    "id": "cef4bc5a-77b3-4506-8d78-14d282235429",
                    "name": "ACM Transactions on the Web",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Transactions on The Web",
                        "ACM Trans Web"
                    ],
                    "issn": "1559-1131",
                    "url": "http://www.acm.org/tweb/index.html",
                    "alternate_urls": [
                        "http://portal.acm.org/browse_dl.cfm?coll=ACM&dl=ACM&idx=J1062&linked=1&part=transaction",
                        "https://tweb.acm.org/",
                        "http://portal.acm.org/tweb"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ed28ec8d1e1b98bf348b6f66f7112643faf06d5b",
                "title": "Classification of Layout vs. Relational Tables on the Web: Machine Learning with Rendered Pages",
                "abstract": "Table mining on the web is an open problem, and none of the previously proposed techniques provides a complete solution. Most research focuses on the structure of the HTML document, but because of the nature and structure of the web, it is still a challenging problem to detect relational tables. Web Content Accessibility Guidelines (WCAG) also cover a wide range of recommendations for making tables accessible, but our previous work shows that these recommendations are also not followed; therefore, tables are still inaccessible to disabled people and automated processing. We propose a new approach to table mining by not looking at the HTML structure, but rather, the rendered pages by the browser. The first task in table mining on the web is to classify relational vs. layout tables, and here, we propose two alternative approaches for that task. We first introduce our dataset, which includes 725 web pages with 9,957 extracted tables. Our first approach extracts features from a page after being rendered by the browser, then applies several machine learning algorithms in classifying the layout vs. relational tables. The best result is with Random Forest with the accuracy of 97.2% (F1-score: 0.955) with 10-fold cross-validation. Our second approach classifies tables using images taken from the same sources using Convolutional Neural Network (CNN), which gives an accuracy of 95% (F1-score: 0.95). Our work here shows that the web\u2019s true essence comes after it goes through a browser and using the rendered pages and tables, the classification is more accurate compared to literature and paves the way in making the tables more accessible.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2054881942",
                        "name": "Waqar Haider"
                    },
                    {
                        "authorId": "1760899",
                        "name": "Y. Ye\u015filada"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [39] first applied an FCN-based semantic segmentation method [25] to table structure extraction."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fb6fcca5762b26f225313f86b1f33a1cf198bfd7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-04921",
                    "ArXiv": "2208.04921",
                    "DOI": "10.1145/3503161.3548038",
                    "CorpusId": 251442367
                },
                "corpusId": 251442367,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fb6fcca5762b26f225313f86b1f33a1cf198bfd7",
                "title": "TSRFormer: Table Structure Recognition with Transformers",
                "abstract": "We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage DETR based separator prediction approach, dubbed Sep arator RE gression TR ansformer (SepRETR), to predict separation lines from table images directly. To make the two-stage DETR framework work efficiently and effectively for the separation line prediction task, we propose two improvements: 1) A prior-enhanced matching strategy to solve the slow convergence issue of DETR; 2) A new cross attention module to sample features from a high-resolution convolutional feature map directly so that high localization accuracy is achieved with low computational cost. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet and WTW. Furthermore, we have validated the robustness of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2145130513",
                        "name": "Zhengmao Sun"
                    },
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2181111465",
                        "name": "Mingze Li"
                    },
                    {
                        "authorId": "2110132415",
                        "name": "Jiawei Wang"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Faster R-CNN uses an end-to-end region suggestor instead of selective search method to improve training speed [13][19].",
                "The authors employed, with the model learned features, some heuristic rules and document meta-information to enhance the output result [3][13].",
                "ICDAR2013 Table Competition database [13].",
                "Moreover, the model is independent to the size of input image [13][19].",
                "Their approach is identified as DeepDeSRT [13]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "cc2cd539f0a171380f54447fc3c97b2bb9425fa0",
                "externalIds": {
                    "DOI": "10.1109/ITSIS56166.2022.10118367",
                    "CorpusId": 258725570
                },
                "corpusId": 258725570,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cc2cd539f0a171380f54447fc3c97b2bb9425fa0",
                "title": "Toward a Robust Segmentation Module Based on Deep Learning Approaches Resolving Historical Cursive Fonts Challenges",
                "abstract": "Historical documents are of major importance in conserving cultural and scientific heritage. In order to preserve this patrimony and to allow researchers from multiple fields to manipulate them, experts had opted to digitize these documents. In order to accelerate the process of classification while building logical connections between elements, the segmentation task should be treated carefully. Multiple non-textual blocks are presented within historical documents. Moreover, the non-consideration of the presence of columns generates major issues within the different processing stages. Based on deep learning segmentation approaches, this paper introduces the opted design for text/non-text separation and columns segmentations tasks. Moreover, a novel segmentation approach dealing with cursive fonts is proposed. The accuracy of the text/non-text blocks segmentation accuracy is equal to 97.32%. The columns segmentation approach accuracy is equal to 93.26%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2053851937",
                        "name": "Ilyes Ouled Omar"
                    },
                    {
                        "authorId": "1743567",
                        "name": "Sofiene Haboubi"
                    },
                    {
                        "authorId": "1803588",
                        "name": "F. Benzarti"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the past decade, table detection, extraction and annotation have been key research areas [17], leading to a variety of extraction approaches [11, 22, 26, 27, 50, 61, 70, 71, 86]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "87cb212eeb40171b32a4411a6919e22c36822140",
                "externalIds": {
                    "DBLP": "journals/tmis/AmeriHSLP23",
                    "DOI": "10.1145/3546580",
                    "CorpusId": 250496865
                },
                "corpusId": 250496865,
                "publicationVenue": {
                    "id": "a0b9dfee-fbf4-4bea-835f-20d6db1ce53a",
                    "name": "ACM Transactions on Management Information Systems",
                    "alternate_names": [
                        "ACM Trans Manag Inf Syst"
                    ],
                    "issn": "2158-656X",
                    "url": "http://dl.acm.org/citation.cfm?CFID=72370079&CFTOKEN=39904203&id=J1320",
                    "alternate_urls": [
                        "http://portal.acm.org/tmis",
                        "http://portal.acm.org/citation.cfm?id=J1320&picked=prox",
                        "https://tmis.acm.org/",
                        "http://tmis.acm.org/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/87cb212eeb40171b32a4411a6919e22c36822140",
                "title": "Design of a Novel Information System for Semi-automated Management of Cybersecurity in Industrial Control Systems",
                "abstract": "There is an urgent need in many critical infrastructure sectors, including the energy sector, for attaining detailed insights into cybersecurity features and compliance with cybersecurity requirements related to their Operational Technology (OT) deployments. Frequent feature changes of OT devices interfere with this need, posing a great risk to customers. One effective way to address this challenge is via a semi-automated cyber-physical security assurance approach, which enables verification and validation of the OT device cybersecurity claims against actual capabilities, both pre- and post-deployment. To realize this approach, this article presents new methodology and algorithms to automatically identify cybersecurity-related claims expressed in natural language form in ICS device documents. We developed an identification process that employs natural language processing (NLP) techniques with the goal of semi-automated vetting of detected claims against their device implementation. We also present our novel NLP components for verifying feature claims against relevant cybersecurity requirements. The verification pipeline includes components such as automated vendor identification, device document curation, feature claim identification utilizing sentiment analysis for conflict resolution, and reporting of features that are claimed to be supported or indicated as unsupported. Our novel matching engine represents the first automated information system available in the cybersecurity domain that directly aids the generation of ICS compliance reports.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "134238540",
                        "name": "Kimia Ameri"
                    },
                    {
                        "authorId": "35380390",
                        "name": "M. Hempel"
                    },
                    {
                        "authorId": "145505074",
                        "name": "H. Sharif"
                    },
                    {
                        "authorId": "2150338498",
                        "name": "Juan Lopez"
                    },
                    {
                        "authorId": "1750770",
                        "name": "K. Perumalla"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[18] proposed the deep transfer learning-based algorithm DeepDeSRT for table detection and table structure recognition.",
                "With the surge in deep learning, various CNN based architectures [6, 7, 10, 15,16,18,22] have been proposed for table detection and cell extraction."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "79af7ca206d1da599760bd99c9581a32cb4e8c7e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-06823",
                    "ArXiv": "2207.06823",
                    "DOI": "10.48550/arXiv.2207.06823",
                    "CorpusId": 250526703
                },
                "corpusId": 250526703,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/79af7ca206d1da599760bd99c9581a32cb4e8c7e",
                "title": "DEXTER: An end-to-end system to extract table contents from electronic medical health documents",
                "abstract": "In this paper, we propose DEXTER, an end to end system to extract information from tables present in medical health documents, such as electronic health records (EHR) and explanation of benefits (EOB). DEXTER consists of four sub-system stages: i) table detection ii) table type classification iii) cell detection; and iv) cell content extraction. We propose a two-stage transfer learning-based approach using CDeC-Net architecture along with Non-Maximal suppression for table detection. We design a conventional computer vision-based approach for table type classification and cell detection using parameterized kernels based on image size for detecting rows and columns. Finally, we extract the text from the detected cells using pre-existing OCR engine Tessaract. To evaluate our system, we manually annotated a sample of the real-world medical dataset (referred to as Meddata) consisting of wide variations of documents (in terms of appearance) covering different table structures, such as bordered, partially bordered, borderless, or coloured tables. We experimentally show that DEXTER outperforms the commercially available Amazon Textract and Microsoft Azure Form Recognizer systems on the annotated real-world medical dataset",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176184231",
                        "name": "PR Nandhinee"
                    },
                    {
                        "authorId": "2176184223",
                        "name": "Harinath Krishnamoorthy"
                    },
                    {
                        "authorId": "88862568",
                        "name": "K. Srivatsan"
                    },
                    {
                        "authorId": "46479704",
                        "name": "Anil Goyal"
                    },
                    {
                        "authorId": "35628253",
                        "name": "Sudarsun Santhiappan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "644b991959bf5fb036d28d67e9918d419b212e75",
                "externalIds": {
                    "DBLP": "journals/pr/LiYDL22",
                    "DOI": "10.1016/j.patcog.2022.108946",
                    "CorpusId": 251140530
                },
                "corpusId": 251140530,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/644b991959bf5fb036d28d67e9918d419b212e75",
                "title": "Table Structure Recognition and Form Parsing by End-to-End Object Detection and Relation Parsing",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118890077",
                        "name": "Xiaohui Li"
                    },
                    {
                        "authorId": "145820427",
                        "name": "Fei Yin"
                    },
                    {
                        "authorId": "2179477671",
                        "name": "He-Sen Dai"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We approach table detection as a general object detection problem, as was done by Schreiber et al. ((Schreiber et al. 2017)), and use a Faster-RCNN model to detect and localize tables in page images (Ren et al. 2015).",
                "For example, DeepDeSRT is another approach which uses a Fast-RCNN to detect the tables (Schreiber et al. 2017).",
                "((Schreiber et al. 2017)), and use a Faster-RCNN model to detect and localize tables in page images (Ren et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e9b167321db5002fb2ebfe1357ac5bf461c3f568",
                "externalIds": {
                    "DBLP": "conf/aaai/TekinYCFK22",
                    "DOI": "10.1609/aaai.v36i11.21507",
                    "CorpusId": 250290310
                },
                "corpusId": 250290310,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e9b167321db5002fb2ebfe1357ac5bf461c3f568",
                "title": "Harvest - a System for Creating Structured Rate Filing Data from Filing PDFs",
                "abstract": "We present a machine-learning-guided process that can efficiently extract factor tables from unstructured rate filing documents. Our approach combines multiple deep-learning-based models that work in tandem to create structured representations of tabular data present in unstructured documents such as pdf files. This process combines CNN's to detect tables, language-based models to extract table metadata and conventional computer vision techniques to improve the accuracy of tabular data on the machine-learning side. The extracted tabular data is validated through an intuitive user interface. This process, which we call Harvest, significantly reduces the time needed to extract tabular information from PDF files, enabling analysis of such data at a speed and scale that was previously unattainable.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2940704",
                        "name": "Ender Tekin"
                    },
                    {
                        "authorId": "49038552",
                        "name": "Qian You"
                    },
                    {
                        "authorId": "143728756",
                        "name": "D. Conathan"
                    },
                    {
                        "authorId": "2064481572",
                        "name": "G. Fung"
                    },
                    {
                        "authorId": "2174871450",
                        "name": "Thomas S. Kneubuehl"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The table can therefore be represented as a grid [68,78]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1aa8da3bbd2bca7071fae029d6fa8ea851f805b7",
                "externalIds": {
                    "DBLP": "conf/clef/SkalickySUS22",
                    "ArXiv": "2206.11229",
                    "DOI": "10.48550/arXiv.2206.11229",
                    "CorpusId": 249926391
                },
                "corpusId": 249926391,
                "publicationVenue": {
                    "id": "ab453bce-d4ec-48ec-ad78-ef19dc9333ab",
                    "name": "Conference and Labs of the Evaluation Forum",
                    "type": "conference",
                    "alternate_names": [
                        "CLEF",
                        "Conf Lab Evaluation Forum",
                        "Cross-language Evaluation Forum",
                        "Cross-Language Evaluation Forum"
                    ],
                    "url": "http://www.clef-initiative.eu/"
                },
                "url": "https://www.semanticscholar.org/paper/1aa8da3bbd2bca7071fae029d6fa8ea851f805b7",
                "title": "Business Document Information Extraction: Towards Practical Benchmarks",
                "abstract": "Information extraction from semi-structured documents is crucial for frictionless business-to-business (B2B) communication. While machine learning problems related to Document Information Extraction (IE) have been studied for decades, many common problem definitions and benchmarks do not reflect domain-specific aspects and practical needs for automating B2B document communication. We review the landscape of Document IE problems, datasets and benchmarks. We highlight the practical aspects missing in the common definitions and define the Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR) problems. There is a lack of relevant datasets and benchmarks for Document IE on semi-structured business documents as their content is typically legally protected or sensitive. We discuss potential sources of available documents including synthetic data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071515342",
                        "name": "Maty\u00e1s Skalick\u00fd"
                    },
                    {
                        "authorId": "46369981",
                        "name": "Step\u00e1n Simsa"
                    },
                    {
                        "authorId": "3406363",
                        "name": "Michal U\u0159i\u010d\u00e1\u0159"
                    },
                    {
                        "authorId": "2052167",
                        "name": "Milan \u0160ulc"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4e7db59a793d2caed15d677f0fded0d4a16d5648",
                "externalIds": {
                    "DBLP": "journals/fi/KallempudiHPLSA22",
                    "DOI": "10.3390/fi14060176",
                    "CorpusId": 249596110
                },
                "corpusId": 249596110,
                "publicationVenue": {
                    "id": "c3e5f1c8-9ba7-47e5-acde-53063a69d483",
                    "name": "Future Internet",
                    "type": "journal",
                    "issn": "1999-5903",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-156830",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-156830",
                        "https://www.mdpi.com/journal/futureinternet"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4e7db59a793d2caed15d677f0fded0d4a16d5648",
                "title": "Toward Semi-Supervised Graphical Object Detection in Document Images",
                "abstract": "The graphical page object detection classifies and localizes objects such as Tables and Figures in a document. As deep learning techniques for object detection become increasingly successful, many supervised deep neural network-based methods have been introduced to recognize graphical objects in documents. However, these models necessitate a substantial amount of labeled data for the training process. This paper presents an end-to-end semi-supervised framework for graphical object detection in scanned document images to address this limitation. Our method is based on a recently proposed Soft Teacher mechanism that examines the effects of small percentage-labeled data on the classification and localization of graphical objects. On both the PubLayNet and the IIIT-AR-13K datasets, the proposed approach outperforms the supervised models by a significant margin in all labeling ratios (1%,\u00a05%, and 10%). Furthermore, the 10% PubLayNet Soft Teacher model improves the average precision of Table, Figure, and List by +5.4,+1.2, and +3.2 points, respectively, with a similar total mAP as the Faster-RCNN baseline. Moreover, our model trained on 10% of IIIT-AR-13K labeled data beats the previous fully supervised method +4.5 points.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2169854688",
                        "name": "Goutham Kallempudi"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1771057",
                        "name": "A. Pagani"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [41] proposes a novel end-to-end system for table understanding in document images called DeepDeSRT."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0b3b6e22fba1c9dc5e5b386128f1194618692158",
                "externalIds": {
                    "DBLP": "journals/ijdar/ZhangMGJZ23",
                    "DOI": "10.1007/s10032-022-00400-z",
                    "CorpusId": 248581054
                },
                "corpusId": 248581054,
                "publicationVenue": {
                    "id": "46722266-4327-4a22-871d-5a9fe3c5ad0c",
                    "name": "International Journal on Document Analysis and Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Doc Anal Recognit"
                    ],
                    "issn": "1433-2825",
                    "url": "https://link.springer.com/journal/10032"
                },
                "url": "https://www.semanticscholar.org/paper/0b3b6e22fba1c9dc5e5b386128f1194618692158",
                "title": "YOLO-table: disclosure document table detection with involution",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2164852187",
                        "name": "Daqian Zhang"
                    },
                    {
                        "authorId": "1380055201",
                        "name": "Ruibin Mao"
                    },
                    {
                        "authorId": "1387681701",
                        "name": "R. Guo"
                    },
                    {
                        "authorId": "2164709014",
                        "name": "Yang Jiang"
                    },
                    {
                        "authorId": "2164850015",
                        "name": "Jing Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "564acb3b027a439b1a44e5b2d73ff5814312ad58",
                "externalIds": {
                    "DBLP": "journals/asc/KashinathJAAS22",
                    "DOI": "10.1016/j.asoc.2022.108942",
                    "CorpusId": 248716737
                },
                "corpusId": 248716737,
                "publicationVenue": {
                    "id": "b1994124-f1e8-4f96-a165-b6f19a04fe7e",
                    "name": "Applied Soft Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Soft Comput"
                    ],
                    "issn": "1568-4946",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/621920/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/applied-soft-computing",
                        "http://www.sciencedirect.com/science/journal/15684946"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/564acb3b027a439b1a44e5b2d73ff5814312ad58",
                "title": "End-to-end table structure recognition and extraction in heterogeneous documents",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2164965360",
                        "name": "Tejas Kashinath"
                    },
                    {
                        "authorId": "2164964715",
                        "name": "Twisha Jain"
                    },
                    {
                        "authorId": "2066307014",
                        "name": "Yash Agrawal"
                    },
                    {
                        "authorId": "1455135470",
                        "name": "Tanvi Anand"
                    },
                    {
                        "authorId": "2118414009",
                        "name": "S. Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The RoI segmentation is basically categorized into four different ategories: Hough Transform [ 26 ], Projection profiles [ 23 , 28 ], Smearing [ 29 ], and segmentation\nCM Trans.",
                "Second, the RoIs class type (title, text, table, figure, and list) [26] within each image.",
                "The RoI segmentation is basically categorized into four different categories: Hough Transform [26], Projection profiles [23, 28], Smearing [29], and segmentation based on connected components.",
                "The segmentaion techniques are basically characterized into four categories: Hough Transform [ 20 ], Projection rofiles [ 21 ], Smearing [ 22 ], and segmentation based on connected components."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6afe984e44961e16645ccec4173cb2e50cd36efc",
                "externalIds": {
                    "DBLP": "journals/talip/AlBarhamtoshyJRA23",
                    "DOI": "10.1145/3532609",
                    "CorpusId": 248419310
                },
                "corpusId": 248419310,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6afe984e44961e16645ccec4173cb2e50cd36efc",
                "title": "An Arabic Manuscript Regions Detection, Recognition and Its Applications for OCRing",
                "abstract": "The problem of Region of Interest (RoI) in document layout analysis and document recognition has recently become an essential topic in OCRing systems. Arabic manuscript layout analysis and OCRing recognition using language detection, document category, and RoI with Keras and TensorFlow are terms of the state-of-the-art that should be investigated. This article investigates the problem of Arabic manuscript recognition problems with respect to in OCRing-based recognition. A new framework architecture, which integrates Fast Gradient Sign Method (FGSM) using Keras and TensorFlow with adversarial image generation during training procedure is proposed. Also, the article tries to improve the OCRing accuracy of the image enhancement, alignment, layout analysis, and recognition using deep learning in multilingual system. RoIs detections will be performed using a custom trained deep learning model using bounding box regression with Keras and TensorFlow. This topic investigates an extension of Page Segmentation Method (PSM) to enhance OCRing parameter modes and enhances Arabic OCRing system accuracy from reinforcement strategy. Therefore, the article achieves a significant improvement of OCRing results due to the three parameters: language identification, document category, and RoI types (Table, Title, Paragraph, figure, and list). This model is based on \u201cregion proposal algorithm\u201d as a basis of CNN object detectors to find the number of the RoIs. Therefore, the proposed framework performs three distinctive tasks: (1) CNN architecture for adversarial training, (2) an implementation of the FGSM with Keras and TensorFlow, and (3) an adversarial training script implementation with the CNN and the FGSM method. The experiments on Arabic manuscript dataset including Arabic text, English/Arabic documents, and Latin digits\u2019 datasets, demonstrate the accuracy of the proposed method. Moreover, the proposed framework performs well and succeeded in defending against adversarial attacks or adversarial images. The experimental results on our collected dataset illustrate the novelty of our proposed framework over the other existing PSM methods to be extended and updated to improve the quality of the OCRing system. The results show that the influence of PSM after expanding using the RoI types, language ID, and document/manuscript category can improve the OCRing accuracy. Also, the experimental results show significant performance by the new framework model with accuracy reached to 99% compared to relative methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1403342188",
                        "name": "Hassanin M. Al-Barhamtoshy"
                    },
                    {
                        "authorId": "20864050",
                        "name": "K. Jambi"
                    },
                    {
                        "authorId": "144474786",
                        "name": "M. Rashwan"
                    },
                    {
                        "authorId": "143993049",
                        "name": "Sherif M. Abdou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Driven by the success of object detection [17, 42] and semantic segmentation [8, 31] in images, layout analysis in documents is also framed as detection and segmentation tasks in some works [26,43,62], where detector models are trained to detect semantically coherent text blocks as objects."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d0f4407d7cbf5bb607f99e0a30fc7e311b486b60",
                "externalIds": {
                    "ArXiv": "2203.15143",
                    "DBLP": "journals/corr/abs-2203-15143",
                    "DOI": "10.1109/CVPR52688.2022.00112",
                    "CorpusId": 247778779
                },
                "corpusId": 247778779,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d0f4407d7cbf5bb607f99e0a30fc7e311b486b60",
                "title": "Towards End-to-End Unified Scene Text Detection and Layout Analysis",
                "abstract": "Scene text detection and document layout analysis have long been treated as two separate tasks in different image domains. In this paper, we bring them together and introduce the task of unified scene text detection and layout analysis. The first hierarchical scene text dataset is introduced to enable this novel research task. We also propose a novel method that is able to simultaneously detect scene text and form text clusters in a unified way. Comprehensive experiments show that our unified model achieves better performance than multiple well-designed baseline methods. Additionally, this model achieves state-of-the-art results on multiple scene text detection datasets without the need of complex post-processing. Dataset and code: https://github.com/google-research-datasets/hiertext.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51126615",
                        "name": "Shangbang Long"
                    },
                    {
                        "authorId": "3407327",
                        "name": "Siyang Qin"
                    },
                    {
                        "authorId": "122970345",
                        "name": "Dmitry Panteleev"
                    },
                    {
                        "authorId": "1726358",
                        "name": "A. Bissacco"
                    },
                    {
                        "authorId": "2114175058",
                        "name": "Yasuhisa Fujii"
                    },
                    {
                        "authorId": "3063676",
                        "name": "Michalis Raptis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[15] is the first to apply FCN based semantic segmentation models to the TSR task."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "29942b5588d9978af5da046ebfc642e4779b8597",
                "externalIds": {
                    "ArXiv": "2203.09056",
                    "DBLP": "journals/pr/MaLSH23",
                    "DOI": "10.1016/j.patcog.2022.109006",
                    "CorpusId": 247518554
                },
                "corpusId": 247518554,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/29942b5588d9978af5da046ebfc642e4779b8597",
                "title": "Robust Table Detection and Structure Recognition from Heterogeneous Document Images",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Generally speaking, early attempts can be divided into the categories of textual-based [4,6,10], convolution-based [12,15, 24,26,34] and GCN-based [19] methods."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "93c1dffe2bae737da8f342fd749aa783df572a14",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-06947",
                    "ArXiv": "2203.06947",
                    "DOI": "10.1109/CVPR52688.2022.00454",
                    "CorpusId": 247446585
                },
                "corpusId": 247446585,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/93c1dffe2bae737da8f342fd749aa783df572a14",
                "title": "XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding",
                "abstract": "Recently, various multimodal networks for Visually-Rich Document Understanding(VRDU) have been proposed, showing the promotion of transformers by integrating visual and layout information with the text embeddings. However, most existing approaches utilize the position embeddings to incorporate the sequence information, neglecting the noisy improper reading order obtained by OCR tools. In this paper, we propose a robust layout-aware multimodal network named XYLayoutLM to capture and leverage rich layout information from proper reading orders produced by our Augmented XY Cut. Moreover, a Dilated Conditional Position Encoding module is proposed to deal with the input sequence of variable lengths, and it additionally extracts local layout information from both textual and vi-sual modalities while generating position embeddings. Experiment results show that our XYLayoutLM achieves competitive results on document understanding tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "148427191",
                        "name": "Zhangxuan Gu"
                    },
                    {
                        "authorId": "2114323322",
                        "name": "Changhua Meng"
                    },
                    {
                        "authorId": "2160908253",
                        "name": "Ke Wang"
                    },
                    {
                        "authorId": "2158820459",
                        "name": "Jun Lan"
                    },
                    {
                        "authorId": null,
                        "name": "Weiqiang Wang"
                    },
                    {
                        "authorId": "2158815171",
                        "name": "Ming Gu"
                    },
                    {
                        "authorId": "2108911758",
                        "name": "Liqing Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One such method [2] trains a deep learning network on Marmot dataset [9] for locating structural components like rows, columns and cells.",
                "Recent methods such as TableNet [1], DeepDeSRT [2], Graph Neural Networks based table recognition [3] often fail to work on very complex tables due to their low generalization ability.",
                "Table 1: Performance Comparison of TSR-DSAW against TableNet [1] and DeepDeSRT [2].",
                "Experimental Results: Now, we present comparison results of TSR-DSAW against two previous methods of TSR - TableNet [1] and DeepDeSRT [2]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "51352def34edefacc5ef57b6358620bf3f65a7d1",
                "externalIds": {
                    "ArXiv": "2203.06873",
                    "DBLP": "journals/corr/abs-2203-06873",
                    "MAG": "3205690813",
                    "DOI": "10.14428/esann/2021.es2021-109",
                    "CorpusId": 244573440
                },
                "corpusId": 244573440,
                "publicationVenue": {
                    "id": "93d6c444-c90a-48ee-a3ad-ef1f015bc28a",
                    "name": "The European Symposium on Artificial Neural Networks",
                    "type": "conference",
                    "alternate_names": [
                        "Eur Symp Artif Neural Netw",
                        "ESANN"
                    ],
                    "url": "https://www.esann.org/"
                },
                "url": "https://www.semanticscholar.org/paper/51352def34edefacc5ef57b6358620bf3f65a7d1",
                "title": "TSR-DSAW: Table Structure Recognition via Deep Spatial Association of Words",
                "abstract": "Existing methods for Table Structure Recognition (TSR) from camera-captured or scanned documents perform poorly on complex tables consisting of nested rows / columns, multi-line texts and missing cell data. This is because current data-driven methods work by simply training deep models on large volumes of data and fail to generalize when an unseen table structure is encountered. In this paper, we propose to train a deep network to capture the spatial associations between different word pairs present in the table image for unravelling the table structure. We present an end-to-end pipeline, named TSR-DSAW: TSR via Deep Spatial Association of Words, which outputs a digital representation of a table image in a structured format such as HTML. Given a table image as input, the proposed method begins with the detection of all the words present in the image using a text-detection network like CRAFT which is followed by the generation of word-pairs using dynamic programming. These word-pairs are highlighted in individual images and subsequently, fed into a DenseNet-121 classifier trained to capture spatial associations such as same-row, same-column, same-cell or none. Finally, we perform post-processing on the classifier output to generate the table structure in HTML format. We evaluate our TSR-DSAW pipeline on two public table-image datasets -- PubTabNet and ICDAR 2013, and demonstrate improvement over previous methods such as TableNet and DeepDeSRT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1560540918",
                        "name": "Arushi Jain"
                    },
                    {
                        "authorId": "46956825",
                        "name": "Shubham Paliwal"
                    },
                    {
                        "authorId": "145092223",
                        "name": "Monika Sharma"
                    },
                    {
                        "authorId": "3213990",
                        "name": "L. Vig"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [24] is a deep learning based approach that aims at detecting tables and recognizing table structures.",
                "DeepDeSRT [24] uses semantic segmentation to segment the rows and columns in a table, meaning that it only relies on the visual features."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "007f2dc393e363abcfd023c1a1b47f5b84d4b2ae",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-03819",
                    "ArXiv": "2203.03819",
                    "DOI": "10.48550/arXiv.2203.03819",
                    "CorpusId": 247315039
                },
                "corpusId": 247315039,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/007f2dc393e363abcfd023c1a1b47f5b84d4b2ae",
                "title": "Table Structure Recognition with Conditional Attention",
                "abstract": "Tabular data in digital documents is widely used to express compact and important information for readers. However, it is challenging to parse tables from unstructured digital documents, such as PDFs and images, into machine-readable format because of the complexity of table structures and the missing of meta-information. Table Structure Recognition (TSR) problem aims to recognize the structure of a table and transform the unstructured tables into a structured and machine-readable format so that the tabular data can be further analysed by the down-stream tasks, such as semantic modeling and information retrieval. In this study, we hypothesize that a complicated table structure can be represented by a graph whose vertices and edges represent the cells and association between cells, respectively. Then we define the table structure recognition problem as a cell association classification problem and propose a conditional attention network (CATT-Net). The experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods on various datasets. Besides, we investigate whether the alignment of a cell bounding box or a text-focused approach has more impact on the model performance. Due to the lack of public dataset annotations based on these two approaches, we further annotate the ICDAR2013 dataset providing both types of bounding boxes, which can be a new benchmark dataset for evaluating the methods in this field. Experimental results show that the alignment of a cell bounding box can help improve the Micro-averaged F1 score from 0.915 to 0.963, and the Macro-average F1 score from 0.787 to 0.923.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144025674",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "081312b993ef1df7e64e1a5f8715852a2e7ed527",
                "externalIds": {
                    "ArXiv": "2203.01017",
                    "DBLP": "journals/corr/abs-2203-01017",
                    "DOI": "10.1109/CVPR52688.2022.00457",
                    "CorpusId": 247218660
                },
                "corpusId": 247218660,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/081312b993ef1df7e64e1a5f8715852a2e7ed527",
                "title": "TableFormer: Table Structure Understanding with Transformers",
                "abstract": "Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortu-nately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct iden-tification of the table-structure from an image is a nontrivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from program-matic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "37525891",
                        "name": "A. Nassar"
                    },
                    {
                        "authorId": "2051177400",
                        "name": "Nikolaos Livathinos"
                    },
                    {
                        "authorId": "1578242957",
                        "name": "Maksym Lysak"
                    },
                    {
                        "authorId": "1831851",
                        "name": "P. Staar"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a9e00a1c3de05e3d06a4287b8ef07096f0cfe609",
                "externalIds": {
                    "DBLP": "journals/pr/RibaGTRFL22",
                    "DOI": "10.1016/j.patcog.2022.108641",
                    "CorpusId": 247427729
                },
                "corpusId": 247427729,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a9e00a1c3de05e3d06a4287b8ef07096f0cfe609",
                "title": "Table detection in business document images by message passing networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40420775",
                        "name": "Pau Riba"
                    },
                    {
                        "authorId": "145029669",
                        "name": "Lutz Goldmann"
                    },
                    {
                        "authorId": "3045937",
                        "name": "O. R. Terrades"
                    },
                    {
                        "authorId": "1491424368",
                        "name": "Diede Rusticus"
                    },
                    {
                        "authorId": "1686569",
                        "name": "A. Forn\u00e9s"
                    },
                    {
                        "authorId": "143826881",
                        "name": "J. Llad\u00f3s"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several published works [46,42,56] exploit the advances in object detection [40,19] to further improve the accuracy in document layout analysis."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6289874e3b499593dddd51ae229062fe07d30b56",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-12985",
                    "ArXiv": "2202.12985",
                    "DOI": "10.1007/978-3-031-25069-9_16",
                    "CorpusId": 247158436
                },
                "corpusId": 247158436,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6289874e3b499593dddd51ae229062fe07d30b56",
                "title": "OCR-IDL: OCR Annotations for Industry Document Library Dataset",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35570245",
                        "name": "Ali Furkan Biten"
                    },
                    {
                        "authorId": "134682605",
                        "name": "Rub\u00e8n P\u00e9rez Tito"
                    },
                    {
                        "authorId": "51231577",
                        "name": "Llu\u00eds G\u00f3mez"
                    },
                    {
                        "authorId": "2864362",
                        "name": "Ernest Valveny"
                    },
                    {
                        "authorId": "1694974",
                        "name": "Dimosthenis Karatzas"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3a562b74e3dbc9cefd2245a2015346558b63df81",
                "externalIds": {
                    "DBLP": "journals/sncs/AjijPRH22",
                    "DOI": "10.1007/s42979-022-01041-z",
                    "CorpusId": 246809635
                },
                "corpusId": 246809635,
                "publicationVenue": {
                    "id": "7a7dc89b-e1a6-44df-a496-46c330a87840",
                    "name": "SN Computer Science",
                    "type": "journal",
                    "alternate_names": [
                        "SN Comput Sci"
                    ],
                    "issn": "2661-8907",
                    "alternate_issns": [
                        "2662-995X"
                    ],
                    "url": "https://link.springer.com/journal/42979"
                },
                "url": "https://www.semanticscholar.org/paper/3a562b74e3dbc9cefd2245a2015346558b63df81",
                "title": "Robust Detection of Tables in Documents Using Scores from Table Cell Cores",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7536937",
                        "name": "Md. Ajij"
                    },
                    {
                        "authorId": "3267968",
                        "name": "Sanjoy Pratihar"
                    },
                    {
                        "authorId": "36929516",
                        "name": "D. S. Roy"
                    },
                    {
                        "authorId": "1797464",
                        "name": "T. Hanne"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [18] became the first popular object detection approach for table detection"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9634b09af379de5839657b9f0ddea73806db138b",
                "externalIds": {
                    "ArXiv": "2201.11438",
                    "DBLP": "journals/corr/abs-2201-11438",
                    "CorpusId": 246294806
                },
                "corpusId": 246294806,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9634b09af379de5839657b9f0ddea73806db138b",
                "title": "DocSegTr: An Instance-Level End-to-End Document Image Segmentation Transformer",
                "abstract": "Understanding documents with rich layouts is an essential step towards information extraction. Business intelligence processes often require the extraction of useful semantic content from documents at a large scale for subsequent decision-making tasks. In this context, instance-level segmentation of different document objects (title, sections, figures etc.) has emerged as an interesting problem for the document analysis and understanding community. To advance the research in this direction, we present a transformer-based model called \\emph{DocSegTr} for end-to-end instance segmentation of complex layouts in document images. The method adapts a twin attention module, for semantic reasoning, which helps to become highly computationally efficient compared with the state-of-the-art. To the best of our knowledge, this is the first work on transformer-based document segmentation. Extensive experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical Japanese (HJ) and TableBank demonstrate that our model achieved comparable or better segmentation performance than the existing state-of-the-art approaches with the average precision of 89.4, 40.3, 83.4 and 93.3. This simple and flexible framework could serve as a promising baseline for instance-level recognition tasks in document images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2150473007",
                        "name": "Sanket Biswas"
                    },
                    {
                        "authorId": "2153585985",
                        "name": "Ayan Banerjee"
                    },
                    {
                        "authorId": "2117637297",
                        "name": "Josep Llad'os"
                    },
                    {
                        "authorId": "144167309",
                        "name": "U. Pal"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "287bcf827cd4522031609f4e52f7d435a5971aa6",
                "externalIds": {
                    "DBLP": "journals/mta/MishraKC23",
                    "DOI": "10.1007/s11042-021-11582-9",
                    "CorpusId": 245834995
                },
                "corpusId": 245834995,
                "publicationVenue": {
                    "id": "477368e9-7a8e-475a-8c93-6d623797fd06",
                    "name": "Multimedia tools and applications",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Tools and Applications",
                        "Multimedia Tool Appl",
                        "Multimedia tool appl"
                    ],
                    "issn": "1380-7501",
                    "url": "https://www.springer.com/computer/information+systems+and+applications/journal/11042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/287bcf827cd4522031609f4e52f7d435a5971aa6",
                "title": "Graph Interpretation, Summarization and Visualization Techniques: A Review and Open Research Issues",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2068143050",
                        "name": "P. Mishra"
                    },
                    {
                        "authorId": "2143544359",
                        "name": "Santosh Kumar"
                    },
                    {
                        "authorId": "145655298",
                        "name": "M. K. Chaube"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The DL-based methods in [7, 11] are among the first to apply neural networks designed for object detection to table parsing.",
                "As pioneering works in table structure parsing, [3] and [7] have both included a review of works in table structure recognition prior to DL.",
                "After [7], researchers have started to revisit table structure parsing with DL methods, which turned out highly promising compared to the rule-based (e.",
                "Table detection is a popular task with a large body of literature, table structure parsing and table recognition were revisited2 after the pioneering work of [7] using state-of-the-art deep neural networks.",
                "The success of DL has marked the revisiting of table structure parsing by [7], which inspired follow-up research [27, 1, 6, 2, 28, 29, 30, 19, 31, 18, 32, 17]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "10c3efc40e72674b615864c94a231e1f11913619",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-01654",
                    "ArXiv": "2201.01654",
                    "CorpusId": 245704311
                },
                "corpusId": 245704311,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/10c3efc40e72674b615864c94a231e1f11913619",
                "title": "TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets",
                "abstract": "Tables have been an ever-existing structure to store data. There exist now different approaches to store tabular data physically. PDFs, images, spreadsheets, and CSVs are leading examples. Being able to parse table structures and extract content bounded by these structures is of high importance in many applications. In this paper, we devise TableParser, a system capable of parsing tables in both native PDFs and scanned images with high precision. We have conducted extensive experiments to show the efficacy of domain adaptation in developing such a tool. Moreover, we create TableAnnotator and ExcelAnnotator, which constitute a spreadsheet-based weak supervision mechanism and a pipeline to enable table parsing. We share these resources with the research community to facilitate further research in this interesting direction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1576005199",
                        "name": "Susie Xi Rao"
                    },
                    {
                        "authorId": "5185738",
                        "name": "Johannes Rausch"
                    },
                    {
                        "authorId": "2074406990",
                        "name": "P. Egger"
                    },
                    {
                        "authorId": "1776014",
                        "name": "Ce Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other researchers have used transfer learning to alleviate the lack of training data  [23, 26].",
                "For instance, in [26, 29, 30, 32], the authors considered columns and rows as object types and reconstructed table structures from the detected rows and columns.",
                ", object detection and segmentation), table recognition has not been formulated in a principled way so far, and deep neural networks have only been used to address sub-tasks in heuristically designed steps [26, 30].",
                "[26] suggested the first method to address the table detection and structure recognition problems using deep learning."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d9c676eccdff80a7d1154660a09c4d2ea5164610",
                "externalIds": {
                    "DOI": "10.1007/s11042-021-11819-7",
                    "CorpusId": 254860167
                },
                "corpusId": 254860167,
                "publicationVenue": {
                    "id": "477368e9-7a8e-475a-8c93-6d623797fd06",
                    "name": "Multimedia tools and applications",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Tools and Applications",
                        "Multimedia Tool Appl",
                        "Multimedia tool appl"
                    ],
                    "issn": "1380-7501",
                    "url": "https://www.springer.com/computer/information+systems+and+applications/journal/11042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d9c676eccdff80a7d1154660a09c4d2ea5164610",
                "title": "Deep-learning and graph-based approach to table structure recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115242015",
                        "name": "Eunji Lee"
                    },
                    {
                        "authorId": "2370571",
                        "name": "Jaewoo Park"
                    },
                    {
                        "authorId": "2463454",
                        "name": "H. Koo"
                    },
                    {
                        "authorId": "1707645",
                        "name": "N. Cho"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fffb4d2f028cef2628f342e8125aba9ce70a3a25",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-14070",
                    "ArXiv": "2112.14070",
                    "CorpusId": 245537893
                },
                "corpusId": 245537893,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fffb4d2f028cef2628f342e8125aba9ce70a3a25",
                "title": "Intelligent Document Processing - Methods and Tools in the real world",
                "abstract": "The originality of this publication is to look at the subject of IDP (Intelligent Document Processing) from the perspective of an end-user and industrialist and not that of a Computer Science researcher. This domain is one part of the challenge of information digitalisation that constitutes the Industrial Revolution of the twenty first century (Industry 4.0) and this paper looks specifically at the difficult areas of classifying, extracting information and subsequent integration into business processes with respect to forms and invoices. Since the focus is on practical implementation a brief review is carried out of the market in commercial tools for OCR, document classification and data extraction in so far as this is publicly available together with pricing (if known). Brief definitions of the main terms encountered in Computer Science publications and commercial prospectuses are provided in order to de-mystify the language for the layman. A small number of practical tests are carried out on a few real documents in order to illustrate the capabilities of tools that are commonly available at a reasonable price. The unsolved (so far) issue of tables contained in invoices is raised. The case of a typical large industrial company is evoked where the requirement is to extract 100 per cent of the information with 100 per cent reliability in order to integrate into the back-end Enterprise Resource Planning system. Finally a brief description is given of the state-of-the-art research by the huge corporations who are pushing the boundaries of deep learning techniques further and further with massive computing and financial power - progress that will undoubtedly trickle down into the real world at some later date. The paper finishes by asking the question whether the objectives and timing of the commercial world and the progress of Computer Science are fully aligned.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148498372",
                        "name": "Graham A. Cutting"
                    },
                    {
                        "authorId": "1399442452",
                        "name": "A. Cutting-Decelle"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Due to recent rapid development of image acquisition devices, data storage and processing capacity, document processing has made a great advancements [6, 26]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1c3c43c40af13956b175e06a1507e004902d562c",
                "externalIds": {
                    "DOI": "10.1145/3490035.3490306",
                    "CorpusId": 245132736
                },
                "corpusId": 245132736,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1c3c43c40af13956b175e06a1507e004902d562c",
                "title": "TexRGAN: a deep adversarial framework for text restoration from deformed handwritten documents",
                "abstract": "Free form handwritten document images commonly contains deformed text images such as struck-out and underlined words. The deformed text images drastically degrades the performance of intensely used document processing applications like optical character recognition (OCR). Here we propose an end-to-end text image restoration system based on generative adversarial network (GAN). The proposed model TexRGAN is perhaps the first attempt to restore deformed handwritten texts like struck-out and underlined text images using GAN model and it simultaneously handles strikeout and underline words both with a single deep network model. The proposed GAN model uses spatial as well as structural loss to generate restored text images from a given deformed text image input as condition. The proposed network is trained in weakly supervised approach to avoid unavailability of training data and the cost and error of manual annotations. We evaluate the performance of the proposed TexRGAN on various types of deformation and shapes of strike-through-strokes such as slanted strokes, nearly straight strokes, multiple strokes, underlines, crossed strokes etc. The TexRGAN is also evaluated directly in terms the OCR performance. The evaluation metrics show robustness and applicability in real-world scenario.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "29831149",
                        "name": "Arnab Poddar"
                    },
                    {
                        "authorId": "2146976472",
                        "name": "Akash Chakraborty"
                    },
                    {
                        "authorId": "143769377",
                        "name": "J. Mukhopadhyay"
                    },
                    {
                        "authorId": "1758797",
                        "name": "P. Biswas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "With the development of deep learning, DeepDeSRT [7] was proposed, which uses deep learning to conduct table detection and table structure recognition, that is, to identify the positions of rows, columns and cells in the detected table."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a5adea80377a623011fbc63b8dc686da01821126",
                "externalIds": {
                    "DBLP": "conf/icncc/FanTLZ21",
                    "DOI": "10.1145/3510513.3510515",
                    "CorpusId": 248572782
                },
                "corpusId": 248572782,
                "publicationVenue": {
                    "id": "12ed15a2-5218-48ca-a29f-e39af72dc504",
                    "name": "International Conference on Network, Communication and Computing",
                    "type": "conference",
                    "alternate_names": [
                        "ICNCC",
                        "International Conference Network, Communication and Computing",
                        "Int Conf Netw Commun Comput"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a5adea80377a623011fbc63b8dc686da01821126",
                "title": "DeTable: Table data extraction model based on deep",
                "abstract": "The rapid development of the information age leads to the mass production and frequent transmission of data, which is difficult to deal with by human alone. With the rise and development of artificial intelligence, the use of data is becoming more efficient. Table, as a special data form, has attracted wide attention gradually. However, extracting data from table subimages presents a number of challenges, including accurately detecting table regions in the image, and then detecting and extracting information from detected table rows and columns. While some progress has been made in table detection, extracting table contents remains a challenge because it involves more fine-grained table structure identification. In this paper, DeTable: table data extraction model based on deep learning is proposed. The model uses the interdependence between the twin tasks of table detection and table structure recognition to divide the text region and the box-line region of the table. Then, rows based on semantic rules are extracted from the identified table regions. The proposed models and extraction methods were evaluated on publicly available ICDAR 2019 and Marmot table datasets and the most advanced results were obtained.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Yuxuan Fan"
                    },
                    {
                        "authorId": "30726645",
                        "name": "Huobin Tan"
                    },
                    {
                        "authorId": "6782471",
                        "name": "Yu Liu"
                    },
                    {
                        "authorId": "2164727626",
                        "name": "Jingxuan Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Therefore, subsequent post-processing steps are necessary [16], [18], [20]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1439ff1476c5222a7ad3f7c8fd5991fcc7ccf215",
                "externalIds": {
                    "DBLP": "conf/ssci/RamamurthyLBGUU21",
                    "DOI": "10.1109/SSCI50451.2021.9659977",
                    "CorpusId": 246289542
                },
                "corpusId": 246289542,
                "publicationVenue": {
                    "id": "8a9e9f3b-a025-473d-801e-72cdb0653d22",
                    "name": "IEEE Symposium Series on Computational Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Symp Ser Comput Intell",
                        "SSCI"
                    ],
                    "url": "http://www.ieee-ssci.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1439ff1476c5222a7ad3f7c8fd5991fcc7ccf215",
                "title": "Automatic Indexing of Financial Documents via Information Extraction",
                "abstract": "The problem of extracting information from large volumes of unstructured documents is pervasive in the domain of financial business. Enterprises and investors need automatic methods that can extract information from these documents, particularly for indexing and efficiently retrieving information. To this end, we present a scalable end-to-end document processing system for indexing and information retrieval from large volumes of financial documents. While we show our system works for the use case of financial document processing, the entire system itself is agnostic of the document type and machine learning model type. Thus, it can be applied to any large-scale document processing task involving domain-specific extractors.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "21780262",
                        "name": "Rajkumar Ramamurthy"
                    },
                    {
                        "authorId": "1396407321",
                        "name": "Max L\u00fcbbering"
                    },
                    {
                        "authorId": "1381415318",
                        "name": "Thiago Bell"
                    },
                    {
                        "authorId": "34665345",
                        "name": "M. Gebauer"
                    },
                    {
                        "authorId": "2151200759",
                        "name": "Bilge Ulusay"
                    },
                    {
                        "authorId": "2151201280",
                        "name": "Daniel Uedelhoven"
                    },
                    {
                        "authorId": "1396757045",
                        "name": "T. Khameneh"
                    },
                    {
                        "authorId": "2090348766",
                        "name": "R\u00fcdiger Loitz"
                    },
                    {
                        "authorId": "153758438",
                        "name": "Maren Pielka"
                    },
                    {
                        "authorId": "1692283",
                        "name": "C. Bauckhage"
                    },
                    {
                        "authorId": "2018549",
                        "name": "R. Sifa"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the field of document analysis, the combination of Support Vector Machine and Deep Neural Network initially replaced traditional machine learning to promote RPA to help solve unstructured document data [22], thus, the convolution model has been gradually used for document analysis [23,24].",
                "[24] proposed that DeepDeSRT could conduct table detection and structure identification, as based on Faster R-CNN, to identify the positions of rows, columns, and cells in a table."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "86c3f187c3553e5a8eb5980b689db9636ffe053c",
                "externalIds": {
                    "DOI": "10.3390/app112311446",
                    "CorpusId": 244891812
                },
                "corpusId": 244891812,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/86c3f187c3553e5a8eb5980b689db9636ffe053c",
                "title": "U-SSD: Improved SSD Based on U-Net Architecture for End-to-End Table Detection in Document Images",
                "abstract": "Tables are an important element in a document and can express more information with fewer words. Due to the different arrangements of tables and texts, as well as the variety of layouts, table detection is a challenge in the field of document analysis. Nowadays, as Optical Character Recognition technology has gradually matured, it can help us to obtain text information quickly, and the ability to accurately detect table structures can improve the efficiency of obtaining text content. The process of document digitization is influenced by the editor\u2019s style on the table layout. In addition, many industries rely on a large number of people to process data, which has high expense, thus, the industry imports artificial intelligence and Robotic Process Automation to handle simple and complicated routine text digitization work. Therefore, this paper proposes an end-to-end table detection model, U-SSD, as based on the object detection method of deep learning, takes the Single Shot MultiBox Detector (SSD) as the basic model architecture, improves it by U-Net, and adds dilated convolution to enhance the feature learning capability of the network. The experiment in this study uses the dataset of accident claim documents, as provided by a Taiwanese Law Firm, and conducts table detection. The experimental results show that the proposed method is effective. In addition, the results of the evaluation on open dataset of TableBank, Github, and ICDAR13 show that the SSD-based network architectures can achieve good performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116353555",
                        "name": "Shih-Hsiung Lee"
                    },
                    {
                        "authorId": "2108261827",
                        "name": "Hung-Chun Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9fb2744ef2b91033de39c121be25d3f86f759458",
                "externalIds": {
                    "DBLP": "conf/cvpr/0003LLJL022",
                    "ArXiv": "2111.13359",
                    "DOI": "10.1109/CVPR52688.2022.00449",
                    "CorpusId": 244709555
                },
                "corpusId": 244709555,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9fb2744ef2b91033de39c121be25d3f86f759458",
                "title": "Neural Collaborative Graph Machines for Table Structure Recognition",
                "abstract": "Recently, table structure recognition has achieved impressive progress with the help of deep graph models. Most of them exploit single visual cues of tabular elements or simply combine visual cues with other modalities via early fusion to reason their graph relationships. However, neither early fusion nor individually reasoning in terms of multiple modalities can be appropriate for all varieties of table structures with great diversity. Instead, different modalities are expected to collaborate with each other in different patterns for different table cases. In the community, the importance of intrainter modality interactions for table structure reasoning is still unexplored. In this paper, we define it as heterogeneous table structure recognition (HeteroTSR) problem. With the aim offilling this gap, we present a novel Neural Collaborative Graph Machines (NCGM) equipped with stacked collaborative blocks, which alternatively extracts intramodality context and models inter-modality interactions in a hierarchical way. It can represent the intrainter modality relationships of tabular elements more robustly, which significantly improves the recognition performance. We also show that the proposed NCGM can modulate collaborative pattern of different modalities conditioned on the context of intramodality cues, which is vital for diversified table cases. Experimental results on benchmarks demonstrate our proposed NCGM achieves state-of-the-art performance and beats other contemporary methods by a large margin especially under challenging scenarios.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48446712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2149124539",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also compare performance of several best methods on this dataset in Table 5, including rule-based method using text extracted from PDF files [43] and page images [2], deep CNN-basedmethods likeDeepDeSRT [16], TableNet [23], CascadeTableNet [18], and the result of two commercial systems ABBYY FineReader 11.0 and OmniPage 18 Professional.",
                "The DeepDeSRT [16], DeCNT [17], and CascadeTableNet [18] use Faster R-CNN as the basic framework for table detection.",
                "For example, the DeepDeSRT and TableNet were tested on a set of randomly chosen 34 images since the remaining images were used for training [16,23].",
                "We also compare performance of several best methods on this dataset in Table 5, including rule-based method using text extracted from PDF files [43] and page images [2], deep CNN-basedmethods likeDeepDeSRT [16], TableNet [23], CascadeTableNet [18], and the result of two commercial systems ABBYY FineReader 11.",
                "For learning feature maps, DeepDeSRT uses ZFNet [19] and VGG-16 [10] as the backbone, while DeCNT uses the deformable convolution."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9eb3421792bf9e411b12d7f82c3256cafa7cf2a0",
                "externalIds": {
                    "DOI": "10.1007/s10032-021-00390-4",
                    "CorpusId": 254112157
                },
                "corpusId": 254112157,
                "publicationVenue": {
                    "id": "46722266-4327-4a22-871d-5a9fe3c5ad0c",
                    "name": "International Journal on Document Analysis and Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Doc Anal Recognit"
                    ],
                    "issn": "1433-2825",
                    "url": "https://link.springer.com/journal/10032"
                },
                "url": "https://www.semanticscholar.org/paper/9eb3421792bf9e411b12d7f82c3256cafa7cf2a0",
                "title": "TableSegNet: a fully convolutional network for table detection and segmentation in document images",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2149981",
                        "name": "D. Nguyen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In Section 2.1, we introduced the application of the Convolutional Neural Network (CNN) in document layout analysis (He et al., 2015; Ren et al., 2016; He et al., 2018; Liu et al., 2016; Redmon & Farhadi, 2018; Yang et al., 2017a; Schreiber et al., 2017).",
                "Schreiber et al. (2017) first apply the Faster R-CNN model to table detection and recognition in document layout analysis as shown in Figure 2, achieving SOTA performance in the ICDAR 2013 table detection dataset (Go\u0308bel et al., 2013).",
                "1, we introduced the application of the Convolutional Neural Network (CNN) in document layout analysis (He et al., 2015; Ren et al., 2016; He et al., 2018; Liu et al., 2016; Redmon & Farhadi, 2018; Yang et al., 2017a; Schreiber et al., 2017)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "34c61db889ce5fd1002b9c1cd2331bfa1072cef7",
                "externalIds": {
                    "ArXiv": "2111.08609",
                    "DBLP": "journals/corr/abs-2111-08609",
                    "CorpusId": 244130192
                },
                "corpusId": 244130192,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/34c61db889ce5fd1002b9c1cd2331bfa1072cef7",
                "title": "Document AI: Benchmarks, Models and Applications",
                "abstract": "Document AI, or Document Intelligence, is a relatively new research topic that refers to the techniques for automatically reading, understanding, and analyzing business documents. It is an important research direction for natural language processing and computer vision. In recent years, the popularity of deep learning technology has greatly advanced the development of Document AI, such as document layout analysis, visual information extraction, document visual question answering, document image classification, etc. This paper briefly reviews some of the representative models, tasks, and benchmark datasets. Furthermore, we also introduce early-stage heuristic rule-based document analysis, statistical machine learning algorithms, and deep learning approaches especially pre-training methods. Finally, we look into future directions for Document AI research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2114843952",
                        "name": "Lei Cui"
                    },
                    {
                        "authorId": "3032611",
                        "name": "Yiheng Xu"
                    },
                    {
                        "authorId": "1379581011",
                        "name": "Tengchao Lv"
                    },
                    {
                        "authorId": "49807919",
                        "name": "Furu Wei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Cognitive methods in this space broadly classified into five categories \u2014 image-to-sequence models [17, 2, 14], segmentation networks [26, 18, 20, 23], graph formulations [22, 4, 24], conditional generative adversarial networks [16] and a recent multi-modal method by [40].",
                "For DeepDeSRT [26], we use our implementation.",
                "In order to compare our method against others on TUCD dataset, we develop our implementation of DeepDeSRT [26], and use open source implementations of DGCNN (TIES) [22], SPLERGE [30], and TabStruct-Net [24]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d499cd8a77e7fc4e12d1f5bd7875ce2e3f5465a4",
                "externalIds": {
                    "DBLP": "conf/wacv/RajaMV22",
                    "ArXiv": "2111.07129",
                    "DOI": "10.1109/WACV51458.2022.00260",
                    "CorpusId": 240285297
                },
                "corpusId": 240285297,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/d499cd8a77e7fc4e12d1f5bd7875ce2e3f5465a4",
                "title": "Visual Understanding of Complex Table Structures from Document Images",
                "abstract": "Table structure recognition is necessary for a comprehensive understanding of documents. Tables in unstructured business documents are tough to parse due to the high diversity of layouts, varying alignments of contents, and the presence of empty cells. The problem is particularly difficult because of challenges in identifying individual cells using visual or linguistic contexts or both. Accurate detection of table cells (including empty cells) simplifies structure extraction and hence, it becomes the prime focus of our work. We propose a novel object-detection-based deep model that captures the inherent alignments of cells within tables and is fine-tuned for fast optimization. Despite accurate detection of cells, recognizing structures for dense tables may still be challenging because of difficulties in capturing long-range row/column dependencies in presence of multi-row/column spanning cells. Therefore, we also aim to improve structure recognition by deducing a novel rectilinear graph-based formulation. From a semantics perspective, we highlight the significance of empty cells in a table. To take these cells into account, we suggest an enhancement to a popular evaluation criterion. Finally, we introduce a modestly sized evaluation dataset with an annotation style inspired by human cognition to encourage new approaches to the problem. Our framework improves the previous state-of-the-art performance by a 2.7% average F1-score on benchmark datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2053910624",
                        "name": "S. Raja"
                    },
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "2226828175",
                        "name": "Jawahar C V"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT, a deep learning neural network, performs image analysis to identify table formats and constructs templates [27]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "87855468ffaf2594f28968044bed937575cffd77",
                "externalIds": {
                    "DBLP": "conf/asunam/SetiantoTSDTK21",
                    "DOI": "10.1145/3487351.3492723",
                    "CorpusId": 246036979
                },
                "corpusId": 246036979,
                "publicationVenue": {
                    "id": "255a371e-2a0c-42d1-b4e7-c3cf3e21c89a",
                    "name": "International Conference on Advances in Social Networks Analysis and Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Advances in Social Networks Analysis and Mining",
                        "Int Conf Adv Soc Netw Anal Min",
                        "ASONAM",
                        "Adv Soc Netw Anal Min"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=239"
                },
                "url": "https://www.semanticscholar.org/paper/87855468ffaf2594f28968044bed937575cffd77",
                "title": "GPT-2C: a parser for honeypot logs using large pre-trained language models",
                "abstract": "Deception technologies like honeypots generate large volumes of log data, which include illegal Unix shell commands used by latent intruders. Several prior works have reported promising results in overcoming the weaknesses of network-level and program-level Intrusion Detection Systems (IDSs) by fussing network traffic with data from honeypots. However, because honeypots lack the plug-in infrastructure to enable real-time parsing of log outputs, it remains technically challenging to feed illegal Unix commands into downstream predictive analytics. As a result, advances on honeypot-based user-level IDSs remain greatly hindered. This article presents a run-time system (GPT-2C) that leverages a large pre-trained language model (GPT-2) to parse dynamic logs generated by a live Cowrie SSH honeypot instance. After fine-tuning the GPT-2 model on an existing corpus of illegal Unix commands, the model achieved 89% inference accuracy in parsing Unix commands with acceptable execution latency.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35915638",
                        "name": "Febrian Setianto"
                    },
                    {
                        "authorId": "2126865117",
                        "name": "Erion Tsani"
                    },
                    {
                        "authorId": "2126857544",
                        "name": "Fatima Sadiq"
                    },
                    {
                        "authorId": "17733590",
                        "name": "Georgios Domalis"
                    },
                    {
                        "authorId": "2060868377",
                        "name": "Dimitris Tsakalidis"
                    },
                    {
                        "authorId": "35688956",
                        "name": "Panos Kostakos"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7b23b4285f6f302b96d97b1363685e3551704067",
                "externalIds": {
                    "DBLP": "conf/cikm/00020CP21",
                    "DOI": "10.1145/3459637.3482484",
                    "CorpusId": 240230620
                },
                "corpusId": 240230620,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7b23b4285f6f302b96d97b1363685e3551704067",
                "title": "Tabular Functional Block Detection with Embedding-based Agglomerative Cell Clustering",
                "abstract": "Tables are a widely-used format for data curation. The diversity of domains, layouts, and content of tables makes knowledge extraction challenging. Understanding table layouts is an important step for automatically harvesting knowledge from tabular data. Since table cells are spatially organized into regions, correctly identifying such regions and inferring their functional roles, referred to as functional block detection, is a critical part of understanding table layouts. Earlier functional block detection approaches fail to leverage spatial relationships and higher-level structure, either depending on cell-level predictions or relying on data types as signals for identifying blocks. In this paper, we introduce a flexible functional block detection method by applying agglomerative clustering techniques which merge smaller blocks into larger blocks using two merging strategies. Our proposed method uses cell embeddings with a customized dissimilarity function which utilizes local and margin distances, as well as block coherence metrics to capture cell, block, and table scoped features. Given the diversity of tables in real-world corpora, we also introduce a sampling-based approach for automatically tuning distance thresholds for each table. Experimental results show that our method improves over the earlier state-of-the-art method in terms of several evaluation metrics.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35329068",
                        "name": "Kexuan Sun"
                    },
                    {
                        "authorId": "47939052",
                        "name": "Fei Wang"
                    },
                    {
                        "authorId": "1998918",
                        "name": "Muhao Chen"
                    },
                    {
                        "authorId": "2634786",
                        "name": "J. Pujara"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Method Train Dataset P R F1 DeepDeSRT [32] ICDAR-2013 0.",
                "DeepDeSRT [32] - - SPLERGE [35] SciTSR + ICDAR-2019 0.",
                "[13, 25, 32] attempt to predict row/column boundaries or even invisible grid lines, which are limited in identifying cells spanning multiple rows and columns.",
                "They can be categorized into two groups: non-table-element-based approaches [13, 17, 25, 32, 35, 44] and tableelement-based approaches [2, 19, 27\u201329, 31, 41, 43]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7b99f9995a4b086c0e6913a8d2f109a5dbaa5014",
                "externalIds": {
                    "DBLP": "conf/mm/LiuLLJLRJ21",
                    "DOI": "10.1145/3474085.3481534",
                    "CorpusId": 239011898
                },
                "corpusId": 239011898,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7b99f9995a4b086c0e6913a8d2f109a5dbaa5014",
                "title": "Show, Read and Reason: Table Structure Recognition with Flexible Context Aggregator",
                "abstract": "We investigate the challenging problem of table structure recognition in this work. Many recent methods adopt graph-based context aggregator with strong inductive bias to reason sparse contextual relationships of table elements. However, the strong constraints may be too restrictive to represent the complicated table relationships. In order to learn more appropriate inductive bias from data, we try to introduce Transformer as context aggregator in this work. Nevertheless, Transformer taking dense context as input requires larger scale data and may suffer from unstable training procedure due to the weakening of inductive bias. To overcome the above limitations, we in this paper design a FLAG (FLexible context AGgregator), which marries Transformer with graph-based context aggregator in an adaptive way. Based on FLAG, an end-to-end framework requiring no extra meta-data or OCR information, termed FLAG-Net, is proposed to flexibly modulate the aggregation of dense context and sparse one for the relational reasoning of table elements. We investigate the modulation pattern in FLAG and show what contextual information is focused, which is vital for recognizing table structure. Extensive experimental results on benchmarks demonstrate the performance of our proposed FLAG-Net surpasses other compared methods by a large margin.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48446712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2149124539",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    },
                    {
                        "authorId": "145592290",
                        "name": "R. Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[16] ICDAR 2013 ICDAR 2013 (Test Set) N/A \u00fc \u00fc \u00fc Row/Column Adjacency [19] relations",
                "[16] propose a deep learning based model for Faster-RCNN-based table detection and Fully Convolutional Network (FCN)-based structure recognition to segment the rows and columns in detected table regions."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b8bdf103e70e5bf962f85007462a4d14cfb22c78",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-02933",
                    "ArXiv": "2110.02933",
                    "DOI": "10.1016/j.neucom.2022.09.094",
                    "CorpusId": 238407904
                },
                "corpusId": 238407904,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b8bdf103e70e5bf962f85007462a4d14cfb22c78",
                "title": "On Cropped versus Uncropped Training Sets in Tabular Structure Detection",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2130012695",
                        "name": "Yakup Akkaya"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "152235107",
                        "name": "Shahzad Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "TE is challenging for automated systems [8, 11, 16, 22] due to the wide variety of formats, styles, and layouts found in presented tables.",
                "Recently, there has been a shift in the research literature from traditional rule-based methods [3,10,17] for TE to data-driven methods based on deep learning (DL) [13,16,21].",
                "To get around deficiencies in training data, some approaches model TSR in ways that are only partial solutions to the task, such as row and column detection in DeepDeSRT [16], which ignores spanning cells, or image-tomarkup without text content, as in models trained on TableBank [8].",
                "Modeling approaches One of the most common modeling approaches for TSR is to frame the task as some form of object detection [13, 16, 21]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a94c3e400fc5426a0b8a650b924242abcc1e46f2",
                "externalIds": {
                    "ArXiv": "2110.00061",
                    "DBLP": "conf/cvpr/SmockPA22",
                    "DOI": "10.1109/CVPR52688.2022.00459",
                    "CorpusId": 244462899
                },
                "corpusId": 244462899,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a94c3e400fc5426a0b8a650b924242abcc1e46f2",
                "title": "PubTables-1M: Towards comprehensive table extraction from unstructured documents",
                "abstract": "Recently, significant progress has been made applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, one of the greatest challenges remains the creation of datasets with complete, unambiguous ground truth at scale. To address this, we develop a new, more comprehensive dataset for table extraction, called PubTables-1M. PubTables-1M contains nearly one million tables from scientific articles, supports multiple input modalities, and contains detailed header and location information for table structures, making it useful for a wide variety of modeling approaches. It also addresses a significant source of ground truth inconsistency observed in prior datasets called oversegmentation, using a novel canonicalization procedure. We demonstrate that these improvements lead to a significant increase in training performance and a more reliable estimate of model performance at evaluation for table structure recognition. Further, we show that transformer-based object detection models trained on PubTables-1M produce excellent results for all three tasks of detection, structure recognition, and functional analysis without the need for any special customization for these tasks. Data and code will be released at https://github.com/microsoft/table-transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2394344",
                        "name": "Robin Abraham"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT, a deep learning neural network performs image analysis to identify table formats, constructs templates [17]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "08d208d24124161b99f04608adf033672a47a7c2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-06595",
                    "ArXiv": "2109.06595",
                    "CorpusId": 237502710
                },
                "corpusId": 237502710,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/08d208d24124161b99f04608adf033672a47a7c2",
                "title": "GPT-2C: A GPT-2 parser for Cowrie honeypot logs",
                "abstract": "Deception technologies like honeypots produce comprehensive log reports, but often lack interoperability with EDR and SIEM technologies. A key bottleneck is that existing information transformation plugins perform well on static logs (e.g. geolocation), but face limitations when it comes to parsing dynamic log topics (e.g. user-generated content). In this paper, we present a run-time system (GPT-2C) that leverages large pre-trained models (GPT-2) to parse dynamic logs generate by a Cowrie SSH honeypot. Our fine-tuned model achieves 89\\% inference accuracy in the new domain and demonstrates acceptable execution latency.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35915638",
                        "name": "Febrian Setianto"
                    },
                    {
                        "authorId": "2126865117",
                        "name": "Erion Tsani"
                    },
                    {
                        "authorId": "2126857544",
                        "name": "Fatima Sadiq"
                    },
                    {
                        "authorId": "17733590",
                        "name": "Georgios Domalis"
                    },
                    {
                        "authorId": "2060868377",
                        "name": "Dimitris Tsakalidis"
                    },
                    {
                        "authorId": "35688956",
                        "name": "Panos Kostakos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Limited by this, existing TSP approaches can only handle table structure parsing in a relative simple scenario by grouping detected cells into tables [11, 16, 9, 23].",
                "For a more practical requirement of parsing table structures from images taken by hand-held cameras in the wild, the existing state-of-the-art approaches [13, 14, 11, 16, 9, 23] are prone to fail as the commonly-used assumption of tabular images no longer holds.",
                "However, these methods mainly focuses on the well-conditioned document images, where the tables [16, 23, 12, 9, 23, 14] are well-aligned to the image axes."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7a4c901e00fef01d66efb125bf0e2fdd773d8bc6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-02199",
                    "ArXiv": "2109.02199",
                    "DOI": "10.1109/ICCV48922.2021.00098",
                    "CorpusId": 237420694
                },
                "corpusId": 237420694,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/7a4c901e00fef01d66efb125bf0e2fdd773d8bc6",
                "title": "Parsing Table Structures in the Wild",
                "abstract": "This paper tackles the problem of table structure parsing (TSP) from images in the wild. In contrast to existing studies that mainly focus on parsing well-aligned tabular images with simple layouts from scanned PDF documents, we aim to establish a practical table structure parsing system for real-world scenarios where tabular input images are taken or scanned with severe deformation, bending or occlusions. For designing such a system, we propose an approach named Cycle-CenterNet on the top of CenterNet with a novel cycle-pairing module to simultaneously detect and group tabular cells into structured tables. In the cycle-pairing module, a new pairing loss function is proposed for the network training. Alongside with our Cycle-CenterNet, we also present a large-scale dataset, named Wired Table in the Wild (WTW), which includes well-annotated structure parsing of multiple style tables in several scenes like photo, scanning files, web pages, etc.. In experiments, we demonstrate that our Cycle-CenterNet consistently achieves the best accuracy of table structure parsing on the new WTW dataset by 24.6% absolute improvement evaluated by the TEDS metric. A more comprehensive experimental analysis also validates the advantages of our proposed methods for the TSP task.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143799988",
                        "name": "Rujiao Long"
                    },
                    {
                        "authorId": "2119293226",
                        "name": "Wen Wang"
                    },
                    {
                        "authorId": "2685089",
                        "name": "Nan Xue"
                    },
                    {
                        "authorId": "2112256",
                        "name": "Feiyu Gao"
                    },
                    {
                        "authorId": "2109432908",
                        "name": "Zhibo Yang"
                    },
                    {
                        "authorId": "153709848",
                        "name": "Yongpan Wang"
                    },
                    {
                        "authorId": "39943835",
                        "name": "Gui-Song Xia"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "637c7cd9aefbf6af01431b4d26e4be548053dc9c",
                "externalIds": {
                    "DBLP": "conf/icdar/ZiomekM21",
                    "DOI": "10.1145/3476887.3476890",
                    "CorpusId": 240289942
                },
                "corpusId": 240289942,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/637c7cd9aefbf6af01431b4d26e4be548053dc9c",
                "title": "GloSAT Historical Measurement Table Dataset: Enhanced Table Structure Recognition Annotation for Downstream Historical Data Rescue",
                "abstract": "Understanding and extracting tables from documents is a research problem that has been studied for decades. Table structure recognition is the labelling of components within a detected table, which can be detected automatically or manually provided. This paper presents the GloSAT historical measurement table dataset designed to train table structure recognition models for use in downstream historical data rescue applications. The dataset contains 500 scanned and manually annotated images of pages from meteorological measurement logbooks. We enhance standard full table and individual cell annotations by adding additional annotations for headings, headers, and table bodies. We also provide annotations for coarse segmentation cells consisting of multiple data cells logically grouped by ruling lines of ink or whitespace in the table, which often represent data cells that are semantically grouped. Our dataset annotations are provided in VOC2007 and ICDAR-2019 Competition on Table Detection and Recognition (cTDaR-19) XML formats, and our dataset can easily be aggregated with the cTDaR-19 dataset. We report results running a series of benchmark algorithms on our new dataset, concluding that post-processing is very important for performance, and that page style is not as significant a feature as table type on model performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1884921320",
                        "name": "Juliusz Ziomek"
                    },
                    {
                        "authorId": "3097680",
                        "name": "S. Middleton"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For direct comparison with previous work [20], we used the cleaned version of the dataset by Reference [7] and did not incorporate any sample of the dataset in the training set.",
                "Moreover, in Reference [33], the authors employed Faster RCNN with a coronerocating an approach to improve the predicted tabular boundaries in document images.",
                "For a comprehensive explanation of the RFP module, please refer to Reference [24].",
                "(4)\nIn case of SAC explained in Reference [24], the above convolutionalayer converts into:\nCon(i, w, 1) SAC\u2212\u2212\u2192 S(i) .",
                "It is important to emphasize that methods introduced in References [1,20] either rely on the heavy backbone with memory-intensive deformable convolutions [53] or are dependent on multiple preand post-processing methods to achieve the results.",
                "Analogous to Reference [55], the network in the final cascaded stage segments the object in a bounding box, along with classification and regression.",
                "We direct our readers to References [15,16,38\u201340] for a thorough understanding of these rule-based methods.",
                "We refer readers to Reference [24] for a detailed explanation on SAC.",
                "[7] presented another method that exploits Faster R-CNN [50] equipped with pre-trained base networks (ZFNet [51] and VGG-16 [52]) to detect tables in document images.",
                "If we include feature transformations Tj before joining\nthe feedback connections from FPN to the bottom-up backbone, then, the output feature f j of RFP is explained in Reference [24] as:\nf j = Fj( f j+1, ij), ij = Nj(ij\u22121, Tj( f j)), (2)\nwhere j enumerates over S, and the transformation of FPN to RFP makes it a recursive function.",
                "As depicted in Figure 5, the architecture of our utilized cascade Mask R-CNN closely follows the cascaded architecture introduced in Reference [25], along with the addition of segmentation branch at the final network head [55]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d10d8164e472864968725424d2195789fb5e67bb",
                "externalIds": {
                    "PubMedCentral": "8540682",
                    "MAG": "3197383755",
                    "DBLP": "journals/jimaging/HashmiPLSA21",
                    "DOI": "10.3390/jimaging7100214",
                    "CorpusId": 239202542,
                    "PubMed": "34677300"
                },
                "corpusId": 239202542,
                "publicationVenue": {
                    "id": "c0fc53c7-b0ed-487d-9191-1262c8322621",
                    "name": "Journal of Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "J Imaging"
                    ],
                    "issn": "2313-433X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-556372",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jimaging",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-556372"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d10d8164e472864968725424d2195789fb5e67bb",
                "title": "CasTabDetectoRS: Cascade Network for Table Detection in Document Images with Recursive Feature Pyramid and Switchable Atrous Convolution",
                "abstract": "Table detection is a preliminary step in extracting reliable information from tables in scanned document images. We present CasTabDetectoRS, a novel end-to-end trainable table detection framework that operates on Cascade Mask R-CNN, including Recursive Feature Pyramid network and Switchable Atrous Convolution in the existing backbone architecture. By utilizing a comparativelyightweight backbone of ResNet-50, this paper demonstrates that superior results are attainable without relying on pre- and post-processing methods, heavier backbone networks (ResNet-101, ResNeXt-152), and memory-intensive deformable convolutions. We evaluate the proposed approach on five different publicly available table detection datasets. Our CasTabDetectoRS outperforms the previous state-of-the-art results on four datasets (ICDAR-19, TableBank, UNLV, and Marmot) and accomplishes comparable results on ICDAR-17 POD. Upon comparing with previous state-of-the-art results, we obtain a significant relative error reduction of 56.36%, 20%, 4.5%, and 3.5% on the datasets of ICDAR-19, TableBank, UNLV, and Marmot, respectively. Furthermore, this paper sets a new benchmark by performing exhaustive cross-datasets evaluations to exhibit the generalization capabilities of the proposed method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1771057",
                        "name": "A. Pagani"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3adcc337a333650350a876be569ac7c3dfedcacd",
                "externalIds": {
                    "DBLP": "conf/mlcad/ChenLLWC21",
                    "DOI": "10.1109/MLCAD52597.2021.9531275",
                    "CorpusId": 237474135
                },
                "corpusId": 237474135,
                "publicationVenue": {
                    "id": "e539173b-8278-4cc8-9244-377441945a7a",
                    "name": "Workshop on Machine Learning for CAD",
                    "type": "conference",
                    "alternate_names": [
                        "MLCAD",
                        "Workshop Mach Learn CAD"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3adcc337a333650350a876be569ac7c3dfedcacd",
                "title": "Massive Figure Extraction and Classification in Electronic Component Datasheets for Accelerating PCB Design Preparation",
                "abstract": "Before starting printed-circuit-board (PCB) design, it is usually very time-consuming for PCB and system designers to review a large amount of electronic component datasheets in order to determine the best integration of electronic components for the target electronic systems. Each datasheet may contain over hundred figures and tables, while the figures and tables usually present the most important electronic component specifications. This paper categorizes various figures, including tables, in electronic component datasheets, and proposes the ECS-YOLO model for massive figure extraction and classification in order to accelerate PCB design preparation process. The experimental results show that, compared with the state-of-the-art object detection model, the proposed ECS-YOLO can consistently achieve better accuracy for figure extraction and classification in electronic component datasheets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110723027",
                        "name": "Kuan-Chun Chen"
                    },
                    {
                        "authorId": "2145035454",
                        "name": "Chou-Chen Lee"
                    },
                    {
                        "authorId": "2651071",
                        "name": "Mark Po-Hung Lin"
                    },
                    {
                        "authorId": "51288120",
                        "name": "Yan-Jhih Wang"
                    },
                    {
                        "authorId": "2109060307",
                        "name": "Yi-Ting Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "209ac7327a0153ed2c03c1b07d067a822c35c8ce",
                "externalIds": {
                    "MAG": "3196203992",
                    "DOI": "10.20944/preprints202108.0360.v1",
                    "CorpusId": 238713567
                },
                "corpusId": 238713567,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/209ac7327a0153ed2c03c1b07d067a822c35c8ce",
                "title": "HybridTabNet: Towards Better Table Detection in Scanned Document Images",
                "abstract": "Tables in the document image are one of the most important entities since they contain crucial information. Therefore, accurate table detection can significantly improve information extraction from tables. In this work, we present a novel end-to-end trainable pipeline, HybridTabNet, for table detection in scanned document images. Our two-stage table detector uses the ResNeXt-101 backbone for feature extraction and Hybrid Task Cascade (HTC) to localize the tables in scanned document images. Moreover, we replace conventional convolutions with deformable convolutions in the backbone network. This enables our network to detect tables of arbitrary layouts precisely. We evaluate our approach comprehensively on ICDAR-13, ICDAR-17 POD, ICDAR-19, TableBank, Marmot, and UNLV. Apart from the ICDAR-17 POD dataset, our proposed HybridTabNet outperforms earlier state-of-the-art results without depending on pre and post-processing steps. Furthermore, to investigate how the proposed method generalizes unseen data, we conduct an exhaustive leave-one-out-evaluation. In comparison to prior state-of-the-art results, our method reduces the relative error by 27.57% on ICDAR-2019-TrackA-Modern, 42.64% on TableBank (Latex), 41.33% on TableBank (Word), 55.73% on TableBank (Latex + Word), 10% on Marmot, and 9.67% on UNLV dataset. The achieved results reflect the superior performance of the proposed method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "52053159",
                        "name": "Danish Nazir"
                    },
                    {
                        "authorId": "1771057",
                        "name": "A. Pagani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Different from Census; CDC; BLS; IMF that only provide PDF reports where table hierarchies are hard to extract precisely (Schreiber et al., 2017), StaCan and NSF also provide reports in HTML, from which cell information such as text and formats can be extracted precisely using HTML tags."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a1364257028332760208827cd0c7af08d91e058b",
                "externalIds": {
                    "ACL": "2022.acl-long.78",
                    "DBLP": "conf/acl/Cheng0WJG0HLZ22",
                    "ArXiv": "2108.06712",
                    "DOI": "10.18653/v1/2022.acl-long.78",
                    "CorpusId": 237091377
                },
                "corpusId": 237091377,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/a1364257028332760208827cd0c7af08d91e058b",
                "title": "HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation",
                "abstract": "Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge numerical reasoning by complex hierarchical indexing, as well as implicit relationships of calculation and semantics. We present a new dataset, HiTab, to study question answering (QA) and natural language generation (NLG) over hierarchical tables. HiTab is a cross-domain dataset constructed from a wealth of statistical reports and Wikipedia pages, and has unique characteristics: (1) nearly all tables are hierarchical, and (2) QA pairs are not proposed by annotators from scratch, but are revised from real and meaningful sentences authored by analysts. (3) to reveal complex numerical reasoning in statistical reports, we provide fine-grained annotations of quantity and entity alignment. Experiments suggest that this HiTab presents a strong challenge for existing baselines and a valuable benchmark for future research. Targeting hierarchical structure, we devise a hierarchy-aware logical form for symbolic reasoning over tables, which shows high effectiveness. Targeting table reasoning, we leverage entity and quantity alignment to explore partially supervised training in QA and conditional generation in NLG, and largely reduce spurious predictions in QA and produce better descriptions in NLG.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1471878967",
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "authorId": "2113413583",
                        "name": "Haoyu Dong"
                    },
                    {
                        "authorId": "1390877035",
                        "name": "Zhiruo Wang"
                    },
                    {
                        "authorId": "2053919577",
                        "name": "Ran Jia"
                    },
                    {
                        "authorId": "2148899355",
                        "name": "Jiaqi Guo"
                    },
                    {
                        "authorId": "152673873",
                        "name": "Yan Gao"
                    },
                    {
                        "authorId": "2109750123",
                        "name": "Shi Han"
                    },
                    {
                        "authorId": "153249455",
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "authorId": "1485159990",
                        "name": "Dongmei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[28] proposed a deep learning architecture for detecting table location and identifying the row and column positions."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10c8c9733cf7c671505010213ad397a50f5664d8",
                "externalIds": {
                    "DBLP": "conf/kdd/YangC0021",
                    "DOI": "10.1145/3447548.3467425",
                    "CorpusId": 236980243
                },
                "corpusId": 236980243,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/10c8c9733cf7c671505010213ad397a50f5664d8",
                "title": "Numerical Formula Recognition from Tables",
                "abstract": "Claims over the numerical relationships among some measures are commonly expressed in tabular forms, and widely exist in the published documents on the Web. This paper introduces the problem of numerical formula recognition from tables, namely recognizing all numerical formulas inside a given table. It can well support many interesting downstream applications, such as numerical error correction in tables, formula recommendation in tables. Here, we emphasize that table is a kind of language that adopts a different linguistic paradigm from natural language. It uses visual grammar like visual layout and visual settings (e.g., indentation, font style) to express the grammatical relationships among the table cells. Understanding tables and recognizing formulas require decoding the visual grammar while simultaneously understanding the textual information. Another challenge is that formulas are complicated in terms of diverse math functions and variable-length of arguments. To address these challenges, we convert this task into a uniform framework, extracting relations of table cell pairs in a table. A two-channel neural network model TaFor is proposed to embed both the textual and visual features for a table cell. Our framework achieves the formula-level F1-score = 0.90 on a real-world dataset of 190179 tables while a retrieval-based method achieves F1-score = 0.72. We also perform extensive experiments to demonstrate the effectiveness of each component in our model, and conduct a case study to discuss the limits of the proposed model. With our published data this study also aims to attract the community's interest in deep semantic understanding over tables.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2149535920",
                        "name": "Qingping Yang"
                    },
                    {
                        "authorId": "2107994062",
                        "name": "Yixuan Cao"
                    },
                    {
                        "authorId": "2142626460",
                        "name": "Hongwei Li"
                    },
                    {
                        "authorId": "144389949",
                        "name": "Ping Luo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[42] then devised a very standard deep learning approach called DeepDeSRT for table detection and structure recognition, where no prior knowledge or assumption about table structures was necessary.",
                "These specific tasks range from document content understanding, document structural and syntactic analysis [6, 42, 48] and so on."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eb84f64ad349d0dfbac482ea79494ce75633a011",
                "externalIds": {
                    "DBLP": "journals/ijdar/BiswasRLP21",
                    "DOI": "10.1007/s10032-021-00380-6",
                    "CorpusId": 237309680
                },
                "corpusId": 237309680,
                "publicationVenue": {
                    "id": "46722266-4327-4a22-871d-5a9fe3c5ad0c",
                    "name": "International Journal on Document Analysis and Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Doc Anal Recognit"
                    ],
                    "issn": "1433-2825",
                    "url": "https://link.springer.com/journal/10032"
                },
                "url": "https://www.semanticscholar.org/paper/eb84f64ad349d0dfbac482ea79494ce75633a011",
                "title": "Beyond document object detection: instance-level segmentation of complex layouts",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2150473007",
                        "name": "Sanket Biswas"
                    },
                    {
                        "authorId": "40420775",
                        "name": "Pau Riba"
                    },
                    {
                        "authorId": "143826881",
                        "name": "J. Llad\u00f3s"
                    },
                    {
                        "authorId": "144167309",
                        "name": "U. Pal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, [4, 11] do not consider that complex tables which contain spanning cells, so that they cannot handle the structure recognition of complex tables well.",
                "Although significant efforts have been made in the past to recognize the internal structure of tables through an automated process [4, 6, 7, 8, 9, 10], most of these methods [4, 11] only focus on simple tables and are hard to accurately recognize the structure of complex tables.",
                "[4, 11] utilize recently published insights from semantic segmentation [14] research for identifying rows, columns, and cell positions within tables to recognize table structures.",
                "Comparing with other methods [9, 4, 7], our method achieves state-of-the-art.",
                "Analyzing tabular data in unstructured documents focuses mainly on three problems: i) table detection: localizing the bounding boxes of tables in documents [18, 19], ii) table structure recognition: parsing only the structural (row and column layout) information of tables [4, 7, 8], and iii) table recognition: parsing both the structural information and content of table cells [6].",
                "Numerous efforts have been made in the past to automatically extract the relevant information from documents [1, 2, 3, 4, 5].",
                "Due to the rapid development of deep learning and the massive amounts of tabular data in documents on the Web, many deep learning-based methods [4, 6, 7, 9, 11] have also been presented to understand table structures as they are robust to the input type (whether being scanned images or native digital).",
                "Most previous methods [4, 7, 9, 11, 12] only use the spatial or visual features without considering the textual information of each table cell to recognize the table structure."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a0a97c129f1c8339b9fb4fd768cf9f52e64b2b0a",
                "externalIds": {
                    "ArXiv": "2107.05214",
                    "DBLP": "journals/corr/abs-2107-05214",
                    "DOI": "10.1016/j.patcog.2022.108565",
                    "CorpusId": 235795015
                },
                "corpusId": 235795015,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a0a97c129f1c8339b9fb4fd768cf9f52e64b2b0a",
                "title": "Split, embed and merge: An accurate table structure recognizer",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2128660562",
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "authorId": "39557762",
                        "name": "Jianshu Zhang"
                    },
                    {
                        "authorId": "2149030934",
                        "name": "Jun Du"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Document understanding methods in the literature have used various combinations of image, spatial and text features in order to understand and extract information from structurally rich documents such as forms [18, 57, 12], tables [44, 56, 24], receipts [27, 26] and invoices [35, 42, 37]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e6cbb6aa65d7761fabb6c5eb363f66524abe0a4c",
                "externalIds": {
                    "DBLP": "conf/iccv/AppalarajuJKXM21",
                    "ArXiv": "2106.11539",
                    "DOI": "10.1109/iccv48922.2021.00103",
                    "CorpusId": 235592814
                },
                "corpusId": 235592814,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/e6cbb6aa65d7761fabb6c5eb363f66524abe0a4c",
                "title": "DocFormer: End-to-End Transformer for Document Understanding",
                "abstract": "We present DocFormer - a multi-modal transformer based architecture for the task of Visual Document Understanding (VDU). VDU is a challenging problem which aims to understand documents in their varied formats (forms, receipts etc.) and layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction. DocFormer uses text, vision and spatial features and combines them using a novel multi-modal self-attention layer. DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is evaluated on 4 different datasets each with strong baselines. DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "26316634",
                        "name": "Srikar Appalaraju"
                    },
                    {
                        "authorId": "35923416",
                        "name": "Bhavan Jasani"
                    },
                    {
                        "authorId": "3352136",
                        "name": "B. Kota"
                    },
                    {
                        "authorId": "3052897",
                        "name": "Yusheng Xie"
                    },
                    {
                        "authorId": "1398834003",
                        "name": "R. Manmatha"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [25] is a 2- fold system that applies Faster RCNN [24] and FCN [16] for both table detection and row/column segmentation.",
                "For example, the DeepDeSRT [25] model was evaluated on a random subset of ICDAR13-Table.",
                "For cell spatial location detection, we use the same evaluation metrics with recent methods [25, 27, 29, 28, 20].",
                "2, with the success of deep learning, recent deep learning-based table structure recognition approaches can be divided into three categories: 1) identify cell bounding boxes through visual detection and segmentation methods [25, 27, 29, 28, 8]; 2) transform a table image into the markup sequence, such as LaTeX and HTML [10, 2]; and 3) explore the adjacency relation between different table cells [30, 22, 11].",
                "Recently, segmentation-based methods have been popular in table cell detection due to its statistical significance along table rows and columns [25, 27, 28].",
                "DeepDeSRT [25] is a 2fold system that applies Faster RCNN [24] and FCN [16] for both table detection and row/column segmentation."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6e26602986e56d3524c325601674decb05cd8f2b",
                "externalIds": {
                    "DBLP": "conf/iccv/XueYWTL21",
                    "ArXiv": "2106.10598",
                    "DOI": "10.1109/ICCV48922.2021.00133",
                    "CorpusId": 235490364
                },
                "corpusId": 235490364,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/6e26602986e56d3524c325601674decb05cd8f2b",
                "title": "TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition",
                "abstract": "A table arranging data in rows and columns is a very effective data structure, which has been widely used in business and scientific research. Considering large-scale tabular data in online and offline documents, automatic table recognition has attracted increasing attention from the document analysis community. Though human can easily understand the structure of tables, it remains a challenge for machines to understand that, especially due to a variety of different table layouts and styles. Existing methods usually model a table as either the markup sequence or the adjacency matrix between different table cells, failing to address the importance of the logical location of table cells, e.g., a cell is located in the first row and the second column of the table. In this paper, we reformulate the problem of table structure recognition as the table graph reconstruction, and propose an end-to-end trainable table graph reconstruction network (TGRNet) for table structure recognition. Specifically, the proposed method has two main branches, a cell detection branch and a cell logical location branch, to jointly predict the spatial location and the logical location of different cells. Experimental results on three popular table recognition datasets and a new dataset with table graph annotations (TableGraph-350K) demonstrate the effectiveness of the proposed TGRNet for table structure recognition. Code and annotations will be made publicly available at https://github.com/xuewenyuan/TGRNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51469630",
                        "name": "Wenyuan Xue"
                    },
                    {
                        "authorId": "2110431812",
                        "name": "Baosheng Yu"
                    },
                    {
                        "authorId": "2119293226",
                        "name": "Wen Wang"
                    },
                    {
                        "authorId": "2075330732",
                        "name": "Dacheng Tao"
                    },
                    {
                        "authorId": "1612980447",
                        "name": "Qingyong Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [1] is contain two steps: first step is deep learning method for table detection where using fine-tuning a pre-trained model of Faster RCNN and second step is deep learning method for table structure recognition by using fine-tuning FCN proposed by Shelhamer et al.",
                "Deep learning has recently achieving state-of-the-art using convolutional neural network (CNN) [9] in many tasks including object detection [10], face recognition [11], sequence to sequence learning [12, 13], speech recognition [14], semantic segmentation [15], image classification [16], handwritten recognition [17, 18, 19], and table detection [1, 8, 6] is demanding because they need to classify tables among the texts and other figures.",
                "A few studies have been conducted on the identification of tables in documents [1, 2, 3, 4, 5]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0fdc3ba52e00518c29795d44b1303f3ccb400d76",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-15322",
                    "ArXiv": "2106.15322",
                    "DOI": "10.1016/j.neucom.2021.11.101",
                    "CorpusId": 235669757
                },
                "corpusId": 235669757,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0fdc3ba52e00518c29795d44b1303f3ccb400d76",
                "title": "TNCR: Table Net Detection and Classification Dataset",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "115665360",
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "authorId": "2115948777",
                        "name": "Alexander Berendeyev"
                    },
                    {
                        "authorId": "2115949553",
                        "name": "Islam Nuradin"
                    },
                    {
                        "authorId": "3253700",
                        "name": "D. Nurseitov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[8] proposed a deep learning-based approach for table detection and table structure recognition in documents or images."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9275ed7cf4d3adb5e30a90e8f8d660c1d5d4a636",
                "externalIds": {
                    "MAG": "3184371482",
                    "DOI": "10.32913/mic-ict-research.v2021.n1.974",
                    "CorpusId": 237810909
                },
                "corpusId": 237810909,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9275ed7cf4d3adb5e30a90e8f8d660c1d5d4a636",
                "title": "An Integrated Approach for Table Detection and Structure Recognition",
                "abstract": "Detecting and identifying the table structure is an important issue in document digitization. Although there have been many great strides based on current deep learning techniques, table structure identification is still a difficult and difficult problem, especially when solving the problem of digitizing text in practice. The paper proposes a solution to digitize table documents based on the Cascade R-CNN HRNet network to detect, classify tables and integrate image processing algorithms to improve table data identification results. The proposed algorithm proved effective on real data - the hydrometeorological station record book contains tables including simple and complex structures tables with over 98% accuracy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8092281",
                        "name": "Hai-Hong Phan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Schreiber et al. (2017) proposed DeepDeSRT that employs the Faster R-CNN model for table detection and a semantic segmentation approach for structure recognition."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8a1b6d3ad9af207da162439018562f1d2792d607",
                "externalIds": {
                    "DBLP": "conf/visapp/NamyslEBK22",
                    "ArXiv": "2105.11879",
                    "DOI": "10.5220/0010767600003124",
                    "CorpusId": 244800818
                },
                "corpusId": 244800818,
                "publicationVenue": {
                    "id": "3e53351c-7355-4159-a3f2-f3b03a3aa989",
                    "name": "VISIGRAPP",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Comput Vis Imaging Comput Graph Theory Appl",
                        "International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications"
                    ],
                    "issn": "2184-4321",
                    "alternate_issns": [
                        "2184-5921"
                    ],
                    "url": "http://www.visigrapp.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8a1b6d3ad9af207da162439018562f1d2792d607",
                "title": "Flexible Table Recognition and Semantic Interpretation System",
                "abstract": "Table extraction is an important but still unsolved problem. In this paper, we introduce a flexible and modular table extraction system. We develop two rule-based algorithms that perform the complete table recognition process, including table detection and segmentation, and support the most frequent table formats. Moreover, to incorporate the extraction of semantic information, we develop a graph-based table interpretation method. We conduct extensive experiments on the challenging table recognition benchmarks ICDAR 2013 and ICDAR 2019, achieving results competitive with state-of-the-art approaches. Our complete information extraction system exhibited a high F1 score of 0.7380. To support future research on information extraction from documents, we make the resources (ground-truth annotations, evaluation scripts, algorithm parameters) from our table interpretation experiment publicly available.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "134442417",
                        "name": "Marcin Namysl"
                    },
                    {
                        "authorId": "153486145",
                        "name": "Alexander M. Esser"
                    },
                    {
                        "authorId": "1699019",
                        "name": "Sven Behnke"
                    },
                    {
                        "authorId": "152162939",
                        "name": "Joachim Kohler"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "538761ed94c3968d260024d04435550fc2bf62c8",
                "externalIds": {
                    "MAG": "3165795079",
                    "DBLP": "journals/fcsc/LiHCZL21",
                    "DOI": "10.1007/s11704-020-9322-7",
                    "CorpusId": 233178918
                },
                "corpusId": 233178918,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/538761ed94c3968d260024d04435550fc2bf62c8",
                "title": "Rich-text document styling restoration via reinforcement learning",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2142626460",
                        "name": "Hongwei Li"
                    },
                    {
                        "authorId": "2149297924",
                        "name": "Yingpeng Hu"
                    },
                    {
                        "authorId": "10034341",
                        "name": "Yixuan Cao"
                    },
                    {
                        "authorId": "3431182",
                        "name": "Ganbin Zhou"
                    },
                    {
                        "authorId": "144389949",
                        "name": "Ping Luo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[15] addressed TD and TSR in a single approach using a two-fold system based on Faster R-CNN [12] for TD and DL-based semantic segmentation for TSR that utilizes transfer learning."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "928c1ee7131998e7b961c39c6768f5809a268a86",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-11021",
                    "ArXiv": "2105.11021",
                    "DOI": "10.1007/978-3-030-87626-5_8",
                    "CorpusId": 235166100
                },
                "corpusId": 235166100,
                "publicationVenue": {
                    "id": "bff947de-0729-467e-b9cd-57b193e706a7",
                    "name": "Deutsche Jahrestagung f\u00fcr K\u00fcnstliche Intelligenz",
                    "type": "conference",
                    "alternate_names": [
                        "KI",
                        "Dtsch Jahrestag K\u00fcnstl Intell"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/928c1ee7131998e7b961c39c6768f5809a268a86",
                "title": "Multi-Type-TD-TSR - Extracting Tables from Document Images using a Multi-stage Pipeline for Table Detection and Table Structure Recognition: from OCR to Structured Table Representations",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2072761274",
                        "name": "Pascal Fischer"
                    },
                    {
                        "authorId": "2104443511",
                        "name": "Alen Smajic"
                    },
                    {
                        "authorId": "1733174",
                        "name": "Alexander Mehler"
                    },
                    {
                        "authorId": "2064953909",
                        "name": "Giuseppe Abrami"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Conclusion: Despite the extensive interest of the NLP community in leveraging document structures (e. g., Apostolova and Tomuro 2014; Sch\u00e4fer et al. 2011; Sch\u00e4fer and Weitz 2012; Schreiber et al. 2018; Katti et al. 2018), the task of parsing complete document structures from renderings has been overlooked.",
                "\u2026for structure parsing on documents focused on a subset of simpler tasks such as segmentation of text regions (Antonacopoulos et al. 2009), locating tables (Zanibbi, Blostein, and Cordy 2004; Embley et al. 2006), or parsing them (Schreiber et al. 2018), but not parsing complete document structures.",
                "On the other hand, efficient learning is prevented as large-scale training sets are lacking (cf. Arif and Shafait 2018; Schreiber et al. 2018).",
                "The works by Schreiber et al. (2018); Qasim, Mahmood, and Shafait (2019) draw upon deep neural networks to identify table structures for rendered inputs.",
                "Altogether, our weak supervision outperforms the state-of-theart (Schreiber et al. 2018) by a considerable margin.",
                "\u2026Despite the extensive interest of the NLP community in leveraging document structures (e. g., Apostolova and Tomuro 2014; Scha\u0308fer et al. 2011; Scha\u0308fer and Weitz 2012; Schreiber et al. 2018; Katti et al. 2018), the task of parsing complete document structures from renderings has been overlooked.",
                "For instance, nested tables are fairly easy to recognize for human readers, yet detecting them is known to impose computational hurdles (cf. Schreiber et al. 2018).",
                "2006), or parsing them (Schreiber et al. 2018), but not parsing complete document structures."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "adcb8fe66d08e6187f51ecbf4443523870a4b799",
                "externalIds": {
                    "DBLP": "conf/aaai/RauschMB0F21",
                    "DOI": "10.1609/aaai.v35i5.16558",
                    "CorpusId": 235306298
                },
                "corpusId": 235306298,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/adcb8fe66d08e6187f51ecbf4443523870a4b799",
                "title": "DocParser: Hierarchical Document Structure Parsing from Renderings",
                "abstract": "Translating renderings (e. g. PDFs, scans) into hierarchical document structures is extensively demanded in the daily routines of many real-world applications. However, a holistic, principled approach to inferring the complete hierarchical structure in documents is missing. As a remedy, we developed \u201cDocParser\u201d: an end-to-end system for parsing complete document structure \u2013 including all text elements, nested figures, tables, and table cell structures. Our second contribution is to provide a dataset for evaluating hierarchical document structure parsing. Our third contribution is to propose a scalable learning framework for settings where domain-specific data are scarce, which we address by a novel approach to weak supervision that significantly improves the document structure parsing performance. Our experiments confirm the effectiveness of our proposed weak supervision: Compared to the baseline without weak supervision, it improves the mean average precision for detecting document entities by 39.1% and improves the F1 score of classifying hierarchical relations by 35.8%.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "5185738",
                        "name": "Johannes Rausch"
                    },
                    {
                        "authorId": "2059207885",
                        "name": "O. Martinez"
                    },
                    {
                        "authorId": "1402912519",
                        "name": "Fabian Bissig"
                    },
                    {
                        "authorId": "1776014",
                        "name": "Ce Zhang"
                    },
                    {
                        "authorId": "3207649",
                        "name": "S. Feuerriegel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Works of [30,31,32] firstly obtain the rows and columns regions using the detection or segmentation models and then intersect these two regions to obtain the grids of cells.",
                "With the great success of deep neural network in computer vision field, works began to focus on the image-based table with more general structures [21,30,24,9,13,36,23,14,33].",
                "Though recent works [30,22,32,31] attempt to predict row/column regions or even invisible grid lines [33], they are limited to handle tables that cross span multiple rows/columns.",
                "Thus, table structure recognition [10,21,34,30,5,4,39] becomes one of the important techniques in current document understanding systems."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ea25282d27368d3d04db91b165b5003d63e335d6",
                "externalIds": {
                    "DBLP": "conf/icdar/QiaoLCZPNRT021",
                    "ArXiv": "2105.06224",
                    "DOI": "10.1007/978-3-030-86549-8_7",
                    "CorpusId": 234482682
                },
                "corpusId": 234482682,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/ea25282d27368d3d04db91b165b5003d63e335d6",
                "title": "LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2065512194",
                        "name": "Liang Qiao"
                    },
                    {
                        "authorId": "2109614687",
                        "name": "Zaisheng Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    },
                    {
                        "authorId": "2151333065",
                        "name": "Peng Zhang"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    },
                    {
                        "authorId": "1490934795",
                        "name": "Yi Niu"
                    },
                    {
                        "authorId": "144850642",
                        "name": "Wenqi Ren"
                    },
                    {
                        "authorId": "51151126",
                        "name": "Wenming Tan"
                    },
                    {
                        "authorId": "144894837",
                        "name": "Fei Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT can segment cells, which are large enough and fully filled with texts.",
                "We also compared the performance of CluSTi to DeepDeSRT [34], which is known as the best",
                "(2017) proposed a deep learning method called DeepDeSRT to detect tables in document images like PDF, as well as to recognize their structure [34].",
                "In DeepDeSRT, the cell\u2019s boundaries were recognized using a pre-trained Fully Convolutional Networks (FCN) for semantic segmentation [12, 21].",
                "We also compared the performance of CluSTi to DeepDeSRT [34], which is known as the best\nrecent method for table structure recognition on the ICDAR 2013 and ICDAR 2019 competition\u2019s datasets.",
                "In this case, the table structure can not be recognized using DeepDeSRT due to the fact that there exist many empty cells.",
                "table cells relying on their boundaries [34].",
                "This approach overcome the limitations of DeepDeSRT method, which segments table cells relying on their boundaries [34].",
                "That\u2019s why when cells are empty or not large enough, they are still recognized by CluSTi but not by DeepDeSRT.",
                "For instance, Schreiber et al. (2017) proposed a deep learning method called DeepDeSRT to detect tables in document images like PDF, as well as to recognize their structure [34].",
                "This result also outperformed DeepDeSRT, which is known as the best recent method applied on document images.",
                "CluSTi outperforms DeepDeSRT with an overall F1-score of 98.48% on 193 document images compared to 91.44% on 34 images (Table 2).",
                "TableNet showed comparable results to DeepDeSRT method [34], and their model is end-to-end which means further improvements can be made with richer semantic knowledge, and additional branches for learning row-based segmentation.",
                "This method outperformed DeepDeSRT and TableNet with a significant higher F1-score on the 34 test images of ICDAR 2013 competition dataset, that demonstrated for the efficiency of our method.",
                "Table 2 Comparison of detection accuracy (%) among CluSTi, DeepDeSRT [34], and TableNet [23] on ICDAR 2013 dataset",
                "In contrast, DeepDeSRT is based on Faster RCNN, a semantic segmentation model which focuses on cell object detection [28]."
            ],
            "intents": [
                "result",
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "abd3ca5ab184843ad7634b8d8c2a7dd36db2aac7",
                "externalIds": {
                    "DOI": "10.1007/s11036-021-01759-9",
                    "CorpusId": 254832742
                },
                "corpusId": 254832742,
                "publicationVenue": {
                    "id": "6fff9d04-a29e-4e37-87e8-27e06da9055c",
                    "name": "Journal on spesial topics in mobile networks and applications",
                    "type": "journal",
                    "alternate_names": [
                        "J spes top mob netw appl",
                        "Mobile Networks and Applications",
                        "Mob Netw Appl"
                    ],
                    "issn": "1383-469X",
                    "url": "https://link.springer.com/journal/11036"
                },
                "url": "https://www.semanticscholar.org/paper/abd3ca5ab184843ad7634b8d8c2a7dd36db2aac7",
                "title": "ClusTi: Clustering Method for Table Structure Recognition in Scanned Images",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2070874816",
                        "name": "Arthur Zucker"
                    },
                    {
                        "authorId": "2037496520",
                        "name": "Younes Belkada"
                    },
                    {
                        "authorId": "1752866615",
                        "name": "Hanh Vu"
                    },
                    {
                        "authorId": "2196642420",
                        "name": "Van Nam Nguyen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b308c6e2a236b8d73dd4a962cb5b8c7374ee3698",
                "externalIds": {
                    "MAG": "3159314278",
                    "DBLP": "journals/monet/ZuckerBVN21",
                    "DOI": "10.1007/S11036-021-01759-9",
                    "CorpusId": 235557952
                },
                "corpusId": 235557952,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b308c6e2a236b8d73dd4a962cb5b8c7374ee3698",
                "title": "ClusTi: Clustering Method for Table Structure Recognition in Scanned Images",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2070874816",
                        "name": "Arthur Zucker"
                    },
                    {
                        "authorId": "2037496520",
                        "name": "Younes Belkada"
                    },
                    {
                        "authorId": "1752866615",
                        "name": "Hanh Vu"
                    },
                    {
                        "authorId": "2757688",
                        "name": "N. V. Nguyen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The authors have reported the F1-score of 93.42% with an IOU of 0.5 on the ICDAR 2013 dataset [66].",
                "[7], they have formulated the problem of structure recognition as the semantic segmentation problem.",
                "DeepDeSRT [7] Faster R-CNN with transfer learning techniques (Section III-A1a) Simple and effective end-to-end approach to detect tables and structures of the tables.",
                "Deep learning leverages huge datasets [7].",
                "The threshold of Intersection Over Union (IOU) for calculating precision and recall is also defined in Table 6.",
                "These three metrics are calculated on a specific IOU threshold established by the authors.",
                "The performance of deep neural networks has a direct relation with the size of the dataset [7], [24].",
                "Their end to end system known as DeepDeSRT not only detects the tabular region but also distinguishes the structure of the table and both of these tasks are dealt with by applying distinctive\n87666 VOLUME 9, 2021\ndeep learning techniques.",
                "[45] S. Schreiber, S. Agne, I. Wolf, A. Dengel, and S. Ahmed, \u2018\u2018DeepDeSRT: Deep learning for detection and structure recognition of tables in document images,\u2019\u2019 in Proc.",
                "The task of table structural segmentation is evaluated based on how accurate the rows or columns of the tables are separated [7], [44], [47].",
                "We strongly believe that the threshold value for IOU needs to be standardized in order to have an impartial comparison.",
                "For table detection, one of the most exploited evaluation metrics is IOU [45], [46].",
                "To recognize the structure in tables, the authors of DeepDeSRT [7] have exploited the concept of semantic segmentation.",
                "The author tries to convince the readers that due to the interdependence between the table detection and structural segmentation, both of the problems can be solved efficiently by using a single network.\na: FULLY CONVOLUTIONAL NETWORKS To recognize the structure in tables, the authors of DeepDeSRT [45] have exploited the concept of semantic segmentation.",
                "For table detection, one of the most exploited evaluation metrics is IOU [7], [40], [44].",
                "It is crucial to mention that some of the approaches have not quoted the threshold value for IOU; however, they have compared their results with other methods where the threshold value is defined."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "bc8863440e26e48866a64f63438051d24880b793",
                "externalIds": {
                    "DBLP": "journals/access/HashmiLSAAA21",
                    "ArXiv": "2104.14272",
                    "DOI": "10.1109/ACCESS.2021.3087865",
                    "CorpusId": 233444247
                },
                "corpusId": 233444247,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bc8863440e26e48866a64f63438051d24880b793",
                "title": "Current Status and Performance Analysis of Table Recognition in Document Images With Deep Neural Networks",
                "abstract": "The first phase of table recognition is to detect the tabular area in a document. Subsequently, the tabular structures are recognized in the second phase in order to extract information from the respective cells. Table detection and structural recognition are pivotal problems in the domain of table understanding. However, table analysis is a perplexing task due to the colossal amount of diversity and asymmetry in tables. Therefore, it is an active area of research in document image analysis. Recent advances in the computing capabilities of graphical processing units have enabled the deep neural networks to outperform traditional state-of-the-art machine learning methods. Table understanding has substantially benefited from the recent breakthroughs in deep neural networks. However, there has not been a consolidated description of the deep learning methods for table detection and table structure recognition. This review paper provides a thorough analysis of the modern methodologies that utilize deep neural networks. Moreover, it presents a comprehensive understanding of the current state-of-the-art and related challenges of table understanding in document images. The leading datasets and their intricacies have been elaborated along with the quantitative results. Furthermore, a brief overview is given regarding the promising directions that can further improve table analysis in document images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "2206035070",
                        "name": "Muhammad Adnan Afzal"
                    },
                    {
                        "authorId": "2206035072",
                        "name": "Muhammad Ahtsham Afzal"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[8], [18], [1] and [22] made use of Faster R-CNN [17] with their own take on handcrafted features and preprocessing methodologies.",
                "[18] mentions the unnatural approach of detecting rows and columns through Faster R-CNN [17] and instead proposes fine-grained image segmentation through FCN-X\u2019s architecture by Shelhamer et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5dbb224c85d767730e880c3226c9dc5d59cb765f",
                "externalIds": {
                    "ArXiv": "2104.14237",
                    "DBLP": "journals/corr/abs-2104-14237",
                    "DOI": "10.1007/978-3-030-86331-9_38",
                    "CorpusId": 233443846
                },
                "corpusId": 233443846,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/5dbb224c85d767730e880c3226c9dc5d59cb765f",
                "title": "TabAug: Data Driven Augmentation for Enhanced Table Structure Recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2054749978",
                        "name": "U. Khan"
                    },
                    {
                        "authorId": "49192264",
                        "name": "Sohaib Zahid"
                    },
                    {
                        "authorId": "2111291830",
                        "name": "Muhammad Asad Ali"
                    },
                    {
                        "authorId": "1403326950",
                        "name": "A. Ul-Hasan"
                    },
                    {
                        "authorId": "1688013",
                        "name": "F. Shafait"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To achieve convergence, datasets with a massive amount of images are required to train these networks optimally [14,19].",
                "[14] where they implemented Faster R-CNN for detection of tables in document images.",
                "DeepDeSRT [14] Faster R-CNN [50] with transfer learning Straightforward and effective approach to detect tables."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6e54539d26a3d4cbf5752f26c86b93577d1cdf71",
                "externalIds": {
                    "MAG": "3157266006",
                    "DOI": "10.20944/PREPRINTS202104.0739.V1",
                    "CorpusId": 235563173
                },
                "corpusId": 235563173,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6e54539d26a3d4cbf5752f26c86b93577d1cdf71",
                "title": "A Survey of Graphical Page Object Detection with Deep Neural Networks",
                "abstract": "In any document, graphical elements like tables, figures, and formulas contain essential information. The processing and interpretation of such information require specialized algorithms. Off-the-shelf OCR components cannot process this information reliably. Therefore, an essential step in document analysis pipelines is to detect these graphical components. It leads to a high-level conceptual understanding of the documents that makes digitization of documents viable. Since the advent of deep learning, the performance of deep learning-based object detection has improved many folds. In this work, we outline and summarize the deep learning approaches for detecting graphical page objects in the document images. Therefore, we discuss the most relevant deep learning-based approaches and state-of-the-art graphical page object detection in document images. This work provides a comprehensive understanding of the current state-of-the-art and related challenges. Furthermore, we discuss leading datasets along with the quantitative evaluation. Moreover, it discusses briefly the promising directions that can be utilized for further improvements.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2123987051",
                        "name": "Jwalin Bhatt"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [26], uses separate, specially made neural networks to identify tables and extract their data."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "80c62ab972ec45f3e7e465539613f2cf779474cb",
                "externalIds": {
                    "DBLP": "journals/array/ColterFYKYD22",
                    "ArXiv": "2104.11287",
                    "DOI": "10.2139/ssrn.4044585",
                    "CorpusId": 233388132
                },
                "corpusId": 233388132,
                "publicationVenue": {
                    "id": "a7e4161c-9216-40df-a896-ed51a04aab3a",
                    "name": "Array",
                    "type": "journal",
                    "issn": "2590-0056",
                    "url": "https://www.journals.elsevier.com/array",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/array"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/80c62ab972ec45f3e7e465539613f2cf779474cb",
                "title": "Tablext: A Combined Neural Network And Heuristic Based Table Extractor",
                "abstract": "A significant portion of the data available today is found within tables. Therefore, it is necessary to use automated table extraction to obtain thorough results when data-mining. Today's popular state-of-the-art methods for table extraction struggle to adequately extract tables with machine-readable text and structural data. To make matters worse, many tables do not have machine-readable data, such as tables saved as images, making most extraction methods completely ineffective. In order to address these issues, a novel, general format table extractor tool, Tablext, is proposed. This tool uses a combination of computer vision techniques and machine learning methods to efficiently and effectively identify and extract data from tables. Tablext begins by using a custom Convolutional Neural Network (CNN) to identify and separate all potential tables. The identification process is optimized by combining the custom CNN with the YOLO object detection network. Then, the high-level structure of each table is identified with computer vision methods. This high-level, structural meta-data is used by another CNN to identify exact cell locations. As a final step, Optical Characters Recognition (OCR) is performed on every individual cell to extract their content without needing machine-readable text. This multi-stage algorithm allows for the neural networks to focus on completing complex tasks, while letting image processing methods efficiently complete the simpler ones. This leads to the proposed approach to be general-purpose enough to handle a large batch of tables regardless of their internal encodings or their layout complexity. Additionally, it becomes accurate enough to outperform competing state-of-the-art table extractors on the ICDAR 2013 table dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2087607913",
                        "name": "Zachary Colter"
                    },
                    {
                        "authorId": "33935286",
                        "name": "Morteza Fayazi"
                    },
                    {
                        "authorId": "7936000",
                        "name": "Z. Youbi"
                    },
                    {
                        "authorId": "2083548026",
                        "name": "Serafina Kamp"
                    },
                    {
                        "authorId": "2052192972",
                        "name": "Shuyang Yu"
                    },
                    {
                        "authorId": "1793651",
                        "name": "R. Dreslinski"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, extracting tabular structures directly from images is perplexing compared to operating over digital-born PDFs [17].",
                "[27] published the first natural image-based deep learning system which explored the problem of table structure analysis.",
                "ICDAR-2013 [18] dataset has been used to standardize the state-of-the-art results for the task of table detection and table structure recognition [27], [32].",
                "However, extracting tabular structures directly from images is perplexing as compared to operating over digital-born PDFs [27].",
                "In this paper, we exhibit that current object detectors that have already shown remarkable improvements in resolving the problem of table detection [27], [42], are also extremely effective in improving the performance of table structure recognition systems.",
                "This problem can be further dissolved into detecting rows and columns in a table that can be later combined to produce the respective cells [27].",
                "In order to compare our approach with state-of-the-art methods [27], [32], [33], we have used the identical evaluation metrics which are explained below:",
                "[27] in order to implement a direct comparison against the similar approaches [27], [32], [33].",
                "All these techniques are heavily dependent on the meta-data available in digital-born PDFs."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2a242e4a54323eee9e0f514510b008d9b2119641",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-10538",
                    "ArXiv": "2104.10538",
                    "DOI": "10.1109/ACCESS.2021.3103413",
                    "CorpusId": 233324171
                },
                "corpusId": 233324171,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2a242e4a54323eee9e0f514510b008d9b2119641",
                "title": "Guided Table Structure Recognition Through Anchor Optimization",
                "abstract": "This paper presents the novel approach towards table structure recognition by leveraging the guided anchors. The concept differs from current state-of-the-art systems for table structure recognition that naively apply object detection methods. In contrast to prior techniques, first, we estimate the viable anchors for table structure recognition. Subsequently, these anchors are exploited to locate the rows and columns in tabular images. Furthermore, the paper introduces a simple and effective method that improves the results using tabular layouts in realistic scenarios. The proposed method is exhaustively evaluated on the two publicly available datasets of table structure recognition: ICDAR-2013 and TabStructDB. Moreover, we empirically established the validity of our method by implementing it on the previous approaches. We accomplished state-of-the-art results on the ICDAR-2013 dataset with an average F1-measure of 94.19% (92.06% for rows and 96.32% for columns). Thus, a relative error reduction of more than 25% is achieved. Furthermore, our proposed post-processing improves the average F1-measure to 95.46% that results in a relative error reduction of more than 35%. Moreover, we surpassed the baseline results on the TabStructDB dataset with an average F1-measure of 94.57% (94.08% for rows and 95.06% for columns).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "2207509298",
                        "name": "Muhammad Noman Afzal"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Borrowing from computer vision, other works [1, 5, 6, 12] showed that a trained object detection or image semantic model, such as a convolutional neural network, can learn to detect boundaries of tables, figures, lists, paragraphs with satisfying results; however, they do not tackle the issue of parsing smaller elements such as list items or sentences with computer vision techniques."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7575b837acbbac9180b5e8b80e4d50e21d8f7662",
                "externalIds": {
                    "DBLP": "conf/www/AuAK21",
                    "DOI": "10.1145/3442442.3451378",
                    "CorpusId": 235324783
                },
                "corpusId": 235324783,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7575b837acbbac9180b5e8b80e4d50e21d8f7662",
                "title": "FinSBD-2021: The 3rd Shared Task on Structure Boundary Detection in Unstructured Text in the Financial Domain",
                "abstract": "Document processing is a foundational pre-processing task in natural language application applied in the financial domain. In this paper, we present the result of FinSBD-3, the 3rd shared task on Structure Boundary Detection in unstructured text in the financial domain. The shared task is organized as part of the 1st Workshop on Financial Technology on the Web. Participants were asked to create system detecting the boundaries of elements in unstructured text extracted from financial PDF. This edition extends the previous shared tasks by adding boundaries of visual elements such as tables, figures, page headers and page footers; on top of sentences, lists and list items which were already present in previous edition of the shared tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1805993918",
                        "name": "Willy Au"
                    },
                    {
                        "authorId": "2106764095",
                        "name": "Abderrahim Ait-Azzi"
                    },
                    {
                        "authorId": "40625229",
                        "name": "Juyeon Kang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many previous works [22, 26, 23] and tools 16 have been developed to identify and parse table structures.",
                "Object detection-based methods like Faster R-CNN [24] and Mask R-CNN [11] are used for identifying document elements [34] and detecting tables [26, 22]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "300cbaf1644920b15a97692e6309f5d58e1abc0e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-15348",
                    "ArXiv": "2103.15348",
                    "DOI": "10.1007/978-3-030-86549-8_9",
                    "CorpusId": 232404723
                },
                "corpusId": 232404723,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/300cbaf1644920b15a97692e6309f5d58e1abc0e",
                "title": "LayoutParser: A Unified Toolkit for Deep Learning Based Document Image Analysis",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "101568984",
                        "name": "Zejiang Shen"
                    },
                    {
                        "authorId": "49775305",
                        "name": "Ruochen Zhang"
                    },
                    {
                        "authorId": "2065276972",
                        "name": "Melissa Dell"
                    },
                    {
                        "authorId": "145485725",
                        "name": "B. Lee"
                    },
                    {
                        "authorId": "2060946945",
                        "name": "Jacob Carlson"
                    },
                    {
                        "authorId": "2108801852",
                        "name": "Weining Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026Fu, & Zhang, 2019; Holec\u030cek, Hoskovec, Baudi\u0161, & Klinger, 2019; Schreiber, Agne, Wolf, Dengel, & Ahmed, 2017) through convolutional (Dong et al., 2019; Holec\u030cek et al., 2019) layers and recurrent neural\nnetworks (Schreiber et al., 2017) trained either on the imaged document or on other formats."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cd839ac63055e01e18c7d5429aad1217bb7d1979",
                "externalIds": {
                    "DBLP": "journals/widm/BonfittoCM21",
                    "MAG": "3148091059",
                    "DOI": "10.1002/widm.1407",
                    "CorpusId": 233648033
                },
                "corpusId": 233648033,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cd839ac63055e01e18c7d5429aad1217bb7d1979",
                "title": "Table understanding approaches for extracting knowledge from heterogeneous tables",
                "abstract": "Table understanding methods extract, transform, and interpret the information contained in tabular data embedded in documents/files of different formats. Such automatic understanding would allow to exploit tabular information with the aim of accurately answering queries, or integrating heterogeneous repositories of information in a common knowledge base, or exchanging information among different sources. The purpose of this survey is to provide a comprehensive analysis of the research efforts so far devoted to the problem of table understanding and to describe systems that support the transformation of heterogeneous tables into meaningful information.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150292430",
                        "name": "Sara Bonfitto"
                    },
                    {
                        "authorId": "2597140",
                        "name": "E. Casiraghi"
                    },
                    {
                        "authorId": "1741262",
                        "name": "M. Mesiti"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Examples include detecting tables, equations, figures [53, 43, 44], signatures and logos [46, 47]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "31664ced397985e4c30a8b8d9028865df2f13e6e",
                "externalIds": {
                    "ArXiv": "2103.08298",
                    "DBLP": "journals/ijdar/GoyalCB21",
                    "DOI": "10.1007/s10032-021-00367-3",
                    "CorpusId": 232232962
                },
                "corpusId": 232232962,
                "publicationVenue": {
                    "id": "46722266-4327-4a22-871d-5a9fe3c5ad0c",
                    "name": "International Journal on Document Analysis and Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Doc Anal Recognit"
                    ],
                    "issn": "1433-2825",
                    "url": "https://link.springer.com/journal/10032"
                },
                "url": "https://www.semanticscholar.org/paper/31664ced397985e4c30a8b8d9028865df2f13e6e",
                "title": "Knowledge-driven description synthesis for floor plan interpretation",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47254212",
                        "name": "Shreya Goyal"
                    },
                    {
                        "authorId": "3302494",
                        "name": "C. Chattopadhyay"
                    },
                    {
                        "authorId": "2073179601",
                        "name": "G. Bhatnagar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Examples for table and cell region detection include [2, 4, 6] while [4\u20137, 9] address table structure extraction."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9dbb26e4f04f38aa2a11289cbc82a2d9ed533741",
                "externalIds": {
                    "ArXiv": "2102.08445",
                    "DBLP": "journals/corr/abs-2102-08445",
                    "DOI": "10.1145/3397482.3450718",
                    "CorpusId": 231942681
                },
                "corpusId": 231942681,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9dbb26e4f04f38aa2a11289cbc82a2d9ed533741",
                "title": "TableLab: An Interactive Table Extraction System with Adaptive Deep Learning",
                "abstract": "Table extraction from PDF and image documents is a ubiquitous task in the real-world. Perfect extraction quality is difficult to achieve with one single out-of-box model due to (1) the wide variety of table styles, (2) the lack of training data representing this variety and (3) the inherent ambiguity and subjectivity of table definitions between end-users. Meanwhile, building customized models from scratch can be difficult due to the expensive nature of annotating table data. We attempt to solve these challenges with TableLab by providing a system where users and models seamlessly work together to quickly customize high-quality extraction models with a few labelled examples for the user\u2019s document collection, which contains pages with tables. Given an input document collection, TableLab first detects tables with similar structures (templates) by clustering embeddings from the extraction model. Document collections often contain tables created with a limited set of templates or similar structures. It then selects a few representative table examples already extracted with a pre-trained base deep learning model. Via an easy-to-use user interface, users provide feedback to these selections without necessarily having to identify every single error. TableLab then applies such feedback to finetune the pre-trained model and returns the results of the finetuned model back to the user. The user can choose to repeat this process iteratively until obtaining a customized model with satisfactory performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31856827",
                        "name": "N. Wang"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    },
                    {
                        "authorId": "1718694",
                        "name": "Yunyao Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6c6354b0c4d55890399aecd7fdd9e2432710aea4",
                "externalIds": {
                    "MAG": "3126481501",
                    "DBLP": "journals/ijon/JiangSKK21",
                    "DOI": "10.1016/j.neucom.2021.01.103",
                    "CorpusId": 233873553
                },
                "corpusId": 233873553,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6c6354b0c4d55890399aecd7fdd9e2432710aea4",
                "title": "TabCellNet: Deep learning-based tabular cell structure detection",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1995868863",
                        "name": "JiChu Jiang"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "152235107",
                        "name": "Shahzad Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Convolutional neural networks [150, 151], fully-convolutional neural networks [152, 153], region-based convolutional neural networks [154, 155, 156, 157], and graph neural networks [158, 159] have all been exploited to parse the physical layout of documents or to detect elements of interest (e."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a7ef6b6e94090993d08f824f3254e5420a71b3ed",
                "externalIds": {
                    "ArXiv": "2101.06573",
                    "DBLP": "journals/corr/abs-2101-06573",
                    "CorpusId": 231632613
                },
                "corpusId": 231632613,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a7ef6b6e94090993d08f824f3254e5420a71b3ed",
                "title": "Understanding in Artificial Intelligence",
                "abstract": "Current Artificial Intelligence (AI) methods, most based on deep learning, have facilitated progress in several fields, including computer vision and natural language understanding. The progress of these AI methods is measured using benchmarks designed to solve challenging tasks, such as visual question answering. A question remains of how much understanding is leveraged by these methods and how appropriate are the current benchmarks to measure understanding capabilities. To answer these questions, we have analysed existing benchmarks and their understanding capabilities, defined by a set of understanding capabilities, and current research streams. We show how progress has been made in benchmark development to measure understanding capabilities of AI methods and we review as well how current methods develop understanding capabilities.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2428309",
                        "name": "S. Maetschke"
                    },
                    {
                        "authorId": "91474933",
                        "name": "D. M. Iraola"
                    },
                    {
                        "authorId": "2055078423",
                        "name": "Pieter Barnard"
                    },
                    {
                        "authorId": "9585722",
                        "name": "Elaheh Shafieibavani"
                    },
                    {
                        "authorId": "2062708279",
                        "name": "Peter Zhong"
                    },
                    {
                        "authorId": "2118669012",
                        "name": "Ying Xu"
                    },
                    {
                        "authorId": "1399097376",
                        "name": "Antonio Jimeno-Yepes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, Schreiber et al. (2017) [4]) proposed a method based on Faster-RCNN network; Li et al. (2019) [5]) presented a method combined with GAN and so on.",
                "(2017) [4]) used the FCN to recognize rows and columns in tables.",
                "(2017) [4]) proposed a method based on Faster-RCNN network; Li et al.",
                "Schreiber et al. (2017) [4]) used the FCN to recognize rows and columns in tables."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e0dd683a69a85cd9d574017c6c0993c84eda159f",
                "externalIds": {
                    "DBLP": "journals/puc/WuZXZL22",
                    "MAG": "3167970107",
                    "DOI": "10.1007/s00779-020-01485-1",
                    "CorpusId": 231202527
                },
                "corpusId": 231202527,
                "publicationVenue": {
                    "id": "68fd8242-3be0-4b1e-8b5d-ad0a8a02db12",
                    "name": "Personal and Ubiquitous Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Pers Ubiquitous Comput"
                    ],
                    "issn": "1617-4909",
                    "url": "http://www.springer.com/computer/hci/journal/779",
                    "alternate_urls": [
                        "https://link.springer.com/journal/volumesAndIssues/779"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e0dd683a69a85cd9d574017c6c0993c84eda159f",
                "title": "TableRobot: an automatic annotation method for heterogeneous tables",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118065009",
                        "name": "Guibin Wu"
                    },
                    {
                        "authorId": "2151550608",
                        "name": "Junjie Zhou"
                    },
                    {
                        "authorId": "48935767",
                        "name": "Yongping Xiong"
                    },
                    {
                        "authorId": "2110840865",
                        "name": "Chao Zhou"
                    },
                    {
                        "authorId": "2109665210",
                        "name": "Chong Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, Schreiber et al. (2017) [4]) proposed a method based on Faster-RCNN network; Li et al. (2019) [5]) presented a method combined with GAN and so on.",
                "(2017) [4]) used the FCN to recognize rows and columns in tables.",
                "(2017) [4]) proposed a method based on Faster-RCNN network; Li et al.",
                "Schreiber et al. (2017) [4]) used the FCN to recognize rows and columns in tables."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c2e50e044b38d2df1fe073086857130509828b63",
                "externalIds": {
                    "DOI": "10.1007/s00779-020-01485-1",
                    "CorpusId": 254089122
                },
                "corpusId": 254089122,
                "publicationVenue": {
                    "id": "68fd8242-3be0-4b1e-8b5d-ad0a8a02db12",
                    "name": "Personal and Ubiquitous Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Pers Ubiquitous Comput"
                    ],
                    "issn": "1617-4909",
                    "url": "http://www.springer.com/computer/hci/journal/779",
                    "alternate_urls": [
                        "https://link.springer.com/journal/volumesAndIssues/779"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c2e50e044b38d2df1fe073086857130509828b63",
                "title": "TableRobot: an automatic annotation method for heterogeneous tables",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118065009",
                        "name": "Guibin Wu"
                    },
                    {
                        "authorId": "2151550608",
                        "name": "Junjie Zhou"
                    },
                    {
                        "authorId": "48935767",
                        "name": "Yongping Xiong"
                    },
                    {
                        "authorId": "2191394384",
                        "name": "Chaoyi Zhou"
                    },
                    {
                        "authorId": "2109665210",
                        "name": "Chong Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a1e997ca9eaffc598b96df37aeca73acf4e75f52",
                "externalIds": {
                    "MAG": "3119108181",
                    "DBLP": "journals/bdr/LiangCH21",
                    "DOI": "10.1016/J.BDR.2021.100195",
                    "CorpusId": 234149026
                },
                "corpusId": 234149026,
                "publicationVenue": {
                    "id": "ee2dd599-75c3-4b69-b970-053972631b37",
                    "name": "Big Data Research",
                    "type": "journal",
                    "alternate_names": [
                        "Big Data Res"
                    ],
                    "issn": "2214-5796",
                    "url": "https://www.journals.elsevier.com/big-data-research/",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/22145796"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a1e997ca9eaffc598b96df37aeca73acf4e75f52",
                "title": "Comparative Study of Layout Analysis of Tabulated Historical Documents",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110914908",
                        "name": "Xusheng Liang"
                    },
                    {
                        "authorId": "2784141",
                        "name": "A. Cheddad"
                    },
                    {
                        "authorId": "1712813",
                        "name": "Johan Hall"
                    }
                ]
            }
        },
        {
            "contexts": [
                "been restricted to structure detection only [6, 9, 11]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "357896ab585d98175d6be721a9578b988cf4101b",
                "externalIds": {
                    "DBLP": "conf/comad/JainGVSDS21",
                    "DOI": "10.1145/3430984.3430992",
                    "CorpusId": 229548609
                },
                "corpusId": 229548609,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/357896ab585d98175d6be721a9578b988cf4101b",
                "title": "Extracting Tabular data for Question-Answering from Documents",
                "abstract": "In this paper, we present the designs of a system that is targeted at automated data curation from tables appearing in large documents like reports, technical articles etc.. There is an increasing demand for applications that can support question answering over tables. It can also be used to automatically fill-up data-sheets in a regular fashion. The task is complex since there are no fixed formats for these tables. Inferring table structure and extracting tabular data are known to be difficult problems. In this paper we present methods to retrieve tabular data from approximately inferred structures. We present QuATab, an end-to-end system that can do Q&A from tables contained in documents.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2026108634",
                        "name": "Palak Jain"
                    },
                    {
                        "authorId": "2042282787",
                        "name": "Tushar Goel"
                    },
                    {
                        "authorId": "2126226",
                        "name": "Ishan Verma"
                    },
                    {
                        "authorId": "2042286770",
                        "name": "Mohammad Shakir"
                    },
                    {
                        "authorId": "1724193",
                        "name": "Lipika Dey"
                    },
                    {
                        "authorId": "40595239",
                        "name": "Geetika Sharma"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "19887be4f0dc5be93ae2968e9270f66a6e592723",
                "externalIds": {
                    "MAG": "3175634416",
                    "DOI": "10.1007/978-3-030-80478-7_13",
                    "CorpusId": 238097021
                },
                "corpusId": 238097021,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/19887be4f0dc5be93ae2968e9270f66a6e592723",
                "title": "Method for Processing Document with Tables in Russian-Language Automated Information Systems",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1580466066",
                        "name": "Dudnikov Sergey"
                    },
                    {
                        "authorId": "1580472290",
                        "name": "Mikheev Petr"
                    },
                    {
                        "authorId": "2135873873",
                        "name": "Dobrovolskiy Alexander"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5afded5e53199d2bee71c2067afde4b338d6d49c",
                "externalIds": {
                    "DOI": "10.1109/ICSP48669.2020.9321003",
                    "CorpusId": 231682183
                },
                "corpusId": 231682183,
                "publicationVenue": {
                    "id": "63410070-a9b9-46ac-bdec-92b016246795",
                    "name": "International Conference on the Software Process",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Signal Process",
                        "ICSP",
                        "Int Conf Softw Process",
                        "International Conference on Signal Processing"
                    ],
                    "url": "http://www.ece.utexas.edu/~perry/prof/ispa/index.html"
                },
                "url": "https://www.semanticscholar.org/paper/5afded5e53199d2bee71c2067afde4b338d6d49c",
                "title": "A Deep Semantic Segmentation Model for Image-based Table Structure Recognition",
                "abstract": "Table structure recognition is a crucial step for automatic table information extraction. It is conventional to utilize the features such as ruling lines or words for parsing the rows, columns and cells in a table. However, these conventional methods are ineffective for image-based tables when ruling lines are not visible or the words cannot be recognized through the OCR system. In order to overcome these problems, we propose a deep semantic segmentation model for image-based table structure recognition. Specifically, it is an end-to-end semantic segmentation neural network to determine a pixel-wise prediction map for an input table image where the labels are row separator, column separator, cell content and background. Moreover, by making the connected componnet analysis on the prediction map, we can obtain the bounding boxes of row separators, column separators and cell contents, more accurately. Then we number row/column separators in order by coordinate sorting. Thus, we can make full use of relative positions between row/column separators and cell contents, and further assign the row/column number to each cell. Due to the lack of training data, a large amount of synthetic data are automatically generated in our experiments. It is demonstrated by the experimental results that our proposed model is suitable for various table types, which can achieve 0.9769 and 0.9343 average F1 scores on a generative dataset when the IoU threshold is set to 0.6 and 0.8, respectively.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2047123130",
                        "name": "Y. Zou"
                    },
                    {
                        "authorId": "1685259",
                        "name": "Jinwen Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such approaches include the following: (i) binary classification based on convolutional neural networks [10], (ii) fine-tuning pre-trained models for object detection in images [1], [6], [30], [34], [35], (iii) semantic segmentation [11], [14], and (iv) feature generation [19].",
                "[30] discovered that the table detection in document images is similar to detection of objects in natural scene images.",
                "The contemporary deep neural networks for table detection in document images [1], [6], [13], [24], [30], [34], [35] demonstrate a high accuracy on the competition datasets.",
                "[30] for developing a DNN-model for table detection."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "cb9b3508cf9a8b3a669d47198819f0cc0c6ed135",
                "externalIds": {
                    "DOI": "10.1109/ISPRAS51486.2020.00020",
                    "CorpusId": 233262202
                },
                "corpusId": 233262202,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cb9b3508cf9a8b3a669d47198819f0cc0c6ed135",
                "title": "On Graph-Based Verification for PDF Table Detection",
                "abstract": "Many non-editable documents are shared in PDF (Portable Document Format). They are typically not accompanied by tags for annotating the page layout, including table positions. One of the important challenges of the analysis and understanding of such documents is table detection. This paper outlines a novel two-phase approach to the table detection in untagged PDF documents. The first phase uses deep neural networks (DNN) to predict some table candidates. The second phase selects probable tables from the candidates by verifying their graph representation. We build a weighted directed graph from text blocks inside a predicted area of a table. A set of such graphs produced from the \u201cICDAR 2013 Table Competition\u201d dataset allowed us to train a verification model based on the Random Forest technique. The empirical results for competitive dataset demonstrated high performance of our implementation of this approach. We showed that additional verification enables reduction of errors and improvement of results of the PDF table detection.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144116982",
                        "name": "A. Mikhailov"
                    },
                    {
                        "authorId": "3355714",
                        "name": "A. Shigarov"
                    },
                    {
                        "authorId": "2077521734",
                        "name": "Egor Rozhkov"
                    },
                    {
                        "authorId": "13719083",
                        "name": "Igor Cherepanov"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "08b2ff5176db4dd27dde654e899b561841d47b0f",
                "externalIds": {
                    "DBLP": "journals/ett/IbanezCZ21",
                    "MAG": "3098115677",
                    "DOI": "10.1002/ett.4169",
                    "CorpusId": 228833813
                },
                "corpusId": 228833813,
                "publicationVenue": {
                    "id": "9ee7345a-5e9f-4002-9880-d6b27d275524",
                    "name": "Transactions on Emerging Telecommunications Technologies",
                    "type": "journal",
                    "alternate_names": [
                        "European Transactions on Telecommunications",
                        "Eur Trans Telecommun",
                        "TETT",
                        "Trans Emerg Telecommun Technol"
                    ],
                    "issn": "1120-3862",
                    "alternate_issns": [
                        "2161-3915",
                        "1124-318X"
                    ],
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/104087069",
                    "alternate_urls": [
                        "http://eu.wiley.com/WileyCDA/WileyTitle/productCd-ETT.html",
                        "http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1541-8251",
                        "https://onlinelibrary.wiley.com/journal/21613915",
                        "https://onlinelibrary.wiley.com/journal/15418251"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/08b2ff5176db4dd27dde654e899b561841d47b0f",
                "title": "Deep learning support for intelligent transportation systems",
                "abstract": "Intelligent Transportation Systems (ITS) help improve the ever\u2010increasing vehicular flow and traffic efficiency in urban traffic to reduce the number of accidents. The generation of massive amounts of data generated by all the digital devices connected to the transportation network enables the creation of datasets to perform an in\u2010depth analysis of the data using deep learning methods. Such methods can help predict traffic performance, automated traffic light management, lane detection, and identifying objects near vehicles to increase the safety and efficiency of ITS. We discuss some of the challenges that need to be solved to achieve seamless integration between ITS and deep learning methods to address issues such as (1) improving traffic flow/transportation logistics, (2) predicting best routes for the transportation of goods, (3) optimal fuel consumption, (4) intelligent environmental conditions perception, (5) traffic speed management, and accident prevention.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "37742212",
                        "name": "J. Ib\u00e1\u00f1ez"
                    },
                    {
                        "authorId": "1399175725",
                        "name": "J. Contreras-Castillo"
                    },
                    {
                        "authorId": "1706796",
                        "name": "S. Zeadally"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We explore three neural models used in previous works on abusive language classi-\ncation: Convolutional Neural Network (CNN)\nGRU\nfasttext .887 .661 .312 .284\nword2vec .887 .633 .301 .254\n-GRU random .868 .586 .236 .219 fasttext .891 .639 .324 .365 word2vec .890 .631 .315 .306\nTable 4: Results on st. False negative/positive equality differences are larger when pre-trained embedding is used and CNN or -RNN is trained\n(Park and Fung, 2017), Gated Recurrent Unit (GRU) (Cho et al., 2014), and Bidirectional GRU with self-attention ( -GRU) (Pavlopoulos et al.,\n2017), but with a simpler mechanism used in\nFelbo et al. (2017).",
                "CNN: Convolution layers with 3 lters with the size of [3,4,5], feature map size=100, Embedding Size=300, Maxpooling, Dropout=0.5\n2.",
                "In: IJCNN. pp. 1\u20138 (2019) 38.",
                "More recently, some deep learning methods based on Convolutional Neural Networks (CNN) [34], Conditional Generative Adversarial Networks (CGAN) [37], and a combination of a CNN, saliency and graphical models [20] have been evaluated."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2f014ed269c77bdf1286bb540278199fb45adea1",
                "externalIds": {
                    "DBLP": "conf/semweb/KruitHU20",
                    "MAG": "3107405279",
                    "ArXiv": "2107.13306",
                    "DOI": "10.1007/978-3-030-62419-4_20",
                    "CorpusId": 226229411
                },
                "corpusId": 226229411,
                "publicationVenue": {
                    "id": "efa3ff7a-4d96-44a1-a022-a683408919b6",
                    "name": "International Workshop on the Semantic Web",
                    "type": "conference",
                    "alternate_names": [
                        "SemWeb",
                        "Int Workshop Semantic Web"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2f014ed269c77bdf1286bb540278199fb45adea1",
                "title": "Tab2Know: Building a Knowledge Base from Tables in Scientific Papers",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2503217",
                        "name": "B. Kruit"
                    },
                    {
                        "authorId": "145911458",
                        "name": "Hongyu He"
                    },
                    {
                        "authorId": "1915156",
                        "name": "J. Urbani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The natural language approach has been widely researched and implemented in recent years [5]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c1f98b60e758aac40adc9c045fae16fbecbc270f",
                "externalIds": {
                    "MAG": "3108996037",
                    "DBLP": "conf/cnsm/JainBF20",
                    "DOI": "10.23919/CNSM50824.2020.9269096",
                    "CorpusId": 227278176
                },
                "corpusId": 227278176,
                "publicationVenue": {
                    "id": "7a0ab88e-224a-475b-9ea6-e66a6b3448c8",
                    "name": "Conference on Network and Service Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Netw Serv Manag",
                        "CNSM"
                    ],
                    "url": "http://www.cnsm-conf.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c1f98b60e758aac40adc9c045fae16fbecbc270f",
                "title": "Unsupervised Noise Detection in Unstructured data for Automatic Parsing",
                "abstract": "The telecommunications industry makes extensive use of data extracted from logs, alarms, traces, diagnostics, and other monitoring devices. Analyzing the generated data requires that the data be parsed, re-structured, and re-formatted. Developing custom parsers for each input format is labor-intensive and requires domain knowledge. In this paper, we describe a novel unsupervised text processing pipeline to automatically detect and label relevant data and eliminate noise using Levenshtein similarity and Agglomerative clustering. We experiment with different similarity and clustering algorithms on a selection of common data formats to verify the accuracy of the proposed technique. The results suggest that the proposed methodology has higher accuracy.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48326213",
                        "name": "Shubham Jain"
                    },
                    {
                        "authorId": "1904691817",
                        "name": "Amy de Buitl\u00e9ir"
                    },
                    {
                        "authorId": "144789260",
                        "name": "Enda Fallon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026using convolutional neural networks (CNNs) provide considerably better performance (Xu et al., 2020; Kavasidis et al., 2019; Siegel et al., 2018; Schreiber et al., 2017; Gilani et al., 2017; Hao et al., 2016), the quality of the labeled training data often determines the success of these\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10682868f013a538129ad2bbc90f873c0b81cab3",
                "externalIds": {
                    "ACL": "2020.sdp-1.10",
                    "DBLP": "conf/emnlp/LingC20",
                    "MAG": "3098562592",
                    "DOI": "10.18653/v1/2020.sdp-1.10",
                    "CorpusId": 226283959
                },
                "corpusId": 226283959,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/10682868f013a538129ad2bbc90f873c0b81cab3",
                "title": "DeepPaperComposer: A Simple Solution for Training Data Preparation for Parsing Research Papers",
                "abstract": "We present DeepPaperComposer, a simple solution for preparing highly accurate (100%) training data without manual labeling to extract content from scholarly articles using convolutional neural networks (CNNs). We used our approach to generate data and trained CNNs to extract eight categories of both textual (titles, abstracts, headers, figure and table captions, and other texts) and non-textural content (figures and tables) from 30 years of IEEE VIS conference papers, of which a third were scanned bitmap PDFs. We curated this dataset and named it VISpaper-3K. We then showed our initial benchmark performance using VISpaper-3K over itself and CS-150 using YOLOv3 and Faster-RCNN. We open-source DeepPaperComposer of our training data generation and released the resulting annotation data VISpaper-3K to promote re-producible research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2004871801",
                        "name": "Meng Ling"
                    },
                    {
                        "authorId": "49251914",
                        "name": "Jian Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dca3db21cb5e386a8754c060f37e5035af324fb6",
                "externalIds": {
                    "DBLP": "conf/icpr/ZhangPLG20",
                    "MAG": "3096126694",
                    "DOI": "10.1109/ICPR48806.2021.9413069",
                    "CorpusId": 228975594
                },
                "corpusId": 228975594,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dca3db21cb5e386a8754c060f37e5035af324fb6",
                "title": "An Integrated Approach of Deep Learning and Symbolic Analysis for Digital PDF Table Extraction",
                "abstract": "Deep learning has shown great success at interpreting unstructured data such as object recognition in images. Symbolic/logical-reasoning techniques have shown great success in interpreting structured data such as table extraction in webpages, custom text files, spreadsheets. The tables in PDF documents are often generated from such structured sources (text-based Word/LATEX documents, spreadsheets, webpages) but end up being unstructured. We thus explore novel combinations of deep learning and symbolic reasoning techniques to build an effective solution for PDF table extraction. We evaluate effectiveness without granting partial credit for matching part of a table (which may cause silent errors in downstream data processing). Our method achieves a 0.725 $F_{1}$ score (vs. 0.339 for the state-of-the-art) on detecting correct table bounds-a much stricter metric than the common one of detecting characters within tables-in a well known public benchmark (ICDAR 2013) and a 0.404 $F_{1}$ score (vs. 0.144 for the state-of-the-art) on our private benchmark with more widely varied table structures.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48985600",
                        "name": "Mengshi Zhang"
                    },
                    {
                        "authorId": "39513661",
                        "name": "Daniel Perelman"
                    },
                    {
                        "authorId": "143914877",
                        "name": "Vu Le"
                    },
                    {
                        "authorId": "2108314",
                        "name": "Sumit Gulwani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Among these, the problem of table structure recognition has been of high interest in the community [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20].",
                "In the space of document images, researchers have been working on understanding equations [30,31], figures [32,33] and tables [6,7,8,9,10,11,12,13,14,15,16,17].",
                "deepdesrt [7] scitsr 12K S-A 0.",
                "Table structure recognition is a challenging problem due to complex structures and high variability in table layouts [4,5,6,7,8,9,10,11,12,13,14,15,16,17].",
                "In S-A, we observe that our model outperforms deepdesrt [7] method by a 27.",
                "Many cognitive methods [6,7,8,9,10,11,12,14,15,16,37,38,39,40,41,42,43] have also been presented to understand table structures as they are robust to the input type (whether being scanned images or native digital).",
                "We compare the performance of our tabstruct-net against seven benchmark methods \u2014 deepdesrt [7], tablenet [12], graphtsr [14], splerge [10], dgcnn [9], Bi-directional gru [15] and Image-to-Text [11]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "30860e7c569d777db8d2b6fb0f7c374965ac62dd",
                "externalIds": {
                    "MAG": "3109870706",
                    "DBLP": "conf/eccv/RajaMJ20",
                    "ArXiv": "2010.04565",
                    "DOI": "10.1007/978-3-030-58604-1_5",
                    "CorpusId": 221990801
                },
                "corpusId": 221990801,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/30860e7c569d777db8d2b6fb0f7c374965ac62dd",
                "title": "Table Structure Recognition using Top-Down and Bottom-Up Cues",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2053910624",
                        "name": "S. Raja"
                    },
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "1694502",
                        "name": "C. V. Jawahar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Deep learning-based approaches can be used for document layout analysis and content parsing (Shen et al., 2021; Zhong et al., 2019; Schreiber et al., 2017)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bd2a7c4432bd22d06cf4df2d450d6dea8df21ab4",
                "externalIds": {
                    "ACL": "2022.nlpcss-1.19",
                    "MAG": "3146762582",
                    "DOI": "10.18653/v1/2022.nlpcss-1.19",
                    "CorpusId": 238442944
                },
                "corpusId": 238442944,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bd2a7c4432bd22d06cf4df2d450d6dea8df21ab4",
                "title": "OLALA: Object-Level Active Learning for Efficient Document Layout Annotation",
                "abstract": "Layout detection is an essential step for accurately extracting structured contents from historical documents. The intricate and varied layouts present in these document images make it expensive to label the numerous layout regions that can be densely arranged on each page. Current active learning methods typically rank and label samples at the image level, where the annotation budget is not optimally spent due to the overexposure of common objects per image. Inspired by recent progress in semi-supervised learning and self-training, we propose OLALA, an Object-Level Active Learning framework for efficient document layout Annotation. OLALA aims to optimize the annotation process by selectively annotating only the most ambiguous regions within an image, while using automatically generated labels for the rest. Central to OLALA is a perturbation-based scoring function that determines which objects require manual annotation. Extensive experiments show that OLALA can significantly boost model performance and improve annotation efficiency, facilitating the extraction of masses of structured text for downstream NLP applications.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "101568984",
                        "name": "Zejiang Shen"
                    },
                    {
                        "authorId": "2143573527",
                        "name": "Jian Zhao"
                    },
                    {
                        "authorId": "2065276972",
                        "name": "Melissa Dell"
                    },
                    {
                        "authorId": "40508553",
                        "name": "Yaoliang Yu"
                    },
                    {
                        "authorId": "2108801852",
                        "name": "Weining Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Deep learning-based approaches have recently been widely applied to document layout analysis and content parsing (Zhong, Tang, and Yepes 2019; Schreiber et al. 2017).",
                "applied to document layout analysis and content parsing (Zhong, Tang, and Yepes 2019; Schreiber et al. 2017)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c8eaf2380f2efc19ca35cba0cecd4f590b9f7060",
                "externalIds": {
                    "ArXiv": "2010.01762",
                    "MAG": "3090826562",
                    "DBLP": "journals/corr/abs-2010-01762",
                    "CorpusId": 222133860
                },
                "corpusId": 222133860,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c8eaf2380f2efc19ca35cba0cecd4f590b9f7060",
                "title": "OLALA: Object-Level Active Learning Based Layout Annotation",
                "abstract": "In layout object detection problems, the ground-truth datasets are constructed by annotating object instances individually. Yet active learning for object detection is typically conducted at the image level, not at the object level. Because objects appear with different frequencies across images, image-level active learning may be subject to over-exposure to common objects. This reduces the efficiency of human labeling. This work introduces an Object-Level Active Learning based Layout Annotation framework, OLALA, which includes an object scoring method and a prediction correction algorithm. The object scoring method estimates the object prediction informativeness considering both the object category and the location. It selects only the most ambiguous object prediction regions within an image for annotators to label, optimizing the use of the annotation budget. For the unselected model predictions, we propose a correction algorithm to rectify two types of potential errors with minor supervision from ground-truths. The human annotated and model predicted objects are then merged as new image annotations for training the object detection models. In simulated labeling experiments, we show that OLALA helps to create the dataset more efficiently and report strong accuracy improvements of the trained models compared to image-level active learning baselines. The code is available at this https URL",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "101568984",
                        "name": "Zejiang Shen"
                    },
                    {
                        "authorId": "2143573527",
                        "name": "Jian Zhao"
                    },
                    {
                        "authorId": "2065276972",
                        "name": "Melissa Dell"
                    },
                    {
                        "authorId": "40508553",
                        "name": "Yaoliang Yu"
                    },
                    {
                        "authorId": "2108801852",
                        "name": "Weining Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For analysing the tables in scanned documents, (Schreiber et al. 2017; Paliwal et al. 2019; Prasad et al. 2020) propose different modifications to standard CNN network architectures such as VGGNet (Si-\n.\nmonyan and Zisserman 2014) used for classification and Faster R-CNN (Ren et al. 2015) for\u2026",
                "For analysing the tables in scanned documents, (Schreiber et al. 2017; Paliwal et al. 2019; Prasad et al. 2020) propose different modifications to standard CNN network architectures such as VGGNet (SiFigure 3: Results of Table Token Classification."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d1cec8d516931428b492a8e4fbbcef38957bce2b",
                "externalIds": {
                    "ArXiv": "2009.14457",
                    "MAG": "3090669478",
                    "DBLP": "journals/corr/abs-2009-14457",
                    "CorpusId": 222067107
                },
                "corpusId": 222067107,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d1cec8d516931428b492a8e4fbbcef38957bce2b",
                "title": "Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning",
                "abstract": "In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation. We design the network architecture and the pre-training tasks to incorporate the multi-modal document information across text, layout, and image dimensions and allow the network to work with multi-page documents. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks such as document classification, document information extraction, and document retrieval. We conduct exhaustive experiments to compare performance against different ablations of our framework and state-of-the-art baselines. We discuss the current limitations and next steps for our work.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51011359",
                        "name": "S. Pramanik"
                    },
                    {
                        "authorId": "33906968",
                        "name": "Shashank Mujumdar"
                    },
                    {
                        "authorId": "1557607422",
                        "name": "Hima Patel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We refer the readers to [25, 26] for table detection and [27, 28] for figure detection algorithms."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "11a0ea1fa4753a8798598a15bdbb5bd2be0ca038",
                "externalIds": {
                    "DBLP": "conf/doceng/WangL20",
                    "MAG": "3088711103",
                    "DOI": "10.1145/3395027.3419580",
                    "CorpusId": 221866201
                },
                "corpusId": 221866201,
                "publicationVenue": {
                    "id": "32c90593-78aa-40de-8700-dd31a625b15a",
                    "name": "ACM Symposium on Document Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "Doc Eng",
                        "Document Engineering",
                        "DocEng",
                        "ACM Symp Doc Eng"
                    ],
                    "url": "http://www.documentengineering.org/"
                },
                "url": "https://www.semanticscholar.org/paper/11a0ea1fa4753a8798598a15bdbb5bd2be0ca038",
                "title": "PDF2LaTeX: A Deep Learning System to Convert Mathematical Documents from PDF to LaTeX",
                "abstract": "The mathematical contents of scientific publications in PDF format cannot be easily analyzed by regular PDF parsers and OCR tools. In this paper, we propose a novel OCR system called PDF2LaTeX, which extracts math expressions and text in both postscript and image-based PDF files and translates them into LaTeX markup. As a preprocessing step, PDF2LaTeX first renders a PDF file into its image format, and then uses projection profile cutting (PPC) to analyze the page layout. The analysis of math expressions and text is based on a series of deep learning algorithms. First, it uses a convolutional neural network (CNN) as a binary classifier to detect math image blocks based on visual features. Next, it uses a conditional random field (CRF) to detect math-text boundaries by incorporating semantics and context information. In the end, the system uses two different models based on a CNN-LSTM neural network architecture to translate image blocks of math expressions and plaintext into the LaTeX representations. For testing, we created a new dataset composed of 102 PDF pages collected from publications on arXiv.org and compared the performance between PDF2LaTeX and the state-of-the-art commercial software InftyReader. The experiment results showed that the proposed system achieved a better recognition accuracy (81.1%) measured by the string edit distance between the predicted LaTeX and the ground truth.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2510212",
                        "name": "Zelun Wang"
                    },
                    {
                        "authorId": "2108808292",
                        "name": "Jyh-Charn S. Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The contemporary deep neural network models for the table detection [1], [5], [16], [18] show a high accuracy on the competition datasets such as UNLV [17], Marmot1, ICDAR 2013 [7], and ICDAR 2017 [4]).",
                "The current trend involves deep learning techniques: binary classification based on convolutional neural networks [8], fine-tuned object detection models [1], [5], [16], [18], and semantic segmentation [9], [10]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "41cd98cc8df87b3c214aac36e2480111d3711cc1",
                "externalIds": {
                    "MAG": "3096535843",
                    "DBLP": "conf/mipro/CherepanovMSP20",
                    "DOI": "10.23919/MIPRO48935.2020.9245241",
                    "CorpusId": 226853374
                },
                "corpusId": 226853374,
                "publicationVenue": {
                    "id": "25710d4b-a2d9-4b6c-9f28-154866d9fbbe",
                    "name": "International Convention on Information and Communication Technology, Electronics and Microelectronics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conv Inf Commun Technol Electron Microelectron",
                        "MIPRO"
                    ],
                    "url": "http://www.mipro.hr/"
                },
                "url": "https://www.semanticscholar.org/paper/41cd98cc8df87b3c214aac36e2480111d3711cc1",
                "title": "On automated workflow for fine-tuning deepneural network models for table detection in document images",
                "abstract": "Nowadays methods and software for extracting tables from document images and portable documents (PDF) continue to be actively developed. One of the promising approaches to this task is the usage of fine-tuned object detection models. However, this approach involves many manipulations with data preparation and training process configuration. This paper proposes an automated workflow for fine-tuning deep neural network models for the table detection in document images. It enables us to automate two sub-tasks: (i) preparing a training dataset in the PascalVOC format with image transformation and augmentation; (ii) training a table detection model by using the well-known Faster R-CNN architecture. Implementation of the workflow design simplifies the use of the approach proposed by decreasing the number of required manipulations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "13719083",
                        "name": "Igor Cherepanov"
                    },
                    {
                        "authorId": "144116982",
                        "name": "A. Mikhailov"
                    },
                    {
                        "authorId": "3355714",
                        "name": "A. Shigarov"
                    },
                    {
                        "authorId": "144364060",
                        "name": "Viacheslav V. Paramonov"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44aae45af6335eb332cc961b924f43c75b3e36e1",
                "externalIds": {
                    "MAG": "3088382477",
                    "DOI": "10.3390/APP10186460",
                    "CorpusId": 224892124
                },
                "corpusId": 224892124,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/44aae45af6335eb332cc961b924f43c75b3e36e1",
                "title": "Fi-Fo Detector: Figure and Formula Detection Using Deformable Networks",
                "abstract": "We propose a novel hybrid approach that fuses traditional computer vision techniques with deep learning models to detect figures and formulas from document images. The proposed approach first fuses the different computer vision based image representations, i.e., color transform, connected component analysis, and distance transform, termed as Fi-Fo image representation. The Fi-Fo image representation is then fed to deep models for further refined representation-learning for detecting figures and formulas from document images. The proposed approach is evaluated on a publicly available ICDAR-2017 Page Object Detection (POD) dataset and its corrected version. It produces the state-of-the-art results for formula and figure detection in document images with an f1-score of 0.954 and 0.922, respectively. Ablation study results reveal that the Fi-Fo image representation helps in achieving superior performance in comparison to raw image representation. Results also establish that the hybrid approach helps deep models to learn more discriminating and refined features.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "7207683",
                        "name": "Junaid Younas"
                    },
                    {
                        "authorId": "29005173",
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "authorId": "49961156",
                        "name": "Mohsin Munir"
                    },
                    {
                        "authorId": "49012494",
                        "name": "M. I. Malik"
                    },
                    {
                        "authorId": "1688013",
                        "name": "F. Shafait"
                    },
                    {
                        "authorId": "143951813",
                        "name": "P. Lukowicz"
                    },
                    {
                        "authorId": "1734717217",
                        "name": "Sheraz Ahmed"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thanks to the development of several public databases, document processing has made a great progress in recent years [1], [2]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "824d7f06c640c1c7f2d2905e29795f2b700247ce",
                "externalIds": {
                    "ArXiv": "2010.08764",
                    "DBLP": "journals/corr/abs-2010-08764",
                    "MAG": "3083907077",
                    "DOI": "10.1109/TPAMI.2020.3022406",
                    "CorpusId": 221541496,
                    "PubMed": "32894707"
                },
                "corpusId": 221541496,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/824d7f06c640c1c7f2d2905e29795f2b700247ce",
                "title": "DE-GAN: A Conditional Generative Adversarial Network for Document Enhancement",
                "abstract": "Documents often exhibit various forms of degradation, which make it hard to be read and substantially deteriorate the performance of an OCR system. In this paper, we propose an effective end-to-end framework named document enhancement generative adversarial networks (DE-GAN) that uses the conditional GANs (cGANs) to restore severely degraded document images. To the best of our knowledge, this practice has not been studied within the context of generative adversarial deep networks. We demonstrate that, in different tasks (document clean up, binarization, deblurring and watermark removal), DE-GAN can produce an enhanced version of the degraded document with a high quality. In addition, our approach provides consistent improvements compared to state-of-the-art methods over the widely used DIBCO 2013, DIBCO 2017, and H-DIBCO 2018 datasets, proving its ability to restore a degraded document image to its ideal condition. The obtained results on a wide variety of degradation reveal the flexibility of the proposed model to be exploited in other document enhancement problems.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1742323071",
                        "name": "Mohamed Ali Souibgui"
                    },
                    {
                        "authorId": "3188383",
                        "name": "Yousri Kessentini"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fb695a22e3d2c26dd114d9749f86d02b653437af",
                "externalIds": {
                    "MAG": "3083068328",
                    "DOI": "10.3390/app10186182",
                    "CorpusId": 225336362
                },
                "corpusId": 225336362,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fb695a22e3d2c26dd114d9749f86d02b653437af",
                "title": "Development of a Framework to Understand Tables in Engineering Specification Documents",
                "abstract": "Several works have been done in the last decades for understanding tables in documents, but most of them were not specifically designed to understand tables in engineering specification documents. Tables in engineering specifications have characteristics such as various table structures with restricted terms. A framework is developed to address the issues in understanding tables in engineering specification documents. The framework consists of three steps: (1) Identifying minimal tables, (2) classifying cells, and (3) extending a domain knowledge map. A modified XY-tree algorithm was developed to find minimal tables, and a neural network algorithm was adopted to classify cells into labels and data. Then, specific domain rules were developed to discover concepts and relationships from terms in the classified cells. It is assumed a domain ontology is given, and it is extended with new concepts and relationships extracted from tables. We illustrated how each step performed with engineering table examples. The proposed framework could be used for searching product specification and for discovering hidden knowledge from tables in engineering specification documents.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2005940746",
                        "name": "Valentin Agossou"
                    },
                    {
                        "authorId": "1692100",
                        "name": "H. Suh"
                    },
                    {
                        "authorId": "1982851",
                        "name": "Heejung Lee"
                    },
                    {
                        "authorId": "2108805582",
                        "name": "Jae-Hyun Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There also exist deep learning methods to detect tables from images (Schreiber et al., 2017); (Paliwal et al., 2019).",
                "There also exist deep learning methods to detect tables from images (Schreiber et al., 2017); (Paliwal et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8af9a995d661b959366b0c7e26303c10acf20cc2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2008-11409",
                    "MAG": "3082122984",
                    "ArXiv": "2008.11409",
                    "CorpusId": 221319797
                },
                "corpusId": 221319797,
                "publicationVenue": {
                    "id": "1c65904b-0167-4c45-acfd-67d8c430d00b",
                    "name": "Journ\u00e9es Francophones sur les Entrep\u00f4ts de Donn\u00e9es et l'Analyse en ligne",
                    "type": "conference",
                    "alternate_names": [
                        "EDA",
                        "Journ\u00e9es Francoph sur Entrep\u00f4ts Donn\u00e9es l'analyse ligne"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8af9a995d661b959366b0c7e26303c10acf20cc2",
                "title": "Automatic Integration Issues of Tabular Data for On-Line Analysis Processing",
                "abstract": "Companies and individuals produce numerous tabular data. The objective of this position paper is to draw up the challenges posed by the automatic integration of data in the form of tables so that they can be cross-analyzed. We provide a first automatic solution for the integration of such tabular data to allow On-Line Analysis Processing. To fulfil this task, features of tabular data should be analyzed and the challenge of automatic multidimensional schema generation should be addressed. Hence, we propose a typology of tabular data and discuss our idea of an automatic solution.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2108581753",
                        "name": "Yuzhao Yang"
                    },
                    {
                        "authorId": "145025853",
                        "name": "J. Darmont"
                    },
                    {
                        "authorId": "2174616",
                        "name": "F. Ravat"
                    },
                    {
                        "authorId": "2862341",
                        "name": "O. Teste"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "08b65e546c84f36e81010798a1c64e49372ea5c9",
                "externalIds": {
                    "ArXiv": "2008.10831",
                    "DBLP": "conf/icpr/AgarwalMJ20",
                    "MAG": "3081020959",
                    "DOI": "10.1109/ICPR48806.2021.9411922",
                    "CorpusId": 221293251
                },
                "corpusId": 221293251,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/08b65e546c84f36e81010798a1c64e49372ea5c9",
                "title": "CDeC-Net: Composite Deformable Cascade Network for Table Detection in Document Images",
                "abstract": "Localizing page elements/objects such as tables, figures, equations, etc. is the primary step in extracting information from document images. We propose a novel end-to-end trainable deep network, (cnec-xet) for detecting tables present in the documents. The proposed network consists of a multistage extension of Mask R-CNN with a dual backbone having deformable convolution for detecting tables varying in scale with high detection accuracy at higher IoU threshold. We empirically evaluate CDeC-Net on the publicly available benchmark datasets with extensive experiments. Our solution has three important properties: (i) a single trained model CDeC-Net\u2021that performs well across all the popular benchmark datasets; (ii) we report excellent performances across multiple, including higher, thresholds of IoU; (iii) by following the same protocol of the recent papers for each of the benchmarks, we consistently demonstrate the superior quantitative performance. Our code and models are publicly available at https://github.com/mdv3101/CDeCNet for enabling reproducibility of the results.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2148629212",
                        "name": "Madhav Agarwal"
                    },
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "1694502",
                        "name": "C. V. Jawahar"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c535e3da4ed7659266fdf23166127a5c2b604ec4",
                "externalIds": {
                    "MAG": "3049072356",
                    "DBLP": "conf/ercimdl/LehenmeierBM20",
                    "DOI": "10.1007/978-3-030-54956-5_17",
                    "CorpusId": 221159378
                },
                "corpusId": 221159378,
                "publicationVenue": {
                    "id": "14508953-a0fc-42e5-9a54-ab6f00373c45",
                    "name": "International Conference on Theory and Practice of Digital Libraries",
                    "type": "conference",
                    "alternate_names": [
                        "Theory Pract Digit Libr",
                        "International Conference Theory and Practice Digital Libraries",
                        "TPDL",
                        "Int Conf Theory Pract Digit Libr",
                        "Theory and Practice of Digital Libraries"
                    ],
                    "url": "http://www.tpdl.eu/"
                },
                "url": "https://www.semanticscholar.org/paper/c535e3da4ed7659266fdf23166127a5c2b604ec4",
                "title": "Layout Detection and Table Recognition - Recent Challenges in Digitizing Historical Documents and Handwritten Tabular Data",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1868530",
                        "name": "Constantin Lehenmeier"
                    },
                    {
                        "authorId": "2745991",
                        "name": "M. Burghardt"
                    },
                    {
                        "authorId": "1885326976",
                        "name": "Bernadette Mischka"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2e3c9a60d6b7519710e610fcce9fd1396b19e167",
                "externalIds": {
                    "MAG": "3080177284",
                    "DBLP": "conf/cdmake/AgombarLS20",
                    "DOI": "10.1007/978-3-030-57321-8_23",
                    "CorpusId": 221196341
                },
                "corpusId": 221196341,
                "publicationVenue": {
                    "id": "774cadb7-0e9c-4d06-be58-c9bc77655652",
                    "name": "International Cross-Domain Conference on Machine Learning and Knowledge Extraction",
                    "type": "conference",
                    "alternate_names": [
                        "CD-MAKE",
                        "Int Cross-domain Conf Mach Learn Knowl Extr"
                    ],
                    "url": "https://cd-make.net/"
                },
                "url": "https://www.semanticscholar.org/paper/2e3c9a60d6b7519710e610fcce9fd1396b19e167",
                "title": "A Clustering Backed Deep Learning Approach for Document Layout Analysis",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "102151480",
                        "name": "R. Agombar"
                    },
                    {
                        "authorId": "1396407321",
                        "name": "Max L\u00fcbbering"
                    },
                    {
                        "authorId": "2018549",
                        "name": "R. Sifa"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c5a5e783d752f3df2dd920b09e47826188b9924f",
                "externalIds": {
                    "DBLP": "conf/ercimdl/2020",
                    "MAG": "3090550431",
                    "DOI": "10.1007/978-3-030-54956-5",
                    "CorpusId": 221137410
                },
                "corpusId": 221137410,
                "publicationVenue": {
                    "id": "14508953-a0fc-42e5-9a54-ab6f00373c45",
                    "name": "International Conference on Theory and Practice of Digital Libraries",
                    "type": "conference",
                    "alternate_names": [
                        "Theory Pract Digit Libr",
                        "International Conference Theory and Practice Digital Libraries",
                        "TPDL",
                        "Int Conf Theory Pract Digit Libr",
                        "Theory and Practice of Digital Libraries"
                    ],
                    "url": "http://www.tpdl.eu/"
                },
                "url": "https://www.semanticscholar.org/paper/c5a5e783d752f3df2dd920b09e47826188b9924f",
                "title": "Digital Libraries for Open Knowledge: 24th International Conference on Theory and Practice of Digital Libraries, TPDL 2020, Lyon, France, August 25\u201327, 2020, Proceedings",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2114040164",
                        "name": "Mark Hall"
                    },
                    {
                        "authorId": "2326695",
                        "name": "Tanja Mercun"
                    },
                    {
                        "authorId": "25407337",
                        "name": "T. Risse"
                    },
                    {
                        "authorId": "1757970",
                        "name": "F. Duchateau"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fea7fe700da8864f103811c5b57d46bff8b43472",
                "externalIds": {
                    "DBLP": "journals/sensors/AraujoCASSM20",
                    "PubMedCentral": "7472071",
                    "MAG": "3047833812",
                    "DOI": "10.3390/s20164370",
                    "CorpusId": 221074513,
                    "PubMed": "32764352"
                },
                "corpusId": 221074513,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fea7fe700da8864f103811c5b57d46bff8b43472",
                "title": "A Real-World Approach on the Problem of Chart Recognition Using Classification, Detection and Perspective Correction",
                "abstract": "Data charts are widely used in our daily lives, being present in regular media, such as newspapers, magazines, web pages, books, and many others. In general, a well-constructed data chart leads to an intuitive understanding of its underlying data. In the same way, when data charts have wrong design choices, a redesign of these representations might be needed. However, in most cases, these charts are shown as a static image, which means that the original data are not usually available. Therefore, automatic methods could be applied to extract the underlying data from the chart images to allow these changes. The task of recognizing charts and extracting data from them is complex, largely due to the variety of chart types and their visual characteristics. Other features in real-world images that can make this task difficult are photo distortions, noise, alignment, etc. Two computer vision techniques that can assist this task and have been little explored in this context are perspective detection and correction. These methods transform a distorted and noisy chart in a clear chart, with its type ready for data extraction or other uses. This paper proposes a classification, detection, and perspective correction process that is suitable for real-world usage, when considering the data used for training a state-of-the-art model for the extraction of a chart in real-world photography. The results showed that, with slight changes, chart recognition methods are now ready for real-world charts, when taking time and accuracy into consideration.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3452561",
                        "name": "Tiago Ara\u00fajo"
                    },
                    {
                        "authorId": "39477126",
                        "name": "Paulo Chagas"
                    },
                    {
                        "authorId": "144468028",
                        "name": "J. Alves"
                    },
                    {
                        "authorId": "3452653",
                        "name": "Carlos G. R. Santos"
                    },
                    {
                        "authorId": "2881766",
                        "name": "B. Santos"
                    },
                    {
                        "authorId": "1763558",
                        "name": "B. Meiguins"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[22], Siddiqui et al.",
                "State of the art algorithms (such as [7, 13, 14, 21, 22, 24, 26]) for graphical object detection are motivated by the success of object detection in computer vision (such as faster r-cnn and mask r-cnn).",
                "Recent advances in object detection in natural scene images using deep learning inspire researchers [7, 13, 14, 16, 18, 21, 22, 24, 26] to develop deep learning based algorithms for detecting graphical objects in documents.",
                "a lot of attention in the research community [7, 10, 13\u201316, 18, 21, 22, 24, 26, 27]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4c8614f714f2b4daae86e8e1ccf8c8ccd7b3b608",
                "externalIds": {
                    "DBLP": "conf/das/MondalLJ20",
                    "MAG": "3047271086",
                    "ArXiv": "2008.02569",
                    "DOI": "10.1007/978-3-030-57058-3_16",
                    "CorpusId": 221006010
                },
                "corpusId": 221006010,
                "publicationVenue": {
                    "id": "02d53b80-30d7-493c-9453-ed7406056b31",
                    "name": "International Workshop on Document Analysis Systems",
                    "type": "conference",
                    "alternate_names": [
                        "DAS",
                        "Document Analysis Systems",
                        "Int Workshop Doc Anal Syst",
                        "Doc Anal Syst"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=647"
                },
                "url": "https://www.semanticscholar.org/paper/4c8614f714f2b4daae86e8e1ccf8c8ccd7b3b608",
                "title": "IIIT-AR-13K: A New Dataset for Graphical Object Detection in Documents",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "47991809",
                        "name": "Peter Lipps"
                    },
                    {
                        "authorId": "1694502",
                        "name": "C. V. Jawahar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There have been many deep learning models that have been successful at tabular structure detection [4] [29] ."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b284b8fdd357902dede307b774e43d7d6a1f71ad",
                "externalIds": {
                    "DBLP": "conf/iscc/JiangSKK20",
                    "MAG": "3094550052",
                    "DOI": "10.1109/ISCC50000.2020.9219583",
                    "CorpusId": 222418965
                },
                "corpusId": 222418965,
                "publicationVenue": {
                    "id": "159aed30-148b-4b50-99a4-372b0af958d9",
                    "name": "International Symposium on Computers and Communications",
                    "type": "conference",
                    "alternate_names": [
                        "ISCC",
                        "Int Symp Comput Commun"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000156/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/b284b8fdd357902dede307b774e43d7d6a1f71ad",
                "title": "High Precision Deep Learning-Based Tabular Position Detection",
                "abstract": "Documents are constantly being processed within supply chains in various industries throughout the globe. Within those documents, often times the most important content is stored in tabular format. Therefore an automated technique for supply chain document processing is highly desired. Deep learning approaches show promise to deliver an end-to-end extraction model. However, it has been shown that tabular detection accuracy is not always correlated to tabular localization accuracy. Portions of the desired tabular information can easily be cropped out due to a lack of localization accuracy. In this paper, we propose a two stage convolutional neural network-based deep learning framework to improve tabular localization accuracy. We use pre-trained backbone network ResNet-50 and then apply transfer learning to fit our application. One of our main contributions is the introduction of the KL loss function into Faster-RCNN. Once the bounding box variances are acquired from the KL loss function, we introduce a voting procedure with soft-non-maximum suppression (Soft-NMS) to improve localization performance. The proposed framework is trained and evaluated on public and private datasets that span from scientific documents to various electronic components. Our test results show that the precision of tabular detection can be improved by 1.2% while achieving the same recall as other state-of-the-art models on the public ICDAR2013 dataset. Furthermore, a large improvement in precision has been achieved at extremely high intersection over union (IoU) thresholds (i.e. 95%). Thus, 10.9% higher precision is achieved at 95% IoU for ICDAR2013. For another public dataset, namely ICDAR2017, 8.4% higher precision is achieved at 95% IoU .",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1995868863",
                        "name": "JiChu Jiang"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "152235107",
                        "name": "Shahzad Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Schreiber et al.11 presented a system that is completely data driven and not require heuristics or meta data to detect and recognize table structures on the ICDAR 2013 dataset.",
                "The comparison of performance between open source F. Shafait et al.5 technique (Tesseract),Schreiber et al. Hao et al.10 , Gilani et al.12 and our method is shown in Table 2.",
                "Performance Measures Accuracy (%) Tesseract(5) Schreiber et al.(11) Gilani et al.",
                "Schreiber et al.(11) presented a system that is completely data driven and not require heuristics or meta data to detect and recognize table structures on the ICDAR 2013 dataset."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "20f1ddf4402588d1f1f9d1878f8720e322f08031",
                "externalIds": {
                    "MAG": "3034220757",
                    "DOI": "10.1117/12.2573174",
                    "CorpusId": 219842475
                },
                "corpusId": 219842475,
                "publicationVenue": {
                    "id": "f20ccd07-a0af-4caf-94fb-1f2273acff59",
                    "name": "International Conference on Digital Image Processing",
                    "type": "conference",
                    "alternate_names": [
                        "DIP",
                        "Int Conf Digit Image Process",
                        "ICDIP"
                    ],
                    "url": "http://icdip.org/"
                },
                "url": "https://www.semanticscholar.org/paper/20f1ddf4402588d1f1f9d1878f8720e322f08031",
                "title": "Table detection method based on feature pyramid network with faster R-CNN",
                "abstract": "Table detection is a crucial step in many document analysis applications as tables are used for presenting essential information to the reader in a structured manner. It is a hard problem due to varying layouts and encodings of the tables. This paper uses Faster R-CNN with the feature pyramid structure as the main network structure to detect the table.It presents a deep learning-based solution for table detection in document images. In order to adapt to different shapes of tables, we classify the tables and preprocess the text with Run Smooth Length algorithm and open source OCR tools. In contrast to most existed table detection methods that only applicable to PDFs, our method can detect document images, which also applies to PDF (because PDF format can be automatically converted to pictures). To evaluate the effectiveness of our method, we tested on ICDAR, UNLV and TableBank datasets, and achieves F1-measure of 92.59% in UNLV, which is higher than some previous methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46399019",
                        "name": "Yawen Liu"
                    },
                    {
                        "authorId": "1752807724",
                        "name": "Yinghui Jin"
                    },
                    {
                        "authorId": "1752765264",
                        "name": "Chenchao Huang"
                    },
                    {
                        "authorId": "98346347",
                        "name": "W. Bao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b4d86f3aaf96467a704305253057cce5fd3375fe",
                "externalIds": {
                    "MAG": "3034882695",
                    "DBLP": "journals/ijgi/ZhangZXZTWL20",
                    "DOI": "10.3390/ijgi9060389",
                    "CorpusId": 220838553
                },
                "corpusId": 220838553,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b4d86f3aaf96467a704305253057cce5fd3375fe",
                "title": "Urban Geological 3D Modeling Based on Papery Borehole Log",
                "abstract": "Borehole log is important data for urban geological 3D modeling. Most of the current borehole logs are stored in a papery form. The construction of a smart city puts forward requirements for the automatic and intelligent 3D modeling of urban geology. However, it is difficult to extract the information from the papery borehole log quickly. What is more, it is unreliable to rely entirely on automated algorithms for modeling without artificial participation, but there is no effective way to integrate geological knowledge into 3D geological modeling currently. Therefore, it is necessary to research how to use existing papery borehole logs efficiently. To overcome the above obstacles, we designed a method that combines structural analysis and layout understanding to extract information from the borehole log. Then, the knowledge-driven three-dimensional geological modeling is proposed based on dynamic profiles. With these methods, the papery borehole log can be converted into structured data which can be used for data analysis directly, and geological knowledge can be integrated into the process of 3D geological modeling. The 3D geological modeling of Xinyang City based on a papery borehole log has been taken as an example to verify the feasibility of the method.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2141829485",
                        "name": "Xinyu Zhang"
                    },
                    {
                        "authorId": "2153914866",
                        "name": "Yi Zhang"
                    },
                    {
                        "authorId": "12902030",
                        "name": "Lirui Xu"
                    },
                    {
                        "authorId": "2118076280",
                        "name": "Junqiang Zhang"
                    },
                    {
                        "authorId": "48391589",
                        "name": "Yiping Tian"
                    },
                    {
                        "authorId": "2117075613",
                        "name": "Song Wang"
                    },
                    {
                        "authorId": "2116773000",
                        "name": "Zhilong Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One particular area where automation is of utmost importance and desirable is digitization of scanned documents like bank application forms, receipts and insurance claim forms [22, 20, 16, 14] to facilitate easy and quick retrieval and management of information for faster processing."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "94272993c8067f9da972dee1a785d773d1e3514a",
                "externalIds": {
                    "DBLP": "conf/cvpr/GoelSV20",
                    "MAG": "3035326541",
                    "DOI": "10.1109/CVPRW50498.2020.00286",
                    "CorpusId": 220267374
                },
                "corpusId": 220267374,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/94272993c8067f9da972dee1a785d773d1e3514a",
                "title": "Font-ProtoNet: Prototypical Network based Font Identification of Document Images in Low Data Regime",
                "abstract": "While optical character recognition has attracted considerable interest from researchers in recent times, automating font identification in printed/scanned documents is still not a well explored problem. With the increasing variety of fonts in the open community, identifying the different fonts used in a given document image can often provide important visual cues for document understanding. Font identification is a challenging task owing to smaller inter-class variations and limited availability of labeled image data for a large variety of font images. In the absence of the original true type format (ttf) files, even synthetic data generation is not possible. To this end, we propose to utilize recent few-shot learning techniques like prototypical networks for font identification in scanned / printed document images using character images from different fonts as input for scarce data scenarios and call the proposed method Font-ProtoNet. This approach uses an initial set of classes to learn an embedding and centroid representations (as class prototypes), that are used to classify novel samples based on euclidean distance. We demonstrate that Font-ProtoNet gives encouraging results by training prototypical networks in few-shot learning settings on a synthetic dataset of 200 font classes and using the trained network to identify fonts on a synthetic dataset of 100 novel font classes. We have also tested our approach on the real-world Adobe Visual Font Recognition (Adobe VFR) dataset and obtained 59.86% and 71.01% word-level accuracy of font identification using 1-shot and 5-shot i.e., 1 and 5 character images per font class, respectively.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47867325",
                        "name": "Nikita Goel"
                    },
                    {
                        "authorId": "145092223",
                        "name": "Monika Sharma"
                    },
                    {
                        "authorId": "3213990",
                        "name": "L. Vig"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For converting the image into a binary image adaptive thresholding algorithm can be used and for removing the noise, algorithms such as KFILL algorithms can be utilized [24]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2e8cca0dda1f595d053e38ba718bbb3f11645ed5",
                "externalIds": {
                    "MAG": "3040920819",
                    "DOI": "10.1109/ICCES48766.2020.9137859",
                    "CorpusId": 220568806
                },
                "corpusId": 220568806,
                "publicationVenue": {
                    "id": "4a0a181c-3807-4883-8ac2-b0ddf5f46497",
                    "name": "International Conference on Communication and Electronics Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Commun Electron Syst",
                        "ICCES",
                        "Int Conf Comput Eng Syst",
                        "International Conference on Computer Engineering and Systems"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2e8cca0dda1f595d053e38ba718bbb3f11645ed5",
                "title": "Layout and Text Extraction from Document Images using Neural Networks",
                "abstract": "This paper attempts to provide a new perspective for efficient text extraction techniques by including the layout information of the document images along with the text. The study proposes a novel approach of segmenting the image into smaller images based on its meta-data knowledge and then applying functions for recognition of text from the smaller images. Due to a lack of layout information of the text, poor results are generated during text searching by office automation tools. With this restriction usage of the text for various environments becomes limited and usage of extracted text may not be done effectively. The study proposes a technique to understand the structural and functional layout of the document image and using this knowledge to develop a better model. To verify the point of view, additional intelligence is attached to the data, making it capable to be used in varied environments. With this added quality, the new proposed system can extract and identify text or group of text into different entities within the document which the previous systems could not achieve. The proposal can be beneficial particularly for the development of various document processing tools.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1491240430",
                        "name": "M. Gorai"
                    },
                    {
                        "authorId": "2321111",
                        "name": "M. Nene"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Table Detection and Tabular Structure Identification: Often real world invoices and other documents contain data in tables with highly variable structure, and while deep learning has made important strides in table detection [12, 16], extraction techniques that generalize well across unseen table formats remains a challenge."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5ce22a1dd9fa76a9297ad94665aae0b425c8a128",
                "externalIds": {
                    "MAG": "3035158030",
                    "DBLP": "conf/cvpr/RastogiARVAS020",
                    "DOI": "10.1109/CVPRW50498.2020.00287",
                    "CorpusId": 220892137
                },
                "corpusId": 220892137,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5ce22a1dd9fa76a9297ad94665aae0b425c8a128",
                "title": "Information Extraction from Document Images via FCA based Template Detection and Knowledge Graph Rule Induction",
                "abstract": "We view information extraction from document images as a complex problem that requires a combination of 1) state of the art deep learning vision models for detection of entities and primitive relations, 2) symbolic background knowledge that expresses prior information of spatial and semantic relationships, using the entities and primitive relations from the neural detectors, and 3) learning of symbolic extraction rules using one, or few examples of annotated document images. Several challenges arise in ensuring that this neuro-symbolic software stack works together seamlessly. These include vision-based challenges to ensure that the documents are \"seen\" at the appropriate level of detail to detect entities; symbolic representation challenges in identifying primitive relations between the entities identified by the vision system; learning-based challenges of identifying the appropriate level of symbolic abstraction for the retrieval rules, the need to identify background knowledge that is relevant to the documents being analyzed, and learning general symbolic rules in data-deficient domains. In this paper, we describe how we meet some of these challenges in the design of our document-reading platform. In particular, we focus on use cases with multiple templates which additionally involves finding structurally similar images in large heterogeneous document image collections. An adaptive lattice based template allocation module was utilized for evaluating document similarity based on both textual content and document structure. A knowledge graph is used for capturing document structure and a relational rule learning system is employed on the knowledge graph for generating extraction rules. Experiments on a publicly shared data-set1 of 1400 trade finance documents demon strates the viability of the proposed system.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1575117755",
                        "name": "M. Rastogi"
                    },
                    {
                        "authorId": "1845870479",
                        "name": "Syed Afshan Ali"
                    },
                    {
                        "authorId": "46486363",
                        "name": "Mrinal Rawat"
                    },
                    {
                        "authorId": "3213990",
                        "name": "L. Vig"
                    },
                    {
                        "authorId": "145968137",
                        "name": "P. Agarwal"
                    },
                    {
                        "authorId": "143725466",
                        "name": "Gautam M. Shroff"
                    },
                    {
                        "authorId": "143780768",
                        "name": "A. Srinivasan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most existing work on object detection-based methods detect entire rows and column separately, and represents the intersection of detected rows and columns as cells [26, 33].",
                "Deep learning approaches include two categories: (a) End-to-end image-to-sequence models [18, 36]; (b) Object detection based methods [26, 33, 23].",
                "However, few vision-based deep learning models for table extraction that have been proposed, with most existing deep learning approaches directly use off-the-shelf object detectors [24, 26, 18] without any major architectural adaptation.",
                "Also, [23, 26] used different train/test split from the original competition without publishing their split and so cannot be compared directly.",
                "used to detect tables by adapting state-of-the-art object detectors such as Faster-RCNN to table detection [26, 18, 6]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "73a906a988e54defee536a120125f957059d595e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-00589",
                    "MAG": "3022479961",
                    "ArXiv": "2005.00589",
                    "DOI": "10.1109/WACV48630.2021.00074",
                    "CorpusId": 218487305
                },
                "corpusId": 218487305,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/73a906a988e54defee536a120125f957059d595e",
                "title": "Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context",
                "abstract": "Documents are often used for knowledge sharing and preservation in business and science, within which are tables that capture most of the critical data. Unfortunately, most documents are stored and distributed as PDF or scanned images, which fail to preserve logical table structure. Recent vision-based deep learning approaches have been proposed to address this gap, but most still cannot achieve state-of-the-art results. We present Global Table Extractor (GTE), a vision-guided systematic framework for joint table detection and cell structured recognition, which could be built on top of any object detection model. With GTE-Table, we invent a new penalty based on the natural cell containment constraint of tables to train our table network aided by cell location predictions. GTE-Cell is a new hierarchical cell detection network that leverages table styles. Further, we design a method to automatically label table and cell structure in existing documents to cheaply create a large corpus of training and test data. We use this to enhance PubTabNet with cell labels and create FinTabNet, real-world and complex scientific and financial datasets with detailed table structure annotations to help train and test structure recognition. Our framework surpasses previous state-of-the-art results on the ICDAR 2013 and ICDAR 2019 table competition in both table detection and cell structure recognition. Further experiments demonstrate a greater than 45% improvement in cell structure recognition when compared to a vanilla RetinaNet object detection model in our new out-of-domain FinTabNet.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1662767314",
                        "name": "Xinyi Zheng"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    },
                    {
                        "authorId": "145378077",
                        "name": "Lucian Popa"
                    },
                    {
                        "authorId": "31856827",
                        "name": "N. Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[19] adapt Faster R-CNN [17] and FCN to identify their structures and parse the contents."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ffe0400b1d86bc4b52cce28c0000b0dd02f2e75b",
                "externalIds": {
                    "DBLP": "conf/cvpr/ShenZD20",
                    "ArXiv": "2004.08686",
                    "MAG": "3035011051",
                    "DOI": "10.1109/CVPRW50498.2020.00282",
                    "CorpusId": 215828408
                },
                "corpusId": 215828408,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ffe0400b1d86bc4b52cce28c0000b0dd02f2e75b",
                "title": "A Large Dataset of Historical Japanese Documents with Complex Layouts",
                "abstract": "Deep learning-based approaches for automatic document layout analysis and content extraction have the potential to unlock rich information trapped in historical documents on a large scale. One major hurdle is the lack of large datasets for training robust models. In particular, little training data exist for Asian languages. To this end, we present HJDataset, a Large Dataset of Historical Japanese Documents with Complex Layouts. It contains over 250,000 layout element annotations of seven types. In addition to bounding boxes and masks of the content regions, it also includes the hierarchical structures and reading orders for layout elements. The dataset is constructed using a combination of human and machine efforts. A semi-rule based method is developed to extract the layout elements, and the results are checked by human inspectors. The resulting large-scale dataset is used to provide baseline performance analyses for text region detection using state-of-the-art deep learning models. And we demonstrate the usefulness of the dataset on real-world document digitization tasks. The dataset is available at https://dell-research-harvard.github.io/HJDataset/.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "101568984",
                        "name": "Zejiang Shen"
                    },
                    {
                        "authorId": "2158519493",
                        "name": "Kaixuan Zhang"
                    },
                    {
                        "authorId": "2065276972",
                        "name": "Melissa Dell"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9f741560245a80eef4bd576d82afa80a6c2036f2",
                "externalIds": {
                    "DBLP": "conf/aicv2/AwadEMR20",
                    "MAG": "3013600811",
                    "DOI": "10.1007/978-3-030-44289-7_42",
                    "CorpusId": 214642305
                },
                "corpusId": 214642305,
                "publicationVenue": {
                    "id": "de63160a-b4d0-4d36-bcdd-3a2e09ca7b48",
                    "name": "International Conferences on Artificial Intelligence and Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "AICV",
                        "Int Conf Artif Intell Comput Vis"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9f741560245a80eef4bd576d82afa80a6c2036f2",
                "title": "Metadata Extraction for Low-Quality Semi-structured Spreadsheets",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1591280305",
                        "name": "A. Awad"
                    },
                    {
                        "authorId": "2106229",
                        "name": "R. Elgohary"
                    },
                    {
                        "authorId": "1741003",
                        "name": "I. Moawad"
                    },
                    {
                        "authorId": "2288960",
                        "name": "M. Roushdy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We only use 40 images from the dataset for fine-tuning the general model and 198 images for testing, while [18] and [21] used only 34 images for testing and rest of the dataset for training.",
                "As done by DeepDeSRT [21], to achieve the best possible results, we removed the errors in the ground-truth annotations of the dataset.",
                "[21] were the first to perform table detection and structure recognition together with a 2 fold system which Faster RCNN for table detection and, Subsequently, deep learning-based semantic segmentation for table structure recognition."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "25018e89fc6edb26cf4e759c6c97859f862da753",
                "externalIds": {
                    "MAG": "3034997246",
                    "DBLP": "journals/corr/abs-2004-12629",
                    "ArXiv": "2004.12629",
                    "DOI": "10.1109/cvprw50498.2020.00294",
                    "CorpusId": 216553259
                },
                "corpusId": 216553259,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/25018e89fc6edb26cf4e759c6c97859f862da753",
                "title": "CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents",
                "abstract": "An automatic table recognition method for interpretation of tabular data in document images majorly involves solving two problems of table detection and table structure recognition. The prior work involved solving both problems independently using two separate approaches. More recent works signify the use of deep learning-based solutions while also attempting to design an end to end solution. In this paper, we present an improved deep learning-based end to end approach for solving both problems of table detection and structure recognition using a single Convolution Neural Network (CNN) model. We propose CascadeTabNet: a Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet) based model that detects the regions of tables and recognizes the structural body cells from the detected tables at the same time. We evaluate our results on ICDAR 2013, ICDAR 2019 and Table-Bank public datasets. We achieved 3rd rank in ICDAR 2019 post-competition results for table detection while attaining the best accuracy results for the ICDAR 2013 and Table-Bank dataset. We also attain the highest accuracy results on the ICDAR 2019 table structure recognition dataset. Additionally, we demonstrate effective transfer learning and image augmentation techniques that enable CNNs to achieve very accurate table detection results. Code and dataset has been made available at: https://github.com/DevashishPrasad/CascadeTabNet",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1657433059",
                        "name": "D. Prasad"
                    },
                    {
                        "authorId": "1659640979",
                        "name": "Ayan Gadpal"
                    },
                    {
                        "authorId": "1659640726",
                        "name": "Kshitij Kapadni"
                    },
                    {
                        "authorId": "1659635627",
                        "name": "Manish Visave"
                    },
                    {
                        "authorId": "2098485762",
                        "name": "Kavita A. Sultanpure"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ef9f04f52e1e432b0b9b4f8dc9c9ffd0289651f0",
                "externalIds": {
                    "DBLP": "journals/eaai/KaraTSKK20",
                    "MAG": "3005638477",
                    "DOI": "10.1016/j.engappai.2020.103551",
                    "CorpusId": 212845616
                },
                "corpusId": 212845616,
                "publicationVenue": {
                    "id": "1a24ea21-4c37-41d8-9e76-ab802d4afb3e",
                    "name": "Engineering applications of artificial intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Eng appl artif intell",
                        "Eng Appl Artif Intell",
                        "Engineering Applications of Artificial Intelligence"
                    ],
                    "issn": "0952-1976",
                    "url": "http://www.sciencedirect.com/science/journal/09521976"
                },
                "url": "https://www.semanticscholar.org/paper/ef9f04f52e1e432b0b9b4f8dc9c9ffd0289651f0",
                "title": "Holistic design for deep learning-based discovery of tabular structures in datasheet images",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1491166537",
                        "name": "Ertugrul Kara"
                    },
                    {
                        "authorId": "2065915247",
                        "name": "Mark Traquair"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "152235107",
                        "name": "Shahzad Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Figure 1: Table Detection Result on ICDAR 2013 dataset [3]",
                "1) Faster Recurrent Conventional Neural Network (FRCNN) [2] [3] algorithm to detect the table.",
                "[3], proposed a deep learning-based approach to detect table and structure recognition of table in Document or Image by using the ICDAR 2013 table completion dataset which containing 67 documents with 238 pages."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ef0f0cef3ed37eb6364f351f3bb4c5b6b8ce6247",
                "externalIds": {
                    "MAG": "3023455652",
                    "CorpusId": 219110768
                },
                "corpusId": 219110768,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ef0f0cef3ed37eb6364f351f3bb4c5b6b8ce6247",
                "title": "Survey on Table Detection from Document Image",
                "abstract": "There are many types of invoice having table exist in the current system such as table in native text invoices, table in image invoices (II), table in handwritten invoices (HI) and so on. Nowadays, these different types of invoices are processing manually. Now our aim to survey such system which can handle invoices having the table automatically by using OCR (Optical Character Reader) and Deep Learning Technologies. Moreover, we will also discussed multiple technologies and suggest the best model as per our survey",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2116970933",
                        "name": "Shashank Jain"
                    },
                    {
                        "authorId": "47264639",
                        "name": "Ashutosh Kumar Singh"
                    },
                    {
                        "authorId": "2115290352",
                        "name": "Rahul Singh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e45f77fa5e4169aca566100f287bda14b1ab4844",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-13197",
                    "MAG": "3013544976",
                    "ArXiv": "2003.13197",
                    "DOI": "10.1109/CVPR42600.2020.01293",
                    "CorpusId": 214714458
                },
                "corpusId": 214714458,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e45f77fa5e4169aca566100f287bda14b1ab4844",
                "title": "Cross-Domain Document Object Detection: Benchmark Suite and Method",
                "abstract": "Decomposing images of document pages into high-level semantic regions (e.g., figures, tables, paragraphs), document object detection (DOD) is fundamental for downstream tasks like intelligent document editing and understanding. DOD remains a challenging problem as document objects vary significantly in layout, size, aspect ratio, texture, etc. An additional challenge arises in practice because large labeled training datasets are only available for domains that differ from the target domain. We investigate cross-domain DOD, where the goal is to learn a detector for the target domain using labeled data from the source domain and only unlabeled data from the target domain. Documents from the two domains may vary significantly in layout, language, and genre. We establish a benchmark suite consisting of different types of PDF document datasets that can be utilized for cross-domain DOD model training and evaluation. For each dataset, we provide the page images, bounding box annotations, PDF files, and the rendering layers extracted from the PDF files. Moreover, we propose a novel cross-domain DOD model which builds upon the standard detection model and addresses domain shifts by incorporating three novel alignment modules: Feature Pyramid Alignment (FPA) module, Region Alignment (RA) module and Rendering Layer alignment (RLA) module. Extensive experiments on the benchmark suite substantiate the efficacy of the three proposed modules and the proposed method significantly outperforms the baseline methods. The project page is at \\url{https://github.com/kailigo/cddod}.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "94451829",
                        "name": "K. Li"
                    },
                    {
                        "authorId": "26360698",
                        "name": "Curtis Wigington"
                    },
                    {
                        "authorId": "67319819",
                        "name": "Chris Tensmeyer"
                    },
                    {
                        "authorId": "7574699",
                        "name": "Handong Zhao"
                    },
                    {
                        "authorId": "1598478975",
                        "name": "Nikolaos Barmpalios"
                    },
                    {
                        "authorId": "2852035",
                        "name": "Vlad I. Morariu"
                    },
                    {
                        "authorId": "1977256",
                        "name": "Varun Manjunatha"
                    },
                    {
                        "authorId": "1500530510",
                        "name": "Tong Sun"
                    },
                    {
                        "authorId": "46956675",
                        "name": "Y. Fu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0ba2e51f90ea3c4deb5eda01538de8680b3600da",
                "externalIds": {
                    "DBLP": "journals/pami/EberleBKMVM22",
                    "ArXiv": "2003.05431",
                    "MAG": "3011005294",
                    "DOI": "10.1109/TPAMI.2020.3020738",
                    "CorpusId": 212657498,
                    "PubMed": "32870784"
                },
                "corpusId": 212657498,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0ba2e51f90ea3c4deb5eda01538de8680b3600da",
                "title": "Building and Interpreting Deep Similarity Models",
                "abstract": "Many learning algorithms such as kernel machines, nearest neighbors, clustering, or anomaly detection, are based on distances or similarities. Before similarities are used for training an actual machine learning model, we would like to verify that they are bound to meaningful patterns in the data. In this paper, we propose to make similarities interpretable by augmenting them with an explanation. We develop BiLRP, a scalable and theoretically founded method to systematically decompose the output of an already trained deep similarity model on pairs of input features. Our method can be expressed as a composition of LRP explanations, which were shown in previous works to scale to highly nonlinear models. Through an extensive set of experiments, we demonstrate that BiLRP robustly explains complex similarity models, e.g., built on VGG-16 deep neural network features. Additionally, we apply our method to an open problem in digital humanities: detailed assessment of similarity between historical documents, such as astronomical tables. Here again, BiLRP provides insight and brings verifiability into a highly engineered and problem-specific similarity model.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1557932201",
                        "name": "Oliver Eberle"
                    },
                    {
                        "authorId": "101119294",
                        "name": "Jochen B\u00fcttner"
                    },
                    {
                        "authorId": "2876150",
                        "name": "Florian Kr\u00e4utli"
                    },
                    {
                        "authorId": "145034054",
                        "name": "K. M\u00fcller"
                    },
                    {
                        "authorId": "47347875",
                        "name": "M. Valleriani"
                    },
                    {
                        "authorId": "144535526",
                        "name": "G. Montavon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Method proposed in [1, 2] handles the unavailability of large training data by using transfer learning approach with very carefully tuned parameters [2] also uses data augmentation to handle annotated data problem.",
                "Although both are validated on totally different data sets, F1 score in [1] is 0.",
                "DL-based techniques are reasonably good in localization [19, 20], and taking inspiration from it [1, 9] have proposed the use of Faster-RCNN.",
                "In [1, 2], a framework for table detection and recognition is proposed.",
                "In [1, 2], efforts are made to apply DL-based methods for table localization and detection from image or scanned documents.",
                "The results of the method presented in [2] are validated on publicly available UNLV data set [23] and [1] on ICDAR 2013 data."
            ],
            "intents": [],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f2dadbe009633675c0e1330bee0a19c7a9ab8cca",
                "externalIds": {
                    "DBLP": "journals/sncs/GuravN20",
                    "MAG": "3011090271",
                    "DOI": "10.1007/s42979-020-0113-x",
                    "CorpusId": 215919894
                },
                "corpusId": 215919894,
                "publicationVenue": {
                    "id": "7a7dc89b-e1a6-44df-a496-46c330a87840",
                    "name": "SN Computer Science",
                    "type": "journal",
                    "alternate_names": [
                        "SN Comput Sci"
                    ],
                    "issn": "2661-8907",
                    "alternate_issns": [
                        "2662-995X"
                    ],
                    "url": "https://link.springer.com/journal/42979"
                },
                "url": "https://www.semanticscholar.org/paper/f2dadbe009633675c0e1330bee0a19c7a9ab8cca",
                "title": "Weakly Supervised Learning-based Table Detection",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "70568134",
                        "name": "Aniket Gurav"
                    },
                    {
                        "authorId": "2321111",
                        "name": "M. Nene"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The literature focuses on using NLP-based techniques for data that has a model, template-based techniques for data that does not have a meta-dictionary available and deep learning-based extraction techniques for sequence [26] and image [9] based analysis of the unstructured data.",
                "Thus, the problem of pattern or relationship extraction is extremely challenging due to the uneven distribution and high degree of variation between different structures [9]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ce3f6cfa5dc7b711b04d0fd280d2747d96a72656",
                "externalIds": {
                    "MAG": "3054225939",
                    "DOI": "10.1109/ESCI48226.2020.9167588",
                    "CorpusId": 221281703
                },
                "corpusId": 221281703,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ce3f6cfa5dc7b711b04d0fd280d2747d96a72656",
                "title": "A Review of Unstructured Data Analysis and Parsing Methods",
                "abstract": "Computer applications generate an enormous amount of data every day through their logs, system-generated files or other reports. This generated data depicts the state of the running system and contains abundant information that can be used for system diagnostics and monitoring. Network monitoring systems produce a wide variety of unstructured information, so there is a need for an automated way to extract the relevant data, which currently requires multitude of custom parsers. Developing and testing custom parsers can be time-consuming. Instead, data can be automatically processed and parsed into a machine-readable format, building a generic model for standard or vendor-specific data, and generating insights for analytics, anomaly detection, intrusion detection, node failures and various other applications. This paper reviews some existing approaches for unstructured data mining and parsing and discusses the challenges in information extraction, creation of knowledge bases and presents a generic framework for automatic parsing.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48326213",
                        "name": "Shubham Jain"
                    },
                    {
                        "authorId": "1904691817",
                        "name": "Amy de Buitl\u00e9ir"
                    },
                    {
                        "authorId": "144789260",
                        "name": "Enda Fallon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "After that, [21, 24, 29] also leveraged more advanced Faster R-CNN model [19] or Mask R-CNN model [9] to further improve the accuracy of document layout analysis."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3465c06c872d8c48d628c5fc2d484087719351b6",
                "externalIds": {
                    "MAG": "2997154779",
                    "ArXiv": "1912.13318",
                    "DBLP": "journals/corr/abs-1912-13318",
                    "DOI": "10.1145/3394486.3403172",
                    "CorpusId": 209515395
                },
                "corpusId": 209515395,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/3465c06c872d8c48d628c5fc2d484087719351b6",
                "title": "LayoutLM: Pre-training of Text and Layout for Document Image Understanding",
                "abstract": "Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "3032611",
                        "name": "Yiheng Xu"
                    },
                    {
                        "authorId": "123545597",
                        "name": "Minghao Li"
                    },
                    {
                        "authorId": "145500855",
                        "name": "Lei Cui"
                    },
                    {
                        "authorId": "3110003",
                        "name": "Shaohan Huang"
                    },
                    {
                        "authorId": "49807919",
                        "name": "Furu Wei"
                    },
                    {
                        "authorId": "92660691",
                        "name": "Ming Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Nowadays document image analysis is still a relevant and challenging problem in computer vision [1-4]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b1b30fe860f20bd659d0af1387ef50049c9f2e35",
                "externalIds": {
                    "MAG": "2997225729",
                    "ArXiv": "1912.11658",
                    "DBLP": "journals/corr/abs-1912-11658",
                    "DOI": "10.1145/3440084.3441192",
                    "CorpusId": 209500627
                },
                "corpusId": 209500627,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b1b30fe860f20bd659d0af1387ef50049c9f2e35",
                "title": "DDI-100: Dataset for Text Detection and Recognition",
                "abstract": "With the increasing popularity of document analysis and recognition systems, text detection (TD) and optical character recognition (OCR) in document images become challenging tasks. However, according to our best knowledge, no publicly available datasets for these particular problems exist. In this paper, we introduce a Distorted Document Images dataset (DDI-100) and provide a detailed analysis of the DDI-100 in its current state. To create the dataset we collected 7000 unique document pages, and extended it by applying different types of distortions and geometric transformations. In total, DDI-100 contains more than 100,000 document images together with binary text masks, text and character locations in terms of bounding boxes. We conduct the experiments with several TD and OCR approaches trained on the introduced dataset. Obtained results demonstrate the usefulness of DDI-100 dataset to achieve high-quality results using a small amount of real data.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2086000125",
                        "name": "I. Zharikov"
                    },
                    {
                        "authorId": "1390027278",
                        "name": "Filipp Nikitin"
                    },
                    {
                        "authorId": "1471046445",
                        "name": "I. Vasiliev"
                    },
                    {
                        "authorId": "1390027214",
                        "name": "V. Dokholyan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Currently, deep learning techniques are the state of the art approach to deal with computer vision tasks [32]; and, this is also the case for table detection in document images [21, 23, 34].",
                "This is the approach followed in [11, 34, 36], where they use models trained on natural images to fine-tune their models for table detection.",
                "Namely, the main algorithm applied in this context is Faster R-CNN [31], that has been directly employed using different backbone architectures [21,23,34], and combined with deformable CNNs [36] or with image transformations [11]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "696ebea828c114fb94416fa0254d67f115d1afe4",
                "externalIds": {
                    "MAG": "2995818640",
                    "ArXiv": "1912.05846",
                    "DBLP": "conf/das/Casado-GarciaDH20",
                    "DOI": "10.1007/978-3-030-57058-3_15",
                    "CorpusId": 209324081
                },
                "corpusId": 209324081,
                "publicationVenue": {
                    "id": "02d53b80-30d7-493c-9453-ed7406056b31",
                    "name": "International Workshop on Document Analysis Systems",
                    "type": "conference",
                    "alternate_names": [
                        "DAS",
                        "Document Analysis Systems",
                        "Int Workshop Doc Anal Syst",
                        "Doc Anal Syst"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=647"
                },
                "url": "https://www.semanticscholar.org/paper/696ebea828c114fb94416fa0254d67f115d1afe4",
                "title": "The Benefits of Close-Domain Fine-Tuning for Table Detection in Document Images",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1473246264",
                        "name": "\u00c1. Casado-Garc\u00eda"
                    },
                    {
                        "authorId": "1691068",
                        "name": "C. Dom\u00ednguez"
                    },
                    {
                        "authorId": "1751150",
                        "name": "J\u00f3nathan Heras"
                    },
                    {
                        "authorId": "144279987",
                        "name": "Eloy J. Mata"
                    },
                    {
                        "authorId": "34564712",
                        "name": "Vico Pascual"
                    }
                ]
            }
        },
        {
            "contexts": [
                "With the introduction of deep-learning methods, major developments have been made for page objects segmentation and detection [4, 8, 9, 10].",
                "Significant efforts have been invested in the domain of table detection [11] and table structure detection [8].",
                "Faster-RCNN has been successfully used for page object detection from document images in the recent past [8, 11, 21], but mask-RCNN is used for the very first time to detect objects from document images to the best of the authors\u2019 knowledge."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "346c6c09f327d01b47a705a83735d9cc6a2631bb",
                "externalIds": {
                    "MAG": "2998227124",
                    "DBLP": "conf/dicta/YounasRMSLA19",
                    "DOI": "10.1109/DICTA47822.2019.8945972",
                    "CorpusId": 208244245
                },
                "corpusId": 208244245,
                "publicationVenue": {
                    "id": "375cb686-e96e-4b79-825c-1589c99aca1d",
                    "name": "International Conference on Digital Image Computing: Techniques and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Digit Image Comput Tech Appl",
                        "DICTA",
                        "Digital Image Computing: Techniques and Applications",
                        "Digit Image Comput Tech Appl"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=710"
                },
                "url": "https://www.semanticscholar.org/paper/346c6c09f327d01b47a705a83735d9cc6a2631bb",
                "title": "FFD: Figure and Formula Detection from Document Images",
                "abstract": "In this work, we present a novel and generic approach, Figure and Formula Detector (FFD) to detect the formulas and figures from document images. Our proposed method employs traditional computer vision approaches in addition to deep models. We transform input images by applying connected component analysis (CC), distance transform, and colour transform, which are stacked together to generate an input image for the network. The best results produced by FFD for figure and formula detection are with F1-score of 0.906 and 0.905, respectively. We also propose a new dataset for figures and formulas detection to aid future research in this direction. The obtained results advocate that enhancing the input representation can simplify the subsequent optimization problem resulting in significant gains over their conventional counterparts.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "7207683",
                        "name": "Junaid Younas"
                    },
                    {
                        "authorId": "40795206",
                        "name": "Syed Tahseen Raza Rizvi"
                    },
                    {
                        "authorId": "49012494",
                        "name": "M. I. Malik"
                    },
                    {
                        "authorId": "1688013",
                        "name": "F. Shafait"
                    },
                    {
                        "authorId": "143951813",
                        "name": "P. Lukowicz"
                    },
                    {
                        "authorId": "1734717217",
                        "name": "Sheraz Ahmed"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For instance, deep networks trained on ImageNet data, which comprises of natural images of various objects, have been shown to perform surprisingly well on various document analysis tasks such as document classification [11] and table detection [12].",
                "[12] as the baseline for our experiment as their approach provides the unprocessed image as an input to the Faster R-CNN for table detection.",
                "[12] presented a data-driven, deep learning approach for table detection."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c0455080da8967fb33059f24029121f5097faf2",
                "externalIds": {
                    "DBLP": "conf/dicta/ShahzadNAMS19",
                    "MAG": "2997854261",
                    "DOI": "10.1109/DICTA47822.2019.8945929",
                    "CorpusId": 209854321
                },
                "corpusId": 209854321,
                "publicationVenue": {
                    "id": "375cb686-e96e-4b79-825c-1589c99aca1d",
                    "name": "International Conference on Digital Image Computing: Techniques and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Digit Image Comput Tech Appl",
                        "DICTA",
                        "Digital Image Computing: Techniques and Applications",
                        "Digit Image Comput Tech Appl"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=710"
                },
                "url": "https://www.semanticscholar.org/paper/0c0455080da8967fb33059f24029121f5097faf2",
                "title": "Feature Engineering Meets Deep Learning: A Case Study on Table Detection in Documents",
                "abstract": "Traditional computer vision approaches heavily relied on hand-crafted features for tasks such as visual object detection and recognition. The recent success of deep learning in automatically extracting representative and powerful features from images has brought a paradigm shift in this area. As a side effect, decades of research into hand-crafted features is considered outdated. In this paper, we present an approach for table detection in which we leverage a deep learning based table detection model with hand-crafted features from a classical table detection method. We demonstrate that by using a suitable encoding of hand-crafted features, the deep learning model is able to perform better at the detection task. Experiments on publicly available UNLV dataset show that the presented method achieves an accuracy comparable with the state-of-the-art deep learning methods without the need of extensive hyper-parameter tuning.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2066563469",
                        "name": "M. Shahzad"
                    },
                    {
                        "authorId": "1476825178",
                        "name": "Rabeya Noor"
                    },
                    {
                        "authorId": "1476720265",
                        "name": "Sheraz Ahmad"
                    },
                    {
                        "authorId": "1747500",
                        "name": "A. Mian"
                    },
                    {
                        "authorId": "1688013",
                        "name": "F. Shafait"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", paragraphs, figures, lists, tables [48, 38, 30].",
                "While there are other works [33, 38] that perform table decomposition into rows and columns (which our model is capable of doing), we discuss table detection only in the scope of this paper."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6d738a42d3048171882c5c0a7b959060043ab657",
                "externalIds": {
                    "ArXiv": "1911.12170",
                    "DBLP": "journals/corr/abs-1911-12170",
                    "MAG": "2991098654",
                    "CorpusId": 208310264
                },
                "corpusId": 208310264,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6d738a42d3048171882c5c0a7b959060043ab657",
                "title": "Document Structure Extraction for Forms using Very High Resolution Semantic Segmentation",
                "abstract": "In this work, we look at the problem of structure extraction from document images with a specific focus on forms. Forms as a document class have not received much attention, even though they comprise a significant fraction of documents and enable several applications. Forms possess a rich, complex, hierarchical, and high-density semantic structure that poses several challenges to semantic segmentation methods. We propose a prior based deep CNN-RNN hierarchical network architecture that enables document structure extraction using very high resolution(1800 x 1000) images. We divide the document image into overlapping horizontal strips such that the network segments a strip and uses its prediction mask as prior while predicting the segmentation for the subsequent strip. We perform experiments establishing the effectiveness of our strip based network architecture through ablation methods and comparison with low-resolution variations. We introduce our new rich human-annotated forms dataset, and we show that our method significantly outperforms other segmentation baselines in extracting several hierarchical structures on this dataset. We also outperform other baselines in table detection task on the Marmot dataset. Our method is currently being used in a world-leading customer experience management software suite for automated conversion of paper and PDF forms to modern HTML based forms.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "39230373",
                        "name": "Mausoom Sarkar"
                    },
                    {
                        "authorId": "6657914",
                        "name": "Milan Aggarwal"
                    },
                    {
                        "authorId": "1432235684",
                        "name": "Arneh Jain"
                    },
                    {
                        "authorId": "1381295186",
                        "name": "Hiresh Gupta"
                    },
                    {
                        "authorId": "145846953",
                        "name": "Balaji Krishnamurthy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ion [23]\u2013[25]. In addition, deep neural networks for object detection, such as Faster-RCNN [26], Mask-RCNN [27], and YOLO [28] have been exploited for table detection and row/column segmentation [7], [29]\u2013[31]. Furthermore, graph neural networks are used for table detection and recognition by encoding document images as graphs [5], [32]. There are several tools (see Table II) that can convert tables i"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "99b2a05177c17dd86627863608ac45dc4cfe0446",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1911-10683",
                    "MAG": "2990963180",
                    "ArXiv": "1911.10683",
                    "DOI": "10.1007/978-3-030-58589-1_34",
                    "CorpusId": 208267858
                },
                "corpusId": 208267858,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/99b2a05177c17dd86627863608ac45dc4cfe0446",
                "title": "Image-based table recognition: data, model, and evaluation",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2113309578",
                        "name": "Xu Zhong"
                    },
                    {
                        "authorId": "9585722",
                        "name": "Elaheh Shafieibavani"
                    },
                    {
                        "authorId": "1399097376",
                        "name": "Antonio Jimeno-Yepes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Altogether, our weak supervision outperforms the state-of-the-art with image-based input (Schreiber et al., 2018) by a considerable margin.",
                "The work by Schreiber et al. (2018) draws upon a CNN to perform semantic segmentation of row and column pixels and identifies structure via postprocessing.",
                "The ICDAR labels now serve as the target dataset, while we continue to use arXivdocs-weak for weak supervision.7 Following (Schreiber et al., 2018), we use a random subset of 50 % of the ICDAR 2013 competition dataset for testing.",
                "On the other hand, efficient learning is prevented as large-scale training sets are lacking (cf. Arif and Shafait, 2018; Schreiber et al., 2018).",
                "Earlier attempts for structure parsing on documents focused on a subset of simpler tasks such as segmentation of text regions (Antonacopoulos et al., 2009), locating tables (Zanibbi et al., 2004; Embley et al., 2006), or detecting hierarchical structures in tables (Schreiber et al., 2018).",
                "Furthermore, (Schreiber et al., 2018) chooses the best system based on the test set as indicated by F1*.",
                "For instance, nested tables are fairly easy to recognize for human readers, yet detecting them is known to impose computational hurdles (cf. Schreiber et al., 2018).",
                "(Schreiber et al., 2018) uses a different, non-public 50 % random subset.",
                "7 Following (Schreiber et al., 2018), we use a random subset of 50 % of the ICDAR 2013 competition dataset for testing.",
                "The dataset comes without predefined train/test split; hence, we follow Schreiber et al. (2018) and split the so-called \u201ccompetition\u201d part of the dataset with a 50%/50%-ratio."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c49d3e214055e176c47b3358e77acbbea7d74ecc",
                "externalIds": {
                    "ArXiv": "1911.01702",
                    "MAG": "2983729490",
                    "DBLP": "journals/corr/abs-1911-01702",
                    "CorpusId": 207870462
                },
                "corpusId": 207870462,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c49d3e214055e176c47b3358e77acbbea7d74ecc",
                "title": "DocParser: Hierarchical Structure Parsing of Document Renderings",
                "abstract": "Translating document renderings (e.g. PDFs, scans) into hierarchical structures is extensively demanded in the daily routines of many real-world applications, and is often a prerequisite step of many downstream NLP tasks. Earlier attempts focused on different but simpler tasks such as the detection of table or cell locations within documents; however, a holistic, principled approach to inferring the complete hierarchical structure in documents is missing. As a remedy, we developed \"DocParser\": an end-to-end system for parsing the complete document structure - including all text elements, figures, tables, and table cell structures. To the best of our knowledge, DocParser is the first system that derives the full hierarchical document compositions. Given the complexity of the task, annotating appropriate datasets is costly. Therefore, our second contribution is to provide a dataset for evaluating hierarchical document structure parsing. Our third contribution is to propose a scalable learning framework for settings where domain-specific data is scarce, which we address by a novel approach to weak supervision. Our computational experiments confirm the effectiveness of our proposed weak supervision: Compared to the baseline without weak supervision, it improves the mean average precision for detecting document entities by 37.1%. When classifying hierarchical relations between entity pairs, it improves the F1 score by 27.6%.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "5185738",
                        "name": "Johannes Rausch"
                    },
                    {
                        "authorId": "7729843",
                        "name": "Octavio Mart\u00ednez"
                    },
                    {
                        "authorId": "1402912519",
                        "name": "Fabian Bissig"
                    },
                    {
                        "authorId": "1776014",
                        "name": "Ce Zhang"
                    },
                    {
                        "authorId": "3207649",
                        "name": "S. Feuerriegel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Others build models specifically to extract tables [6, 12, 22, 23, 14, 9], figures [15], and equations [11, 18, 26, 7]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "558adccd5dc482be3a17169ba1a921ea4b74101d",
                "externalIds": {
                    "MAG": "2981697272",
                    "ArXiv": "1910.12462",
                    "DBLP": "journals/corr/abs-1910-12462",
                    "CorpusId": 204904208
                },
                "corpusId": 204904208,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/558adccd5dc482be3a17169ba1a921ea4b74101d",
                "title": "Fine-Grained Object Detection over Scientific Document Images with Region Embeddings",
                "abstract": "We study the problem of object detection over scanned images of scientific documents. We consider images that contain objects of varying aspect ratios and sizes and range from coarse elements such as tables and figures to fine elements such as equations and section headers. We find that current object detectors fail to produce properly localized region proposals over such page objects. We revisit the original R-CNN model and present a method for generating fine-grained proposals over document elements. We also present a region embedding model that uses the convolutional maps of a proposal's neighbors as context to produce an embedding for each proposal. This region embedding is able to capture the semantic relationships between a target region and its surrounding context. Our end-to-end model produces an embedding for each proposal, then classifies each proposal by using a multi-head attention model that attends to the most important neighbors of a proposal. To evaluate our model, we collect and annotate a dataset of publications from heterogeneous journals. We show that our model, referred to as Attentive-RCNN, yields a 17% mAP improvement compared to standard object detection models.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "47581221",
                        "name": "Ankur Goswami"
                    },
                    {
                        "authorId": "2057591724",
                        "name": "Joshua McGrath"
                    },
                    {
                        "authorId": "35065399",
                        "name": "S. Peters"
                    },
                    {
                        "authorId": "145071799",
                        "name": "Theodoros Rekatsinas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Deep learning based semantic segmentation has been used by Schreiber et al. (2017) where an image of a document is fed to the neural network to identify the rows and columns of a table."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d995654560e05049b1d08dc612b4e22078b63c33",
                "externalIds": {
                    "ACL": "R19-1001",
                    "DBLP": "conf/ranlp/AdigaBSV19",
                    "MAG": "2989267215",
                    "DOI": "10.26615/978-954-452-056-4_001",
                    "CorpusId": 209061929
                },
                "corpusId": 209061929,
                "publicationVenue": {
                    "id": "3413b6f7-e718-4940-a26a-e208f732ada0",
                    "name": "Recent Advances in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "RANLP",
                        "Recent Adv Nat Lang Process"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d995654560e05049b1d08dc612b4e22078b63c33",
                "title": "Table Structure Recognition Based on Cell Relationship, a Bottom-Up Approach",
                "abstract": "In this paper, we present a relationship extraction based methodology for table structure recognition in PDF documents. The proposed deep learning-based method takes a bottom-up approach to table recognition in PDF documents. We outline the shortcomings of conventional approaches based on heuristics and machine learning-based top-down approaches. In this work, we explain how the task of table structure recognition can be modeled as a cell relationship extraction task and the importance of the bottom-up approach in recognizing the table cells. We use Multilayer Feedforward Neural Network for table structure recognition and compare the results of three feature sets. To gauge the performance of the proposed method, we prepared a training dataset using 250 tables in PDF documents, carefully selecting the table structures that are most commonly found in the documents. Our model achieves an overall accuracy of 97.95% and an F1-Score of 92.62% on the test dataset.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1453623511",
                        "name": "D. Adiga"
                    },
                    {
                        "authorId": "7184785",
                        "name": "S. Bhat"
                    },
                    {
                        "authorId": "73698413",
                        "name": "M. Shah"
                    },
                    {
                        "authorId": "1453627462",
                        "name": "Viveka Vyeth"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We can enhance the novel deep learning-based approach for table structure detection, DeepDeSRT [14], as proposed by Sebastian Schreiber et al. The PDF documents can be converted to image format.",
                "We can enhance the novel deep learning-based approach for table structure detection, DeepDeSRT [14], as proposed by Sebastian Schreiber et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7c7dedd527374f6ed3ca62cac005884875fc0d4f",
                "externalIds": {
                    "MAG": "2973075829",
                    "DOI": "10.18178/ijmlc.2019.9.5.844",
                    "CorpusId": 203147241
                },
                "corpusId": 203147241,
                "publicationVenue": {
                    "id": "04577424-d20d-4cc7-98f8-6c77afd87de8",
                    "name": "International Journal of Machine Learning and Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Mach Learn Comput"
                    ],
                    "issn": "2010-3700",
                    "url": "http://www.ijmlc.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7c7dedd527374f6ed3ca62cac005884875fc0d4f",
                "title": "Discovery of Structured Data Using Unsupervised Spatial Clustering and Human Supervision",
                "abstract": "Commercial data has been preserved digitally in portable document format (PDF) for its ease of encapsulating multiple data formats. In this digitization era, there comes a need to capture and store this data in structured format to facilitate its access for automated b2b services and business intelligence. In this paper, we propose a framework that automates discovery and extraction of tabular data incorporating both artificial and human intelligence. The framework involves clustering and heuristics to group cartesian location of text and spaces in a page to determine a table. The discovered table is then validated by the user using a user-interface designed to moderate the determined boundaries and fed back to the layout knowledge repository. The table data obtained is extracted as JSON key-value pairs which can then be loaded into any database. The framework thus provides enhanced accuracy and continuous human assisted learning for the automated document digitization process. The knowledge repository is further used to train the machine to generate document templates to be used for processing unseen documents. However, this paper concentrates on the discovery of structured data alone.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1397016956",
                        "name": "Nikitha Rachapudi"
                    },
                    {
                        "authorId": "1396408876",
                        "name": "Lakshmipathy Ganesh"
                    },
                    {
                        "authorId": "114486554",
                        "name": "Abinaya Sekar"
                    },
                    {
                        "authorId": null,
                        "name": "S AnandK"
                    },
                    {
                        "authorId": "2105413757",
                        "name": "Rajkumar Sakthibalan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, DeepDeSRT [8] was proposed which uses deep learning for both table detection and table structure recognition, i.",
                "In DeepDSert, separate models are made for Table detection and structure recognition, which were trained on different datasets such as Marmot for table detection, and the ICDAR 2013 table dataset for table structure recognition.",
                "While the results are not conclusively better than DeepDSert, they are certainly comparable and the fact that our model is end-to-end means further improvements can be made with richer semantic knowledge, and additional branches for learning row based segmentation.",
                "Recently, DeepDeSRT [8] was proposed which uses deep learning for both table detection and table structure recognition, i.e. identifying rows, columns, and cell positions in the detected tables.",
                "Average time taken for our system for each document image is 0.3765 seconds, however this could not be compared with DeepDSert as their model was not publicly available.",
                "Additionally, Experiment 3 was carried out to compare TableNet with the closest deep-learning based solution, DeepDSert [8].",
                "As done in DeepDSert, we also randomly chose 34 images for testing and used the rest of the data images for fine-tuning our model."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d7284721854bd9db96a9e442caef0609d4324415",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2001-01469",
                    "ArXiv": "2001.01469",
                    "MAG": "2996791521",
                    "DOI": "10.1109/ICDAR.2019.00029",
                    "CorpusId": 209862087
                },
                "corpusId": 209862087,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/d7284721854bd9db96a9e442caef0609d4324415",
                "title": "TableNet: Deep Learning Model for End-to-end Table Detection and Tabular Data Extraction from Scanned Document Images",
                "abstract": "With the widespread use of mobile phones and scanners to photograph and upload documents, the need for extracting the information trapped in unstructured document images such as retail receipts, insurance claim forms and financial invoices is becoming more acute. A major hurdle to this objective is that these images often contain information in the form of tables and extracting data from tabular sub-images presents a unique set of challenges. This includes accurate detection of the tabular region within an image, and subsequently detecting and extracting information from the rows and columns of the detected table. While some progress has been made in table detection, extracting the table contents is still a challenge since this involves more fine grained table structure(rows & columns) recognition. Prior approaches have attempted to solve the table detection and structure recognition problems independently using two separate models. In this paper, we propose TableNet: a novel end-to-end deep learning model for both table detection and structure recognition. The model exploits the interdependence between the twin tasks of table detection and table structure recognition to segment out the table and column regions. This is followed by semantic rule-based row extraction from the identified tabular sub-regions. The proposed model and extraction approach was evaluated on the publicly available ICDAR 2013 and Marmot Table datasets obtaining state of the art results. Additionally, we demonstrate that feeding additional semantic features further improves model performance and that the model exhibits transfer learning across datasets. Another contribution of this paper is to provide additional table structure annotations for the Marmot data, which currently only has annotations for table detection.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "46956825",
                        "name": "Shubham Paliwal"
                    },
                    {
                        "authorId": "144819655",
                        "name": "D. Vishwanath"
                    },
                    {
                        "authorId": "35888656",
                        "name": "R. Rahul"
                    },
                    {
                        "authorId": "145092223",
                        "name": "Monika Sharma"
                    },
                    {
                        "authorId": "3213990",
                        "name": "L. Vig"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We reimplemented the DeepDeSRT table structure model [6] and trained it on the same private data as our proposed model.",
                "We did so by reimplementing the DeepDeSRT model [6] and training on the same data as our proposed model.",
                "However, we were unable to obtain reasonable performance, even after exploring a variety of values for post processing thresholds and training hyperparameters (values not specified in [6]).",
                "The Deep DeSRT model [6] uses a neural network originally designed for semantic segmentation of natural scenes and thus primarily uses local information to classify pixels.",
                "The DeepDeSRT [6] learning model formulates table structure extraction as two independent pixel labeling tasks to respectively segment row objects and segment column objects.",
                "Recently, deep learning has been proposed to learn table structure directly from images [6].",
                "In [6], better results were obtained when resizing the initial input to make the separator regions bigger.",
                "We were unable to find any official implementations of prior works, so for comparison, we use the commercial software system Acrobat Pro DC and our reimplementation of the DeepDeSRT model [6].",
                "Category Method F-measure Recall Precision PDF Input Image Input\nProposed Models\nSplit 86.79 86.64 86.93 Split-PDF 91.63 91.26 92.00 SPLERGE-PDF 90.89 90.44 91.36 SPLERGE-PDF + Heuristics 91.95 91.46 92.45\nSplit + Heuristics 93.00 92.24 93.78 Split-PDF + Heuristics 95.26 94.64 95.89\nPreviously Reported Results\nNurminen [3] 94.60 94.09 95.12 TEXUS [4] 82.59 84.23 81.02 Shigarov [5] C1 91.50 91.21 91.80 Shigarov [5] C2 93.64 92.33 94.99 DeepDeSRT [6] 91.44\u2020 87.36\u2020 95.93\u2020\n\u2020 result not directly comparable due to evaluation on a random subset of 34 tables."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "658eef14952dc6dee87f5ef1c0f7313441aadf98",
                "externalIds": {
                    "DBLP": "conf/icdar/TensmeyerMPCM19",
                    "MAG": "3003931580",
                    "DOI": "10.1109/ICDAR.2019.00027",
                    "CorpusId": 211027046
                },
                "corpusId": 211027046,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/658eef14952dc6dee87f5ef1c0f7313441aadf98",
                "title": "Deep Splitting and Merging for Table Structure Decomposition",
                "abstract": "Given the large variety and complexity of tables, table structure extraction is a challenging task in automated document analysis systems. We present a pair of novel deep learning models (Split and Merge models) that given an input image, 1) predicts the basic table grid pattern and 2) predicts which grid elements should be merged to recover cells that span multiple rows or columns. We propose projection pooling as a novel component of the Split model and grid pooling as a novel part of the Merge model. While most Fully Convolutional Networks rely on local evidence, these unique pooling regions allow our models to take advantage of the global table structure. We achieve state-of-the-art performance on the public ICDAR 2013 Table Competition dataset of PDF documents. On a much larger private dataset which we used to train the models, we significantly outperform both a state-ofthe-art deep model and a major commercial software system.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "67319819",
                        "name": "Chris Tensmeyer"
                    },
                    {
                        "authorId": "2852035",
                        "name": "Vlad I. Morariu"
                    },
                    {
                        "authorId": "31844147",
                        "name": "Brian L. Price"
                    },
                    {
                        "authorId": "2183492748",
                        "name": "Scott D. Cohen"
                    },
                    {
                        "authorId": "143769852",
                        "name": "Tony R. Martinez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[2] proposed a deep learning based approach for recognizing rows and columns of tables in document images.",
                "[2] table structure extraction system based on semantic segmentation by a significant margin.",
                "This approach provides a significant improvement over heuristic algorithms and CNN based models [2], owing to the powerful representation of the sequence models that capture",
                "[2] which is the state-of-the-art deep learning based approach towards table structure recognition.",
                "[2] proposed a deep learning based approach for table structure recognition.",
                "For that purpose, we used the publicly available ICDAR 2013 table competition dataset containing 67 documents with 238 pages, since this dataset was used in [2]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ec6c3b38d181280c57d11393d2f68d794febd85c",
                "externalIds": {
                    "MAG": "2999605892",
                    "ArXiv": "2001.02501",
                    "DBLP": "conf/icdar/KhanKSS19",
                    "DOI": "10.1109/ICDAR.2019.00220",
                    "CorpusId": 210064474
                },
                "corpusId": 210064474,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/ec6c3b38d181280c57d11393d2f68d794febd85c",
                "title": "Table Structure Extraction with Bi-Directional Gated Recurrent Unit Networks",
                "abstract": "Tables present summarized and structured information to the reader, which makes table's structure extraction an important part of document understanding applications. However, table structure identification is a hard problem not only because of the large variation in the table layouts and styles, but also owing to the variations in the page layouts and the noise contamination levels. A lot of research has been done to identify table structure, most of which is based on applying heuristics with the aid of optical character recognition (OCR) to hand pick layout features of the tables. These methods fail to generalize well because of the variations in the table layouts and the errors generated by OCR. In this paper, we have proposed a robust deep learning based approach to extract rows and columns from a detected table in document images with a high precision. In the proposed solution, the table images are first pre-processed and then fed to a bi-directional Recurrent Neural Network with Gated Recurrent Units (GRU) followed by a fully-connected layer with softmax activation. The network scans the images from top-to-bottom as well as left-to-right and classifies each input as either a row-separator or a column-separator. We have benchmarked our system on publicly available UNLV as well as ICDAR 2013 datasets on which it outperformed the state-of-theart table structure extraction systems by a significant margin.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "148073753",
                        "name": "Saqib Ali Khan"
                    },
                    {
                        "authorId": "1480142935",
                        "name": "Syed Khalid"
                    },
                    {
                        "authorId": "2066563469",
                        "name": "M. Shahzad"
                    },
                    {
                        "authorId": "1688013",
                        "name": "F. Shafait"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, this limits the applicability of these systems to only PDFs where many of the documents are scanned or present in the form of images [2].",
                "Schreiber et al. (2017) [2] used only 31 images to test their model.",
                "Automatic extraction of such information for the purpose of digitization/processing is highly valuable for organizations [2].",
                "(2017) [2] used only 31 images to test their model.",
                "Following the work of Schreiber et al. (2017) [2], we converted the cell-based annotations to the corresponding annotations for rows and columns.",
                "In contrast to the annotations generated by Schreiber et al. (2017) [2], we label the complete row regardless of the textual region for consistency.",
                "Training data-driven models for the task of table structure analysis requires access to a large amount of labeled data, specifically for deep models [2].",
                "Significant efforts have been made in the past for automated extraction of information from documents [1], [2], [3], [4], [5].",
                "The same evaluation scheme was used for the ICDAR-13 table structure recognition competition and Schreiber et al. (2017) [6], [2].",
                "Schreiber et al. (2017) [2] translated the dataset to labels for rows and columns in order for the image-based models to be trained.",
                "[2] and our custom ICDAR-17) neglect row/column span (hierarchical labels).",
                "Once a table is detected, the structure of the table is analyzed [2].",
                "detection of the table itself [7], [3], [2].",
                "One of the recent image-based deep learning methods for table structure analysis has been proposed by Schreiber et al. (2017) [2].",
                "We used the same train and test split as used by Schreiber et al. (2017) [2] in order to enable a direct comparison against their approach.",
                "hand-designed heuristics to data-driven methodologies [1], [2].",
                "In contrast to the Fully-Convolutional Network (FCN) based formulation presented by Schreiber et al. (2017) [2], we treat the problem of row/column identification in a tabular structure as that of object detection where the document can be considered analogous to scene and row/column can be\u2026",
                "(2017) [2] translated the dataset to labels for rows and columns in order for the image-based models to be trained.",
                "This is consistent with ICDAR-13 image-based table structure recognition dataset generated by Schreiber et al. (2017) [2] where row/column span (hierarchical labels) were also discarded."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "28b9fbf6e4edde43a580f05c28f435709d09b9a6",
                "externalIds": {
                    "DBLP": "conf/icdar/SiddiquiFR0A19",
                    "MAG": "3004127423",
                    "DOI": "10.1109/ICDAR.2019.00226",
                    "CorpusId": 211026989
                },
                "corpusId": 211026989,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/28b9fbf6e4edde43a580f05c28f435709d09b9a6",
                "title": "DeepTabStR: Deep Learning based Table Structure Recognition",
                "abstract": "This paper presents a novel method for the analysis of tabular structures in document images using the potential of deformable convolutional networks. In order to assess the suitability of the model to the task of table structure recognition, most of the prior methods have been tested on the smaller ICDAR-13 table structure recognition dataset comprising of just 156 tables. We curated a new image-based table structure recognition dataset, TabStructDB2, comprising of 1081 tables densely labeled with row and column information. Instead of collecting new images for this purpose, we leveraged the famous Page-Object Detection dataset from ICDAR-17, and added structural information for all the tabular regions present in the dataset. This new publicly available dataset will enable the development of more sophisticated table structure recognition techniques in the future. We performed extensive evaluation on the two datasets (ICDAR-13 and TabStructDB) including crossdataset testing in order to evaluate the efficacy of the proposed approach. We achieved state-of-the-art results with deformable models on ICDAR-13 with an average F-Measure of 92.98% (89.42% for rows and 96.55% for columns) and report baseline results on TabStructDB for guiding future research efforts with an F-Measure of 93.72% (91.26% for rows and 95.59% for columns). Despite promising results, structural analysis of tables with arbitrary layouts is still far from achievable at this point.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "29005173",
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "authorId": "1491425514",
                        "name": "Imran Ali Fateh"
                    },
                    {
                        "authorId": "40795206",
                        "name": "Syed Tahseen Raza Rizvi"
                    },
                    {
                        "authorId": "145279674",
                        "name": "A. Dengel"
                    },
                    {
                        "authorId": "1734717217",
                        "name": "Sheraz Ahmed"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Significant efforts have been made in the past to extract this tabular information from documents through an automated process [1], [2], [3], [6], [7].",
                "Similarly, we follow an identical data split (with the same files segregated into train and test sets) as the one defined by Schreiber et al. (2017) [1] comprising of 125 training and 31 test table images.",
                "This problem of cell region detection can be further decomposed into row and column identification which can ultimately be combined together for the discovery of the corresponding cells in the table [1].",
                "This intermediate representation can also be the output of the system as in the case of Schreiber et al. (2017) [1].",
                "We compare our method using these image-based row and column-level statistics [1].",
                "[1] proposed a table recognition system where they computed precision and recall based on the rows and columns instead of using the cell-level information from the ICDAR-2013 table competition dataset [7].",
                "\u2022 Benchmarking of the results obtained by the proposed prediction tiling approach using the evaluation metrics used by Schreiber et al. (2017) [1] on the publicly available ICDAR-13 table structure recognition dataset [7].",
                "Schreiber et al. (2017) [1] made a recent attempt for the incorporation of deep learning based techniques for the task of table structure recognition where they used two different fully-convolutional networks for the identification of the corresponding rows and columns.",
                "Therefore, automated extraction of these tabular regions can be useful in a wide range of applications [1], [2], [3].",
                "Our post-processing is similar to the one used by Schreiber et al. (2017) [1].",
                "Abstract\u2014Based on the recent advancements in the domain of semantic segmentation, Fully-Convolutional Networks (FCN) have been successfully applied for the task of table structure recognition in the past [1].",
                "Minor efforts have also been made on performing analysis of tabular regions directly on images [1].",
                "Numerous efforts have been made in the past to automatically extract the relevant information from documents [1], [2], [3], [4], [5].",
                "Numerous methods have dealt with this problem in the past [1], [2], [3] and achieved near perfect detection rates on publicly available datasets.",
                "(2017) [1] on the publicly available ICDAR-13 table structure recognition dataset [7]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "12387bb33e37b79a43b880be0efc8d010138081b",
                "externalIds": {
                    "MAG": "3004240503",
                    "DBLP": "conf/icdar/SiddiquiK0A19",
                    "DOI": "10.1109/ICDAR.2019.00225",
                    "CorpusId": 211026555
                },
                "corpusId": 211026555,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/12387bb33e37b79a43b880be0efc8d010138081b",
                "title": "Rethinking Semantic Segmentation for Table Structure Recognition in Documents",
                "abstract": "Based on the recent advancements in the domain of semantic segmentation, Fully-Convolutional Networks (FCN) have been successfully applied for the task of table structure recognition in the past. We analyze the efficacy of semantic segmentation networks for this purpose and simplify the problem by proposing prediction tiling based on the consistency assumption which holds for tabular structures. For an image of dimensions H \u00d7 W, we predict a single column for the rows (\u0177_row \u220a H) and a predict a single row for the columns (\u0177_row \u220a W). We use a dual-headed architecture where initial feature maps (from the encoder-decoder model) are shared while the last two layers generate class specific (row/column) predictions. This allows us to generate predictions using a single model for both rows and columns simultaneously, where previous methods relied on two separate models for inference. With the proposed method, we were able to achieve state-of-the-art results on ICDAR-13 image-based table structure recognition dataset with an average F-Measure of 92.39% (91.90% and 92.88% F-Measure for rows and columns respectively). With the proposed method, we were able to achieve state-of-the-art results on ICDAR-13. The obtained results advocate that constraining the problem space in the case of FCN by imposing valid constraints can lead to significant performance gains.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "29005173",
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "authorId": "144777215",
                        "name": "P. Khan"
                    },
                    {
                        "authorId": "145279674",
                        "name": "A. Dengel"
                    },
                    {
                        "authorId": "1734717217",
                        "name": "Sheraz Ahmed"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use Marmot data set for training our model similar to DeepDeSRT [6].",
                "[6] proposed a deep learning based solution for table detection and structure identification, which does not require any assumption about the structure of the tables.",
                "The GOD is compared to state-of-the-arts: Kavasidis et al. [8], DeepDeSRT [6] and Tran et al. [26] on ICDAR-2013 table competition data set.",
                "precision, recall and F1 measures [6], [8], [26] to evaluate the performance of our algorithm for detecting graphical objects in the document images."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "b587b90cfc662779c3ab7e891d8147551c6720fd",
                "externalIds": {
                    "MAG": "3080731453",
                    "DBLP": "conf/icdar/SahaMJ19",
                    "ArXiv": "2008.10843",
                    "DOI": "10.1109/ICDAR.2019.00018",
                    "CorpusId": 203558412
                },
                "corpusId": 203558412,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/b587b90cfc662779c3ab7e891d8147551c6720fd",
                "title": "Graphical Object Detection in Document Images",
                "abstract": "Graphical elements: particularly tables and figures contain a visual summary of the most valuable information contained in a document. Therefore, localization of such graphical objects in the document images is the initial step to understand the content of such graphical objects or document images. In this paper, we present a novel end-to-end trainable deep learning based framework to localize graphical objects in the document images called as Graphical Object Detection ( GOD ). Our framework is data-driven and does not require any heuristics or meta-data to locate graphical objects in the document images. The GOD explores the concept of transfer learning and domain adaptation to handle scarcity of labeled training images for graphical object detection task in the document images. Performance analysis carried out on the various public benchmark data sets: ICDAR -2013, ICDAR - POD2017 and UNLV shows that our model yields promising results as compared to state-of-the-art techniques.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "145297102",
                        "name": "Ranajit Saha"
                    },
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "1694502",
                        "name": "C. V. Jawahar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[3] used a pre-trained model over Faster RCNN.",
                "There are also proposed methods [2], [3] for table detection based on Faster R-CNN [10] architecture; however, Schreiber et al.",
                "[3] have used data-driven deep learning approach using fully convolutional networks (FCN) for semantic segmentation architectures [17] with skip connections.",
                "Recently deep learning techniques are used to localize a table but mostly applied on contemporary documents [2], [3], [4]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9fd3f3ca7585600879233223bed93c25d97b43a1",
                "externalIds": {
                    "DBLP": "conf/icdar/RezaBJ019",
                    "MAG": "2987932197",
                    "DOI": "10.1109/ICDARW.2019.40097",
                    "CorpusId": 207950574
                },
                "corpusId": 207950574,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9fd3f3ca7585600879233223bed93c25d97b43a1",
                "title": "Table Localization and Segmentation using GAN and CNN",
                "abstract": "Table localization and segmentation is an important but critical step in document image analysis. Table segmentation is much harder than table localization particularly in the invoice document because sometimes there are nested rows or nested columns or even nested table in an invoice. Moreover, rows or columns are very close to each other and sometimes columns overlaps with each other. Most of the existing techniques fail to generalize because they rely on hand engineered features which are not robust to layout variations. Recently, deep learning approaches are applied in table localization and segmentation which achieved promising result. However, these techniques are mostly applied to contemporary document images like UNLV or Marmot datasets. Additionally, there is still some limitation to generalize them on different layout variations and preprocessing. In this paper, we have applied conditional Generative Adversarial Networks (cGAN) based architecture for table area localization and SegNet based encoder-decoder with skip connections architecture for table structure segmentation. We applied ICDAR 2013 table competition dataset for evaluating the table localization performance. Result shows that our approaches are outperforming with existing models. On the other hand, we used private complex invoice dataset for table area segmentation.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "104144966",
                        "name": "M. Reza"
                    },
                    {
                        "authorId": "145461897",
                        "name": "S. S. Bukhari"
                    },
                    {
                        "authorId": "10415155",
                        "name": "Martin Jenckel"
                    },
                    {
                        "authorId": "145279674",
                        "name": "A. Dengel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is interesting to note that [9] also uses FRCNN to detect multiple line-items regions, but their task-oriented evaluation measure makes the comparison with [7] difficult (and most of the invoices in their dataset have only less than three items).",
                "[7] uses Faster R-CNN for the table recognition task, and also offers a complementary",
                "A follow-up work of [7] proposes DeCNT [8], an approach based on a"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "45d21951e2c95be1a2c50d97a1d7055728aaee71",
                "externalIds": {
                    "MAG": "2969990743",
                    "DBLP": "conf/icdar/PrasadDM19",
                    "DOI": "10.1109/ICDAR.2019.00054",
                    "CorpusId": 198949384
                },
                "corpusId": 198949384,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/45d21951e2c95be1a2c50d97a1d7055728aaee71",
                "title": "Versatile Layout Understanding via Conjugate Graph",
                "abstract": "Recent advances in document understanding, especially text recognition, provide new opportunities to address the page segmentation problem. In this paper, we propose a method to groups text lines into semantic objects. We model a page as a graph where nodes represent text lines and the edges their geometric relations. The logical segmentation task then refers to identify all text lines belonging to some logical sub-division of the page. We model this task as categorizing edges as relevant or not to build the targeted sub-division (sub-graph). This edge categorization is performed using structured machine learning algorithms (graph Conditional Random Field and Edge Convolutional Network). We use a connected components-based approach following the edge classification for aggregating the nodes. This simple approach shows very robust results for various layout and various page sub-division. We experiment on table segmentation into multiple sub-divisions (rows, columns, and cells) and minutes segmentation into resolutions. Our sub-division and page-layout oblivious approach shows near-par performance as compared to task dedicated approaches and even outperforms them in certain setups.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "40172901",
                        "name": "Animesh Prasad"
                    },
                    {
                        "authorId": "2131960",
                        "name": "Herv\u00e9 D\u00e9jean"
                    },
                    {
                        "authorId": "2066113788",
                        "name": "J. Meunier"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "277564ae1d8b38c4128d0d3b50b54fe54770979b",
                "externalIds": {
                    "MAG": "3003791552",
                    "DBLP": "conf/icdar/LeeHOU19",
                    "DOI": "10.1109/ICDAR.2019.00167",
                    "CorpusId": 208510393
                },
                "corpusId": 208510393,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/277564ae1d8b38c4128d0d3b50b54fe54770979b",
                "title": "Page Segmentation using a Convolutional Neural Network with Trainable Co-Occurrence Features",
                "abstract": "In document analysis, page segmentation is a fundamental task that divides a document image into semantic regions. In addition to local features, such as pixel-wise information, co-occurrence features are also useful for extracting texture-like periodic information for accurate segmentation. However, existing convolutional neural network (CNN)-based methods do not have any mechanisms that explicitly extract co-occurrence features. In this paper, we propose a method for page segmentation using a CNN with trainable multiplication layers (TMLs). The TML is specialized for extracting co-occurrences from feature maps, thereby supporting the detection of objects with similar textures and periodicities. This property is also considered to be effective for document image analysis because of regularity in text line structures, tables, etc. In the experiment, we achieved promising performance on a pixel-wise page segmentation task by combining TMLs with U-Net. The results demonstrate that TMLs can improve performance compared to the original U-Net. The results also demonstrate that TMLs are helpful for detecting regions with periodically repeating features, such as tables and main text.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2116659707",
                        "name": "Joonho Lee"
                    },
                    {
                        "authorId": "1381961694",
                        "name": "Hideaki Hayashi"
                    },
                    {
                        "authorId": "1744332",
                        "name": "W. Ohyama"
                    },
                    {
                        "authorId": "1809705",
                        "name": "S. Uchida"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[17] and Gilani et al."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "57c0a849a56e6e5cd4b748522409f187432cdbcb",
                "externalIds": {
                    "DBLP": "conf/icdar/LiGTYH19",
                    "MAG": "3003482937",
                    "DOI": "10.1109/ICDAR.2019.00127",
                    "CorpusId": 211027092
                },
                "corpusId": 211027092,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/57c0a849a56e6e5cd4b748522409f187432cdbcb",
                "title": "A GAN-Based Feature Generator for Table Detection",
                "abstract": "Table detection is of great significance for the documents analysis and recognition. Although many methods have been proposed and great progress have been made, it is still a great challenge to recognize the less-ruled tables due to the lack of table line features. In this paper, we propose a novel network to generate the layout features for table text to improve the performance of less-ruled table recognition. This feature generator model is similar to the Generative Adversarial Networks (GAN). We force the feature generator model to extract similar features for both ruling tables and less-ruled tables. It can be added into some common object detection and semantic segmentation models such as Mask R-CNN, U-Net. Extensive experiments are conducted on the dataset of ICDAR2017 Page Object Detection Competition dataset and a closed dataset full of the less-ruled tables and non-ruled tables. The primary experimental results show that the proposed GAN-based feature generator is very helpful for less-ruled table detection.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2110470261",
                        "name": "Yibo Li"
                    },
                    {
                        "authorId": "1777642",
                        "name": "Liangcai Gao"
                    },
                    {
                        "authorId": "2087321561",
                        "name": "Zhi Tang"
                    },
                    {
                        "authorId": "2072780764",
                        "name": "Qinqin Yan"
                    },
                    {
                        "authorId": "49866988",
                        "name": "Yilun Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A similar approach based on a region proposal network is also proposed in DeepDeSRT [24] for detecting tables, they further extended it to rows and column detection.",
                "Table detection and recognition in unconstrained documents is considered a challenging task and recently has received significant attention within the community [2], [7], [11], [16], [19], [22], [24]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e45d822c782fd9e52b4e5abf60b3b7d6d5e3df0",
                "externalIds": {
                    "MAG": "2998913931",
                    "DBLP": "conf/icdar/Riba0GFT019",
                    "DOI": "10.1109/ICDAR.2019.00028",
                    "CorpusId": 211027013
                },
                "corpusId": 211027013,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/0e45d822c782fd9e52b4e5abf60b3b7d6d5e3df0",
                "title": "Table Detection in Invoice Documents by Graph Neural Networks",
                "abstract": "Tabular structures in documents offer a complementary dimension to the raw textual data, representing logical or quantitative relationships among pieces of information. In digital mail room applications, where a large amount of administrative documents must be processed with reasonable accuracy, the detection and interpretation of tables is crucial. Table recognition has gained interest in document image analysis, in particular in unconstrained formats (absence of rule lines, unknown information of rows and columns). In this work, we propose a graph-based approach for detecting tables in document images. Instead of using the raw content (recognized text), we make use of the location, context and content type, thus it is purely a structure perception approach, not dependent on the language and the quality of the text reading. Our framework makes use of Graph Neural Networks (GNNs) in order to describe the local repetitive structural information of tables in invoice documents. Our proposed model has been experimentally validated in two invoice datasets and achieved encouraging results. Additionally, due to the scarcity of benchmark datasets for this task, we have contributed to the community a novel dataset derived from the RVL-CDIP invoice data. It will be publicly released to facilitate future research.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "40420775",
                        "name": "Pau Riba"
                    },
                    {
                        "authorId": "2149815080",
                        "name": "Anjan Dutta"
                    },
                    {
                        "authorId": "145029669",
                        "name": "Lutz Goldmann"
                    },
                    {
                        "authorId": "1686569",
                        "name": "A. Forn\u00e9s"
                    },
                    {
                        "authorId": "3045937",
                        "name": "O. R. Terrades"
                    },
                    {
                        "authorId": "143826881",
                        "name": "J. Llad\u00f3s"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[5] presented a deep learning-based solution for the identification of rows, columns, and cells, where transfer learning is performed by augmenting and fine-tuning a FCN semantic segmentation model [15].",
                "The first is to classify table cells into specific categories [4], [5], e.",
                "More recent works [4], [5] have used deep-learning methods to recognize table structure."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e03ada27dc07ee80dd199308aa243a9ccffeff5d",
                "externalIds": {
                    "MAG": "3003953860",
                    "DBLP": "conf/icdar/XueLT19",
                    "DOI": "10.1109/ICDAR.2019.00125",
                    "CorpusId": 211026900
                },
                "corpusId": 211026900,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/e03ada27dc07ee80dd199308aa243a9ccffeff5d",
                "title": "ReS2TIM: Reconstruct Syntactic Structures from Table Images",
                "abstract": "Tables often represent densely packed but structured data. Understanding table semantics is vital for effective information retrieval and data mining. Unlike web tables, whose semantics are readable directly from markup language and contents, the full analysis of tables published as images requires the conversion of discrete data into structured information. This paper presents a novel framework to convert a table image into its syntactic representation through the relationships between its cells. In order to reconstruct the syntactic structures of a table, we build a cell relationship network to predict the neighbors of each cell in four directions. During the training stage, a distance-based sample weight is proposed to handle the class imbalance problem. According to the detected relationships, the table is represented by a weighted graph that is then employed to infer the basic syntactic table structure. Experimental evaluation of the proposed framework using two datasets demonstrates the effectiveness of our model for cell relationship detection and table structure inference.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "51469630",
                        "name": "Wenyuan Xue"
                    },
                    {
                        "authorId": "1612980447",
                        "name": "Qingyong Li"
                    },
                    {
                        "authorId": "143719920",
                        "name": "D. Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently deep learning approaches have been attempted for table detection [19], [20] and are showing very good results."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "16d7d2f1a0231d0e87fcaa2bf60ebc4d01c95bff",
                "externalIds": {
                    "MAG": "3003737912",
                    "DBLP": "conf/icdar/MelindaB19",
                    "DOI": "10.1109/ICDAR.2019.00079",
                    "CorpusId": 211027124
                },
                "corpusId": 211027124,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/16d7d2f1a0231d0e87fcaa2bf60ebc4d01c95bff",
                "title": "Parameter-Free Table Detection Method",
                "abstract": "In this paper, we propose two parameter-free table detection methods: one for the closed tables and other for open tables. The unifying idea is multigaussian analysis. Multigaussian analysis of text height histograms classifies the document content into text and non-text blocks. Closed tables are classified as non-text and their identification from the non-text blocks is similar to many earlier methods that remove the separators. We do not need any parameters to identify rows and columns and discriminate them from text blocks because of multigaussian analysis. Open tables are initially classified as text blocks and are detected by extending the multigaussian analysis to the heights and widths of text blocks. The text-blocks are grouped into three categories by multigaussian analysis. These groups are used to classify table cells and distinguish them from text blocks. Table blocks are merged to obtain the table region. Evaluation on various Indic script newspapers and ICDAR2013 table competition dataset shows that our methods achieve more than 90% in table recognition. The strength of our algorithm is that it is a parameter-free approach and requires no training dataset.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "35348888",
                        "name": "Laiphangbam Melinda"
                    },
                    {
                        "authorId": "1716502",
                        "name": "C. Bhagvati"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, [6] proposes DeepDeSRT, a combination of two deep learning systems, one for table detection, and a second for table understanding."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ca464f6a6e1e039711576e501017e6e1d759d885",
                "externalIds": {
                    "MAG": "3004376248",
                    "DBLP": "conf/icdar/DejeanM19",
                    "DOI": "10.1109/ICDAR.2019.00080",
                    "CorpusId": 211027026
                },
                "corpusId": 211027026,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/ca464f6a6e1e039711576e501017e6e1d759d885",
                "title": "Table Rows Segmentation",
                "abstract": "We consider the Document Understanding problem of segmenting tables in rows. We propose a method that first enumerates virtual row separator candidates and then select the correct ones thanks to a classification task, solved using supervised structured machine learning. Interestingly, the task is the joint-classification of virtual separators and real text lines. We describe and tested several alternative candidate generation methods and report the results of our experiment for each, on two different types of registry books from the 19th century.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2131960",
                        "name": "Herv\u00e9 D\u00e9jean"
                    },
                    {
                        "authorId": "2066113788",
                        "name": "J. Meunier"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "27cf3a7f79ca352065f6389ab4b8bdd7e780ce79",
                "externalIds": {
                    "DBLP": "conf/icdar/HuangYLCWGT19",
                    "MAG": "3003760744",
                    "DOI": "10.1109/ICDAR.2019.00135",
                    "CorpusId": 211026656
                },
                "corpusId": 211026656,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/27cf3a7f79ca352065f6389ab4b8bdd7e780ce79",
                "title": "A YOLO-Based Table Detection Method",
                "abstract": "Due to various table layouts and styles, table detection is always a difficult task in the field of document analysis. Inspired by the great progress of deep learning based methods on object detection, in this paper, we present a YOLO-based method for this task. Considering the large difference between document objects and natural objects, we introduce some adaptive adjustments to YOLOv3, including an anchor optimization strategy and two post processing methods. For anchor optimization, we use k-means clustering to find anchors which are more suitable for tables rather than natural objects and make it easier for our model to find exact positions of tables. In post-processing process, the extra whitespaces and noisy page objects (e.g. page headers, page footers) are removed from the predicted results, so that our model can get more accurate table margins and higher IoU scores. The proposed method is evaluated on two datasets from ICDAR 2013 Table Competition and ICDAR 2017 Page Object Detection (POD) Competition and achieves state-of-the-art performance.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "49866988",
                        "name": "Yilun Huang"
                    },
                    {
                        "authorId": "2072780764",
                        "name": "Qinqin Yan"
                    },
                    {
                        "authorId": "2110470261",
                        "name": "Yibo Li"
                    },
                    {
                        "authorId": "2145279898",
                        "name": "Yifan Chen"
                    },
                    {
                        "authorId": "2108419828",
                        "name": "Xiong Wang"
                    },
                    {
                        "authorId": "1777642",
                        "name": "Liangcai Gao"
                    },
                    {
                        "authorId": "2087321561",
                        "name": "Zhi Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The fine tuned F-RCNN model achieves the state-of-the-art performance reported in [6], where the F-RCNN model was fine tuned with 1600 samples from a pre-trained object detection model.",
                "More recently, image analytics methods based on deep learning are becoming available [5] and are used to train document layout understanding pipelines [1], [6]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b5799d10df17de3232540e990da69553800d6376",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1908-07836",
                    "MAG": "3003711898",
                    "ArXiv": "1908.07836",
                    "DOI": "10.1109/ICDAR.2019.00166",
                    "CorpusId": 201124789
                },
                "corpusId": 201124789,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/b5799d10df17de3232540e990da69553800d6376",
                "title": "PubLayNet: Largest Dataset Ever for Document Layout Analysis",
                "abstract": "Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the PubLayNet dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on PubLayNet accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/PubLayNet) to support development and evaluation of more advanced models for document layout analysis.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2113309578",
                        "name": "Xu Zhong"
                    },
                    {
                        "authorId": "2328282",
                        "name": "Jianbin Tang"
                    },
                    {
                        "authorId": "1399097376",
                        "name": "Antonio Jimeno-Yepes"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In 2017, Schreiber et al. first proposed a model called DeepDeSRT, which treats the table structure recognition task as an image semantic segmentation problem, and then recognizes the regions of columns and rows respectively (Schreiber et al., 2017).",
                "first proposed a model called DeepDeSRT, which treats the table structure recognition task as an image semantic segmentation problem, and then recognizes the regions of columns and rows respectively (Schreiber et al., 2017).",
                "The existing approaches can be classified into two categories: rule-based methods (Ramel et al., 2003; Yildiz et al., 2005; Hassan and Baumgartner, 2007) and data-driven methods (Schreiber et al., 2017; Li et al., 2019).",
                ", 2005; Hassan and Baumgartner, 2007) and data-driven methods (Schreiber et al., 2017; Li et al., 2019).",
                "I R\n] 2\n8 A\nug 2\n01 9\nDeSRT (Schreiber et al., 2017) and Tabby (Shigarov et al., 2016).",
                "\u2022 DeepDeSRT: DeepDeSRT (Schreiber et al., 2017) is a data-driven method that utilizes a semantic segmentation model to recognize the table structure as a set of segmented rows and columns."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1fb193f5a0d809f12094cad3a55c299969c87baf",
                "externalIds": {
                    "MAG": "2968868378",
                    "DBLP": "journals/corr/abs-1908-04729",
                    "ArXiv": "1908.04729",
                    "CorpusId": 199552037
                },
                "corpusId": 199552037,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1fb193f5a0d809f12094cad3a55c299969c87baf",
                "title": "Complicated Table Structure Recognition",
                "abstract": "The task of table structure recognition aims to recognize the internal structure of a table, which is a key step to make machines understand tables. Currently, there are lots of studies on this task for different file formats such as ASCII text and HTML. It also attracts lots of attention to recognize the table structures in PDF files. However, it is hard for the existing methods to accurately recognize the structure of complicated tables in PDF files. The complicated tables contain spanning cells which occupy at least two columns or rows. To address the issue, we propose a novel graph neural network for recognizing the table structure in PDF files, named GraphTSR. Specifically, it takes table cells as input, and then recognizes the table structures by predicting relations among cells. Moreover, to evaluate the task better, we construct a large-scale table structure recognition dataset from scientific papers, named SciTSR, which contains 15,000 tables from PDF files and their corresponding structure labels. Extensive experiments demonstrate that our proposed model is highly effective for complicated tables and outperforms state-of-the-art baselines over a benchmark dataset and our new constructed dataset.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "46221722",
                        "name": "Zewen Chi"
                    },
                    {
                        "authorId": "4590286",
                        "name": "Heyan Huang"
                    },
                    {
                        "authorId": "152337253",
                        "name": "Heng-Da Xu"
                    },
                    {
                        "authorId": "152338019",
                        "name": "Houjin Yu"
                    },
                    {
                        "authorId": "152135547",
                        "name": "Wanxuan Yin"
                    },
                    {
                        "authorId": "134880677",
                        "name": "Xian-Ling Mao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "is good and is comparable to [16] which uses FCN.",
                "A variety of methods tackled this problem [16], [18]\u2013[20] on publicly available datasets.",
                "DeepDeSRT [16] uses Fully Convolutional Networks (FCN) [17] to segment the rows and columns of tables."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "25347d4011c8f7246efc4edfb3653d24eb8a13b8",
                "externalIds": {
                    "MAG": "3003627710",
                    "DBLP": "conf/iscc/KaraTKK19",
                    "DOI": "10.1109/ISCC47284.2019.8969683",
                    "CorpusId": 210972292
                },
                "corpusId": 210972292,
                "publicationVenue": {
                    "id": "159aed30-148b-4b50-99a4-372b0af958d9",
                    "name": "International Symposium on Computers and Communications",
                    "type": "conference",
                    "alternate_names": [
                        "ISCC",
                        "Int Symp Comput Commun"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000156/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/25347d4011c8f7246efc4edfb3653d24eb8a13b8",
                "title": "Deep Learning for Recognizing the Anatomy of Tables on Datasheets",
                "abstract": "With the growth of information flow through supply chains, we address the issue of semantically segmenting tabular data from document flows. This challenge is abstract in definition as there is no guideline to what defines a table, therefore we primarily applied deep learning methods to compare performances, as well as a morphology-based method to compare several methods for the task of table structure detection. Taking advantage of transfer learning, we found that Mask-RCNN was the most capable network at abstracting to segmentation of tables. Due to the large variation in sizes between columns and rows, most networks failed to detect both with equal effectiveness. However with Mask-RCNN 97.01% precision and 98.28% recall were attained, which put it far ahead of other models in row detection, cementing Mask-RCNN as the most effective choice in this task.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1491166537",
                        "name": "Ertugrul Kara"
                    },
                    {
                        "authorId": "2065915247",
                        "name": "Mark Traquair"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "152235107",
                        "name": "Shahzad Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DeepDeSRT [24] 0."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9785b120c7f7f46374a731709ebf213bf3e94d36",
                "externalIds": {
                    "DBLP": "conf/iscc/TraquairKKK19",
                    "MAG": "3003525420",
                    "DOI": "10.1109/ISCC47284.2019.8969682",
                    "CorpusId": 210970715
                },
                "corpusId": 210970715,
                "publicationVenue": {
                    "id": "159aed30-148b-4b50-99a4-372b0af958d9",
                    "name": "International Symposium on Computers and Communications",
                    "type": "conference",
                    "alternate_names": [
                        "ISCC",
                        "Int Symp Comput Commun"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000156/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/9785b120c7f7f46374a731709ebf213bf3e94d36",
                "title": "Deep Learning for the Detection of Tabular Information from Electronic Component Datasheets",
                "abstract": "The global electronic components supply chain consists of tens of thousands of e-component manufacturers who fabricate over a billion distinct components. These are described in datasheets that differ in style, layout and content, and frequently publish the salient product information in tables. Keeping up-to-date on this information consumes a great deal of human effort and corporate resources. Based on the motivation that AI-based techniques are strong candidates to minimize human intervention in many applications, in this paper, we aim at the first stage of this problem and conduct a comparison of deep learning methods in detecting tabular elements in these documents. Deep learning-based object detectors are shown to be state of the art in detection tasks in different domains therefore we chose two cutting-edge models to adapt to this field, namely Faster-RCNN and RetinaNet. We use backbone networks which are pre-trained on visually salient datasets then employ transfer learning techniques to adapt to our domain. We compare the two networks under two different datasets, namely a dataset that is widely used in academic studies and a private dataset that is used by the suppliers in real supply chains. Our numerical results show that the two networks adapt well to the domain with Faster-RCNN exhibiting marginally better precision with more than 1% difference. However, RetinaNet stands out with promising recall values indicating Feature Pyramid Network architecture can potentially detect technical documents better.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2065915247",
                        "name": "Mark Traquair"
                    },
                    {
                        "authorId": "1491166537",
                        "name": "Ertugrul Kara"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "152235107",
                        "name": "Shahzad Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Approaches like [15] partially solve the issue but it is still not a natural approach.",
                "[15] also used Faster RCNN for table detection and extraction of rows and columns."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f39b5b60981bc3c37f0557963737f67e9241b8eb",
                "externalIds": {
                    "DBLP": "conf/icdar/QasimMS19",
                    "MAG": "2955530511",
                    "DOI": "10.1109/ICDAR.2019.00031",
                    "CorpusId": 195791577
                },
                "corpusId": 195791577,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/f39b5b60981bc3c37f0557963737f67e9241b8eb",
                "title": "Rethinking Table Recognition using Graph Neural Networks",
                "abstract": "Document structure analysis, such as zone segmentation and table recognition, is a complex problem in document processing and is an active area of research. The recent success of deep learning in solving various computer vision and machine learning problems has not been reflected in document structure analysis since conventional neural networks are not well suited to the input structure of the problem. In this paper, we propose an architecture based on graph networks as a better alternative to standard neural networks for table recognition. We argue that graph networks are a more natural choice for these problems, and explore two gradient-based graph neural networks. Our proposed architecture combines the benefits of convolutional neural networks for visual feature extraction and graph networks for dealing with the problem structure. We empirically demonstrate that our method outperforms the baseline by a significant margin. In addition, we identify the lack of large scale datasets as a major hindrance for deep learning research for structure analysis and present a new large scale synthetic dataset for the problem of table recognition. Finally, we open-source our implementation of dataset generation and the training framework of our graph networks to promote reproducible research in this direction.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "39404123",
                        "name": "S. Qasim"
                    },
                    {
                        "authorId": "2588430",
                        "name": "Hassan Mahmood"
                    },
                    {
                        "authorId": "1688013",
                        "name": "F. Shafait"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Approaches like [15] partially solve the issue but it is still not a natural approach.",
                "[15] also used Faster RCNN for table detection and extraction of rows and columns."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ef5f918981330ee83e253bc0e01401954bd56223",
                "externalIds": {
                    "MAG": "2947372801",
                    "ArXiv": "1905.13391",
                    "DBLP": "journals/corr/abs-1905-13391",
                    "CorpusId": 173188913
                },
                "corpusId": 173188913,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ef5f918981330ee83e253bc0e01401954bd56223",
                "title": "Rethinking Table Parsing using Graph Neural Networks",
                "abstract": "Document structure analysis, such as zone segmentation and table parsing, is a complex problem in document processing and is an active area of research. The recent success of deep learning in solving various computer vision and machine learning problems has not been reflected in document structure analysis since conventional neural networks are not well suited to the input structure of the problem. In this paper, we propose an architecture based on graph networks as a better alternative to standard neural networks for table parsing. We argue that graph networks are a more natural choice for these problems, and explore two gradient-based graph neural networks. Our proposed architecture combines the benefits of convolutional neural networks for visual feature extraction and graph networks for dealing with the problem structure. We empirically demonstrate that our method outperforms the baseline by a significant margin. In addition, we identify the lack of large scale datasets as a major hindrance for deep learning research for structure analysis, and present a new large scale synthetic dataset for the problem of table parsing. Finally, we open-source our implementation of dataset generation and the training framework of our graph networks to promote reproducible research in this direction.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "39404123",
                        "name": "S. Qasim"
                    },
                    {
                        "authorId": "2588430",
                        "name": "Hassan Mahmood"
                    },
                    {
                        "authorId": "1688013",
                        "name": "F. Shafait"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Reference [17] also uses (pretrained) Fast R-CNN and FCN semantic segmentation model for table extraction problem."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "92ed450c18e56bcf15656c700f68d4543dbdd7b0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1904-12577",
                    "MAG": "2941833814",
                    "CorpusId": 139101249
                },
                "corpusId": 139101249,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/92ed450c18e56bcf15656c700f68d4543dbdd7b0",
                "title": "Line-items and table understanding in structured documents",
                "abstract": "Table detection and extraction has been studied in the context of documents like scientific papers, where tables are clearly outlined and stand out from the visual document structure. We study this topic in a rather more challenging domain of layout-heavy business documents, particularly invoices. Invoices present the novel challenges of tables being often without outlines - either in the form of borders or surrounding text flow - with ragged columns and widely varying data content. We will also show, that we can extract different structural information from different table-like structures. We present a comprehensive representation of a page using graph over word boxes, positional embeddings, trainable textual features and rephrase the table detection as a text box labeling problem. We will work on a new dataset of invoices using this representation and propose multiple baselines to solve this labeling problem. We then propose a novel neural network model that achieves strong, practical results on the presented dataset and analyze the model performance and effects of graph convolutions and self-attention in detail.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2076144746",
                        "name": "Martin Holecek"
                    },
                    {
                        "authorId": "102406695",
                        "name": "A. Hoskovec"
                    },
                    {
                        "authorId": "35219266",
                        "name": "P. Baudis"
                    },
                    {
                        "authorId": "116883157",
                        "name": "Pavel Klinger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Reference [17] also uses (pretrained) Fast R-CNN and FCN semantic segmentation"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "20561ec176d1a1b5369c0d5cd78e4c6e3ff566a3",
                "externalIds": {
                    "ArXiv": "1904.12577",
                    "MAG": "2957647015",
                    "DBLP": "conf/icdar/HolecekHBK19",
                    "DOI": "10.1109/ICDARW.2019.40098",
                    "CorpusId": 195847957
                },
                "corpusId": 195847957,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/20561ec176d1a1b5369c0d5cd78e4c6e3ff566a3",
                "title": "Table Understanding in Structured Documents",
                "abstract": "Table detection and extraction has been studied in the context of documents like reports, where tables are clearly outlined and stand out from the document structure visually. We study this topic in a rather more challenging domain of layout-heavy business documents, particularly invoices. Invoices present the novel challenges of tables being often without outlines - either in the form of borders or surrounding text flow - with ragged columns and widely varying data content. We will also show, that we can extract specific information from structurally different tables or table-like structures with one model. We present a comprehensive representation of a page using graph over word boxes, positional embeddings, trainable textual features and rephrase the table detection as a text box labeling problem. We will work on our newly presented dataset of pro forma invoices, invoices and debit note documents using this representation and propose multiple baselines to solve this labeling problem. We then propose a novel neural network model that achieves strong, practical results on the presented dataset and analyze the model performance and effects of graph convolutions and self-attention in detail.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2076144746",
                        "name": "Martin Holecek"
                    },
                    {
                        "authorId": "102406695",
                        "name": "A. Hoskovec"
                    },
                    {
                        "authorId": "35219266",
                        "name": "P. Baudis"
                    },
                    {
                        "authorId": "116883157",
                        "name": "Pavel Klinger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, (Schreiber et al., 2017) used the deep learningbased object detection model with pre-processing to recognize the row and column structures for the ICDAR 2013 dataset.",
                "(Schreiber et al., 2017) presented a system that is totally data-driven and does not need any heuristics or metadata to detect as well as to recognize tabular structures on the ICDAR 2013 dataset."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "616c6ae44307accb3c82059aec806e905378e4f2",
                "externalIds": {
                    "MAG": "2919502278",
                    "DBLP": "journals/corr/abs-1903-01949",
                    "ACL": "2020.lrec-1.236",
                    "ArXiv": "1903.01949",
                    "CorpusId": 67877016
                },
                "corpusId": 67877016,
                "publicationVenue": {
                    "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
                    "name": "International Conference on Language Resources and Evaluation",
                    "type": "conference",
                    "alternate_names": [
                        "LREC",
                        "Int Conf Lang Resour Evaluation"
                    ],
                    "url": "http://www.lrec-conf.org/"
                },
                "url": "https://www.semanticscholar.org/paper/616c6ae44307accb3c82059aec806e905378e4f2",
                "title": "TableBank: Table Benchmark for Image-based Table Detection and Recognition",
                "abstract": "We present TableBank, a new image-based table detection and recognition dataset built with novel weak supervision from Word and Latex documents on the internet. Existing research for image-based table detection and recognition usually fine-tunes pre-trained models on out-of-domain data with a few thousand human-labeled examples, which is difficult to generalize on real-world applications. With TableBank that contains 417K high quality labeled tables, we build several strong baselines using state-of-the-art models with deep neural networks. We make TableBank publicly available and hope it will empower more deep learning approaches in the table detection and recognition task. The dataset and models can be downloaded from https://github.com/doc-analysis/TableBank.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "123545597",
                        "name": "Minghao Li"
                    },
                    {
                        "authorId": "145500855",
                        "name": "Lei Cui"
                    },
                    {
                        "authorId": "3110003",
                        "name": "Shaohan Huang"
                    },
                    {
                        "authorId": "49807919",
                        "name": "Furu Wei"
                    },
                    {
                        "authorId": "143849609",
                        "name": "M. Zhou"
                    },
                    {
                        "authorId": "1707275",
                        "name": "Zhoujun Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Table II provides A comparison of the proposed system with the approaches presented in [15] and [14], trained on our proposed dataset.",
                "[14] used ICDAR 2013 table competition dataset for testing.",
                "[14] focuses on table detection and structure analysis as well."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2958f4aa3a7062e361941596008893121722df60",
                "externalIds": {
                    "MAG": "2910897241",
                    "DBLP": "conf/dicta/ArifS18",
                    "DOI": "10.1109/DICTA.2018.8615795",
                    "CorpusId": 58676064
                },
                "corpusId": 58676064,
                "publicationVenue": {
                    "id": "375cb686-e96e-4b79-825c-1589c99aca1d",
                    "name": "International Conference on Digital Image Computing: Techniques and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Digit Image Comput Tech Appl",
                        "DICTA",
                        "Digital Image Computing: Techniques and Applications",
                        "Digit Image Comput Tech Appl"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=710"
                },
                "url": "https://www.semanticscholar.org/paper/2958f4aa3a7062e361941596008893121722df60",
                "title": "Table Detection in Document Images using Foreground and Background Features",
                "abstract": "Table detection is an important step in many document analysis systems. It is a difficult problem due to the variety of table layouts, encoding techniques and the similarity of tabular regions with non-tabular document elements. Earlier approaches of table detection are based on heuristic rules or require additional PDF metadata. Recently proposed methods based on machine learning have shown good results. This paper demonstrates performance improvement to these table detection techniques. The proposed solution is based on the observation that tables tend to contain more numeric data and hence it applies color coding/coloration as a signal for telling apart numeric and textual data. Deep learning based Faster R-CNN is used for detection of tabular regions from document images. To gauge the performance of our proposed solution, publicly available UNLV dataset is used. Performance measures indicate improvement when compared with best in-class strategies.",
                "year": 2018,
                "authors": [
                    {
                        "authorId": "2066304789",
                        "name": "Saman Arif"
                    },
                    {
                        "authorId": "1688013",
                        "name": "F. Shafait"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This evaluation scheme is different from [5], [30] where they first compute",
                "the problem of table detection and recognition which makes the problem significantly harder to tackle [5]\u2013[7].",
                "[5] (2017) where they utilized Faster R-CNN for detection of documents achieving state-of-the-art performance on ICDAR-2013.",
                "5 for IoU to compute the F1-measure [5], we also evaluated our model based on this threshold."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "117513d07ecbb9c82110f2ae006c0c2aaf002c45",
                "externalIds": {
                    "MAG": "2901890385",
                    "DBLP": "journals/access/SiddiquiMADA18",
                    "DOI": "10.1109/ACCESS.2018.2880211",
                    "CorpusId": 56598106
                },
                "corpusId": 56598106,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/117513d07ecbb9c82110f2ae006c0c2aaf002c45",
                "title": "DeCNT: Deep Deformable CNN for Table Detection",
                "abstract": "This paper presents a novel approach for the detection of tables present in documents, leveraging the potential of deep neural networks. Conventional approaches for table detection rely on heuristics that are error prone and specific to a dataset. In contrast, the presented approach harvests the potential of data to recognize tables of arbitrary layout. Most of the prior approaches for table detection are only applicable to PDFs, whereas, the presented approach directly works on images making it generally applicable to any format. The presented approach is based on a novel combination of deformable CNN with faster R-CNN/FPN. Conventional CNN has a fixed receptive field which is problematic for table detection since tables can be present at arbitrary scales along with arbitrary transformations (orientation). Deformable convolution conditions its receptive field on the input itself allowing it to mold its receptive field according to its input. This adaptation of the receptive field enables the network to cater for tables of arbitrary layout. We evaluated the proposed approach on two major publicly available table detection datasets: ICDAR-2013 and ICDAR-2017 POD. The presented approach was able to surpass the state-of-the-art performance on both ICDAR-2013 and ICDAR-2017 POD datasets with a F-measure of 0.994 and 0.968, respectively, indicating its effectiveness and superiority for the task of table detection.",
                "year": 2018,
                "authors": [
                    {
                        "authorId": "29005173",
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "authorId": "49012494",
                        "name": "M. I. Malik"
                    },
                    {
                        "authorId": "2582412",
                        "name": "S. Agne"
                    },
                    {
                        "authorId": "145279674",
                        "name": "A. Dengel"
                    },
                    {
                        "authorId": "1734717217",
                        "name": "Sheraz Ahmed"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[6] and Gilani et al."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "15e415ff92aaf2e550a7443f158843bbad780c2c",
                "externalIds": {
                    "MAG": "2902089218",
                    "DBLP": "conf/icpr/LiYL18",
                    "DOI": "10.1109/ICPR.2018.8546073",
                    "CorpusId": 54441599
                },
                "corpusId": 54441599,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/15e415ff92aaf2e550a7443f158843bbad780c2c",
                "title": "Page Object Detection from PDF Document Images by Deep Structured Prediction and Supervised Clustering",
                "abstract": "Page object detection in document images remains a challenge because the page objects are diverse in scale and aspect ratio, and an object may contain largely apart components. In this paper, we propose a hybrid method combining deep structured prediction and supervised clustering to detect formulas, tables and figures in PDF document images within a unified framework. The primitive region proposals extracted from each column region are classified and clustered with conditional random field (CRF) based graphical models which can integrate both local and contextual information. Both the unary and pairwise potentials of CRFs are formulated as convolutional neural networks (CNNs) to better exploit spatial contextual information. The CRF for clustering predicts the linked/cut label of between-region links. After CRF inference, the line regions of same class within a cluster are grouped into a page object. The state-of-the-art performance obtained on the public available ICDAR2017 POD competition dataset demonstrates the effectiveness and superiority of the nronosed method.",
                "year": 2018,
                "authors": [
                    {
                        "authorId": "48569353",
                        "name": "Xiaohui Li"
                    },
                    {
                        "authorId": "145820427",
                        "name": "Fei Yin"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The latest approaches use deep learning for table detection and recognition [8], [9]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e52cdda25470c058195f19c1645d5d619044c70b",
                "externalIds": {
                    "DBLP": "conf/icfhr/KleberDDML18",
                    "MAG": "2905780982",
                    "DOI": "10.1109/ICFHR-2018.2018.00046",
                    "CorpusId": 56596806
                },
                "corpusId": 56596806,
                "publicationVenue": {
                    "id": "d67ba234-c048-45ed-af5b-64102ca46b2c",
                    "name": "International Conference on Frontiers in Handwriting Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Front Handwrit Recognit",
                        "ICFHR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1366"
                },
                "url": "https://www.semanticscholar.org/paper/e52cdda25470c058195f19c1645d5d619044c70b",
                "title": "Matching Table Structures of Historical Register Books using Association Graphs",
                "abstract": "In this paper we present a template-based table structure matching using association graphs for handwritten/printed historical documents. The recognition of the table structure consisting of column and header information is the prerequisite for the subsequent row detection and handwritten text recognition used for information extraction. The table matching is done by detecting the maximum clique in an association graph, which represents the matching of the line information of the template and a document of interest. This allows for variations of widths and heights of rows and columns. The presented methodology is evaluated on historical register books (death records) of the Archive of the Diocese of Passau. The method shows a reliable detection of the structure of handwritten/printed tables with a mean cell match of 88.28%.",
                "year": 2018,
                "authors": [
                    {
                        "authorId": "1729636",
                        "name": "Florian Kleber"
                    },
                    {
                        "authorId": "1715229",
                        "name": "Markus Diem"
                    },
                    {
                        "authorId": "2131960",
                        "name": "Herv\u00e9 D\u00e9jean"
                    },
                    {
                        "authorId": "2066113788",
                        "name": "J. Meunier"
                    },
                    {
                        "authorId": "2059294500",
                        "name": "E. Lang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e652226d2f58435d97de6899bb347e0e39de310a",
                "externalIds": {
                    "MAG": "2954218846",
                    "ArXiv": "1906.11901",
                    "DBLP": "journals/corr/abs-1906-11901",
                    "DOI": "10.1109/DAS.2018.44",
                    "CorpusId": 49407309
                },
                "corpusId": 49407309,
                "publicationVenue": {
                    "id": "02d53b80-30d7-493c-9453-ed7406056b31",
                    "name": "International Workshop on Document Analysis Systems",
                    "type": "conference",
                    "alternate_names": [
                        "DAS",
                        "Document Analysis Systems",
                        "Int Workshop Doc Anal Syst",
                        "Doc Anal Syst"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=647"
                },
                "url": "https://www.semanticscholar.org/paper/e652226d2f58435d97de6899bb347e0e39de310a",
                "title": "Comparing Machine Learning Approaches for Table Recognition in Historical Register Books",
                "abstract": "We present in this paper experiments on Table Recognition in hand-written register books. We first explain how the problem of row and column detection is modelled, and then compare two Machine Learning approaches (Conditional Random Field and Graph Convolutional Network) for detecting these table elements. Evaluation was conducted on death records provided by the Archives of the Diocese of Passau. With an F-1 score of 89, both methods provide a quality which allows for Information Extraction. Software and dataset are open source/data.",
                "year": 2018,
                "authors": [
                    {
                        "authorId": "2207074",
                        "name": "S. Clinchant"
                    },
                    {
                        "authorId": "2131960",
                        "name": "Herv\u00e9 D\u00e9jean"
                    },
                    {
                        "authorId": "2066113788",
                        "name": "J. Meunier"
                    },
                    {
                        "authorId": "2059294500",
                        "name": "E. Lang"
                    },
                    {
                        "authorId": "1729636",
                        "name": "Florian Kleber"
                    }
                ]
            }
        },
        {
            "contexts": [
                "we compared the performance of the four different configurations of our method to those achieved by DeepDeSRT [59], Tran [34] and Hao [27] in detecting only tables on the ICDAR 2013 dataset.",
                "To ground our work with state of the art in table detection, we compared the performance of the four different configurations of our method to those achieved by DeepDeSRT [59], Tran [34] and Hao [27] in detecting only tables on the ICDAR\n2013 dataset."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03672cfa599950f208d424d5298cdc12b72c2492",
                "externalIds": {
                    "MAG": "2797320628",
                    "DBLP": "journals/corr/abs-1804-06236",
                    "ArXiv": "1804.06236",
                    "DOI": "10.1007/978-3-030-30645-8_27",
                    "CorpusId": 4899629
                },
                "corpusId": 4899629,
                "publicationVenue": {
                    "id": "c89c0957-b21e-40c3-9e6c-0ab46adf1e53",
                    "name": "International Conference on Image Analysis and Processing",
                    "type": "conference",
                    "alternate_names": [
                        "ICIAP",
                        "Int Conf Image Anal Process"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1380"
                },
                "url": "https://www.semanticscholar.org/paper/03672cfa599950f208d424d5298cdc12b72c2492",
                "title": "A Saliency-based Convolutional Neural Network for Table and Chart Detection in Digitized Documents",
                "abstract": null,
                "year": 2018,
                "authors": [
                    {
                        "authorId": "2912641",
                        "name": "I. Kavasidis"
                    },
                    {
                        "authorId": "46792082",
                        "name": "S. Palazzo"
                    },
                    {
                        "authorId": "2441118",
                        "name": "C. Spampinato"
                    },
                    {
                        "authorId": "145587660",
                        "name": "C. Pino"
                    },
                    {
                        "authorId": "144027622",
                        "name": "D. Giordano"
                    },
                    {
                        "authorId": "39097156",
                        "name": "D. Giuffrida"
                    },
                    {
                        "authorId": "2060572853",
                        "name": "P. Messina"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6c411f52c151fe537ee76c310f45f0dd3da88e92",
                "externalIds": {
                    "DBLP": "series/sbcs/Bhowmik23",
                    "DOI": "10.1007/978-981-99-4277-0",
                    "CorpusId": 260337124
                },
                "corpusId": 260337124,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6c411f52c151fe537ee76c310f45f0dd3da88e92",
                "title": "Document Layout Analysis",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3272848",
                        "name": "Showmik Bhowmik"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "68d8fbea8a714717974237278b2afb3cf7996ca8",
                "externalIds": {
                    "DBLP": "journals/tmm/BiXSLLZQ23",
                    "DOI": "10.1109/TMM.2022.3165717",
                    "CorpusId": 248045381
                },
                "corpusId": 248045381,
                "publicationVenue": {
                    "id": "10e76a35-58d6-443c-9683-fc16f2dd0a92",
                    "name": "IEEE transactions on multimedia",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Transactions on Multimedia",
                        "IEEE Trans Multimedia",
                        "IEEE trans multimedia"
                    ],
                    "issn": "1520-9210",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046"
                },
                "url": "https://www.semanticscholar.org/paper/68d8fbea8a714717974237278b2afb3cf7996ca8",
                "title": "SRRV: A Novel Document Object Detector Based on Spatial-Related Relation and Vision",
                "abstract": "Document object detection is a challenging task due to layout complexity and object diversity. Most of existing methods mainly focus on vision information, neglecting representative inherent spatial-related relationship among document objects. To capture structural information and contextual dependencies, we propose a novel document object detector based on spatial-related relation and vision (SRRV). It consists of three parts: vision feature extraction network, relation feature aggregation network and result refinement network. Vision feature extraction network enhances information propagation of hierarchical feature pyramid by adopting feature augmentation paths. Then, relation feature aggregation network combines graph construction module and graph learning module. Specifically, graph construction module calculates spatial information from geometric attributes of region proposals to encode relation information, while graph learning module stacks Graph Convolutional Network (GCN) layers to aggregate relation information at global scale. Both the vision and relation features are fed into result refinement network for feature fusion and relational reasoning. Experiments on the PubLayNet, POD and Article Regions datasets demonstrate that spatial relation information improves the performance with better accuracy and more precise bounding box prediction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2135080215",
                        "name": "Hengyue Bi"
                    },
                    {
                        "authorId": "2648125",
                        "name": "Canhui Xu"
                    },
                    {
                        "authorId": "1452349425",
                        "name": "Cao Shi"
                    },
                    {
                        "authorId": "47061847",
                        "name": "Guozhu Liu"
                    },
                    {
                        "authorId": "2162399223",
                        "name": "Yuteng Li"
                    },
                    {
                        "authorId": "2162481723",
                        "name": "Honghong Zhang"
                    },
                    {
                        "authorId": "2069480186",
                        "name": "Jing Qu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "915 for the tablestructure recognition and data extraction task, which outperformed that of a deep neural network-based method, DeepDeSRT (Schreiber et al., 2017), by 0.",
                "As a result, their method achieved an F-measure value of 0.915 for the tablestructure recognition and data extraction task, which outperformed that of a deep neural network-based method, DeepDeSRT (Schreiber et al., 2017), by 0.07 percentage points."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fc846fd6cd62b1bcb7bd73314ff4c18a3182bb74",
                "externalIds": {
                    "DBLP": "conf/icpram/AoyagiKTUO22",
                    "DOI": "10.5220/0010817700003122",
                    "CorpusId": 246966938
                },
                "corpusId": 246966938,
                "publicationVenue": {
                    "id": "8ef5945c-5b25-4774-b55a-15cd5450f6e4",
                    "name": "International Conference on Pattern Recognition Applications and Methods",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Pattern Recognit Appl Method",
                        "ICPRAM"
                    ],
                    "url": "http://icpram.org/"
                },
                "url": "https://www.semanticscholar.org/paper/fc846fd6cd62b1bcb7bd73314ff4c18a3182bb74",
                "title": "Table-structure Recognition Method Consisting of Plural Neural Network Modules",
                "abstract": "In academic papers, tables are often used to summarize experimental results. However, graphs are more suitable than tables for grasping many experimental results at a glance because of the high visibility. Therefore, automatic graph generation from a table has been studied. Because the structure and style of a table vary depending on the authors, this paper proposes a table-structure recognition method using plural neural network (NN) modules. The proposed method consists of four NN modules: two of them merge detected tokens in a table, one estimates implicit ruled lines that are necessary to separate cells but undrawn, and the last estimates cells by merging the tokens. We demonstrated the effectiveness of the proposed method by experiments using the ICDAR 2013 table competition dataset. Consequently, the proposed method achieved an F-measure of 0.972, outperforming those of our earlier work (Ohta et al., 2021) by 1.7 percentage points and of the topranked participant in that competition by 2.6 percentage points.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144827799",
                        "name": "Hiroyuki Aoyagi"
                    },
                    {
                        "authorId": "1894424",
                        "name": "T. Kanazawa"
                    },
                    {
                        "authorId": "144752372",
                        "name": "A. Takasu"
                    },
                    {
                        "authorId": "7672960",
                        "name": "Fumito Uwano"
                    },
                    {
                        "authorId": "46592126",
                        "name": "Manabu Ohta"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44220fb755ed8259a6d1183e47e5e957ab5b0874",
                "externalIds": {
                    "CorpusId": 253857301
                },
                "corpusId": 253857301,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/44220fb755ed8259a6d1183e47e5e957ab5b0874",
                "title": "Relative Layout Matching for Document Data Extraction",
                "abstract": "This thesis explores the field of business document information extraction, emphasizing one-shot learning systems that improve their performance by utilizing a database of previously processed documents. A benchmark to evaluate one-shot information extraction systems was defined and used with a newly created dataset. A novel representation-learning approach to one-shot document information extraction was proposed. For a newly received document, the proposed approach uses learned document representation to first retrieve field representations from similar documents. Retrieved representations are then used to localize information on the newly received document. The proposed method was evaluated and compared against several proposed baselines showing an improvement on fields with high positional variance. The baseline method still achieves better results on fields that remain fixed within the layout.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071515342",
                        "name": "Maty\u00e1s Skalick\u00fd"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2016) and (Schreiber et al. 2017) proposed table detection model in document image based on CNN and Faster R-CNN, respectively.",
                "(Hao et al. 2016) and (Schreiber et al. 2017) proposed table detection model in document image based on CNN and Faster R-CNN, respectively."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "36772cb19d16c69849c8c578d9680ac7271e2ae5",
                "externalIds": {
                    "DBLP": "conf/aaai/LeeHK21",
                    "CorpusId": 232369460
                },
                "corpusId": 232369460,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/36772cb19d16c69849c8c578d9680ac7271e2ae5",
                "title": "Data Augmentations for Document Images",
                "abstract": "Data augmentation has the potential to significantly improve the generalization capability of deep neural networks. Especially in image recognition, recent augmentation techniques such as Mixup, CutOut, CutMix, and RandAugment have shown great performance improvement. These augmentation techniques have also shown effectiveness in semi-supervised learning or self-supervised learning. Despite of these effects and usefulness, these techniques cannot be applied directly to document image analysis, which require text semantic feature preservation. To tackle this problem, we propose novel augmentation methods, DocCutout and DocCutMix, that are more suitable for document images, by applying the transform to each word unit and thus preserving text semantic feature during augmentation. We conduct intensive experiments to find the most effective data augmentation techniques among various approaches for document object detection and show our proposed augmentation methods outperform stateof-the-arts with +1.77 AP in PubMed dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46357976",
                        "name": "Yunsung Lee"
                    },
                    {
                        "authorId": "3431201",
                        "name": "Teakgyu Hong"
                    },
                    {
                        "authorId": "2099537",
                        "name": "Seungryong Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[18] proposed DeepDeSRT that employs the Faster R-CNN model for table detection and a semantic segmentation approach for structure recognition."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ae184fcffda938790b0eb40a3957ea96cd9da0c8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-11879",
                    "CorpusId": 235187264
                },
                "corpusId": 235187264,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ae184fcffda938790b0eb40a3957ea96cd9da0c8",
                "title": "Tab.IAIS: Flexible Table Recognition and Semantic Interpretation System",
                "abstract": "Table extraction is an important but still unsolved problem. In this paper, we introduce a flexible end-to-end table extraction system. We develop two rule-based algorithms that perform the complete table recognition process and support the most frequent table formats found in the scientific literature. Moreover, to incorporate the extraction of semantic information into the table recognition process, we develop a graph-based table interpretation method. We conduct extensive experiments on the challenging table recognition benchmarks ICDAR 2013 and ICDAR 2019. Our table recognition approach achieves results competitive with state-of-the-art approaches. Moreover, our complete information extraction system exhibited a high F1 score of 0.7380 proving the utility of our approach.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "134442417",
                        "name": "Marcin Namysl"
                    },
                    {
                        "authorId": "153486145",
                        "name": "Alexander M. Esser"
                    },
                    {
                        "authorId": "1699019",
                        "name": "Sven Behnke"
                    },
                    {
                        "authorId": "152162939",
                        "name": "Joachim Kohler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] for TE to data-driven methods based on deep learning (DL) [2, 10, 11]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "17119b49d68800bbc57bbd786ca339917c6f00fa",
                "externalIds": {
                    "CorpusId": 238253171
                },
                "corpusId": 238253171,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/17119b49d68800bbc57bbd786ca339917c6f00fa",
                "title": "Scientific evidence extraction",
                "abstract": "Recently, interest has grown in applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, progress in this area has been challenging both to make and to measure, due to several issues that arise in training and evaluating models from labeled data. This includes challenges as fundamental as the lack of a single definitive ground truth output for each input sample and the lack of an ideal metric for measuring partial correctness for this task. To address these we propose a new dataset, PubMed Tables One Million (PubTables-1M), and a new class of metric, grid table similarity (GriTS). PubTables-1M is nearly twice as large as the previous largest comparable dataset, can be used for models across multiple architectures and modalities, and addresses issues such as ambiguity and lack of consistency in the annotations. We apply DETR [1] to table extraction for the first time and show that object detection models trained on PubTables-1M produce excellent results out-of-the-box for all three tasks of detection, structure recognition, and functional analysis. We describe the dataset in detail to enable others to build on our work and combine this data with other datasets for these and related tasks. It is our hope that PubTables-1M and the proposed metrics can further progress in this area by creating a benchmark suitable for training and evaluating a wide variety of models for table extraction. Data and code will be released at https: //github.com/microsoft/table-transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2394344",
                        "name": "Robin Abraham"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] for TE to data-driven methods based on deep learning (DL) [2, 10, 11]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2773c7621402cf014b9871796f0e3ba788a95f60",
                "externalIds": {
                    "CorpusId": 238634624
                },
                "corpusId": 238634624,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2773c7621402cf014b9871796f0e3ba788a95f60",
                "title": "PubTables-1M: Towards a universal dataset and metrics for training and evaluating table extraction models",
                "abstract": "Recently, interest has grown in applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, progress in this area has been challenging both to make and to measure, due to several issues that arise in training and evaluating models from labeled data. This includes challenges as fundamental as the lack of a single definitive ground truth output for each input sample and the lack of an ideal metric for measuring partial correctness for this task. To address these issues we propose a new dataset, PubMed Tables One Million (PubTables-1M), and a new class of metric, grid table similarity (GriTS). PubTables-1M is nearly twice as large as the previous largest comparable dataset, contains highly-detailed structure annotations, and can be used for models across multiple architectures and modalities. Further, it addresses issues such as ambiguity and lack of consistency in the annotations via a novel canonicalization and quality control procedure. We apply DETR [1] to table extraction for the first time and show that object detection models trained on PubTables-1M produce excellent results out-of-the-box for all three tasks of detection, structure recognition, and functional analysis. It is our hope that PubTables-1M and GriTS can further progress in this area by creating data and metrics suitable for training and evaluating a wide variety of models for table extraction. Data and code will be released at https://github.com/microsoft/table-transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2394344",
                        "name": "Robin Abraham"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[20] use a fine-tuned version of popular object detection framework, Faster RCNN [4], to detect tables in document images."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d22282974925d53d57ab11bcd91f8dc27c2f9f4a",
                "externalIds": {
                    "DBLP": "conf/icdar/DeshpandePS21",
                    "DOI": "10.1007/978-3-030-86198-8_9",
                    "CorpusId": 237457885
                },
                "corpusId": 237457885,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d22282974925d53d57ab11bcd91f8dc27c2f9f4a",
                "title": "MediTables: A New Dataset and Deep Network for Multi-category Table Localization in Medical Documents",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2090545649",
                        "name": "A. Deshpande"
                    },
                    {
                        "authorId": "2126073962",
                        "name": "Vaishnav Potlapalli"
                    },
                    {
                        "authorId": "2126074083",
                        "name": "R. Sarvadevabhatla"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[5] first discovered that deep learning (DL) based \u201cobject detection\u201d [6] in natural scene images can be successfully applied to the table detection in document images via the transfer learning."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bd58579901b2894f03bbe46edd9d81ab3f7548ad",
                "externalIds": {
                    "DBLP": "conf/itams/ShigarovDMPY21",
                    "CorpusId": 240005427
                },
                "corpusId": 240005427,
                "publicationVenue": {
                    "id": "8e09466c-7dda-4a37-811b-dd680962bf60",
                    "name": "Scientific-practical Workshop Information Technologies: Algorithms, Models, Systems",
                    "type": "conference",
                    "alternate_names": [
                        "ITAMS",
                        "Sci Workshop Inf Technol Algorithm Model Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd58579901b2894f03bbe46edd9d81ab3f7548ad",
                "title": "Table extraction, analysis, and interpretation: the current state of the TabbyDOC project",
                "abstract": "The freely available tabular data represented in various digital formats, such as print-oriented documents, spreadsheets, and web pages, are a valuable source to populate knowledge graphs. However, difficulties that inevitably arise with the extraction and integration of the tabular data often hinder their intensive use in practice. TabbyDOC project aims at elaborating a theoretical basis and developing open software for data extraction from arbitrary tables. Previously, it was devoted to the following issues: (i) table extraction tables from print-oriented documents, (ii) data transformation from spreadsheet tables to relational and linked data. This paper summarizes the project\u2019s results that are intended for the following tasks: (i) automation of fine-tuning artificial neural networks for table detection in document images, (ii) a synthesis of programs for spreadsheet data transformation driven by user-defined rules of table analysis and interpretation, and (iii) generating RDF-triples from entities extracted from relational tables.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3355714",
                        "name": "A. Shigarov"
                    },
                    {
                        "authorId": "46251605",
                        "name": "N. Dorodnykh"
                    },
                    {
                        "authorId": "144116982",
                        "name": "A. Mikhailov"
                    },
                    {
                        "authorId": "144364060",
                        "name": "Viacheslav V. Paramonov"
                    },
                    {
                        "authorId": "1956220",
                        "name": "A. Y. Yurin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cab5eaa396843ab72df8dfbde49591217b07be25",
                "externalIds": {
                    "CorpusId": 245928163
                },
                "corpusId": 245928163,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cab5eaa396843ab72df8dfbde49591217b07be25",
                "title": "Accessibility of Tables in PDF Documents: Issues, Challenges, and Future Directions",
                "abstract": "People access and share information over the web and in other digital environments, including digital libraries, in the form of documents such as books, articles, technical reports, etc. These documents are in a variety of formats, of which the Portable Document Format (PDF) is most widely used because of its emphasis on preserving the layout of the original material. The retrieval of relevant material from these derivative documents is challenging for information retrieval (IR) because the rich semantic structure of these documents is lost. The retrieval of important units such as images, figures, algorithms, mathematical formulas, and tables becomes a challenge. Among these elements, tables are particularly important because they can add value to the resource description, discovery, and accessibility of documents not only on the web but also in libraries if they are made retrievable and presentable to readers. Sighted users comprehend tables for sensemaking using visual cues, but blind and visually impaired users must rely on assistive technologies, including textto-speech and screen readers, to comprehend tables. However, these technologies do not pay sufficient attention to tables in order to effectively present tables to visually impaired individuals. Therefore, ways must be found to make tables in PDF documents not only retrievable but also comprehensible. Before developing such solutions, it is necessary to review the available assistive technologies, tools, and frameworks for their capabilities, strengths, and limitations from the comprehension perspective of blind and visually impaired people, along with suitable environments like digital libraries. We found no such review article that critically and analytically presents and evaluates these technologies. To fill this gap in the literature, this review paper reports on the current state of the accessibility of PDF documents, digital libraries, assistive technologies, tools, and frameworks that make PDF tables comprehensible and accessible to blind and visually impaired people. The study findings have implications for libraries, information sciences, and information retrieval.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "80236432",
                        "name": "N. Fayyaz"
                    },
                    {
                        "authorId": "3351931",
                        "name": "Shah Khusro"
                    },
                    {
                        "authorId": "1665494560",
                        "name": "Shakir Ullah"
                    }
                ]
            }
        },
        {
            "contexts": [
                "From 2013 to date, in addition to the enhancement of these deterministic approaches, several new techniques based on machine learning algorithms have been introduced, like a method based on the identification of the horizontal and vertical lines classified through Support Vector Machine (SVM) [15] or methods based on Fast-RCNN (FRCNN) [34] trained on the Marmot dataset for table recognition [35]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f65e67307aaf545cef0a561e06902dd333c61e89",
                "externalIds": {
                    "DBLP": "conf/aiia/MassarentiL21",
                    "CorpusId": 247614379
                },
                "corpusId": 247614379,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f65e67307aaf545cef0a561e06902dd333c61e89",
                "title": "A Deep Learning based Methodology for Information Extraction from Documents in Robotic Process Automation",
                "abstract": "In recent years, thanks to Optical Character Recognition techniques and technologies to deal with low scan quality and complex document structure, there has been a continuous evolution and automation of the digitization processes to allow Robotic Process Automation. In this paper we propose a methodology based both on deep learning algorithms (as generative adversarial network) and statistical tools (as the Hough transform) for the creation of a digitization system capable of managing critical issues, like low scan quality and complex structure of documents. The methodology is composed of 5 modules to manage the poor quality of scanned documents, identify the template and detect tables in documents, extract and organize the text into an easy-to-query schema and perform queries on it through search patterns. For each module different state-of-the-art algorithms are compared and analyzed, with the aim of identifying the best solution to be adopted in an industrial environment. The implemented methodology is measured with respect to the business needs over real data by comparing the extracted information with the target value and shows performance of 90%, in terms of Gestalt Pattern Matching measure.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2049049676",
                        "name": "Nicola Massarenti"
                    },
                    {
                        "authorId": "2159679725",
                        "name": "Giorgio Lazzarinetti"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] 34 for table extraction to data-driven methods based on deep learning (DL) [2, 10, 11]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4f723c06164b3eacfb6787e9cd732e04a31e5a93",
                "externalIds": {
                    "CorpusId": 259311838
                },
                "corpusId": 259311838,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4f723c06164b3eacfb6787e9cd732e04a31e5a93",
                "title": "Towards a universal dataset and metrics for training and evaluating table extraction models",
                "abstract": "Recently, interest has grown in applying machine learning approaches to the 1 problem of table structure inference and extraction from unstructured documents. 2 However, progress in this area has been challenging not only to make but to 3 measure, due to several issues that arise in both training and evaluating such 4 systems from labeled data. This includes challenges as fundamental as the lack of 5 a single definitive ground truth output for a given input sample and the lack of an 6 ideal metric for measuring partial correctness for this task. To address these we 7 propose a new dataset, PubMed Tables One Million (PubTables1M), and a new 8 class of metric, grid table similarity (GriTS). PubTables1M is nearly twice as large 9 as the current largest comparable dataset, can be used for models across multiple 10 architectures and modalities, and addresses issues such as ambiguity and lack of 11 consistency in the annotations. We apply DETR [1] to table extraction for the first 12 time and show that object detection models trained on images and bounding boxes 13 derived from this data produce excellent results out-of-the-box for all three tasks of 14 detection, structure recognition, and functional analysis. In addition to releasing 15 the data, we describe the dataset creation process in detail to enable others to build 16 on our work and to ensure forward and backward compatibility of this data for 17 combining it with other datasets created for these tasks. It is our hope that this data 18 and the proposed metrics can further progress in this area by serving as a single 19 source of data for training and evaluation of a wide variety of models for table 20 extraction. 21",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51981031",
                        "name": "Microsoft"
                    },
                    {
                        "authorId": "2063070799",
                        "name": "Redmond"
                    }
                ]
            }
        },
        {
            "contexts": [
                "With the recent and rapid development of deep learning techniques, approaches employing convolutional neural networks (CNNs) have been proposed and have shown a superior detection accuracy [1], [2], [11].",
                "However, table detection is a challenging task due to the high degree of intra-class variability and inter-class similarity between tables, as shown in 1(b) [2].",
                "[2] proposed a table detection system applying a completely data-driven method that does not require metadata of the document or heuristic rules for detection."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bf0c7fb03f910f30a96b4060c91a115a462a6cfa",
                "externalIds": {
                    "DBLP": "journals/access/KimH20a",
                    "MAG": "3021344331",
                    "DOI": "10.1109/ACCESS.2020.2990901",
                    "CorpusId": 218651430
                },
                "corpusId": 218651430,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bf0c7fb03f910f30a96b4060c91a115a462a6cfa",
                "title": "A Rule-Based Method for Table Detection in Website Images",
                "abstract": "Table detection is an essential part of a document analysis because tables are among the most efficient methods for systematically summarizing information. Therefore, numerous studies on detecting tables not only from documents but also from websites have been conducted. Although, the number of websites has been growing explosively recently, most of these studies suffer from detecting tables which are image types rather than tagging due to the variability of size, contents, color, and shapes. In this paper, we propose an efficient yet robust method for detecting tables in image formats, which can apply to both documents and websites. Instead of employing recently developed deep learning methods, which require extensive training for diversity, we apply a rule-based detection method by using key features of many tables, namely, the grid format of the text provided in the tables. The proposed method consists of two stages: a feature extraction stage and a grid pattern recognition stage. In the first stage, we extract the features of the contents in the tables. We then remove the features of non-text objects and texts not included in tables. In the second stage, we build tree structures from the features and apply a novel algorithm for determining the grid pattern. When we applied our method to a website dataset, the experimental results showed a precision, recall, and F1-measure of 84.5%, 72%, and 0.778, which are improvements of 3.6%, 24.16%, and 0.276 over a previous method, respectively, while also achieving the fastest processing time. In addition, the proposed rule-based method allows the structure of the contents in the table to be easily restored.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1698939296",
                        "name": "Jihu Kim"
                    },
                    {
                        "authorId": "2073600754",
                        "name": "Hyoseok Hwang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "36145fd6281884a8e59ec3cc8d76c22fcb388628",
                "externalIds": {
                    "DBLP": "conf/das/2020",
                    "DOI": "10.1007/978-3-030-57058-3",
                    "CorpusId": 221128794
                },
                "corpusId": 221128794,
                "publicationVenue": {
                    "id": "02d53b80-30d7-493c-9453-ed7406056b31",
                    "name": "International Workshop on Document Analysis Systems",
                    "type": "conference",
                    "alternate_names": [
                        "DAS",
                        "Document Analysis Systems",
                        "Int Workshop Doc Anal Syst",
                        "Doc Anal Syst"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=647"
                },
                "url": "https://www.semanticscholar.org/paper/36145fd6281884a8e59ec3cc8d76c22fcb388628",
                "title": "Document Analysis Systems: 14th IAPR International Workshop, DAS 2020, Wuhan, China, July 26\u201329, 2020, Proceedings",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145905113",
                        "name": "X. Bai"
                    },
                    {
                        "authorId": "1694974",
                        "name": "Dimosthenis Karatzas"
                    },
                    {
                        "authorId": "1828940",
                        "name": "D. Lopresti"
                    },
                    {
                        "authorId": "1743774",
                        "name": "E. Bertino"
                    }
                ]
            }
        },
        {
            "contexts": [
                "And we can see these problems in the multitude of methods currently available to detect tables in PDF documents [26, 27, 28, 29], where there is no author who proposes a general deterministic solution to identify tables in PDF documents[25].",
                "Trying to teach computers to see and also\u201cunderstand\u201d what is a table has proven be extremely difficult [25]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "110f8eadf2808029668e164408dc58d3968038fb",
                "externalIds": {
                    "CorpusId": 222122977
                },
                "corpusId": 222122977,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/110f8eadf2808029668e164408dc58d3968038fb",
                "title": "Acurio Machine Learning applied to improve accessibility of PDF documents for Visually Impaired Users",
                "abstract": "Digital documents are accessed by visually impaired people (VIP) through screen readers. Traditionally, digital documents were translated to braille text, but screen readers have proved to be efficient for the acquisition of digital document knowledge by VIP. However, screen readers and other assistive technologies have significant limitations when there exist tables in digital documents such as portable document format (PDF). For instance, screen readers can not follow the correct reading sequence of the table based on its visual structure causing this content is inaccessible for VIP. In order to deal with this problem, in this work, we developed a system for the retrieval of table information from PDF documents for use in screen readers used by visually impaired people. The proposed methodology takes advantage of computer vision techniques with a deep learning approach to make documents accessible instead of the classical rule-based programming approach. We explained in detail the methodology that we used and how to objectively evaluate the approach through entropy, information gain, and purity metrics. The results show that our proposed methodology can be used to reduce the uncertainty experienced by visually impaired people when listening to the contents of tables in digital documents through screen readers. Our table information retrieval system presents two improvements compared with traditional approaches of tagging text-based PDF files. First, our approach does not require supervision by sighted people. Second, our system is capable of working with image-based as well as text-based PDFs. Key-words: Accessibility, computer vision, deep learning, statistical approach.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "90417505",
                        "name": "By A. Acu\u00f1a"
                    },
                    {
                        "authorId": "143681414",
                        "name": "L. Martini"
                    },
                    {
                        "authorId": "2111119207",
                        "name": "Meire da Silva"
                    },
                    {
                        "authorId": "2090132230",
                        "name": "Luiz Cesar Martini Orientador"
                    },
                    {
                        "authorId": "48652983",
                        "name": "Y. Iano"
                    },
                    {
                        "authorId": "2075042897",
                        "name": "Felipe Leonel Grijalva Arevalo"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", paragraphs, figures, lists, tables [49,41,30].",
                "While there are other works [34,41] that perform table decomposition into rows and columns (which our model is capable of doing), we discuss table detection only in the scope of this paper."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e4506cdc954ad4eb3d752dd75b342aea0701df54",
                "externalIds": {
                    "MAG": "3109115753",
                    "DBLP": "conf/eccv/SarkarAJGK20",
                    "DOI": "10.1007/978-3-030-58604-1_39",
                    "CorpusId": 221771142
                },
                "corpusId": 221771142,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/e4506cdc954ad4eb3d752dd75b342aea0701df54",
                "title": "Document Structure Extraction Using Prior Based High Resolution Hierarchical Semantic Segmentation",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "39230373",
                        "name": "Mausoom Sarkar"
                    },
                    {
                        "authorId": "6657914",
                        "name": "Milan Aggarwal"
                    },
                    {
                        "authorId": "1432235684",
                        "name": "Arneh Jain"
                    },
                    {
                        "authorId": "1381295186",
                        "name": "Hiresh Gupta"
                    },
                    {
                        "authorId": "145846953",
                        "name": "Balaji Krishnamurthy"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0d7a65f01299aa6105e8d8aeccd9ee04cbb19d6c",
                "externalIds": {
                    "DBLP": "conf/iniscom/2020",
                    "DOI": "10.1007/978-3-030-63083-6",
                    "CorpusId": 227076558
                },
                "corpusId": 227076558,
                "publicationVenue": {
                    "id": "96b61289-172a-48b7-b9c0-93b525b45575",
                    "name": "International Conference on Industrial Networks and Intelligent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Ind Netw Intell Syst",
                        "INISCOM"
                    ],
                    "issn": "2411-7129",
                    "url": "https://eudl.eu/proceedings/INISCom/2015"
                },
                "url": "https://www.semanticscholar.org/paper/0d7a65f01299aa6105e8d8aeccd9ee04cbb19d6c",
                "title": "Industrial Networks and Intelligent Systems: 6th EAI International Conference, INISCOM 2020, Hanoi, Vietnam, August 27\u201328, 2020, Proceedings",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1834516",
                        "name": "Nguyen-Son Vo"
                    },
                    {
                        "authorId": "2190756",
                        "name": "Van\u2010Phuc Hoang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d760645d5363cf8d1309d0cb71d5cd1d47f34d1d",
                "externalIds": {
                    "DBLP": "conf/iniscom/NguyenVZBDNLH20",
                    "MAG": "3110339029",
                    "DOI": "10.1007/978-3-030-63083-6_12",
                    "CorpusId": 229183023
                },
                "corpusId": 229183023,
                "publicationVenue": {
                    "id": "96b61289-172a-48b7-b9c0-93b525b45575",
                    "name": "International Conference on Industrial Networks and Intelligent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Ind Netw Intell Syst",
                        "INISCOM"
                    ],
                    "issn": "2411-7129",
                    "url": "https://eudl.eu/proceedings/INISCom/2015"
                },
                "url": "https://www.semanticscholar.org/paper/d760645d5363cf8d1309d0cb71d5cd1d47f34d1d",
                "title": "Table Structure Recognition in Scanned Images Using a Clustering Method",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2757688",
                        "name": "N. V. Nguyen"
                    },
                    {
                        "authorId": "1752866615",
                        "name": "Hanh Vu"
                    },
                    {
                        "authorId": "2070874816",
                        "name": "Arthur Zucker"
                    },
                    {
                        "authorId": "2037496520",
                        "name": "Younes Belkada"
                    },
                    {
                        "authorId": "49343821",
                        "name": "H. Do"
                    },
                    {
                        "authorId": "2037502069",
                        "name": "Doanh Ngoc-Nguyen"
                    },
                    {
                        "authorId": "47267436",
                        "name": "T. T. Le"
                    },
                    {
                        "authorId": "31854609",
                        "name": "D. V. Hoang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The solution is suggested to be a deep neural network, which typically consists of several stacked deep neural networks ([2]), which reduces the problem to the object detection task.",
                "In particular, deep neural networks and machine learning were shown to be very effective in the context of tabulating (see [2], [3], [4], [5])."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "62654e547c9e35f8b9624c91c26da85b91a7e1cf",
                "externalIds": {
                    "DOI": "10.26583/sv.12.5.01",
                    "CorpusId": 233422309
                },
                "corpusId": 233422309,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/62654e547c9e35f8b9624c91c26da85b91a7e1cf",
                "title": "Solutions to recognize the table structure by an image in the absence of a priori information",
                "abstract": "In this paper, we consider the problem of recognizing a table structure through the analysis of the provided picture. The problem statement is the following: we have a photo with an unknown number of particular objects captured, and we know that they are arranged in a flat table structure. It is assumed that the provided picture complies reasonable restrictions concerning perspective distortion and rotation magnitudes. The goal is to recognize the underlying table structure, i.e., to arrange the recognized objects into some table structure that appropriately fits the picture. From now on, we call this procedure the tabulating of the objects. This paper then considers the task of tabulating objects under the conditions of the absence of any antecedent information concerning the table structure, except for the actual picture.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "32367049",
                        "name": "N. Besshaposhnikov"
                    },
                    {
                        "authorId": "145776847",
                        "name": "A. Leonov"
                    },
                    {
                        "authorId": "113612488",
                        "name": "M. A. Matyushin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "250409dd5a5b656d7710737897719adb7f1acdf1",
                "externalIds": {
                    "MAG": "3150029667",
                    "DBLP": "conf/cvip/SharmaBSS20",
                    "DOI": "10.1007/978-981-16-1103-2_1",
                    "CorpusId": 234220841
                },
                "corpusId": 234220841,
                "publicationVenue": {
                    "id": "73dc1b2c-812e-4118-be6b-501cf924387d",
                    "name": "International Conference on Computer Vision and Image Processing",
                    "type": "conference",
                    "alternate_names": [
                        "CVIP",
                        "Int Conf Comput Vis Image Process"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/250409dd5a5b656d7710737897719adb7f1acdf1",
                "title": "U-Net-Based Approach for Segmentation of Tables from Scanned Pages",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2134580300",
                        "name": "Ravish Sharma"
                    },
                    {
                        "authorId": "2090239363",
                        "name": "Romit Bhattacharrya"
                    },
                    {
                        "authorId": "35773525",
                        "name": "R. Sanyal"
                    },
                    {
                        "authorId": "35140464",
                        "name": "S. Sanyal"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "aa111c8920e963195968360f59c9de271ae470c2",
                "externalIds": {
                    "CorpusId": 236923196
                },
                "corpusId": 236923196,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/aa111c8920e963195968360f59c9de271ae470c2",
                "title": "BROS: A PRE-TRAINED LANGUAGE MODEL",
                "abstract": "Understanding document from their visual snapshots is an emerging and challenging problem that requires both advanced computer vision and NLP methods. Although the recent advance in OCR enables the accurate extraction of text segments, it is still challenging to extract key information from documents due to the diversity of layouts. To compensate for the difficulties, this paper introduces a pre-trained language model, BERT Relying On Spatiality (BROS), that represents and understands the semantics of spatially distributed texts. Different from previous pre-training methods on 1D text, BROS is pre-trained on large-scale semistructured documents with a novel area-masking strategy while efficiently including the spatial layout information of input documents. Also, to generate structured outputs in various document understanding tasks, BROS utilizes a powerful graphbased decoder that can capture the relation between text segments. BROS achieves state-of-the-art results on four benchmark tasks: FUNSD, SROIE*, CORD, and SciTSR. Our experimental settings and implementation codes will be publicly available.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7109b10475c4287aa84498c36b8ac7b54a4f7c6d",
                "externalIds": {
                    "MAG": "2972173400",
                    "CorpusId": 202981663
                },
                "corpusId": 202981663,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7109b10475c4287aa84498c36b8ac7b54a4f7c6d",
                "title": "Scanning Single Shot Detector for Math in Document Images",
                "abstract": "Scanning Single Shot Detector for Math in Document Images Parag Shrikrishna Mali, M.S. Rochester Institute of Technology, 2019 Supervisor: Dr. Richard Zanibbi We introduce the Scanning Single Shot Detector (ScanSSD) for detecting both embedded and displayed math expressions in document images using a single-stage network that does not require page layout, font, or, character information. ScanSSD uses sliding windows to generate sub-images of large document page images rendered at 600 dpi and applies Single Shot Detector (SSD) on each sub-image. Detection results from sub-images are pooled to generate page-level results. For pooling sub-image level detections, we introduce new methods based on the confidence scores and density of detections. ScanSSD is a modular architecture that can be easily applied to detecting other objects in document images.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1379555980",
                        "name": "P. Mali"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As contemporary works [47, 26, 2, 58] show the deep neural network models provide a good performance for the table detection.",
                "The current trend involves deep learning techniques: binary classification based on convolutional neural networks [29], fine-tuned object detection models [47, 26, 2, 58], and semantic segmentation [30, 33]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fd2fef193e5a4f4bbed9c89d4b18af5f7364ea7d",
                "externalIds": {
                    "MAG": "2982329385",
                    "DBLP": "conf/itams/ShigarovCCDKMPR19",
                    "CorpusId": 204832269
                },
                "corpusId": 204832269,
                "publicationVenue": {
                    "id": "8e09466c-7dda-4a37-811b-dd680962bf60",
                    "name": "Scientific-practical Workshop Information Technologies: Algorithms, Models, Systems",
                    "type": "conference",
                    "alternate_names": [
                        "ITAMS",
                        "Sci Workshop Inf Technol Algorithm Model Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fd2fef193e5a4f4bbed9c89d4b18af5f7364ea7d",
                "title": "Towards End-to-End Transformation of Arbitrary Tables from Untagged Portable Documents (PDF) to Linked Data",
                "abstract": "The paper is devoted to the problem of an end-to-end table transformation from untagged portable documents (PDF) to linked data. It covers the issues of the table extraction from documents, the reconstruction of logical table structure, the conceptualization of their natural-language content, and the linking of extracted data with external vocabularies. We consider some perspective approaches for the deeplearning-based table detection, heuristic-based table structure recognition, rule-based table analysis, and knowledge-based table interpretation. They can be used as a basis to develop a consistent solution for this problem. Our application experience confirms that such solutions are demanded for populating databases and generating ontologies with tabular data being extracted from weakly and semi-structured documents.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "3355714",
                        "name": "A. Shigarov"
                    },
                    {
                        "authorId": "13719083",
                        "name": "Igor Cherepanov"
                    },
                    {
                        "authorId": "51224234",
                        "name": "Evgeniy A. Cherkashin"
                    },
                    {
                        "authorId": "46251605",
                        "name": "N. Dorodnykh"
                    },
                    {
                        "authorId": "83453745",
                        "name": "Vasiliy V. Khristyuk"
                    },
                    {
                        "authorId": "144116982",
                        "name": "A. Mikhailov"
                    },
                    {
                        "authorId": "144364060",
                        "name": "Viacheslav V. Paramonov"
                    },
                    {
                        "authorId": "1379973520",
                        "name": "Egor Rozhkow"
                    },
                    {
                        "authorId": "27772662",
                        "name": "A. Yurin"
                    }
                ]
            }
        }
    ]
}