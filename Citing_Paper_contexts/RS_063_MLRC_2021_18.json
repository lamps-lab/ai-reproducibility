{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4c06a9a328fb098351ec18d43d2c5194995481a4",
                "externalIds": {
                    "DBLP": "journals/isci/LiZZZ23",
                    "DOI": "10.1016/j.ins.2023.119335",
                    "CorpusId": 259247937
                },
                "corpusId": 259247937,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/4c06a9a328fb098351ec18d43d2c5194995481a4",
                "title": "Deep discriminative causal domain generalization",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "31436351",
                        "name": "Shanshan Li"
                    },
                    {
                        "authorId": "7345071",
                        "name": "Qingjie Zhao"
                    },
                    {
                        "authorId": "2115693524",
                        "name": "Changchun Zhang"
                    },
                    {
                        "authorId": "2220606593",
                        "name": "Yuanbing Zou"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "40a456a55932a9c02343864b6c15fc44b4548f12",
                "externalIds": {
                    "ArXiv": "2309.16483",
                    "CorpusId": 263135518
                },
                "corpusId": 263135518,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/40a456a55932a9c02343864b6c15fc44b4548f12",
                "title": "Rethinking Domain Generalization: Discriminability and Generalizability",
                "abstract": "Domain generalization (DG) endeavors to develop robust models that possess strong generalizability while preserving excellent discriminability. Nonetheless, pivotal DG techniques tend to improve the feature generalizability by learning domain-invariant representations, inadvertently overlooking the feature discriminability. On the one hand, the simultaneous attainment of generalizability and discriminability of features presents a complex challenge, often entailing inherent contradictions. This challenge becomes particularly pronounced when domain-invariant features manifest reduced discriminability owing to the inclusion of unstable factors, \\emph{i.e.,} spurious correlations. On the other hand, prevailing domain-invariant methods can be categorized as category-level alignment, susceptible to discarding indispensable features possessing substantial generalizability and narrowing intra-class variations. To surmount these obstacles, we rethink DG from a new perspective that concurrently imbues features with formidable discriminability and robust generalizability, and present a novel framework, namely, Discriminative Microscopic Distribution Alignment (DMDA). DMDA incorporates two core components: Selective Channel Pruning~(SCP) and Micro-level Distribution Alignment (MDA). Concretely, SCP attempts to curtail redundancy within neural networks, prioritizing stable attributes conducive to accurate classification. This approach alleviates the adverse effect of spurious domain invariance and amplifies the feature discriminability. Besides, MDA accentuates micro-level alignment within each class, going beyond mere category-level alignment. This strategy accommodates sufficient generalizable features and facilitates within-class variations. Extensive experiments on four benchmark datasets corroborate the efficacy of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2247773849",
                        "name": "Shaocong Long"
                    },
                    {
                        "authorId": "67190665",
                        "name": "Qianyu Zhou"
                    },
                    {
                        "authorId": "51265775",
                        "name": "Chenhao Ying"
                    },
                    {
                        "authorId": "2248349676",
                        "name": "Lizhuang Ma"
                    },
                    {
                        "authorId": "1683396",
                        "name": "Yuan Luo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, due to the existence of hard samples, the pursuit of invariant features and indiscriminate elimination of domain-specific information can lead to adverse effects on within-class variations and amplify the empirical source risk, ultimately undermining the model\u2019s generalization performance [44], [45]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9ab354c1d73e29bb52aeb2b2e60186b91859ed9a",
                "externalIds": {
                    "ArXiv": "2309.16460",
                    "CorpusId": 263143347
                },
                "corpusId": 263143347,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9ab354c1d73e29bb52aeb2b2e60186b91859ed9a",
                "title": "Diverse Target and Contribution Scheduling for Domain Generalization",
                "abstract": "Generalization under the distribution shift has been a great challenge in computer vision. The prevailing practice of directly employing the one-hot labels as the training targets in domain generalization~(DG) can lead to gradient conflicts, making it insufficient for capturing the intrinsic class characteristics and hard to increase the intra-class variation. Besides, existing methods in DG mostly overlook the distinct contributions of source (seen) domains, resulting in uneven learning from these domains. To address these issues, we firstly present a theoretical and empirical analysis of the existence of gradient conflicts in DG, unveiling the previously unexplored relationship between distribution shifts and gradient conflicts during the optimization process. In this paper, we present a novel perspective of DG from the empirical source domain's risk and propose a new paradigm for DG called Diverse Target and Contribution Scheduling (DTCS). DTCS comprises two innovative modules: Diverse Target Supervision (DTS) and Diverse Contribution Balance (DCB), with the aim of addressing the limitations associated with the common utilization of one-hot labels and equal contributions for source domains in DG. In specific, DTS employs distinct soft labels as training targets to account for various feature distributions across domains and thereby mitigates the gradient conflicts, and DCB dynamically balances the contributions of source domains by ensuring a fair decline in losses of different source domains. Extensive experiments with analysis on four benchmark datasets show that the proposed method achieves a competitive performance in comparison with the state-of-the-art approaches, demonstrating the effectiveness and advantages of the proposed DTCS.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2247773849",
                        "name": "Shaocong Long"
                    },
                    {
                        "authorId": "67190665",
                        "name": "Qianyu Zhou"
                    },
                    {
                        "authorId": "51265775",
                        "name": "Chenhao Ying"
                    },
                    {
                        "authorId": "2248349676",
                        "name": "Lizhuang Ma"
                    },
                    {
                        "authorId": "1683396",
                        "name": "Yuan Luo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There have been many works on the unsupervised domain adaptation (UDA) [10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21], the multi-target domain adaptation (MTDA) [22; 23; 23; 23; 24; 25] and the domain generalization (DG) [26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37] for semantic segmentation."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8e879ffbf68681ca65b1e6922ca6066c0086b10f",
                "externalIds": {
                    "ArXiv": "2309.16127",
                    "CorpusId": 263135652
                },
                "corpusId": 263135652,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8e879ffbf68681ca65b1e6922ca6066c0086b10f",
                "title": "Open Compound Domain Adaptation with Object Style Compensation for Semantic Segmentation",
                "abstract": "Many methods of semantic image segmentation have borrowed the success of open compound domain adaptation. They minimize the style gap between the images of source and target domains, more easily predicting the accurate pseudo annotations for target domain's images that train segmentation network. The existing methods globally adapt the scene style of the images, whereas the object styles of different categories or instances are adapted improperly. This paper proposes the Object Style Compensation, where we construct the Object-Level Discrepancy Memory with multiple sets of discrepancy features. The discrepancy features in a set capture the style changes of the same category's object instances adapted from target to source domains. We learn the discrepancy features from the images of source and target domains, storing the discrepancy features in memory. With this memory, we select appropriate discrepancy features for compensating the style information of the object instances of various categories, adapting the object styles to a unified style of source domain. Our method enables a more accurate computation of the pseudo annotations for target domain's images, thus yielding state-of-the-art results on different datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2247941321",
                        "name": "Tingliang Feng"
                    },
                    {
                        "authorId": null,
                        "name": "Hao Shi"
                    },
                    {
                        "authorId": "2248151413",
                        "name": "Xueyang Liu"
                    },
                    {
                        "authorId": "2249738768",
                        "name": "Wei Feng"
                    },
                    {
                        "authorId": "2248136951",
                        "name": "Liang Wan"
                    },
                    {
                        "authorId": "2248685247",
                        "name": "Yanlin Zhou"
                    },
                    {
                        "authorId": "2248561428",
                        "name": "Di Lin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7fc07a581048aeea18805b632d1252f6ebc00df4",
                "externalIds": {
                    "DBLP": "journals/imwut/WangGCY23",
                    "DOI": "10.1145/3610877",
                    "CorpusId": 263153616
                },
                "corpusId": 263153616,
                "publicationVenue": {
                    "id": "4c51a870-1809-485b-8c20-3c1326b3fe16",
                    "name": "Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies",
                    "alternate_names": [
                        "Proc ACM Interact Mob Wearable Ubiquitous Technol"
                    ],
                    "issn": "2474-9567",
                    "url": "https://dl.acm.org/journal/imwut",
                    "alternate_urls": [
                        "http://imwut.acm.org/",
                        "https://dl.acm.org/pub.cfm?id=J1566"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7fc07a581048aeea18805b632d1252f6ebc00df4",
                "title": "sUrban",
                "abstract": "Recent machine learning research on smart cities has achieved great success in predicting future trends, under the key assumption that the test data follows the same distribution of the training data. The rapid urbanization, however, makes this assumption challenging to hold in practice. Because new data is emerging from new environments (e.g., an emerging city or region), which may follow different distributions from data in existing environments. Different from transfer-learning methods accessing target data during training, we often do not have any prior knowledge about the new environment. Therefore, it is critical to explore a predictive model that can be effectively adapted to unseen new environments. This work aims to address this Out-of-Distribution (OOD) challenge for sustainable cities. We propose to identify two kinds of features that are useful for OOD prediction in each environment: (1) the environment-invariant features to capture the shared commonalities for predictions across different environments; and (2) the environment-aware features to characterize the unique information of each environment. Take bike riding as an example. The bike demands of different cities often follow the same pattern that they significantly increase during the rush hour on workdays. Meanwhile, there are also some local patterns in each city because of different cultures and citizens' travel preferences. We introduce a principled framework -- sUrban -- that consists of an environment-invariant optimization module for learning invariant representation and an environment-aware optimization module for learning environment-aware representation. Evaluation on real-world datasets from various urban application domains corroborates the generalizability of sUrban. This work opens up new avenues to smart city development.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2606838",
                        "name": "Qianru Wang"
                    },
                    {
                        "authorId": "2153414163",
                        "name": "Bin Guo"
                    },
                    {
                        "authorId": "2248181944",
                        "name": "Lu Cheng"
                    },
                    {
                        "authorId": "2118174528",
                        "name": "Zhiwen Yu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d4fba10db7b4c8912cea3aa9a7fbdeb1587f1092",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-09888",
                    "ArXiv": "2309.09888",
                    "DOI": "10.48550/arXiv.2309.09888",
                    "CorpusId": 261959395
                },
                "corpusId": 261959395,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d4fba10db7b4c8912cea3aa9a7fbdeb1587f1092",
                "title": "Context is Environment",
                "abstract": "Two lines of work are taking the central stage in AI research. On the one hand, the community is making increasing efforts to build models that discard spurious correlations and generalize better in novel test environments. Unfortunately, the bitter lesson so far is that no proposal convincingly outperforms a simple empirical risk minimization baseline. On the other hand, large language models (LLMs) have erupted as algorithms able to learn in-context, generalizing on-the-fly to eclectic contextual circumstances that users enforce by means of prompting. In this paper, we argue that context is environment, and posit that in-context learning holds the key to better domain generalization. Via extensive theory and experiments, we show that paying attention to context$\\unicode{x2013}\\unicode{x2013}$unlabeled examples as they arrive$\\unicode{x2013}\\unicode{x2013}$allows our proposed In-Context Risk Minimization (ICRM) algorithm to zoom-in on the test environment risk minimizer, leading to significant out-of-distribution performance improvements. From all of this, two messages are worth taking home. Researchers in domain generalization should consider environment as context, and harness the adaptive power of in-context learning. Researchers in LLMs should consider context as environment, to better structure data towards generalization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2242501778",
                        "name": "Sharut Gupta"
                    },
                    {
                        "authorId": "2242248678",
                        "name": "Stefanie Jegelka"
                    },
                    {
                        "authorId": "1401804750",
                        "name": "David Lopez-Paz"
                    },
                    {
                        "authorId": "2242252645",
                        "name": "Kartik Ahuja"
                    }
                ]
            }
        },
        {
            "contexts": [
                "mismatch is to take the semantic labels into account [114], [115]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "576a3159d6c3f646d6fda6d047dfece4ea941fdd",
                "externalIds": {
                    "ArXiv": "2309.07438",
                    "DBLP": "journals/corr/abs-2309-07438",
                    "DOI": "10.48550/arXiv.2309.07438",
                    "CorpusId": 261822539
                },
                "corpusId": 261822539,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/576a3159d6c3f646d6fda6d047dfece4ea941fdd",
                "title": "Towards Artificial General Intelligence (AGI) in the Internet of Things (IoT): Opportunities and Challenges",
                "abstract": "Artificial General Intelligence (AGI), possessing the capacity to comprehend, learn, and execute tasks with human cognitive abilities, engenders significant anticipation and intrigue across scientific, commercial, and societal arenas. This fascination extends particularly to the Internet of Things (IoT), a landscape characterized by the interconnection of countless devices, sensors, and systems, collectively gathering and sharing data to enable intelligent decision-making and automation. This research embarks on an exploration of the opportunities and challenges towards achieving AGI in the context of the IoT. Specifically, it starts by outlining the fundamental principles of IoT and the critical role of Artificial Intelligence (AI) in IoT systems. Subsequently, it delves into AGI fundamentals, culminating in the formulation of a conceptual framework for AGI's seamless integration within IoT. The application spectrum for AGI-infused IoT is broad, encompassing domains ranging from smart grids, residential environments, manufacturing, and transportation to environmental monitoring, agriculture, healthcare, and education. However, adapting AGI to resource-constrained IoT settings necessitates dedicated research efforts. Furthermore, the paper addresses constraints imposed by limited computing resources, intricacies associated with large-scale IoT communication, as well as the critical concerns pertaining to security and privacy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2240538675",
                        "name": "Fei Dou"
                    },
                    {
                        "authorId": "2241081066",
                        "name": "Jin Ye"
                    },
                    {
                        "authorId": "2240529410",
                        "name": "Geng Yuan"
                    },
                    {
                        "authorId": "2240766814",
                        "name": "Qin Lu"
                    },
                    {
                        "authorId": "2240534539",
                        "name": "Wei Niu"
                    },
                    {
                        "authorId": "2214271343",
                        "name": "Haijian Sun"
                    },
                    {
                        "authorId": "2240557067",
                        "name": "Le Guan"
                    },
                    {
                        "authorId": "2240532673",
                        "name": "Guoyu Lu"
                    },
                    {
                        "authorId": "2240534117",
                        "name": "Gengchen Mai"
                    },
                    {
                        "authorId": "2238404369",
                        "name": "Ninghao Liu"
                    },
                    {
                        "authorId": "2240550872",
                        "name": "Jin Lu"
                    },
                    {
                        "authorId": "2145977326",
                        "name": "Zheng Liu"
                    },
                    {
                        "authorId": "47039788",
                        "name": "Zihao Wu"
                    },
                    {
                        "authorId": "50979840",
                        "name": "Chenjiao Tan"
                    },
                    {
                        "authorId": "2211904452",
                        "name": "Shaochen Xu"
                    },
                    {
                        "authorId": "2214283393",
                        "name": "Xianqiao Wang"
                    },
                    {
                        "authorId": "2240843637",
                        "name": "Guoming Li"
                    },
                    {
                        "authorId": "2240528378",
                        "name": "Lilong Chai"
                    },
                    {
                        "authorId": "2153703773",
                        "name": "Sheng Li"
                    },
                    {
                        "authorId": "2214298401",
                        "name": "Jin Sun"
                    },
                    {
                        "authorId": "2240853173",
                        "name": "Hongyue Sun"
                    },
                    {
                        "authorId": "2240766399",
                        "name": "Yunli Shao"
                    },
                    {
                        "authorId": "2166099489",
                        "name": "Changying Li"
                    },
                    {
                        "authorId": "2115345993",
                        "name": "Tianming Liu"
                    },
                    {
                        "authorId": "2107484804",
                        "name": "Wenzhan Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In particular, they are devoted to either acquiring invariant causal mechanisms [17] or recovering causal features [27, 2] for image classification problem."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5d99b2e3467d6a903654334b108c3290a74f66e2",
                "externalIds": {
                    "ArXiv": "2308.12218",
                    "DBLP": "journals/corr/abs-2308-12218",
                    "DOI": "10.48550/arXiv.2308.12218",
                    "CorpusId": 261076371
                },
                "corpusId": 261076371,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5d99b2e3467d6a903654334b108c3290a74f66e2",
                "title": "CIParsing: Unifying Causality Properties into Multiple Human Parsing",
                "abstract": "Existing methods of multiple human parsing (MHP) apply statistical models to acquire underlying associations between images and labeled body parts. However, acquired associations often contain many spurious correlations that degrade model generalization, leading statistical models to be vulnerable to visually contextual variations in images (e.g., unseen image styles/external interventions). To tackle this, we present a causality inspired parsing paradigm termed CIParsing, which follows fundamental causal principles involving two causal properties for human parsing (i.e., the causal diversity and the causal invariance). Specifically, we assume that an input image is constructed by a mix of causal factors (the characteristics of body parts) and non-causal factors (external contexts), where only the former ones cause the generation process of human parsing.Since causal/non-causal factors are unobservable, a human parser in proposed CIParsing is required to construct latent representations of causal factors and learns to enforce representations to satisfy the causal properties. In this way, the human parser is able to rely on causal factors w.r.t relevant evidence rather than non-causal factors w.r.t spurious correlations, thus alleviating model degradation and yielding improved parsing ability. Notably, the CIParsing is designed in a plug-and-play fashion and can be integrated into any existing MHP models. Extensive experiments conducted on two widely used benchmarks demonstrate the effectiveness and generalizability of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144182733",
                        "name": "Xiaojia Chen"
                    },
                    {
                        "authorId": "9764377",
                        "name": "Xuanhan Wang"
                    },
                    {
                        "authorId": "2671321",
                        "name": "Lianli Gao"
                    },
                    {
                        "authorId": "2233229262",
                        "name": "Beitao Chen"
                    },
                    {
                        "authorId": "2346105",
                        "name": "Jingkuan Song"
                    },
                    {
                        "authorId": "2233143470",
                        "name": "HenTao Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is reasonable because the causal factors for decision-making are often stable patterns that persist across samples and domains [44]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "934c25c60fbb19bf7c34657fcd0eb6e5b1b5f6ea",
                "externalIds": {
                    "ArXiv": "2308.11158",
                    "DBLP": "journals/corr/abs-2308-11158",
                    "DOI": "10.48550/arXiv.2308.11158",
                    "CorpusId": 261064978
                },
                "corpusId": 261064978,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/934c25c60fbb19bf7c34657fcd0eb6e5b1b5f6ea",
                "title": "Domain Generalization via Rationale Invariance",
                "abstract": "This paper offers a new perspective to ease the challenge of domain generalization, which involves maintaining robust results even in unseen environments. Our design focuses on the decision-making process in the final classifier layer. Specifically, we propose treating the element-wise contributions to the final results as the rationale for making a decision and representing the rationale for each sample as a matrix. For a well-generalized model, we suggest the rationale matrices for samples belonging to the same category should be similar, indicating the model relies on domain-invariant clues to make decisions, thereby ensuring robust results. To implement this idea, we introduce a rationale invariance loss as a simple regularization technique, requiring only a few lines of code. Our experiments demonstrate that the proposed approach achieves competitive results across various datasets, despite its simplicity. Code is available at \\url{https://github.com/liangchen527/RIDG}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Liang Chen"
                    },
                    {
                        "authorId": "2144290563",
                        "name": "Yong Zhang"
                    },
                    {
                        "authorId": "2255687",
                        "name": "Yibing Song"
                    },
                    {
                        "authorId": "5546141",
                        "name": "A. Hengel"
                    },
                    {
                        "authorId": "2161037",
                        "name": "Lingqiao Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Deep neural networks (DNNs) have shown impressive performance in computer vision tasks over the past few years.",
                "greatly impaired the applications of DNNs [33, 61], as training and test data often come from different distributions in reality.",
                "The work is supported by NSFC Program (62222604, 62206052, 62192783), China Postdoctoral Science Foundation Project (2023T160100), Jiangsu Natural Science Foundation Project (BK20210224), and CCF-Lenovo Bule Ocean Research Fund.\ngreatly impaired the applications of DNNs [33, 61], as training and test data often come from different distributions in reality."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9dbeee78d7f1d9bf3c3b001632c8b4883d861094",
                "externalIds": {
                    "ArXiv": "2308.10285",
                    "DBLP": "journals/corr/abs-2308-10285",
                    "DOI": "10.48550/arXiv.2308.10285",
                    "CorpusId": 261049380
                },
                "corpusId": 261049380,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9dbeee78d7f1d9bf3c3b001632c8b4883d861094",
                "title": "DomainDrop: Suppressing Domain-Sensitive Channels for Domain Generalization",
                "abstract": "Deep Neural Networks have exhibited considerable success in various visual tasks. However, when applied to unseen test datasets, state-of-the-art models often suffer performance degradation due to domain shifts. In this paper, we introduce a novel approach for domain generalization from a novel perspective of enhancing the robustness of channels in feature maps to domain shifts. We observe that models trained on source domains contain a substantial number of channels that exhibit unstable activations across different domains, which are inclined to capture domain-specific features and behave abnormally when exposed to unseen target domains. To address the issue, we propose a DomainDrop framework to continuously enhance the channel robustness to domain shifts, where a domain discriminator is used to identify and drop unstable channels in feature maps of each network layer during forward propagation. We theoretically prove that our framework could effectively lower the generalization bound. Extensive experiments on several benchmarks indicate that our framework achieves state-of-the-art performance compared to other competing methods. Our code is available at https://github.com/lingeringlight/DomainDrop.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50115584",
                        "name": "Jintao Guo"
                    },
                    {
                        "authorId": "1785352346",
                        "name": "Lei Qi"
                    },
                    {
                        "authorId": "2475959",
                        "name": "Yinghuan Shi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following this line, some works [34, 38, 39, 63, 74] achieve the goal by the pre-defined causal graph to learn the key features involving class labels."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "86d11be5dbe8ae139a16049d11f715ea592416f1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-15889",
                    "DOI": "10.1145/3580305.3599481",
                    "CorpusId": 258888052
                },
                "corpusId": 258888052,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/86d11be5dbe8ae139a16049d11f715ea592416f1",
                "title": "Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization",
                "abstract": "Domain generalization (DG) is a prevalent problem in real-world applications, which aims to train well-generalized models for unseen target domains by utilizing several source domains. Since domain labels, i.e., which domain each data point is sampled from, naturally exist, most DG algorithms treat them as a kind of supervision information to improve the generalization performance. However, the original domain labels may not be the optimal supervision signal due to the lack of domain heterogeneity, i.e., the diversity among domains. For example, a sample in one domain may be closer to another domain, its original label thus can be the noise to disturb the generalization learning. Although some methods try to solve it by re-dividing domains and applying the newly generated dividing pattern, the pattern they choose may not be the most heterogeneous due to the lack of the metric for heterogeneity. In this paper, we point out that domain heterogeneity mainly lies in variant features under the invariant learning framework. With contrastive learning, we propose a learning potential-guided metric for domain heterogeneity by promoting learning variant features. Then we notice the differences between seeking variance-based heterogeneity and training invariance-based generalizable model. We thus propose a novel method called H eterogeneity-based Two-stage Contrastive Learning (HTCL) for the DG task. In the first stage, we generate the most heterogeneous dividing pattern with our contrastive metric. In the second stage, we employ an invariance-aimed contrastive learning by re-building pairs with the stable relation hinted by domains and classes, which better utilizes generated domain labels for generalization learning. Extensive experiments show HTCL better digs heterogeneity and yields great generalization performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2154972909",
                        "name": "Yunze Tong"
                    },
                    {
                        "authorId": "38511927",
                        "name": "Junkun Yuan"
                    },
                    {
                        "authorId": "2157502742",
                        "name": "Min Zhang"
                    },
                    {
                        "authorId": "1477669613",
                        "name": "Di-hua Zhu"
                    },
                    {
                        "authorId": "2119058743",
                        "name": "Keli Zhang"
                    },
                    {
                        "authorId": "93192602",
                        "name": "Fei Wu"
                    },
                    {
                        "authorId": "33870528",
                        "name": "Kun Kuang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, many past works also utilize causality to find stable features across domains [1, 28]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13a575170d7658923925db4d4516efbbf367a11c",
                "externalIds": {
                    "DBLP": "conf/kdd/ZhangGYC23",
                    "DOI": "10.1145/3580305.3599377",
                    "CorpusId": 260499885
                },
                "corpusId": 260499885,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/13a575170d7658923925db4d4516efbbf367a11c",
                "title": "Hierarchical Invariant Learning for Domain Generalization Recommendation",
                "abstract": "Most cross-domain recommenders require samples on target domains or source-target overlaps to carry out domain adaptation. However, in many real-world situations, target domains are lack of such knowledge. Few works discuss this problem, whose essence is domain generalization recommendation. In this paper, we figure out domain generalization recommendation with a clear symbolized definition and propose corresponding models. Moreover, we illustrate its strong connection with zero-shot recommendation, pretrained recommendation and cold-start recommendation, distinguishing it from content-based recommendation. By analyzing its properties, we propose HIRL^+ and a series of heuristic methods to solve this problem. We propose hierarchical invariant learning to expel the specific patterns in both domain-level and environment-level, and find the common patterns in generalization space. To make the division of environments flexible, fine-grained and balanced, we put forward a learnable environment assignment method. To improve the robustness against distribution shifts inside domain generalization, we present an adversarial environment refinement method. In addition, we conduct experiments on real-word datasets to verify the effectiveness of our models, and carry out further studies on the domain distance and domain diversity. To benefit the research community and promote this direction, we discuss the future of this field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118689375",
                        "name": "Zeyu Zhang"
                    },
                    {
                        "authorId": "2160700215",
                        "name": "Heyang Gao"
                    },
                    {
                        "authorId": "102320235",
                        "name": "Hao Yang"
                    },
                    {
                        "authorId": "2144230136",
                        "name": "Xu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 Fundamental of Causal Robustness [20] \u2022 Causal Data Augmentation [2] \u2022 Invariant Causal Representation Learning [4, 14] \u2022 Invariant Causal Mechanism Learning [1] (5) Conclusion and Future Trends Finally, we conclude our tutorial with a discussion on open challenges and future directions with respect to the following: \u2022 Future directions and Promises: Causality, security, and privacy \u2022 Challenges: Limitation of data and unidentifiability"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4dcd680cc4306e7bc28d9b807e6d91352170c1bc",
                "externalIds": {
                    "DBLP": "conf/kdd/MoraffahKRL23",
                    "DOI": "10.1145/3580305.3599571",
                    "CorpusId": 260499709
                },
                "corpusId": 260499709,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/4dcd680cc4306e7bc28d9b807e6d91352170c1bc",
                "title": "Socially Responsible Machine Learning: A Causal Perspective",
                "abstract": "The evergrowing reliance of humans and society on machine learning methods has raised concerns about their trustworthiness and liability. As a response to these concerns, Socially Responsible Machine Learning (SRML) aims at developing fair, transparent, and robust machine learning algorithms. However, traditional approaches to SRML do not incorporate human perspectives, and therefore are not sufficient to build long-lasting trust between machines and human being. Causality as the key to human intelligence plays a vital role in achieving socially responsible machine learning algorithms which are compatible with human notions. Bridging the gap between traditional SRML and causality, in this tutorial, we aim at providing a holistic overview of SRML through the lens of causality. In particular, we will focus on state-of-the-art techniques on causal socially responsible ML in terms of fairness, interpretability, and robustness. The objectives of this tutorial are as follows: (1) we provide a taxonomy of existing literature on causal socially responsible ML from fairness, interpretability, and robustness perspective; (2) we review the state-of-the-art techniques for each task; and (3) we elucidate open questions and future research directions. We believe this tutorial is beneficial to researchers and practitioners from the areas of data mining, machine learning, and social sciences.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "11064745",
                        "name": "Raha Moraffah"
                    },
                    {
                        "authorId": "145926563",
                        "name": "Amir-Hossein Karimi"
                    },
                    {
                        "authorId": "19251475",
                        "name": "A. Raglin"
                    },
                    {
                        "authorId": "2146398099",
                        "name": "Huan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The extent of overlap between these representations indicates the model\u2019s ability to extract invariant features, which are crucial for generalization [17].",
                "Causality is recently explored to capture the invariance across different platforms [16, 17]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c4fe5949355e4eb5c5684566f134a155fe3e3312",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-02080",
                    "ArXiv": "2308.02080",
                    "DOI": "10.48550/arXiv.2308.02080",
                    "CorpusId": 260611122
                },
                "corpusId": 260611122,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c4fe5949355e4eb5c5684566f134a155fe3e3312",
                "title": "Causality Guided Disentanglement for Cross-Platform Hate Speech Detection",
                "abstract": "Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causal relationships, which remain constant across diverse environments, can significantly aid in understanding invariant representations in hate speech. By disentangling input into platform-dependent features (useful for predicting hate targets) and platform-independent features (used to predict the presence of hate), we learn invariant representations resistant to distribution shifts. These features are then used to predict hate speech across unseen platforms. Our extensive experiments across four platforms highlight our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "73409823",
                        "name": "Paras Sheth"
                    },
                    {
                        "authorId": "40899329",
                        "name": "Tharindu Kumarage"
                    },
                    {
                        "authorId": "11064745",
                        "name": "Raha Moraffah"
                    },
                    {
                        "authorId": "118679299",
                        "name": "Amanat Chadha"
                    },
                    {
                        "authorId": "2146398099",
                        "name": "Huan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3580d855dfe151885a3515cd818eb300bb33fb37",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-15199",
                    "ArXiv": "2307.15199",
                    "DOI": "10.48550/arXiv.2307.15199",
                    "CorpusId": 260316052
                },
                "corpusId": 260316052,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3580d855dfe151885a3515cd818eb300bb33fb37",
                "title": "PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization",
                "abstract": "In a joint vision-language space, a text feature (e.g., from\"a photo of a dog\") could effectively represent its relevant image features (e.g., from dog photos). Also, a recent study has demonstrated the cross-modal transferability phenomenon of this joint space. From these observations, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. The proposed method learns to generate a variety of style features (from\"a S* style of a\") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from\"a S* style of a [class]\") to be located nearby their corresponding content features (from\"[class]\") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, even though it does not require any images for training.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149468483",
                        "name": "Junhyeong Cho"
                    },
                    {
                        "authorId": "2182292362",
                        "name": "Gilhyun Nam"
                    },
                    {
                        "authorId": "2144247302",
                        "name": "Sungyeon Kim"
                    },
                    {
                        "authorId": "97598866",
                        "name": "Hunmin Yang"
                    },
                    {
                        "authorId": "2483916",
                        "name": "Suha Kwak"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[42] CLS 2021 generalizability Causal inference Domain (I)"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4814a0b3afb8b45749d7408aa798e2f389dd8aef",
                "externalIds": {
                    "ArXiv": "2307.13992",
                    "DBLP": "journals/corr/abs-2307-13992",
                    "DOI": "10.48550/arXiv.2307.13992",
                    "CorpusId": 260164880
                },
                "corpusId": 260164880,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4814a0b3afb8b45749d7408aa798e2f389dd8aef",
                "title": "Causal reasoning in typical computer vision tasks",
                "abstract": "Deep learning has revolutionized the field of artificial intelligence. Based on the statistical correlations uncovered by deep learning-based methods, computer vision has contributed to tremendous growth in areas like autonomous driving and robotics. Despite being the basis of deep learning, such correlation is not stable and is susceptible to uncontrolled factors. In the absence of the guidance of prior knowledge, statistical correlations can easily turn into spurious correlations and cause confounders. As a result, researchers are now trying to enhance deep learning methods with causal theory. Causal theory models the intrinsic causal structure unaffected by data bias and is effective in avoiding spurious correlations. This paper aims to comprehensively review the existing causal methods in typical vision and vision-language tasks such as semantic segmentation, object detection, and image captioning. The advantages of causality and the approaches for building causal paradigms will be summarized. Future roadmaps are also proposed, including facilitating the development of causal theory and its application in other complex scenes and systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2162701921",
                        "name": "Kexuan Zhang"
                    },
                    {
                        "authorId": "2163757516",
                        "name": "Qiyu Sun"
                    },
                    {
                        "authorId": "1454206187",
                        "name": "Chaoqiang Zhao"
                    },
                    {
                        "authorId": "2111285394",
                        "name": "Yang Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mainstream Domain Generalization studies [1,23,28,32, 33, 33, 42] primarily focus on extracting invariant representations from source domains that can be effectively generalized to the target domain, which remains inaccessible during training.",
                "Hence, with reference to [5, 33], we make the following assumption for components of the Fourier transform:"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "577ff3a8c665998670c8114d10022a1a494481ae",
                "externalIds": {
                    "ArXiv": "2307.12622",
                    "DBLP": "journals/corr/abs-2307-12622",
                    "DOI": "10.48550/arXiv.2307.12622",
                    "CorpusId": 260704076
                },
                "corpusId": 260704076,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/577ff3a8c665998670c8114d10022a1a494481ae",
                "title": "Phase Matching for Out-of-Distribution Generalization",
                "abstract": "The Fourier transform, serving as an explicit decomposition method for visual signals, has been employed to explain the out-of-distribution generalization behaviors of Convolutional Neural Networks (CNNs). Previous studies have indicated that the amplitude spectrum is susceptible to the disturbance caused by distribution shifts. On the other hand, the phase spectrum preserves highly-structured spatial information, which is crucial for robust visual representation learning. However, the spatial relationships of phase spectrum remain unexplored in previous research. In this paper, we aim to clarify the relationships between Domain Generalization (DG) and the frequency components, and explore the spatial relationships of the phase spectrum. Specifically, we first introduce a Fourier-based structural causal model which interprets the phase spectrum as semi-causal factors and the amplitude spectrum as non-causal factors. Then, we propose Phase Matching (PhaMa) to address DG problems. Our method introduces perturbations on the amplitude spectrum and establishes spatial relationships to match the phase components. Through experiments on multiple benchmarks, we demonstrate that our proposed method achieves state-of-the-art performance in domain generalization and out-of-distribution robustness tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "11577774",
                        "name": "Chengming Hu"
                    },
                    {
                        "authorId": "2132788144",
                        "name": "Ye Du"
                    },
                    {
                        "authorId": "2151036763",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "2242179244",
                        "name": "Hao Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5b21ca2aa0d65fc4edf2a10d74b29029ac5d7401",
                "externalIds": {
                    "DOI": "10.23919/CCC58697.2023.10239995",
                    "CorpusId": 262071359
                },
                "corpusId": 262071359,
                "publicationVenue": {
                    "id": "23f8fe4c-6537-4027-a334-6a5863115984",
                    "name": "Cybersecurity and Cyberforensics Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Chin Control Conf",
                        "Computational Complexity Conference",
                        "CCC",
                        "Comput Complex Conf",
                        "Cybersecur Cyberforensics Conf",
                        "Conference on Computational Complexity",
                        "Computing Colombian Conference",
                        "Conf Comput Complex",
                        "Comput Colomb Conf",
                        "Chinese Control Conference"
                    ],
                    "url": "http://computationalcomplexity.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5b21ca2aa0d65fc4edf2a10d74b29029ac5d7401",
                "title": "Domain Generalized Solar Cell Defect Segmentation Based on Shape-Aware and Multi-View Meta-Learning",
                "abstract": "Various defects are inevitably generated in the manufacturing process of solar cells. Deep learning-based methods for defect segmentation under closed situation have achieved remarkable progress. Due to the difference of imaging condition and camera parameter under different production line, there are large differences in brightness distribution of solar cell images. The model trained under closed situation cannot achieve good performance in opened situation such as multi-production lines. In this paper, a new shape-aware and multi-view meta-learning scheme is proposed to improve the model generalization performance for single-domain solar cell defect segmentation. The scheme roots in gradient-based meta-learning. Multi-brightness view information is yielded from the generated augmented images, and explicitly simulating domain shift with virtual meta-train and meta-test during training to mitigate overfitting in single-source domain train and unstable prediction. Importantly, considering tiny and faint solar cell defects are not easily identified under different image brightness conditions, shape edge constraint loss is proposed to encourage shape compactness and shape smoothness of segmentations. Experimental results show that the proposed method has high segmentation accuracy, which outperforms the state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243322199",
                        "name": "Kun Liu"
                    },
                    {
                        "authorId": "2243305756",
                        "name": "Ying Yang"
                    },
                    {
                        "authorId": "2184620778",
                        "name": "Jingkun Mao"
                    },
                    {
                        "authorId": "2243377940",
                        "name": "Weipeng Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Existing methods mainly fall into three categories: domain generalization [15]\u2013 [17], [41], [42], causal learning [43]\u2013[46] and stable learning [47]\u2013[50]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24dc8882cbfb2cc2155e68c96d26b182c8d778da",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-10997",
                    "ArXiv": "2307.10997",
                    "DOI": "10.48550/arXiv.2307.10997",
                    "CorpusId": 259991287
                },
                "corpusId": 259991287,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/24dc8882cbfb2cc2155e68c96d26b182c8d778da",
                "title": "DREAM: Domain-free Reverse Engineering Attributes of Black-box Model",
                "abstract": "Deep learning models are usually black boxes when deployed on machine learning platforms. Prior works have shown that the attributes ($e.g.$, the number of convolutional layers) of a target black-box neural network can be exposed through a sequence of queries. There is a crucial limitation: these works assume the dataset used for training the target model to be known beforehand and leverage this dataset for model attribute attack. However, it is difficult to access the training dataset of the target black-box model in reality. Therefore, whether the attributes of a target black-box model could be still revealed in this case is doubtful. In this paper, we investigate a new problem of Domain-agnostic Reverse Engineering the Attributes of a black-box target Model, called DREAM, without requiring the availability of the target model's training dataset, and put forward a general and principled framework by casting this problem as an out of distribution (OOD) generalization problem. In this way, we can learn a domain-agnostic model to inversely infer the attributes of a target black-box model with unknown training data. This makes our method one of the kinds that can gracefully apply to an arbitrary domain for model attribute reverse engineering with strong generalization ability. Extensive experimental studies are conducted and the results validate the superiority of our proposed method over the baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119920623",
                        "name": "Rongqing Li"
                    },
                    {
                        "authorId": "2115934774",
                        "name": "Jiaqi Yu"
                    },
                    {
                        "authorId": "2145407298",
                        "name": "Changsheng Li"
                    },
                    {
                        "authorId": "145909988",
                        "name": "Wenhan Luo"
                    },
                    {
                        "authorId": "14886336",
                        "name": "Ye Yuan"
                    },
                    {
                        "authorId": "8349792",
                        "name": "Guoren Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides, several studies [23, 45] have leveraged domain generalization [17, 20, 29], mainly focusing on pretraining a recommender in one domain and applying it in downstream domains."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3eb2dbe48eaa1406ab4e4bb90f094e70eda85738",
                "externalIds": {
                    "DBLP": "conf/sigir/Yang0ZW0CW23",
                    "DOI": "10.1145/3539618.3591624",
                    "CorpusId": 259949780
                },
                "corpusId": 259949780,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/3eb2dbe48eaa1406ab4e4bb90f094e70eda85738",
                "title": "A Generic Learning Framework for Sequential Recommendation with Distribution Shifts",
                "abstract": "Leading sequential recommendation (SeqRec) models adopt empirical risk minimization (ERM) as the learning framework, which inherently assumes that the training data (historical interaction sequences) and the testing data (future interactions) are drawn from the same distribution. However, such i.i.d. assumption hardly holds in practice, due to the online serving and dynamic nature of recommender system.For example, with the streaming of new data, the item popularity distribution would change, and the user preference would evolve after consuming some items. Such distribution shifts could undermine the ERM framework, hurting the model's generalization ability for future online serving. In this work, we aim to develop a generic learning framework to enhance the generalization of recommenders in the dynamic environment. Specifically, on top of ERM, we devise a Distributionally Robust Optimization mechanism for SeqRec (DROS). At its core is our carefully-designed distribution adaption paradigm, which considers the dynamics of data distribution and explores possible distribution shifts between training and testing. Through this way, we can endow the backbone recommenders with better generalization ability.It is worth mentioning that DROS is an effective model-agnostic learning framework, which is applicable to general recommendation scenarios.Theoretical analyses show that DROS enables the backbone recommenders to achieve robust performance in future testing data.Empirical studies verify the effectiveness against dynamic distribution shifts of DROS. Codes are anonymously open-sourced at https://github.com/YangZhengyi98/DROS.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150358651",
                        "name": "Zhengyi Yang"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "2116265843",
                        "name": "Jizhi Zhang"
                    },
                    {
                        "authorId": "1491035012",
                        "name": "Jiancan Wu"
                    },
                    {
                        "authorId": "2113821128",
                        "name": "Xin Xin"
                    },
                    {
                        "authorId": "1452347263",
                        "name": "Jiawei Chen"
                    },
                    {
                        "authorId": "2144796537",
                        "name": "Xiang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Along this line, there are mainly four types of methods: kernel-based methods [3, 11], domain adversarial learning [8], explicit feature alignment [17, 18], and invariant risk minimization [1, 13]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4712ae9c9d9562881d2de243f53bd8e83a2c1594",
                "externalIds": {
                    "DBLP": "conf/sigir/ZhangSWGZ23",
                    "DOI": "10.1145/3539618.3591965",
                    "CorpusId": 259949906
                },
                "corpusId": 259949906,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/4712ae9c9d9562881d2de243f53bd8e83a2c1594",
                "title": "Connecting Unseen Domains: Cross-Domain Invariant Learning in Recommendation",
                "abstract": "As web applications continue to expand and diversify their services, user interactions exist in different scenarios. To leverage this wealth of information, cross-domain recommendation (CDR) has gained significant attention in recent years. However, existing CDR approaches mostly focus on information transfer between observed domains, with little attention paid to generalizing to unseen domains. Although recent research on invariant learning can help for the purpose of generalization, relying only on invariant preference may be overly conservative and result in mediocre performance when the unseen domain shifts slightly. In this paper, we present a novel framework that considers both CDR and domain generalization through a united causal invariant view. We assume that user interactions are determined by domain-invariant preference and domain-specific preference. The proposed approach differentiates the invariant preference and the specific preference from observational behaviors in a way of adversarial learning. Additionally, a novel domain routing module is designed to connect unseen domains to observed domains. Extensive experiments on public and industry datasets have proved the effectiveness of the proposed approach under both CDR and domain generalization settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145953897",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "2180240771",
                        "name": "Yue Shen"
                    },
                    {
                        "authorId": "2152686930",
                        "name": "Dong Wang"
                    },
                    {
                        "authorId": "50771151",
                        "name": "Jinjie Gu"
                    },
                    {
                        "authorId": "119557985",
                        "name": "Guannan Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Invariant risk minimization (IRM) [3] and its variants [2, 11, 21, 25, 29, 34] are originally proposed for out-of-distribution (OOD) generalization [29, 43]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ade4176edb0e88190035215f83c1545a2856ff4f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-08232",
                    "ArXiv": "2307.08232",
                    "DOI": "10.1145/3580305.3599408",
                    "CorpusId": 259937535
                },
                "corpusId": 259937535,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/ade4176edb0e88190035215f83c1545a2856ff4f",
                "title": "Learning for Counterfactual Fairness from Observational Data",
                "abstract": "Fairness-aware machine learning has attracted a surge of attention in many domains, such as online advertising, personalized recommendation, and social media analysis in web applications. Fairness-aware machine learning aims to eliminate biases of learning models against certain subgroups described by certain protected (sensitive) attributes such as race, gender, and age. Among many existing fairness notions, counterfactual fairness is a popular notion defined from a causal perspective. It measures the fairness of a predictor by comparing the prediction of each individual in the original world and that in the counterfactual worlds in which the value of the sensitive attribute is modified. A prerequisite for existing methods to achieve counterfactual fairness is the prior human knowledge of the causal model for the data. However, in real-world scenarios, the underlying causal model is often unknown, and acquiring such human knowledge could be very difficult. In these scenarios, it is risky to directly trust the causal models obtained from information sources with unknown reliability and even causal discovery methods, as incorrect causal models can consequently bring biases to the predictor and lead to unfair predictions. In this work, we address the problem of counterfactually fair prediction from observational data without given causal models by proposing a novel framework CLAIRE. Specifically, under certain general assumptions, CLAIRE effectively mitigates the biases from the sensitive attribute with a representation learning framework based on counterfactual data augmentation and an invariant penalty. Experiments conducted on both synthetic and real-world datasets validate the superiority of CLAIRE in both counterfactual fairness and prediction performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2157405959",
                        "name": "Jing Ma"
                    },
                    {
                        "authorId": "2773849",
                        "name": "Ruocheng Guo"
                    },
                    {
                        "authorId": "2153660635",
                        "name": "Aidong Zhang"
                    },
                    {
                        "authorId": "1737121128",
                        "name": "Jundong Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The concept of causal-invariant prediction (CIP), as defined above, is not a new one and frequently appears in the DG literature in various forms [34, 32].",
                "Theorems 2 and 3 share a similar essence with the theoretical findings of several previous works [32, 34, 58].",
                "[32] propose an iterative algorithm that uses contrastive learning to map images to a latent space, and then match up images from different domains that have the same class label and are close to each other in the latent space.",
                "When contrastive pairs are available, we can directly apply CIP constraints to the feature extractor f as in MatchDG, or apply CIP constraints to the whole model g\u25e6f as in ReLIC and CoRE.",
                "Probability matching is utilized in ReLIC (Representation Learning via Invariant Causal Mechanisms)[34], logit matching is employed in CoRE (Conditional Variance Regularization) [15], and feature matching is used in MatchDG [32].",
                "This is done in CoRE and MatchDG."
            ],
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "65ca19e20cc595c1ba3025efa4500589fae338b7",
                "externalIds": {
                    "ArXiv": "2307.06825",
                    "DBLP": "journals/corr/abs-2307-06825",
                    "DOI": "10.48550/arXiv.2307.06825",
                    "CorpusId": 259847805
                },
                "corpusId": 259847805,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/65ca19e20cc595c1ba3025efa4500589fae338b7",
                "title": "A Causal Framework to Unify Common Domain Generalization Approaches",
                "abstract": "Domain generalization (DG) is about learning models that generalize well to new domains that are related to, but different from, the training domain(s). It is a fundamental problem in machine learning and has attracted much attention in recent years. A large number of approaches have been proposed. Different approaches are motivated from different perspectives, making it difficult to gain an overall understanding of the area. In this paper, we propose a causal framework for domain generalization and present an understanding of common DG approaches in the framework. Our work sheds new lights on the following questions: (1) What are the key ideas behind each DG method? (2) Why is it expected to improve generalization to new domains theoretically? (3) How are different DG methods related to each other and what are relative advantages and limitations? By providing a unified perspective on DG, we hope to help researchers better understand the underlying principles and develop more effective approaches for this critical problem in machine learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3587277",
                        "name": "N. Zhang"
                    },
                    {
                        "authorId": "3249631",
                        "name": "Kaican Li"
                    },
                    {
                        "authorId": "2112514104",
                        "name": "Han Gao"
                    },
                    {
                        "authorId": "4868695",
                        "name": "Weiyan Xie"
                    },
                    {
                        "authorId": "2159234497",
                        "name": "Zhi Lin"
                    },
                    {
                        "authorId": "48459506",
                        "name": "Z. Li"
                    },
                    {
                        "authorId": "2153520311",
                        "name": "Luning Wang"
                    },
                    {
                        "authorId": "2156083887",
                        "name": "Yongxiang Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7b47f512f57a69ace5572b325c13c9d876ed1062",
                "externalIds": {
                    "DOI": "10.1109/ICPRS58416.2023.10179035",
                    "CorpusId": 259978856
                },
                "corpusId": 259978856,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7b47f512f57a69ace5572b325c13c9d876ed1062",
                "title": "Wh-AI-les: Exploring harmonized vision models robustness against distribution shift",
                "abstract": "The remarkable and increasing efficiency of learning-based vision strategies has induced strong paradigm shift in favor of neural architectures that are consequently finding their way into real-world applications with significant impact. Nevertheless, neural networks display a particular brittleness that can significantly hurt performance when deployed outside lab conditions, which is symptomatic of the independent and identically distributed (i.i.d.) hypothesis violation. This lack of consistency across domains is hurtful since it lessens reliability and trustworthiness and could have severe consequences in sensitive environments such as automated driving. Building on recent progresses in vision strategy harmonization and, through a flexible data generation process, this work explores the impact of alignment over robustness for a wide set of models as well as an encouraging theoretical configuration that uses aligned saliency features through a brute-force approach.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1395882562",
                        "name": "Mehdi Mounsif"
                    },
                    {
                        "authorId": "1596866445",
                        "name": "Mohamed Benabdelkrim"
                    },
                    {
                        "authorId": "2388246",
                        "name": "Marie-Anne Bauda"
                    },
                    {
                        "authorId": "2223943291",
                        "name": "Yassine Motie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "casual matching [29], extrinsic-intrinsic interaction [53], balance invariance [2], batch normalization embeddings [45] and multiple latent domain modeling [30]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "efc16345ea3f7fef1a823486ce0c26a3d2c9ccaf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-00371",
                    "ArXiv": "2307.00371",
                    "DOI": "10.48550/arXiv.2307.00371",
                    "CorpusId": 259316957
                },
                "corpusId": 259316957,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/efc16345ea3f7fef1a823486ce0c26a3d2c9ccaf",
                "title": "Learning Content-enhanced Mask Transformer for Domain Generalized Urban-Scene Segmentation",
                "abstract": "Domain-generalized urban-scene semantic segmentation (USSS) aims to learn generalized semantic predictions across diverse urban-scene styles. Unlike domain gap challenges, USSS is unique in that the semantic categories are often similar in different urban scenes, while the styles can vary significantly due to changes in urban landscapes, weather conditions, lighting, and other factors. Existing approaches typically rely on convolutional neural networks (CNNs) to learn the content of urban scenes. In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for domain-generalized USSS. The main idea is to enhance the focus of the fundamental component, the mask attention mechanism, in Transformer segmentation models on content information. To achieve this, we introduce a novel content-enhanced mask attention mechanism. It learns mask queries from both the image feature and its down-sampled counterpart, as lower-resolution image features usually contain more robust content information and are less sensitive to style variations. These features are fused into a Transformer decoder and integrated into a multi-resolution content-enhanced mask attention learning scheme. Extensive experiments conducted on various domain-generalized urban-scene segmentation datasets demonstrate that the proposed CMFormer significantly outperforms existing CNN-based methods for domain-generalized semantic segmentation, achieving improvements of up to 14.00\\% in terms of mIoU (mean intersection over union). The source code for CMFormer will be made available at this \\href{https://github.com/BiQiWHU/domain-generalized-urban-scene-segmentation}{repository}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50367841",
                        "name": "Qi Bi"
                    },
                    {
                        "authorId": "2941564",
                        "name": "Shaodi You"
                    },
                    {
                        "authorId": "1695527",
                        "name": "T. Gevers"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "57cf07309da33bc48c3b89d436f15904fc32c986",
                "externalIds": {
                    "DBLP": "conf/aaai/0001FFFF0LWZZCW23",
                    "DOI": "10.1609/aaai.v37i12.26787",
                    "CorpusId": 259732095
                },
                "corpusId": 259732095,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/57cf07309da33bc48c3b89d436f15904fc32c986",
                "title": "Video-Audio Domain Generalization via Confounder Disentanglement",
                "abstract": "Existing video-audio understanding models are trained and evaluated in an intra-domain setting, facing performance degeneration in real-world applications where multiple domains and distribution shifts naturally exist. The key to video-audio domain generalization (VADG) lies in alleviating spurious correlations over multi-modal features. To achieve this goal, we resort to causal theory and attribute such correlation to confounders affecting both video-audio features and labels. We propose a DeVADG framework that conducts uni-modal and cross-modal deconfounding through back-door adjustment. DeVADG performs cross-modal disentanglement and obtains fine-grained confounders at both class-level and domain-level using half-sibling regression and unpaired domain transformation, which essentially identifies domain-variant factors and class-shared factors that cause spurious correlations between features and false labels. To promote VADG research, we collect a VADG-Action dataset for video-audio action recognition with over 5,000 video clips across four domains (e.g., cartoon and game) and ten action classes (e.g., cooking and riding). We conduct extensive experiments, i.e., multi-source DG, single-source DG, and qualitative analysis, validating the rationality of our causal analysis and the effectiveness of the DeVADG framework.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1739188006",
                        "name": "Shengyu Zhang"
                    },
                    {
                        "authorId": "2222968929",
                        "name": "Xusheng Feng"
                    },
                    {
                        "authorId": "2117590711",
                        "name": "W. Fan"
                    },
                    {
                        "authorId": "104108744",
                        "name": "Wenjing Fang"
                    },
                    {
                        "authorId": "2163400298",
                        "name": "Fuli Feng"
                    },
                    {
                        "authorId": "2072613978",
                        "name": "Wei Ji"
                    },
                    {
                        "authorId": "2117893328",
                        "name": "Shuo Li"
                    },
                    {
                        "authorId": "2222600243",
                        "name": "Li Wang"
                    },
                    {
                        "authorId": "1965885413",
                        "name": "Shanshan Zhao"
                    },
                    {
                        "authorId": "47122432",
                        "name": "Zhou Zhao"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "authorId": "2110922423",
                        "name": "Fei Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "71283cba48ffb39dbaf764b82b7e115ae1b1a6f8",
                "externalIds": {
                    "DOI": "10.1109/icassp49357.2023.10096469",
                    "CorpusId": 258537209
                },
                "corpusId": 258537209,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/71283cba48ffb39dbaf764b82b7e115ae1b1a6f8",
                "title": "Two-Phase Prototypical Contrastive Domain Generalization for Cross-Subject EEG-Based Emotion Recognition",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214445690",
                        "name": "Honghua Cai"
                    },
                    {
                        "authorId": "7588999",
                        "name": "Jiahui Pan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d339ed7c020ce1ea5342ff0cfb46cbf382b29372",
                "externalIds": {
                    "DOI": "10.1109/icassp49357.2023.10095329",
                    "CorpusId": 258536977
                },
                "corpusId": 258536977,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/d339ed7c020ce1ea5342ff0cfb46cbf382b29372",
                "title": "Learning Causal Representations for Generalizable Face Anti Spoofing",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216456122",
                        "name": "Guanghao Zheng"
                    },
                    {
                        "authorId": "2116486976",
                        "name": "Yuchen Liu"
                    },
                    {
                        "authorId": "3207464",
                        "name": "Wenrui Dai"
                    },
                    {
                        "authorId": "144535686",
                        "name": "Chenglin Li"
                    },
                    {
                        "authorId": "38871632",
                        "name": "Junni Zou"
                    },
                    {
                        "authorId": "144045763",
                        "name": "H. Xiong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b1046ae36d76fc5cddb4e87a1e353fa0aaa54eae",
                "externalIds": {
                    "DOI": "10.1109/icassp49357.2023.10097227",
                    "CorpusId": 258540875
                },
                "corpusId": 258540875,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/b1046ae36d76fc5cddb4e87a1e353fa0aaa54eae",
                "title": "Long-Tailed Recognition with Causal Invariant Transformation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108371796",
                        "name": "Yahong Zhang"
                    },
                    {
                        "authorId": "2216541505",
                        "name": "Sheng Shi"
                    },
                    {
                        "authorId": "8794942",
                        "name": "Chenchen Fan"
                    },
                    {
                        "authorId": "2108735515",
                        "name": "Yixin Wang"
                    },
                    {
                        "authorId": "2066307747",
                        "name": "Wenli Ouyang"
                    },
                    {
                        "authorId": "2216468658",
                        "name": "WeiFan"
                    },
                    {
                        "authorId": "2152732801",
                        "name": "Jianping Fan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite the remarkable success, DNNs tend to take shortcuts to learn spurious features [24, 27]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "60aacbc9937b4b0a77ac9645c893e7b663b673c7",
                "externalIds": {
                    "DBLP": "conf/cvpr/MeiZYN23",
                    "DOI": "10.1109/CVPR52729.2023.00731",
                    "CorpusId": 260084593
                },
                "corpusId": 260084593,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/60aacbc9937b4b0a77ac9645c893e7b663b673c7",
                "title": "Exploring and Utilizing Pattern Imbalance",
                "abstract": "In this paper, we identify pattern imbalance from several aspects, and further develop a new training scheme to avert pattern preference as well as spurious correlation. In contrast to prior methods which are mostly concerned with category or domain granularity, ignoring the potential finer structure that existed in datasets, we give a new definition of seed category as an appropriate optimization unit to distinguish different patterns in the same category or domain. Extensive experiments on domain generalization datasets of diverse scales demonstrate the effectiveness of the proposed method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2089571307",
                        "name": "Shibin Mei"
                    },
                    {
                        "authorId": "50015607",
                        "name": "Chenglong Zhao"
                    },
                    {
                        "authorId": "2214192264",
                        "name": "Shengchao Yuan"
                    },
                    {
                        "authorId": "5796401",
                        "name": "Bingbing Ni"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To address the above problem, the widely concerned Domain Generalization (DG) methods provide us with some inspiration [19, 32, 38, 42]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ba612f963d30c880f71ee290987fad36d16f0b0f",
                "externalIds": {
                    "DBLP": "conf/cvpr/LiuXZWZWGH23",
                    "DOI": "10.1109/CVPRW59228.2023.00676",
                    "CorpusId": 260916344
                },
                "corpusId": 260916344,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ba612f963d30c880f71ee290987fad36d16f0b0f",
                "title": "Adversarial Domain Generalization for Surveillance Face Anti-Spoofing",
                "abstract": "In traditional scenes (short-distance applications), the current Face Anti-Spoofing (FAS) methods have achieved satisfactory performance. However, in surveillance scenes (long-distance applications), those methods cannot be generalized well due to the deviation in image quality. Some methods attempt to recover lost details from low-quality images through image reconstruction, but unknown image degradation results in suboptimal performance. In this paper, we regard image quality degradation as a domain generalization problem. Specifically, we propose an end-to-end Adversarial Domain Generalization Network (ADGN) to improve the generalization of FAS. We first divide the accessible training data into multiple sub-source domains based on image quality scores. Then, a feature extractor and a domain discriminator are trained to make the extracted features from different sub-source domains undistinguishable (i.e., quality-invariant features), thus forming an adversarial learning procedure. At the same time, we have introduced the transfer learning strategy to address the problem of insufficient training data. Our method won second place in \"Track Surveillance Face Anti-spoofing\" of the 4th Face Anti-spoofing Challenge@CVPR2023. Our final submission obtains 9.21% APCER, 1.90% BPCER, and 5.56% ACER, respectively.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2120344161",
                        "name": "Yongluo Liu"
                    },
                    {
                        "authorId": "1492121969",
                        "name": "Yaowen Xu"
                    },
                    {
                        "authorId": "17304119",
                        "name": "Zhaofan Zou"
                    },
                    {
                        "authorId": "2108371101",
                        "name": "Zhuming Wang"
                    },
                    {
                        "authorId": "2141912700",
                        "name": "Bowen Zhang"
                    },
                    {
                        "authorId": "2148913083",
                        "name": "Lifang Wu"
                    },
                    {
                        "authorId": "3392208",
                        "name": "Zhizhi Guo"
                    },
                    {
                        "authorId": "2027792",
                        "name": "Zhixiang He"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following this line, some works [34, 38, 39, 63, 74] achieve the goal by the pre-defined causal graph to learn the key features involving class labels."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fb1bdff0315ea7d04bd08bd3db7c4cf485039b3b",
                "externalIds": {
                    "ArXiv": "2305.15889",
                    "CorpusId": 259313802
                },
                "corpusId": 259313802,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fb1bdff0315ea7d04bd08bd3db7c4cf485039b3b",
                "title": "Quantifying and Exploring Heterogeneity in Domain Generalization through Contrastive Analysis",
                "abstract": "Domain generalization (DG) is a commonly encountered issue in real-world applications. Its objective is to train models that can generalize well to unseen target domains by utilizing multiple source domains. In most DG algorithms, domain labels, which indicate the domain from which each data point is sampled, are treated as a form of supervision to enhance generalization performance. However, using the original domain labels as the supervision signal may not be optimal due to a lack of diversity among domains, known as heterogeneity. This lack of heterogeneity can lead to the original labels being noisy and disrupting the generalization learning process. Some methods attempt to address this by re-dividing the domains and applying a new dividing pattern. However, the chosen pattern may not capture the maximum heterogeneity since there is no metric available to quantify it accurately. In this paper, we propose that domain heterogeneity primarily lies in variant features within the invariant learning framework. We introduce a novel approach which utilizes contrastive learning to guide the metric for domain heterogeneity. By promoting the learning of variant features, we develop a metric that captures models' learning potential for data heterogeneity. We also emphasize the distinction between seeking variance-based heterogeneity and training an invariance-based generalizable model. In the first stage, we generate the most heterogeneous dividing pattern using our contrastive metric. In the second stage, we employ contrastive learning focused on invariance by constructing pairs based on the stable relationships indicated by domains and classes. This approach effectively utilizes the generated domain labels for generalization. Extensive experiments demonstrate that our method successfully uncovers heterogeneity and achieves remarkable generalization performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2154972909",
                        "name": "Yunze Tong"
                    },
                    {
                        "authorId": "38511927",
                        "name": "Junkun Yuan"
                    },
                    {
                        "authorId": "2157502742",
                        "name": "Min Zhang"
                    },
                    {
                        "authorId": "1477669613",
                        "name": "Di-hua Zhu"
                    },
                    {
                        "authorId": "2119060077",
                        "name": "Ke Zhang"
                    },
                    {
                        "authorId": "93192602",
                        "name": "Fei Wu"
                    },
                    {
                        "authorId": "33870528",
                        "name": "Kun Kuang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mahajan et al. (2021) introduces a novel regularizer to match the representation of the same object in different environments."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3d28c016fdd5345f737e3889d83ef746c4dd820a",
                "externalIds": {
                    "ArXiv": "2305.12686",
                    "CorpusId": 258832518
                },
                "corpusId": 258832518,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3d28c016fdd5345f737e3889d83ef746c4dd820a",
                "title": "Conformal Inference for Invariant Risk Minimization",
                "abstract": "The application of machine learning models can be significantly impeded by the occurrence of distributional shifts, as the assumption of homogeneity between the population of training and testing samples in machine learning and statistics may not be feasible in practical situations. One way to tackle this problem is to use invariant learning, such as invariant risk minimization (IRM), to acquire an invariant representation that aids in generalization with distributional shifts. This paper develops methods for obtaining distribution-free prediction regions to describe uncertainty estimates for invariant representations, accounting for the distribution shifts of data from different environments. Our approach involves a weighted conformity score that adapts to the specific environment in which the test sample is situated. We construct an adaptive conformal interval using the weighted conformity score and prove its conditional average under certain conditions. To demonstrate the effectiveness of our approach, we conduct several numerical experiments, including simulation studies and a practical example using real-world data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1802651450",
                        "name": "Wenlu Tang"
                    },
                    {
                        "authorId": "2145253136",
                        "name": "Zicheng Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5331d4d9dd4b9c3e94681f8242e50de92a319c0e",
                "externalIds": {
                    "DBLP": "conf/infocom/HuKCHZC23",
                    "DOI": "10.1109/INFOCOM53939.2023.10229094",
                    "CorpusId": 261387008
                },
                "corpusId": 261387008,
                "publicationVenue": {
                    "id": "7f92b1d2-f2b3-454d-adbe-ff02c83fe404",
                    "name": "IEEE Conference on Computer Communications",
                    "type": "conference",
                    "alternate_names": [
                        "INFOCOM",
                        "IEEE Conf Comput Commun"
                    ],
                    "url": "http://www.ieee-infocom.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5331d4d9dd4b9c3e94681f8242e50de92a319c0e",
                "title": "CSI-StripeFormer: Exploiting Stripe Features for CSI Compression in Massive MIMO System",
                "abstract": "The massive MIMO gain for wireless communication has been greatly hindered by the feedback overhead of channel state information (CSI) growing linearly with the number of antennas. Recent efforts leverage the DNN-based encoder-decoder framework to exploit correlations within the CSI matrix for better CSI compression. However, existing works have not fully exploited the unique features of CSI, resulting in an unsatisfactory performance under high compression ratios and sensitivity to multipath effects. Instead of treating CSI as common 2D matrices like images, we reveal the intrinsic stripe-based correlation across the CSI matrix. Driven by this insight, we propose CSI-StripeFormer, a stripe-aware encoder-decoder framework to exploit the unique stripe feature for better CSI compression. We design a lightweight encoder with asymmetric convolution kernels to capture various shape features. We further incorporate novel designs tailored for stripe features, including a novel hierarchical Transformer backbone in the decoder and a hybrid attention mechanism to extract and fuse correlations in angular and delay domains. Our evaluation results show that our system achieves an over 7dB channel reconstruction gain under a high compression ratio of 64 in multipath-rich scenarios, significantly superior to current state-of-the-art approaches. This gain can be further improved to 17dB given the extended embedded dimension of our backbone.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237079867",
                        "name": "Qingyong Hu"
                    },
                    {
                        "authorId": "2115331228",
                        "name": "Hua Kang"
                    },
                    {
                        "authorId": "19494350",
                        "name": "Huangxun Chen"
                    },
                    {
                        "authorId": "3176991",
                        "name": "Qianyi Huang"
                    },
                    {
                        "authorId": "2145947371",
                        "name": "Qian Zhang"
                    },
                    {
                        "authorId": "2216534578",
                        "name": "Min Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", CoRe [24] and MatchDG [41], are closely related to LAM since they also based on data pairs.",
                "data pair MBDG [52], RICE [63] CoRe [24], MatchDG [41]",
                "As for existing DG approaches, there are only several attempts to utilize data pairs that are either discovered from real data [24, 41] or generated by cross-domain image translation [52, 63].",
                "Similar models are also proposed in [38, 24, 41, 45], where X/X are referred to as semantic/variation factors, causal/noncausal factors, core/non-core factors, and content/style.",
                "For example, MatchDG [41] pushes the feature representations of a data pair (x, x\u0303; y) close to each other regardless of the class label y.",
                "These methods include MatchDG [41] and CoRe [24].",
                "Existing methods that are most related to LAM are CoRe [24], MatchDG [41], MBDG [52], and RICE [63]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f858a5b4902d8de8588d2d94bc5d2652a683c21d",
                "externalIds": {
                    "ArXiv": "2305.07888",
                    "DBLP": "journals/corr/abs-2305-07888",
                    "DOI": "10.48550/arXiv.2305.07888",
                    "CorpusId": 258686562
                },
                "corpusId": 258686562,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f858a5b4902d8de8588d2d94bc5d2652a683c21d",
                "title": "Contrastive Domain Generalization via Logit Attribution Matching",
                "abstract": "Domain Generalization (DG) is an important open problem in machine learning. Deep models are susceptible to domain shifts of even minute degrees, which severely compromises their reliability in real applications. To alleviate the issue, most existing methods enforce various invariant constraints across multiple training domains. However,such an approach provides little performance guarantee for novel test domains in general. In this paper, we investigate a different approach named Contrastive Domain Generalization (CDG), which exploits semantic invariance exhibited by strongly contrastive data pairs in lieu of multiple domains. We present a causal DG theory that shows the potential capability of CDG; together with a regularization technique, Logit Attribution Matching (LAM), for realizing CDG. We empirically show that LAM outperforms state-of-the-art DG methods with only a small portion of paired data and that LAM helps models better focus on semantic features which are crucial to DG.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112514104",
                        "name": "Han Gao"
                    },
                    {
                        "authorId": "3249631",
                        "name": "Kaican Li"
                    },
                    {
                        "authorId": "6564288",
                        "name": "Yongxiang Huang"
                    },
                    {
                        "authorId": "2153520311",
                        "name": "Luning Wang"
                    },
                    {
                        "authorId": "3151540",
                        "name": "Caleb Chen Cao"
                    },
                    {
                        "authorId": "3587277",
                        "name": "N. Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, all representation invariance approaches su er from their neglect of di erences in intra-class variations between groups due to, for example, di erent disease subtypes [17, 19, 23].",
                "A key drawback of any notion of representation invariance concerns their neglect of di erences in the within-class distributions between groups [17, 19]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fc4f9eeb107dcea344e4d55a9533c8aa9ea28740",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-01397",
                    "ArXiv": "2305.01397",
                    "DOI": "10.48550/arXiv.2305.01397",
                    "CorpusId": 258436862
                },
                "corpusId": 258436862,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fc4f9eeb107dcea344e4d55a9533c8aa9ea28740",
                "title": "Are demographically invariant models and representations in medical imaging fair?",
                "abstract": "Medical imaging models have been shown to encode information about patient demographics such as age, race, and sex in their latent representation, raising concerns about their potential for discrimination. Here, we ask whether requiring models not to encode demographic attributes is desirable. We point out that marginal and class-conditional representation invariance imply the standard group fairness notions of demographic parity and equalized odds, respectively, while additionally requiring risk distribution matching, thus potentially equalizing away important group differences. Enforcing the traditional fairness notions directly instead does not entail these strong constraints. Moreover, representationally invariant models may still take demographic attributes into account for deriving predictions. The latter can be prevented using counterfactual notions of (individual) fairness or invariance. We caution, however, that properly defining medical image counterfactuals with respect to demographic attributes is highly challenging. Finally, we posit that encoding demographic attributes may even be advantageous if it enables learning a task-specific encoding of demographic features that does not rely on social constructs such as 'race' and 'gender.' We conclude that demographically invariant representations are neither necessary nor sufficient for fairness in medical imaging. Models may need to encode demographic attributes, lending further urgency to calls for comprehensive model fairness assessments in terms of predictive performance across diverse patient groups.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "38482905",
                        "name": "Eike Petersen"
                    },
                    {
                        "authorId": "35335260",
                        "name": "Enzo Ferrante"
                    },
                    {
                        "authorId": "50996871",
                        "name": "M. Ganz"
                    },
                    {
                        "authorId": "1808965",
                        "name": "Aasa Feragen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "methods [23], [24], domain adversarial learning [25], [26]"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ff25858a9a1a71ddbf8bc05f92fb438f35c36d13",
                "externalIds": {
                    "DBLP": "journals/tits/GaoLMX23",
                    "DOI": "10.1109/TITS.2023.3244827",
                    "CorpusId": 257147895
                },
                "corpusId": 257147895,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ff25858a9a1a71ddbf8bc05f92fb438f35c36d13",
                "title": "GroupPlate: Toward Multi-Category License Plate Recognition",
                "abstract": "License Plate Detection and Recognition (LPDR) is widely used in Intelligent Transportation Systems (ITS). Although there are typically multiple categories of license plates, the majority of existing research cannot be applied to multi-category plates due to that existing methods are not optimised for multi-category plate scenarios and the scarcity of large-scale multi-category plate datasets. In this paper, we propose a multi-category license plate recognition framework called GroupPlate, which consists of Group Module and Indirect Supervision Module, making full use of the implicit and explicit grouping information of license plate. In addition, the Category Decouple Module is intended to decouple the grouping information from the original features, allowing the decoder to concentrate on character features. Simultaneously, we propose a large-scale All-Category license Plate detection and recognition Dataset (ACPD) for vehicles on the Chinese mainland, which also includes annotations of plates\u2019 categories. Considering the domain gap between synthetic data and real data, we propose a simple but effective strategy called Feature Shift to mitigate the performance degradation caused by this gap. Experiments demonstrate that GroupPlate achieves the comparable performance to the existing methods on single-category license plate dataset and outperforms our baseline on the multi-category license plates dataset. Ablation experiments demonstrate the effectiveness of the modules in GroupPlate. Extensive results demonstrate that the dataset we proposed can mitigate the problem of models trained on a single-category license plate dataset failing to recognize multi-category license plates, and that our model can generalizes well to unseen categories. The work will be available at https://github.com/YilinGao-SHU/ACPD.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2209964343",
                        "name": "Yilin Gao"
                    },
                    {
                        "authorId": "2115917391",
                        "name": "Hengjie Lu"
                    },
                    {
                        "authorId": "2124381503",
                        "name": "Shiyi Mu"
                    },
                    {
                        "authorId": "1802931",
                        "name": "Shugong Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026made in many directions, such as Invariant Representation (Chuang et al., 2020; Nguyen et al., 2021; Xiao et al., 2021; Shi et al., 2022), Causal (Mahajan et al., 2021; Mouli & Ribeiro, 2021; Lv et al., 2022), and Optimization (Krueger et al., 2021; Zhang et al., 2021; Lei et al., 2021;\u2026",
                "Specifically, following prior works (Suter et al., 2019; Zhang et al., 2020a; Mitrovic et al., 2021; Mahajan et al., 2021; Zhang et al., 2022b; Veitch et al., 2021; Lv et al., 2022), we assume that observed data X are generated by an causal mechanism G with two causes: semantic factor C and\u2026",
                "Over the years, great efforts have been made in many directions, such as Invariant Representation (Chuang et al., 2020; Nguyen et al., 2021; Xiao et al., 2021; Shi et al., 2022), Causal (Mahajan et al., 2021; Mouli & Ribeiro, 2021; Lv et al., 2022), and Optimization (Krueger et al., 2021; Zhang et al., 2021; Lei et al., 2021; Gulrajani & Lopez-Paz, 2021).",
                ", 2022), Causal (Mahajan et al., 2021; Mouli & Ribeiro, 2021; Lv et al., 2022), and Optimization (Krueger et al.",
                "Following prior works (Mitrovic et al., 2021; Zhang et al., 2020a; Suter et al., 2019; Mahajan et al., 2021; Zhang et al., 2022b; Lv et al., 2022; Nguyen et al., 2022; Chen et al., 2022), we assume that the feature random variables are generated by the following causal mechanism.",
                "Specifically, following prior works (Suter et al., 2019; Zhang et al., 2020a; Mitrovic et al., 2021; Mahajan et al., 2021; Zhang et al., 2022b; Veitch et al., 2021; Lv et al., 2022), we assume that observed data X are generated by an causal mechanism G with two causes: semantic factor C and nonsemantic factor S, i.",
                "Causal Assumption.",
                "Therefore, following Mitrovic et al. (2021); Zhang et al. (2020a); Suter et al. (2019); Mahajan et al. (2021); Lv et al. (2022); Nguyen et al. (2022), we further assume that for any l = 1, ..., N ,\nY \u2190 C and Yl = Yt = Y. (2)\nUncertainty Set and Non-semantic Space."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f87d1379fdca3c2af8239f441f21a6e2ff90bf45",
                "externalIds": {
                    "ArXiv": "2304.13976",
                    "DBLP": "journals/corr/abs-2304-13976",
                    "DOI": "10.48550/arXiv.2304.13976",
                    "CorpusId": 258352373
                },
                "corpusId": 258352373,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f87d1379fdca3c2af8239f441f21a6e2ff90bf45",
                "title": "Moderately Distributional Exploration for Domain Generalization",
                "abstract": "Domain generalization (DG) aims to tackle the distribution shift between training domains and unknown target domains. Generating new domains is one of the most effective approaches, yet its performance gain depends on the distribution discrepancy between the generated and target domains. Distributionally robust optimization is promising to tackle distribution discrepancy by exploring domains in an uncertainty set. However, the uncertainty set may be overwhelmingly large, leading to low-confidence prediction in DG. It is because a large uncertainty set could introduce domains containing semantically different factors from training domains. To address this issue, we propose to perform a $\\textbf{mo}$derately $\\textbf{d}$istributional $\\textbf{e}$xploration (MODE) for domain generalization. Specifically, MODE performs distribution exploration in an uncertainty $\\textit{subset}$ that shares the same semantic factors with the training domains. We show that MODE can endow models with provable generalization performance on unknown target domains. The experimental results show that MODE achieves competitive performance compared to state-of-the-art baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143299123",
                        "name": "Ruiqi Dai"
                    },
                    {
                        "authorId": "2109116068",
                        "name": "Yonggang Zhang"
                    },
                    {
                        "authorId": "1382582313",
                        "name": "Zhen Fang"
                    },
                    {
                        "authorId": "2153287285",
                        "name": "Bo Han"
                    },
                    {
                        "authorId": "40434674",
                        "name": "Xinmei Tian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "34085c9e7e897604dfebd8917b123c5d11f1ab0c",
                "externalIds": {
                    "DBLP": "journals/nca/ChenC23",
                    "DOI": "10.1007/s00521-023-08520-1",
                    "CorpusId": 258307964
                },
                "corpusId": 258307964,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/34085c9e7e897604dfebd8917b123c5d11f1ab0c",
                "title": "Joint-product representation learning for domain generalization in classification and regression",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "66860739",
                        "name": "Sentao Chen"
                    },
                    {
                        "authorId": "2146034678",
                        "name": "Liang Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "075439dee13811848f51e97307e6ce748b6cfffe",
                "externalIds": {
                    "ArXiv": "2304.10226",
                    "CorpusId": 261965501
                },
                "corpusId": 261965501,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/075439dee13811848f51e97307e6ce748b6cfffe",
                "title": "Domain Generalization for Mammographic Image Analysis with Contrastive Learning",
                "abstract": "The deep learning technique has been shown to be effectively addressed several image analysis tasks in the computer-aided diagnosis scheme for mammography. The training of an efficacious deep learning model requires large data with diverse styles and qualities. The diversity of data often comes from the use of various scanners of vendors. But, in practice, it is impractical to collect a sufficient amount of diverse data for training. To this end, a novel contrastive learning is developed to equip the deep learning models with better style generalization capability. Specifically, the multi-style and multi-view unsupervised self-learning scheme is carried out to seek robust feature embedding against style diversity as a pretrained model. Afterward, the pretrained network is further fine-tuned to the downstream tasks, e.g., mass detection, matching, BI-RADS rating, and breast density classification. The proposed method has been evaluated extensively and rigorously with mammograms from various vendor style domains and several public datasets. The experimental results suggest that the proposed domain generalization method can effectively improve performance of four mammographic image tasks on the data from both seen and unseen domains, and outperform many state-of-the-art (SOTA) generalization methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9018214",
                        "name": "Zhuoxu Cui"
                    },
                    {
                        "authorId": "1805367",
                        "name": "Sen Jia"
                    },
                    {
                        "authorId": "145165483",
                        "name": "Qingyong Zhu"
                    },
                    {
                        "authorId": "2108152350",
                        "name": "Congcong Liu"
                    },
                    {
                        "authorId": "104316398",
                        "name": "Zhilang Qiu"
                    },
                    {
                        "authorId": "2143860379",
                        "name": "Yuanyuan Liu"
                    },
                    {
                        "authorId": "2112805715",
                        "name": "Jing Cheng"
                    },
                    {
                        "authorId": "2108969013",
                        "name": "Haifeng Wang"
                    },
                    {
                        "authorId": "145350977",
                        "name": "Yanjie Zhu"
                    },
                    {
                        "authorId": "2113984841",
                        "name": "Dong Liang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The domain distance measures [68] include moments [71], contrastive loss [72], [73], Kullback\u2013Leibler divergence [74], Maximum Mean Discrepancy (MMD) distance [75] and adversarial learning [70].",
                "[72] selected images of the same objects for domain generalization by a causal matching algorithm."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4f999a503e5b35331a04a173a672561a57e56346",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-03550",
                    "ArXiv": "2304.03550",
                    "DOI": "10.48550/arXiv.2304.03550",
                    "CorpusId": 258041071
                },
                "corpusId": 258041071,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4f999a503e5b35331a04a173a672561a57e56346",
                "title": "Hierarchical Disentanglement-Alignment Network for Robust SAR Vehicle Recognition",
                "abstract": "Due to Synthetic Aperture Radar (SAR) imaging characteristics, SAR vehicle recognition faces the problem of extracting discriminative and robust target features from a small dataset. Deep learning has shown impressive performance on the MSTAR dataset. However, data bias in a small dataset, such as background correlation, impairs the causality of these methods, i.e., discriminative features contain target and background differences. Moreover, different operating conditions of SAR lead to target signatures and background clutter variations in imaging results. However, many deep learning-based methods only verify robustness to target or background variations in the current experimental setting. In this paper, we propose a novel domain alignment framework named Hierarchical Disentanglement-Alignment Network (HDANet) to enhance features' causality and robustness. Concisely, HDANet consists of three parts: The first part uses data augmentation to generate signature variations for domain alignment. The second part disentangles the target features through a multitask-assisted mask to prevent non-causal clutter from interfering with subsequent alignment and recognition. Thirdly, a contrastive loss is employed for domain alignment to extract robust target features, and the SimSiam structure is applied to mitigate conflicts between contrastive loss and feature discrimination. Finally, the proposed method shows high robustness across MSTAR's multiple target, sensor, and environment variants. Noteworthy, we add a new scene variant to verify the robustness to target and background variations. Moreover, the saliency map and Shapley value qualitatively and quantitatively demonstrate causality. Our code is available in \\url{https://github.com/waterdisappear/SAR-ATR-HDANet}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1483778900",
                        "name": "Wei-Jang Li"
                    },
                    {
                        "authorId": "2150081199",
                        "name": "Wei Yang"
                    },
                    {
                        "authorId": "2156059116",
                        "name": "Li Li"
                    },
                    {
                        "authorId": "2108467535",
                        "name": "Wenpeng Zhang"
                    },
                    {
                        "authorId": "1679704",
                        "name": "Y. Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fd24a9679356bc66ca3cdda857997e1efdc32f7c",
                "externalIds": {
                    "DBLP": "conf/cvpr/0009GWL23",
                    "ArXiv": "2304.03709",
                    "DOI": "10.1109/CVPR52729.2023.00742",
                    "CorpusId": 258041169
                },
                "corpusId": 258041169,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fd24a9679356bc66ca3cdda857997e1efdc32f7c",
                "title": "Meta-Causal Learning for Single Domain Generalization",
                "abstract": "Single domain generalization aims to learn a model from a single training domain (source domain) and apply it to multiple unseen test domains (target domains). Existing methods focus on expanding the distribution of the training domain to cover the target domains, but without estimating the domain shift between the source and target domains. In this paper, we propose a new learning paradigm, namely simulate-analyze-reduce, which first simulates the domain shift by building an auxiliary domain as the target domain, then learns to analyze the causes of domain shift, and finally learns to reduce the domain shift for model adaptation. Under this paradigm, we propose a meta-causal learning method to learn meta-knowledge, that is, how to infer the causes of domain shift between the auxiliary and source domains during training. We use the meta-knowledge to analyze the shift between the target and source domains during testing. Specifically, we perform multiple transformations on source data to generate the auxiliary domain, perform counterfactual inference to learn to discover the causal factors of the shift between the auxiliary and source domains, and incorporate the inferred causality into factor-aware domain alignments. Extensive experiments on several benchmarks of image classification show the effectiveness of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108458375",
                        "name": "Jin Chen"
                    },
                    {
                        "authorId": "2187550120",
                        "name": "Zhiqiang Gao"
                    },
                    {
                        "authorId": "2125709",
                        "name": "Xinxiao Wu"
                    },
                    {
                        "authorId": "2116782926",
                        "name": "Jiebo Luo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Specifically, MatchDG narrowed the difference between different machines and working conditions, which was similar to the purpose of DAN.",
                "[15] proposed an iterative algorithm named matching-based algorithms (MatchDG), which minimized the contrast loss of multiple domains to realize crossdomain image recognition.",
                "However, some methods, such as MatchDG in TD and ADIG in TF, do not explicitly deal with the individualized biases in machine data, which causes overfitting of the classification space and limits the generalization ability of the network.",
                "To verify the effectiveness and rationality of the designed method, traditional domain generalization methods, domain adversarial network (DAN) [12], ADIG [13], and IEDGNet [14], and causal learning methods, MatchDG [15], ANDMask [16], and IRM [17], were adopted for comparison.",
                "Mahajan et al. [15] proposed an iterative algorithm named matching-based algorithms (MatchDG), which minimized the contrast loss of multiple domains to realize crossdomain image recognition."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "295bc9d77005a8f8c993dba73f44c54129734b06",
                "externalIds": {
                    "DBLP": "journals/tii/LiWZZL23",
                    "DOI": "10.1109/TII.2022.3174711",
                    "CorpusId": 248769372
                },
                "corpusId": 248769372,
                "publicationVenue": {
                    "id": "2135230a-3b24-4b71-9583-60624389377a",
                    "name": "IEEE Transactions on Industrial Informatics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Ind Informatics"
                    ],
                    "issn": "1551-3203",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=9424",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9424"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/295bc9d77005a8f8c993dba73f44c54129734b06",
                "title": "Causal Consistency Network: A Collaborative Multimachine Generalization Method for Bearing Fault Diagnosis",
                "abstract": "Due to the lack of faulty data on the target machine, intelligent networks often need to learn fault knowledge from other relevant machines. Unfortunately, data from different machines introduce individualized deviations, which may lead to overfitting of network learning and reduce its generalization ability. To address the problem, this article proposes a collaborative multimachine generalization method\u2014causal consistency network (CCN), which mines the invariant causal information in individualized machines through a collaborative way to achieve knowledge generalization. In CCN, instead of emphasizing the domain invariance of features, causal consistency loss depicting the consistency of fault causality representations in deep latent variables is proposed. Moreover, to transform the individualized data of different machines into consistent representations, a collaborative training loss is proposed to describe the underlying invariant causal mechanism of the fault features. The generalization results among 6 machines containing 43 individual bearings and 20 operating conditions demonstrate the superiority of CCN.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2048334187",
                        "name": "Jiejue Li"
                    },
                    {
                        "authorId": "113310344",
                        "name": "Yu Wang"
                    },
                    {
                        "authorId": "1952345",
                        "name": "Y. Zi"
                    },
                    {
                        "authorId": "49724537",
                        "name": "Haijun Zhang"
                    },
                    {
                        "authorId": "2116522388",
                        "name": "Chen Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3c3bc5352a94bd6bc730de78eee5a40a2478352a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-15698",
                    "ArXiv": "2303.15698",
                    "DOI": "10.48550/arXiv.2303.15698",
                    "CorpusId": 257771796
                },
                "corpusId": 257771796,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3c3bc5352a94bd6bc730de78eee5a40a2478352a",
                "title": "TFS-ViT: Token-Level Feature Stylization for Domain Generalization",
                "abstract": "Standard deep learning models such as convolutional neural networks (CNNs) lack the ability of generalizing to domains which have not been seen during training. This problem is mainly due to the common but often wrong assumption of such models that the source and target data come from the same i.i.d. distribution. Recently, Vision Transformers (ViTs) have shown outstanding performance for a broad range of computer vision tasks. However, very few studies have investigated their ability to generalize to new domains. This paper presents a first Token-level Feature Stylization (TFS-ViT) approach for domain generalization, which improves the performance of ViTs to unseen data by synthesizing new domains. Our approach transforms token features by mixing the normalization statistics of images from different domains. We further improve this approach with a novel strategy for attention-aware stylization, which uses the attention maps of class (CLS) tokens to compute and mix normalization statistics of tokens corresponding to different image regions. The proposed method is flexible to the choice of backbone model and can be easily applied to any ViT-based architecture with a negligible increase in computational complexity. Comprehensive experiments show that our approach is able to achieve state-of-the-art performance on five challenging benchmarks for domain generalization, and demonstrate its ability to deal with different types of domain shifts. The implementation is available at: https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1380287805",
                        "name": "Mehrdad Noori"
                    },
                    {
                        "authorId": "2188346816",
                        "name": "Milad Cheraghalikhani"
                    },
                    {
                        "authorId": "108062243",
                        "name": "Ali Bahri"
                    },
                    {
                        "authorId": "2037886454",
                        "name": "G. A. V. Hakim"
                    },
                    {
                        "authorId": "2188345071",
                        "name": "David Osowiechi"
                    },
                    {
                        "authorId": "144019647",
                        "name": "Ismail Ben Ayed"
                    },
                    {
                        "authorId": "1739646",
                        "name": "Christian Desrosiers"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The problem of uncertainty in the feature vector has been addressed in the literature and deals with issues such as generalization of the output hypothesis once a new data set is available or concerns with noisy input [25, 26, 27, 28, 29, 30]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ef2893cbc851b0b4234f10f23797a2b9d8f2bd11",
                "externalIds": {
                    "ArXiv": "2303.16091",
                    "DBLP": "journals/corr/abs-2303-16091",
                    "DOI": "10.48550/arXiv.2303.16091",
                    "CorpusId": 257771217
                },
                "corpusId": 257771217,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ef2893cbc851b0b4234f10f23797a2b9d8f2bd11",
                "title": "Learnability, Sample Complexity, and Hypothesis Class Complexity for Regression Models",
                "abstract": "The goal of a learning algorithm is to receive a training data set as input and provide a hypothesis that can generalize to all possible data points from a domain set. The hypothesis is chosen from hypothesis classes with potentially different complexities. Linear regression modeling is an important category of learning algorithms. The practical uncertainty of the target samples affects the generalization performance of the learned model. Failing to choose a proper model or hypothesis class can lead to serious issues such as underfitting or overfitting. These issues have been addressed by alternating cost functions or by utilizing cross-validation methods. These approaches can introduce new hyperparameters with their own new challenges and uncertainties or increase the computational complexity of the learning algorithm. On the other hand, the theory of probably approximately correct (PAC) aims at defining learnability based on probabilistic settings. Despite its theoretical value, PAC does not address practical learning issues on many occasions. This work is inspired by the foundation of PAC and is motivated by the existing regression learning issues. The proposed approach, denoted by epsilon-Confidence Approximately Correct (epsilon CoAC), utilizes Kullback Leibler divergence (relative entropy) and proposes a new related typical set in the set of hyperparameters to tackle the learnability issue. Moreover, it enables the learner to compare hypothesis classes of different complexity orders and choose among them the optimum with the minimum epsilon in the epsilon CoAC framework. Not only the epsilon CoAC learnability overcomes the issues of overfitting and underfitting, but it also shows advantages and superiority over the well known cross-validation method in the sense of time consumption as well as in the sense of accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1696667",
                        "name": "S. Beheshti"
                    },
                    {
                        "authorId": "40258162",
                        "name": "Mahdi Shamsi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e6357834e6145b607c350b92238b5bc4d662e920",
                "externalIds": {
                    "ArXiv": "2303.13899",
                    "DBLP": "conf/cvpr/YuanX023",
                    "DOI": "10.1109/CVPR52729.2023.01528",
                    "CorpusId": 257757140
                },
                "corpusId": 257757140,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e6357834e6145b607c350b92238b5bc4d662e920",
                "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
                "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distributions. However, these attempts may fail in dynamic scenarios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we explore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Extensive experiments prove that RoTTA enables continual test-time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145235797",
                        "name": "Longhui Yuan"
                    },
                    {
                        "authorId": "1896719712",
                        "name": "Binhui Xie"
                    },
                    {
                        "authorId": "2165375715",
                        "name": "Shuangliang Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, many existing OOD generalization methods, such as domain alignment [28, 32, 43, 44, 51, 55, 89], meta-learning [15\u201317, 41, 75], and disentangled representation learning [10, 31, 33, 42, 61, 76], require class/domain-related supervision, which are inapplicable for the AD task."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e90e30b2e8c11c4c459b32fbbe0cd0dbe95d2c3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-13845",
                    "ArXiv": "2303.13845",
                    "DOI": "10.48550/arXiv.2303.13845",
                    "CorpusId": 257757373
                },
                "corpusId": 257757373,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3e90e30b2e8c11c4c459b32fbbe0cd0dbe95d2c3",
                "title": "Anomaly Detection under Distribution Shift",
                "abstract": "Anomaly detection (AD) is a crucial machine learning task that aims to learn patterns from a set of normal training samples to identify abnormal samples in test data. Most existing AD studies assume that the training and test data are drawn from the same data distribution, but the test data can have large distribution shifts arising in many real-world applications due to different natural variations such as new lighting conditions, object poses, or background appearances, rendering existing AD methods ineffective in such cases. In this paper, we consider the problem of anomaly detection under distribution shift and establish performance benchmarks on four widely-used AD and out-of-distribution (OOD) generalization datasets. We demonstrate that simple adaptation of state-of-the-art OOD generalization methods to AD settings fails to work effectively due to the lack of labeled anomaly data. We further introduce a novel robust AD approach to diverse distribution shifts by minimizing the distribution gap between in-distribution and OOD normal samples in both the training and inference stages in an unsupervised way. Our extensive empirical results on the four datasets show that our approach substantially outperforms state-of-the-art AD methods and OOD generalization methods on data with various distribution shifts, while maintaining the detection accuracy on in-distribution data. Code and data are available at https://github.com/mala-lab/ADShift.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149281913",
                        "name": "T. Cao"
                    },
                    {
                        "authorId": "2109445463",
                        "name": "Jiawen Zhu"
                    },
                    {
                        "authorId": "3224619",
                        "name": "Guansong Pang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our investigated problem is closely related to OOD generalization which is a broad field that contains many popular research topics, such as Domain Generalization [3, 18, 36, 38, 41, 43, 47, 66], Causal Invariant Learning [1, 25, 27, 46, 62, 63]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "716c25f3c9dda90a6980fbdd8664326868a1bde3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-13087",
                    "ArXiv": "2303.13087",
                    "DOI": "10.1109/CVPR52729.2023.01552",
                    "CorpusId": 257687647
                },
                "corpusId": 257687647,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/716c25f3c9dda90a6980fbdd8664326868a1bde3",
                "title": "Robust Generalization Against Photon-Limited Corruptions via Worst-Case Sharpness Minimization",
                "abstract": "Robust generalization aims to tackle the most challenging data distributions which are rare in the training set and contain severe noises, i.e., photon-limited corruptions. Common solutions such as distributionally robust optimization (DRO) focus on the worst-case empirical risk to ensure low training error on the uncommon noisy distributions. However, due to the over-parameterized model being optimized on scarce worst-case data, DRO fails to produce a smooth loss landscape, thus struggling on generalizing well to the test set. Therefore, instead of focusing on the worst-case risk minimization, we propose SharpDRO by penalizing the sharpness of the worst-case distribution, which measures the loss changes around the neighbor of learning parameters. Through worst-case sharpness minimization, the proposed method successfully produces a flat loss curve on the corrupted distributions, thus achieving robust generalization. Moreover, by considering whether the distribution annotation is available, we apply SharpDRO to two problem settings and design a worst-case selection process for robust generalization. Theoretically, we show that SharpDRO has a great convergence guarantee. Experimentally, we simulate photon-limited corruptions using CIFAR10/100 and ImageNet30 datasets and show that SharpDRO exhibits a strong generalization ability against severe corruptions and exceeds well-known baseline methods with large performance gains.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144686871",
                        "name": "Zhuo Huang"
                    },
                    {
                        "authorId": "2212531105",
                        "name": "Miaoxi Zhu"
                    },
                    {
                        "authorId": "2077454998",
                        "name": "Xiaobo Xia"
                    },
                    {
                        "authorId": "2144034222",
                        "name": "Li Shen"
                    },
                    {
                        "authorId": null,
                        "name": "Jun Yu"
                    },
                    {
                        "authorId": "2171109070",
                        "name": "Chen Gong"
                    },
                    {
                        "authorId": "2087238859",
                        "name": "Bo Han"
                    },
                    {
                        "authorId": "2212029373",
                        "name": "Bo Du"
                    },
                    {
                        "authorId": "121698214",
                        "name": "Tongliang Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other DG methods also employ self-supervised learning [16, 17, 27], ensemble learning [1, 50, 67] and dropout regularization [13, 14, 38]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f429439dc3b803ac6187859aeac339fd8911a1ea",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-11674",
                    "ArXiv": "2303.11674",
                    "DOI": "10.1109/CVPR52729.2023.02311",
                    "CorpusId": 257637058
                },
                "corpusId": 257637058,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f429439dc3b803ac6187859aeac339fd8911a1ea",
                "title": "ALOFT: A Lightweight MLP-Like Architecture with Dynamic Low-Frequency Transform for Domain Generalization",
                "abstract": "Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing multiple source domains without re-training. Most existing DG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local representations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based methods have achieved promising results in supervised learning tasks by learning global interactions among different patches of the image. Inspired by this, in this paper, we first analyze the difference between CNN and MLP methods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods. Then, based on a recent lightweight MLP method, we obtain a strong baseline that outperforms most state-of-the-art CNN-based methods. The baseline can learn global structure representations with a filter to suppress structureirrelevant information in the frequency space. Moreover, we propose a dynAmic LOw-Frequency spectrum Transform (ALOFT) that can perturb local texture features while preserving global structure features, thus enabling the filter to remove structure-irrelevant information sufficiently. Extensive experiments on four benchmarks have demonstrated that our method can achieve great performance improvement with a small number of parameters compared to SOTA CNN-based DG methods. Our code is available at https://github.com/lingeringlight/ALOFT/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50115584",
                        "name": "Jintao Guo"
                    },
                    {
                        "authorId": "2212174035",
                        "name": "Na Wang"
                    },
                    {
                        "authorId": "1785352346",
                        "name": "Lei Qi"
                    },
                    {
                        "authorId": "2475959",
                        "name": "Yinghuan Shi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A primary strategy of DG is domain-invariant feature learning which aims to reduce domain gaps, including aligning distributions among multiple domains with contrastive learning [22, 41, 68, 70], learning useful representations with self supervise learning [6], matching statistics of feature distributions across domains [54,59], domain adversarial learning [14,31] and causality inference [38,39]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a363b989a11e11de76022945390c3eddef769413",
                "externalIds": {
                    "ArXiv": "2303.10902",
                    "DBLP": "journals/corr/abs-2303-10902",
                    "DOI": "10.1109/CVPR52729.2023.01920",
                    "CorpusId": 257632461
                },
                "corpusId": 257632461,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a363b989a11e11de76022945390c3eddef769413",
                "title": "Feature Alignment and Uniformity for Test Time Adaptation",
                "abstract": "Test time adaptation (TTA) aims to adapt deep neural networks when receiving out of distribution test domain samples. In this setting, the model can only access online unlabeled test samples and pretrained models on the training domains. We first address TTA as a feature revision problem due to the domain gap between source domains and target domains. After that, we follow the two measurements alignment and uniformity to discuss the test time feature revision. For test time feature uniformity, we propose a test time self-distillation strategy to guarantee the consistency of uniformity between representations of the current batch and all the previous batches. For test time feature alignment, we propose a memorized spatial local clustering strategy to align the representations among the neighborhood samples for the upcoming batch. To deal with the common noisy label problem, we propound the entropy and consistency filters to select and drop the possible noisy labels. To prove the scalability and efficacy of our method, we conduct experiments on four domain generalization bench marks and four medical image segmentation tasks with various backbones. Experiment results show that our method not only improves baseline stably but also outperforms existing state-of-the-art test time adaptation methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2186872970",
                        "name": "Shuai Wang"
                    },
                    {
                        "authorId": "2188765758",
                        "name": "Daoan Zhang"
                    },
                    {
                        "authorId": "2203151423",
                        "name": "Zipei Yan"
                    },
                    {
                        "authorId": "2108313681",
                        "name": "Jiang Zhang"
                    },
                    {
                        "authorId": "48881714",
                        "name": "Ruizhen Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous works [36,37,54,64,65] have incorporated causal inference in high-level tasks by instantiating the back-door criterion [17] with attention intervention [64], feature interventions [66], etc, which are arduous to be exploited in the low-level task of image restoration.",
                "To improve the generalization capability, [29, 36, 37, 65] introduce the causal learning to domain adaptation/generalization."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7e84b1a647ad12a865f09548f1b18d61d8142529",
                "externalIds": {
                    "ArXiv": "2303.06859",
                    "DBLP": "conf/cvpr/LiLJL023",
                    "DOI": "10.1109/CVPR52729.2023.00171",
                    "CorpusId": 257496408
                },
                "corpusId": 257496408,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7e84b1a647ad12a865f09548f1b18d61d8142529",
                "title": "Learning Distortion Invariant Representation for Image Restoration from a Causality Perspective",
                "abstract": "In recent years, we have witnessed the great advancement of Deep neural networks (DNNs) in image restoration. However, a critical limitation is that they cannot generalize well to real-world degradations with different degrees or types. In this paper, we are the first to propose a novel training strategy for image restoration from the causality perspective, to improve the generalization ability of DNNs for unknown degradations. Our method, termed Distortion Invariant representation Learning (DIL), treats each distortion type and degree as one specific confounder, and learns the distortion-invariant representation by eliminating the harmful confounding effect of each degradation. We derive our DIL with the back-door criterion in causality by modeling the interventions of different distortions from the optimization perspective. Particularly, we introduce counterfactual distortion augmentation to simulate the virtual distortion types and degrees as the confounders. Then, we instantiate the intervention of each distortion with a virtual model updating based on corresponding distorted images, and eliminate them from the meta-learning perspective. Extensive experiments demonstrate the generalization capability of our DIL on unseen distortion types and degrees. Our code will be available at https://github.com/lixinustc/Causal-IR-DIL.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2211993275",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "38953293",
                        "name": "B. Li"
                    },
                    {
                        "authorId": "2149170492",
                        "name": "Xin Jin"
                    },
                    {
                        "authorId": "40093162",
                        "name": "Cuiling Lan"
                    },
                    {
                        "authorId": "31482866",
                        "name": "Zhibo Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This perspective has motivated several invariancelearning methods that make causal assumptions on the datagenerating process [1, 48, 49].",
                "\u2022 Evaluating more generalization techniques on Spawrious, including different robustness penalties [44, 3, 37, 5, 48, 27, 57], meta-learning [77, 9, 32, 70, 28], unsupervised domain adaptation [14, 46, 75], dropout [39], flat minima [5, 30], weight averaging [58, 73, 29], (counterfactual) data augmentation [31, 19, 76], finetuning of only specific layers [35, 40], diversity [65, 58], etc."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eb6399becbc470e3f15cc92ce6ea364f815ad1cd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-05470",
                    "ArXiv": "2303.05470",
                    "DOI": "10.48550/arXiv.2303.05470",
                    "CorpusId": 257427315
                },
                "corpusId": 257427315,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eb6399becbc470e3f15cc92ce6ea364f815ad1cd",
                "title": "Spawrious: A Benchmark for Fine Control of Spurious Correlation Biases",
                "abstract": "The problem of spurious correlations (SCs) arises when a classifier relies on non-predictive features that happen to be correlated with the labels in the training data. For example, a classifier may misclassify dog breeds based on the background of dog images. This happens when the backgrounds are correlated with other breeds in the training data, leading to misclassifications during test time. Previous SC benchmark datasets suffer from varying issues, e.g., over-saturation or only containing one-to-one (O2O) SCs, but no many-to-many (M2M) SCs arising between groups of spurious attributes and classes. In this paper, we present \\benchmark-\\{O2O, M2M\\}-\\{Easy, Medium, Hard\\}, an image classification benchmark suite containing spurious correlations between classes and backgrounds. To create this dataset, we employ a text-to-image model to generate photo-realistic images and an image captioning model to filter out unsuitable ones. The resulting dataset is of high quality and contains approximately 152k images. Our experimental results demonstrate that state-of-the-art group robustness methods struggle with \\benchmark, most notably on the Hard-splits with none of them getting over $70\\%$ accuracy on the hardest split using a ResNet50 pretrained on ImageNet. By examining model misclassifications, we detect reliances on spurious backgrounds, demonstrating that our dataset provides a significant challenge.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2174176979",
                        "name": "Aengus Lynch"
                    },
                    {
                        "authorId": "2193244036",
                        "name": "G. Dovonon"
                    },
                    {
                        "authorId": "66914903",
                        "name": "Jean Kaddour"
                    },
                    {
                        "authorId": "2140070169",
                        "name": "Ricardo M. A. Silva"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Structural equation models are often assumed for theoretical analysis (von K\u00fcgelgen et al., 2021; Liu et al., 2020; Mahajan et al., 2021).",
                "Most works aim to learn a representation that performs well on different source domains simultaneously (Rojas-Carulla et al., 2018; Mahajan et al., 2021; Jin et al., 2020), following the idea of causal invariance (Peters et al.",
                "Structural equation models are often assumed for theoretical analysis (von Ku\u0308gelgen et al., 2021; Liu et al., 2020; Mahajan et al., 2021).",
                "It aims to find features that capture some invariance across different distributions, and assume that such invariance also applies to test distributions (Peters et al., 2016; Rojas-Carulla et al., 2018; Arjovsky et al., 2019; Mahajan et al., 2021; Jin et al., 2020; Ye et al., 2021).",
                "Most works aim to learn a representation that performs well on different source domains simultaneously (Rojas-Carulla et al., 2018; Mahajan et al., 2021; Jin et al., 2020), following the idea of causal invariance (Peters et al., 2016; Arjovsky et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "20eaccbe4d6ffc3652594fb8783f64a653cb881a",
                "externalIds": {
                    "ArXiv": "2303.01092",
                    "DBLP": "conf/iclr/ZhaoD0Y023",
                    "DOI": "10.48550/arXiv.2303.01092",
                    "CorpusId": 257280014
                },
                "corpusId": 257280014,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/20eaccbe4d6ffc3652594fb8783f64a653cb881a",
                "title": "ArCL: Enhancing Contrastive Learning with Augmentation-Robust Representations",
                "abstract": "Self-Supervised Learning (SSL) is a paradigm that leverages unlabeled data for model training. Empirical studies show that SSL can achieve promising performance in distribution shift scenarios, where the downstream and training distributions differ. However, the theoretical understanding of its transferability remains limited. In this paper, we develop a theoretical framework to analyze the transferability of self-supervised contrastive learning, by investigating the impact of data augmentation on it. Our results reveal that the downstream performance of contrastive learning depends largely on the choice of data augmentation. Moreover, we show that contrastive learning fails to learn domain-invariant features, which limits its transferability. Based on these theoretical insights, we propose a novel method called Augmentation-robust Contrastive Learning (ArCL), which guarantees to learn domain-invariant features and can be easily integrated with existing contrastive learning algorithms. We conduct experiments on several datasets and show that ArCL significantly improves the transferability of contrastive learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "98212954",
                        "name": "Xuyang Zhao"
                    },
                    {
                        "authorId": "2210265408",
                        "name": "Tianqi Du"
                    },
                    {
                        "authorId": "2115869684",
                        "name": "Yisen Wang"
                    },
                    {
                        "authorId": "2188218346",
                        "name": "Jun Yao"
                    },
                    {
                        "authorId": "8007867",
                        "name": "Weiran Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mahajan et al. (2021) focus on cases where the distribution of causal features vary across domains; we additionally allow for xd:robust to be non-causal, such as habitat features in iWildCam and BirdCalls."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "91fa893b358ab07094922761e19f8794d7eb79a4",
                "externalIds": {
                    "ArXiv": "2302.11861",
                    "DBLP": "journals/corr/abs-2302-11861",
                    "DOI": "10.48550/arXiv.2302.11861",
                    "CorpusId": 257102936
                },
                "corpusId": 257102936,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/91fa893b358ab07094922761e19f8794d7eb79a4",
                "title": "Out-of-Domain Robustness via Targeted Augmentations",
                "abstract": "Models trained on one set of domains often suffer performance drops on unseen domains, e.g., when wildlife monitoring models are deployed in new camera locations. In this work, we study principles for designing data augmentations for out-of-domain (OOD) generalization. In particular, we focus on real-world scenarios in which some domain-dependent features are robust, i.e., some features that vary across domains are predictive OOD. For example, in the wildlife monitoring application above, image backgrounds vary across camera locations but indicate habitat type, which helps predict the species of photographed animals. Motivated by theoretical analysis on a linear setting, we propose targeted augmentations, which selectively randomize spurious domain-dependent features while preserving robust ones. We prove that targeted augmentations improve OOD performance, allowing models to generalize better with fewer domains. In contrast, existing approaches such as generic augmentations, which fail to randomize domain-dependent features, and domain-invariant augmentations, which randomize all domain-dependent features, both perform poorly OOD. In experiments on three real-world datasets, we show that targeted augmentations set new states-of-the-art for OOD performance by 3.2-15.2%.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "8687620",
                        "name": "Irena Gao"
                    },
                    {
                        "authorId": "2389237",
                        "name": "Shiori Sagawa"
                    },
                    {
                        "authorId": "2572525",
                        "name": "Pang Wei Koh"
                    },
                    {
                        "authorId": "2117567142",
                        "name": "Tatsunori Hashimoto"
                    },
                    {
                        "authorId": "145419642",
                        "name": "Percy Liang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, Mahajan et al. (2021) introduced causal matching to model within-class variations for generalization.",
                "One of the predominant methods is domain invariant learning (Muandet et al., 2013; Ghifary et al., 2016; Motiian et al., 2017; Seo et al., 2020; Zhao et al., 2020; Xiao et al., 2021a; Mahajan et al., 2021; Nguyen et al., 2021; Phung et al., 2021; Shi et al., 2022)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "af47a562da0a4d0a120a92fb7e0d48a58b09bd08",
                "externalIds": {
                    "ArXiv": "2302.11215",
                    "DBLP": "journals/corr/abs-2302-11215",
                    "DOI": "10.48550/arXiv.2302.11215",
                    "CorpusId": 257078985
                },
                "corpusId": 257078985,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/af47a562da0a4d0a120a92fb7e0d48a58b09bd08",
                "title": "Energy-Based Test Sample Adaptation for Domain Generalization",
                "abstract": "In this paper, we propose energy-based sample adaptation at test time for domain generalization. Where previous works adapt their models to target domains, we adapt the unseen target samples to source-trained models. To this end, we design a discriminative energy-based model, which is trained on source domains to jointly model the conditional distribution for classification and data distribution for sample adaptation. The model is optimized to simultaneously learn a classifier and an energy function. To adapt target samples to source distributions, we iteratively update the samples by energy minimization with stochastic gradient Langevin dynamics. Moreover, to preserve the categorical information in the sample during adaptation, we introduce a categorical latent variable into the energy-based model. The latent variable is learned from the original sample before adaptation by variational inference and fixed as a condition to guide the sample update. Experiments on six benchmarks for classification of images and microblog threads demonstrate the effectiveness of our proposal.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152287370",
                        "name": "Zehao Xiao"
                    },
                    {
                        "authorId": "34798935",
                        "name": "Xiantong Zhen"
                    },
                    {
                        "authorId": "40397682",
                        "name": "Shengcai Liao"
                    },
                    {
                        "authorId": "145404204",
                        "name": "Cees G. M. Snoek"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other methods include using adversarial training [53\u201355] inspired from [17], meta learning [56], domain mixup [57\u201359], distributionally robust optimization [60], causal matching [61] and invariant risk minimization [62]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fc935d1739f45cf56aa1dde3f95cfff64e9a0d18",
                "externalIds": {
                    "ArXiv": "2302.00995",
                    "DBLP": "journals/corr/abs-2302-00995",
                    "DOI": "10.48550/arXiv.2302.00995",
                    "CorpusId": 256503726
                },
                "corpusId": 256503726,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fc935d1739f45cf56aa1dde3f95cfff64e9a0d18",
                "title": "Open-Set Multi-Source Multi-Target Domain Adaptation",
                "abstract": "Single-Source Single-Target Domain Adaptation (1S1T) aims to bridge the gap between a labelled source domain and an unlabelled target domain. Despite 1S1T being a well-researched topic, they are typically not deployed to the real world. Methods like Multi-Source Domain Adaptation and Multi-Target Domain Adaptation have evolved to model real-world problems but still do not generalise well. The fact that most of these methods assume a common label-set between source and target is very restrictive. Recent Open-Set Domain Adaptation methods handle unknown target labels but fail to generalise in multiple domains. To overcome these difficulties, first, we propose a novel generic domain adaptation (DA) setting named Open-Set Multi-Source Multi-Target Domain Adaptation (OS-nSmT), with n and m being number of source and target domains respectively. Next, we propose a graph attention based framework named DEGAA which can capture information from multiple source and target domains without knowing the exact label-set of the target. We argue that our method, though offered for multiple sources and multiple targets, can also be agnostic to various other DA settings. To check the robustness and versatility of DEGAA, we put forward ample experiments and ablation studies.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1474565160",
                        "name": "Rohit Lal"
                    },
                    {
                        "authorId": "1874889279",
                        "name": "Arihant Gaur"
                    },
                    {
                        "authorId": "2203910567",
                        "name": "Aadhithya Iyer"
                    },
                    {
                        "authorId": "2203910590",
                        "name": "Muhammed Abdullah Shaikh"
                    },
                    {
                        "authorId": "2139890413",
                        "name": "Ritik Agrawal"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6fa043cf67d7dff6169441f57df56d22033dd183",
                "externalIds": {
                    "DOI": "10.1016/j.egyai.2023.100242",
                    "CorpusId": 257168703
                },
                "corpusId": 257168703,
                "publicationVenue": {
                    "id": "317431e6-4e6a-4e79-b126-a2924bbd4144",
                    "name": "Energy and AI",
                    "type": "journal",
                    "alternate_names": [
                        "Energy AI"
                    ],
                    "issn": "2666-5468",
                    "url": "https://www.journals.elsevier.com/energy-and-ai"
                },
                "url": "https://www.semanticscholar.org/paper/6fa043cf67d7dff6169441f57df56d22033dd183",
                "title": "Smart energy management system framework for population dynamics modelling and suitable energy trajectories identification in islanded micro-grids",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1395882562",
                        "name": "Mehdi Mounsif"
                    },
                    {
                        "authorId": "2083717490",
                        "name": "Fabienne M\u00e9dard"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "26c61243f679b2cfb1edb4a8f95d1b44132c38f3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-12067",
                    "ArXiv": "2301.12067",
                    "DOI": "10.48550/arXiv.2301.12067",
                    "CorpusId": 256389943
                },
                "corpusId": 256389943,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/26c61243f679b2cfb1edb4a8f95d1b44132c38f3",
                "title": "Learning Optimal Features via Partial Invariance",
                "abstract": "Learning models that are robust to distribution shifts is a key concern in the context of their real-life applicability. Invariant Risk Minimization (IRM) is a popular framework that aims to learn robust models from multiple environments. The success of IRM requires an important assumption: the underlying causal mechanisms/features remain invariant across environments. When not satisfied, we show that IRM can over-constrain the predictor and to remedy this, we propose a relaxation via partial invariance. In this work, we theoretically highlight the sub-optimality of IRM and then demonstrate how learning from a partition of training domains can help improve invariant models. Several experiments, conducted both in linear settings as well as with deep neural networks on tasks over both language and image data, allow us to verify our conclusions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1491449524",
                        "name": "Moulik Choraria"
                    },
                    {
                        "authorId": "51065644",
                        "name": "Ibtihal Ferwana"
                    },
                    {
                        "authorId": "40429226",
                        "name": "Ankur Mani"
                    },
                    {
                        "authorId": "1697944",
                        "name": "L. Varshney"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In feature-related algorithms, domain-invariant representation learning including kernel-based methods[4], adversarial learning[5][6] and invariant risk minimization[7][8] are proposed."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e13b021d4975d798ddc244eb04c78a3ac10a0fcb",
                "externalIds": {
                    "ArXiv": "2301.09120",
                    "DBLP": "journals/corr/abs-2301-09120",
                    "DOI": "10.48550/arXiv.2301.09120",
                    "CorpusId": 256105772
                },
                "corpusId": 256105772,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e13b021d4975d798ddc244eb04c78a3ac10a0fcb",
                "title": "Causality-based Dual-Contrastive Learning Framework for Domain Generalization",
                "abstract": "Domain Generalization (DG) is essentially a sub-branch of out-of-distribution generalization, which trains models from multiple source domains and generalizes to unseen target domains. Recently, some domain generalization algorithms have emerged, but most of them were designed with non-transferable complex architecture. Additionally, contrastive learning has become a promising solution for simplicity and efficiency in DG. However, existing contrastive learning neglected domain shifts that caused severe model confusions. In this paper, we propose a Dual-Contrastive Learning (DCL) module on feature and prototype contrast. Moreover, we design a novel Causal Fusion Attention (CFA) module to fuse diverse views of a single image to attain prototype. Furthermore, we introduce a Similarity-based Hard-pair Mining (SHM) strategy to leverage information on diversity shift. Extensive experiments show that our method outperforms state-of-the-art algorithms on three DG datasets. The proposed algorithm can also serve as a plug-and-play module without usage of domain labels.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2117097755",
                        "name": "Zining Chen"
                    },
                    {
                        "authorId": "2142139646",
                        "name": "Weiqiu Wang"
                    },
                    {
                        "authorId": "2146630405",
                        "name": "Zhicheng Zhao"
                    },
                    {
                        "authorId": "1736046",
                        "name": "Aidong Men"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[31] assume that domains are generated by mixing causal and non-causal features and that the same object from different domains should have similar representations.",
                "One typical approach to domain generalization is to learn domain-invariant representations across domains [18, 30, 42, 3, 11, 14, 45, 31, 35].",
                "We compare our framework with previous domain generalization works including domain-invariant based methods [30, 41, 11, 45, 14, 31, 35, 8] and other state-of-the-art methods [15, 4, 9, 28, 13, 46, 34, 22, 7, 44, 10] including data augmentation based methods [34, 46, 7], meta-learning based methods [4, 28, 13], etc.",
                "48 ResNet50 Target Baseline Metareg DSON DMG ER RSC MatchDG SWAD Fishr mDSDI LRDG [4] [38] [11] [45] [22] [31] [10] [35] [8] (ours) A 82."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "221f89d98efa2be12996e1ee2668a32fee649342",
                "externalIds": {
                    "DBLP": "conf/nips/DingW0L0022",
                    "ArXiv": "2212.07101",
                    "DOI": "10.48550/arXiv.2212.07101",
                    "CorpusId": 254636222
                },
                "corpusId": 254636222,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/221f89d98efa2be12996e1ee2668a32fee649342",
                "title": "Domain Generalization by Learning and Removing Domain-specific Features",
                "abstract": "Deep Neural Networks (DNNs) suffer from domain shift when the test dataset follows a distribution different from the training dataset. Domain generalization aims to tackle this issue by learning a model that can generalize to unseen domains. In this paper, we propose a new approach that aims to explicitly remove domain-specific features for domain generalization. Following this approach, we propose a novel framework called Learning and Removing Domain-specific features for Generalization (LRDG) that learns a domain-invariant model by tactically removing domain-specific features from the input images. Specifically, we design a classifier to effectively learn the domain-specific features for each source domain, respectively. We then develop an encoder-decoder network to map each input image into a new image space where the learned domain-specific features are removed. With the images output by the encoder-decoder network, another classifier is designed to learn the domain-invariant features to conduct image classification. Extensive experiments demonstrate that our framework achieves superior performance compared with state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111071407",
                        "name": "Yuzhu Ding"
                    },
                    {
                        "authorId": "46659935",
                        "name": "Lei Wang"
                    },
                    {
                        "authorId": "3444382",
                        "name": "Binxin Liang"
                    },
                    {
                        "authorId": "2046798898",
                        "name": "Shuming Liang"
                    },
                    {
                        "authorId": "2155651793",
                        "name": "Yang Wang"
                    },
                    {
                        "authorId": "31811316",
                        "name": "Fangxiao Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The matching algorithm to produce features invariant to adversarial perturbations has been shown to produce robust models [33, 51]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "82d19ceba300875f108a91539ca555dfad142a99",
                "externalIds": {
                    "ArXiv": "2212.06079",
                    "DBLP": "conf/icml/MaoZJYWV23",
                    "DOI": "10.48550/arXiv.2212.06079",
                    "CorpusId": 254563940
                },
                "corpusId": 254563940,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/82d19ceba300875f108a91539ca555dfad142a99",
                "title": "Robust Perception through Equivariance",
                "abstract": "Deep networks for computer vision are not reliable when they encounter adversarial examples. In this paper, we introduce a framework that uses the dense intrinsic constraints in natural images to robustify inference. By introducing constraints at inference time, we can shift the burden of robustness from training to the inference algorithm, thereby allowing the model to adjust dynamically to each individual image's unique and potentially novel characteristics at inference time. Among different constraints, we find that equivariance-based constraints are most effective, because they allow dense constraints in the feature space without overly constraining the representation at a fine-grained level. Our theoretical results validate the importance of having such dense constraints at inference time. Our empirical experiments show that restoring feature equivariance at inference time defends against worst-case adversarial perturbations. The method obtains improved adversarial robustness on four datasets (ImageNet, Cityscapes, PASCAL VOC, and MS-COCO) on image recognition, semantic segmentation, and instance segmentation tasks. Project page is available at equi4robust.cs.columbia.edu.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7700460",
                        "name": "Chengzhi Mao"
                    },
                    {
                        "authorId": "2196168322",
                        "name": "Lingyu Zhang"
                    },
                    {
                        "authorId": "2066732439",
                        "name": "Abhishek Joshi"
                    },
                    {
                        "authorId": "2110694456",
                        "name": "Junfeng Yang"
                    },
                    {
                        "authorId": "2359832",
                        "name": "Hongya Wang"
                    },
                    {
                        "authorId": "1856025",
                        "name": "Carl Vondrick"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(4) Causal domain adaptation/generalization on graphs: Recent studies have revealed the importance of incorporating causal perspective to remove the spurious correlations (Arjovsky et al. 2019; Mahajan et al. 2021) and enhance the performance of domain adaptation/generalization."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0887f9cbbb7727de46c8b38544d8e0464d46fb16",
                "externalIds": {
                    "DBLP": "journals/aim/MaL22",
                    "DOI": "10.1002/aaai.12070",
                    "CorpusId": 254672613
                },
                "corpusId": 254672613,
                "publicationVenue": {
                    "id": "6fedff74-7525-4b7f-bbb4-4df4e23948e4",
                    "name": "The AI Magazine",
                    "type": "journal",
                    "alternate_names": [
                        "AI Mag",
                        "Ai Mag",
                        "Ai Magazine"
                    ],
                    "issn": "0738-4602",
                    "url": "https://www.aaai.org/Library/Magazine/magazine-library.php",
                    "alternate_urls": [
                        "https://www.aaai.org/ojs/index.php/aimagazine/",
                        "https://www.aaai.org/Magazine/magazine.php"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0887f9cbbb7727de46c8b38544d8e0464d46fb16",
                "title": "Learning Causality with Graphs",
                "abstract": "Recent years have witnessed a rocketing growth of machine learning methods on graph data, especially those powered by effective neural networks. Despite their success in different real-world scenarios, the majority of these methods on graphs only focus on predictive or descriptive tasks, but lack consideration of causality. Causal inference can reveal the causality inside data, promote human understanding of the learning process and model prediction, and serve as a significant component of artificial intelligence (AI). An important problem in causal inference is causal effect estimation, which aims to estimate the causal effects of a certain treatment (e.g., prescription of medicine) on an outcome (e.g., cure of disease) at an individual level (e.g., each patient) or a population level (e.g., a group of patients). In this paper, we introduce the background of causal effect estimation from observational data, envision the challenges of causal effect estimation with graphs, and then summarize representative approaches of causal effect estimation with graphs in recent years. Furthermore, we provide some insights for future research directions in related area. Link to video abstract: https://youtu.be/BpDPOOqw-ns",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157405959",
                        "name": "Jing Ma"
                    },
                    {
                        "authorId": "1737121128",
                        "name": "Jundong Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It can be observed that FAST surpasses MatchDG [31], a quite state-of-the-art (SOTA) causality-based method that aims to learn domain-invariant representation, by about 5.0% and 1.3% in ArtPainting and Sketch on PACS respectively.",
                "One drawback of MatchDG is its reliance on the availability of domain labels.",
                "On the other hand, MatchDG [31] assumes a causal graph where the content feature is independent of the domain given the hidden object and proposes a method that enforces this constraint to the classifier by alternately performing contrastive learning and finding good positive matches.",
                "Many causality-inspired approaches [17, 29, 31, 55] learn the domain-invariant representation or content feature that can be seen as a direct cause of the label.",
                "Such variable Z can be captured by several learning methods, such as style transfer models [15, 20] or contrastive learning [31, 53].",
                "Following previous works [29, 31, 69], we evaluate our proposed methods on three standard OOD generalisation benchmark datasets described below.",
                "The baselines are divided into two groups: non-causality-based methods (from MMD-AAE [25] to FACT [58]), and causality-based methods (from MatchDG [31] to CIRL [29]).",
                "Many recent state-of-the-art causality-based methods in OOD generalisation enforce regularisation with different data transformations on the learnt feature to achieve the domain-invariant feature [29, 31, 55].",
                "Causal inference has been applied to many deep learning research problems, especially in domain adaptation (DA) [30, 42, 62, 64] and domain generalisation (DG) [31, 35, 55].",
                "It can be observed that FAST surpasses MatchDG [31], a quite state-of-the-art (SOTA) causality-based method that aims to learn domain-invariant representation, by about 5.",
                "The baselines are divided into two groups: non-causality-based methods (from DeepAll [68] to FACT [58]), and causality-based methods (from MatchDG [31] to CIRL [29])."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fa2b08534993ab2c9448deecd718006b23f94650",
                "externalIds": {
                    "DBLP": "conf/kdd/NguyenDNDN23",
                    "ArXiv": "2212.03063",
                    "DOI": "10.1145/3580305.3599270",
                    "CorpusId": 259138417
                },
                "corpusId": 259138417,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/fa2b08534993ab2c9448deecd718006b23f94650",
                "title": "Causal Inference via Style Transfer for Out-of-distribution Generalisation",
                "abstract": "Out-of-distribution (OOD) generalisation aims to build a model that can generalise well on an unseen target domain using knowledge from multiple source domains. To this end, the model should seek the causal dependence between inputs and labels, which may be determined by the semantics of inputs and remain invariant across domains. However, statistical or non-causal methods often cannot capture this dependence and perform poorly due to not considering spurious correlations learnt from model training via unobserved confounders. A well-known existing causal inference method like back-door adjustment cannot be applied to remove spurious correlations as it requires the observation of confounders. In this paper, we propose a novel method that effectively deals with hidden confounders by successfully implementing front-door adjustment (FA). FA requires the choice of a mediator, which we regard as the semantic information of images that helps access the causal mechanism without the need for observing confounders. Further, we propose to estimate the combination of the mediator with other observed images in the front-door formula via style transfer algorithms. Our use of style transfer to estimate FA is novel and sensible for OOD generalisation, which we justify by extensive experimental results on widely used benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32125163",
                        "name": "Toan Q. Nguyen"
                    },
                    {
                        "authorId": "36072771",
                        "name": "Kien Do"
                    },
                    {
                        "authorId": "1779016",
                        "name": "D. Nguyen"
                    },
                    {
                        "authorId": "2159149299",
                        "name": "Bao Duong"
                    },
                    {
                        "authorId": "150322672",
                        "name": "T. Nguyen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[42] proposed an unsupervised matching algorithm and [76] introduced the propensity score matching method to balance the mini-batch.",
                "Representative methods include incorporating invariance constraints by designing new loss functions [5, 8, 28, 37], learning latent semantic features in causal graphs by VAE [38,40], and eliminating selection bias by matching [42,76]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fdf70e3275e7c6f6f5eff43a167b4812f68d7df6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-14594",
                    "ArXiv": "2211.14594",
                    "DOI": "10.48550/arXiv.2211.14594",
                    "CorpusId": 254044432
                },
                "corpusId": 254044432,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fdf70e3275e7c6f6f5eff43a167b4812f68d7df6",
                "title": "Direct-Effect Risk Minimization for Domain Generalization",
                "abstract": "We study the problem of out-of-distribution (o.o.d.) generalization where spurious correlations of attributes vary across training and test domains. This is known as the problem of correlation shift and has posed concerns on the reliability of machine learning. In this work, we introduce the concepts of direct and indirect effects from causal inference to the domain generalization problem. We argue that models that learn direct effects minimize the worst-case risk across correlation-shifted domains. To eliminate the indirect effects, our algorithm consists of two stages: in the first stage, we learn an indirect-effect representation by minimizing the prediction error of domain labels using the representation and the class labels; in the second stage, we remove the indirect effects learned in the first stage by matching each data with another data of similar indirect-effect representation but of different class labels in the training and validation phase. Our approach is shown to be compatible with existing methods and improve the generalization performance of them on correlation-shifted datasets. Experiments on 5 correlation-shifted datasets and the DomainBed benchmark verify the effectiveness of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192674200",
                        "name": "Yuhui Li"
                    },
                    {
                        "authorId": "2192669149",
                        "name": "Zejia Wu"
                    },
                    {
                        "authorId": "2188875716",
                        "name": "Chao Zhang"
                    },
                    {
                        "authorId": "40975176",
                        "name": "Hongyang Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Benefiting from the recent advances in self-supervised learning [9,19], contrastive-based learning has been proposed as a promising solution [10, 16, 26, 33, 37, 38, 52, 64, 71]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cd15c78e0578a681992f5c92d2d0c21036181d73",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-06843",
                    "ArXiv": "2211.06843",
                    "DOI": "10.48550/arXiv.2211.06843",
                    "CorpusId": 253510795
                },
                "corpusId": 253510795,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cd15c78e0578a681992f5c92d2d0c21036181d73",
                "title": "Generalization Beyond Feature Alignment: Concept Activation-Guided Contrastive Learning",
                "abstract": "Learning invariant representations via contrastive learning has seen state-of-the-art performance in domain generalization (DG). Despite such success, in this paper, we \ufb01nd that its core learning strategy \u2013 feature alignment \u2013 could heavily hinder the model generalization. Inspired by the recent progress in neuron interpretability, we characterize this problem from a neuron activation view. Speci\ufb01cally, by treating feature elements as neuron activation states, we show that conventional alignment methods tend to deteriorate the diversity of learned invariant features, as they indiscriminately minimize all neuron activation differences. This instead ignores rich relations among neurons \u2013 many of them often identify the same visual concepts though they emerge differently. With this \ufb01nding, we present a simple yet effective approach, Concept Contrast (CoCo), which relaxes element-wise feature alignments by contrasting high-level concepts encoded in neurons. This approach is highly \ufb02exible and can be integrated into any contrastive method in DG. Through extensive experiments, we further demonstrate that our CoCo promotes the diversity of feature representations, and consistently improves model generalization capability over the DomainBed benchmark.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35435925",
                        "name": "Y. Liu"
                    },
                    {
                        "authorId": "2645793",
                        "name": "Chris Xing Tian"
                    },
                    {
                        "authorId": "144878149",
                        "name": "Haoliang Li"
                    },
                    {
                        "authorId": "2144190725",
                        "name": "Shiqi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Domain generalization (DG) [5, 6, 7, 8], as well as unsupervised domain adaptation (UDA) methods [9, 10, 11], aims to solve this hard and significant problem."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c89749b78682ed62d9dd1e3871b8f86dc3e8fad8",
                "externalIds": {
                    "ArXiv": "2211.04393",
                    "DBLP": "journals/corr/abs-2211-04393",
                    "DOI": "10.48550/arXiv.2211.04393",
                    "CorpusId": 253397461
                },
                "corpusId": 253397461,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c89749b78682ed62d9dd1e3871b8f86dc3e8fad8",
                "title": "Normalization Perturbation: A Simple Domain Generalization Method for Real-World Domain Shifts",
                "abstract": "Improving model's generalizability against domain shifts is crucial, especially for safety-critical applications such as autonomous driving. Real-world domain styles can vary substantially due to environment changes and sensor noises, but deep models only know the training domain style. Such domain style gap impedes model generalization on diverse real-world domains. Our proposed Normalization Perturbation (NP) can effectively overcome this domain style overfitting problem. We observe that this problem is mainly caused by the biased distribution of low-level features learned in shallow CNN layers. Thus, we propose to perturb the channel statistics of source domain features to synthesize various latent styles, so that the trained deep model can perceive diverse potential domains and generalizes well even without observations of target domain data in training. We further explore the style-sensitive channels for effective style synthesis. Normalization Perturbation only relies on a single source domain and is surprisingly effective and extremely easy to implement. Extensive experiments verify the effectiveness of our method for generalizing models under real-world domain shifts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2105557652",
                        "name": "Qi Fan"
                    },
                    {
                        "authorId": "151136071",
                        "name": "Mattia Segu"
                    },
                    {
                        "authorId": "5068280",
                        "name": "Yu-Wing Tai"
                    },
                    {
                        "authorId": "1807197",
                        "name": "F. Yu"
                    },
                    {
                        "authorId": "2088295",
                        "name": "Chi-Keung Tang"
                    },
                    {
                        "authorId": "48920094",
                        "name": "B. Schiele"
                    },
                    {
                        "authorId": "1778526",
                        "name": "Dengxin Dai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many studies have focused on how causality can help deep learning address long-standing issues such as interpretability [44-45, 47, 170-171], generalization [172-174], robustness [175-177], and fairness [178-180]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2642594866fedbc6b35ceab3b8c3dc3c3ec5d57d",
                "externalIds": {
                    "ArXiv": "2211.03374",
                    "DBLP": "journals/corr/abs-2211-03374",
                    "DOI": "10.48550/arXiv.2211.03374",
                    "CorpusId": 253383801
                },
                "corpusId": 253383801,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2642594866fedbc6b35ceab3b8c3dc3c3ec5d57d",
                "title": "Deep Causal Learning: Representation, Discovery and Inference",
                "abstract": "Causal learning has attracted much attention in recent years because causality reveals the essential relationship between things and indicates how the world progresses. However, there are many problems and bottlenecks in traditional causal learning methods, such as high-dimensional unstructured variables, combinatorial optimization problems, unknown intervention, unobserved confounders, selection bias and estimation bias. Deep causal learning, that is, causal learning based on deep neural networks, brings new insights for addressing these problems. While many deep learning-based causal discovery and causal inference methods have been proposed, there is a lack of reviews exploring the internal mechanism of deep learning to improve causal learning. In this article, we comprehensively review how deep learning can contribute to causal learning by addressing conventional challenges from three aspects: representation, discovery, and inference. We point out that deep causal learning is important for the theoretical extension and application expansion of causal science and is also an indispensable part of general artificial intelligence. We conclude the article with a summary of open issues and potential directions for future work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1576127334",
                        "name": "Zizhen Deng"
                    },
                    {
                        "authorId": "145091983",
                        "name": "Xiaolong Zheng"
                    },
                    {
                        "authorId": "2069991042",
                        "name": "Hu Tian"
                    },
                    {
                        "authorId": "2106064673",
                        "name": "D. Zeng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar to Okapi, MatchDG [51] draws upon causal matching to tackle DG."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "682e744f1b4100bb2528cbf3e71e8e3b854a6fef",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-05236",
                    "ArXiv": "2211.05236",
                    "DOI": "10.48550/arXiv.2211.05236",
                    "CorpusId": 253447381
                },
                "corpusId": 253447381,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/682e744f1b4100bb2528cbf3e71e8e3b854a6fef",
                "title": "Okapi: Generalising Better by Making Statistical Matches Match",
                "abstract": "We propose Okapi, a simple, efficient, and general method for robust semi-supervised learning based on online statistical matching. Our method uses a nearest-neighbours-based matching procedure to generate cross-domain views for a consistency loss, while eliminating statistical outliers. In order to perform the online matching in a runtime- and memory-efficient way, we draw upon the self-supervised literature and combine a memory bank with a slow-moving momentum encoder. The consistency loss is applied within the feature space, rather than on the predictive distribution, making the method agnostic to both the modality and the task in question. We experiment on the WILDS 2.0 datasets Sagawa et al., which significantly expands the range of modalities, applications, and shifts available for studying and benchmarking real-world unsupervised adaptation. Contrary to Sagawa et al., we show that it is in fact possible to leverage additional unlabelled data to improve upon empirical risk minimisation (ERM) results with the right method. Our method outperforms the baseline methods in terms of out-of-distribution (OOD) generalisation on the iWildCam (a multi-class classification task) and PovertyMap (a regression task) image datasets as well as the CivilComments (a binary classification task) text dataset. Furthermore, from a qualitative perspective, we show the matches obtained from the learned encoder are strongly semantically related. Code for our paper is publicly available at https://github.com/wearepal/okapi/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "147413412",
                        "name": "Myles Bartlett"
                    },
                    {
                        "authorId": "2180166207",
                        "name": "Sara Romiti"
                    },
                    {
                        "authorId": "1790503",
                        "name": "V. Sharmanska"
                    },
                    {
                        "authorId": "1704531",
                        "name": "Novi Quadrianto"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are also other methods for domain-invariant learning on DG [28, 29]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "592ee7f56dd16537d8f726a3334ee0bb74b08cd9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-05228",
                    "ArXiv": "2211.05228",
                    "DOI": "10.48550/arXiv.2211.05228",
                    "CorpusId": 253447229
                },
                "corpusId": 253447229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/592ee7f56dd16537d8f726a3334ee0bb74b08cd9",
                "title": "FIXED: Frustratingly Easy Domain Generalization with Mixup",
                "abstract": "Domain generalization (DG) aims to learn a generalizable model from multiple training domains such that it can perform well on unseen target domains. A popular strategy is to augment training data to benefit generalization through methods such as Mixup~\\cite{zhang2018mixup}. While the vanilla Mixup can be directly applied, theoretical and empirical investigations uncover several shortcomings that limit its performance. Firstly, Mixup cannot effectively identify the domain and class information that can be used for learning invariant representations. Secondly, Mixup may introduce synthetic noisy data points via random interpolation, which lowers its discrimination capability. Based on the analysis, we propose a simple yet effective enhancement for Mixup-based DG, namely domain-invariant Feature mIXup (FIX). It learns domain-invariant representations for Mixup. To further enhance discrimination, we leverage existing techniques to enlarge margins among classes to further propose the domain-invariant Feature MIXup with Enhanced Discrimination (FIXED) approach. We present theoretical insights about guarantees on its effectiveness. Extensive experiments on seven public datasets across two modalities including image classification (Digits-DG, PACS, Office-Home) and time series (DSADS, PAMAP2, UCI-HAR, and USC-HAD) demonstrate that our approach significantly outperforms nine state-of-the-art related methods, beating the best performing baseline by 6.5\\% on average in terms of test accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154002462",
                        "name": "Wang Lu"
                    },
                    {
                        "authorId": "1519290245",
                        "name": "Jindong Wang"
                    },
                    {
                        "authorId": "2187083103",
                        "name": "Han Yu"
                    },
                    {
                        "authorId": "48544864",
                        "name": "Lei Huang"
                    },
                    {
                        "authorId": "48505793",
                        "name": "Xiang Zhang"
                    },
                    {
                        "authorId": "2155140959",
                        "name": "Yiqiang Chen"
                    },
                    {
                        "authorId": "1576441343",
                        "name": "Xingxu Xie"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b376193a78391ab1b9e1616a2f0dcb735b60c0ed",
                "externalIds": {
                    "DBLP": "conf/icmla/NguyenLISA22",
                    "ArXiv": "2210.15000",
                    "DOI": "10.1109/ICMLA55696.2022.00132",
                    "CorpusId": 253157629
                },
                "corpusId": 253157629,
                "publicationVenue": {
                    "id": "f6752838-f268-4a1b-87e7-c5f30a36713c",
                    "name": "International Conference on Machine Learning and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Mach Learn Appl",
                        "ICMLA"
                    ],
                    "url": "http://www.icmla-conference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b376193a78391ab1b9e1616a2f0dcb735b60c0ed",
                "title": "Trade-off between reconstruction loss and feature alignment for domain generalization",
                "abstract": "Domain generalization (DG) is a branch of transfer learning that aims to train the learning models on several seen domains and subsequently apply these pre-trained models to other unseen (unknown but related) domains. To deal with challenging settings in DG where both data and label of the unseen domain are not available at training time, the most common approach is to design the classifiers based on the domain-invariant representation features, i.e., the latent representations that are unchanged and transferable between domains. Contrary to popular belief, we show that designing classifiers based on invariant representation features alone is necessary but insufficient in DG. Our analysis indicates the necessity of imposing a constraint on the reconstruction loss induced by representation functions to preserve most of the relevant information about the label in the latent space. More importantly, we point out the trade-off between minimizing the reconstruction loss and achieving domain alignment in DG. Our theoretical results motivate a new DG framework that jointly optimizes the reconstruction loss and the domain discrepancy. Both theoretical and numerical results are provided to justify our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "123233017",
                        "name": "Thuan Q. Nguyen"
                    },
                    {
                        "authorId": "51454393",
                        "name": "Boyang Lyu"
                    },
                    {
                        "authorId": "1756038",
                        "name": "P. Ishwar"
                    },
                    {
                        "authorId": "121848090",
                        "name": "matthias. scheutz"
                    },
                    {
                        "authorId": "2139553851",
                        "name": "Shuchin Aeron"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The justification is that the topological structure of these prototypes is unexplored, making them being sensitive to the change of environments [44, 9, 79].",
                "We follow the same settings as in [44] to create spurious correlation.",
                "Prevailing approaches [54, 53, 16, 44, 61, 40] resort to causal graphs [50] to explicitly identify causal and non-causal factors with theoretical guarantees.",
                ", ERM [64], IRM [3], CSD [53] and MatchDG [44]) are cited from [44].",
                "However, we argue that these operations may still be constrained by the spurious correlation as style-agnostic representation is not sufficient to ensure semantic invariance [44].",
                "The robustness of ASTR is further evaluated on a challenging Chest X-ray benchmark [44] that has explicit spurious correlation.",
                "(2) To statistically avoid the spurious correlations, a body of research [54, 53, 16, 44, 61, 40] proposes to identify the common component through latent factor disentanglement."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4664221ba7f7878773ffa19139849290b2d6b308",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-07571",
                    "ArXiv": "2210.07571",
                    "DOI": "10.48550/arXiv.2210.07571",
                    "CorpusId": 252907345
                },
                "corpusId": 252907345,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4664221ba7f7878773ffa19139849290b2d6b308",
                "title": "Mix and Reason: Reasoning over Semantic Topology with Data Mixing for Domain Generalization",
                "abstract": "Domain generalization (DG) enables generalizing a learning machine from multiple seen source domains to an unseen target one. The general objective of DG methods is to learn semantic representations that are independent of domain labels, which is theoretically sound but empirically challenged due to the complex mixture of common and domain-specific factors. Although disentangling the representations into two disjoint parts has been gaining momentum in DG, the strong presumption over the data limits its efficacy in many real-world scenarios. In this paper, we propose Mix and Reason (\\mire), a new DG framework that learns semantic representations via enforcing the structural invariance of semantic topology. \\mire\\ consists of two key components, namely, Category-aware Data Mixing (CDM) and Adaptive Semantic Topology Refinement (ASTR). CDM mixes two images from different domains in virtue of activation maps generated by two complementary classification losses, making the classifier focus on the representations of semantic objects. ASTR introduces relation graphs to represent semantic topology, which is progressively refined via the interactions between local feature aggregation and global cross-domain relational reasoning. Experiments on multiple DG benchmarks validate the effectiveness and robustness of the proposed \\mire.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145762695",
                        "name": "Chaoqi Chen"
                    },
                    {
                        "authorId": "2110211447",
                        "name": "Luyao Tang"
                    },
                    {
                        "authorId": "40405236",
                        "name": "Feng Liu"
                    },
                    {
                        "authorId": "22658530",
                        "name": "Gangming Zhao"
                    },
                    {
                        "authorId": "2108716197",
                        "name": "Yue Huang"
                    },
                    {
                        "authorId": "1841911",
                        "name": "Yizhou Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", meta learning [16], casual matching [24], disentangled representation [15] and adaptive methods [6]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d3f4ec711a95151422eb5b0639dc308c9766e091",
                "externalIds": {
                    "DBLP": "conf/mm/ZhaoZC022",
                    "DOI": "10.1145/3503161.3547812",
                    "CorpusId": 252782715
                },
                "corpusId": 252782715,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d3f4ec711a95151422eb5b0639dc308c9766e091",
                "title": "Revisiting Stochastic Learning for Generalizable Person Re-identification",
                "abstract": "Generalizable person re-identification aims to achieve a well generalization capability on target domains without accessing target data. Existing methods focus on suppressing domain-specific information or simulating unseen environments by meta-learning strategies, which could damage the capture ability on fine-grained visual patterns or lead to overfitting issues by the repetitive training of episodes. In this paper, we revisit the stochastic behaviors from two different perspectives: 1) Stochastic splitting-sliding sampler. It splits domain sources into approximately equal sample-size subsets and selects several subsets from various sources by a sliding window, forcing the model to step out of local minimums under stochastic sources. 2) Variance-varying gradient dropout. Gradients in parts of network are also selected by a sliding window and multiplied by binary masks generated from Bernoulli distribution, making gradients in varying variance and preventing the model from local minimums. By applying these two proposed stochastic behaviors, the model achieves a better generalization performance on unseen target domains without any additional computation costs or auxiliary modules. Extensive experiments demonstrate that our proposed model is effective and outperforms state-of-the-art methods on public domain generalizable person Re-ID benchmarks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2130185194",
                        "name": "Jia-jun Zhao"
                    },
                    {
                        "authorId": "151469503",
                        "name": "Yifan Zhao"
                    },
                    {
                        "authorId": "1425091532",
                        "name": "Xiaowu Chen"
                    },
                    {
                        "authorId": "49404071",
                        "name": "Jia Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Amplifying the domain gap in image decomposition: Domain gaps between training and testing data have been a long-standing issue in Computer Vision [2, 29, 30, 46]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "18094fa25ee2bfab47ee7636c7cc8023e80e600e",
                "externalIds": {
                    "DBLP": "conf/icb/MishraSCHBJ22",
                    "DOI": "10.1109/IJCB54206.2022.10007943",
                    "CorpusId": 254125885
                },
                "corpusId": 254125885,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/18094fa25ee2bfab47ee7636c7cc8023e80e600e",
                "title": "Improved Presentation Attack Detection Using Image Decomposition",
                "abstract": "Presentation attack detection (PAD) is a critical component in secure face authentication. We present a PAD algorithm to distinguish face spoofs generated by a photograph of a subject from live images. Our method uses an image decomposition network to extract albedo and normal. The domain gap between the real and spoof face images leads to easily identifiable differences, especially between the re-covered albedo maps. We enhance this domain gap by retraining existing methods using supervised contrastive loss. We present empirical and theoretical analysis that demonstrates that contrast and lighting effects can play a significant role in PAD; these show up particularly in the recovered albedo. Finally, we demonstrate that by combining all of these methods we achieve state-of-the-art results on both intra-dataset testing for CelebA-Spoof, OULU, CASIA-SURF datasets and inter-dataset setting on SiW, CASIA-MFSD, Replay-Attack and MSU-MFSD datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2850880",
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "authorId": "144037321",
                        "name": "K. Sengupta"
                    },
                    {
                        "authorId": "39336289",
                        "name": "Wen-Sheng Chu"
                    },
                    {
                        "authorId": "2057487802",
                        "name": "Max Horowitz-Gelb"
                    },
                    {
                        "authorId": "35119991",
                        "name": "Sofien Bouaziz"
                    },
                    {
                        "authorId": "2059096514",
                        "name": "David Jacobs"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7ede79760454da631a20bce64c86c05612bbbbc3",
                "externalIds": {
                    "ArXiv": "2210.04155",
                    "DBLP": "journals/corr/abs-2210-04155",
                    "DOI": "10.48550/arXiv.2210.04155",
                    "CorpusId": 252780700,
                    "PubMed": "37440378"
                },
                "corpusId": 252780700,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7ede79760454da631a20bce64c86c05612bbbbc3",
                "title": "Constrained Maximum Cross-Domain Likelihood for Domain Generalization",
                "abstract": "As a recent noticeable topic, domain generalization aims to learn a generalizable model on multiple source domains, which is expected to perform well on unseen test domains. Great efforts have been made to learn domain-invariant features by aligning distributions across domains. However, existing works are often designed based on some relaxed conditions which are generally hard to satisfy and fail to realize the desired joint distribution alignment. In this article, we propose a novel domain generalization method, which originates from an intuitive idea that a domain-invariant classifier can be learned by minimizing the Kullback-Leibler (KL)-divergence between posterior distributions from different domains. To enhance the generalizability of the learned classifier, we formalize the optimization objective as an expectation computed on the ground-truth marginal distribution. Nevertheless, it also presents two obvious deficiencies, one of which is the side-effect of entropy increase in KL-divergence and the other is the unavailability of ground-truth marginal distributions. For the former, we introduce a term named maximum in-domain likelihood to maintain the discrimination of the learned domain-invariant representation space. For the latter, we approximate the ground-truth marginal distribution with source domains under a reasonable convex hull assumption. Finally, a constrained maximum cross-domain likelihood (CMCL) optimization problem is deduced, by solving which the joint distributions are naturally aligned. An alternating optimization strategy is carefully designed to approximately solve this optimization problem. Extensive experiments on four standard benchmark datasets, i.e., Digits-DG, PACS, Office-Home, and miniDomainNet, highlight the superior performance of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46698009",
                        "name": "Jianxin Lin"
                    },
                    {
                        "authorId": "2111321753",
                        "name": "Yongqiang Tang"
                    },
                    {
                        "authorId": "2110149124",
                        "name": "Junping Wang"
                    },
                    {
                        "authorId": "2108167359",
                        "name": "Wensheng Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026literature on images Zhou et al. (2021b); Wang et al. (2022), the goal then is use one of these methods to learn Xc/Zc from observed data: regularization Mahajan et al. (2021); Lee et al. (2022), weighting Sagawa et al. (2019); Yao et al. (2022) or data augmentation Zhou et al. (2021b)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c26ed75366c0fb2047ad5d7f2b292e0f83057f83",
                "externalIds": {
                    "ArXiv": "2210.10636",
                    "CorpusId": 252992603
                },
                "corpusId": 252992603,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c26ed75366c0fb2047ad5d7f2b292e0f83057f83",
                "title": "Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems",
                "abstract": "Given a user's input text, text-matching recommender systems output relevant items by comparing the input text to available items' description, such as product-to-product recommendation on e-commerce platforms. As users' interests and item inventory are expected to change, it is important for a text-matching system to generalize to data shifts, a task known as out-of-distribution (OOD) generalization. However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization. For a product recommendation task, fine-tuning obtains worse accuracy than the base model when recommending items in a new category or for a future time period. To explain this generalization failure, we consider an intervention-based importance metric, which shows that a fine-tuned model captures spurious correlations and fails to learn the causal features that determine the relevance between any two text inputs. Moreover, standard methods for causal regularization do not apply in this setting, because unlike in images, there exist no universally spurious features in a text-matching task (the same token may be spurious or causal depending on the text it is being matched to). For OOD generalization on text inputs, therefore, we highlight a different goal: avoiding high importance scores for certain features. We do so using an intervention-based regularizer that constraints the causal effect of any token on the model's relevance score to be similar to the base model. Results on Amazon product and 3 question recommendation datasets show that our proposed regularizer improves generalization for both in-distribution and OOD evaluation, especially in difficult scenarios when the base model is not accurate.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48691300",
                        "name": "Parikshit Bansal"
                    },
                    {
                        "authorId": "40252215",
                        "name": "Yashoteja Prabhu"
                    },
                    {
                        "authorId": "3528206",
                        "name": "Emre K\u0131c\u0131man"
                    },
                    {
                        "authorId": "144676398",
                        "name": "Amit Sharma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "With the idea of invariant causal mechanism, increasing attention has been paid to causality-inspired generalization learning (Wald et al. 2021; Liu et al. 2021; Sun et al. 2021; Mahajan, Tople, and Sharma 2021).",
                "There is a series of prior works (Christiansen et al. 2021; Yuan et al. 2021; Mahajan, Tople, and Sharma 2021; Chen et al. 2021; Sun et al. 2021; Li et al. 2021a) investigate domain generalization task and provide their own approaches in causal view.",
                "In addition, several recent works(Mahajan, Tople, and Sharma 2021; Chen et al. 2021; Sun et al. 2021; Li et al. 2021a) have further identified other hidden causal relationship.",
                "In order to deal with domain shift problems, Domain Generalization (DG) (Zhang et al. 2021; Chen et al. 2021; Liu et al.\n2021; Sun et al. 2021; Mahajan, Tople, and Sharma 2021; Wald et al. 2021) is introduced, which aims to learn stable knowledge from multiple source domains and train a\u2026",
                "Especially in works(Li et al. 2021a; Mahajan, Tople, and Sharma 2021; Chen et al. 2021), these SCMs introduce variables that are only relevant to objects.",
                "Mahajan et al. (Mahajan, Tople, and Sharma 2021) consider high-level causal features and domain-dependent features while the labels are only determined by the former.",
                "(Mahajan, Tople, and Sharma 2021) proposes a causal graph with the domainD and objectO which is similar to ours, as shown in Figure 1 (d)."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a92160282e21493ab410c2d85ae3a6df2cadbb51",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-02655",
                    "ArXiv": "2210.02655",
                    "DOI": "10.48550/arXiv.2210.02655",
                    "CorpusId": 252735286
                },
                "corpusId": 252735286,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a92160282e21493ab410c2d85ae3a6df2cadbb51",
                "title": "Domain Generalization via Contrastive Causal Learning",
                "abstract": "Domain Generalization (DG) aims to learn a model that can generalize well to unseen target domains from a set of source domains. With the idea of invariant causal mechanism, a lot of efforts have been put into learning robust causal effects which are determined by the object yet insensitive to the domain changes. Despite the invariance of causal effects, they are dif\ufb01cult to be quanti\ufb01ed and optimized. Inspired by the abil- ity that humans adapt to new environments by prior knowledge, We develop a novel Contrastive Causal Model (CCM) to transfer unseen images to taught knowledge which are the features of seen images, and quantify the causal effects based on taught knowledge. Considering the transfer is af-fected by domain shifts in DG, we propose a more inclusive causal graph to describe DG task. Based on this causal graph, CCM controls the domain factor to cut off excess causal paths and uses the remaining part to calculate the causal effects of images to labels via the front-door criterion. Specif- ically, CCM is composed of three components: (i) domain-conditioned supervised learning which teaches CCM the cor- relation between images and labels, (ii) causal effect learning which helps CCM measure the true causal effects of im- ages to labels, (iii) contrastive similarity learning which clusters the features of images that belong to the same class and provides the quanti\ufb01cation of similarity. Finally, we test the performance of CCM on multiple datasets including PACS , Of\ufb01ceHome , and TerraIncognita . The extensive experiments demonstrate that CCM surpasses the previous DG methods with clear margins.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1580304095",
                        "name": "Qiaowei Miao"
                    },
                    {
                        "authorId": "38511927",
                        "name": "Junkun Yuan"
                    },
                    {
                        "authorId": "33870528",
                        "name": "Kun Kuang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "74946bb440c0c54734cfd126be44aed1ee2f18ac",
                "externalIds": {
                    "DBLP": "conf/ictai/LiMZ22",
                    "DOI": "10.1109/ICTAI56018.2022.00091",
                    "CorpusId": 258219358
                },
                "corpusId": 258219358,
                "publicationVenue": {
                    "id": "ba1e9488-a629-4abe-a0c2-ec2c79c91616",
                    "name": "IEEE International Conference on Tools with Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Tool Artif Intell",
                        "ICTAI",
                        "IEEE Int Conf Tool Artif Intell",
                        "International Conference on Tools with Artificial Intelligence"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1488"
                },
                "url": "https://www.semanticscholar.org/paper/74946bb440c0c54734cfd126be44aed1ee2f18ac",
                "title": "On Domain Generalization for Batched Prediction: the Benefit of Contextual Adversarial Training",
                "abstract": "This paper considers the domain generalization problem in which the labels of the query batch may be predicted at once and the model is allowed to assign a label to a query example utilizing all information contained in the query batch. In this setting, we identify two problems in the standard adaptive risk minimization (ARM) framework, namely, mismatched context and overfitted context, which are most severely manifested respectively at small and large support batch sizes. The existence of these problems signifies a fundamental tradeoff between within-domain learning and cross-domain generalization. We also propose two context adversarial training approaches to alleviate these problems so as to achieve a better tradeoff. We demonstrate experimentally that the proposed approaches uniformly outperform the standard ARM training for all choices of support batch sizes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1698974",
                        "name": "Chune Li"
                    },
                    {
                        "authorId": "2047889",
                        "name": "Yongyi Mao"
                    },
                    {
                        "authorId": "2109975984",
                        "name": "Richong Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The authors in [13, 23, 24] aim to utilize auxiliary variables to separate causal from non-causal features, and learn the representations accordingly.",
                "Thus, causality can be a useful tool in capturing the invariance present in the data, justifying the range of methods that have leveraged different causal theories for improving the generalization capabilities of models [4, 13].",
                "For instance, the leave-one-domain-out rule is one of the most prominent when dealing with multi-source domain generalization [13, 23, 31, 118].",
                "Thus, to improve the generalization performance, a range of works [13, 19, 49] leveraged real-world medical imaging datasets such as Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) [82].",
                "[13] argue that for representations, the class-conditional domain invariant objective is inadequate."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fb82c9d1b8fe321d477ab8cd8653800aa685a64c",
                "externalIds": {
                    "ArXiv": "2209.15177",
                    "DBLP": "journals/corr/abs-2209-15177",
                    "DOI": "10.48550/arXiv.2209.15177",
                    "CorpusId": 252668353
                },
                "corpusId": 252668353,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fb82c9d1b8fe321d477ab8cd8653800aa685a64c",
                "title": "Domain Generalization - A Causal Perspective",
                "abstract": "Machine learning models rely on various assumptions to attain high accuracy. One of the preliminary assumptions of these models is the independent and identical distribution, which suggests that the train and test data are sampled from the same distribution. However, this assumption seldom holds in the real world due to distribution shifts. As a result models that rely on this assumption exhibit poor generalization capabilities. Over the recent years, dedicated efforts have been made to improve the generalization capabilities of these models collectively known as -- \\textit{domain generalization methods}. The primary idea behind these methods is to identify stable features or mechanisms that remain invariant across the different distributions. Many generalization approaches employ causal theories to describe invariance since causality and invariance are inextricably intertwined. However, current surveys deal with the causality-aware domain generalization methods on a very high-level. Furthermore, we argue that it is possible to categorize the methods based on how causality is leveraged in that method and in which part of the model pipeline is it used. To this end, we categorize the causal domain generalization methods into three categories, namely, (i) Invariance via Causal Data Augmentation methods which are applied during the data pre-processing stage, (ii) Invariance via Causal representation learning methods that are utilized during the representation learning stage, and (iii) Invariance via Transferring Causal mechanisms methods that are applied during the classification stage of the pipeline. Furthermore, this survey includes in-depth insights into benchmark datasets and code repositories for domain generalization methods. We conclude the survey with insights and discussions on future directions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "73409823",
                        "name": "Paras Sheth"
                    },
                    {
                        "authorId": "11064745",
                        "name": "Raha Moraffah"
                    },
                    {
                        "authorId": "1720972",
                        "name": "K. Candan"
                    },
                    {
                        "authorId": "19251475",
                        "name": "A. Raglin"
                    },
                    {
                        "authorId": "2146398099",
                        "name": "Huan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d07f137bdb29d9fb2bb01750f634d9dec8c078c9",
                "externalIds": {
                    "DBLP": "conf/ccis/LinTWZ22",
                    "ArXiv": "2209.08253",
                    "DOI": "10.1109/CCIS57298.2022.10016385",
                    "CorpusId": 252368240
                },
                "corpusId": 252368240,
                "publicationVenue": {
                    "id": "b8e3fb18-a48d-49f0-96bd-38cca7124b0d",
                    "name": "International Conference on Cloud Computing and Intelligence Systems",
                    "type": "conference",
                    "alternate_names": [
                        "CCIS",
                        "Int Conf Cloud Comput Intell Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d07f137bdb29d9fb2bb01750f634d9dec8c078c9",
                "title": "Mitigating Both Covariate and Conditional Shift for Domain Generalization",
                "abstract": "Domain generalization (DG) aims to learn a model on several source domains, hoping that the model can generalize well to unseen target domains. The distribution shift between domains contains the covariate shift and conditional shift, both of which the model must be able to handle for better generalizability. In this paper, a novel DG method is proposed to deal with the distribution shift via Visual Alignment and Uncertainty-guided belief Ensemble (VAUE). Specifically, for the covariate shift, a visual alignment module is designed to align the distribution of image style to a common empirical Gaussian distribution so that the covariate shift can be eliminated in the visual space. For the conditional shift, we adopt an uncertainty-guided belief ensemble strategy based on subjective logic and Dempster-Shafer theory. The conditional distribution given a test sample is estimated by the dynamic combination of that of source domains. Comprehensive experiments are conducted to demonstrate the superior performance of the proposed method on four widely used datasets, i.e., Office-Home, VLCS, TerraIncognita, and PACS.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46698009",
                        "name": "Jianxin Lin"
                    },
                    {
                        "authorId": "2111321753",
                        "name": "Yongqiang Tang"
                    },
                    {
                        "authorId": "2110149124",
                        "name": "Junping Wang"
                    },
                    {
                        "authorId": "2108167359",
                        "name": "Wensheng Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To reduce environmentspecific overfitting, techniques including invariant risk minimization [35, 36], self-training [37, 38], dropout [39] and feature selection [40\u201342] are proposed to focus more on features with causal relationships to the outcome."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d69ef3f2bbb3077db630ec148db18c0866bd7b4a",
                "externalIds": {
                    "ArXiv": "2209.07682",
                    "DBLP": "journals/corr/abs-2209-07682",
                    "DOI": "10.48550/arXiv.2209.07682",
                    "CorpusId": 252355020
                },
                "corpusId": 252355020,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d69ef3f2bbb3077db630ec148db18c0866bd7b4a",
                "title": "Masked Imitation Learning: Discovering Environment-Invariant Modalities in Multimodal Demonstrations",
                "abstract": "Multimodal demonstrations provide robots with an abundance of information to make sense of the world. However, such abundance may not always lead to good performance when it comes to learning sensorimotor control policies from human demonstrations. Extraneous data modalities can lead to state over-specification, where the state contains modalities that are not only useless for decision-making but also can change data distribution across environments. State over-specification leads to issues such as the learned policy not generalizing outside of the training data distribution. In this work, we propose Masked Imitation Learning (MIL) to address state over-specification by selectively using informative modalities. Specifically, we design a masked policy network with a binary mask to block certain modalities. We develop a bi-level optimization algorithm that learns this mask to accurately filter over-specified modalities. We demonstrate empirically that MIL outperforms baseline algorithms in simulated domains including MuJoCo and a robot arm environment using the Robomimic dataset, and effectively recovers the environment-invariant modalities on a multimodal dataset collected on a real robot. Our project website presents supplemental details and videos of our results at: https://tinyurl.com/masked-il",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2049032558",
                        "name": "Yilun Hao"
                    },
                    {
                        "authorId": "2144714403",
                        "name": "Ruinan Wang"
                    },
                    {
                        "authorId": "3451430",
                        "name": "Zhangjie Cao"
                    },
                    {
                        "authorId": null,
                        "name": "Zihan Wang"
                    },
                    {
                        "authorId": "7332443",
                        "name": "Yuchen Cui"
                    },
                    {
                        "authorId": "1779671",
                        "name": "Dorsa Sadigh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[25] proposed Exact Pareto Optimal (EPO) Search to find a preference-specific Pareto optimal solution.",
                "[25] proposed MatchDG and tried to deal with domain generalization from the view of a structural causal model."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d76d451c0e518ad98ff56cd2ab6357500fa5d5ea",
                "externalIds": {
                    "ArXiv": "2209.00652",
                    "DBLP": "journals/corr/abs-2209-00652",
                    "DOI": "10.48550/arXiv.2209.00652",
                    "CorpusId": 252070650
                },
                "corpusId": 252070650,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d76d451c0e518ad98ff56cd2ab6357500fa5d5ea",
                "title": "Towards Optimization and Model Selection for Domain Generalization: A Mixup-guided Solution",
                "abstract": "The distribution shifts between training and test data typically undermine the performance of deep learning models. In recent years, lots of work pays attention to domain generalization (DG) where distribution shift exists and target data are unseen. Despite the progress in algorithm design, two foundational factors have long been ignored: 1) the optimization for regularization-based objectives (e.g., distribution alignment), and 2) the model selection for DG since no knowledge about the target domain can be utilized. In this paper, we propose Mixup guided optimization and selection techniques for domain generalization. For optimization, we utilize an adapted Mixup to generate an out-of-distribution dataset that can guide the preference direction and optimize with Pareto optimization. For model selection, we generate a validation dataset with a closer distance to the target distribution, and thereby it can better represent the target data. We also present some theoretical insights behind our proposals. Comprehensive experiments on one visual classification benchmark and three time-series benchmarks demonstrate that our model optimization and selection techniques can largely improve the performance of existing domain generalization algorithms and even achieve new state-of-the-art results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154002462",
                        "name": "Wang Lu"
                    },
                    {
                        "authorId": "1519290245",
                        "name": "Jindong Wang"
                    },
                    {
                        "authorId": "2108024273",
                        "name": "Yidong Wang"
                    },
                    {
                        "authorId": "144931569",
                        "name": "Kan Ren"
                    },
                    {
                        "authorId": "2155140959",
                        "name": "Yiqiang Chen"
                    },
                    {
                        "authorId": "1576441343",
                        "name": "Xingxu Xie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Therefore, assuming zc \u2192 zs is more persuasive and consistent with previous works (Gong et al., 2016; Stojanov et al., 2019; Mahajan et al., 2021).",
                "This assumption has been employed by some recent works (Mahajan et al., 2021; Liu et al., 2021; Sun et al., 2021).",
                "Particularly, these two different labels, y\u0302 and y, has been simultaneously considered in Mahajan et al. (2021).\nzc causes zs: Here we consider having the object essence zc first, from which a latent style zs springs to render x."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9fdbe27355edd585a69f29105d45f4c11366be1b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-14161",
                    "ArXiv": "2208.14161",
                    "DOI": "10.48550/arXiv.2208.14161",
                    "CorpusId": 251928911
                },
                "corpusId": 251928911,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9fdbe27355edd585a69f29105d45f4c11366be1b",
                "title": "Identifying Latent Causal Content for Multi-Source Domain Adaptation",
                "abstract": "Multi-source domain adaptation (MSDA) learns to predict the labels in target domain data, under the setting that data from multiple source domains are labelled and data from the target domain are unlabelled. Most methods for this task focus on learning invariant representations across domains. However, their success relies heavily on the assumption that the label distribution remains consistent across domains, which may not hold in general real-world problems. In this paper, we propose a new and more flexible assumption, termed \\textit{latent covariate shift}, where a latent content variable $\\mathbf{z}_c$ and a latent style variable $\\mathbf{z}_s$ are introduced in the generative process, with the marginal distribution of $\\mathbf{z}_c$ changing across domains and the conditional distribution of the label given $\\mathbf{z}_c$ remaining invariant across domains. We show that although (completely) identifying the proposed latent causal model is challenging, the latent content variable can be identified up to scaling by using its dependence with labels from source domains, together with the identifiability conditions of nonlinear ICA. This motivates us to propose a novel method for MSDA, which learns the invariant label distribution conditional on the latent content variable, instead of learning invariant representations. Empirical evaluation on simulation and real data demonstrates the effectiveness of the proposed method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144422920",
                        "name": "Yuhang Liu"
                    },
                    {
                        "authorId": "2144352261",
                        "name": "Zhen Zhang"
                    },
                    {
                        "authorId": "2183355008",
                        "name": "Dong Gong"
                    },
                    {
                        "authorId": "29393235",
                        "name": "Mingming Gong"
                    },
                    {
                        "authorId": "1938684",
                        "name": "Biwei Huang"
                    },
                    {
                        "authorId": "2119016656",
                        "name": "Kun Zhang"
                    },
                    {
                        "authorId": "3177281",
                        "name": "Javen Qinfeng Shi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f0ca7321b23ac61781e9a0b1531af50b1f05db82",
                "externalIds": {
                    "DOI": "10.33258/bioex.v4i2.726",
                    "CorpusId": 251964673
                },
                "corpusId": 251964673,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f0ca7321b23ac61781e9a0b1531af50b1f05db82",
                "title": "Unambiguous Identification of Objects in Different Environments and Conditions Based on Holographic Machine Learning Algorithms",
                "abstract": "Unambiguous provision of results in different environments and conditions by machine learning algorithms is an unresolved problem until now. Solving the problem of machine learning with unambiguous provision of results in different environments and conditions can be approached by focusing on the psychophysical holographic process of human learning. A person, with a mental concentration of attention, experimentally teaches vision, hearing, psyche and mind in a holographic way and in a resonant way to perceive, recognize and recognize phenomena, processes, objects, subjects, meanings, music and other entities in various environments and conditions. A person experimentally teaches the psyche and feelings to rationally navigate in various environments and conditions. Holographic algorithms of experienced machine learning will help neural network ensembles to unambiguously recognize objects, subjects, music, texts in various environments and conditions using a model of recognizing their own or someone else's. Machine learning simulates holographic processes of human communication memorization of entities. Searching for objects in different environments in different conditions based on experienced machine learning simulates resonant associative processes of human entity detection. By simulating holographic processes of the human psyche based on artificial intelligence of machine learning with Fourier transformation, using full parametric sequences of necessary and sufficient data of holograms of target objects, it is possible to solve the problem of their unambiguous detection in different environments and in different conditions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "121608603",
                        "name": "E. Bryndin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, contrastive learning with multi-view data augmentation has shown their efficacy in self-supervised representation learning [45,19,4,31] and domain generalization [28]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "77d95e914e2d7f84750eb470c6615a039e17748a",
                "externalIds": {
                    "DBLP": "conf/eccv/NamCL22",
                    "ArXiv": "2208.10024",
                    "DOI": "10.48550/arXiv.2208.10024",
                    "CorpusId": 251718714
                },
                "corpusId": 251718714,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/77d95e914e2d7f84750eb470c6615a039e17748a",
                "title": "GCISG: Guided Causal Invariant Learning for Improved Syn-to-real Generalization",
                "abstract": "Training a deep learning model with artificially generated data can be an alternative when training data are scarce, yet it suffers from poor generalization performance due to a large domain gap. In this paper, we characterize the domain gap by using a causal framework for data generation. We assume that the real and synthetic data have common content variables but different style variables. Thus, a model trained on synthetic dataset might have poor generalization as the model learns the nuisance style variables. To that end, we propose causal invariance learning which encourages the model to learn a style-invariant representation that enhances the syn-to-real generalization. Furthermore, we propose a simple yet effective feature distillation method that prevents catastrophic forgetting of semantic knowledge of the real domain. In sum, we refer to our method as Guided Causal Invariant Syn-to-real Generalization that effectively improves the performance of syn-to-real generalization. We empirically verify the validity of proposed methods, and especially, our method achieves state-of-the-art on visual syn-to-real domain generalization tasks such as image classification and semantic segmentation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182292362",
                        "name": "Gilhyun Nam"
                    },
                    {
                        "authorId": "115438399",
                        "name": "Gyeongjae Choi"
                    },
                    {
                        "authorId": "2110049370",
                        "name": "Kyungmin Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "And many other alternatives could also be found [19, 29, 6].",
                "[29] Divyat Mahajan, Shruti Tople, and Amit Sharma."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8ce8a844d35a264d1fb8b928a00b48f27d4aafd6",
                "externalIds": {
                    "DBLP": "journals/entropy/DengJ23",
                    "ArXiv": "2208.07798",
                    "PubMedCentral": "9955031",
                    "DOI": "10.3390/e25020193",
                    "CorpusId": 251594538,
                    "PubMed": "36832560"
                },
                "corpusId": 251594538,
                "publicationVenue": {
                    "id": "8270cfe1-3713-4325-a7bd-c6a87eed889e",
                    "name": "Entropy",
                    "type": "journal",
                    "issn": "1099-4300",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606",
                    "alternate_urls": [
                        "http://www.mdpi.com/journal/entropy/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155606",
                        "https://www.mdpi.com/journal/entropy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8ce8a844d35a264d1fb8b928a00b48f27d4aafd6",
                "title": "Counterfactual Supervision-Based Information Bottleneck for Out-of-Distribution Generalization",
                "abstract": "Learning invariant (causal) features for out-of-distribution (OOD) generalization have attracted extensive attention recently, and among the proposals, invariant risk minimization (IRM) is a notable solution. In spite of its theoretical promise for linear regression, the challenges of using IRM in linear classification problems remain. By introducing the information bottleneck (IB) principle into the learning of IRM, the IB-IRM approach has demonstrated its power to solve these challenges. In this paper, we further improve IB-IRM from two aspects. First, we show that the key assumption of support overlap of invariant features used in IB-IRM guarantees OOD generalization, and it is still possible to achieve the optimal solution without this assumption. Second, we illustrate two failure modes where IB-IRM (and IRM) could fail in learning the invariant features, and to address such failures, we propose a Counterfactual Supervision-based Information Bottleneck (CSIB) learning algorithm that recovers the invariant features. By requiring counterfactual inference, CSIB works even when accessing data from a single environment. Empirical experiments on several datasets verify our theoretical results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2070545950",
                        "name": "Bin Deng"
                    },
                    {
                        "authorId": "2370507",
                        "name": "K. Jia"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "954279d37fdec7b5fa9d38020621cd2dec092c8d",
                "externalIds": {
                    "ArXiv": "2208.03644",
                    "DBLP": "journals/corr/abs-2208-03644",
                    "DOI": "10.1145/3503161.3548059",
                    "CorpusId": 251402948
                },
                "corpusId": 251402948,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/954279d37fdec7b5fa9d38020621cd2dec092c8d",
                "title": "Label-Efficient Domain Generalization via Collaborative Exploration and Generalization",
                "abstract": "Considerable progress has been made in domain generalization (DG) which aims to learn a generalizable model from multiple well-annotated source domains to unknown target domains. However, it can be prohibitively expensive to obtain sufficient annotation for source datasets in many real scenarios. To escape from the dilemma between domain generalization and annotation costs, in this paper, we introduce a novel task named label-efficient domain generalization (LEDG) to enable model generalization with label-limited source domains. To address this challenging task, we propose a novel framework called Collaborative Exploration and Generalization (CEG) which jointly optimizes active exploration and semi-supervised generalization. Specifically, in active exploration, to explore class and domain discriminability while avoiding information divergence and redundancy, we query the labels of the samples with the highest overall ranking of class uncertainty, domain representativeness, and information diversity. In semi-supervised generalization, we design MixUp-based intra- and inter-domain knowledge augmentation to expand domain knowledge and generalize domain invariance. We unify active exploration and semi-supervised generalization in a collaborative way and promote mutual enhancement between them, boosting model generalization with limited annotation. Extensive experiments show that CEG yields superior generalization performance. In particular, CEG can even use only 5% data annotation budget to achieve competitive results compared to the previous DG methods with fully labeled data on PACS dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "38511927",
                        "name": "Junkun Yuan"
                    },
                    {
                        "authorId": "2141122566",
                        "name": "Xu Ma"
                    },
                    {
                        "authorId": "1684692",
                        "name": "Defang Chen"
                    },
                    {
                        "authorId": "33870528",
                        "name": "Kun Kuang"
                    },
                    {
                        "authorId": "93192602",
                        "name": "Fei Wu"
                    },
                    {
                        "authorId": "2940227",
                        "name": "Lanfen Lin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7a5f6d9a257460a441661d5a4d361503ec7dd592",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-03462",
                    "ArXiv": "2208.03462",
                    "DOI": "10.48550/arXiv.2208.03462",
                    "CorpusId": 251402732
                },
                "corpusId": 251402732,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/7a5f6d9a257460a441661d5a4d361503ec7dd592",
                "title": "Class Is Invariant to Context and Vice Versa: On Learning Invariance for Out-Of-Distribution Generalization",
                "abstract": "Out-Of-Distribution generalization (OOD) is all about learning invariance against environmental changes. If the context in every class is evenly distributed, OOD would be trivial because the context can be easily removed due to an underlying principle: class is invariant to context. However, collecting such a balanced dataset is impractical. Learning on imbalanced data makes the model bias to context and thus hurts OOD. Therefore, the key to OOD is context balance. We argue that the widely adopted assumption in prior work, the context bias can be directly annotated or estimated from biased class prediction, renders the context incomplete or even incorrect. In contrast, we point out the everoverlooked other side of the above principle: context is also invariant to class, which motivates us to consider the classes (which are already labeled) as the varying environments to resolve context bias (without context labels). We implement this idea by minimizing the contrastive loss of intra-class sample similarity while assuring this similarity to be invariant across all classes. On benchmarks with various context biases and domain gaps, we show that a simple re-weighting based classifier equipped with our context estimation achieves state-of-the-art performance. We provide the theoretical justifications in Appendix and codes on https://github.com/simpleshinobu/IRMCon.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3167894",
                        "name": "Jiaxin Qi"
                    },
                    {
                        "authorId": "10817432",
                        "name": "Kaihua Tang"
                    },
                    {
                        "authorId": "2138109020",
                        "name": "Qianru Sun"
                    },
                    {
                        "authorId": "143863244",
                        "name": "Xiansheng Hua"
                    },
                    {
                        "authorId": "2119078220",
                        "name": "Hanwang Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also compare our proposed method with a variety of state-of-the-art domain generalization methods, including DeepAll [64], Jigen [3], CCSA [26], MMD-AAE [19] , CrossGrad [38], DDAIG [65], L2AOT [64], ATSRL [55], MetaReg [2] , Epi-FCR [18], MMLD [24], CSD [32], InfoDrop [40], MASF [9], Mixstyle [66], EISNet [48], MDGH [23], RSC [14] and FACT [53].",
                "Compared with the state-of-the-art methods, our method clearly beats the methods based on the GAN-based data augmentation and meta-learning methods, such as the latest L2A-OT [64], MDGH [23] and ATSRL [55].",
                "We also compare our proposed method with a variety of state-of-the-art domain generalization methods, including DeepAll [58], Jigen [3], CCSA [23], MMD-AAE [16] , CrossGrad [35], DDAIG [59], L2A-OT [58], ATSRL [50], MetaReg [2] , Epi-FCR [15], MMLD [21], CSD [29], InfoDrop [37], MASF [7], Mixstyle [60], EISNet [44], MDGH [20], RSC [11] and FACT [48]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f488bc27ae01a8d88bafb23f339867827895caba",
                "externalIds": {
                    "DBLP": "conf/mm/WangY0W022",
                    "ArXiv": "2208.02803",
                    "DOI": "10.1145/3503161.3547866",
                    "CorpusId": 252212037
                },
                "corpusId": 252212037,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f488bc27ae01a8d88bafb23f339867827895caba",
                "title": "Semantic Data Augmentation based Distance Metric Learning for Domain Generalization",
                "abstract": "Domain generalization (DG) aims to learn a model on one or more different but related source domains that could be generalized into an unseen target domain. Existing DG methods try to prompt the diversity of source domains for the model's generalization ability, while they may have to introduce auxiliary networks or striking computational costs. On the contrary, this work applies the implicit semantic augmentation in feature space to capture the diversity of source domains. Concretely, an additional loss function of distance metric learning (DML) is included to optimize the local geometry of data distribution. Besides, the logits from cross entropy loss with infinite augmentations is adopted as input features for the DML loss in lieu of the deep features. We also provide a theoretical analysis to show that the logits can approximate the distances defined on original features well. Further, we provide an in-depth analysis of the mechanism and rational behind our approach, which gives us a better understanding of why leverage logits in lieu of features can help domain generalization. The proposed DML loss with the implicit augmentation is incorporated into a recent DG method, that is, Fourier Augmented Co-Teacher framework (FACT). Meanwhile, our method also can be easily plugged into various DG methods. Extensive experiments on three benchmarks (Digits-DG, PACS and Office-Home) have demonstrated that the proposed method is able to achieve the state-of-the-art performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "66986539",
                        "name": "Mengzhu Wang"
                    },
                    {
                        "authorId": "1699363778",
                        "name": "Jianlong Yuan"
                    },
                    {
                        "authorId": "145303057",
                        "name": "Qi Qian"
                    },
                    {
                        "authorId": "2051262469",
                        "name": "Zhibin Wang"
                    },
                    {
                        "authorId": null,
                        "name": "Hao Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mahajan et al. (2021) have proposed that correlations independent of domain conditional on class (\u03a6(x) \u2aeb D\u2223Y ) are not necessarily causal correlations if P (x\u0307\u2223Y ) changes across domains.",
                "For instance, the computational complexity of causal matching in MatchDG (Mahajan et al., 2021) and gradient matching in FISH (Shi et al.",
                "Mahajan et al. (2021) state that learning representations independent of the domain after conditioning on the class label is insufficient for training a robust model.",
                "Backbones We take MatchDG (Mahajan et al., 2021), FISH (Shi et al., 2021b) and DANN (Ganin et al., 2016) as backbone algorithms.",
                "Backbones We take MatchDG (Mahajan et al., 2021), FISH (Shi et al.",
                "For instance, the computational complexity of causal matching in MatchDG (Mahajan et al., 2021) and gradient matching in FISH (Shi et al., 2021b) is O(n2) with n training domains."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "52a5ac81156bce6df0594b5dadda8dc35a39b0da",
                "externalIds": {
                    "ArXiv": "2207.13865",
                    "DBLP": "journals/corr/abs-2207-13865",
                    "DOI": "10.48550/arXiv.2207.13865",
                    "CorpusId": 251135207
                },
                "corpusId": 251135207,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/52a5ac81156bce6df0594b5dadda8dc35a39b0da",
                "title": "Diversity Boosted Learning for Domain Generalization with Large Number of Domains",
                "abstract": "Machine learning algorithms minimizing the average training loss usually suffer from poor generalization performance due to the greedy exploitation of correlations among the training data, which are not stable under distributional shifts. It inspires various works for domain generalization (DG), where a series of methods, such as Causal Matching and FISH, work by pairwise domain operations. They would need $O(n^2)$ pairwise domain operations with $n$ domains, where each one is often highly expensive. Moreover, while a common objective in the DG literature is to learn invariant representations against domain-induced spurious correlations, we highlight the importance of mitigating spurious correlations caused by objects. Based on the observation that diversity helps mitigate spurious correlations, we propose a Diversity boosted twO-level saMplIng framework (DOMI) utilizing Determinantal Point Processes (DPPs) to efficiently sample the most informative ones among large number of domains. We show that DOMI helps train robust models against spurious correlations from both domain-side and object-side, substantially enhancing the performance of the backbone DG algorithms on rotated MNIST, rotated Fashion MNIST, and iwildcam datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159512430",
                        "name": "Xinlin Leng"
                    },
                    {
                        "authorId": "2166492136",
                        "name": "Xiaoying Tang"
                    },
                    {
                        "authorId": "2419616",
                        "name": "Yatao Bian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ac2df928db8dbd7e9eebe655edb759525b3634e5",
                "externalIds": {
                    "DBLP": "conf/ijcnn/DongOH22",
                    "DOI": "10.1109/IJCNN55064.2022.9892061",
                    "CorpusId": 252626213
                },
                "corpusId": 252626213,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/ac2df928db8dbd7e9eebe655edb759525b3634e5",
                "title": "Domain Generalization via Maximum Intra-class Average Diameter Minimization",
                "abstract": "Domain generalization aims to train a model which can generalize to unknown domains after training on several source domains. When the training domain and testing domain are under similar distributions, the well-trained model can achieve similar performance on two domains. But when generalize to unseen domains, the performance of the model will drop significantly, because of the existence of the domain-shift. To solve domain generalization problems, it is important to extract rich and general features from data. To improve the generalize ability, recently, lots of contrastive learning based feature alignment approaches had been proposed. Instance-level contrastive learning approaches force the model to learn domain-invariant features by pushing the positive pairs closer in the representation space. But they ignored the effect of class-level feature distribution in the representation space. In this study, the authors propose a simple method Maximum Intra-class Average Diameter minimization (MIADM) to regularize the class-level feature distribution. The authors evaluate our method on three benchmark datasets including PACS, VLCS, and OfficeHome. The extensive experimental results demonstrate that our method achieves competitive performance compared with SOTA methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149469765",
                        "name": "Benyu Dong"
                    },
                    {
                        "authorId": "2924631",
                        "name": "Chung-Ming Own"
                    },
                    {
                        "authorId": "117110855",
                        "name": "Qinghua Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "If the spurious attributes are domain labels, the conditional independence in Theorem 1 becomes the ones in (Liu et al., 2015; Hu et al., 2020; Mahajan et al., 2021), while they do not explore its correlation with the OOD generalization.",
                ", (Hu et al., 2020; Mahajan et al., 2021; Ben-David et al., 2007; 2010; Muandet et al., 2013; Ganin et al., 2016), but none of them directly control the OOD generalization error.",
                "Besides, the counterexample in Mahajan et al. (2021) violates our conditional invariant assumption in (1) and hence is not contrary to our theorem.",
                "Notably, in contrast to the existing metrics related to OOD generalization (Hu et al., 2020; Mahajan et al., 2021), our CSV can control the OOD generalization error.",
                "\u2026perspectives, e.g., distributional robust optimization (Sinha et al., 2018; Volpi et al., 2018; Sagawa et al., 2019; Yi et al., 2021b; Levy et al., 2020) or causal inference (Arjovsky et al., 2019; He et al., 2021; Liu et al., 2021b; Mahajan et al., 2021; Wang et al., 2022; Ye et al., 2021).",
                "To this end, Arjovsky et al. (2019); Hu et al. (2020); Li et al. (2018); Mahajan et al. (2021); Heinze-Deml & Meinshausen (2021); Krueger et al. (2021); Wald et al. (2021); Seo et al. (2022) propose plenty of invariant metrics as training regularizer.",
                "Our definition is different from the ones in (Mahajan et al., 2021; Makar & D\u2019Amour, 2022), as they rely on a causal directed acyclic graph and the existence of a sufficient statistic such that Y only affects X through it.",
                "The OOD generalization error is also connected to many other metrics e.g., (Hu et al., 2020; Mahajan et al., 2021; Ben-David et al., 2007; 2010; Muandet et al., 2013; Ganin et al., 2016), but none of them directly control the OOD generalization error."
            ],
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "132505e9c068b50f10665055f357c2fa0e9b6b6e",
                "externalIds": {
                    "DBLP": "conf/iclr/Yi0SLM23",
                    "ArXiv": "2207.06687",
                    "CorpusId": 257205872
                },
                "corpusId": 257205872,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/132505e9c068b50f10665055f357c2fa0e9b6b6e",
                "title": "Breaking Correlation Shift via Conditional Invariant Regularizer",
                "abstract": "Recently, generalization on out-of-distribution (OOD) data with correlation shift has attracted great attentions. The correlation shift is caused by the spurious attributes that correlate to the class label, as the correlation between them may vary in training and test data. For such a problem, we show that given the class label, the models that are conditionally independent of spurious attributes are OOD generalizable. Based on this, a metric Conditional Spurious Variation (CSV) which controls the OOD generalization error, is proposed to measure such conditional independence. To improve the OOD generalization, we regularize the training process with the proposed CSV. Under mild assumptions, our training objective can be formulated as a nonconvex-concave mini-max problem. An algorithm with a provable convergence rate is proposed to solve the problem. Extensive empirical results verify our algorithm's efficacy in improving OOD generalization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3541183",
                        "name": "Mingyang Yi"
                    },
                    {
                        "authorId": "144570917",
                        "name": "Ruoyu Wang"
                    },
                    {
                        "authorId": "2136769001",
                        "name": "Jiacheng Sun"
                    },
                    {
                        "authorId": "7718952",
                        "name": "Zhenguo Li"
                    },
                    {
                        "authorId": "2249674",
                        "name": "Zhi-Ming Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Then the main classifier is regularized to have the same representation for such pairs of inputs ([3, 26]).",
                "[26] Divyat Mahajan, Shruti Tople, and Amit Sharma."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ee2990dcda6bba863cfbc078dc217c4961315f57",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-04153",
                    "ArXiv": "2207.04153",
                    "DOI": "10.48550/arXiv.2207.04153",
                    "CorpusId": 250425701
                },
                "corpusId": 250425701,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ee2990dcda6bba863cfbc078dc217c4961315f57",
                "title": "Probing Classifiers are Unreliable for Concept Removal and Detection",
                "abstract": "Neural network models trained on text data have been found to encode undesirable linguistic or sensitive concepts in their representation. Removing such concepts is non-trivial because of a complex relationship between the concept, text input, and the learnt representation. Recent work has proposed post-hoc and adversarial methods to remove such unwanted concepts from a model's representation. Through an extensive theoretical and empirical analysis, we show that these methods can be counter-productive: they are unable to remove the concepts entirely, and in the worst case may end up destroying all task-relevant features. The reason is the methods' reliance on a probing classifier as a proxy for the concept. Even under the most favorable conditions for learning a probing classifier when a concept's relevant features in representation space alone can provide 100% accuracy, we prove that a probing classifier is likely to use non-concept features and thus post-hoc or adversarial methods will fail to remove the concept correctly. These theoretical implications are confirmed by experiments on models trained on synthetic, Multi-NLI, and Twitter datasets. For sensitive applications of concept removal such as fairness, we recommend caution against using these methods and propose a spuriousness metric to gauge the quality of the final classifier.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109224465",
                        "name": "Abhinav Kumar"
                    },
                    {
                        "authorId": "40348583",
                        "name": "Chenhao Tan"
                    },
                    {
                        "authorId": "2143678801",
                        "name": "Amit Sharma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Existing DG methods handle domain shift from various perspectives, including domain alignment [35]\u2013[40], training strategy [41]\u2013 [43], data augmentation [28], [32], [44]\u2013[46], and the causal mechanism [27], [47]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "38f8fc966deb5c0c9e1bc8a5145da87c18bfe246",
                "externalIds": {
                    "ArXiv": "2207.03132",
                    "DBLP": "journals/corr/abs-2207-03132",
                    "DOI": "10.48550/arXiv.2207.03132",
                    "CorpusId": 250334203
                },
                "corpusId": 250334203,
                "publicationVenue": {
                    "id": "10e76a35-58d6-443c-9683-fc16f2dd0a92",
                    "name": "IEEE transactions on multimedia",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Transactions on Multimedia",
                        "IEEE Trans Multimedia",
                        "IEEE trans multimedia"
                    ],
                    "issn": "1520-9210",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046"
                },
                "url": "https://www.semanticscholar.org/paper/38f8fc966deb5c0c9e1bc8a5145da87c18bfe246",
                "title": "Style Interleaved Learning for Generalizable Person Re-identification",
                "abstract": "Domain generalization (DG) for person re-identification (ReID) is a challenging problem, as access to target domain data is not permitted during the training process. Most existing DG ReID methods update the feature extractor and classifier parameters based on the same features. This common practice causes the model to overfit to existing feature styles in the source domain, resulting in sub-optimal generalization ability on target domains. To solve this problem, we propose a novel style interleaved learning (IL) framework. Unlike conventional learning strategies, IL incorporates two forward propagations and one backward propagation for each iteration. We employ the features of interleaved styles to update the feature extractor and classifiers using different forward propagations, which helps to prevent the model from overfitting to certain domain styles. To generate interleaved feature styles, we further propose a new feature stylization approach. It produces a wide range of meaningful styles that are both different and independent from the original styles in the source domain, which caters to the IL methodology. Extensive experimental results show that our model not only consistently outperforms state-of-the-art methods on large-scale benchmarks for DG ReID, but also has clear advantages in computational efficiency. The code is available at https://github.com/WentaoTan/Interleaved-Learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2087228450",
                        "name": "Wentao Tan"
                    },
                    {
                        "authorId": "2108238034",
                        "name": "Pengfei Wang"
                    },
                    {
                        "authorId": "144116132",
                        "name": "Changxing Ding"
                    },
                    {
                        "authorId": "29393235",
                        "name": "Mingming Gong"
                    },
                    {
                        "authorId": "2370507",
                        "name": "K. Jia"
                    }
                ]
            }
        },
        {
            "contexts": [
                "By focusing on observed co-occurrence patterns, they can be easily misled by such spurious correlations and hard to identify the real causal correlations [16]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "edafcbaf70537468ddac4b5ce2ad483aec9fc775",
                "externalIds": {
                    "DBLP": "conf/sigir/MuLZWDW22",
                    "DOI": "10.1145/3477495.3531934",
                    "CorpusId": 250340148
                },
                "corpusId": 250340148,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/edafcbaf70537468ddac4b5ce2ad483aec9fc775",
                "title": "Alleviating Spurious Correlations in Knowledge-aware Recommendations through Counterfactual Generator",
                "abstract": "Limited by the statistical-based machine learning framework, a spurious correlation is likely to appear in existing knowledge-aware recommendation methods. It refers to a knowledge fact that appears causal to the user behaviors (inferred by the recommender) but is not in fact. For tackling this issue, we present a novel approach to discovering and alleviating the potential spurious correlations from a counterfactual perspective. To be specific, our approach consists of two counterfactual generators and a recommender. The counterfactual generators are designed to generate counterfactual interactions via reinforcement learning, while the recommender is implemented with two different graph neural networks to aggregate the information from KG and user-item interactions respectively. The counterfactual generators and recommender are integrated in a mutually collaborative way. With this approach, the recommender helps the counterfactual generators better identify potential spurious correlations and generate high-quality counterfactual interactions, while the counterfactual generators help the recommender weaken the influence of the potential spurious correlations simultaneously. Extensive experiments on three real-world datasets have shown the effectiveness of the proposed approach by comparing it with a number of competitive baselines. Our implementation code is available at: https://github.com/RUCAIBox/CGKR.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51396195",
                        "name": "Shanlei Mu"
                    },
                    {
                        "authorId": "2110479359",
                        "name": "Yaliang Li"
                    },
                    {
                        "authorId": "2542603",
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "authorId": "2115891766",
                        "name": "Jingyuan Wang"
                    },
                    {
                        "authorId": "1696332",
                        "name": "Bolin Ding"
                    },
                    {
                        "authorId": "153693432",
                        "name": "Ji-rong Wen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[25] Divyat Mahajan, Shruti Tople, and Amit Sharma."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dd6ccdf9720d3600c7b0c3a27f4941096906b91c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-14534",
                    "ArXiv": "2206.14534",
                    "DOI": "10.48550/arXiv.2206.14534",
                    "CorpusId": 250113467
                },
                "corpusId": 250113467,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/dd6ccdf9720d3600c7b0c3a27f4941096906b91c",
                "title": "When Does Group Invariant Learning Survive Spurious Correlations?",
                "abstract": "By inferring latent groups in the training data, recent works introduce invariant learning to the case where environment annotations are unavailable. Typically, learning group invariance under a majority/minority split is empirically shown to be effective in improving out-of-distribution generalization on many datasets. However, theoretical guarantee for these methods on learning invariant mechanisms is lacking. In this paper, we reveal the insufficiency of existing group invariant learning methods in preventing classifiers from depending on spurious correlations in the training set. Specifically, we propose two criteria on judging such sufficiency. Theoretically and empirically, we show that existing methods can violate both criteria and thus fail in generalizing to spurious correlation shifts. Motivated by this, we design a new group invariant learning method, which constructs groups with statistical independence tests, and reweights samples by group label proportion to meet the criteria. Experiments on both synthetic and real data demonstrate that the new method significantly outperforms existing group invariant learning methods in generalizing to spurious correlation shifts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "98716335",
                        "name": "Yimeng Chen"
                    },
                    {
                        "authorId": "51130380",
                        "name": "Ruibin Xiong"
                    },
                    {
                        "authorId": "2174196923",
                        "name": "Zhiming Ma"
                    },
                    {
                        "authorId": "37510256",
                        "name": "Yanyan Lan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Meanwhile, the ML model is also trained to perform consistently across all environments, which is achieved by adding a penalty term to the loss function (Arjovsky et al., 2019; Koyama and Yamaguchi, 2020; Krueger et al., 2021; Mahajan et al., 2021)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e5ff436aa8e640f511dc7a675e78e36b410dad91",
                "externalIds": {
                    "ArXiv": "2206.14917",
                    "DBLP": "journals/corr/abs-2206-14917",
                    "DOI": "10.1016/j.cma.2022.115569",
                    "CorpusId": 250144228
                },
                "corpusId": 250144228,
                "publicationVenue": {
                    "id": "3bfaa538-a67d-47d7-bfda-6f82748e9a29",
                    "name": "Computer Methods in Applied Mechanics and Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Method Appl Mech Eng"
                    ],
                    "issn": "0045-7825",
                    "url": "https://www.journals.elsevier.com/computer-methods-in-applied-mechanics-and-engineering",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00457825"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e5ff436aa8e640f511dc7a675e78e36b410dad91",
                "title": "Towards out of distribution generalization for problems in mechanics",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3274910",
                        "name": "Lingxiao Yuan"
                    },
                    {
                        "authorId": "4159960",
                        "name": "Harold S. Park"
                    },
                    {
                        "authorId": "3804927",
                        "name": "E. Lejeune"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4a1938c9fd3db160a34b8673afd6ba51f8c151f6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-07876",
                    "ArXiv": "2206.07876",
                    "DOI": "10.1109/ICPR56361.2022.9956338",
                    "CorpusId": 249712399
                },
                "corpusId": 249712399,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4a1938c9fd3db160a34b8673afd6ba51f8c151f6",
                "title": "Domain Generalization via Selective Consistency Regularization for Time Series Classification",
                "abstract": "Domain generalization methods aim to learn models robust to domain shift with data from a limited number of source domains and without access to target domain samples during training. Popular domain alignment methods for domain generalization seek to extract domain-invariant features by minimizing the discrepancy between feature distributions across all domains, disregarding inter-domain relationships. In this paper, we instead propose a novel representation learning methodology that selectively enforces prediction consistency between source domains estimated to be closely-related. Specifically, we hypothesize that domains share different class-informative representations, so instead of aligning all domains which can cause negative transfer, we only regularize the discrepancy between closely-related domains. We apply our method to time-series classification tasks and conduct comprehensive experiments on three public real-world datasets. Our method significantly improves over the baseline and achieves better or competitive performance in comparison with state-of-the-art methods in terms of both accuracy and model calibration.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108346455",
                        "name": "Wenyu Zhang"
                    },
                    {
                        "authorId": "122101015",
                        "name": "Mohamed Ragab"
                    },
                    {
                        "authorId": "2121484",
                        "name": "Chuan-Sheng Foo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Given a dataset with auxiliary attributes and their relationship with the target label, CACM constrains the model\u2019s representation to obey the conditional independence constraints satisfied by causal features of the label, generalizing past work on causality-based regularization [10, 14, 15] to multi-attribute shifts.",
                "[10] Divyat Mahajan, Shruti Tople, and Amit Sharma.",
                "Following [10], the two training domains have p as 0.",
                "Here, we compare the performance of two popular independence constraints in the literature [10]: unconditional Xc \u22a5\u22a5 A|E, and conditional on label Xc \u22a5\u22a5 A|Y,E (we condition on E for fully generality) on Synthetic Causal, Confounded and Selected shift datasets (Table 6).",
                "Our synthetic dataset is constructed based on the data-generating processes of the slab dataset [10, 30].",
                "Partly because advances in representation learning for DG [8, 9, 10, 7, 11, 12] have focused on either one of the shifts, these studies find that performance of state-of-the-art DG algorithms are not consistent across different shifts: algorithms performing well on datasets with one kind of shift fail on datasets with another kind of shift.",
                "Our canonical multi-attribute graph generalizes the DG graph from [10] that considered an Independent domain/environment as the only attribute.",
                "We evaluate two constraints motivated by DG literature [10]: unconditional Xc \u22a5\u22a5 A|E, and conditional on label Xc \u22a5\u22a5 A|Y,E.",
                "Our extended slab dataset, adds to the setting from [10] by using non-binary attributes and class labels to create a more challenging task and allows us to study DG algorithms in the presence of linear spurious features.",
                "We utilize a strategy from past work [10, 14] to use graph structure of the underlying data-generating process (DGP)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "810f0c7a6f8e0e684cb0be4a7bd1675fc8a0bbb7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-07837",
                    "ArXiv": "2206.07837",
                    "DOI": "10.48550/arXiv.2206.07837",
                    "CorpusId": 249712407
                },
                "corpusId": 249712407,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/810f0c7a6f8e0e684cb0be4a7bd1675fc8a0bbb7",
                "title": "Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization",
                "abstract": "Recent empirical studies on domain generalization (DG) have shown that DG algorithms that perform well on some distribution shifts fail on others, and no state-of-the-art DG algorithm performs consistently well on all shifts. Moreover, real-world data often has multiple distribution shifts over different attributes; hence we introduce multi-attribute distribution shift datasets and find that the accuracy of existing DG algorithms falls even further. To explain these results, we provide a formal characterization of generalization under multi-attribute shifts using a canonical causal graph. Based on the relationship between spurious attributes and the classification label, we obtain realizations of the canonical causal graph that characterize common distribution shifts and show that each shift entails different independence constraints over observed variables. As a result, we prove that any algorithm based on a single, fixed constraint cannot work well across all shifts, providing theoretical evidence for mixed empirical results on DG algorithms. Based on this insight, we develop Causally Adaptive Constraint Minimization (CACM), an algorithm that uses knowledge about the data-generating process to adaptively identify and apply the correct independence constraints for regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds datasets, covering binary and multi-valued attributes and labels, show that adaptive dataset-dependent constraints lead to the highest accuracy on unseen domains whereas incorrect constraints fail to do so. Our results demonstrate the importance of modeling the causal relationships inherent in the data-generating process.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2084554148",
                        "name": "Jivat Neet Kaur"
                    },
                    {
                        "authorId": "3528206",
                        "name": "Emre K\u0131c\u0131man"
                    },
                    {
                        "authorId": "2109648033",
                        "name": "Amit Sharma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It further inspires plentiful invariant learning works (Parascandolo et al., 2021; Mahajan et al., 2021; Creager et al., 2021; Wald et al., 2021; Ahuja et al., 2021a; Chen et al., 2022b; Lin et al., 2022b)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9a7a51cf95e5cb796847ec6d32c0b9ed95f1eec2",
                "externalIds": {
                    "ArXiv": "2206.07766",
                    "DBLP": "conf/iclr/0002ZBXWZ00Z0C23",
                    "CorpusId": 257280243
                },
                "corpusId": 257280243,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9a7a51cf95e5cb796847ec6d32c0b9ed95f1eec2",
                "title": "Pareto Invariant Risk Minimization: Towards Mitigating the Optimization Dilemma in Out-of-Distribution Generalization",
                "abstract": "Recently, there has been a growing surge of interest in enabling machine learning systems to generalize well to Out-of-Distribution (OOD) data. Most efforts are devoted to advancing optimization objectives that regularize models to capture the underlying invariance; however, there often are compromises in the optimization process of these OOD objectives: i) Many OOD objectives have to be relaxed as penalty terms of Empirical Risk Minimization (ERM) for the ease of optimization, while the relaxed forms can weaken the robustness of the original objective; ii) The penalty terms also require careful tuning of the penalty weights due to the intrinsic conflicts between ERM and OOD objectives. Consequently, these compromises could easily lead to suboptimal performance of either the ERM or OOD objective. To address these issues, we introduce a multi-objective optimization (MOO) perspective to understand the OOD optimization process, and propose a new optimization scheme called PAreto Invariant Risk Minimization (PAIR). PAIR improves the robustness of OOD objectives by cooperatively optimizing with other OOD objectives, thereby bridging the gaps caused by the relaxations. Then PAIR approaches a Pareto optimal solution that trades off the ERM and OOD objectives properly. Extensive experiments on challenging benchmarks, WILDS, show that PAIR alleviates the compromises and yields top OOD performances.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108962670",
                        "name": "Yongqiang Chen"
                    },
                    {
                        "authorId": "2075360362",
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "authorId": "2419616",
                        "name": "Yatao Bian"
                    },
                    {
                        "authorId": "2051756680",
                        "name": "Binghui Xie"
                    },
                    {
                        "authorId": "2152564746",
                        "name": "Bing Wu"
                    },
                    {
                        "authorId": "2109116068",
                        "name": "Yonggang Zhang"
                    },
                    {
                        "authorId": "47737190",
                        "name": "Kaili Ma"
                    },
                    {
                        "authorId": "50841357",
                        "name": "Han Yang"
                    },
                    {
                        "authorId": "144259957",
                        "name": "P. Zhao"
                    },
                    {
                        "authorId": "2153287285",
                        "name": "Bo Han"
                    },
                    {
                        "authorId": "2116502293",
                        "name": "James Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These works can broadly be divided into three categories\u2014learning invariant representations [39, 40, 41, 42], causal representation learning to embed priors in the learning strategy [43, 44, 45], and custom optimization methods to enable generalization [46, 47, 48, 49]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "99fa29bec0f8824ec40cd777f14fd11de1a5c9fb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-07802",
                    "ArXiv": "2206.07802",
                    "DOI": "10.48550/arXiv.2206.07802",
                    "CorpusId": 249712096
                },
                "corpusId": 249712096,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/99fa29bec0f8824ec40cd777f14fd11de1a5c9fb",
                "title": "What makes domain generalization hard?",
                "abstract": "While several methodologies have been proposed for the daunting task of domain generalization, understanding what makes this task challenging has received little attention. Here we present SemanticDG (Semantic Domain Generalization): a benchmark with 15 photo-realistic domains with the same geometry, scene layout and camera parameters as the popular 3D ScanNet dataset, but with controlled domain shifts in lighting, materials, and viewpoints. Using this benchmark, we investigate the impact of each of these semantic shifts on generalization independently. Visual recognition models easily generalize to novel lighting, but struggle with distribution shifts in materials and viewpoints. Inspired by human vision, we hypothesize that scene context can serve as a bridge to help models generalize across material and viewpoint domain shifts and propose a context-aware vision transformer along with a contrastive loss over material and viewpoint changes to address these domain shifts. Our approach (dubbed as CDCNet) outperforms existing domain generalization methods by over an 18% margin. As a critical benchmark, we also conduct psychophysics experiments and find that humans generalize equally well across lighting, materials and viewpoints. The benchmark and computational model introduced here help understand the challenges associated with generalization across domains and provide initial steps towards extrapolation to semantic distribution shifts. We include all data and source code in the supplement.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7232330",
                        "name": "Spandan Madan"
                    },
                    {
                        "authorId": "2170538624",
                        "name": "Li You"
                    },
                    {
                        "authorId": "2418491",
                        "name": "Mengmi Zhang"
                    },
                    {
                        "authorId": "143758236",
                        "name": "H. Pfister"
                    },
                    {
                        "authorId": "2066787605",
                        "name": "Gabriel Kreiman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such methods often aim to find invariant data representations using new loss function designs that incorporate the invariance conditions across different domains into the training process (Arjovsky et al., 2020; Mahajan et al., 2021; Liu et al., 2021; Lu et al., 2022; Wald et al., 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "63d30a86ec3e590c2032aeec80a50493d9889c9a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-05263",
                    "ArXiv": "2206.05263",
                    "DOI": "10.48550/arXiv.2206.05263",
                    "CorpusId": 249605400
                },
                "corpusId": 249605400,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/63d30a86ec3e590c2032aeec80a50493d9889c9a",
                "title": "Causal Balancing for Domain Generalization",
                "abstract": "While machine learning models rapidly advance the state-of-the-art on various real-world tasks, out-of-domain (OOD) generalization remains a challenging problem given the vulnerability of these models to spurious correlations. We propose a balanced mini-batch sampling strategy to transform a biased data distribution into a spurious-free balanced distribution, based on the invariance of the underlying causal mechanisms for the data generation process. We argue that the Bayes optimal classifiers trained on such balanced distribution are minimax optimal across a diverse enough environment space. We also provide an identifiability guarantee of the latent variable model of the proposed data generation process, when utilizing enough train environments. Experiments are conducted on DomainBed, demonstrating empirically that our method obtains the best performance across 20 baselines reported on the benchmark.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115553132",
                        "name": "Xinyi Wang"
                    },
                    {
                        "authorId": "48227633",
                        "name": "Michael Stephen Saxon"
                    },
                    {
                        "authorId": "2125031571",
                        "name": "Jiachen Li"
                    },
                    {
                        "authorId": "40975176",
                        "name": "Hongyang Zhang"
                    },
                    {
                        "authorId": "37844380",
                        "name": "Kun Zhang"
                    },
                    {
                        "authorId": "1682479",
                        "name": "William Yang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Learning from multi-source data has a long history in machine lerning (Cortes et al. 2021, Hoffman et al. 2018, Zhao et al. 2018, Blanchard et al. 2011,Muandet et al. 2013,Mahajan et al. 2021, Wang et al. 2021, Zhou et al. 2021, Zhang and Yang 2021, Sener and Koltun 2018).",
                "\u2026essential part in multi-task learning (Caruana 1997, Zhang and Yang 2021), domain generalization (DG) (Blanchard et al. 2011, Muandet et al. 2013, Mahajan et al. 2021, Wang et al. 2021, Zhou et al. 2021), and out-of-distribution (OOD) generalization (Arjovsky 2019, Wald et al. 2021) that\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7ddec8808c8a4fd2b42a97e87de64a71dbc63f93",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-02786",
                    "ArXiv": "2206.02786",
                    "DOI": "10.48550/arXiv.2206.02786",
                    "CorpusId": 249431922
                },
                "corpusId": 249431922,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7ddec8808c8a4fd2b42a97e87de64a71dbc63f93",
                "title": "Impossibility of Collective Intelligence",
                "abstract": "Democratization of AI involves training and deploying machine learning models across heterogeneous and potentially massive environments. Diversity of data opens up a number of possibilities to advance AI systems, but also introduces pressing concerns such as privacy, security, and equity that require special attention. This work shows that it is theoretically impossible to design a rational learning algorithm that has the ability to successfully learn across heterogeneous environments, which we decoratively call collective intelligence (CI). By representing learning algorithms as choice correspondences over a hypothesis space, we are able to axiomatize them with essential properties. Un-fortunately, the only feasible algorithm compatible with all of the axioms is the standard empirical risk minimization (ERM) which learns arbitrarily from a single environment. Our impossibility result reveals informational incomparability between environments as one of the foremost obstacles for researchers who design novel algorithms that learn from multiple environments, which sheds light on prerequisites for success in critical areas of machine learning such as out-of-distribution generalization, federated learning, algorithmic fairness, and multi-modal learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2276351",
                        "name": "Krikamol Muandet"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mahajan et al. (2021) show that class-conditional invariance can fail if P (zc|y) does not remain the same across domains, which also applies to our method.",
                "\u2026invariance of Ee[y|f(x)] (Arjovsky et al., 2019) with information bottleneck constraint (Ahuja et al. (2021)), imposing object-invariant condition (Mahajan et al. (2021)), using domain inference (Creager et al., 2021), model calibration (Wald et al., 2021), and others (Krueger et al., 2021; Li et\u2026"
            ],
            "intents": [
                "result",
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1fac67764becb99e2262c31e17466e1bfb372df3",
                "externalIds": {
                    "ArXiv": "2205.14546",
                    "DBLP": "journals/corr/abs-2205-14546",
                    "DOI": "10.48550/arXiv.2205.14546",
                    "CorpusId": 249191553
                },
                "corpusId": 249191553,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1fac67764becb99e2262c31e17466e1bfb372df3",
                "title": "The Missing Invariance Principle Found - the Reciprocal Twin of Invariant Risk Minimization",
                "abstract": "Machine learning models often generalize poorly to out-of-distribution (OOD) data as a result of relying on features that are spuriously correlated with the label during training. Recently, the technique of Invariant Risk Minimization (IRM) was proposed to learn predictors that only use invariant features by conserving the feature-conditioned label expectation $\\mathbb{E}_e[y|f(x)]$ across environments. However, more recent studies have demonstrated that IRM-v1, a practical version of IRM, can fail in various settings. Here, we identify a fundamental flaw of IRM formulation that causes the failure. We then introduce a complementary notion of invariance, MRI, based on conserving the label-conditioned feature expectation $\\mathbb{E}_e[f(x)|y]$, which is free of this flaw. Further, we introduce a simplified, practical version of the MRI formulation called MRI-v1. We prove that for general linear problems, MRI-v1 guarantees invariant predictors given sufficient number of environments. We also empirically demonstrate that MRI-v1 strongly out-performs IRM-v1 and consistently achieves near-optimal OOD generalization in image-based nonlinear problems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3222813",
                        "name": "Dongsung Huh"
                    },
                    {
                        "authorId": "153675019",
                        "name": "A. Baidya"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mahajan et al. (2021) argues learning invariant representations for inputs derived from the same object."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c40c0f9f453b95775ed67a29683a8a2675373716",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-11101",
                    "ArXiv": "2205.11101",
                    "DOI": "10.48550/arXiv.2205.11101",
                    "CorpusId": 248986662
                },
                "corpusId": 248986662,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c40c0f9f453b95775ed67a29683a8a2675373716",
                "title": "FL Games: A federated learning framework for distribution shifts",
                "abstract": "Federated learning aims to train predictive models for data that is distributed across clients, under the orchestration of a server. However, participating clients typically each hold data from a different distribution, whereby predictive models with strong in-distribution generalization can fail catastrophically on unseen domains. In this work, we argue that in order to generalize better across non-i.i.d. clients, it is imperative to only learn correlations that are stable and invariant across domains. We propose FL Games, a game-theoretic framework for federated learning for learning causal features that are invariant across clients. While training to achieve the Nash equilibrium, the traditional best response strategy suffers from high-frequency oscillations. We demonstrate that FL Games effectively resolves this challenge and exhibits smooth performance curves. Further, FL Games scales well in the number of clients, requires significantly fewer communication rounds, and is agnostic to device heterogeneity. Through empirical evaluation, we demonstrate that FL Games achieves high out-of-distribution performance on various benchmarks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1752754194",
                        "name": "Sharut Gupta"
                    },
                    {
                        "authorId": "3048927",
                        "name": "Kartik Ahuja"
                    },
                    {
                        "authorId": "2203743",
                        "name": "Mohammad Havaei"
                    },
                    {
                        "authorId": "3216579",
                        "name": "N. Chatterjee"
                    },
                    {
                        "authorId": "1865800402",
                        "name": "Y. Bengio"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To that end, Domain Generalization (DG) [52, 53, 30], which aims at learning a domain-agnostic model, has drawn increasing attention in the ReID community."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6fe7d90b07ffeda4a7c71db9eb1e37a72f780a7a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-11197",
                    "ArXiv": "2205.11197",
                    "DOI": "10.48550/arXiv.2205.11197",
                    "CorpusId": 248987147
                },
                "corpusId": 248987147,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6fe7d90b07ffeda4a7c71db9eb1e37a72f780a7a",
                "title": "Feature-Distribution Perturbation and Calibration for Generalized Person ReID",
                "abstract": "Person Re-identification (ReID) has been advanced remarkably over the last 10 years along with the rapid development of deep learning for visual recognition. However, the i.i.d. (independent and identically distributed) assumption commonly held in most deep learning models is somewhat non-applicable to ReID considering its objective to identify images of the same pedestrian across cameras at different locations often of variable and independent domain characteristics that are also subject to view-biased data distribution. In this work, we propose a Feature-Distribution Perturbation and Calibration (PECA) method to derive generic feature representations for person ReID, which is not only discriminative across cameras but also agnostic and deployable to arbitrary unseen target domains. Specifically, we perform per-domain feature-distribution perturbation to refrain the model from overfitting to the domain-biased distribution of each source (seen) domain by enforcing feature invariance to distribution shifts caused by perturbation. Furthermore, we design a global calibration mechanism to align feature distributions across all the source domains to improve the model generalization capacity by eliminating domain bias. These local perturbation and global calibration are conducted simultaneously, which share the same principle to avoid models overfitting by regularization respectively on the perturbed and the original distributions. Extensive experiments were conducted on eight person ReID datasets and the proposed PECA model outperformed the state-of-the-art competitors by significant margins.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108056391",
                        "name": "Qilei Li"
                    },
                    {
                        "authorId": "2109967617",
                        "name": "Jiabo Huang"
                    },
                    {
                        "authorId": "2166077628",
                        "name": "Jian Hu"
                    },
                    {
                        "authorId": "144784813",
                        "name": "S. Gong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The main idea is to identify and extract the transferable components that are invariant across different domains under certain causal models (Gong et al., 2016; Magliacane et al., 2017; Mahajan et al., 2021; Rojas-Carulla et al., 2018)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bb5b2597ce37c1294e8dd94d63fa27dd8134a628",
                "externalIds": {
                    "ArXiv": "2205.04641",
                    "DBLP": "journals/corr/abs-2205-04641",
                    "DOI": "10.48550/arXiv.2205.04641",
                    "CorpusId": 248665526
                },
                "corpusId": 248665526,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bb5b2597ce37c1294e8dd94d63fa27dd8134a628",
                "title": "On Causality in Domain Adaptation and Semi-Supervised Learning: an Information-Theoretic Analysis",
                "abstract": "The establishment of the link between causality and unsupervised domain adaptation (UDA)/semi-supervised learning (SSL) has led to methodological advances in these learning problems in recent years. However, a formal theory that explains the role of causality in the generalization performance of UDA/SSL is still lacking. In this paper, we consider the UDA/SSL setting where we access m labeled source data and n unlabeled target data as training instances under a parametric probabilistic model. We study the learning performance (e.g., excess risk) of prediction in the target domain. Speci\ufb01cally, we distinguish two scenarios: the learning problem is called causal learning if the feature is the cause and the label is the e\ufb00ect, and is called anti-causal learning otherwise. We show that in causal learning, the excess risk depends on the size of the source sample at a rate of O ( 1 m ) only if the labelling distribution between the source and target domains remains unchanged. In anti-causal learning, we show that the unlabeled data dominate the performance at a rate of typically O ( 1 n ). Our analysis is based on the notion of potential outcome random variables and information theory. These results bring out the relationship between the data sample size and the hardness of the learning problem with di\ufb00erent causal mechanisms. rate. We empirically depict the rate of the learning performance under di\ufb00erent causal mechanisms and domain shift conditions, from which the usefulness of the source and target data is manifested.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150344924",
                        "name": "Xuetong Wu"
                    },
                    {
                        "authorId": "29393235",
                        "name": "Mingming Gong"
                    },
                    {
                        "authorId": "1727993",
                        "name": "J. Manton"
                    },
                    {
                        "authorId": "1678366",
                        "name": "U. Aickelin"
                    },
                    {
                        "authorId": "3168848",
                        "name": "Jingge Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A few image generation works have modeled a causal connection between images and their labels, often assuming the labels are generating the images [24, 48], and some prior work studied the connection between causality and specific types of generalizations [4,37,38,58]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b400b066929e8070842b33b450fe69698c5ed826",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-12363",
                    "ArXiv": "2204.12363",
                    "DOI": "10.1109/CVPR52688.2022.00737",
                    "CorpusId": 248392254
                },
                "corpusId": 248392254,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b400b066929e8070842b33b450fe69698c5ed826",
                "title": "Causal Transportability for Visual Recognition",
                "abstract": "Visual representations underlie object recognition tasks, but they often contain both robust and non-robust features. Our main observation is that image classifiers may perform poorly on out-of-distribution samples because spurious correlations between non-robust features and labels can be changed in a new environment. By analyzing procedures for out-of-distribution generalization with a causal graph, we show that standard classifiers fail because the association between images and labels is not transportable across settings. However, we then show that the causal effect, which severs all sources of confounding, remains invariant across domains. This motivates us to develop an algorithm to estimate the causal effect for image classification, which is transportable (i.e., invariant) across source and target environments. Without observing additional variables, we show that we can derive an estimand for the causal effect under empirical assumptions using representations in deep models as proxies. Theoretical analysis, empirical results, and visualizations show that our approach captures causal invariances and improves overall generalization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7700460",
                        "name": "Chengzhi Mao"
                    },
                    {
                        "authorId": "2125225990",
                        "name": "Kevin Xia"
                    },
                    {
                        "authorId": "2163667482",
                        "name": "James Wang"
                    },
                    {
                        "authorId": "2359832",
                        "name": "Hongya Wang"
                    },
                    {
                        "authorId": "2110694456",
                        "name": "Junfeng Yang"
                    },
                    {
                        "authorId": "2778721",
                        "name": "E. Bareinboim"
                    },
                    {
                        "authorId": "1856025",
                        "name": "Carl Vondrick"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Existing works show that it is indeed possible to approach this computationally via sub-sampling methods [52] or by learning elaborate matching functions to identify image or object pairs across sites that are \u201csimilar\u201d [28] or have the same value for C.",
                "2a, similar to existing works [28,52] with minimal changes.",
                "Lastly, (v) RM [30]: Also used in [28], RandMatch (RM) learns invariant representations on samples across sites that \u201dmatch\u201d in terms of the class label (we match based on both Y and C values) .",
                "Specially, recent work [28] has shown the value of integrating structural causal models for domain generalization, which is related to dataset pooling."
            ],
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f7bbed29c06588153620ae9f56f1688d45513ba1",
                "externalIds": {
                    "DBLP": "conf/cvpr/LokhandeCRS22",
                    "ArXiv": "2203.15234",
                    "DOI": "10.1109/CVPR52688.2022.01018",
                    "CorpusId": 247779058,
                    "PubMed": "36268536"
                },
                "corpusId": 247779058,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f7bbed29c06588153620ae9f56f1688d45513ba1",
                "title": "Equivariance Allows Handling Multiple Nuisance Variables When Analyzing Pooled Neuroimaging Datasets",
                "abstract": "Pooling multiple neuroimaging datasets across institutions often enables improvements in statistical power when evaluating associations (e.g., between risk factors and disease outcomes) that may otherwise be too weak to detect. When there is only a single source of variability (e.g., different scanners), domain adaptation and matching the distributions of representations may suffice in many scenarios. But in the presence of more than one nuisance variable which concurrently influence the measurements, pooling datasets poses unique challenges, e.g., variations in the data can come from both the acquisition method as well as the demographics of participants (gender, age). Invariant representation learning, by itself, is illsuited to fully model the data generation process. In this paper, we show how bringing recent results on equivariant representation learning (for studying symmetries in neural networks) instantiated on structured spaces together with simple use of classical results on causal inference provides an effective practical solution. In particular, we demonstrate how our model allows dealing with more than one nuisance variable under some assumptions and can enable analysis of pooled scientific datasets in scenarios that would otherwise entail removing a large portion of the samples. Our code is available on https://github.com/vsingh-group/DatasetPooling.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40900716",
                        "name": "Vishnu Suresh Lokhande"
                    },
                    {
                        "authorId": "39072588",
                        "name": "Rudrasis Chakraborty"
                    },
                    {
                        "authorId": "3023295",
                        "name": "S. Ravi"
                    },
                    {
                        "authorId": "144711711",
                        "name": "Vikas Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "IRM and its variants (Krueger et al., 2021; Xie et al., 2020; Mahajan et al., 2021) posit the existence of a feature embedder such that the optimal classifier on top of these features is the same for every environment from which data can be drawn.",
                "then, there have been many works on improving the IRM objective (Xie et al., 2020; Chang et al., 2020; Ahuja et al., 2020; Krueger et al., 2021; Mahajan et al., 2021) and comparing ERM and IRM from theoretical (Ahuja et al.",
                "\u2026there have been many works on improving the IRM objective (Xie et al., 2020; Chang et al., 2020; Ahuja et al., 2020; Krueger et al., 2021; Mahajan et al., 2021) and comparing ERM and IRM from theoretical (Ahuja et al., 2021; Rosenfeld et al., 2021; 2022) and empirical perspectives\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "71854d12f72a2d62105b2e0f3e24fb6cada55f7c",
                "externalIds": {
                    "ArXiv": "2203.15566",
                    "DBLP": "journals/corr/abs-2203-15566",
                    "DOI": "10.48550/arXiv.2203.15566",
                    "CorpusId": 247778575
                },
                "corpusId": 247778575,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/71854d12f72a2d62105b2e0f3e24fb6cada55f7c",
                "title": "Core Risk Minimization using Salient ImageNet",
                "abstract": "Deep neural networks can be unreliable in the real world especially when they heavily use spurious features for their predictions. Recently, Singla&Feizi (2022) introduced the Salient Imagenet dataset by annotating and localizing core and spurious features of ~52k samples from 232 classes of Imagenet. While this dataset is useful for evaluating the reliance of pretrained models on spurious features, its small size limits its usefulness for training models. In this work, we first introduce the Salient Imagenet-1M dataset with more than 1 million soft masks localizing core and spurious features for all 1000 Imagenet classes. Using this dataset, we first evaluate the reliance of several Imagenet pretrained models (42 total) on spurious features and observe that: (i) transformers are more sensitive to spurious features compared to Convnets, (ii) zero-shot CLIP transformers are highly susceptible to spurious features. Next, we introduce a new learning paradigm called Core Risk Minimization (CoRM) whose objective ensures that the model predicts a class using its core features. We evaluate different computational approaches for solving CoRM and achieve significantly higher (+12%) core accuracy (accuracy when non-core regions corrupted using noise) with no drop in clean accuracy compared to models trained via Empirical Risk Minimization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144190575",
                        "name": "Sahil Singla"
                    },
                    {
                        "authorId": "104644443",
                        "name": "Mazda Moayeri"
                    },
                    {
                        "authorId": "34389431",
                        "name": "S. Feizi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Very recently, MatchDG [32] introduces causality into DG literature by enforcing the inputs across domains have the same representation via contrastive learning if they are derived from the same object.",
                "Specifically, compared with MatchDG [32], which also introduces causality into DG problem, CIRL outperforms MatchDG by a large margin of 1.",
                "It can be observed that MatchDG [32] and CIRL present higher representation importance since they embed the causal information that truly affects the classification into representations."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "77c1263d87616363e8ba4239ba61149e822a34a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-14237",
                    "ArXiv": "2203.14237",
                    "DOI": "10.1109/CVPR52688.2022.00788",
                    "CorpusId": 247762830
                },
                "corpusId": 247762830,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/77c1263d87616363e8ba4239ba61149e822a34a3",
                "title": "Causality Inspired Representation Learning for Domain Generalization",
                "abstract": "Domain generalization (DG) is essentially an out-of-distribution problem, aiming to generalize the knowledge learned from multiple source domains to an unseen target domain. The mainstream is to leverage statistical models to model the dependence between data and labels, intending to learn representations independent of domain. Nevertheless, the statistical models are superficial descriptions of reality since they are only required to model dependence instead of the intrinsic causal mechanism. When the dependence changes with the target distribution, the statistic models may fail to generalize. In this regard, we introduce a general structural causal model to formalize the DG problem. Specifically, we assume that each input is constructed from a mix of causal factors (whose relationship with the label is invariant across domains) and non-causal factors (category-independent), and only the former cause the classification judgments. Our goal is to extract the causal factors from inputs and then reconstruct the invariant causal mechanisms. However, the theoretical idea is far from practical of DG since the required causal/non-causal factors are unobserved. We highlight that ideal causal factors should meet three basic properties: separated from the non-causal ones, jointly independent, and causally sufficient for the classification. Based on that, we propose a Causality Inspired Representation Learning (CIRL) algorithm that enforces the representations to satisfy the above properties and then uses them to simulate the causal factors, which yields improved generalization ability. Extensive experimental results on several widely used datasets verify the effectiveness of our approach. 11Code is available at \u201chttps://github.com/BIT-DA/CIRL\u201d.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2124763047",
                        "name": "Fangrui Lv"
                    },
                    {
                        "authorId": null,
                        "name": "Jian Liang"
                    },
                    {
                        "authorId": "1796229955",
                        "name": "Shuang Li"
                    },
                    {
                        "authorId": "2091438108",
                        "name": "Bin Zang"
                    },
                    {
                        "authorId": "2144632579",
                        "name": "Chi Harold Liu"
                    },
                    {
                        "authorId": "2117414666",
                        "name": "Ziteng Wang"
                    },
                    {
                        "authorId": "2146082494",
                        "name": "Di Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To capture the invariant causal mechanism, existing works have assumed a particular form of the causal diagram [25,46,48,57,67], which may be restrictive in practice and is untestable from the observed data.",
                "To utilize the invariant causal mechanism and hence improve OOD generalization, some works impose restrictive assumptions on the causal diagram or structural equations [25, 46, 57, 67]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "29e76575b2fb237f9f73703a7cabee19cfde0451",
                "externalIds": {
                    "DBLP": "conf/cvpr/0016YC022",
                    "ArXiv": "2203.11528",
                    "DOI": "10.1109/CVPR52688.2022.00047",
                    "CorpusId": 247596865
                },
                "corpusId": 247596865,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/29e76575b2fb237f9f73703a7cabee19cfde0451",
                "title": "Out-of-distribution Generalization with Causal Invariant Transformations",
                "abstract": "In real-world applications, it is important and desirable to learn a model that performs well on out-of-distribution (OOD) data. Recently, causality has become a powerful tool to tackle the OOD generalization problem, with the idea resting on the causal mechanism that is invariant across domains of interest. To leverage the generally unknown causal mechanism, existing works assume a linear form of causal feature or require sufficiently many and diverse training domains, which are usually restrictive in practice. In this work, we obviate these assumptions and tackle the OOD problem without explicitly recovering the causal feature. Our approach is based on transformations that modify the non-causal feature but leave the causal part unchanged, which can be either obtained from prior knowledge or learned from the training data in the multi-domain scenario. Under the setting of invariant causal mechanism, we theoretically show that if all such transformations are available, then we can learn a minimax optimal model across the domains using only single domain data. Noticing that knowing a complete set of these causal invariant transformations may be impractical, we further show that it suffices to know only a subset of these transformations. Based on the theoretical findings, a regularized training procedure is proposed to improve the OOD generalization capability. Extensive experimental results on both synthetic and real datasets verify the effectiveness of the proposed algorithm, even with only a few causal invariant transformations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108694625",
                        "name": "Ruoyu Wang"
                    },
                    {
                        "authorId": "3541183",
                        "name": "Mingyang Yi"
                    },
                    {
                        "authorId": "2827164",
                        "name": "Zhitang Chen"
                    },
                    {
                        "authorId": "145239435",
                        "name": "Shengyu Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Self-supervision as an emerging technique has been employed to train neural networks for more generalizable predictions on the image field [110, 111, 112]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2a3349c9f48b322400cd1d2d720fc42a42d19d5f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-07987",
                    "ArXiv": "2202.07987",
                    "CorpusId": 246867220
                },
                "corpusId": 246867220,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2a3349c9f48b322400cd1d2d720fc42a42d19d5f",
                "title": "Out-Of-Distribution Generalization on Graphs: A Survey",
                "abstract": "\u2014Graph machine learning has been extensively studied in both academia and industry. Although booming with a vast number of emerging methods and techniques, most of the literature is built on the in-distribution hypothesis, i.e., testing and training graph data are identically distributed. However, this in-distribution hypothesis can hardly be satis\ufb01ed in many real-world graph scenarios where the model performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this critical problem, out-of-distribution (OOD) generalization on graphs, which goes beyond the in-distribution hypothesis, has made great progress and attracted ever-increasing attention from the research community. In this paper, we comprehensively survey OOD generalization on graphs and present a detailed review of recent advances in this area. First, we provide a formal problem de\ufb01nition of OOD generalization on graphs. Second, we categorize existing methods into three classes from conceptually different perspectives, i.e., data, model, and learning strategy, based on their positions in the graph machine learning pipeline, followed by detailed discussions for each category. We also review the theories related to OOD generalization on graphs and introduce the commonly used graph datasets for thorough evaluations. Finally, we share our insights on future research directions. This paper is the \ufb01rst systematic and comprehensive review of OOD generalization on graphs, to the best of our knowledge.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144911687",
                        "name": "Haoyang Li"
                    },
                    {
                        "authorId": "153316152",
                        "name": "Xin Wang"
                    },
                    {
                        "authorId": "2116460208",
                        "name": "Ziwei Zhang"
                    },
                    {
                        "authorId": "145583986",
                        "name": "Wenwu Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although using contrastive learning to improve OOD generalization is not new in the literature [27, 61, 124], previous methods cannot yield OOD guarantees in graph circumstances due to the highly non-linearity and the unavailability of domain labels E.",
                "Moreover, most existing methods require environment labels that are however expensive to obtain in graphs, which limits their applications to graphs [4, 49, 2, 81, 31, 93, 27, 61].",
                "IRM [4] Yes R Yes PIIF IB-IRM [2] Yes R Yes PIIF&FIIF EIIL [23] Yes R No PIIF DANN [31] N/A R Yes N/A MatchDG [61] N/A R Yes FIIF GroupDro [81] N/A R Yes N/A CNC [124] N/A R No N/A GIB [120] Yes G No FIIF DIR [104] No G No FIIF CIGA (Ours) Yes G No PIIF&FIIF",
                "On Euclidean data, Invariant Learning [4, 23, 2], Group Distributionally Robust Optimization [49, 81, 124], Domain Adaption and Domain Generalization [31, 93, 52, 27, 61, 100] are three widely adopted approaches to enable OOD generalization.",
                "Besides, traditional approaches to tackle OOD generalization also include Domain Adaption, Transfer Learning and Domain Generalization[79, 21, 31, 93, 52, 27, 61, 100], which aim to learn the class conditional invariant representation shared across source domain and target domain.",
                "Algorithm OODGuarantee Regime E Known SCMSupport IRM[4] Yes Yes PIIF IB-IRM[2] Yes Yes PIIF&FIIF EIIL[23] Yes No PIIF DANN[31] N/A Yes N/A MatchDG[61] N/A Yes FIIF GroupDro[81] N/A Yes N/A CNC[124] N/A No N/A GIB[120] Yes G No FIIF DIR[104] No Figure 1: (a) Illustration of C ausality Inspired I nvariant G raph Le A rning (CIGA): GNNs need to classify graphs based on the specific motif (\u201cHouse\u201d or \u201cCycle\u201d)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "aa39c5a3080de756ad00648548e8f4faa2cfbf54",
                "externalIds": {
                    "DBLP": "conf/nips/0002ZB00XL0C22",
                    "ArXiv": "2202.05441",
                    "CorpusId": 252815553
                },
                "corpusId": 252815553,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/aa39c5a3080de756ad00648548e8f4faa2cfbf54",
                "title": "Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs",
                "abstract": "Despite recent success in using the invariance principle for out-of-distribution (OOD) generalization on Euclidean data (e.g., images), studies on graph data are still limited. Different from images, the complex nature of graphs poses unique challenges to adopting the invariance principle. In particular, distribution shifts on graphs can appear in a variety of forms such as attributes and structures, making it difficult to identify the invariance. Moreover, domain or environment partitions, which are often required by OOD methods on Euclidean data, could be highly expensive to obtain for graphs. To bridge this gap, we propose a new framework, called Causality Inspired Invariant Graph LeArning (CIGA), to capture the invariance of graphs for guaranteed OOD generalization under various distribution shifts. Specifically, we characterize potential distribution shifts on graphs with causal models, concluding that OOD generalization on graphs is achievable when models focus only on subgraphs containing the most information about the causes of labels. Accordingly, we propose an information-theoretic objective to extract the desired subgraphs that maximally preserve the invariant intra-class information. Learning with these subgraphs is immune to distribution shifts. Extensive experiments on 16 synthetic or real-world datasets, including a challenging setting -- DrugOOD, from AI-aided drug discovery, validate the superior OOD performance of CIGA.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108962670",
                        "name": "Yongqiang Chen"
                    },
                    {
                        "authorId": "2109116068",
                        "name": "Yonggang Zhang"
                    },
                    {
                        "authorId": "2419616",
                        "name": "Yatao Bian"
                    },
                    {
                        "authorId": "50841357",
                        "name": "Han Yang"
                    },
                    {
                        "authorId": "47737190",
                        "name": "Kaili Ma"
                    },
                    {
                        "authorId": "2051756680",
                        "name": "Binghui Xie"
                    },
                    {
                        "authorId": "121698214",
                        "name": "Tongliang Liu"
                    },
                    {
                        "authorId": "2153287285",
                        "name": "Bo Han"
                    },
                    {
                        "authorId": "2116502293",
                        "name": "James Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For instance, Ahmed et al. (2021) proposes to match the output distribution spaces from different domains via some divergence, while a recent work (Mahajan et al., 2021) also leverages a matching-based algorithm that resorts to shared representations of cross-domain inputs from the same object.",
                "(2021) proposes to match the output distribution spaces from different domains via some divergence, while a recent work (Mahajan et al., 2021) also leverages a matching-based algorithm that resorts to shared representations of cross-domain inputs from the same object."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4818381d399636a4caefc24710b5c26a1e6e906c",
                "externalIds": {
                    "DBLP": "conf/iclr/WuZYW22",
                    "ArXiv": "2202.02466",
                    "CorpusId": 247292293
                },
                "corpusId": 247292293,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4818381d399636a4caefc24710b5c26a1e6e906c",
                "title": "Handling Distribution Shifts on Graphs: An Invariance Perspective",
                "abstract": "There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given two-fold fundamental challenges: 1) the inter-connection among nodes in one graph, which induces non-IID generation of data points even under the same environment, and 2) the structural information in the input graph, which is also informative for prediction. In this paper, we formulate the OOD problem on graphs and develop a new invariant learning approach, Explore-to-Extrapolate Risk Minimization (EERM), that facilitates graph neural networks to leverage invariance principles for prediction. EERM resorts to multiple context explorers (specified as graph structure editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node-level prediction. We prove the validity of our method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51171144",
                        "name": "Qitian Wu"
                    },
                    {
                        "authorId": "35466544",
                        "name": "Hengrui Zhang"
                    },
                    {
                        "authorId": "3063894",
                        "name": "Junchi Yan"
                    },
                    {
                        "authorId": "2242717",
                        "name": "D. Wipf"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Notably, domain-invariant representation learning has emerged as one of the most common and efficient approaches in Domain Generalization and provided many promising results [1] [2] [7] [9] [10] [21] [22] [23] [24].",
                "This assumption can be verified upon the setting of causal graphical models that illustrate the relationship between the data and its label [22] [32] [33] [34]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "73147cd7a3b12b94c8685f19faa41f1a1c42ad09",
                "externalIds": {
                    "ArXiv": "2201.10460",
                    "DBLP": "conf/icpr/NguyenLISA22",
                    "DOI": "10.1109/ICPR56361.2022.9956548",
                    "CorpusId": 246275695
                },
                "corpusId": 246275695,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/73147cd7a3b12b94c8685f19faa41f1a1c42ad09",
                "title": "Conditional entropy minimization principle for learning domain invariant representation features",
                "abstract": "Invariance-principle-based methods such as Invariant Risk Minimization (IRM), have recently emerged as promising approaches for Domain Generalization (DG). Despite promising theory, such approaches fail in common classification tasks due to mixing of true invariant features and spurious invariant features1. To address this, we propose a framework based on the conditional entropy minimization (CEM) principle to filter-out the spurious invariant features leading to a new algorithm with a better generalization capability. We show that our proposed approach is closely related to the well-known Information Bottleneck (IB) framework and prove that under certain assumptions, entropy minimization can exactly recover the true invariant features. Our approach provides competitive classification accuracy compared to recent theoretically-principled state-of-the-art alternatives across several DG datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "123233017",
                        "name": "Thuan Q. Nguyen"
                    },
                    {
                        "authorId": "51454393",
                        "name": "Boyang Lyu"
                    },
                    {
                        "authorId": "1756038",
                        "name": "P. Ishwar"
                    },
                    {
                        "authorId": "121848090",
                        "name": "matthias. scheutz"
                    },
                    {
                        "authorId": "2139553851",
                        "name": "Shuchin Aeron"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "772c25f32a61015f2a973d8169f8591af3f57062",
                "externalIds": {
                    "DBLP": "conf/mm/WangDCLM22",
                    "ArXiv": "2201.08029",
                    "DOI": "10.1145/3503161.3548267",
                    "CorpusId": 251040558
                },
                "corpusId": 251040558,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/772c25f32a61015f2a973d8169f8591af3f57062",
                "title": "Domain Generalization via Frequency-domain-based Feature Disentanglement and Interaction",
                "abstract": "Adaptation to out-of-distribution data is a meta-challenge for all statistical learning algorithms that strongly rely on the i.i.d. assumption. It leads to unavoidable labor costs and confidence crises in realistic applications. For that, domain generalization aims at mining domain-irrelevant knowledge from multiple source domains that can generalize to unseen target domains. In this paper, by leveraging the frequency domain of an image, we uniquely work with two key observations: (i) the high-frequency information of an image depicts object edge structure, which preserves high-level semantic information of the object is naturally consistent across different domains, and (ii) the low-frequency component retains object smooth structure, while this information is susceptible to domain shifts. Motivated by the above observations, we introduce (i) an encoder-decoder structure to disentangle high- and low-frequency features of an image, (ii) an information interaction mechanism to ensure the helpful knowledge from both two parts can cooperate effectively, and (iii) a novel data augmentation technique that works on the frequency domain to encourage the robustness of frequency-wise feature disentangling. The proposed method obtains state-of-the-art performance on three widely used domain generalization benchmarks (Digit-DG, Office-Home, and PACS).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115891607",
                        "name": "Jing-Yi Wang"
                    },
                    {
                        "authorId": "1557389697",
                        "name": "Ruoyi Du"
                    },
                    {
                        "authorId": "67146777",
                        "name": "Dongliang Chang"
                    },
                    {
                        "authorId": "2582309",
                        "name": "K. Liang"
                    },
                    {
                        "authorId": "1755773",
                        "name": "Zhanyu Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The first class, domain generalization methods [2, 23, 26], aim at learning representations invariant to a predefined set of extraneous attributes or groups."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4082d87e304173af03eb0d6e31a582bb7016dd3e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-03668",
                    "ArXiv": "2201.03668",
                    "CorpusId": 245853826
                },
                "corpusId": 245853826,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4082d87e304173af03eb0d6e31a582bb7016dd3e",
                "title": "Towards Group Robustness in the presence of Partial Group Labels",
                "abstract": "Learning invariant representations is an important requirement when training machine learning models that are driven by spurious correlations in the datasets. These spurious correlations, between input samples and the target labels, wrongly direct the neural network predictions resulting in poor performance on certain groups, especially the minority groups. Robust training against these spurious correlations requires the knowledge of group membership for every sample. Such a requirement is impractical in situations where the data labeling efforts for minority or rare groups are significantly laborious or where the individuals comprising the dataset choose to conceal sensitive information. On the other hand, the presence of such data collection efforts results in datasets that contain partially labeled group information. Recent works have tackled the fully unsupervised scenario where no labels for groups are available. Thus, we aim to fill the missing gap in the literature by tackling a more realistic setting that can leverage partially available sensitive or group information during training. First, we construct a constraint set and derive a high probability bound for the group assignment to belong to the set. Second, we propose an algorithm that optimizes for the worst-off group assignments from the constraint set. Through experiments on image and tabular datasets, we show improvements in the minority group's performance while preserving overall aggregate accuracy across groups.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40900716",
                        "name": "Vishnu Suresh Lokhande"
                    },
                    {
                        "authorId": "1729571",
                        "name": "Kihyuk Sohn"
                    },
                    {
                        "authorId": "2144029",
                        "name": "Jinsung Yoon"
                    },
                    {
                        "authorId": "2384128",
                        "name": "Madeleine Udell"
                    },
                    {
                        "authorId": "50521003",
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "authorId": "1945962",
                        "name": "Tomas Pfister"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a673e335874a08a3d7a9846ecaca7e61ecdff049",
                "externalIds": {
                    "DBLP": "journals/tnn/LiWZZW23",
                    "DOI": "10.1109/TNNLS.2021.3135036",
                    "CorpusId": 245538630,
                    "PubMed": "34962885"
                },
                "corpusId": 245538630,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a673e335874a08a3d7a9846ecaca7e61ecdff049",
                "title": "Causal Disentanglement: A Generalized Bearing Fault Diagnostic Framework in Continuous Degradation Mode",
                "abstract": "In recent years, the identification of out-of-distribution faults has become a hot topic in the field of intelligent diagnosis. Existing researches usually adopt domain adaptation methods to complete the generalization of diagnostic knowledge with the aid of target domain data, but the acquisition of fault samples in real industries is extremely time-consuming and costly. Moreover, most researches focus on samples with fixed fault levels, ignoring the fact that system degradation is a continuous process. In response to the above intractable problems, this article proposed a causal disentanglement network (CDN) to realize cross-machine knowledge generalization and continuous degradation mode diagnosis. In CDN, multitask instance normalization and batch normalization structure was proposed to learn task-specific knowledge and enhance the informativeness of the extracted features. On this basis, a causal disentanglement loss was proposed, which minimized the mutual information of features between subtask structures and captured the causal invariant fault information for better generalization. The experimental results proved the superiority and generalization ability of CDN, and the visualization results proved the performance of CDN in causality mining.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2048334187",
                        "name": "Jiejue Li"
                    },
                    {
                        "authorId": "46395365",
                        "name": "Yu Wang"
                    },
                    {
                        "authorId": "1952345",
                        "name": "Y. Zi"
                    },
                    {
                        "authorId": "2187723675",
                        "name": "Haijun Zhang"
                    },
                    {
                        "authorId": "2056791731",
                        "name": "Zhiguo Wan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "71c4c288c8cd003ee3d4e35f79f6660f4648955e",
                "externalIds": {
                    "DBLP": "conf/eccv/Zhang0SG22",
                    "ArXiv": "2112.12329",
                    "DOI": "10.1007/978-3-031-19812-0_10",
                    "CorpusId": 251402294
                },
                "corpusId": 251402294,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/71c4c288c8cd003ee3d4e35f79f6660f4648955e",
                "title": "MVDG: A Unified Multi-view Framework for Domain Generalization",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2151812616",
                        "name": "Jian Zhang"
                    },
                    {
                        "authorId": "1785352346",
                        "name": "Lei Qi"
                    },
                    {
                        "authorId": "2475959",
                        "name": "Yinghuan Shi"
                    },
                    {
                        "authorId": null,
                        "name": "Yang Gao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While the IRM framework assumes only the invariance of the conditional expectation of the label given the representation, some follow-up works rely on stronger invariance assumptions [23, 31]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "75f72ab0684fa012510e2c3cb2c381ccdc6db17b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-09346",
                    "ArXiv": "2112.09346",
                    "CorpusId": 245329645
                },
                "corpusId": 245329645,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/75f72ab0684fa012510e2c3cb2c381ccdc6db17b",
                "title": "Balancing Fairness and Robustness via Partial Invariance",
                "abstract": "The Invariant Risk Minimization (IRM) framework aims to learn invariant features from a set of environments for solving the out-of-distribution (OOD) generalization problem. The underlying assumption is that the causal components of the data generating distributions remain constant across the environments or alternately, the data\"overlaps\"across environments to find meaningful invariant features. Consequently, when the\"overlap\"assumption does not hold, the set of truly invariant features may not be sufficient for optimal prediction performance. Such cases arise naturally in networked settings and hierarchical data-generating models, wherein the IRM performance becomes suboptimal. To mitigate this failure case, we argue for a partial invariance framework. The key idea is to introduce flexibility into the IRM framework by partitioning the environments based on hierarchical differences, while enforcing invariance locally within the partitions. We motivate this framework in classification settings with causal distribution shifts across environments. Our results show the capability of the partial invariant risk minimization to alleviate the trade-off between fairness and risk in certain settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1491449524",
                        "name": "Moulik Choraria"
                    },
                    {
                        "authorId": "51065644",
                        "name": "Ibtihal Ferwana"
                    },
                    {
                        "authorId": "40429226",
                        "name": "Ankur Mani"
                    },
                    {
                        "authorId": "1697944",
                        "name": "L. Varshney"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In general, causal learning has inspired MDG approaches such as IRM Arjovsky et al. (2019), MatchDG Mahajan et al. (2021) and Deep CAMA Zhang et al. (2020a).",
                "Torralba and Efros (2011). Such drops in performance indicate poor generalization capabilities of the models."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7b5f1542e88024894e8d77905a41ab8646104743",
                "externalIds": {
                    "ArXiv": "2112.09802",
                    "CorpusId": 253265392
                },
                "corpusId": 253265392,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7b5f1542e88024894e8d77905a41ab8646104743",
                "title": "Automated Domain Discovery from Multiple Sources to Improve Zero-Shot Generalization",
                "abstract": "Domain generalization (DG) methods aim to develop models that generalize to settings where the test distribution is different from the training data. In this paper, we focus on the challenging problem of multi-source zero shot DG (MDG), where labeled training data from multiple source domains is available but with no access to data from the target domain. A wide range of solutions have been proposed for this problem, including the state-of-the-art multi-domain ensembling approaches. Despite these advances, the na\\\"ive ERM solution of pooling all source data together and training a single classifier is surprisingly effective on standard benchmarks. In this paper, we hypothesize that, it is important to elucidate the link between pre-specified domain labels and MDG performance, in order to explain this behavior. More specifically, we consider two popular classes of MDG algorithms -- distributional robust optimization (DRO) and multi-domain ensembles, in order to demonstrate how inferring custom domain groups can lead to consistent improvements over the original domain labels that come with the dataset. To this end, we propose (i) Group-DRO++, which incorporates an explicit clustering step to identify custom domains in an existing DRO technique; and (ii) DReaME, which produces effective multi-domain ensembles through implicit domain re-labeling with a novel meta-optimization algorithm. Using empirical studies on multiple standard benchmarks, we show that our variants consistently outperform ERM by significant margins (1.5% - 9%), and produce state-of-the-art MDG performance. Our code can be found at https://github.com/kowshikthopalli/DREAME",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2123725394",
                        "name": "Kowshik Thopalli"
                    },
                    {
                        "authorId": "32777195",
                        "name": "Sameeksha Katoch"
                    },
                    {
                        "authorId": "143655174",
                        "name": "P. Turaga"
                    },
                    {
                        "authorId": "2064767378",
                        "name": "J. Thiagarajan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Generalization via assuming a causal structure has also been explored in [9, 28]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8717327b209b8fa9aec336c8f689cf64ef983f1b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-04766",
                    "ArXiv": "2112.04766",
                    "CorpusId": 245006085
                },
                "corpusId": 245006085,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8717327b209b8fa9aec336c8f689cf64ef983f1b",
                "title": "Adaptive Methods for Aggregated Domain Generalization",
                "abstract": "Domain generalization involves learning a classifier from a heterogeneous collection of training sources such that it generalizes to data drawn from similar unknown target domains, with applications in large-scale learning and personalized inference. In many settings, privacy concerns prohibit obtaining domain labels for the training data samples, and instead only have an aggregated collection of training points. Existing approaches that utilize domain labels to create domain-invariant feature representations are inapplicable in this setting, requiring alternative approaches to learn generalizable classifiers. In this paper, we propose a domain-adaptive approach to this problem, which operates in two steps: (a) we cluster training data within a carefully chosen feature space to create pseudo-domains, and (b) using these pseudo-domains we learn a domain-adaptive classifier that makes predictions using information about both the input and the pseudo-domain it belongs to. Our approach achieves state-of-the-art performance on a variety of domain generalization benchmarks without using domain labels whatsoever. Furthermore, we provide novel theoretical guarantees on domain generalization using cluster information. Our approach is amenable to ensemble-based methods and provides substantial gains even on large-scale benchmark datasets. The code can be found at: https://github.com/xavierohan/AdaClust_DomainBed",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40661962",
                        "name": "Xavier Thomas"
                    },
                    {
                        "authorId": "144542135",
                        "name": "D. Mahajan"
                    },
                    {
                        "authorId": "144994682",
                        "name": "A. Pentland"
                    },
                    {
                        "authorId": "2479521",
                        "name": "Abhimanyu Dubey"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "12d20b0a56393273856a750b62cef0a184179ba9",
                "externalIds": {
                    "ArXiv": "2112.03676",
                    "DOI": "10.1145/3624015",
                    "CorpusId": 244920905
                },
                "corpusId": 244920905,
                "publicationVenue": {
                    "id": "bb2eb372-4df2-4181-9988-71aecb1dcc5e",
                    "name": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Multimedia Comput Commun Appl (TOMCCAP",
                        "ACM Transactions on Multimedia Computing, Communications, and Applications",
                        "ACM Trans Multimedia Comput Commun Appl"
                    ],
                    "issn": "1551-6857",
                    "url": "http://www.acm.org/tomccap/",
                    "alternate_urls": [
                        "http://tomccap.acm.org/",
                        "http://tomm.acm.org/",
                        "http://portal.acm.org/tomccap/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/12d20b0a56393273856a750b62cef0a184179ba9",
                "title": "PLACE dropout: A Progressive Layer-wise and Channel-wise Dropout for Domain Generalization",
                "abstract": "Domain generalization (DG) aims to learn a generic model from multiple observed source domains that generalizes well to arbitrary unseen target domains without further training. The major challenge in DG is that the model inevitably faces a severe overfitting issue due to the domain gap between source and target domains. To mitigate this problem, some dropout-based methods have been proposed to resist overfitting by discarding part of the representation of the intermediate layers. However, we observe that most of these methods only conduct the dropout operation in some specific layers, leading to an insufficient regularization effect on the model. We argue that applying dropout at multiple layers can produce stronger regularization effects, which could alleviate the overfitting problem on source domains more adequately than previous layer-specific dropout methods. In this paper, we develop a novel layer-wise and channel-wise dropout for DG, which randomly selects one layer and then randomly selects its channels to conduct dropout. Particularly, the proposed method can generate a variety of data variants to better deal with the overfitting issue. We also provide theoretical analysis for our dropout method and prove that it can effectively reduce the generalization error bound. Besides, we leverage the progressive scheme to increase the dropout ratio with the training progress, which can gradually boost the difficulty of training the model to enhance its robustness. Extensive experiments on three standard benchmark datasets have demonstrated that our method outperforms several state-of-the-art DG methods. Our code is available at https://github.com/lingeringlight/PLACEdropout.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50115584",
                        "name": "Jintao Guo"
                    },
                    {
                        "authorId": "1785352346",
                        "name": "Lei Qi"
                    },
                    {
                        "authorId": "2475959",
                        "name": "Yinghuan Shi"
                    },
                    {
                        "authorId": "145644819",
                        "name": "Yang Gao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[19] Divyat Mahajan, Shruti Tople, and Amit Sharma.",
                "Recently, several works [18, 19, 20] utilize causal structures to learn invariance factors, which have shown better model generalizability on unseen data, especially on data from different distributions than the train distribution.",
                "Inspired from Theorem 2 in [19], we are able to derive the following theorem under our problem setting.",
                "Many researchers have been working on developing DG algorithms [12, 13, 14, 15, 16, 17, 18, 19, 20, 21], but most of these models are designed for tasks such as object recognition, semantic segmentation, image classification, and disease detection.",
                "A commonly-used strategy is to extract task-specific but domaininvariant features [12, 13, 17, 18, 19, 20].",
                "Inspired from [19], for this unsupervised contrastive learning process, we initialize \u03a9 with a random match based on classes and keep updating \u03a9 by minimizing the contrastive loss (3) until convergence."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6e93f9e8a1817d4c26a32ffc5ea8ce79e273d157",
                "externalIds": {
                    "DBLP": "conf/icra/HuJTZ22",
                    "ArXiv": "2112.02093",
                    "DOI": "10.1109/icra46639.2022.9812188",
                    "CorpusId": 237361587
                },
                "corpusId": 237361587,
                "publicationVenue": {
                    "id": "3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                    "name": "IEEE International Conference on Robotics and Automation",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Robotics and Automation",
                        "Int Conf Robot Autom",
                        "ICRA",
                        "IEEE Int Conf Robot Autom"
                    ],
                    "issn": "2152-4092",
                    "alternate_issns": [
                        "2379-9544"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639",
                    "alternate_urls": [
                        "http://www.ncsu.edu/IEEE-RAS/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6e93f9e8a1817d4c26a32ffc5ea8ce79e273d157",
                "title": "Causal-based Time Series Domain Generalization for Vehicle Intention Prediction",
                "abstract": "Accurately predicting the possible behaviors of traffic participants is an essential capability for autonomous vehicles. Since autonomous vehicles need to navigate in dynamically changing environments, they are expected to make accurate predictions regardless of where they are and what driving circumstances they encountered. Therefore, generalization capability to unseen domains is crucial for prediction models when autonomous vehicles are deployed in the real world. In this paper, we aim to address the domain generalization problem for vehicle intention prediction tasks and a causal-based time series domain generalization (CTSDG) model is proposed. We construct a structural causal model for vehicle intention prediction tasks to learn an invariant representation of input driving data for domain generalization. We further integrate a recurrent latent variable model into our structural causal model to better capture temporal latent dependencies from time-series input data. The effectiveness of our approach is evaluated via real-world driving data. We demonstrate that our proposed method has consistent improvement on prediction accuracy compared to other state-of-the-art domain generalization and behavior prediction methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46972349",
                        "name": "Yeping Hu"
                    },
                    {
                        "authorId": "2026993876",
                        "name": "Xiaogang Jia"
                    },
                    {
                        "authorId": "1680165",
                        "name": "M. Tomizuka"
                    },
                    {
                        "authorId": "144267500",
                        "name": "W. Zhan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The first category intends to learn domain-invariant features that follow the same distributions [17,24,25,28,30,31,47]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9e4193fc6a07af5488abaf88987e764e0b8f29af",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-13420",
                    "ArXiv": "2111.13420",
                    "CorpusId": 244709204
                },
                "corpusId": 244709204,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9e4193fc6a07af5488abaf88987e764e0b8f29af",
                "title": "Confounder Identification-free Causal Visual Feature Learning",
                "abstract": "Confounders in deep learning are in general detrimental to model's generalization where they infiltrate feature representations. Therefore, learning causal features that are free of interference from confounders is important. Most previous causal learning based approaches employ back-door criterion to mitigate the adverse effect of certain specific confounder, which require the explicit identification of confounder. However, in real scenarios, confounders are typically diverse and difficult to be identified. In this paper, we propose a novel Confounder Identification-free Causal Visual Feature Learning (CICF) method, which obviates the need for identifying confounders. CICF models the interventions among different samples based on front-door criterion, and then approximates the global-scope intervening effect upon the instance-level interventions from the perspective of optimization. In this way, we aim to find a reliable optimization direction, which avoids the intervening effects of confounders, to learn causal features. Furthermore, we uncover the relation between CICF and the popular meta-learning strategy MAML, and provide an interpretation of why MAML works from the theoretical perspective of causal learning for the first time. Thanks to the effective learning of causal features, our CICF enables models to have superior generalization capability. Extensive experiments on domain generalization benchmark datasets demonstrate the effectiveness of our CICF, which achieves the state-of-the-art performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2153897589",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "1486397342",
                        "name": "Zhizheng Zhang"
                    },
                    {
                        "authorId": "2087658756",
                        "name": "Guoqiang Wei"
                    },
                    {
                        "authorId": "40093162",
                        "name": "Cuiling Lan"
                    },
                    {
                        "authorId": "1634494276",
                        "name": "Wenjun Zeng"
                    },
                    {
                        "authorId": "1485402830",
                        "name": "Xin Jin"
                    },
                    {
                        "authorId": "31482866",
                        "name": "Zhibo Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[53] improve domain robustness using contrastive losses.",
                "Causal ideas have been used for discovering image features that are semantically essential and robust [52], [53], [54], [55]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "58d5cdd43d2dfb6f74ef82ca4eaa63c49ce6c078",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-12525",
                    "ArXiv": "2111.12525",
                    "DOI": "10.1109/TMI.2022.3224067",
                    "CorpusId": 244527518,
                    "PubMed": "36417741"
                },
                "corpusId": 244527518,
                "publicationVenue": {
                    "id": "e0cda45d-3074-4ac0-80b8-e5250df00b89",
                    "name": "IEEE Transactions on Medical Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Med Imaging"
                    ],
                    "issn": "0278-0062",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=42",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/58d5cdd43d2dfb6f74ef82ca4eaa63c49ce6c078",
                "title": "Causality-Inspired Single-Source Domain Generalization for Medical Image Segmentation",
                "abstract": "Deep learning models usually suffer from the domain shift issue, where models trained on one source domain do not generalize well to other unseen domains. In this work, we investigate the single-source domain generalization problem: training a deep network that is robust to unseen domains, under the condition that training data are only available from one source domain, which is common in medical imaging applications. We tackle this problem in the context of cross-domain medical image segmentation. In this scenario, domain shifts are mainly caused by different acquisition processes. We propose a simple causality-inspired data augmentation approach to expose a segmentation model to synthesized domain-shifted training examples. Specifically, 1) to make the deep model robust to discrepancies in image intensities and textures, we employ a family of randomly-weighted shallow networks. They augment training images using diverse appearance transformations. 2) Further we show that spurious correlations among objects in an image are detrimental to domain robustness. These correlations might be taken by the network as domain-specific clues for making predictions, and they may break on unseen domains. We remove these spurious correlations via causal intervention. This is achieved by resampling the appearances of potentially correlated objects independently. The proposed approach is validated on three cross-domain segmentation scenarios: cross-modality (CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI segmentation, and cross-site prostate MRI segmentation. The proposed approach yields consistent performance gains compared with competitive methods when tested on unseen domains.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38373850",
                        "name": "C. Ouyang"
                    },
                    {
                        "authorId": "2127378914",
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "2144066682",
                        "name": "Surui Li"
                    },
                    {
                        "authorId": "2109967458",
                        "name": "Zeju Li"
                    },
                    {
                        "authorId": "143771778",
                        "name": "C. Qin"
                    },
                    {
                        "authorId": "2064774",
                        "name": "Wenjia Bai"
                    },
                    {
                        "authorId": "1717710",
                        "name": "D. Rueckert"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Variants of such a regularization include the minimization across the source domains of the maximum mean discrepancy criteria (MMD) [21, 35], the minimization of a distance metric between the domain-specific means [71] or covariance matrices [69], the minimization of a contrastive loss [50, 83, 44, 29], or the maximization of loss gradient alignment [65, 63]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "30131a0dcaa236a4c78094653035e801213ea653",
                "externalIds": {
                    "DBLP": "conf/acml/0002CKRT22",
                    "ArXiv": "2110.04545",
                    "CorpusId": 238582914
                },
                "corpusId": 238582914,
                "publicationVenue": {
                    "id": "2486528b-036c-4f3c-953f-c574eb381d12",
                    "name": "Asian Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Mach Learn",
                        "ACML"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=40"
                },
                "url": "https://www.semanticscholar.org/paper/30131a0dcaa236a4c78094653035e801213ea653",
                "title": "Towards Data-Free Domain Generalization",
                "abstract": "In this work, we investigate the unexplored intersection of domain generalization (DG) and data-free learning. In particular, we address the question: How can knowledge contained in models trained on different source domains be merged into a single model that generalizes well to unseen target domains, in the absence of source and target domain data? Machine learning models that can cope with domain shift are essential for real-world scenarios with often changing data distributions. Prior DG methods typically rely on using source domain data, making them unsuitable for private decentralized data. We define the novel problem of Data-Free Domain Generalization (DFDG), a practical setting where models trained on the source domains separately are available instead of the original datasets, and investigate how to effectively solve the domain generalization problem in that case. We propose DEKAN, an approach that extracts and fuses domain-specific knowledge from the available teacher models into a student model robust to domain shift. Our empirical evaluation demonstrates the effectiveness of our method which achieves first state-of-the-art results in DFDG by significantly outperforming data-free knowledge distillation and ensemble baselines.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2560012",
                        "name": "A. Frikha"
                    },
                    {
                        "authorId": "12775277",
                        "name": "Haokun Chen"
                    },
                    {
                        "authorId": "2614774",
                        "name": "Denis Krompass"
                    },
                    {
                        "authorId": "1727058",
                        "name": "T. Runkler"
                    },
                    {
                        "authorId": "1742501819",
                        "name": "Volker Tresp"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "170a4c761e7fa2af48000626bbb8d65436e01b69",
                "externalIds": {
                    "ArXiv": "2110.03369",
                    "DBLP": "journals/corr/abs-2110-03369",
                    "CorpusId": 238419135
                },
                "corpusId": 238419135,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/170a4c761e7fa2af48000626bbb8d65436e01b69",
                "title": "The Connection between Out-of-Distribution Generalization and Privacy of ML Models",
                "abstract": "With the goal of generalizing to out-of-distribution (OOD) data, recent domain generalization methods aim to learn\"stable\"feature representations whose effect on the output remains invariant across domains. Given the theoretical connection between generalization and privacy, we ask whether better OOD generalization leads to better privacy for machine learning models, where privacy is measured through robustness to membership inference (MI) attacks. In general, we find that the relationship does not hold. Through extensive evaluation on a synthetic dataset and image datasets like MNIST, Fashion-MNIST, and Chest X-rays, we show that a lower OOD generalization gap does not imply better robustness to MI attacks. Instead, privacy benefits are based on the extent to which a model captures the stable features. A model that captures stable features is more robust to MI attacks than models that exhibit better OOD generalization but do not learn stable features. Further, for the same provable differential privacy guarantees, a model that learns stable features provides higher utility as compared to others. Our results offer the first extensive empirical study connecting stable features and privacy, and also have a takeaway for the domain generalization community; MI attack can be used as a complementary metric to measure model quality.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "133841722",
                        "name": "Divyat Mahajan"
                    },
                    {
                        "authorId": "2855848",
                        "name": "Shruti Tople"
                    },
                    {
                        "authorId": "2109648033",
                        "name": "Amit Sharma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[44] provide a causal interpretation of domain generalization, and show the importance of learning within-class variations for generalization.",
                "However, previous methods may either perform experiments on low-dimensional toy data [11, 16, 78] or lack simulation results to show its causality learning performance [44, 70].",
                "To learn distribution-irrelevant features and models for stable generalization, numerous causality-based distribution generalization methods [11, 13, 16, 23, 44, 48, 67, 70, 74, 76, 78] have been introduced recently."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c46309e622f640989ee90ff7e027be72c1f093f5",
                "externalIds": {
                    "DBLP": "journals/tkdd/YuanMXGL0LK23",
                    "ArXiv": "2110.01438",
                    "DOI": "10.1145/3595380",
                    "CorpusId": 258377770
                },
                "corpusId": 258377770,
                "publicationVenue": {
                    "id": "ca61b508-d706-40a4-8b54-d657dd6cd9d6",
                    "name": "ACM Transactions on Knowledge Discovery from Data",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Knowl Discov Data",
                        "ACM Transactions on Knowledge Discovery From Data"
                    ],
                    "issn": "1556-4681",
                    "url": "http://www.acm.org/pubs/tkdd/",
                    "alternate_urls": [
                        "http://portal.acm.org/tkdd",
                        "http://portal.acm.org/browse_dl.cfm?coll=portal&dl=ACM&idx=J1054&linked=1&part=transaction"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c46309e622f640989ee90ff7e027be72c1f093f5",
                "title": "Instrumental Variable-Driven Domain Generalization with Unobserved Confounders",
                "abstract": "Domain generalization (DG) aims to learn from multiple source domains a model that can generalize well on unseen target domains. Existing DG methods mainly learn the representations with invariant marginal distribution of the input features, however, the invariance of the conditional distribution of the labels given the input features is more essential for unknown domain prediction. Meanwhile, the existing of unobserved confounders which affect the input features and labels simultaneously cause spurious correlation and hinder the learning of the invariant relationship contained in the conditional distribution. Interestingly, with a causal view on the data generating process, we find that the input features of one domain are valid instrumental variables for other domains. Inspired by this finding, we propose an instrumental variable-driven DG method (IV-DG) by removing the bias of the unobserved confounders with two-stage learning. In the first stage, it learns the conditional distribution of the input features of one domain given input features of another domain. In the second stage, it estimates the relationship by predicting labels with the learned conditional distribution. Theoretical analyses and simulation experiments show that it accurately captures the invariant relationship. Extensive experiments on real-world datasets demonstrate that IV-DG method yields state-of-the-art results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38511927",
                        "name": "Junkun Yuan"
                    },
                    {
                        "authorId": "2141122566",
                        "name": "Xu Ma"
                    },
                    {
                        "authorId": "50824401",
                        "name": "Ruoxuan Xiong"
                    },
                    {
                        "authorId": "2052304322",
                        "name": "Mingming Gong"
                    },
                    {
                        "authorId": "2215748689",
                        "name": "Xiangyu Liu"
                    },
                    {
                        "authorId": "93192602",
                        "name": "Fei Wu"
                    },
                    {
                        "authorId": "2940227",
                        "name": "Lanfen Lin"
                    },
                    {
                        "authorId": "33870528",
                        "name": "Kun Kuang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1 Domain generalization Unlike domain adaptation, domain generalization [8, 24, 25] cannot use any sample of the target domain, but it still has to capture transferable information across domains."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ba94c91d06b9051fb2d545670c45b3c89a0e5c6",
                "externalIds": {
                    "DBLP": "journals/ejwcn/WangLWZZW22",
                    "MAG": "3201639495",
                    "DOI": "10.21203/rs.3.rs-877944/v1",
                    "CorpusId": 240503657
                },
                "corpusId": 240503657,
                "publicationVenue": {
                    "id": "3215af4b-a40f-474d-bc19-27c154ff31a3",
                    "name": "EURASIP Journal on Wireless Communications and Networking",
                    "type": "journal",
                    "alternate_names": [
                        "Eurasip J Wirel Commun Netw",
                        "Eurasip Journal on Wireless Communications and Networking",
                        "EURASIP J Wirel Commun Netw"
                    ],
                    "issn": "1687-1472",
                    "url": "http://jwcn.eurasipjournals.com/",
                    "alternate_urls": [
                        "https://jwcn-eurasipjournals.springeropen.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2ba94c91d06b9051fb2d545670c45b3c89a0e5c6",
                "title": "A framework for self-supervised federated domain adaptation",
                "abstract": "Unsupervised federated domain adaptation uses the knowledge from several distributed unlabelled source domains to complete the learning on the unlabelled target domain. Some of the existing methods have limited effectiveness and involve frequent communication. This paper proposes a framework to solve the distributed multi-source domain adaptation problem, referred as self-supervised federated domain adaptation (SFDA). Specifically, a multi-domain model generalization balance is proposed to aggregate the models from multiple source domains in each round of communication. A weighted strategy based on centroid similarity is also designed for SFDA. SFDA conducts self-supervised training on the target domain to tackle domain shift. Compared with the classical federated adversarial domain adaptation algorithm, SFDA is not only strong in communication cost and privacy protection but also improves in the accuracy of the model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152594588",
                        "name": "Bin Wang"
                    },
                    {
                        "authorId": "2149146451",
                        "name": "Gang Li"
                    },
                    {
                        "authorId": "2115422697",
                        "name": "Chao Wu"
                    },
                    {
                        "authorId": "2108238375",
                        "name": "Weishan Zhang"
                    },
                    {
                        "authorId": "1709431",
                        "name": "Jiehan Zhou"
                    },
                    {
                        "authorId": "2136763355",
                        "name": "Ye Wei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "the difference between the means [16] or covariance matrices (CORAL) [17] in the embedding space across different domains, or minimizing a contrastive loss [18]\u2013[20], e."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "254b1644c1f4c1694535c4b186dc7e55ac19cb11",
                "externalIds": {
                    "ArXiv": "2109.04320",
                    "DBLP": "conf/icpr/0002KT22",
                    "DOI": "10.1109/ICPR56361.2022.9956088",
                    "CorpusId": 237453673
                },
                "corpusId": 237453673,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/254b1644c1f4c1694535c4b186dc7e55ac19cb11",
                "title": "Discovery of New Multi-Level Features for Domain Generalization via Knowledge Corruption",
                "abstract": "Machine learning models that can generalize to unseen domains are essential when applied in real-world scenarios involving strong domain shifts. We address the challenging domain generalization (DG) problem, where a model trained on a set of source domains is expected to generalize well in unseen domains without any exposure to their data. The main challenge of DG is that the features learned from the source domains are not necessarily present in the unseen target domains, leading to performance deterioration. We assume that learning a richer set of features is crucial to improve the transfer to a wider set of unknown domains. For this reason, we propose COLUMBUS, a method that enforces new feature discovery via a targeted corruption of the most relevant input and multilevel representations of the data. We conduct an extensive empirical evaluation to demonstrate the effectiveness of the proposed approach which achieves new state-of-the-art results by outperforming 18 DG algorithms on multiple DG benchmark datasets in the DomainBed framework.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2560012",
                        "name": "A. Frikha"
                    },
                    {
                        "authorId": "2614774",
                        "name": "Denis Krompass"
                    },
                    {
                        "authorId": "1742501819",
                        "name": "Volker Tresp"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2) [2], [3], [56], [111], [112], [113], [114], [115], [116], [117](Section 4.",
                "[114] introduce a contrastive regularizer which matches the representation of same objects across environments."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e5b2e2a284db5ba7c2c011daba9769d2c56b6586",
                "externalIds": {
                    "ArXiv": "2108.13624",
                    "DBLP": "journals/corr/abs-2108-13624",
                    "CorpusId": 237364121
                },
                "corpusId": 237364121,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e5b2e2a284db5ba7c2c011daba9769d2c56b6586",
                "title": "Towards Out-Of-Distribution Generalization: A Survey",
                "abstract": "Traditional machine learning paradigms are based on the assumption that both training and test data follow the same statistical pattern, which is mathematically referred to as Independent and Identically Distributed ($i.i.d.$). However, in real-world applications, this $i.i.d.$ assumption often fails to hold due to unforeseen distributional shifts, leading to considerable degradation in model performance upon deployment. This observed discrepancy indicates the significance of investigating the Out-of-Distribution (OOD) generalization problem. OOD generalization is an emerging topic of machine learning research that focuses on complex scenarios wherein the distributions of the test data differ from those of the training data. This paper represents the first comprehensive, systematic review of OOD generalization, encompassing a spectrum of aspects from problem definition, methodological development, and evaluation procedures, to the implications and future directions of the field. Our discussion begins with a precise, formal characterization of the OOD generalization problem. Following that, we categorize existing methodologies into three segments: unsupervised representation learning, supervised model learning, and optimization, according to their positions within the overarching learning process. We provide an in-depth discussion on representative methodologies for each category, further elucidating the theoretical links between them. Subsequently, we outline the prevailing benchmark datasets employed in OOD generalization studies. To conclude, we overview the existing body of work in this domain and suggest potential avenues for future research on OOD generalization. A summary of the OOD generalization methodologies surveyed in this paper can be accessed at http://out-of-distribution-generalization.com.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "24069072",
                        "name": "Zheyan Shen"
                    },
                    {
                        "authorId": "2120846500",
                        "name": "Jiashuo Liu"
                    },
                    {
                        "authorId": "2145031333",
                        "name": "Yue He"
                    },
                    {
                        "authorId": "51258901",
                        "name": "Xingxuan Zhang"
                    },
                    {
                        "authorId": "150287491",
                        "name": "Renzhe Xu"
                    },
                    {
                        "authorId": "2187083103",
                        "name": "Han Yu"
                    },
                    {
                        "authorId": "143738684",
                        "name": "Peng Cui"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[11] Divyat Mahajan, Shruti Tople, and Amit Sharma.",
                "Domain generalization and domain-invariant learning methods assume the training data consists of multiple sufficiently different environments to generalize to unseen test data that is related to the given environments or subgroups [2, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28].",
                "Recent work uses multiple sufficiently different environments to generalize to unseen test data that lies in the support of the given environments or subgroups [2, 10, 11, 17, 18, 19, 20, 21, 22].",
                "Methods for invariant representation learning [2, 10, 11, 12] typically require data from multiple different environments.",
                "In table 1, we summarize key differences between NURD and the related work: invariant learning [2, 10], distribution matching [11, 12], shift-stable prediction [13], group-DRO [14], and causal regularization [15, 16].",
                "Methods based on conditional distribution matching [11, 12] build representations that are conditionally independent of the environment variable given the label."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "440c098ce8c0ff2042543d3e4188ebb95acdb75a",
                "externalIds": {
                    "ArXiv": "2107.00520",
                    "DBLP": "conf/iclr/PuliZOR22",
                    "CorpusId": 247519239
                },
                "corpusId": 247519239,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/440c098ce8c0ff2042543d3e4188ebb95acdb75a",
                "title": "Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations",
                "abstract": "In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "81522899",
                        "name": "A. Puli"
                    },
                    {
                        "authorId": "2108109948",
                        "name": "Lily H. Zhang"
                    },
                    {
                        "authorId": "4682692",
                        "name": "E. Oermann"
                    },
                    {
                        "authorId": "2615814",
                        "name": "R. Ranganath"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", multiple source datasets) to learn domain-invariant features using methods like adversarial losses [19], [85], [86], [87], or moment-matching [88], [89]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5f0c07f13684c362272744453220091a1af23463",
                "externalIds": {
                    "ArXiv": "2106.15453",
                    "CorpusId": 257426978
                },
                "corpusId": 257426978,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5f0c07f13684c362272744453220091a1af23463",
                "title": "Critically examining the Domain Generalizability of Facial Expression Recognition models",
                "abstract": "Facial Expression Recognition is a commercially-important application, but one under-appreciated limitation is that such applications require making predictions on out-of-sample distributions, where target images have different properties from the images the model was trained on. How well -- or how badly -- do facial expression recognition models do on unseen target domains? We provide a systematic and critical evaluation of transfer learning -- specifically, domain generalization -- in facial expression recognition. Using a state-of-the-art model with twelve datasets (six collected in-lab and six ``in-the-wild\"), we conduct extensive round-robin-style experiments to evaluate classification accuracies when given new data from an unseen dataset. We also perform multi-source experiments to examine a model's ability to generalize from multiple source datasets, including (i) within-setting (e.g., lab to lab), (ii) cross-setting (e.g., in-the-wild to lab), and (iii) leave-one-out settings. Finally, we compare our results with three commercially-available software. We find sobering results: the accuracy of single- and multi-source domain generalization is only modest. Even for the best-performing multi-source settings, we observe average classification accuracies of 65.6% (range: 34.6%-88.6%; chance: 14.3%), corresponding to an average drop of 10.8 percentage points from the within-corpus classification performance (mean: 76.4%). We discuss the need for regular, systematic investigations into the generalizability of affective computing models and applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2071672225",
                        "name": "Varsha Suresh"
                    },
                    {
                        "authorId": "1563133497",
                        "name": "G. Yeo"
                    },
                    {
                        "authorId": "46673560",
                        "name": "D. Ong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Rosenfeld et al. [2021b] prove that even for a simple generative model and linear classifiers, the environment complexity of IRM\u2014and other objectives based on the same principle of invariance\u2014is at least as large as the dimension of the spurious latent features, ds.",
                "Arjovsky et al. [2019] assume invariant E[y | \u03a6(x)], and follow-up works assume invariance of higher moments [Xie et al., 2020, Jin et al., 2020, Mahajan et al., 2020, Krueger et al., 2020, Bellot and van der Schaar, 2020]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d5d375628b5ed09a4e40c54eccdd1ae97a3a31fc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-09913",
                    "ArXiv": "2106.09913",
                    "CorpusId": 235485549
                },
                "corpusId": 235485549,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d5d375628b5ed09a4e40c54eccdd1ae97a3a31fc",
                "title": "Iterative Feature Matching: Toward Provable Domain Generalization with Logarithmic Environments",
                "abstract": "Domain generalization aims at performing well on unseen test environments with data from a limited number of training environments. Despite a proliferation of proposal algorithms for this task, assessing their performance both theoretically and empirically is still very challenging. Distributional matching algorithms such as (Conditional) Domain Adversarial Networks [Ganin et al., 2016, Long et al., 2018] are popular and enjoy empirical success, but they lack formal guarantees. Other approaches such as Invariant Risk Minimization (IRM) require a prohibitively large number of training environments -- linear in the dimension of the spurious feature space $d_s$ -- even on simple data models like the one proposed by [Rosenfeld et al., 2021]. Under a variant of this model, we show that both ERM and IRM cannot generalize with $o(d_s)$ environments. We then present an iterative feature matching algorithm that is guaranteed with high probability to yield a predictor that generalizes after seeing only $O(\\log d_s)$ environments. Our results provide the first theoretical justification for a family of distribution-matching algorithms widely used in practice under a concrete nontrivial data model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145262885",
                        "name": "Yining Chen"
                    },
                    {
                        "authorId": "49686853",
                        "name": "Elan Rosenfeld"
                    },
                    {
                        "authorId": "27599910",
                        "name": "Mark Sellke"
                    },
                    {
                        "authorId": "2114186424",
                        "name": "Tengyu Ma"
                    },
                    {
                        "authorId": "3181040",
                        "name": "Andrej Risteski"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Like other casual related works (Chang et al. 2020; Mahajan, Tople, and Sharma 2020), we begin with a structural causal model, shown in Figure 2.",
                "Over the past several years, the progress in DG has stemmed from areas such as invariant representation learning [69, 36, 45, 115], causality [7, 53, 106, 64], meta-learning [57, 8, 27], and feature disentanglement [27, 46, 75]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "014c5aa0a61489f09c224d5f43b6a0eeb47a1687",
                "externalIds": {
                    "ArXiv": "2106.06333",
                    "DBLP": "conf/aaai/LiSWZRLK022",
                    "DOI": "10.1609/aaai.v36i7.20703",
                    "CorpusId": 235417355
                },
                "corpusId": 235417355,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/014c5aa0a61489f09c224d5f43b6a0eeb47a1687",
                "title": "Invariant Information Bottleneck for Domain Generalization",
                "abstract": "Invariant risk minimization (IRM) has recently emerged as a promising alternative for domain generalization. Nevertheless, the loss function is difficult to optimize for nonlinear classifiers and the original optimization objective could fail when pseudo-invariant features and geometric skews exist. Inspired by IRM, in this paper we propose a novel formulation for domain generalization, dubbed invariant information bottleneck (IIB). IIB aims at minimizing invariant risks for nonlinear classifiers and simultaneously mitigating the impact of pseudo-invariant features and geometric skews. Specifically, we first present a novel formulation for invariant causal prediction via mutual information. Then we adopt the variational formulation of the mutual information to develop a tractable loss function for nonlinear classifiers. To overcome the failure modes of IRM, we propose to minimize the mutual information between the inputs and the corresponding representations. IIB significantly outperforms IRM on synthetic datasets, where the pseudo-invariant features and geometric skews occur, showing the effectiveness of proposed formulation in overcoming failure modes of IRM. Furthermore, experiments on DomainBed show that IIB outperforms 13 baselines by 0.9% on average across 7 real datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "71788673",
                        "name": "Bo Li"
                    },
                    {
                        "authorId": "2115383310",
                        "name": "Yifei Shen"
                    },
                    {
                        "authorId": "2115738764",
                        "name": "Yezhen Wang"
                    },
                    {
                        "authorId": "2117086566",
                        "name": "Wenzhen Zhu"
                    },
                    {
                        "authorId": "1718827",
                        "name": "Colorado Reed"
                    },
                    {
                        "authorId": "1519064904",
                        "name": "Jun Zhang"
                    },
                    {
                        "authorId": "2119081394",
                        "name": "Dongsheng Li"
                    },
                    {
                        "authorId": "1732330",
                        "name": "K. Keutzer"
                    },
                    {
                        "authorId": "1390716752",
                        "name": "Han Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019), there have been several interesting works \u2014 (Teney et al., 2020; Krueger et al., 2020; Ahuja et al., 2020; Jin et al., 2020; Chang et al., 2020; Ahuja et al., 2021a; Mahajan et al., 2020; Koyama and Yamaguchi, 2020; M\u00fcller et al., 2020; Parascandolo et al., 2021; Ahmed et al., 2021; Robey et al., 2021; Zhang et al., 2021) is an incomplete representative list \u2014 that build new methods inspired from IRM to address the OOD generalization problem."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4390d210bfd4cd7b646f13f287f44f9620a4f214",
                "externalIds": {
                    "ArXiv": "2106.06607",
                    "DBLP": "conf/nips/AhujaCZGBMR21",
                    "CorpusId": 235422593
                },
                "corpusId": 235422593,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4390d210bfd4cd7b646f13f287f44f9620a4f214",
                "title": "Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization",
                "abstract": "The invariance principle from causality is at the heart of notable approaches such as invariant risk minimization (IRM) that seek to address out-of-distribution (OOD) generalization failures. Despite the promising theory, invariance principle-based approaches fail in common classification tasks, where invariant (causal) features capture all the information about the label. Are these failures due to the methods failing to capture the invariance? Or is the invariance principle itself insufficient? To answer these questions, we revisit the fundamental assumptions in linear regression tasks, where invariance-based approaches were shown to provably generalize OOD. In contrast to the linear regression tasks, we show that for linear classification tasks we need much stronger restrictions on the distribution shifts, or otherwise OOD generalization is impossible. Furthermore, even with appropriate restrictions on distribution shifts in place, we show that the invariance principle alone is insufficient. We prove that a form of the information bottleneck constraint along with invariance helps address key failures when invariant features capture all the information about the label and also retains the existing success when they do not. We propose an approach that incorporates both of these principles and demonstrate its effectiveness in several experiments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3048927",
                        "name": "Kartik Ahuja"
                    },
                    {
                        "authorId": "24130593",
                        "name": "Ethan Caballero"
                    },
                    {
                        "authorId": "113087412",
                        "name": "Dinghuai Zhang"
                    },
                    {
                        "authorId": "1751762",
                        "name": "Yoshua Bengio"
                    },
                    {
                        "authorId": "2065139188",
                        "name": "Ioannis Mitliagkas"
                    },
                    {
                        "authorId": "2109771",
                        "name": "I. Rish"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One of the major issues behind this observed behavior is a lack of clarity and applicability for the underlying assumptions regarding the problem statement and the data-generating process (Zhao et al., 2019; Rosenfeld et al., 2020; Mahajan et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c1a3ea2b84d211faf27b0b5e50ccd1c9e2d112e1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-03783",
                    "ArXiv": "2106.03783",
                    "CorpusId": 235364005
                },
                "corpusId": 235364005,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c1a3ea2b84d211faf27b0b5e50ccd1c9e2d112e1",
                "title": "An Information-theoretic Approach to Distribution Shifts",
                "abstract": "Safely deploying machine learning models to the real world is often a challenging process. Models trained with data obtained from a specific geographic location tend to fail when queried with data obtained elsewhere, agents trained in a simulation can struggle to adapt when deployed in the real world or novel environments, and neural networks that are fit to a subset of the population might carry some selection bias into their decision process. In this work, we describe the problem of data shift from a novel information-theoretic perspective by (i) identifying and describing the different sources of error, (ii) comparing some of the most promising objectives explored in the recent domain generalization, and fair classification literature. From our theoretical analysis and empirical evaluation, we conclude that the model selection procedure needs to be guided by careful considerations regarding the observed data, the factors used for correction, and the structure of the data-generating process.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8300792",
                        "name": "M. Federici"
                    },
                    {
                        "authorId": "46670616",
                        "name": "Ryota Tomioka"
                    },
                    {
                        "authorId": "51131843",
                        "name": "Patrick Forr'e"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "62606989dc2cdfa35a2316b4e28bc32097ac6867",
                "externalIds": {
                    "DBLP": "journals/tip/WangLCWHCH23",
                    "ArXiv": "2106.00925",
                    "DOI": "10.1109/TIP.2022.3227457",
                    "CorpusId": 235294321,
                    "PubMed": "37015360"
                },
                "corpusId": 235294321,
                "publicationVenue": {
                    "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                    "name": "IEEE Transactions on Image Processing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Image Process"
                    ],
                    "issn": "1057-7149",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/62606989dc2cdfa35a2316b4e28bc32097ac6867",
                "title": "Contrastive-ACE: Domain Generalization Through Alignment of Causal Mechanisms",
                "abstract": "Domain generalization aims to learn knowledge invariant across different distributions while semantically meaningful for downstream tasks from multiple source domains, to improve the model\u2019s generalization ability on unseen target domains. The fundamental objective is to understand the underlying \u201dinvariance\u201d behind these observational distributions and such invariance has been shown to have a close connection to causality. While many existing approaches make use of the property that causal features are invariant across domains, we consider the invariance of the average causal effect of the features to the labels. This invariance regularizes our training approach in which interventions are performed on features to enforce stability of the causal prediction by the classifier across domains. Our work thus sheds some light on the domain generalization problem by introducing invariance of the mechanisms into the learning process. Experiments on several benchmark datasets demonstrate the performance of the proposed method against SOTAs. The codes are available at: https://github.com/lithostark/Contrastive-ACE.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2125233830",
                        "name": "Yunqi Wang"
                    },
                    {
                        "authorId": "39853706",
                        "name": "Furui Liu"
                    },
                    {
                        "authorId": "2827164",
                        "name": "Zhitang Chen"
                    },
                    {
                        "authorId": "2107900954",
                        "name": "Yik-Chung Wu"
                    },
                    {
                        "authorId": "40513470",
                        "name": "Jianye Hao"
                    },
                    {
                        "authorId": "2653181",
                        "name": "Guangyong Chen"
                    },
                    {
                        "authorId": "1714602",
                        "name": "P. Heng"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e30f357105752b7955300cf8549d358fa4cd383a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-13581",
                    "ArXiv": "2104.13581",
                    "CorpusId": 233423643
                },
                "corpusId": 233423643,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e30f357105752b7955300cf8549d358fa4cd383a",
                "title": "Deep Domain Generalization with Feature-norm Network",
                "abstract": "In this paper, we tackle the problem of training with multiple source domains with the aim to generalize to new domains at test time without an adaptation step. This is known as domain generalization (DG). Previous works on DG assume identical categories or label space across the source domains. In the case of category shift among the source domains, previous methods on DG are vulnerable to negative transfer due to the large mismatch among label spaces, decreasing the target classification accuracy. To tackle the aforementioned problem, we introduce an end-to-end feature-norm network (FNN) which is robust to negative transfer as it does not need to match the feature distribution among the source domains. We also introduce a collaborative feature-norm network (CFNN) to further improve the generalization capability of FNN. The CFNN matches the predictions of the next most likely categories for each training sample which increases each network's posterior entropy. We apply the proposed FNN and CFNN networks to the problem of DG for image classification tasks and demonstrate significant improvement over the state-of-the-art.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2157552555",
                        "name": "Mohammad Mahfujur Rahman"
                    },
                    {
                        "authorId": "3140440",
                        "name": "C. Fookes"
                    },
                    {
                        "authorId": "1729760",
                        "name": "S. Sridharan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "According to [40], T = T \u2217 holds when the training data are large enough."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2370f1fdaca74ebea2b920fdcee8bdc071b4e811",
                "externalIds": {
                    "DBLP": "journals/tnn/LiWZZ22",
                    "DOI": "10.1109/TNNLS.2021.3071564",
                    "CorpusId": 233372152,
                    "PubMed": "33886476"
                },
                "corpusId": 233372152,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2370f1fdaca74ebea2b920fdcee8bdc071b4e811",
                "title": "Whitening-Net: A Generalized Network to Diagnose the Faults Among Different Machines and Conditions",
                "abstract": "Intelligent bearing diagnostic methods are developing rapidly, but they are difficult to implement due to the lack of real industrial data. A feasible way to deal with this problem is to train a network through laboratory data to mine the causality of bearing faults. This means that the constructed network can handle domain deviations caused by the change of machines, working conditions, noise, and so on which is, however, not a simple task. In response to this problem, a new domain generalization framework\u2014Whitening-Net\u2014was proposed in this article. This framework first defined the homologous compound domain signal as the data basis. Subsequently, the causal loss was proposed to impose regularization constraints on the network, which enhances the network\u2019s ability to mine causality. To avoid domain-specific information from interfering with causal mining, a whitening structure was proposed to whiten the domain, prompting the network to pay more attention to the causality of the signal rather than the domain noise. The results of diagnosis and interpretation proved the ability of Whitening-Net in mining causal mechanisms, which shows that the proposed network can generalize to different machines, even if the tested working conditions and bearing types are completely different from the training domains.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2048334187",
                        "name": "Jiejue Li"
                    },
                    {
                        "authorId": "46395365",
                        "name": "Yu Wang"
                    },
                    {
                        "authorId": "1952345",
                        "name": "Y. Zi"
                    },
                    {
                        "authorId": "2116216178",
                        "name": "Zhijie Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6933e2ae7ac4e96183b62f9ba0073a13651684bc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-06557",
                    "ArXiv": "2104.06557",
                    "CorpusId": 233231471
                },
                "corpusId": 233231471,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6933e2ae7ac4e96183b62f9ba0073a13651684bc",
                "title": "Towards Causal Federated Learning For Enhanced Robustness and Privacy",
                "abstract": "Federated Learning is an emerging privacy-preserving distributed machine learning approach to building a shared model by performing distributed training locally on participating devices (clients) and aggregating the local models into a global one. As this approach prevents data collection and aggregation, it helps in reducing associated privacy risks to a great extent. However, the data samples across all participating clients are usually not independent and identically distributed (non-iid), and Out of Distribution(OOD) generalization for the learned models can be poor. Besides this challenge, federated learning also remains vulnerable to various attacks on security wherein a few malicious participating entities work towards inserting backdoors, degrading the generated aggregated model as well as inferring the data owned by participating entities. In this paper, we propose an approach for learning invariant (causal) features common to all participating clients in a federated learning setup and analyze empirically how it enhances the Out of Distribution (OOD) accuracy as well as the privacy of the final learned model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1476818482",
                        "name": "Sreya Francis"
                    },
                    {
                        "authorId": "80844102",
                        "name": "Irene Tenison"
                    },
                    {
                        "authorId": "2109771",
                        "name": "I. Rish"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other methods are also matching the feature covariance across source domains [150] or take a causal interpretation to match representations of features [122]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "972b9f26aa08a60dc0a82dfe51130ff081dd99e5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-01742",
                    "ArXiv": "2104.01742",
                    "CorpusId": 233025110
                },
                "corpusId": 233025110,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/972b9f26aa08a60dc0a82dfe51130ff081dd99e5",
                "title": "Explainability-aided Domain Generalization for Image Classification",
                "abstract": "of thesis entitled Explainability-aided Domain Generalization for Image Classification",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1456064291",
                        "name": "Robin M. Schmidt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other approaches include meta learning [24], invariant risk minimization [2], distributionally robust optimization [38], mixup [46, 47, 44], and causal matching [32]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7f3c4a3fe5bb372be36334deb928362ddfc35e03",
                "externalIds": {
                    "ArXiv": "2103.15796",
                    "DBLP": "journals/corr/abs-2103-15796",
                    "DOI": "10.1109/CVPR46437.2021.01411",
                    "CorpusId": 232417519
                },
                "corpusId": 232417519,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7f3c4a3fe5bb372be36334deb928362ddfc35e03",
                "title": "Adaptive Methods for Real-World Domain Generalization",
                "abstract": "Invariant approaches have been remarkably successful in tackling the problem of domain generalization, where the objective is to perform inference on data distributions different from those used in training. In our work, we investigate whether it is possible to leverage domain information from the unseen test samples themselves. We propose a domain-adaptive approach consisting of two steps: a) we first learn a discriminative domain embedding from unsupervised training examples, and b) use this domain embedding as supplementary information to build a domain-adaptive model, that takes both the input as well as its domain into account while making predictions. For unseen domains, our method simply uses few unlabelled test examples to construct the domain embedding. This enables adaptive classification on any unseen domain. Our approach achieves state-of-the-art performance on various domain generalization benchmarks. In addition, we introduce the first real-world, large-scale domain generalization benchmark, Geo-YFCC, containing 1.1M samples over 40 training, 7 validation and 15 test domains, orders of magnitude larger than prior work. We show that the existing approaches either do not scale to this dataset or underperform compared to the simple baseline of training a model on the union of data from all training domains. In contrast, our approach achieves a significant 1% improvement.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2479521",
                        "name": "Abhimanyu Dubey"
                    },
                    {
                        "authorId": "34066479",
                        "name": "Vignesh Ramanathan"
                    },
                    {
                        "authorId": "1682773",
                        "name": "A. Pentland"
                    },
                    {
                        "authorId": "144542135",
                        "name": "D. Mahajan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "75f429cc281b33968c8d5fd9dddb570300f4ea80",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-13455",
                    "ArXiv": "2103.13455",
                    "CorpusId": 232352329
                },
                "corpusId": 232352329,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/75f429cc281b33968c8d5fd9dddb570300f4ea80",
                "title": "Matched sample selection with GANs for mitigating attribute confounding",
                "abstract": "Measuring biases of vision systems with respect to protected attributes like gender and age is critical as these systems gain widespread use in society. However, significant correlations between attributes in benchmark datasets make it difficult to separate algorithmic bias from dataset bias. To mitigate such attribute confounding during bias analysis, we propose a matching approach that selects a subset of images from the full dataset with balanced attribute distributions across protected attributes. Our matching approach first projects real images onto a generative adversarial network (GAN)'s latent space in a manner that preserves semantic attributes. It then finds image matches in this latent space across a chosen protected attribute, yielding a dataset where semantic and perceptual attributes are balanced across the protected attribute. We validate projection and matching strategies with qualitative, quantitative, and human annotation experiments. We demonstrate our work in the context of gender bias in multiple open-source facial-recognition classifiers and find that bias persists after removing key confounders via matching. Code and documentation to reproduce the results here and apply the methods to new data is available at https://github.com/csinva/matching-with-gans .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145229121",
                        "name": "Chandan Singh"
                    },
                    {
                        "authorId": "47231927",
                        "name": "Guha Balakrishnan"
                    },
                    {
                        "authorId": "1690922",
                        "name": "P. Perona"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Amplifying the domain gap in image decomposition: Traditionally, domain gaps between training and testing data have been a long-standing issue in Computer Vision [2, 28, 29, 46]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4c454db53455ad48da30431fa0cadde7de984fef",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-12201",
                    "ArXiv": "2103.12201",
                    "CorpusId": 232320806
                },
                "corpusId": 232320806,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4c454db53455ad48da30431fa0cadde7de984fef",
                "title": "Improved Detection of Face Presentation Attacks Using Image Decomposition",
                "abstract": "Presentation attack detection (PAD) is a critical component in secure face authentication. We present a PAD algorithm to distinguish face spoofs generated by a photograph of a subject from live images. Our method uses an image decomposition network to extract albedo and normal. The domain gap between the real and spoof face images leads to easily identifiable differences, especially between the recovered albedo maps. We enhance this domain gap by retraining existing methods using supervised contrastive loss. We present empirical and theoretical analysis that demonstrates that the contrast and lighting effects can play a significant role in PAD; these show up particularly in the recovered albedo. Finally, we demonstrate that by combining all of these methods we achieve state-of-the-art results on datasets such as CelebA-Spoof, OULU and CASIA-SURF.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2850880",
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "authorId": "144037321",
                        "name": "K. Sengupta"
                    },
                    {
                        "authorId": "2057487802",
                        "name": "Max Horowitz-Gelb"
                    },
                    {
                        "authorId": "39336289",
                        "name": "Wen-Sheng Chu"
                    },
                    {
                        "authorId": "35119991",
                        "name": "Sofien Bouaziz"
                    },
                    {
                        "authorId": "2059096514",
                        "name": "David Jacobs"
                    }
                ]
            }
        },
        {
            "contexts": [
                "is another option for reducing distribution mismatch [169], [170], [171], which takes into"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b249fe4e5e2bada6655ce5d61e7f50da5d471cb4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-02503",
                    "ArXiv": "2103.02503",
                    "DOI": "10.1109/TPAMI.2022.3195549",
                    "CorpusId": 232104764,
                    "PubMed": "35914036"
                },
                "corpusId": 232104764,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b249fe4e5e2bada6655ce5d61e7f50da5d471cb4",
                "title": "Domain Generalization: A Survey",
                "abstract": "Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other relevant fields like domain adaptation and transfer learning. Then, we conduct a thorough review into existing methods and theories. Finally, we conclude this survey with insights and discussions on future research directions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9368124",
                        "name": "Kaiyang Zhou"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "143970608",
                        "name": "Y. Qiao"
                    },
                    {
                        "authorId": "145406421",
                        "name": "T. Xiang"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "When such object label is unavailable, [76] further learned an object feature based on labels in a separate stage.",
                "activity recognition [65], chest X-ray recognition [76], [90], and EEG-based seizure detection [88]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "085907c9b2bfbf39bcaf6fe3d16bd1dadcef5af5",
                "externalIds": {
                    "DBLP": "conf/ijcai/0001LLOQ21",
                    "ArXiv": "2103.03097",
                    "DOI": "10.1109/TKDE.2022.3178128",
                    "CorpusId": 232110832
                },
                "corpusId": 232110832,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/085907c9b2bfbf39bcaf6fe3d16bd1dadcef5af5",
                "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization",
                "abstract": "Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1519290245",
                        "name": "Jindong Wang"
                    },
                    {
                        "authorId": "40093162",
                        "name": "Cuiling Lan"
                    },
                    {
                        "authorId": "2144545128",
                        "name": "Chang Liu"
                    },
                    {
                        "authorId": "1796267433",
                        "name": "Yidong Ouyang"
                    },
                    {
                        "authorId": "143826491",
                        "name": "Tao Qin"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019] and MatchDG [Mahajan et al., 2020], and use the same hyper-parameters suggested by the original paper.",
                "We use a learning rate of 1e-4 for DIVA and HDUVA, a learning rate of 1e-5 (better than 1e-4) for Deep-All and the suggested learning rate for MatchDG.",
                ", 2019] and Match-DG [Mahajan et al., 2020], while Deep-All is used as baseline by pooling all training domain s together.",
                "For the second domain nominal domain, we rotate the same\nColorMnist (Figure 5 ) Test Domain 1 Test Domain 3\nDIVA 0.53 \u00b1 0.05 0.63 \u00b1 0.05 HDUVA 0.56 \u00b1 0.05 0.68 \u00b1 0.05 Deep-All 0.53 \u00b1 0.06 0.61 \u00b1 0.06 Match-DG 0.44 \u00b1 0.04 0.67 \u00b1 0.10\nsubset of MNIST, by 30, 45 and 60 degrees respectively.",
                ", 2019] and Match-DG [Mahajan et al., 2020], while Deep-All is used as baseline by pooling all training domains together.",
                "For comparing algorithms, we implemented DIVA [Ilse et al., 2019] and MatchDG [Mahajan et al., 2020], and use the same hyper-parameters suggested by the original paper.",
                "Causality based Method Recently, Mahajan et al. [2020] proposed MatchDG with that approximates base object similarity by using a contrastive loss formulation adapted for multiple domains.",
                "We use a learning rate of 1e-5 for HDUVA, DIVA, Deep-All and use default learning rate of MatchDG.",
                ", 2019] and Match-DG [Mahajan et al., 2020], while Deep-All by pooling all training domains together is used as baseline.",
                "Our approach is able to implicitly learn the unobserved domain substructure of the data, resulting is substantially better accuracy on unseen test domains (i.e. a new hospital) compared to state-of-the-art approaches DIVA and MatchDG."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9af91aca546d5837544a389a86edbd196257f00f",
                "externalIds": {
                    "ArXiv": "2101.09436",
                    "CorpusId": 232033877
                },
                "corpusId": 232033877,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9af91aca546d5837544a389a86edbd196257f00f",
                "title": "Hierarchical Variational Auto-Encoding for Unsupervised Domain Generalization",
                "abstract": "We address the task of domain generalization, where the goal is to train a predictive model such that it is able to generalize to a new, previously unseen domain. We choose a hierarchical generative approach within the framework of variational autoencoders and propose a domain-unsupervised algorithm that is able to generalize to new domains without domain supervision. We show that our method is able to learn representations that disentangle domain-specific information from class-label specific information even in complex settings where domain structure is not observed during training. Our interpretable method outperforms previously proposed generative algorithms for domain generalization as well as other non-generative state-of-the-art approaches in several hierarchical domain settings including sequential overlapped near continuous domain shift. It also achieves competitive performance on the standard domain generalization benchmark dataset PACS compared to state-of-the-art approaches which rely on observing domain-specific information during training, as well as another domain unsupervised method. Additionally, we proposed model selection purely based on Evidence Lower Bound (ELBO) and also proposed weak domain supervision where implicit domain information can be added into the algorithm.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48305274",
                        "name": "Xudong Sun"
                    },
                    {
                        "authorId": "2315845",
                        "name": "F. Buettner"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d991f49a59c5e5e001df214840ff274f7df7893d",
                "externalIds": {
                    "DBLP": "conf/aistats/AhujaSD21",
                    "MAG": "3095820579",
                    "ArXiv": "2010.15234",
                    "CorpusId": 225103340
                },
                "corpusId": 225103340,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d991f49a59c5e5e001df214840ff274f7df7893d",
                "title": "Linear Regression Games: Convergence Guarantees to Approximate Out-of-Distribution Solutions",
                "abstract": "Recently, invariant risk minimization (IRM) (Arjovsky et al.) was proposed as a promising solution to address out-of-distribution (OOD) generalization. In Ahuja et al., it was shown that solving for the Nash equilibria of a new class of \"ensemble-games\" is equivalent to solving IRM. In this work, we extend the framework in Ahuja et al. for linear regressions by projecting the ensemble-game on an $\\ell_{\\infty}$ ball. We show that such projections help achieve non-trivial OOD guarantees despite not achieving perfect invariance. For linear models with confounders, we prove that Nash equilibria of these games are closer to the ideal OOD solutions than the standard empirical risk minimization (ERM) and we also provide learning algorithms that provably converge to these Nash Equilibria. Empirical comparisons of the proposed approach with the state-of-the-art show consistent gains in achieving OOD solutions in several settings involving anti-causal variables and confounders.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3048927",
                        "name": "Kartik Ahuja"
                    },
                    {
                        "authorId": "145455859",
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "authorId": "2145784",
                        "name": "Amit Dhurandhar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many papers have suggested similar alternatives (Jin et al., 2020; Mahajan et al., 2020; Bellot & van der Schaar, 2020).",
                "(2019) only assume invariance of E[y | \u03a6(x)]; follow-up works rely on a stronger assumption of invariance of higher conditional moments (Krueger et al., 2020; Xie et al., 2020; Jin et al., 2020; Mahajan et al., 2020; Bellot & van der Schaar, 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1e76e2fbf27198986271a672f462dc38d790d00f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-05761",
                    "MAG": "3092928935",
                    "ArXiv": "2010.05761",
                    "CorpusId": 222290886
                },
                "corpusId": 222290886,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1e76e2fbf27198986271a672f462dc38d790d00f",
                "title": "The Risks of Invariant Risk Minimization",
                "abstract": "Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective$-$as well as these recently proposed alternatives$-$under a fairly natural and general model. In the linear case, we show simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data are sufficiently similar to the training distribution$-$this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49686853",
                        "name": "Elan Rosenfeld"
                    },
                    {
                        "authorId": "145969795",
                        "name": "Pradeep Ravikumar"
                    },
                    {
                        "authorId": "3181040",
                        "name": "Andrej Risteski"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2 Related work A number of contributions under the domain generalization setting borrowed tools from causal inference to enforce the learned representations to be invariant across the different domains presented to the model at training time [11, 12, 13]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a1aaccbb59dda09352e6409f2628f152d79c00dd",
                "externalIds": {
                    "ArXiv": "1911.00804",
                    "MAG": "3035349046",
                    "CorpusId": 219687838
                },
                "corpusId": 219687838,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a1aaccbb59dda09352e6409f2628f152d79c00dd",
                "title": "Generalizing to unseen domains via distribution matching",
                "abstract": "Supervised learning results typically rely on assumptions of i.i.d. data. Unfortunately, those assumptions are commonly violated in practice. In this work, we tackle this problem by focusing on domain generalization: a formalization where the data generating process at test time may yield samples from never-before-seen domains (distributions). Our work relies on a simple lemma: by minimizing a notion of discrepancy between all pairs from a set of given domains, we also minimize the discrepancy between any pairs of mixtures of domains. Using this result, we derive a generalization bound for our setting. We then show that low risk over unseen domains can be achieved by representing the data in a space where (i) the training distributions are indistinguishable, and (ii) relevant information for the task at hand is preserved. Minimizing the terms in our bound yields an adversarial formulation which estimates and minimizes pairwise discrepancies. We validate our proposed strategy on standard domain generalization benchmarks, outperforming a number of recently introduced methods. Notably, we tackle a real-world application where the underlying data corresponds to multi-channel electroencephalography time series from different subjects, each considered as a distinct domain.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "153804922",
                        "name": "Isabela Albuquerque"
                    },
                    {
                        "authorId": "144903711",
                        "name": "Jo\u00e3o Monteiro"
                    },
                    {
                        "authorId": "1401054088",
                        "name": "Mohammad Javad Darvishi Bayazi"
                    },
                    {
                        "authorId": "2632038",
                        "name": "T. Falk"
                    },
                    {
                        "authorId": "3168518",
                        "name": "Ioannis Mitliagkas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026algorithms (Scutari, 2009) or recent methods for learning invariant relationships from training datasets from different distributions (Peters et al., 2016; Arjovsky et al., 2019; Bengio et al., 2019; Mahajan et al., 2020), or learn based on a combination of randomized experiments and observed data.",
                "Alternatively, one may exploit the strong relevance property from (Pellet & Elisseeff, 2008), use score-based learning algorithms (Scutari, 2009) or recent methods for learning invariant relationships from training datasets from different distributions (Peters et al., 2016; Arjovsky et al., 2019; Bengio et al., 2019; Mahajan et al., 2020), or learn based on a combination of randomized experiments and observed data."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "64ec7c9911a3e8ac1037da8d0ae450c237c77778",
                "externalIds": {
                    "MAG": "2975202395",
                    "DBLP": "conf/icml/TopleSN20",
                    "ArXiv": "1909.12732",
                    "CorpusId": 203591531
                },
                "corpusId": 203591531,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/64ec7c9911a3e8ac1037da8d0ae450c237c77778",
                "title": "Alleviating Privacy Attacks via Causal Learning",
                "abstract": "Machine learning models, especially deep neural networks have been shown to be susceptible to privacy attacks such as membership inference where an adversary can detect whether a data point was used for training a black-box model. Such privacy risks are exacerbated when a model's predictions are used on an unseen data distribution. To alleviate privacy attacks, we demonstrate the benefit of predictive models that are based on the causal relationships between input features and the outcome. We first show that models learnt using causal structure generalize better to unseen data, especially on data from different distributions than the train distribution. Based on this generalization property, we establish a theoretical link between causality and privacy: compared to associational models, causal models provide stronger differential privacy guarantees and are more robust to membership inference attacks. Experiments on simulated Bayesian networks and the colored-MNIST dataset show that associational models exhibit upto 80% attack accuracy under different test distributions and sample sizes whereas causal models exhibit attack accuracy close to a random guess.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2855848",
                        "name": "Shruti Tople"
                    },
                    {
                        "authorId": "144676398",
                        "name": "Amit Sharma"
                    },
                    {
                        "authorId": "34894873",
                        "name": "A. Nori"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ca7d26f3a3d18497c6d26fcf1d1c43fd0e3f367f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-10226",
                    "DOI": "10.48550/arXiv.2304.10226",
                    "CorpusId": 258236370
                },
                "corpusId": 258236370,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ca7d26f3a3d18497c6d26fcf1d1c43fd0e3f367f",
                "title": "Domain Generalization for Mammographic Image Analysis via Contrastive Learning",
                "abstract": "\u2014Mammographic image analysis is a funda- mental problem in the computer-aided diagnosis scheme, which has recently made remarkable progress with the advance of deep learning. However, the construction of a deep learning model requires training data that are large and suf\ufb01ciently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, mammogram collection from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. MSVCL is the \ufb01rst contrastive learning based approach to explicitly address the issue of vendor-style domain gap, which can augment the generalization ca- pability of deep learning models to various vendors with limited resources. In this paper, we proposed MSVCL+, with improved contrastive learning schemes and generalization performance. Speci\ufb01cally, the backbone network is \ufb01rstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor styles. Afterward, the backbone network is then recalibrated to the downstream tasks of mass detection, multi-view mass matching, BI-RADS rating and breast density classi\ufb01cation with speci\ufb01c supervised learn- ing. The proposed method is evaluated with mammograms from four vendors and two unseen public datasets. The experimental results suggest that our approach can effectively improve mammographic image analysis performance on unseen domains, and outperform many state-of-the-art (SOTA) generalization methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "25586966",
                        "name": "Zheren Li"
                    },
                    {
                        "authorId": "2089582101",
                        "name": "Zhiming Cui"
                    },
                    {
                        "authorId": "7559893",
                        "name": "Lichi Zhang"
                    },
                    {
                        "authorId": "2151487856",
                        "name": "Sheng Wang"
                    },
                    {
                        "authorId": "2200529234",
                        "name": "Chenjin Lei"
                    },
                    {
                        "authorId": "2056468197",
                        "name": "Xi Ouyang"
                    },
                    {
                        "authorId": "2199162377",
                        "name": "Dongdong Chen"
                    },
                    {
                        "authorId": "2124742549",
                        "name": "Zixu Zhuang"
                    },
                    {
                        "authorId": "2165644396",
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "authorId": "145344984",
                        "name": "Yajia Gu"
                    },
                    {
                        "authorId": "3168855",
                        "name": "Zaiyi Liu"
                    },
                    {
                        "authorId": "49046494",
                        "name": "Chunling Liu"
                    },
                    {
                        "authorId": "2150038187",
                        "name": "Dinggang Shen"
                    },
                    {
                        "authorId": "2519482",
                        "name": "Jie-Zhi Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In these challenging tasks, our method not only beats all popular domain generalization methods but also surpasses the methods of introducing causal inference (MatchDG [14], CSG-ind [12], CIRL [13]), clearly showing the advantages of analyzing the causes of the domain shift by causal inference.",
                "We compare our method with most related methods that introduces causal inference into generalization (MatchDG [14], CSGind [12], CIRL [13]), and existing popular domain generalization methods (MetaReg [1], GUD [18], Epi-FCR [10], MASF [5], JiGen [2], DMG [3], DDAIG [24], CSD [16], L2A-OT [25], EISNet [19], RSC [8], ME-ADA [23], MMLD [15], L2D [20], FACT [21])."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a05496b1d1e51f4f6ee46d8282fe7fcecd99465a",
                "externalIds": {
                    "CorpusId": 260084682
                },
                "corpusId": 260084682,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a05496b1d1e51f4f6ee46d8282fe7fcecd99465a",
                "title": "Supplement Material of \u201cMeta-causal Learning for Single Domain Generalization\u201d",
                "abstract": "Jin Chen1*, Zhi Gao1*, Xinxiao Wu1,2\u2020, Jiebo Luo3 1Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science & Technology, Beijing Institute of Technology, China 2Guangdong Laboratory of Machine Perception and Intelligent Computing, Shenzhen MSU-BIT University, China 3Department of Computer Science, University of Rochester, Rochester NY 14627, USA {chen jin,gaozhi 2017,wuxinxiao}@bit.edu.cn,jluo@cs.rochester.edu",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108458375",
                        "name": "Jin Chen"
                    },
                    {
                        "authorId": "2187550120",
                        "name": "Zhiqiang Gao"
                    },
                    {
                        "authorId": "2125709",
                        "name": "Xinxiao Wu"
                    },
                    {
                        "authorId": "2116782926",
                        "name": "Jiebo Luo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mainstream Domain Generalization (DG) studies [1, 20, 25, 29, 30, 30, 38] primarily focus on extracting invariant representations from source domains that can be effectively generalized to the target domain, which remains inaccessible during training."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5eedb57c05be361e0bfd5ba43e165c563f2fc3ad",
                "externalIds": {
                    "CorpusId": 260125659
                },
                "corpusId": 260125659,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5eedb57c05be361e0bfd5ba43e165c563f2fc3ad",
                "title": "Phase Match for Out-of-Distribution Generalization",
                "abstract": "The Fourier transform, serving as an explicit decomposition method for visual signals, has been employed to explain the out-of-distribution generalization behaviors of Convolutional Neural Networks (CNNs). Previous research and empirical studies have indicated that the amplitude spectrum plays a decisive role in CNN recognition, but it is susceptible to disturbance caused by distribution shifts. On the other hand, the phase spectrum preserves highly-structured spatial information, which is crucial for visual representation learning. In this paper, we aim to clarify the relationships between Domain Generalization (DG) and the frequency components by introducing a Fourier-based structural causal model. Specifically, we interpret the phase spectrum as semi-causal factors and the amplitude spectrum as non-causal factors. Building upon these observations, we propose Phase Match (PhaMa) to address DG problems. Our method introduces perturbations on the amplitude spectrum and establishes spatial relationships to match the phase components. Through experiments on multiple benchmarks, we demonstrate that our proposed method achieves state-of-the-art performance in domain generalization and out-of-distribution robustness tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152409198",
                        "name": "Cheng Hu"
                    },
                    {
                        "authorId": "2108756467",
                        "name": "Ruimin Wang"
                    },
                    {
                        "authorId": "1829089545",
                        "name": "Haoxuan Chen"
                    },
                    {
                        "authorId": "2016529",
                        "name": "Zhouwang Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MATCHDG(Mahajan et al., 2021) constructs a representation using contrastive learning, where representations of the same object across environments are invariant."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "82d3c389e682a81f9f545fb42eb8c3e586369e69",
                "externalIds": {
                    "DBLP": "conf/icml/GuoG0WC23",
                    "CorpusId": 260817749
                },
                "corpusId": 260817749,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/82d3c389e682a81f9f545fb42eb8c3e586369e69",
                "title": "Out-of-Distribution Generalization of Federated Learning via Implicit Invariant Relationships",
                "abstract": "Out-of-distribution generalization is challenging for non-participating clients of federated learning under distribution shifts. A proven strategy is to explore those invariant relationships between input and target variables, working equally well for non-participating clients. However, learning invariant relationships is often in an explicit manner from data, representation, and distribution, which violates the federated principles of privacy-preserving and limited communication. In this paper, we propose F ED IIR, which implicitly learns invariant relationships from parameter for out-of-distribution generalization, adhering to the above principles. Specifically, we utilize the prediction disagreement to quantify invariant relationships and implicitly reduce it through inter-client gradient alignment. Theoretically, we demonstrate the range of non-participating clients to which F ED IIR is expected to generalize and present the convergence results for F ED IIR in the massively distributed with limited communication. Extensive experiments show that F ED IIR significantly outperforms relevant baselines in terms of out-of-distribution generalization of federated learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2184557016",
                        "name": "Yaming Guo"
                    },
                    {
                        "authorId": "2061502351",
                        "name": "Kai Guo"
                    },
                    {
                        "authorId": "48811990",
                        "name": "Xiaofeng Cao"
                    },
                    {
                        "authorId": "2112665639",
                        "name": "Tieru Wu"
                    },
                    {
                        "authorId": "2195485095",
                        "name": "Yi Chang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9e007d9a22a4ddd66262c56fec55346af89a0b9d",
                "externalIds": {
                    "CorpusId": 260932593
                },
                "corpusId": 260932593,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9e007d9a22a4ddd66262c56fec55346af89a0b9d",
                "title": "ON THE FAILURE OF INVARIANT RISK MINIMIZATION AND AN EFFECTIVE FIX VIA CLASSIFICATION ERROR CONTROL",
                "abstract": "Invariant Risk Minimization is a well-known Domain Generalization framework that has received much attention over the past few years. Invariant Risk Minimization is capable of learning domain-invariant features from multiple domains by finding representation features such that the optimal classifier on top of these features matches all training domains. In this paper, we show that even though the Invariant Risk Mini-mization algorithm is based on a compelling idea, it is easily vulnerable in a simple toy example where multiple domain-invariant features exist and each possesses a corresponding classifier that is optimal for all domains. Based on this observation, we propose an effective modification of the traditional Invariant Risk Minimization algorithm named Error-Control Invariant Risk Minimization, which allows learning different domain-invariant features via controlling the training classification error, leading to a new algorithm that works well on both our toy synthetic dataset and the real-world datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "123233017",
                        "name": "Thuan Q. Nguyen"
                    },
                    {
                        "authorId": "121848090",
                        "name": "matthias. scheutz"
                    },
                    {
                        "authorId": "2139553851",
                        "name": "Shuchin Aeron"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Domain generalization (DG) (Muandet et al., 2013; Ghifary et al., 2016; Mahajan et al., 2021; Li et al., 2020) aims to solve this hard and significant problem."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "928ca39340784fafca5411d5d45a322463a1aea6",
                "externalIds": {
                    "DBLP": "conf/iclr/FanSTYTSD23",
                    "CorpusId": 259298184
                },
                "corpusId": 259298184,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/928ca39340784fafca5411d5d45a322463a1aea6",
                "title": "Towards Robust Object Detection Invariant to Real-World Domain Shifts",
                "abstract": "Safety-critical applications such as autonomous driving require robust object detection invariant to real-world domain shifts. Such shifts can be regarded as different domain styles, which can vary substantially due to environment changes, but deep models only know the training domain style. Such domain style gap impedes object detection generalization on diverse real-world domains. Existing classification domain generalization (DG) methods cannot effectively solve the robust object detection problem, because they either rely on multiple source domains with large style variance or destroy the content structures of the original images. In this paper, we analyze and investigate effective solutions to overcome domain style overfitting for robust object detection without the above shortcomings. Our method, dubbed as Normalization Perturbation (NP), perturbs the channel statistics of source domain low-level features to synthesize various latent styles, so that the trained deep model can perceive diverse potential domains and generalizes well even without observations of target domain data in training. This approach is motivated by the observation that feature channel statistics of the target domain images deviate around the source domain statistics. We further explore the style-sensitive channels for effective style synthesis. Normalization Perturbation only relies on a single source domain and is surprisingly simple and effective, contributing a practical solution by effectively adapting or generalizing classification DG methods to robust object detection. Extensive experiments demonstrate the effectiveness of our method for generalizing object detectors under real-world domain shifts.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2105557652",
                        "name": "Qi Fan"
                    },
                    {
                        "authorId": "151136071",
                        "name": "Mattia Segu"
                    },
                    {
                        "authorId": "5068280",
                        "name": "Yu-Wing Tai"
                    },
                    {
                        "authorId": "1807197",
                        "name": "F. Yu"
                    },
                    {
                        "authorId": "2088295",
                        "name": "Chi-Keung Tang"
                    },
                    {
                        "authorId": "48920094",
                        "name": "B. Schiele"
                    },
                    {
                        "authorId": "1778526",
                        "name": "Dengxin Dai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d8fb4eed69982790d8ff04cf9b10f51e62c7efcb",
                "externalIds": {
                    "CorpusId": 262044501
                },
                "corpusId": 262044501,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d8fb4eed69982790d8ff04cf9b10f51e62c7efcb",
                "title": "Context \u2248 Environment",
                "abstract": "Two lines of work are taking the central stage in AI research. On the one hand, the community is making increasing efforts to build models that discard spurious correlations and generalize better in novel test environments. Unfortunately, the bitter lesson so far is that no proposal convincingly outperforms a simple empirical risk minimization baseline. On the other hand, large language models (LLMs) have erupted as algorithms able to learn in-context , generalizing on-the-fly to the eclectic contextual circumstances that users enforce by means of prompting. In this paper, we argue that context \u2248 environment , and posit that in-context learning holds the key to better domain generalization. Via extensive theory and experiments, we show that paying attention to context\u2014unlabeled examples as they arrive\u2014allows our proposed In-Context Risk Minimization (ICRM) algorithm to zoom-in on the test environment risk minimizer, leading to significant out-of-distribution performance improvements. From all of this, two messages are worth taking home. Researchers in domain generalization should consider environment as context , and harness the adaptive power of in-context learning. Researchers in LLMs should consider context as environment , to better structure data towards generalization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2242501778",
                        "name": "Sharut Gupta"
                    },
                    {
                        "authorId": "2242248678",
                        "name": "Stefanie Jegelka"
                    },
                    {
                        "authorId": "1401804750",
                        "name": "David Lopez-Paz"
                    },
                    {
                        "authorId": "2242252645",
                        "name": "Kartik Ahuja"
                    },
                    {
                        "authorId": "2243180774",
                        "name": "AI Meta"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The goal is to \"learn representations independent of the domain after conditioning on the class label\" [21].",
                "The authors [21] propose new methods RandMatch, MatchDG, and MDGHybrid to increase performance over the previous state-of-the-art methods for various ML problems.",
                "Figure 2: Slab Dataset (Slab (y-axis) is the stable feature) [21]",
                "Figure 3: Causal graph proposed by the original authors [21] depicting the relation between Ytrue, the causal features which are domain independent Xc and Y"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "49605b30da69d87c94da72bc8aca6b8040405f79",
                "externalIds": {
                    "CorpusId": 246830632
                },
                "corpusId": 246830632,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/49605b30da69d87c94da72bc8aca6b8040405f79",
                "title": "[Re] Domain Generalization using Causal Matching",
                "abstract": "We reproduced the results of the paper \"Domain Generalization Using Causal Matching.\" Traditional supervised learning assumes that the classes/labels seen in testing must have appeared during the training phase. However, this assumption is often violated in real-world applications. For instance, in e-commerce, new categories of products are released every day. A model that cannot detect new/unseen classes is hard to function in such open environments as they are not generalizable.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [
                "On the one hand, many causality-inspired approaches [25, 27, 49] learn domain-invariant representations based on the theory of invariant causal mechanism [30, 39].",
                "It can be observed that FAST surpasses MatchDG [27], a quite state-of-the-art (SOTA) causality-based method that aims at learning domain-invariant representation, by about 5.",
                "The baselines are divided into two groups: non-causality-based methods (from DeepAll [61] to FACT [52]), and causality-based methods (from MatchDG [27] to CIRL [25]).",
                "The baselines are divided into two groups: non-causality-based methods (from\nDeepAll [61] to FACT [52]), and causality-based methods (from MatchDG [27] to CIRL [25]).",
                "It can be observed that FAST surpasses MatchDG [27], a quite state-of-the-art (SOTA) causality-based method that aims at learning domain-invariant representation, by about 5.0% and 1.3% in Art-Painting and Sketch on PACS respectively.",
                "Following previous works [25, 27, 62], we evaluate our proposed methods on three standard OOD generalisation benchmark datasets described below.",
                "Literature has shown a common research streamline, that is to learn domain-invariant features so that models could maintain good performance on unseen target domains [27, 42, 49].",
                "Causal inference has been applied to many deep learning research problems, especially in domain adaptation [26, 40, 58] and DG [27, 30, 49]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c2b8613eb0f7d5cdbf94b9a004fd92584f394024",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-03063",
                    "DOI": "10.48550/arXiv.2212.03063",
                    "CorpusId": 254275185
                },
                "corpusId": 254275185,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c2b8613eb0f7d5cdbf94b9a004fd92584f394024",
                "title": "Front-door Adjustment via Style Transfer for Out-of-distribution Generalisation",
                "abstract": "Out-of-distribution (OOD) generalisation aims to build a model that can well generalise its learnt knowledge from source domains to an unseen target domain. However, current image classi\ufb01cation models often perform poorly in the OOD setting due to statistically spurious correlations learning from model training. From causality-based perspective, we formulate the data generation process in OOD image classi\ufb01cation using a causal graph. On this graph, we show that prediction P ( Y | X ) of a label Y given an image X in statistical learning is formed by both causal effect P ( Y | do ( X )) and spurious effects caused by confounding features (e.g., background). Since the spurious features are domain-variant, the prediction P ( Y | X ) becomes unstable on unseen domains. In this paper, we propose to mitigate the spurious effect of confounders using front-door adjustment. In our method, the mediator variable is hypothesized as semantic features that are essential to determine a label for an image. Inspired by capability of style transfer in image generation, we interpret the combination of the mediator variable with different generated images in the front-door formula and propose novel algorithms to estimate it. Extensive experimental results on widely used benchmark datasets verify the effectiveness of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32125163",
                        "name": "Toan Q. Nguyen"
                    },
                    {
                        "authorId": "36072771",
                        "name": "Kien Do"
                    },
                    {
                        "authorId": "1779016",
                        "name": "D. Nguyen"
                    },
                    {
                        "authorId": "2159149299",
                        "name": "Bao Duong"
                    },
                    {
                        "authorId": "150322672",
                        "name": "T. Nguyen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Methods for invariant representation learning (Arjovsky et al., 2019; Krueger et al., 2020; Mahajan et al., 2020; Guo et al., 2021) typically require data from multiple different environments.",
                "Recent work uses multiple sufficiently different environments to generalize to unseen test data that lies in the support of the given environments or subgroups (Li et al., 2018a; Arjovsky et al., 2019; Akuzawa et al., 2019; Mahajan et al., 2020; Krueger et al., 2020; Bellot and van der Schaar, 2020; Li et al., 2018b; Goel et al., 2020; Wald et al., 2021).",
                "Domain generalization and domain-invariant learning methods assume the training data consists of multiple sufficiently different environments to generalize to unseen test data that is related to the given environments or subgroups (Li et al., 2018a; Arjovsky et al., 2019; Akuzawa et al., 2019; Mahajan et al., 2020; Krueger et al., 2020; Bellot and van der Schaar, 2020; Li et al., 2018b; Goel et al., 2020; Wald et al., 2021; Ganin et al., 2016; Xie et al., 2017; Zhang et al., 2018; Ghimire et al., 2020; Adeli et al., 2021; Zhang et al., 2021).",
                "Methods based on conditional distribution matching (Mahajan et al., 2020; Guo et al., 2021) build representations that are conditionally independent of the environment variable given the label.",
                "\u2026key differences between NURD and the related work: invariant learning (Arjovsky et al., 2019; Krueger et al., 2020), distribution matching (Mahajan et al., 2020; Guo et al., 2021), shift-stable prediction (Subbaswamy et al., 2019a), group-DRO (Sagawa et al., 2019), and causal\u2026",
                "\u2026to generalize to unseen test data that lies in the support of the given environments or subgroups (Li et al., 2018a; Arjovsky et al., 2019; Akuzawa et al., 2019; Mahajan et al., 2020; Krueger et al., 2020; Bellot and van der Schaar, 2020; Li et al., 2018b; Goel et al., 2020; Wald et al., 2021).",
                "\u2026to unseen test data that is related to the given environments or subgroups (Li et al., 2018a; Arjovsky et al., 2019; Akuzawa et al., 2019; Mahajan et al., 2020; Krueger et al., 2020; Bellot and van der Schaar, 2020; Li et al., 2018b; Goel et al., 2020; Wald et al., 2021; Ganin et al.,\u2026",
                ", 2020), distribution matching (Mahajan et al., 2020; Guo et al., 2021), shift-stable prediction (Subbaswamy et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5da37eaf1b59cd4ec37624a7df3a461df4a86d8b",
                "externalIds": {
                    "CorpusId": 247693791
                },
                "corpusId": 247693791,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5da37eaf1b59cd4ec37624a7df3a461df4a86d8b",
                "title": "NUISANCE-INDUCED SPURIOUS CORRELATIONS",
                "abstract": "In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisancerandomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108109948",
                        "name": "Lily H. Zhang"
                    },
                    {
                        "authorId": "4682692",
                        "name": "E. Oermann"
                    },
                    {
                        "authorId": "2615814",
                        "name": "R. Ranganath"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides that, the counterexample in (Mahajan et al., 2021) violates our invariant conditional distribution assumption in (1) and hence is not contrary to our theorem.",
                "\u2026perspectives, e.g., distributional robust optimization (Sinha et al., 2018; Volpi et al., 2018; Sagawa et al., 2019; Yi et al., 2021b; Levy et al., 2020) or causal inference (Arjovsky et al., 2019; He et al., 2021; Liu et al., 2021b; Mahajan et al., 2021; Wang et al., 2022; Ye et al., 2021a).",
                "But none of them directly control the OOD generalization error as our CSV (Seo et al., 2022; Hu et al., 2020; Liu et al., 2021b; Heinze-Deml and Meinshausen, 2021; Mahajan et al., 2021; Ben-David et al., 2007, 2010; Muandet et al., 2013; Ganin et al., 2016).",
                "To the best of our knowledge, this property has not been explored by existing metrics related to OOD generalization (Hu et al., 2020; Seo et al., 2022; Mahajan et al., 2021; Heinze-Deml and Meinshausen, 2021; Krueger et al., 2021).",
                "To this end, Arjovsky et al. (2019); Hu et al. (2020); Li et al. (2018); Mahajan et al. (2021); Heinze-Deml and Meinshausen (2021); Krueger et al. (2021); Wald et al. (2021); Seo et al. (2022) regularize the training process with plenty of invariant metrics.",
                "If the spurious attributes are domain labels, the conditional independence in Theorem 1 reduces to the conditional independence discussed in (Liu et al., 2015; Hu et al., 2020; Mahajan et al., 2021), while they do not explore its correlation with the OOD generalization."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "71d2bf15ed725a53cb78a4dd883ff0a9fb983890",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-06687",
                    "DOI": "10.48550/arXiv.2207.06687",
                    "CorpusId": 250526316
                },
                "corpusId": 250526316,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/71d2bf15ed725a53cb78a4dd883ff0a9fb983890",
                "title": "Improved OOD Generalization via Conditional Invariant Regularizer",
                "abstract": "Recently, generalization on out-of-distribution (OOD) data with correlation shift has attracted great attention. The correlation shift is caused by the spurious attributes that correlate to the class label, as the correlation between them may vary in training and test data. For such a problem, we show that given the class label, the conditionally independent models of spurious attributes are OOD generalizable. Based on this, a metric Conditional Spurious Variation (CSV) which controls OOD generalization error, is proposed to measure such conditional independence. To improve the OOD generalization, we regularize the training process with the proposed CSV. Under mild assumptions, our training objective can be formulated as a nonconvex-concave mini-max problem. An algorithm with provable convergence rate is proposed to solve the problem. Extensive empirical results verify our algorithm\u2019s e\ufb03cacy in improving OOD generalization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3541183",
                        "name": "Mingyang Yi"
                    },
                    {
                        "authorId": "144570917",
                        "name": "Ruoyu Wang"
                    },
                    {
                        "authorId": "2136769001",
                        "name": "Jiacheng Sun"
                    },
                    {
                        "authorId": "7718952",
                        "name": "Zhenguo Li"
                    },
                    {
                        "authorId": "2249674",
                        "name": "Zhi-Ming Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although using contrastive learning to improve OOD generalization is not new in the literature (Dou et al., 2019; Mahajan et al., 2021; Zhang et al., 2022), previous methods cannot yield OOD guarantees in graph circumstances due to the highly non-linearity and the unavailability of domain labels E.",
                "Besides, traditional approaches to tackle OOD generalization also include Domain Adaption, Transfer Learning and Domain Generalization(Rojas-Carulla et al., 2018; Chuang et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2018b; Dou et al., 2019; Mahajan et al., 2021; Wang et al., 2021), which aim to learn the class conditional invariant representation shared across source domain and target domain.",
                "Moreover, the unavailability of E prevents the direct usage of E in enforcing the independence that is often adopted by previous objectives (Arjovsky et al., 2019; Krueger et al., 2021; Ahuja et al., 2021; Sagawa* et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Dou et al., 2019; Mahajan et al., 2021), making the identification of Gc more challenging.",
                "\u2026methods require environment labels that are however expensive to obtain in graphs, which limits their applications to graphs (Arjovsky et al., 2019; Krueger et al., 2021; Ahuja et al., 2021; Sagawa* et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Dou et al., 2019; Mahajan et al., 2021).",
                "Moreover, most existing methods require environment labels that are however expensive to obtain in graphs, which limits their applications to graphs (Arjovsky et al., 2019; Krueger et al., 2021; Ahuja et al., 2021; Sagawa* et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Dou et al., 2019; Mahajan et al., 2021).",
                "On Euclidean data, Invariant Learning (Arjovsky et al., 2019; Creager et al., 2021; Ahuja et al., 2021), Group Distributionally Robust Optimization (GroupDro) (Krueger et al., 2021; Sagawa* et al., 2020; Zhang et al., 2022), Domain Adaption (DA) and Domain Generalization (DG) (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2018b; Dou et al., 2019; Mahajan et al., 2021; Wang et al., 2021) are three widely adopted approaches to enable OOD generalization.",
                ", 2022), Domain Adaption (DA) and Domain Generalization (DG) (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2018b; Dou et al., 2019; Mahajan et al., 2021; Wang et al., 2021) are three widely adopted approaches to enable OOD generalization.",
                "\u2026of E in enforcing the independence that is often adopted by previous objectives (Arjovsky et al., 2019; Krueger et al., 2021; Ahuja et al., 2021; Sagawa* et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Dou et al., 2019; Mahajan et al., 2021), making the identification of Gc more challenging.",
                "\u2026Domain Generalization(Rojas-Carulla et al., 2018; Chuang et al., 2020; Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2018b; Dou et al., 2019; Mahajan et al., 2021; Wang et al., 2021), which aim to learn the class conditional invariant representation shared across source domain and target\u2026",
                ", 2016) N/A R Yes N/A MatchDG (Mahajan et al., 2021) N/A R Yes FIIF GroupDro (Sagawa* et al.",
                "\u2026et al., 2021; Sagawa* et al., 2020; Zhang et al., 2022), Domain Adaption (DA) and Domain Generalization (DG) (Ganin et al., 2016; Sun & Saenko, 2016; Li et al., 2018b; Dou et al., 2019; Mahajan et al., 2021; Wang et al., 2021) are three widely adopted approaches to enable OOD generalization."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7c15ee1018a87708815c359d920820d8742dd3c8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-05441",
                    "CorpusId": 246823193
                },
                "corpusId": 246823193,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7c15ee1018a87708815c359d920820d8742dd3c8",
                "title": "Invariance Principle Meets Out-of-Distribution Generalization on Graphs",
                "abstract": "Despite recent success in using the invariance principle for out-of-distribution (OOD) generalization on Euclidean data (e.g., images), studies on graph data are still limited. Different from images, the complex nature of graphs poses unique challenges to adopting the invariance principle. In particular, distribution shifts on graphs can appear in a variety of forms such as attributes and structures, making it dif\ufb01cult to identify the invariance. Moreover, domain or environment partitions, which are often required by OOD methods on Euclidean data, could be highly expensive to obtain for graphs. To bridge this gap, we propose a new framework to capture the invariance of graphs for guaranteed OOD generalization under various distribution shifts. Speci\ufb01cally, we characterize potential distribution shifts on graphs with causal models, concluding that OOD generalization on graphs is achievable when models focus only on subgraphs containing the most information about the causes of labels. Accordingly, we propose an information-theoretic objective to extract the desired subgraphs that maximally preserve the invariant intra-class information. Learning with these subgraphs is immune to distribution shifts. Extensive experiments on both synthetic and real-world datasets, including a challenging setting in AI-aided drug discovery, validate the su-perior OOD generalization ability of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108962670",
                        "name": "Yongqiang Chen"
                    },
                    {
                        "authorId": "2109116068",
                        "name": "Yonggang Zhang"
                    },
                    {
                        "authorId": "50841357",
                        "name": "Han Yang"
                    },
                    {
                        "authorId": "47737190",
                        "name": "Kaili Ma"
                    },
                    {
                        "authorId": "2051756680",
                        "name": "Binghui Xie"
                    },
                    {
                        "authorId": "121698214",
                        "name": "Tongliang Liu"
                    },
                    {
                        "authorId": "2153287285",
                        "name": "Bo Han"
                    },
                    {
                        "authorId": "1717691",
                        "name": "James Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Given a dataset with auxiliary attributes and their relationship with the target label, CACM constrains the model\u2019s representation to obey the conditional independence constraints satisfied by causal features of the label, generalizing past work on causality-based regularization [10, 14, 15] to multi-attribute shifts.",
                "Partly because advances in representation learning for DG [8, 9, 10, 7, 11, 12] have focused on either one of the shifts, these studies find that performance of state-of-the-art DG algorithms are not consistent across different shifts: algorithms performing well on datasets with one kind of shift fail on datasets with another kind of shift.",
                "Our canonical multi-attribute graph generalizes the DG graph from [10] that considered an Independent domain/environment as the only attribute.",
                "We evaluate two constraints motivated by DG literature [10]: unconditional Xc \u22a5\u22a5 A|E, and conditional on label Xc \u22a5\u22a5 A|Y,E.",
                "We utilize a strategy from past work [10, 14] to use graph structure of the underlying data-generating process (DGP)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "22dd538e399aaf5d6cfda1341877cdb4396784ee",
                "externalIds": {
                    "CorpusId": 252781553
                },
                "corpusId": 252781553,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/22dd538e399aaf5d6cfda1341877cdb4396784ee",
                "title": "M ODELING THE D ATA -G ENERATING P ROCESS IS N ECESSARY FOR O UT - OF -D ISTRIBUTION G ENERALIZATION",
                "abstract": "Recent empirical studies on domain generalization (DG) have shown that DG algorithms that perform well on some distribution shifts fail on others, and no state-of-the-art DG algorithm performs consistently well on all shifts. Moreover, real-world data often has multiple distribution shifts over different attributes; hence we introduce multi -attribute distribution shift datasets and \ufb01nd that the accuracy of existing DG algorithms falls even further. To explain these results, we provide a formal characterization of generalization under multi-attribute shifts using a canonical causal graph. Based on the relationship between spurious attributes and the classi\ufb01cation label, we obtain realizations of the canonical causal graph that characterize common distribution shifts and show that each shift entails different independence constraints over observed variables. As a result, we prove that any algorithm based on a single, \ufb01xed constraint cannot work well across all shifts, providing theoretical evidence for mixed empirical results on DG algorithms. Based on this insight, we develop Causally Adaptive Constraint Minimization (CACM) , an algorithm that uses knowledge about the data-generating process to adaptively identify and apply the correct independence constraints for regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds datasets, covering binary and multi-valued attributes and labels, show that adaptive dataset-dependent constraints lead to the highest accuracy on unseen domains whereas incorrect constraints fail to do so. Our results demonstrate the importance of modeling the causal relationships inherent in the data-generating process.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2084554148",
                        "name": "Jivat Neet Kaur"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "36165bf51cd9ac2637185c73a037e4f7b5bb25fe",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08029",
                    "CorpusId": 246063665
                },
                "corpusId": 246063665,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/36165bf51cd9ac2637185c73a037e4f7b5bb25fe",
                "title": "Domain Generalization via Frequency-based Feature Disentanglement and Interaction",
                "abstract": "Data out-of-distribution is a meta-challenge for all statistical learning algorithms that strongly rely on the i.i.d. assumption. It leads to unavoidable labor costs and confidence crises in realistic applications. For that, domain generalization aims at mining domain-irrelevant knowledge from multiple source domains that can generalize to unseen target domains with unknown distributions. In this paper, leveraging the image frequency domain, we uniquely work with two key observations: (i) the high-frequency information of images depict object edge structure, which is naturally consistent across different domains, and (ii) the low-frequency component retains object smooth structure but are much more domain-specific. Motivated by these insights, we introduce (i) an encoder-decoder structure for high-frequency and low-frequency feature disentangling, (ii) an information interaction mechanism that ensures helpful knowledge from both two parts can cooperate effectively, and (iii) a novel data augmentation technique that works on the frequency domain for encouraging robustness of the network. The proposed method obtains state-of-the-art results on three widely used domain generalization benchmarks (Digit-DG, Office-Home, and PACS).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115891607",
                        "name": "Jing-Yi Wang"
                    },
                    {
                        "authorId": "1557389697",
                        "name": "Ruoyi Du"
                    },
                    {
                        "authorId": "67146777",
                        "name": "Dongliang Chang"
                    },
                    {
                        "authorId": "2118104867",
                        "name": "Zhanyu Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For instance, Ahmed et al. (2021) proposes to match the output distribution spaces from different domains via some divergence, while a recent work (Mahajan et al., 2021) also leverages a matching-based algorithm that resorts to shared representations of cross-domain inputs from the same object.",
                "(2021) proposes to match the output distribution spaces from different domains via some divergence, while a recent work (Mahajan et al., 2021) also leverages a matching-based algorithm that resorts to shared representations of cross-domain inputs from the same object."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f8c7c4f1e6f86c63d9d5fecdb1ba4c7bf3fbd9e4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-02466",
                    "CorpusId": 247058892
                },
                "corpusId": 247058892,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f8c7c4f1e6f86c63d9d5fecdb1ba4c7bf3fbd9e4",
                "title": "Towards Distribution Shift of Node-Level Prediction on Graphs: An Invariance Perspective",
                "abstract": "There is increasing evidence suggesting neural networks\u2019 sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given the two-fold fundamental challenges: 1) the inter-connection among nodes in one graph, which induces non-IID generation of data points even under the same environment, and 2) the structural information in the input graph, which is also informative for prediction. In this paper, we formulate the OOD problem on graphs and develop a new domain-invariant learning approach, named Explore-to-Extrapolate Risk Minimization (EERM), that facilitates GNNs to leverage invariance principles for prediction. EERM resorts to multiple context explorers (specified as graph structure editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node-level prediction. We prove the validity of our method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51171144",
                        "name": "Qitian Wu"
                    },
                    {
                        "authorId": "35466544",
                        "name": "Hengrui Zhang"
                    },
                    {
                        "authorId": "3063894",
                        "name": "Junchi Yan"
                    },
                    {
                        "authorId": "2242717",
                        "name": "D. Wipf"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also compare our proposed method with a variety of state-of-the-art domain generalization methods, including DeepAll [59], Jigen [3], CCSA [23], MMD-AAE [16] , CrossGrad [35], DDAIG [60], L2A-OT [59], ATSRL [51], MetaReg [2] , Epi-FCR [15], MMLD [21], CSD [29], InfoDrop [37], MASF [7], Mixstyle [61], EISNet [44], MDGH [20], RSC [11] and FACT [49]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c52dbe1daf262504007e251cd91ba87b45f0261",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-02803",
                    "DOI": "10.48550/arXiv.2208.02803",
                    "CorpusId": 251320593
                },
                "corpusId": 251320593,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0c52dbe1daf262504007e251cd91ba87b45f0261",
                "title": "Implicit Semantic Augmentation for Distance Metric Learning in Domain Generalization",
                "abstract": "Domain generalization (DG) aims to learn a model on one or more different but related source domains that could be generalized into an unseen target domain. Existing DG methods try to prompt the diversity of source domains for the model\u2019s generalization ability, while they may have to introduce auxiliary networks or striking computational costs. On the contrary, this work applies the implicit semantic augmentation in feature space to capture the diversity of source domains. Concretely, an additional loss function of distance metric learning (DML) is included to optimize the local geometry of data distribution. Besides, the logits from cross entropy loss with infinite augmentations is adopted as input features for the DML loss in lieu of the deep features. We also provide a theoretical analysis to show that the logits can approximate the distances defined on original features well. Further, we provide an in-depth analysis of the mechanism and rational behind our approach, which gives us a better understanding of why leverage logits in lieu of features can help domain generalization. The proposed DML loss with the implicit augmentation is incorporated into a recent DG method, that is, Fourier Augmented Co-Teacher framework (FACT). Meanwhile, our method also can be easily plugged into various DG methods. Extensive experiments on three benchmarks (Digits-DG, PACS and Office-Home) have demonstrated that the proposed method is able to achieve the state-of-the-art performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146057978",
                        "name": "Meng Wang"
                    },
                    {
                        "authorId": "2141135369",
                        "name": "Jia Yuna"
                    },
                    {
                        "authorId": "145303057",
                        "name": "Qi Qian"
                    },
                    {
                        "authorId": "2051262469",
                        "name": "Zhibin Wang"
                    },
                    {
                        "authorId": "1706574",
                        "name": "Hao Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[51], we introduce the causal domain features in this work to learn the domain-independent representation and reduce the domain gap, as shown in Fig."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c0ea45c58c502575633ab9eb6501c277df14446c",
                "externalIds": {
                    "DBLP": "journals/tgrs/ZhangXCZM22",
                    "DOI": "10.1109/TGRS.2022.3170316",
                    "CorpusId": 248698807
                },
                "corpusId": 248698807,
                "publicationVenue": {
                    "id": "70628d6a-97aa-4571-9701-bc0eb3989c32",
                    "name": "IEEE Transactions on Geoscience and Remote Sensing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Geosci Remote Sens"
                    ],
                    "issn": "0196-2892",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=36",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c0ea45c58c502575633ab9eb6501c277df14446c",
                "title": "Unsupervised SAR and Optical Image Matching Using Siamese Domain Adaptation",
                "abstract": "Due to its highly complementary information about remote sensing, synthetic aperture radar (SAR) and optical imagery matching have drawn much attention in recent years. Compared with traditional methods, deep learning-based SAR-optical image matching models largely rely on supervision with ground truths, where the matching accuracy suffers because of unseen image domains. To mitigate loads in burdensome labeling tasks, transferring deep learning models trained with annotated source domains to nonannotated target domains has attracted great concern. Due to the domain gap, the difference between the source and target domains is likely to deteriorate the matching accuracy on target data if the training process is directly conducted without proper domain adaptation (DA). In this research, a Siamese DA (SDA) approach with a combined loss function is developed in the context of multimodality image matching. Then, a novel rotation/scale-invariant transformation module with regression modules is designed to extract rotation/scale-equivariant features. Finally, the causal inference-based self-learning method and the multiresolution histogram matching approach are employed to enhance the unsupervised matching performance. Experimental results on the RadarSat/Planet dataset and the Sentinel-1/2 dataset demonstrate that the developed model can achieve competitive matching performance with a low overlap ratio between domains and little data labeling. By alleviating the domain discrepancy, the developed model drastically reduces the average L2 score of the unsupervised matching from 9.576 to 0.658, while the less-than-one-pixel matching error rate is enhanced from 66.3% to 90.6%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156120851",
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "authorId": "1771851",
                        "name": "Yuelei Xu"
                    },
                    {
                        "authorId": "2057040078",
                        "name": "Qi Cui"
                    },
                    {
                        "authorId": "2118410888",
                        "name": "Qing Zhou"
                    },
                    {
                        "authorId": "2115502775",
                        "name": "Linhua Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[15] Divyat Mahajan, Shruti Tople, and Amit Sharma.",
                "From OOD generalization literature on images [32, 24], we may use one of these 192 methods to learn Xc, Zc from observed data: regularization [15, 13], weighting [19, 29], or data 193 augmentation [32]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "46636900019d72494f220427135e3e81900ec90f",
                "externalIds": {
                    "CorpusId": 253047673
                },
                "corpusId": 253047673,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/46636900019d72494f220427135e3e81900ec90f",
                "title": "Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Systems",
                "abstract": "Given a user\u2019s input text, text-matching recommender systems output relevant items 1 by comparing the input text to available items\u2019 description, such as product-to- 2 product recommendation on e-commerce platforms. As users\u2019 interests and item 3 inventory are expected to change, it is important for a text-matching system to 4 generalize to data shifts, a task known as out-of-distribution (OOD) generalization. 5 However, we find that the popular approach of fine-tuning a large, base language 6 model on paired item relevance data (e.g., user clicks) can be counter-productive 7 for OOD generalization. For a product recommendation task, fine-tuning obtains 8 worse accuracy than the base model when recommending items in a new category 9 or for a future time period. To explain this generalization failure, we consider 10 an intervention-based importance metric, which shows that a fine-tuned model 11 captures spurious correlations and fails to learn the causal features that determine 12 the relevance between any two text inputs. Moreover, standard methods for causal 13 regularization do not apply in this setting, because unlike in images, there exist 14 no universally spurious features in a text-matching task (the same token may 15 be spurious or causal depending on the text it is being matched to). For OOD 16 generalization on text inputs, therefore, we highlight a different goal: avoiding 17 high importance scores for certain features. We do so using an intervention- 18 based regularizer that constraints the causal effect of any token on the model\u2019s 19 relevance score to be similar to the base model. Results on Amazon product and 3 20 question recommendation datasets show that our proposed regularizer improves 21 generalization for both in-distribution and OOD evaluation, especially in difficult 22 scenarios when the base model is not accurate. 23",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48691300",
                        "name": "Parikshit Bansal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "110 Benchmarking OOD algorithms, especially those that proposed to view distribution shift from a 111 causal perspective [6, 1, 3, 10, 11, 8] since we have mimicked many of their assumptions about the 112 underlying data generating process in our dataset."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e7058effb7da07001f781b60bdbad33114f5f132",
                "externalIds": {
                    "CorpusId": 253065352
                },
                "corpusId": 253065352,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e7058effb7da07001f781b60bdbad33114f5f132",
                "title": "Evaluating the Impact of Geometric and Statistical Skews on Out-Of-Distribution Generalization",
                "abstract": "Out-of-distribution (OOD) or domain generalization is the problem of generalizing to unseen distributions. Recent work suggests that the marginal difficulty of generalizing to OOD over in-distribution data (OOD-ID generalization gap) is due to spurious correlations, which arise due to statistical and geometric skews",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2174176979",
                        "name": "Aengus Lynch"
                    },
                    {
                        "authorId": "66914903",
                        "name": "Jean Kaddour"
                    },
                    {
                        "authorId": "2193244036",
                        "name": "G. Dovonon"
                    },
                    {
                        "authorId": "2140070169",
                        "name": "Ricardo M. A. Silva"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another line of work tries to learn stable/causal features across domains by learning disentangled features (Mahajan et al., 2021; Zhang et al., 2021b; Li et al., 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ef13a4518c7dfb46a35b79d11f87d80999ce6f49",
                "externalIds": {
                    "DBLP": "conf/icml/ChuJZWWZM22",
                    "CorpusId": 250340736
                },
                "corpusId": 250340736,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ef13a4518c7dfb46a35b79d11f87d80999ce6f49",
                "title": "DNA: Domain Generalization with Diversified Neural Averaging",
                "abstract": "The inaccessibility of the target domain data causes domain generalization (DG) methods prone to forget target discriminative features, and challenges the pervasive theme in existing literature in pursuing a single classifier with an ideal joint risk. In contrast, this paper inves-tigates model misspecification and attempts to bridge DG with classifier ensemble theoretically and methodologically. By introducing a pruned Jensen-Shannon (PJS) loss, we show that the target square-root risk w.r.t. the PJS loss of the \u03c1 -ensemble (the averaged classifier weighted by a quasi-posterior \u03c1 ) is bounded by the averaged source square-root risk of the Gibbs classifiers. We derive a tighter bound by enforcing a positive principled diversity measure of the classifiers. We give a PAC-Bayes upper bound on the target square-root risk of the \u03c1 -ensemble. Methodologically, we propose a diversified neural averaging (DNA) method for DG, which optimizes the proposed PAC-Bayes bound approximately. The DNA method samples Gibbs classifiers transversely and longitudinally by simultaneously considering the dropout variational family and optimization trajectory. The \u03c1 -ensemble is approximated by averaging the longitudinal weights in a single run with dropout shut down, ensuring a fast ensemble with low computational overhead. Empirically, the proposed DNA method achieves the state-of-the-art classification performance on standard DG benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056598391",
                        "name": "Xu Chu"
                    },
                    {
                        "authorId": "2175350283",
                        "name": "Yujie Jin"
                    },
                    {
                        "authorId": "2156154955",
                        "name": "Wenwu Zhu"
                    },
                    {
                        "authorId": "2108738446",
                        "name": "Yasha Wang"
                    },
                    {
                        "authorId": "2153687490",
                        "name": "Xin Wang"
                    },
                    {
                        "authorId": "2437353",
                        "name": "Shanghang Zhang"
                    },
                    {
                        "authorId": "2175277699",
                        "name": "Hong Mei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This set may include both spurious and robust features if these distributions vary across domains [18].",
                "A related decomposition has been used in the context of causal approaches to robust learning [16, 17, 18], where prior knowledge is used to map all non-causal features to spurious features, treating only causal features as predictive."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "484f72eab7be9ad3276ebd07b84794eae624b024",
                "externalIds": {
                    "CorpusId": 255300274
                },
                "corpusId": 255300274,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/484f72eab7be9ad3276ebd07b84794eae624b024",
                "title": "Out-of-Distribution Robustness via Targeted Augmentations",
                "abstract": "Machine learning systems often must generalize across real-world domains with different data distributions. These distributions change along multiple factors: while some of these factors are spuriously correlated with the label, others are robustly predictive. For example, in wildlife conservation, animal classification models must generalize across camera deployments, with cameras\u2019 background distributions varying along both spurious factors (e.g., low-level background variations) and robustly predictive factors (e.g., the background\u2019s habitat type). In this work, we show that data augmentations offer significant out-of-distribution gains when they are carefully designed to randomize only spurious variations, while preserving the robust variations. On I W ILD C AM 2020- WILDS and C AMELYON 17-WILDS , two domain generalization datasets, targeted augmentations outperform the previous state-of-the-art by 3.2% and 14.4% respectively. Our results suggest that data augmentations, when targeted to selectively randomize spurious cross-domain variations, can be an effective route to real-world out-of-distribution robustness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8687620",
                        "name": "Irena Gao"
                    },
                    {
                        "authorId": "2389237",
                        "name": "Shiori Sagawa"
                    },
                    {
                        "authorId": "2572525",
                        "name": "Pang Wei Koh"
                    },
                    {
                        "authorId": "2117567142",
                        "name": "Tatsunori Hashimoto"
                    },
                    {
                        "authorId": "145419642",
                        "name": "Percy Liang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[14] Divyat Mahajan, Shruti Tople, and Amit Sharma.",
                "Recent work uses multiple sufficiently different environments to generalize to unseen test data that lies in the support of the given environments or subgroups [2, 12, 13, 14, 15, 16, 17, 18, 19].",
                "Methods for invariant representation learning [2, 14, 15, 29] typically require data from multiple different environments.",
                "Methods based on conditional distribution matching [14, 29] build representations that are conditionally independent of the environment variable given the label."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f8b9f31ecab07e06450eecbc391ce4736aa81e3c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-00520",
                    "CorpusId": 235694249
                },
                "corpusId": 235694249,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f8b9f31ecab07e06450eecbc391ce4736aa81e3c",
                "title": "Predictive Modeling in the Presence of Nuisance-Induced Spurious Correlations",
                "abstract": "In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is the nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We first define the nuisance-varying family, a set of distributions that differ only in the nuisance-label relationship. We then introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance within the set on every distribution in the nuisance-varying family. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "81522899",
                        "name": "A. Puli"
                    },
                    {
                        "authorId": "2108109948",
                        "name": "Lily H. Zhang"
                    },
                    {
                        "authorId": "4682692",
                        "name": "E. Oermann"
                    },
                    {
                        "authorId": "2615814",
                        "name": "R. Ranganath"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another line of work [7, 8, 9, 14, 15] considers domain generalization for unstructured data using specifically designed causal graphs as a tool to model the distribution shift and guide the learning method, which is more task targeted."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c67a97ee5624c9ef1200029cf45da660fcaef961",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-01876",
                    "CorpusId": 235731456
                },
                "corpusId": 235731456,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c67a97ee5624c9ef1200029cf45da660fcaef961",
                "title": "Causally Invariant Predictor with Shift-Robustness",
                "abstract": "This paper proposes an invariant causal predictor that is robust to distribution shift across domains and maximally reserves the transferable invariant information. Based on a disentangled causal factorization, we formulate the distribution shift as soft interventions in the system, which covers a wide range of cases for distribution shift as we do not make prior specifications on the causal structure or the intervened variables. Instead of imposing regularizations to constrain the invariance of the predictor, we propose to predict by the intervened conditional expectation based on the do-operator and then prove that it is invariant across domains. More importantly, we prove that the proposed predictor is the robust predictor that minimizes the worstcase quadratic loss among the distributions of all domains. For empirical learning, we propose an intuitive and flexible estimating method based on data regeneration and present a local causal discovery procedure to guide the regeneration step. The key idea is to regenerate data such that the regenerated distribution is compatible with the intervened graph, which allows us to incorporate standard supervised learning methods with the regenerated data. Experimental results on both synthetic and real data demonstrate the efficacy of our predictor in improving the predictive accuracy and robustness across domains.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152198872",
                        "name": "Xiangyu Zheng"
                    },
                    {
                        "authorId": "8283163",
                        "name": "Xinwei Sun"
                    },
                    {
                        "authorId": "2154939268",
                        "name": "Wei Chen"
                    },
                    {
                        "authorId": "2110264337",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "give a causal interpretation to DG and generalize based on invariant representations [13]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "19d6faed3a5fb23c8288420bd79d98c2bfa524a3",
                "externalIds": {
                    "DBLP": "conf/miccai/CaiZCLWZY21",
                    "DOI": "10.1007/978-3-030-87237-3_27",
                    "CorpusId": 235747216
                },
                "corpusId": 235747216,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/19d6faed3a5fb23c8288420bd79d98c2bfa524a3",
                "title": "Generalizing Nucleus Recognition Model in Multi-source Ki67 Immunohistochemistry Stained Images via Domain-Specific Pruning",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "92652202",
                        "name": "Jiatong Cai"
                    },
                    {
                        "authorId": "50812118",
                        "name": "Chenglu Zhu"
                    },
                    {
                        "authorId": "2052274907",
                        "name": "C. Cui"
                    },
                    {
                        "authorId": "2115564011",
                        "name": "Honglin Li"
                    },
                    {
                        "authorId": "2143626337",
                        "name": "Tong Wu"
                    },
                    {
                        "authorId": "6022603",
                        "name": "Shichuan Zhang"
                    },
                    {
                        "authorId": "2155557627",
                        "name": "Lin Yang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1d272c5f542f9ab9e3178a50be5e651a7b37f181",
                "externalIds": {
                    "CorpusId": 236090250
                },
                "corpusId": 236090250,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1d272c5f542f9ab9e3178a50be5e651a7b37f181",
                "title": "Domain Generalization in Vision: A Survey",
                "abstract": "Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Since first introduced in 2011, research in DG has made great progresses. In particular, intensive research in this topic has led to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, just to name a few; and has covered various vision applications such as object recognition, segmentation, action recognition, and person re-identification. In this paper, for the first time a comprehensive literature review is provided to summarize the developments in DG for computer vision over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other research fields like domain adaptation and transfer learning. Second, we conduct a thorough review into existing methods and present a categorization based on their methodologies and motivations. Finally, we conclude this survey with insights and discussions on future research directions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9368124",
                        "name": "Kaiyang Zhou"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "143970608",
                        "name": "Y. Qiao"
                    },
                    {
                        "authorId": "145406421",
                        "name": "T. Xiang"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2019), there have been several interesting works \u2014 (Teney et al., 2020; Krueger et al., 2020; Ahuja et al., 2020; Chang et al., 2020; Mahajan et al., 2020) is an incomplete representative list \u2014 that build new methods inpired from IRM to address the OOD generalization problem.",
                "\u2026from Arjovsky et al. (2019), there have been several interesting works \u2014 (Teney et al., 2020; Krueger et al., 2020; Ahuja et al., 2020; Chang et al., 2020; Mahajan et al., 2020) is an incomplete representative list \u2014 that build new methods inpired from IRM to address the OOD generalization problem."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "863c1016f8a929e3d475da47d82d622299b98cd6",
                "externalIds": {
                    "CorpusId": 236923519
                },
                "corpusId": 236923519,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/863c1016f8a929e3d475da47d82d622299b98cd6",
                "title": "A SAMPLE COMPLEXITY PERSPECTIVE",
                "abstract": "Recently, invariant risk minimization (IRM) was proposed as a promising solution to address out-of-distribution (OOD) generalization. However, it is unclear when IRM should be preferred over the widely-employed empirical risk minimization (ERM) framework. In this work, we analyze both these frameworks from the perspective of sample complexity, thus taking a firm step towards answering this important question. We find that depending on the type of data generation mechanism, the two approaches might have very different finite sample and asymptotic behavior. For example, in the covariate shift setting we see that the two approaches not only arrive at the same asymptotic solution, but also have similar finite sample behavior with no clear winner. For other distribution shifts such as those involving confounders or anti-causal variables, however, the two approaches arrive at different asymptotic solutions where IRM is guaranteed to be close to the desired OOD solutions in the finite sample regime for polynomial generative models, while ERM is biased even asymptotically. We further investigate how different factors \u2014 the number of environments, complexity of the model, and IRM penalty weight \u2014 impact the sample complexity of IRM in relation to its distance from the OOD solutions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3048927",
                        "name": "Kartik Ahuja"
                    },
                    {
                        "authorId": "2152810321",
                        "name": "Jun Wang"
                    },
                    {
                        "authorId": "2145784",
                        "name": "Amit Dhurandhar"
                    },
                    {
                        "authorId": "145455859",
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "authorId": "1712865",
                        "name": "K. Varshney"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[35] provide a causal interpretation of domain generalization, which is similar to ours in letting input features be generated by domain-specific and domaininvariant factors/features."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "94a4fe59e4b23874188cf5eed412de1abe42a447",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-01438",
                    "CorpusId": 238259592
                },
                "corpusId": 238259592,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/94a4fe59e4b23874188cf5eed412de1abe42a447",
                "title": "Learning Domain-Invariant Relationship with Instrumental Variable for Domain Generalization",
                "abstract": "\u2014Domain generalization (DG) aims to learn from multiple source domains a model that generalizes well on unseen target domains. Existing methods mainly learn input feature representations with invariant marginal distribution, while the invariance of the conditional distribution is more essential for unknown domain generalization. This paper proposes an instrumental variable-based approach to learn the domain-invariant relationship between input features and labels contained in the conditional distribution. Interestingly, with a causal view on the data generating process, we \ufb01nd that the input features of one domain are valid instrumental variables for other domains. In- spired by this \ufb01nding, we design a simple yet effective framework to learn the Domain-invariant Relationship with Instrumental VariablE (DRIVE) via a two-stage IV method. Speci\ufb01cally, it \ufb01rst learns the conditional distribution of input features of one domain given input features of another domain, and then it estimates the domain-invariant relationship by predicting labels with the learned conditional distribution. Simulation experiments show the proposed method accurately captures the domain-invariant relationship. Extensive experiments on several datasets consis-tently demonstrate that DRIVE yields state-of-the-art results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38511927",
                        "name": "Junkun Yuan"
                    },
                    {
                        "authorId": "2141122566",
                        "name": "Xu Ma"
                    },
                    {
                        "authorId": "33870528",
                        "name": "Kun Kuang"
                    },
                    {
                        "authorId": "50824401",
                        "name": "Ruoxuan Xiong"
                    },
                    {
                        "authorId": "29393235",
                        "name": "Mingming Gong"
                    },
                    {
                        "authorId": "2940227",
                        "name": "Lanfen Lin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To overcome the inequality between training and test estimation, many works in domain generalization and adaptation have been proposed (Arjovsky et al., 2019; Zhao et al., 2020; Mahajan et al., 2021; Zhang et al., 2020; Wang et al., 2020b), but casting theses methods into FL is not trivial."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cb771da9e75e3a0854b24b40bc6946d6cc7e0412",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-09974",
                    "CorpusId": 239024310
                },
                "corpusId": 239024310,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cb771da9e75e3a0854b24b40bc6946d6cc7e0412",
                "title": "TsmoBN: Interventional Generalization for Unseen Clients in Federated Learning",
                "abstract": "Generalizing federated learning (FL) models to unseen clients with non-iid data is a crucial topic, yet unsolved so far. In this work, we propose to tackle this problem from a novel causal perspective. Specifically, we form a training structural causal model (SCM) to explain the challenges of model generalization in a distributed learning paradigm. Based on this, we present a simple yet effective method using test-specific and momentum tracked batch normalization (TsmoBN) to generalize FL models to testing clients. We give a causal analysis by formulating another testing SCM and demonstrate that the key factor in TsmoBN is the test-specific statistics (i.e., mean and variance) of features. Such statistics can be seen as a surrogate variable for causal intervention. In addition, by considering generalization bounds in FL, we show that our TsmoBN method can reduce divergence between training and testing feature distributions, which achieves a lower generalization gap than standard model testing. Our extensive experimental evaluations demonstrate significant improvements for unseen client generalization on three datasets with various types of feature distributions and numbers of clients. It is worth noting that our proposed approach can be flexibly applied to different state-of-the-art federated learning algorithms and is orthogonal to existing domain generalization methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2050138741",
                        "name": "Meirui Jiang"
                    },
                    {
                        "authorId": "2108168094",
                        "name": "Xiaofei Zhang"
                    },
                    {
                        "authorId": "2065698865",
                        "name": "Michael Kamp"
                    },
                    {
                        "authorId": "2144456293",
                        "name": "Xiaoxiao Li"
                    },
                    {
                        "authorId": "35647880",
                        "name": "Q. Dou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It learns two match functions, one on the different source domains as per MatchDG, and the other on the augmented domains using Perfect-Match.",
                "Note that Perfect-Match is an ideal training algorithm that assumes knowledge of ground-truth matches across domains, and therefore cannot be applied in real-world settings.",
                "We follow the same procedure as in [33] for the OOD training and evaluation of methods.",
                "In comparison, algorithms like Perfect-Match [22, 33] that ideally capture only the stable features, perform well on both membership inference and attribute inference.",
                "However, in\nreal datasets, augmentations can also provide perfect matches, leading to the Hybrid approach using both the MatchDG and augmented Perfect-Match.",
                "To evaluate on a more practical scenario, we use the dataset of Chest X-rays images from [33], that comprises of data from different hospital systems: NIH [54], ChexPert [24] and RSNA [1].",
                "There are also works that are inspired by causality [4, 17, 32, 33, 40, 43] and take different interpretations towards a causal framework for domain generalization.",
                "We consider two types of ML models: standard empirical risk minimizers, and state-of-theart domain generalization learning algorithms that claim to learn stable features for OOD generalization [4,33,41].",
                "Perfect-Match [22, 33].",
                "For simulated datasets like Rotated-MNIST and Fashion-MNIST, we know the groundtruth matches for each input and use that to evaluate the mean rank metric (the same matches are not provided to the DG methods, except to the ideal Perfect-Match method).",
                "Improving upon the class-based regularization above, matching methods based on causal inference also been proposed that aim to match representations of inputs that share the base object [33].",
                "\u2022 Among ML algorithms, a method based on selfaugmentations (Perfect-Match [22,33]) provides the best privacy robustness.",
                "For the matching based methods (Random-Match, MatchDG, Perfect-Match), we use the final classification layer of the network as \u03c6 and for the matching loss regularizer (Eq 2 ).",
                "[51] show that causal learning methods [4,33] that aim to learn stable features across distributions alleviate membership privacy risks compared to associational models such as neural networks.",
                "Instead, we use the property described by causal domain generalization work [19, 33] that input images that share the base object have the same stable (or causal) features.",
                "ate whether a model has learned stable features or not [33].",
                "Here, the different images across domains share the same base causal object and provide access to true matches required for Perfect-Match approach."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f70aaa749c34b81962f5acb1c3a472cb98df15f8",
                "externalIds": {
                    "CorpusId": 231608928
                },
                "corpusId": 231608928,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f70aaa749c34b81962f5acb1c3a472cb98df15f8",
                "title": "Does Learning Stable Features Provide Privacy Benefits for Machine Learning Models?",
                "abstract": "Privacy attacks such as membership and attribute inference are a serious concern when using machine learning models, and more so when these models are used over data distributions different than their training distribution. As a defense, models that have better generalization properties, including domain generalization (DG) methods that aim to learn stable feature representations across distributions, have been theoretically shown to reduce risk to such privacy attacks. In this work, we investigate the connection between out-of-distribution (OOD) generalization, learning stable features and privacy attacks by performing a rigorous empirical study on two benchmark datasets (Rotated-MNIST and Fashion-MNIST) and a realworld healthcare dataset based on Chest X-ray images. We find that DG methods that rely on learning stable features in theory and indeed learn them in practice provide better privacy guarantees. However, not all state-of-the-art learning algorithms are able to learn the stable features, even though they are optimized for it. On the negative side, we observe that the relationship between OOD accuracy and privacy is not straightforward: a model with high OOD accuracy may also have high privacy risk. Thus, our results indicate the importance of learning stable features to mitigate membership and attribute inference attacks without degrading utility, and motivate the development of better learning algorithms that can learn stable features in practice. For the machine learning community, our work provides novel privacy-based metrics that can be used to measure stability of features learnt by a model.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2132509131",
                        "name": "Divyat Mahajan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other work includes causal inference, which is challenging or even impossible without strong assumptions that we do not use (Arjovsky et al., 2019; Mahajan et al., 2020), and adversarial robustness, where the goal is to build classifiers that are locally Lipschitz or smooth in a given radius around training data (Goodfellow et al.",
                "Other work includes causal inference, which is challenging or even impossible without strong assumptions that we do not use (Arjovsky et al., 2019; Mahajan et al., 2020), and adversarial robustness, where the goal is to build classifiers that are locally Lipschitz or smooth in a given radius around\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4a2126a94bdc9778b3dc92147f7f89f205cc79c4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-08485",
                    "MAG": "3105732517",
                    "CorpusId": 226976053
                },
                "corpusId": 226976053,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4a2126a94bdc9778b3dc92147f7f89f205cc79c4",
                "title": "Close Category Generalization for Out-of-Distribution Classification",
                "abstract": "Out-of-distribution generalization is a core challenge in machine learning. We introduce and propose a solution to a new type of out-of-distribution evaluation, which we call close category generalization. This task specifies how a classifier should extrapolate to unseen classes by considering a bi-criteria objective: (i) on in-distribution examples, output the correct label, and (ii) on out-of-distribution examples, output the label of the nearest neighbor in the training set. In addition to formalizing this problem, we present a new training algorithm to improve the close category generalization of neural networks. We compare to many baselines, including robust algorithms and out-of-distribution detection methods, and we show that our method has better or comparable close category generalization. Then, we investigate a related representation learning task, and we find that performing well on close category generalization correlates with learning a good representation of an unseen class and with finding a good initialization for few-shot learning. The code is available at https://github.com/yangarbiter/close-category-generalization",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2144159168",
                        "name": "Yao-Yuan Yang"
                    },
                    {
                        "authorId": "3125805",
                        "name": "Cyrus Rashtchian"
                    },
                    {
                        "authorId": "145124475",
                        "name": "R. Salakhutdinov"
                    },
                    {
                        "authorId": "38120884",
                        "name": "Kamalika Chaudhuri"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d6a9029c028272612cbbd2d72f4db0403a7d9749",
                "externalIds": {
                    "CorpusId": 263473485
                },
                "corpusId": 263473485,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d6a9029c028272612cbbd2d72f4db0403a7d9749",
                "title": "Medical Image Analysis",
                "abstract": "against style diversity as a pretrained model. Afterward, the pretrained network is further fine-tuned to the downstream tasks, e",
                "year": null,
                "authors": [
                    {
                        "authorId": "25586966",
                        "name": "Zheren Li"
                    },
                    {
                        "authorId": "2250925043",
                        "name": "Zhiming Cui"
                    },
                    {
                        "authorId": "7559893",
                        "name": "Lichi Zhang"
                    },
                    {
                        "authorId": "2151487856",
                        "name": "Sheng Wang"
                    },
                    {
                        "authorId": "2200529234",
                        "name": "Chenjin Lei"
                    },
                    {
                        "authorId": "2251555215",
                        "name": "Ouyang Xi"
                    },
                    {
                        "authorId": "2251292779",
                        "name": "Dongdong Chen"
                    },
                    {
                        "authorId": "2165644396",
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "authorId": "2252555525",
                        "name": "Chunling Liu"
                    },
                    {
                        "authorId": "2247752566",
                        "name": "Zaiyi Liu"
                    },
                    {
                        "authorId": "2251577745",
                        "name": "Yajia Gu"
                    },
                    {
                        "authorId": "2250496503",
                        "name": "Dinggang Shen"
                    },
                    {
                        "authorId": "2250684322",
                        "name": "Jie\u2010Zhi Cheng"
                    }
                ]
            }
        }
    ]
}