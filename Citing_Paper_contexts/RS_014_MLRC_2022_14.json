{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a34b44ca669223e8911f123236a91450afac8245",
                "externalIds": {
                    "DOI": "10.1016/j.eswa.2023.121364",
                    "CorpusId": 261538807
                },
                "corpusId": 261538807,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a34b44ca669223e8911f123236a91450afac8245",
                "title": "ROUGE-SEM: Better evaluation of summarization using ROUGE combined with semantics",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237825413",
                        "name": "Ming Zhang"
                    },
                    {
                        "authorId": "2237884715",
                        "name": "Chengzhang Li"
                    },
                    {
                        "authorId": "2237867821",
                        "name": "Meilin Wan"
                    },
                    {
                        "authorId": "2237903107",
                        "name": "Xuejun Zhang"
                    },
                    {
                        "authorId": "2238065712",
                        "name": "Qingwei Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0da13d3a8b5d80c62ade5aa5be1a7beba2c06468",
                "externalIds": {
                    "ACL": "2023.inlg-main.8",
                    "ArXiv": "2308.06488",
                    "DBLP": "journals/corr/abs-2308-06488",
                    "DOI": "10.48550/arXiv.2308.06488",
                    "CorpusId": 260887250
                },
                "corpusId": 260887250,
                "publicationVenue": {
                    "id": "8648a277-d0ec-4691-9eed-399b31ff9860",
                    "name": "International Conference on Natural Language Generation",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Nat Lang Gener",
                        "INLG"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1613"
                },
                "url": "https://www.semanticscholar.org/paper/0da13d3a8b5d80c62ade5aa5be1a7beba2c06468",
                "title": "Generating Faithful Text From a Knowledge Graph with Noisy Reference Text",
                "abstract": "Knowledge Graph (KG)-to-Text generation aims at generating fluent natural-language text that accurately represents the information of a given knowledge graph. While significant progress has been made in this task by exploiting the power of pre-trained language models (PLMs) with appropriate graph structure-aware modules, existing models still fall short of generating faithful text, especially when the ground-truth natural-language text contains additional information that is not present in the graph. In this paper, we develop a KG-to-text generation model that can generate faithful natural-language text from a given graph, in the presence of noisy reference text. Our framework incorporates two core ideas: Firstly, we utilize contrastive learning to enhance the model\u2019s ability to differentiate between faithful and hallucinated information in the text, thereby encouraging the decoder to generate text that aligns with the input graph. Secondly, we empower the decoder to control the level of hallucination in the generated text by employing a controllable text generation technique. We evaluate our model\u2019s performance through the standard quantitative metrics as well as a ChatGPT-based quantitative and qualitative analysis. Our evaluation demonstrates the superior performance of our model over state-of-the-art KG-to-text models on faithfulness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2466293",
                        "name": "Tahsina Hashem"
                    },
                    {
                        "authorId": "2154869314",
                        "name": "Weiqing Wang"
                    },
                    {
                        "authorId": "2129412",
                        "name": "D. Wijaya"
                    },
                    {
                        "authorId": "2152557920",
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "authorId": "152244300",
                        "name": "Yuan-Fang Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Dialogue summarization, a recently popular subfield of text summarization, has more challenging factual issues involved (Wang et al., 2022; Gao and Wan, 2022).",
                "The outputs of each system on the SAMSum test set are obtained from DialSummEval (Gao and Wan, 2022).",
                "DialSummEval: Revisiting summarization evaluation for dialogues."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8532a206424a33d9803752474d74b602ff8ea211",
                "externalIds": {
                    "DBLP": "conf/acl/00020SWH23",
                    "ArXiv": "2306.05119",
                    "ACL": "2023.acl-long.779",
                    "DOI": "10.48550/arXiv.2306.05119",
                    "CorpusId": 259108899
                },
                "corpusId": 259108899,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/8532a206424a33d9803752474d74b602ff8ea211",
                "title": "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework",
                "abstract": "Factuality is important to dialogue summarization. Factual error correction (FEC) of model-generated summaries is one way to improve factuality. Current FEC evaluation that relies on factuality metrics is not reliable and detailed enough. To address this problem, we are the first to manually annotate a FEC dataset for dialogue summarization containing 4000 items and propose FERRANTI, a fine-grained evaluation framework based on reference correction that automatically evaluates the performance of FEC models on different error categories. Using this evaluation framework, we conduct sufficient experiments with FEC approaches under a variety of settings and find the best training modes and significant differences in the performance of the existing approaches on different factual error categories.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "82340188",
                        "name": "Mingqi Gao"
                    },
                    {
                        "authorId": "117908148",
                        "name": "Xiaojun Wan"
                    },
                    {
                        "authorId": "2202087457",
                        "name": "Jia Su"
                    },
                    {
                        "authorId": "2108271253",
                        "name": "Zhefeng Wang"
                    },
                    {
                        "authorId": "2422046",
                        "name": "Baoxing Huai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7d29cc9cdeba6d2d338342d32abcedcc432e4aeb",
                "externalIds": {
                    "DBLP": "conf/acl/HuGDDF023",
                    "ArXiv": "2305.17529",
                    "ACL": "2023.acl-long.906",
                    "DOI": "10.48550/arXiv.2305.17529",
                    "CorpusId": 258959349
                },
                "corpusId": 258959349,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/7d29cc9cdeba6d2d338342d32abcedcc432e4aeb",
                "title": "MeetingBank: A Benchmark Dataset for Meeting Summarization",
                "abstract": "As the number of recorded meetings increases, it becomes increasingly important to utilize summarization technology to create useful summaries of these recordings. However, there is a crucial lack of annotated meeting corpora for developing this technology, as it can be hard to collect meetings, especially when the topics discussed are confidential. Furthermore, meeting summaries written by experienced writers are scarce, making it hard for abstractive summarizers to produce sensible output without a reliable reference. This lack of annotated corpora has hindered the development of meeting summarization technology. In this paper, we present MeetingBank, a new benchmark dataset of city council meetings over the past decade. MeetingBank is unique among other meeting corpora due to its divide-and-conquer approach, which involves dividing professionally written meeting minutes into shorter passages and aligning them with specific segments of the meeting. This breaks down the process of summarizing a lengthy meeting into smaller, more manageable tasks. The dataset provides a new testbed of various meeting summarization systems and also allows the public to gain insight into how council decisions are made. We make the collection, including meeting video links, transcripts, reference summaries, agenda, and other metadata, publicly available to facilitate the development of better meeting summarization techniques.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218546567",
                        "name": "Yebowen Hu"
                    },
                    {
                        "authorId": "2126496709",
                        "name": "Timothy Jeewun Ganter"
                    },
                    {
                        "authorId": "1787977",
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "authorId": "2075390842",
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "authorId": "1691260",
                        "name": "H. Foroosh"
                    },
                    {
                        "authorId": "144544919",
                        "name": "Fei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "410aae1073582b1bef762457fdd3a234517cebf4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-16548",
                    "ACL": "2023.acl-long.377",
                    "ArXiv": "2305.16548",
                    "DOI": "10.48550/arXiv.2305.16548",
                    "CorpusId": 258947225
                },
                "corpusId": 258947225,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/410aae1073582b1bef762457fdd3a234517cebf4",
                "title": "Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization",
                "abstract": "A series of datasets and models have been proposed for summaries generated for well-formatted documents such as news articles. Dialogue summaries, however, have been under explored. In this paper, we present the first dataset with fine-grained factual error annotations named DIASUMFACT. We define fine-grained factual error detection as a sentence-level multi-label classification problem, and weevaluate two state-of-the-art (SOTA) models on our dataset. Both models yield sub-optimal results, with a macro-averaged F1 score of around 0.25 over 6 error classes. We further propose an unsupervised model ENDERANKER via candidate ranking using pretrained encoder-decoder models. Our model performs on par with the SOTA models while requiring fewer resources. These observations confirm the challenges in detecting factual errors from dialogue summaries, which call for further studies, for which our dataset and results offer a solid foundation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2070268038",
                        "name": "Ron Zhu"
                    },
                    {
                        "authorId": "2149459181",
                        "name": "Jianzhong Qi"
                    },
                    {
                        "authorId": "1800564",
                        "name": "Jey Han Lau"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The DialSummEval (Gao and Wan, 2022) benchmark is a summarization evaluation benchmark created following the format of SummEval (Fabbri et al., 2021) for the domain of dialogue summarization.",
                "The DialSummEval (Gao and Wan, 2022) benchmark is a summarization evaluation benchmark created following the format of SummEval (Fabbri et al.",
                ", 2022) and DialSummEval (Gao and Wan, 2022) and uncover limitations that guide the design principles of the SUMMEDITS benchmark we build.",
                "Prior work (Kry\u015bci\u0144ski et al., 2020; Fabbri et al., 2021; Gao and Wan, 2022) has annotated corpora Human Perform.",
                "In this section we analyze two popular benchmarks for factual consistency detection in summarization: AggreFact (Tang et al., 2022) and DialSummEval (Gao and Wan, 2022) and uncover limitations that guide the design principles of the SUMMEDITS benchmark we build.",
                "In DialSummEval, each (dialogue, summary) tuple is evaluated by three annotators, each assigning a Likert score (1-5) assessing the consistency of the summary.",
                "Although most annotation effort has focused on the summarization of news, some prior work also looked at dialogue summarization (Gao and Wan, 2022), or the medical domain (Tang et al."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6327740b98005b5c9d090e5f1d474ff656d4174b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-14540",
                    "ArXiv": "2305.14540",
                    "DOI": "10.48550/arXiv.2305.14540",
                    "CorpusId": 258865817
                },
                "corpusId": 258865817,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6327740b98005b5c9d090e5f1d474ff656d4174b",
                "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond",
                "abstract": "With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8\\% below estimated human performance, highlighting the gaps in LLMs' ability to reason about facts and detect inconsistencies when they occur.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46180754",
                        "name": "Philippe Laban"
                    },
                    {
                        "authorId": "51232396",
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "authorId": "2057234256",
                        "name": "Divyansh Agarwal"
                    },
                    {
                        "authorId": "46255971",
                        "name": "Alexander R. Fabbri"
                    },
                    {
                        "authorId": "2054594326",
                        "name": "Caiming Xiong"
                    },
                    {
                        "authorId": "2708940",
                        "name": "Shafiq R. Joty"
                    },
                    {
                        "authorId": "30340989",
                        "name": "Chien-Sheng Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following previous research (Gao and Wan, 2022; Kryscinski et al., 2019), we demand human annotators evaluate samples on the summary level from the following three aspect: Relevance measures how well the question summary captures the main concerns of the patient\u2019s questions."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7cf9d1d474ed85f206a3a5e299e0ffdee0c26899",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-10328",
                    "ArXiv": "2303.10328",
                    "DOI": "10.48550/arXiv.2303.10328",
                    "CorpusId": 257631483
                },
                "corpusId": 257631483,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7cf9d1d474ed85f206a3a5e299e0ffdee0c26899",
                "title": "Revisiting Automatic Question Summarization Evaluation in the Biomedical Domain",
                "abstract": "Automatic evaluation metrics have been facilitating the rapid development of automatic summarization methods by providing instant and fair assessments of the quality of summaries. Most metrics have been developed for the general domain, especially news and meeting notes, or other language-generation tasks. However, these metrics are applied to evaluate summarization systems in different domains, such as biomedical question summarization. To better understand whether commonly used evaluation metrics are capable of evaluating automatic summarization in the biomedical domain, we conduct human evaluations of summarization quality from four different aspects of a biomedical question summarization task. Based on human judgments, we identify different noteworthy features for current automatic metrics and summarization systems as well. We also release a dataset of our human annotations to aid the research of summarization evaluation metrics in the biomedical domain.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2114128654",
                        "name": "Hongyi Yuan"
                    },
                    {
                        "authorId": "2118391719",
                        "name": "Yaoyun Zhang"
                    },
                    {
                        "authorId": "143857288",
                        "name": "Fei Huang"
                    },
                    {
                        "authorId": "2410938",
                        "name": "Songfang Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2217 Equal contribution for manual evaluation (Bhandari et al., 2020; Fabbri et al., 2022a; Gao and Wan, 2022).",
                "Thus, recent efforts have focused on aggregating model outputs and annotating quality dimensions to better assess summarization model and metric progress (Huang et al., 2020; Bhandari et al., 2020; Stiennon et al., 2020; Zhang and Bansal, 2021; Fabbri et al., 2022a; Gao and Wan, 2022)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "86da7a99e904481d9146f291088f80eafd181c86",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-07981",
                    "ArXiv": "2212.07981",
                    "ACL": "2023.acl-long.228",
                    "DOI": "10.48550/arXiv.2212.07981",
                    "CorpusId": 254685611
                },
                "corpusId": 254685611,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/86da7a99e904481d9146f291088f80eafd181c86",
                "title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation",
                "abstract": "Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators\u2019 prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108176413",
                        "name": "Yixin Liu"
                    },
                    {
                        "authorId": "46255971",
                        "name": "Alexander R. Fabbri"
                    },
                    {
                        "authorId": "144118452",
                        "name": "Pengfei Liu"
                    },
                    {
                        "authorId": "46316984",
                        "name": "Yilun Zhao"
                    },
                    {
                        "authorId": "51990260",
                        "name": "Linyong Nan"
                    },
                    {
                        "authorId": "2151222479",
                        "name": "Ruilin Han"
                    },
                    {
                        "authorId": "3226782",
                        "name": "Simeng Han"
                    },
                    {
                        "authorId": "2708940",
                        "name": "Shafiq R. Joty"
                    },
                    {
                        "authorId": "30340989",
                        "name": "Chien-Sheng Wu"
                    },
                    {
                        "authorId": "2054594326",
                        "name": "Caiming Xiong"
                    },
                    {
                        "authorId": "9215251",
                        "name": "Dragomir R. Radev"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "71289eaffcd5cccc04038bbce84ffe0060f2c6d2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09894",
                    "ArXiv": "2210.09894",
                    "DOI": "10.1145/3622933",
                    "CorpusId": 252967755
                },
                "corpusId": 252967755,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/71289eaffcd5cccc04038bbce84ffe0060f2c6d2",
                "title": "Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches and Future Directions",
                "abstract": "Abstractive dialogue summarization generates a concise and fluent summary covering the salient information in a dialogue among two or more interlocutors. It has attracted significant attention in recent years based on the massive emergence of social communication platforms and an urgent requirement for efficient dialogue information understanding and digestion. Different from news or articles in traditional document summarization, dialogues bring unique characteristics and additional challenges, including different language styles and formats, scattered information, flexible discourse structures, and unclear topic boundaries. This survey provides a comprehensive investigation of existing work for abstractive dialogue summarization from scenarios, approaches to evaluations. It categorizes the task into two broad categories according to the type of input dialogues, i.e., open-domain and task-oriented, and presents a taxonomy of existing techniques in three directions, namely, injecting dialogue features, designing auxiliary training tasks and using additional data. A list of datasets under different scenarios and widely-accepted evaluation metrics are summarized for completeness. After that, the trends of scenarios and techniques are summarized, together with deep insights into correlations between extensively exploited features and different scenarios. Based on these analyses, we recommend future directions, including more controlled and complicated scenarios, technical innovations and comparisons, publicly available datasets in special domains, etc.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056108122",
                        "name": "Qi Jia"
                    },
                    {
                        "authorId": "1642328639",
                        "name": "Siyu Ren"
                    },
                    {
                        "authorId": "5826956",
                        "name": "Yizhu Liu"
                    },
                    {
                        "authorId": "1796651",
                        "name": "Kenny Q. Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-06935",
                    "ArXiv": "2202.06935",
                    "DOI": "10.1613/jair.1.13715",
                    "CorpusId": 246822399
                },
                "corpusId": 246822399,
                "publicationVenue": {
                    "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
                    "name": "Journal of Artificial Intelligence Research",
                    "type": "journal",
                    "alternate_names": [
                        "JAIR",
                        "J Artif Intell Res",
                        "The Journal of Artificial Intelligence Research"
                    ],
                    "issn": "1076-9757",
                    "url": "http://www.jair.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb",
                "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text",
                "abstract": "Evaluation practices in natural language generation (NLG) have many known flaws, but improved evaluation approaches are rarely widely adopted. This issue has become more urgent, since neural generation models have improved to the point where their outputs can often no longer be distinguished based on the surface-level features that older metrics rely on. This paper surveys the issues with human and automatic model evaluations and with commonly used datasets in NLG that have been pointed out over the past 20 years. We summarize, categorize, and discuss how researchers have been addressing these issues and what their findings mean for the current state of model evaluations. Building on those insights, we lay out a long-term vision for evaluation research and propose concrete steps for researchers to improve their evaluation processes. Finally, we analyze 66 generation papers from recent NLP conferences in how well they already follow these suggestions and identify which areas require more drastic changes to the status quo.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3159346",
                        "name": "Sebastian Gehrmann"
                    },
                    {
                        "authorId": "40684993",
                        "name": "Elizabeth Clark"
                    },
                    {
                        "authorId": "145450400",
                        "name": "Thibault Sellam"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2021b) and DialSummEval (Gao and Wan, 2022) collections of system summaries, respectively, rather than generating summaries from scratch.",
                "With each dataset we collect system summaries for a set of 100 randomly selected samples from the test set, following recent work on measuring correlations between metrics (Bhandari et al., 2020; Fabbri et al., 2021b; Gao and Wan, 2022).",
                "12https://github.com/PKULCWM/PKUSUMSUM is used for Lead, LexPageRank, and ClusterCMRW\n13https://github.com/RaRe-Technologies/ gensim\n14https://pypi.org/project/ bert-extractive-summarizer/\n15https://github.com/Yale-LILY/SummEval 16https://github.com/kite99520/\nDialSummEval\nModels: LEAD-3, LONGEST-3, Pointergenerator (See et al., 2017), Transformer (Vaswani et al., 2017), BART (Lewis et al., 2019), Pegasus (Zhang et al., 2020), UniLM (Dong et al., 2019), CODS (Wu et al., 2021), ConvoSumm (Fabbri et al., 2021a), MV-BART (Chen and Yang, 2020), PLM-BART (Feng et al., 2021), Ctrl-DiaSumm (Chen et al., 2021), S-BART (Chen and Yang, 2021).",
                "Evaluation metrics have also been reevaluated in the context of scientific articles (Cohan and Goharian, 2016), and more recently, dialogues (Gao and Wan, 2022), both using single documents as input.",
                "For comparable results, for the CNN/DM (Hermann et al., 2015) and SAMSum (Gliwa et al., 2019) datasets, we use the model outputs from the SummEval (Fabbri et al., 2021b) and DialSummEval (Gao and Wan, 2022) collections of system summaries, respectively, rather than generating summaries from scratch."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "69c120355fdcdce1628446cd2da02686b4e678d4",
                "externalIds": {
                    "ACL": "2022.gem-1.40",
                    "DOI": "10.18653/v1/2022.gem-1.40",
                    "CorpusId": 256461279
                },
                "corpusId": 256461279,
                "publicationVenue": {
                    "id": "b1dc244e-c5b3-447d-b200-a46d55e8d8ee",
                    "name": "IEEE Games Entertainment Media Conference",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Game Entertain Media Conf",
                        "GEM",
                        "Int Conf Genet Evol Method",
                        "International Conference on Genetic and Evolutionary Methods"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/69c120355fdcdce1628446cd2da02686b4e678d4",
                "title": "Assessing Inter-metric Correlation for Multi-document Summarization Evaluation",
                "abstract": "Recent advances in automatic text summarization have contemporaneously been accompanied by a great deal of new metrics of automatic evaluation. This in turn has inspired recent research to re-assess these evaluation metrics to see how well they correlate with each other as well as with human evaluation, mostly focusing on single-document summarization (SDS) tasks. Although many of these metrics are typically also used for evaluating multi-document summarization (MDS) tasks, so far, little attention has been paid to studying them under such a distinct scenario. To address this gap, we present a systematic analysis of the inter-metric correlations for MDS tasks, while comparing and contrasting the results with SDS models. Using datasets from a wide range of domains (news, peer reviews, tweets, dialogues), we thus study a unified set of metrics under both the task setups. Our empirical analysis suggests that while most reference-based metrics show fairly similar trends across both multi- and single-document summarization, there is a notable lack of correlation between reference-free metrics in multi-document summarization tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "89191233",
                        "name": "M. Ridenour"
                    },
                    {
                        "authorId": "2628916",
                        "name": "Ameeta Agrawal"
                    },
                    {
                        "authorId": "2187455240",
                        "name": "Olubusayo Olabisi"
                    }
                ]
            }
        }
    ]
}