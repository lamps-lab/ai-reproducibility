{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9d83e82128dc002896f61b82b772f7ba42d718be",
                "externalIds": {
                    "ArXiv": "2310.02861",
                    "CorpusId": 263621939
                },
                "corpusId": 263621939,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9d83e82128dc002896f61b82b772f7ba42d718be",
                "title": "Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection",
                "abstract": "Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graphs. Specifically, we introduce a novel framework that consists of two components: the Rayleigh Quotient learning component (RQL) and Chebyshev Wavelet GNN with RQ-pooling (CWGNN-RQ). RQL explicitly captures the Rayleigh Quotient of graphs and CWGNN-RQ implicitly explores the spectral space of graphs. Extensive experiments on 10 real-world datasets show that RQGNN outperforms the best rival by 6.74% in Macro-F1 score and 1.44% in AUC, demonstrating the effectiveness of our framework.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2253607774",
                        "name": "Xiangyu Dong"
                    },
                    {
                        "authorId": "2153648931",
                        "name": "Xingyi Zhang"
                    },
                    {
                        "authorId": "2256078132",
                        "name": "Sibo Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "077a8ceae6e6945596bc36e49eca9328de94dd63",
                "externalIds": {
                    "ArXiv": "2310.00183",
                    "CorpusId": 263334004
                },
                "corpusId": 263334004,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/077a8ceae6e6945596bc36e49eca9328de94dd63",
                "title": "On the Equivalence of Graph Convolution and Mixup",
                "abstract": "This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \\textit{Homophily Relabel} - assigning the target node's label to all its neighbors, and 2) \\textit{Test-Time Mixup} - Mixup the feature during the test time. We establish this equivalence mathematically by demonstrating that graph convolution networks (GCN) and simplified graph convolution (SGC) can be expressed as a form of Mixup. We also empirically verify the equivalence by training an MLP using the two conditions to achieve comparable performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2249766229",
                        "name": "Xiaotian Han"
                    },
                    {
                        "authorId": "2249759429",
                        "name": "Hanqing Zeng"
                    },
                    {
                        "authorId": "2249829419",
                        "name": "Yu Chen"
                    },
                    {
                        "authorId": "2249759005",
                        "name": "Shaoliang Nie"
                    },
                    {
                        "authorId": "2249853379",
                        "name": "Jingzhou Liu"
                    },
                    {
                        "authorId": "2249757531",
                        "name": "Kanika Narang"
                    },
                    {
                        "authorId": "2249758145",
                        "name": "Zahra Shakeri"
                    },
                    {
                        "authorId": "2178963",
                        "name": "Karthik Abinav Sankararaman"
                    },
                    {
                        "authorId": "2249954878",
                        "name": "Song Jiang"
                    },
                    {
                        "authorId": "2072010",
                        "name": "Madian Khabsa"
                    },
                    {
                        "authorId": "2250669429",
                        "name": "Qifan Wang"
                    },
                    {
                        "authorId": "2249844822",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Instead of mixing the representations, G-Mixup [49] augments graphs by interpolating the graphon-based generator of graphs belonging to different classes.",
                "Graph Data Augmentation DropEdge [131] Randomly remove edges Perturbation-based GRAND [39] Stochastically discard nodes Perturbation-based NASA [9] Random neighbor replacement strategy Perturbation-based NodeAug [171] Graph properties based probability Perturbation-based GAUG [209] Learnable edge sampling probability Perturbation-based GraphMix [160] Hidden states and labels interpolation Synthetic sample-based GraphMixup [170] Node/Edge embedding mixup Synthetic sample-based GraphSMOTE [211] Node embedding based synthesis Synthetic sample-based FLAG [79] Gradient-based adversarial perturbations Perturbation-based G-Mixup [49] Graphon-based generator interpolation Synthetic sample-based LAGNN [93] Conditional distribution based synthesis Synthetic sample-based",
                "DropEdge [131] Randomly remove edges Perturbation-based GRAND [39] Stochastically discard nodes Perturbation-based NASA [9] Random neighbor replacement strategy Perturbation-based NodeAug [171] Graph properties based probability Perturbation-based GAUG [209] Learnable edge sampling probability Perturbation-based GraphMix [160] Hidden states and labels interpolation Synthetic sample-based GraphMixup [170] Node/Edge embedding mixup Synthetic sample-based GraphSMOTE [211] Node embedding based synthesis Synthetic sample-based FLAG [79] Gradient-based adversarial perturbations Perturbation-based G-Mixup [49] Graphon-based generator interpolation Synthetic sample-based LAGNN [93] Conditional distribution based synthesis Synthetic sample-based class-balanced graph-structured data in a graph data-centric way, thereby benefiting the training of graph machine learning models."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "911247dddb66d1f3ef3ba3235861bd707d15b9ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-10979",
                    "ArXiv": "2309.10979",
                    "DOI": "10.48550/arXiv.2309.10979",
                    "CorpusId": 262067202
                },
                "corpusId": 262067202,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/911247dddb66d1f3ef3ba3235861bd707d15b9ad",
                "title": "Towards Data-centric Graph Machine Learning: Review and Outlook",
                "abstract": "Data-centric AI, with its primary focus on the collection, management, and utilization of data to drive AI models and applications, has attracted increasing attention in recent years. In this article, we conduct an in-depth and comprehensive review, offering a forward-looking outlook on the current efforts in data-centric AI pertaining to graph data-the fundamental data structure for representing and capturing intricate dependencies among massive and diverse real-life entities. We introduce a systematic framework, Data-centric Graph Machine Learning (DC-GML), that encompasses all stages of the graph data lifecycle, including graph data collection, exploration, improvement, exploitation, and maintenance. A thorough taxonomy of each stage is presented to answer three critical graph-centric questions: (1) how to enhance graph data availability and quality; (2) how to learn from graph data with limited-availability and low-quality; (3) how to build graph MLOps systems from the graph data-centric view. Lastly, we pinpoint the future prospects of the DC-GML domain, providing insights to navigate its advancements and applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1430762233",
                        "name": "Xin Zheng"
                    },
                    {
                        "authorId": "2242962967",
                        "name": "Yixin Liu"
                    },
                    {
                        "authorId": "2242943639",
                        "name": "Zhifeng Bao"
                    },
                    {
                        "authorId": "2242950900",
                        "name": "Meng Fang"
                    },
                    {
                        "authorId": "2243358879",
                        "name": "Xia Hu"
                    },
                    {
                        "authorId": "2243282363",
                        "name": "Alan Wee-Chung Liew"
                    },
                    {
                        "authorId": "2585415",
                        "name": "Shirui Pan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For G-Mixup, we use the same hyper-parameters reported in [7].",
                "Few works have proposed methods to adapt mixup to graph data, including G-Mixup [7] and M-Mixup [21].",
                "Only in the case of the Reddit-5K dataset with label rates of larger than 25 labeled graphs per class, G-Mixup outperforms our proposed method.",
                "G-Mixup performs mixup to the graphons of different classes which are learned from the graph samples, and generates augmented graphs by sampling from the mixed graphons [7].",
                "We apply our proposed Graph Dual Mixup on the Graph Convolution Network (GCN) baseline [11] and compare our proposed method against 5 other graph augmentation methods from the literature: DropNode [25], DropEdge [14], M-Mixup [21], SoftEdge [5] and G-Mixup [7]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "9a6847b4c344fde539d055c83eb3751b984c97d3",
                "externalIds": {
                    "DBLP": "conf/pkdd/AlchihabiG23",
                    "ArXiv": "2309.10134",
                    "DOI": "10.1007/978-3-031-43418-1_19",
                    "CorpusId": 262054267
                },
                "corpusId": 262054267,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9a6847b4c344fde539d055c83eb3751b984c97d3",
                "title": "GDM: Dual Mixup for Graph Classification with Limited Supervision",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "24036257",
                        "name": "Abdullah Alchihabi"
                    },
                    {
                        "authorId": "2238018606",
                        "name": "Yuhong Guo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fedbb003486748af7cfc9102aa288319427e1580",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-09068",
                    "ArXiv": "2309.09068",
                    "DOI": "10.48550/arXiv.2309.09068",
                    "CorpusId": 261884476
                },
                "corpusId": 261884476,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fedbb003486748af7cfc9102aa288319427e1580",
                "title": "Recovering Missing Node Features with Local Structure-based Embeddings",
                "abstract": "Node features bolster graph-based learning when exploited jointly with network structure. However, a lack of nodal attributes is prevalent in graph data. We present a framework to recover completely missing node features for a set of graphs, where we only know the signals of a subset of graphs. Our approach incorporates prior information from both graph topology and existing nodal values. We demonstrate an example implementation of our framework where we assume that node features depend on local graph structure. Missing nodal values are estimated by aggregating known features from the most similar nodes. Similarity is measured through a node embedding space that preserves local topological features, which we train using a Graph AutoEncoder. We empirically show not only the accuracy of our feature estimation approach but also its value for downstream graph classification. Our success embarks on and implies the need to emphasize the relationship between node features and graph structure in graph-based learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2058351334",
                        "name": "Victor M. Tenorio"
                    },
                    {
                        "authorId": "1936823928",
                        "name": "Madeline Navarro"
                    },
                    {
                        "authorId": "2239197971",
                        "name": "Santiago Segarra"
                    },
                    {
                        "authorId": "2241339650",
                        "name": "Antonio G. Marques"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our result generalizes that of [16] by allowing arbitrary convex combinations and any complexon dimension.",
                "Note that for complexons of dimension 1, when \u03b3i = \u03bb, \u03b3j = 1 \u2212 \u03bb, and \u03b3k = 0 for every k \u0338= i, j, Theorem 1 reduces to the result for pairwise graphon mixup in [16].",
                "Step (1) of SC-MAD is common for mixup methods, where samples are interpolated in an embedding space [16, 17, 28].",
                "Graphons allow us to perform tasks on graph data typically restricted to continuous objects, such as barycenter obtention and interpolation for mixup [16, 17, 22].",
                "Variants have been proposed for several domains and applications, including graph mixup [16, 17], interpolation in an embedding space [28], and nonlinear implementations [17].",
                "In particular, we assume that for each class y, there is a finite set of discriminative simplicial complexes Fy such that for every labeled simplicial complex (K, y), there exists at least one F \u2208 Fy that is a subcomplex of K [16], that is, there is a homomorphism from F to K.",
                "We present the following result on the structural similarities between a complexon mixture and one of the complexons, inspired by a similar result for graphon mixup [16]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "citingPaper": {
                "paperId": "cceb11378e7bfb900f68d6d9952b6b6fa1452de8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-07453",
                    "ArXiv": "2309.07453",
                    "DOI": "10.48550/arXiv.2309.07453",
                    "CorpusId": 261822309
                },
                "corpusId": 261822309,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cceb11378e7bfb900f68d6d9952b6b6fa1452de8",
                "title": "SC-MAD: Mixtures of Higher-order Networks for Data Augmentation",
                "abstract": "The myriad complex systems with multiway interactions motivate the extension of graph-based pairwise connections to higher-order relations. In particular, the simplicial complex has inspired generalizations of graph neural networks (GNNs) to simplicial complex-based models. Learning on such systems requires large amounts of data, which can be expensive or impossible to obtain. We propose data augmentation of simplicial complexes through both linear and nonlinear mixup mechanisms that return mixtures of existing labeled samples. In addition to traditional pairwise mixup, we present a convex clustering mixup approach for a data-driven relationship among several simplicial complexes. We theoretically demonstrate that the resultant synthetic simplicial complexes interpolate among existing data with respect to homomorphism densities. Our method is demonstrated on both synthetic and real-world datasets for simplicial complex classification.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1936823928",
                        "name": "Madeline Navarro"
                    },
                    {
                        "authorId": "2239197971",
                        "name": "Santiago Segarra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the past few years, graph neural networks (GNNs) have achieved superior performance in graph-level representation learning [26, 87]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "84a9496667bb5c4bfab353f7e995c456c7cbffe4",
                "externalIds": {
                    "DOI": "10.1145/3624018",
                    "CorpusId": 261697938
                },
                "corpusId": 261697938,
                "publicationVenue": {
                    "id": "ca61b508-d706-40a4-8b54-d657dd6cd9d6",
                    "name": "ACM Transactions on Knowledge Discovery from Data",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Knowl Discov Data",
                        "ACM Transactions on Knowledge Discovery From Data"
                    ],
                    "issn": "1556-4681",
                    "url": "http://www.acm.org/pubs/tkdd/",
                    "alternate_urls": [
                        "http://portal.acm.org/tkdd",
                        "http://portal.acm.org/browse_dl.cfm?coll=portal&dl=ACM&idx=J1054&linked=1&part=transaction"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/84a9496667bb5c4bfab353f7e995c456c7cbffe4",
                "title": "Self-supervised Graph-level Representation Learning with Adversarial Contrastive Learning",
                "abstract": "The recently-developed unsupervised graph representation learning approaches apply contrastive learning into graph-structured data and achieve promising performance. However, these methods mainly focus on graph augmentation for positive samples, while the negative mining strategies for graph contrastive learning are less explored, leading to sub-optimal performance. To tackle this issue, we propose a Graph Adversarial Contrastive Learning (GraphACL) scheme that learns a bank of negative samples for effective self-supervised whole-graph representation learning. Our GraphACL consists of (i) a graph encoding branch that generates the representations of positive samples and (ii) an adversarial generation branch that produces a bank of negative samples. To generate more powerful hard negative samples, our method minimizes the contrastive loss during encoding updating while maximizing the contrastive loss adversarially over the negative samples for providing the challenging contrastive task. Moreover, the quality of representations produced by the adversarial generation branch is enhanced through the regularization of carefully designed bank divergence loss and bank orthogonality loss. We optimize the parameters of the graph encoding branch and adversarial generation branch alternately. Extensive experiments on fourteen real-world benchmarks on both graph classification and transfer learning tasks demonstrate the effectiveness of the proposed approach over existing graph self-supervised representation learning methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145263905",
                        "name": "Xiao Luo"
                    },
                    {
                        "authorId": "2154453854",
                        "name": "Wei Ju"
                    },
                    {
                        "authorId": "2112579634",
                        "name": "Yiyang Gu"
                    },
                    {
                        "authorId": "2239100867",
                        "name": "Zhengyang Mao"
                    },
                    {
                        "authorId": "2118467214",
                        "name": "Luchen Liu"
                    },
                    {
                        "authorId": "2239404721",
                        "name": "Yuhui Yuan"
                    },
                    {
                        "authorId": "1596795872",
                        "name": "Ming Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As Mixup assumes that all the classes are uniformly distributed for image classification, it does not apply when the class distribution is skewed.",
                "The core idea of Mixup is to linearly combine two samples as follows:\n\ud835\udc65syn = \ud835\udefc \u2217 \ud835\udc650 + (1 \u2212 \ud835\udefc) \u2217 \ud835\udc651, (1)\nwhere \ud835\udc650 and \ud835\udc651 are the two selected source samples and \ud835\udefc \u2208 [0.0, 1.0] controls the composition of\ud835\udc65syn.",
                "To generalize label information from two different classes,Mixup [44] performs synthetic data generation over two samples from different classes, which has been extensively studied to augment image and textual data.",
                "Recently, instead of conducting synthetic data sampling on a single class, Mixup [7, 13, 23, 44] achieves a significant improvement in the image domain by synthesizing data points through linearly combining two random samples from different classes with a given combination ratio and creating soft labels for training the neural networks."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "87bbb04b184eed1fdcafa069719a587463abfeed",
                "externalIds": {
                    "ArXiv": "2308.14838",
                    "DBLP": "journals/corr/abs-2308-14838",
                    "DOI": "10.1145/3583780.3615071",
                    "CorpusId": 261277122
                },
                "corpusId": 261277122,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/87bbb04b184eed1fdcafa069719a587463abfeed",
                "title": "Tackling Diverse Minorities in Imbalanced Classification",
                "abstract": "Imbalanced datasets are commonly observed in various real-world applications, presenting significant challenges in training classifiers. When working with large datasets, the imbalanced issue can be further exacerbated, making it exceptionally difficult to train classifiers effectively. To address the problem, over-sampling techniques have been developed to linearly interpolating data instances between minorities and their neighbors. However, in many real-world scenarios such as anomaly detection, minority instances are often dispersed diversely in the feature space rather than clustered together. Inspired by domain-agnostic data mix-up, we propose generating synthetic samples iteratively by mixing data samples from both minority and majority classes. It is non-trivial to develop such a framework, the challenges include source sample selection, mix-up strategy selection, and the coordination between the underlying model and mix-up strategies. To tackle these challenges, we formulate the problem of iterative data mix-up as a Markov decision process (MDP) that maps data attributes onto an augmentation strategy. To solve the MDP, we employ an actor-critic framework to adapt the discrete-continuous decision space. This framework is utilized to train a data augmentation policy and design a reward signal that explores classifier uncertainty and encourages performance improvement, irrespective of the classifier's convergence. We demonstrate the effectiveness of our proposed framework through extensive experiments conducted on seven publicly available benchmark datasets using three different types of classifiers. The results of these experiments showcase the potential and promise of our framework in addressing imbalanced datasets with diverse minorities.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51238382",
                        "name": "Kwei-Herng Lai"
                    },
                    {
                        "authorId": "1759658",
                        "name": "D. Zha"
                    },
                    {
                        "authorId": "1504511015",
                        "name": "Huiyuan Chen"
                    },
                    {
                        "authorId": "31819608",
                        "name": "M. Bendre"
                    },
                    {
                        "authorId": "2215477428",
                        "name": "Yuzhong Chen"
                    },
                    {
                        "authorId": "40308435",
                        "name": "Mahashweta Das"
                    },
                    {
                        "authorId": null,
                        "name": "Hao Yang"
                    },
                    {
                        "authorId": "2109724398",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, the graphs we considered in the experiment all have node features, while G-Mixup [49] only applies to undirected graphs without node features, and therefore is not within the scope of our baselines."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "259ca1a041da8f7305ed99f40efb11f637a2f1ff",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-08344",
                    "ArXiv": "2308.08344",
                    "DOI": "10.48550/arXiv.2308.08344",
                    "CorpusId": 260926525
                },
                "corpusId": 260926525,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/259ca1a041da8f7305ed99f40efb11f637a2f1ff",
                "title": "Graph Out-of-Distribution Generalization with Controllable Data Augmentation",
                "abstract": "Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe \\emph{hybrid structure distribution shift} of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose \\texttt{OOD-GMixup} to jointly manipulate the training distribution with \\emph{controllable data augmentation} in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Secondly, we generate virtual samples with perturbation on graph rationale representation domain to obtain potential OOD training samples. Finally, we propose OOD calibration to measure the distribution deviation of virtual samples by leveraging Extreme Value Theory, and further actively control the training distribution by emphasizing the impact of virtual OOD samples. Extensive studies on several real-world datasets on graph classification demonstrate the superiority of our proposed method over state-of-the-art baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064914349",
                        "name": "Bin Lu"
                    },
                    {
                        "authorId": "1693639",
                        "name": "Xiaoying Gan"
                    },
                    {
                        "authorId": "2231660058",
                        "name": "Ze Zhao"
                    },
                    {
                        "authorId": "2114787589",
                        "name": "Shiyu Liang"
                    },
                    {
                        "authorId": "1922573",
                        "name": "Luoyi Fu"
                    },
                    {
                        "authorId": "2107937507",
                        "name": "Xinbing Wang"
                    },
                    {
                        "authorId": "2111168706",
                        "name": "Cheng Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several recent studies have implemented mixup techniques on graphs, including GraphTransplant [9], G-Mixup [5], IfMixup [3], and Manifold Mixup [13]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c4dbfcc16f63aea939da2f71f781c9f07cf7f2f7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-08097",
                    "ArXiv": "2308.08097",
                    "DOI": "10.1145/3583780.3615280",
                    "CorpusId": 260926300
                },
                "corpusId": 260926300,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c4dbfcc16f63aea939da2f71f781c9f07cf7f2f7",
                "title": "S-Mixup: Structural Mixup for Graph Neural Networks",
                "abstract": "Existing studies for applying the mixup technique on graphs mainly focus on graph classification tasks, while the research in node classification is still under-explored. In this paper, we propose a novel mixup augmentation for node classification called Structural Mixup (S-Mixup). The core idea is to take into account the structural information while mixing nodes. Specifically, S-Mixup obtains pseudo-labels for unlabeled nodes in a graph along with their prediction confidence via a Graph Neural Network (GNN) classifier. These serve as the criteria for the composition of the mixup pool for both inter and intra-class mixups. Furthermore, we utilize the edge gradient obtained from the GNN training and propose a gradient-based edge selection strategy for selecting edges to be attached to the nodes generated by the mixup. Through extensive experiments on real-world benchmark datasets, we demonstrate the effectiveness of S-Mixup evaluated on the node classification task. We observe that S-Mixup enhances the robustness and generalization performance of GNNs, especially in heterophilous situations. The source code of S-Mixup can be found at \\url{https://github.com/SukwonYun/S-Mixup}",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2211356120",
                        "name": "Jung-Hyun Kim"
                    },
                    {
                        "authorId": "3122830",
                        "name": "Sukwon Yun"
                    },
                    {
                        "authorId": "2109120259",
                        "name": "Chanyoung Park"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026pairs of training examples to extend the training distribution and prevent the deep neural network from overfitting the training data (Zhang et al. 2018; Yun et al. 2019; Kim, Choo, and Song 2020; Beckham et al. 2019; Verma et al. 2019; Wang et al. 2021; Han et al. 2022a; Yao et al. 2022).",
                "To this end, mixup-based methods create mixed samples by combining pairs of training examples to extend the training distribution and prevent the deep neural network from overfitting the training data (Zhang et al. 2018; Yun et al. 2019; Kim, Choo, and Song 2020; Beckham et al. 2019; Verma et al. 2019; Wang et al. 2021; Han et al. 2022a; Yao et al. 2022).",
                "Most of the previous mixup variants focus on designing how to mix different samples so that the mixed samples are helpful for neural network training (Yun et al. 2019; Kim, Choo, and Song 2020; Beckham et al. 2019; Verma et al. 2019; Wang et al. 2021; Han et al. 2022a; Yao et al. 2022).",
                "In addition, mixup variants have been shown to be effective on a variety of tasks, including fairness machine learning (Han et al. 2022b, 2023; Mroueh et al. 2021), domain generalization (Zhou et al. 2020; Yao et al. 2022), confidence calibration (Zhang et al. 2022; Thulasidasan et al. 2019).",
                "Due to the simplicity and effectiveness, mixup-based methods have gained popularity in various data types and tasks (Yun et al. 2019; Kim, Choo, and Song 2020; Kim et al. 2023; Sahoo et al. 2021; Wang et al. 2021; Han et al. 2022a; Verma et al. 2019; Han et al. 2022b, 2023; Zhou et al. 2020; Mroueh et al. 2021; Zhang et al. 2022; Thulasidasan et al. 2019).",
                "Overall, previous mixup variants mainly focus on improving the mixing process to extend the training distribution (Yun et al. 2019; Kim, Choo, and Song 2020; Kim et al. 2023; Sahoo et al. 2021; Verma et al. 2019; Wang et al. 2021; Han et al. 2022a).",
                "\u2026have gained popularity in various data types and tasks (Yun et al. 2019; Kim, Choo, and Song 2020; Kim et al. 2023; Sahoo et al. 2021; Wang et al. 2021; Han et al. 2022a; Verma et al. 2019; Han et al. 2022b, 2023; Zhou et al. 2020; Mroueh et al. 2021; Zhang et al. 2022; Thulasidasan et al. 2019)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "11d9abef642cca735eaea2b7846174cc3de3b5d5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-06451",
                    "ArXiv": "2308.06451",
                    "DOI": "10.48550/arXiv.2308.06451",
                    "CorpusId": 260887608
                },
                "corpusId": 260887608,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/11d9abef642cca735eaea2b7846174cc3de3b5d5",
                "title": "Semantic Equivariant Mixup",
                "abstract": "Mixup is a well-established data augmentation technique, which can extend the training distribution and regularize the neural networks by creating ''mixed'' samples based on the label-equivariance assumption, i.e., a proportional mixup of the input data results in the corresponding labels being mixed in the same proportion. However, previous mixup variants may fail to exploit the label-independent information in mixed samples during training, which usually contains richer semantic information. To further release the power of mixup, we first improve the previous label-equivariance assumption by the semantic-equivariance assumption, which states that the proportional mixup of the input data should lead to the corresponding representation being mixed in the same proportion. Then a generic mixup regularization at the representation level is proposed, which can further regularize the model with the semantic information in mixed samples. At a high level, the proposed semantic equivariant mixup (sem) encourages the structure of the input data to be preserved in the representation space, i.e., the change of input will result in the obtained representation information changing in the same way. Different from previous mixup variants, which tend to over-focus on the label-related information, the proposed method aims to preserve richer semantic information in the input with semantic-equivariance assumption, thereby improving the robustness of the model against distribution shifts. We conduct extensive empirical studies and qualitative analyzes to demonstrate the effectiveness of our proposed method. The code of the manuscript is in the supplement.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2008848359",
                        "name": "Zongbo Han"
                    },
                    {
                        "authorId": "2229112999",
                        "name": "Tianchi Xie"
                    },
                    {
                        "authorId": "2152564746",
                        "name": "Bing Wu"
                    },
                    {
                        "authorId": "2150570513",
                        "name": "Qinghua Hu"
                    },
                    {
                        "authorId": "2256775070",
                        "name": "Changqing Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e8b5c9ef9fd927b2bac73eca2b92f3480dde23db",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-05463",
                    "ArXiv": "2308.05463",
                    "DOI": "10.24963/ijcai.2023/509",
                    "CorpusId": 260775812
                },
                "corpusId": 260775812,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e8b5c9ef9fd927b2bac73eca2b92f3480dde23db",
                "title": "G2Pxy: Generative Open-Set Node Classification on Graphs with Proxy Unknowns",
                "abstract": "Node classification is the task of predicting the labels of unlabeled nodes in a graph. State-of-the-art methods based on graph neural networks achieve excellent performance when all labels are available\n\n during training. But in real-life, models are of ten applied on data with new classes, which can lead to massive misclassification and thus significantly degrade performance. Hence, developing\n\n open-set classification methods is crucial to determine if a given sample belongs to a known class. Existing methods for open-set node classification generally use transductive learning with part or all\n\n of the features of real unseen class nodes to help with open-set classification. In this paper, we propose a novel generative open-set node classification method, i.e., G2Pxy, which follows a stricter inductive learning setting where no information about unknown classes is available during training and validation. Two kinds of proxy unknown nodes, inter-class unknown proxies and external unknown proxies are generated via mixup to efficiently anticipate the distribution of novel classes. Using the generated proxies, a closed-set classifier can be transformed into an open-set one, by augmenting it with an extra proxy classifier. Under the constraints\n\n of both cross entropy loss and complement entropy loss, G2Pxy achieves superior effectiveness for unknown class detection and known class classification, which is validated by experiments on bench\n\nmark graph datasets. Moreover, G2Pxy does not have specific requirement on the GNN architecture and shows good generalizations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "143933372",
                        "name": "Q. Zhang"
                    },
                    {
                        "authorId": "2110208567",
                        "name": "Zelin Shi"
                    },
                    {
                        "authorId": "2141906898",
                        "name": "Xiaolin Zhang"
                    },
                    {
                        "authorId": "36774028",
                        "name": "Xiaojun Chen"
                    },
                    {
                        "authorId": "1409842348",
                        "name": "Philippe Fournier-Viger"
                    },
                    {
                        "authorId": "2153326034",
                        "name": "Shirui Pan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6e28bd18ea279b59273d3f291b3627732dc79bca",
                "externalIds": {
                    "DBLP": "conf/ijcai/MaHGZ23",
                    "DOI": "10.24963/ijcai.2023/449",
                    "CorpusId": 260856524
                },
                "corpusId": 260856524,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6e28bd18ea279b59273d3f291b3627732dc79bca",
                "title": "Multi-View Robust Graph Representation Learning for Graph Classification",
                "abstract": "The robustness of graph classification models plays an essential role in providing highly reliable applications. Previous studies along this line primarily focus on seeking the stability of the model in terms of overall data metrics (e.g., accuracy) when facing data perturbations, such as removing edges. Empirically, we find that these graph classification models also suffer from semantic bias and confidence collapse issues, which substantially hinder their applicability in real-world scenarios. To address these issues, we present MGRL, a multi-view representation learning model for graph classification tasks that achieves robust results. Firstly, we proposes an instance-view consistency representation learning method, which utilizes multi-granularity contrastive learning technique to perform semantic constraints on instance representations at both the node and graph levels, thus alleviating the semantic bias issue. Secondly, we proposes a class-view discriminative representation learning method, which employs the prototype-driven class distance optimization technique to adjust intra- and inter-class distances, thereby mitigating the confidence collapse issue.Finally, extensive experiments and visualizations on eight benchmark dataset demonstrate the effectiveness of MGRL.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2147432175",
                        "name": "Guanghui Ma"
                    },
                    {
                        "authorId": "1767378",
                        "name": "Chunming Hu"
                    },
                    {
                        "authorId": "2054614957",
                        "name": "Ling Ge"
                    },
                    {
                        "authorId": "2146243972",
                        "name": "Hong Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "graph classification [34], [35], link prediction [36], [37], [38], [39], node clustering [40], [41], and anomaly detection [42], [43], [44], [45]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "147992e6a8d6280628b715a18412bef7f23daa7b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-11981",
                    "ArXiv": "2307.11981",
                    "DOI": "10.1109/tkde.2023.3298002",
                    "CorpusId": 260125954
                },
                "corpusId": 260125954,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/147992e6a8d6280628b715a18412bef7f23daa7b",
                "title": "Collaborative Graph Neural Networks for Attributed Network Embedding",
                "abstract": "Graph neural networks (GNNs) have shown prominent performance on attributed network embedding. However, existing efforts mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node features at the initial layer. This simple strategy impedes the potential of node attributes in augmenting node connections, leading to limited receptive field for inactive nodes with few or even no neighbors. Furthermore, the training objectives (i.e., reconstructing network structures) of most GNNs also do not include node attributes, although studies have shown that reconstructing node attributes is beneficial. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including graph convolution operations and training objectives. However, this is a nontrivial task since an appropriate way of integration is required to maintain the merits of GNNs. To bridge the gap, in this paper, we propose COllaborative graph Neural Networks--CONN, a tailored GNN architecture for attribute network embedding. It improves model capacity by 1) selectively diffusing messages from neighboring nodes and involved attribute categories, and 2) jointly reconstructing node-to-node and node-to-attribute-category interactions via cross-correlation. Experiments on real-world networks demonstrate that CONN excels state-of-the-art embedding algorithms with a great margin.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9747941",
                        "name": "Qiaoyu Tan"
                    },
                    {
                        "authorId": "2149174861",
                        "name": "Xin Zhang"
                    },
                    {
                        "authorId": "47933250",
                        "name": "Xiao Huang"
                    },
                    {
                        "authorId": "50689319",
                        "name": "Haojun Chen"
                    },
                    {
                        "authorId": "1737121128",
                        "name": "Jundong Li"
                    },
                    {
                        "authorId": "2148950326",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that our proposed mixup approach is different from traditional mixup approaches [15, 49, 54] in data augmentation, where they usually follow a form similar to M (mix) = \u03bbMa + (1 \u2212 \u03bb)Mb .",
                "[15] and [45] generate interpolated graphs with the estimation of the properties in the graph data, like the graphon of each class or nearest neighbors of target nodes.",
                "All the previous methods [13, 15, 39, 40, 43] aim to generalize the mixup approach to improve the performance of classification models like GNNs."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "result"
            ],
            "citingPaper": {
                "paperId": "4936c5acfffcc6368e99cd8326fc7555c76a58f0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-07832",
                    "ArXiv": "2307.07832",
                    "DOI": "10.1145/3580305.3599435",
                    "CorpusId": 259937050
                },
                "corpusId": 259937050,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/4936c5acfffcc6368e99cd8326fc7555c76a58f0",
                "title": "MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation",
                "abstract": "Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. However, their predictions are often not interpretable. Post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we shed light on the existence of the distribution shifting issue in existing methods, which affects explanation quality, particularly in applications on real-life datasets with tight decision boundaries. To address this issue, we introduce a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method, MixupExplainer, with a theoretical guarantee to resolve the distribution shifting issue. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our proposed mixup approach over existing approaches. We also provide a detailed analysis of how our proposed approach alleviates the distribution shifting issue.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144129760",
                        "name": "Jiaxing Zhang"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "2181654032",
                        "name": "Huazhou Wei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are also many mix-up related technologies including GraphMix [40], MixupGraph [42], GMixup [15], and ifMixup [14]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "54019f5026c385da3f9389ee80525c973f0eb6e5",
                "externalIds": {
                    "ArXiv": "2307.07840",
                    "DBLP": "journals/corr/abs-2307-07840",
                    "DOI": "10.48550/arXiv.2307.07840",
                    "CorpusId": 259937463
                },
                "corpusId": 259937463,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/54019f5026c385da3f9389ee80525c973f0eb6e5",
                "title": "RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task",
                "abstract": "Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation. Extensive experiments show the effectiveness of the proposed method in interpreting GNN models in regression tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144129760",
                        "name": "Jiaxing Zhang"
                    },
                    {
                        "authorId": "153267839",
                        "name": "Zhuomin Chen"
                    },
                    {
                        "authorId": "2176403267",
                        "name": "Hao Mei"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "1474226770",
                        "name": "Hua Wei"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", natural image [27]\u2013[29], graph [30], 3D point cloud, [31], visual-language [32], etc.",
                "Mixup (2018) [26] data-agnostic 3 aligned by default interpolate 3 input interpolation scale TransMix (2022) [29] natural image 3 scaling or cropping mask & mix 3 target attention weights G-Mixup (2022) [30] graph 3 graphon estimation interpolate 3 graphon interpolation scale PointPatchMix (2023) [31] point cloud 3 point patch mask & mix 3 patch attention scores"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9050251e3bcf259503f7e426d8fb7ab687f6c268",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-16180",
                    "ArXiv": "2306.16180",
                    "DOI": "10.48550/arXiv.2306.16180",
                    "CorpusId": 259274736
                },
                "corpusId": 259274736,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9050251e3bcf259503f7e426d8fb7ab687f6c268",
                "title": "Pseudo-Bag Mixup Augmentation for Multiple Instance Learning Based Whole Slide Image Classification",
                "abstract": "Given the special situation of modeling gigapixel images, multiple instance learning (MIL) has become one of the most important frameworks for Whole Slide Image (WSI) classification. In current practice, most MIL networks often face two unavoidable problems in training: i) insufficient WSI data, and ii) the sample memorization inclination inherent in neural networks. These problems may hinder MIL models from adequate and efficient training, suppressing the continuous performance promotion of classification models on WSIs. Inspired by the basic idea of Mixup, this paper proposes a new Pseudo-bag Mixup (PseMix) data augmentation scheme to improve the training of MIL models. This scheme generalizes the Mixup strategy for general images to special WSIs via pseudo-bags so as to be applied in MIL-based WSI classification. Cooperated by pseudo-bags, our PseMix fulfills the critical size alignment and semantic alignment in Mixup strategy. Moreover, it is designed as an efficient and decoupled method, neither involving time-consuming operations nor relying on MIL model predictions. Comparative experiments and ablation studies are specially designed to evaluate the effectiveness and advantages of our PseMix. Experimental results show that PseMix could often assist state-of-the-art MIL networks to refresh the classification performance on WSIs. Besides, it could also boost the generalization ability of MIL models, and promote their robustness to patch occlusion and noisy labels. Our source code is available at https://github.com/liupei101/PseMix.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2113249787",
                        "name": "Pei Liu"
                    },
                    {
                        "authorId": "2106516536",
                        "name": "Luping Ji"
                    },
                    {
                        "authorId": "120070100",
                        "name": "Xinyu Zhang"
                    },
                    {
                        "authorId": "2053898508",
                        "name": "Feng Ye"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "dc072922b4a72d649842d53cee83eea7a70b77ae",
                "externalIds": {
                    "ArXiv": "2306.15963",
                    "CorpusId": 259275272
                },
                "corpusId": 259275272,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dc072922b4a72d649842d53cee83eea7a70b77ae",
                "title": "Fused Gromov-Wasserstein Graph Mixup for Graph-level Classifications",
                "abstract": "Graph data augmentation has shown superiority in enhancing generalizability and robustness of GNNs in graph-level classifications. However, existing methods primarily focus on the augmentation in the graph signal space and the graph structure space independently, neglecting the joint interaction between them. In this paper, we address this limitation by formulating the problem as an optimal transport problem that aims to find an optimal inter-graph node matching strategy considering the interactions between graph structures and signals. To solve this problem, we propose a novel graph mixup algorithm called FGWMixup, which seeks a midpoint of source graphs in the Fused Gromov-Wasserstein (FGW) metric space. To enhance the scalability of our method, we introduce a relaxed FGW solver that accelerates FGWMixup by improving the convergence rate from $\\mathcal{O}(t^{-1})$ to $\\mathcal{O}(t^{-2})$. Extensive experiments conducted on five datasets using both classic (MPNNs) and advanced (Graphormers) GNN backbones demonstrate that FGWMixup effectively improves the generalizability and robustness of GNNs. Codes are available at https://github.com/ArthurLeoM/FGWMixup.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2164707525",
                        "name": "Xinyu Ma"
                    },
                    {
                        "authorId": "2056598391",
                        "name": "Xu Chu"
                    },
                    {
                        "authorId": "2108738446",
                        "name": "Yasha Wang"
                    },
                    {
                        "authorId": "2149200707",
                        "name": "Yang Lin"
                    },
                    {
                        "authorId": "2145804723",
                        "name": "Junfeng Zhao"
                    },
                    {
                        "authorId": "2219046611",
                        "name": "Liantao Ma"
                    },
                    {
                        "authorId": "2156154955",
                        "name": "Wenwu Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another work [12] proposes to learn a graph generator to align the pair of graphs and interpolate the generated counterparts."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c16459454845a6686a125a2dcece47c3ef5fbb0c",
                "externalIds": {
                    "ArXiv": "2306.15154",
                    "DBLP": "journals/corr/abs-2306-15154",
                    "DOI": "10.1145/3580305.3599288",
                    "CorpusId": 259262390
                },
                "corpusId": 259262390,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/c16459454845a6686a125a2dcece47c3ef5fbb0c",
                "title": "Contrastive Meta-Learning for Few-shot Node Classification",
                "abstract": "Few-shot node classification, which aims to predict labels for nodes on graphs with only limited labeled nodes as references, is of great significance in real-world graph mining tasks. To tackle such a label shortage issue, existing works generally leverage the meta-learning framework, which utilizes a number of episodes to extract transferable knowledge from classes with abundant labeled nodes and generalizes the knowledge to other classes with limited labeled nodes. In essence, the primary aim of few-shot node classification is to learn node embeddings that are generalizable across different classes. To accomplish this, the GNN encoder must be able to distinguish node embeddings between different classes, while also aligning embeddings for nodes in the same class. Thus, in this work, we propose to consider both the intra-class and inter-class generalizability of the model. We create a novel contrastive meta-learning framework on graphs, named COSMIC, with two key designs. First, we propose to enhance the intra-class generalizability by involving a contrastive two-step optimization in each episode to explicitly align node embeddings in the same classes. Second, we strengthen the inter-class generalizability by generating hard node classes for classification via a novel similarity-sensitive mix-up strategy. Extensive experiments on prevalent few-shot node classification datasets verify the effectiveness of our framework and demonstrate its superiority over other state-of-the-art baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2117075272",
                        "name": "Song Wang"
                    },
                    {
                        "authorId": "144630335",
                        "name": "Zhen Tan"
                    },
                    {
                        "authorId": "2155337763",
                        "name": "Huan Liu"
                    },
                    {
                        "authorId": "2040455",
                        "name": "Jundong Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Graph neural network (GNN) has been widely used in multiple tasks, such as node classification [7], [8], graph classification [9], [10], and link prediction [11], [12], and has achieved better task performance than traditional methods, which proves that graph neural networks can learn the representation of nodes very well."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "12467f15dfa47f8d50e30adb77fec7822c8e51c5",
                "externalIds": {
                    "DBLP": "conf/ijcnn/ChenWS23",
                    "DOI": "10.1109/IJCNN54540.2023.10191411",
                    "CorpusId": 260385859
                },
                "corpusId": 260385859,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/12467f15dfa47f8d50e30adb77fec7822c8e51c5",
                "title": "Meta-MSGAT: Meta Multi-scale Fused Graph Attention Network",
                "abstract": "In recent years, graph attention networks(GAT) have received extensive attention. According to the different attention extraction methods, GAT is mainly divided into two categories: hierarchical graph attention and structural information extraction of attention. However, they cannot achieve plug-and-play effects. Recent studies have found that the excellent performance of Transformer on different tasks lies not in the multi-head attention mechanism but in the structure itself. Inspired by the above, we propose a plug-and-play graph attention convolution structure: Meta Multi-scale Fused Graph Attention Network(Meta-MSGAT). Meta-MSGAT consists of multi-scale feature extraction, feature transformation and aggregation, and message-passing. The Multi-scale feature extraction is implemented using multiple channels. Feature transformation and aggregation consist of different convolution kernels to increase the diverse representation of features. The message-passing layer consists of a mixer that does not limit the specific network layer. To verify the effectiveness of our proposed structure, we performed extensive experiments on six datasets and achieved an improvement from 0.35% to 2.76% compared to baselines. In particular, we use MLP or GAT instead of the mixer for ablation experiments, and both achieve superior performance over baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145358498",
                        "name": "Ting Chen"
                    },
                    {
                        "authorId": "2144499494",
                        "name": "Jianming Wang"
                    },
                    {
                        "authorId": "9177519",
                        "name": "Yukuan Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In recent years, Graph data augmentation techniques based on graphs and subgraphs have been extensively studied [14], [37]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f4577ade580cc67b62932d5d0a7ae9ae61f5bb95",
                "externalIds": {
                    "DBLP": "conf/ijcnn/ChangYZ23",
                    "DOI": "10.1109/IJCNN54540.2023.10191356",
                    "CorpusId": 260386304
                },
                "corpusId": 260386304,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/f4577ade580cc67b62932d5d0a7ae9ae61f5bb95",
                "title": "ConsE: Consistency Exploitation for Semi-Supervised Anomaly Detection in Graphs",
                "abstract": "Graph anomaly detection has attracted considerable interest due to the wide use of graph structure data. Several GNN-based anomaly detection methods discover anomalies through the powerful node representation ability of GNNs. However, real-world graphs are typically rarely labeled, which leads these deep learning methods to face the challenge of under-fitting. A fundamental question here is: can anomalies in graphs be detected with few annotations? In this paper, we propose a novel semi-supervised anomaly detection method in graphs based on Consistency Exploitation (ConsE). First, ConsE adopts a consistency-based neighbor sampler, which ensures the consistency of a central node and its neighbors on attributes and categories during the aggregation process through attribute similarity and soft pseudo-labels. Afterward, ConsE encourages the consistency of node representations generated by a dedicated neighbor sampler and a generic neighbor sampler to improve its robustness in complex neighborhoods. Experimental results on real-world datasets demonstrate that our model significantly outperforms several state-of-the-art baseline methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223912123",
                        "name": "Wenjing Chang"
                    },
                    {
                        "authorId": "2175292655",
                        "name": "Jianjun Yu"
                    },
                    {
                        "authorId": "2110696154",
                        "name": "Xiaojun Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "33538d46152a3f1c7bd220a75ad78075a1570b83",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-08076",
                    "ArXiv": "2306.08076",
                    "DOI": "10.48550/arXiv.2306.08076",
                    "CorpusId": 259164712
                },
                "corpusId": 259164712,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/33538d46152a3f1c7bd220a75ad78075a1570b83",
                "title": "Graph Structure and Feature Extrapolation for Out-of-Distribution Generalization",
                "abstract": "Out-of-distribution (OOD) generalization deals with the prevalent learning scenario where test distribution shifts from training distribution. With rising application demands and inherent complexity, graph OOD problems call for specialized solutions. While data-centric methods exhibit performance enhancements on many generic machine learning tasks, there is a notable absence of data augmentation methods tailored for graph OOD generalization. In this work, we propose to achieve graph OOD generalization with the novel design of non-Euclidean-space linear extrapolation. The proposed augmentation strategy extrapolates both structure and feature spaces to generate OOD graph data. Our design tailors OOD samples for specific shifts without corrupting underlying causal mechanisms. Theoretical analysis and empirical results evidence the effectiveness of our method in solving target shifts, showing substantial and constant improvements across various graph OOD tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118053386",
                        "name": "Xiner Li"
                    },
                    {
                        "authorId": "1914700964",
                        "name": "Shurui Gui"
                    },
                    {
                        "authorId": "2004524780",
                        "name": "Youzhi Luo"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that G-mixup relies on a strong assumption that graphs from the same class can be generated by the same graph generator (i.e., graphon).",
                "Han et al. (2022) proposes to learn a Graphon for each class and performs mixup in Graphon space.",
                ", 2022), which mixes random subgraphs of input graph pairs; (6) G-Mixup (Han et al., 2022), which is a class-level graph mixup method by interpolating graphons of different classes.",
                "Meanwhile, G-Mixup generates the same node features for all augmented graphs, thus leading to performance degradation on PROTEINS and NCI1 datasets which have node features.",
                "\u2026al., 2019; Wang et al., 2021b)4, which linearly interpolates the graph-level representations; (5) SubMix (Yoo et al., 2022), which mixes random subgraphs of input graph pairs; (6) G-Mixup (Han et al., 2022), which is a class-level graph mixup method by interpolating graphons of different classes.",
                "Several existing graph mixup methods (Han et al., 2022; Park et al., 2022; Yoo et al., 2022; Guo & Mao, 2021) use various tricks to sidestep this problem.",
                "Instead of directly mixing instances, G-mixup (Han et al., 2022) proposes a class-level graph mixup method that interpolates the graph generators of different classes.",
                "Comparison between ours and other graph mixup methods Preserving Mixing node Perserving Methods Instance-level motif feature space Input-level graph size G-mixup (Han et al., 2022) \u2713 \u2713",
                "We compare our methods with the following baseline methods, including (1) DropEdge (Rong et al., 2020), which uniformly removes a certain ratio of edges from the input graphs; (2) DropNode (Feng et al., 2020; You et al., 2020), which uniformly drops a certain portion of nodes from the input graphs; (3) Subgraph (You et al., 2020), which extract subgraphs from the input graphs via a random walk sampler; (4) M-Mixup (Verma et al., 2019; Wang et al., 2021b)4, which linearly interpolates the graph-level representations; (5) SubMix (Yoo et al., 2022), which mixes random subgraphs of input graph pairs; (6) G-Mixup (Han et al., 2022), which is a class-level graph mixup method by interpolating graphons of different classes."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ca66912298f2e5c739ac0f6f0b5932422551b608",
                "externalIds": {
                    "ArXiv": "2306.06788",
                    "DBLP": "journals/corr/abs-2306-06788",
                    "DOI": "10.48550/arXiv.2306.06788",
                    "CorpusId": 259138623
                },
                "corpusId": 259138623,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ca66912298f2e5c739ac0f6f0b5932422551b608",
                "title": "Graph Mixup with Soft Alignments",
                "abstract": "We study graph data augmentation by mixup, which has been used successfully on images. A key operation of mixup is to compute a convex combination of a pair of inputs. This operation is straightforward for grid-like data, such as images, but challenging for graph data. The key difficulty lies in the fact that different graphs typically have different numbers of nodes, and thus there lacks a node-level correspondence between graphs. In this work, we propose S-Mixup, a simple yet effective mixup method for graph classification by soft alignments. Specifically, given a pair of graphs, we explicitly obtain node-level correspondence via computing a soft assignment matrix to match the nodes between two graphs. Based on the soft assignments, we transform the adjacency and node feature matrices of one graph, so that the transformed graph is aligned with the other graph. In this way, any pair of graphs can be mixed directly to generate an augmented graph. We conduct systematic experiments to show that S-Mixup can improve the performance and generalization of graph neural networks (GNNs) on various graph classification tasks. In addition, we show that S-Mixup can increase the robustness of GNNs against noisy labels.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2055943899",
                        "name": "Hongyi Ling"
                    },
                    {
                        "authorId": "47653902",
                        "name": "Zhimeng Jiang"
                    },
                    {
                        "authorId": "38813990",
                        "name": "Meng Liu"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    },
                    {
                        "authorId": "49648991",
                        "name": "Na Zou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(19)\nFor the setting of classification, \ud835\udc43 (\ud835\udc46\ud835\udc5d,\ud835\udc5e | \ud835\udc66 = \ud835\udc50) \u223c N ( \ud835\udf07 \ud835\udc5d,\ud835\udc5e \ud835\udc50 , (\ud835\udf0e \ud835\udc5d,\ud835\udc5e \ud835\udc50 )2 ) , (20)\nTo extend G-Mixup for regression, we slightly modify the augmentation process to adapt it for regression tasks as\n\ud835\udc43 (\ud835\udc46\ud835\udc5d,\ud835\udc5e | \ud835\udc66) \u223c N ( \ud835\udf07\ud835\udc5d,\ud835\udc5e + \ud835\udf0e\ud835\udc5d,\ud835\udc5e\n\ud835\udf0e\ud835\udc66 \ud835\udf0c\ud835\udc5d,\ud835\udc5e\n( \ud835\udc66 \u2212 \ud835\udf07\ud835\udc66 ) , ( 1 \u2212 (\ud835\udf0c\ud835\udc5d,\ud835\udc5e)2 ) (\ud835\udf0e\ud835\udc5d,\ud835\udc5e)2 ) ,\n(21)\nwhere \ud835\udf07 and \ud835\udf0e are the mean and standard deviation of the weight for each edge, \ud835\udf0c is the correlation coefficient between \ud835\udc46\ud835\udc5d,\ud835\udc5e and \ud835\udc66.\nC-Mixup [93] shares the same process with the V-Mixup.",
                "There are several attempts to study Mixup on non-Euclidean data, graphs, like NodeMixup [84], GraphMixup [85] and G-Mixup [39].",
                "(18)\nG-Mixup [39] is originally proposed for classification tasks, which augments graphs by interpolating the generator of different classes of graphs.",
                "G-Mixup [39] is originally proposed for classification tasks, which augments graphs by interpolating the generator of different classes of graphs."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bfa7d64ddfb36c1ce196abe71b4a4097cff5a0d4",
                "externalIds": {
                    "DBLP": "conf/kdd/KanLCYXYZGY23",
                    "ArXiv": "2306.02532",
                    "DOI": "10.1145/3580305.3599483",
                    "CorpusId": 259076208,
                    "PubMed": "37332568"
                },
                "corpusId": 259076208,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/bfa7d64ddfb36c1ce196abe71b4a4097cff5a0d4",
                "title": "R-Mixup: Riemannian Mixup for Biological Networks",
                "abstract": "Biological networks are commonly used in biomedical and healthcare domains to effectively model the structure of complex biological systems with interactions linking biological entities. However, due to their characteristics of high dimensionality and low sample size, directly applying deep learning models on biological networks usually faces severe overfitting. In this work, we propose R-MIXUP, a Mixup-based data augmentation technique that suits the symmetric positive definite (SPD) property of adjacency matrices from biological networks with optimized training efficiency. The interpolation process in R-MIXUP leverages the log-Euclidean distance metrics from the Riemannian manifold, effectively addressing the swelling effect and arbitrarily incorrect label issues of vanilla Mixup. We demonstrate the effectiveness of R-MIXUP with five real-world biological network datasets on both regression and classification tasks. Besides, we derive a commonly ignored necessary condition for identifying the SPD matrices of biological networks and empirically study its influence on the model performance. The code implementation can be found in Appendix D.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2052319438",
                        "name": "Xuan Kan"
                    },
                    {
                        "authorId": "2155907765",
                        "name": "Zimu Li"
                    },
                    {
                        "authorId": "2112821580",
                        "name": "Hejie Cui"
                    },
                    {
                        "authorId": "2218865512",
                        "name": "Yue Yu"
                    },
                    {
                        "authorId": "47462790",
                        "name": "Ran Xu"
                    },
                    {
                        "authorId": "2219270021",
                        "name": "Shaojun Yu"
                    },
                    {
                        "authorId": "2184569534",
                        "name": "Zilong Zhang"
                    },
                    {
                        "authorId": "2153202261",
                        "name": "Ying Guo"
                    },
                    {
                        "authorId": "1390553618",
                        "name": "Carl Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other works study OOD graph classification tasks and can be categorized similarly as above (Zhu et al., 2021b; Miao et al., 2022; Chen et al.; Li et al., 2022a; Han et al., 2022; Yang et al., 2022; Suresh et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4b992456d58113054310eeaee22d5a2db62ccad8",
                "externalIds": {
                    "DBLP": "conf/icml/LiuLFTZQL23",
                    "ArXiv": "2306.03221",
                    "DOI": "10.48550/arXiv.2306.03221",
                    "CorpusId": 259088695
                },
                "corpusId": 259088695,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4b992456d58113054310eeaee22d5a2db62ccad8",
                "title": "Structural Re-weighting Improves Graph Domain Adaptation",
                "abstract": "In many real-world applications, graph-structured data used for training and testing have differences in distribution, such as in high energy physics (HEP) where simulation data used for training may not match real experiments. Graph domain adaptation (GDA) is a method used to address these differences. However, current GDA primarily works by aligning the distributions of node representations output by a single graph neural network encoder shared across the training and testing domains, which may often yield sub-optimal solutions. This work examines different impacts of distribution shifts caused by either graph structure or node attributes and identifies a new type of shift, named conditional structure shift (CSS), which current GDA approaches are provably sub-optimal to deal with. A novel approach, called structural reweighting (StruRW), is proposed to address this issue and is tested on synthetic graphs, four benchmark datasets, and a new application in HEP. StruRW has shown significant performance improvement over the baselines in the settings with large graph structure shifts, and reasonable performance improvement when node attribute shift dominates.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Shikun Liu"
                    },
                    {
                        "authorId": "2160876020",
                        "name": "Tianchun Li"
                    },
                    {
                        "authorId": "11722840",
                        "name": "Yongbin Feng"
                    },
                    {
                        "authorId": "1389912827",
                        "name": "Nhan Tran"
                    },
                    {
                        "authorId": "51209090",
                        "name": "H. Zhao"
                    },
                    {
                        "authorId": "2218979539",
                        "name": "Qiu Qiang"
                    },
                    {
                        "authorId": "1561672016",
                        "name": "Pan Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Baselines For baseline augmentation models, we employ two classical graph augmentation methods: DropEdge [4] and DropNode [5], and three Mixup-based augmentations: SubMix [10], ManifoldMixup (M-Mixup) [8], and G-Mixup [11].",
                "G-mixup [11] mixes graphons [32] of different classes and augments training set by generating the graphs from the mixed graphon.",
                "The detailed statistics of each dataset are shown in Appendix A.\nBaselines For baseline augmentation models, we employ two classical graph augmentation methods: DropEdge [4] and DropNode [5], and three Mixup-based augmentations: SubMix [10], ManifoldMixup (M-Mixup) [8], and G-Mixup [11]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f67035a3798e943ad9ceb5481d8961fb7c56d193",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-01310",
                    "ArXiv": "2306.01310",
                    "DOI": "10.48550/arXiv.2306.01310",
                    "CorpusId": 259063783
                },
                "corpusId": 259063783,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f67035a3798e943ad9ceb5481d8961fb7c56d193",
                "title": "EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost",
                "abstract": "Graph-based models have become increasingly important in various domains, but the limited size and diversity of existing graph datasets often limit their performance. To address this issue, we propose EPIC (Edit Path Interpolation via learnable Cost), a novel interpolation-based method for augmenting graph datasets. Our approach leverages graph edit distance to generate new graphs that are similar to the original ones but exhibit some variation in their structures. To achieve this, we learn the graph edit distance through a comparison of labeled graphs and utilize this knowledge to create graph edit paths between pairs of original graphs. With randomly sampled graphs from a graph edit path, we enrich the training set to enhance the generalization capability of classification models. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing augmentation methods in graph classification tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218725226",
                        "name": "Jaeseung Heo"
                    },
                    {
                        "authorId": "2218943824",
                        "name": "Seungbeom Lee"
                    },
                    {
                        "authorId": "70560338",
                        "name": "Sungsoo Ahn"
                    },
                    {
                        "authorId": "2145138660",
                        "name": "Dongwoo Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", node, link [10, 16], and graph predictions [8, 18]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cc1bb26e46ef1984f784bc460df114dfde72fe86",
                "externalIds": {
                    "ArXiv": "2305.19903",
                    "DBLP": "journals/corr/abs-2305-19903",
                    "DOI": "10.1145/3580305.3599388",
                    "CorpusId": 258987380
                },
                "corpusId": 258987380,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/cc1bb26e46ef1984f784bc460df114dfde72fe86",
                "title": "Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization",
                "abstract": "Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism graphs. Furthermore, the proposed SuperNorm scheme is also demonstrated to alleviate the over-smoothing phenomenon. Experimental results related to predictions of graph, node, and link properties on the eight popular datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/chenchkx/SuperNorm.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145937448",
                        "name": "Kaixuan Chen"
                    },
                    {
                        "authorId": "2128786021",
                        "name": "Shunyu Liu"
                    },
                    {
                        "authorId": null,
                        "name": "Tongtian Zhu"
                    },
                    {
                        "authorId": "2062719264",
                        "name": "Tongya Zheng"
                    },
                    {
                        "authorId": "1739347431",
                        "name": "Haofei Zhang"
                    },
                    {
                        "authorId": "7357719",
                        "name": "Zunlei Feng"
                    },
                    {
                        "authorId": "3764313",
                        "name": "Jingwen Ye"
                    },
                    {
                        "authorId": "2152127912",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, the critical edges connecting different clusters within a graph play a crucial role in tasks like graph partition [16, 28], graph classification [18, 12], and link prediction [55, 50]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "254d8da21bf25b3282e3e486502cc76a732ba74a",
                "externalIds": {
                    "ArXiv": "2305.17437",
                    "DBLP": "journals/corr/abs-2305-17437",
                    "DOI": "10.48550/arXiv.2305.17437",
                    "CorpusId": 258960204
                },
                "corpusId": 258960204,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/254d8da21bf25b3282e3e486502cc76a732ba74a",
                "title": "GIMM: InfoMin-Max for Automated Graph Contrastive Learning",
                "abstract": "Graph contrastive learning (GCL) shows great potential in unsupervised graph representation learning. Data augmentation plays a vital role in GCL, and its optimal choice heavily depends on the downstream task. Many GCL methods with automated data augmentation face the risk of insufficient information as they fail to preserve the essential information necessary for the downstream task. To solve this problem, we propose InfoMin-Max for automated Graph contrastive learning (GIMM), which prevents GCL from encoding redundant information and losing essential information. GIMM consists of two major modules: (1) automated graph view generator, which acquires the approximation of InfoMin's optimal views through adversarial training without requiring task-relevant information; (2) view comparison, which learns an excellent encoder by applying InfoMax to view representations. To the best of our knowledge, GIMM is the first method that combines the InfoMin and InfoMax principles in GCL. Besides, GIMM introduces randomness to augmentation, thus stabilizing the model against perturbations. Extensive experiments on unsupervised and semi-supervised learning for node and graph classification demonstrate the superiority of our GIMM over state-of-the-art GCL methods with automated and manual data augmentation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2180558296",
                        "name": "Xin Xiong"
                    },
                    {
                        "authorId": "71883949",
                        "name": "F. Shen"
                    },
                    {
                        "authorId": "2202725612",
                        "name": "Xiangyu Wang"
                    },
                    {
                        "authorId": "2219044941",
                        "name": "Jian Zhao School of Artificial Intelligence"
                    },
                    {
                        "authorId": "2065676867",
                        "name": "N. University"
                    },
                    {
                        "authorId": "102700312",
                        "name": "School of Materials Science"
                    },
                    {
                        "authorId": "2121187686",
                        "name": "Engineering"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e887b52abd262477aaddef1ea98cbfc359db99ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-15529",
                    "ArXiv": "2305.15529",
                    "DOI": "10.48550/arXiv.2305.15529",
                    "CorpusId": 258888018
                },
                "corpusId": 258888018,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e887b52abd262477aaddef1ea98cbfc359db99ad",
                "title": "Editable Graph Neural Network for Node Classifications",
                "abstract": "Despite Graph Neural Networks (GNNs) have achieved prominent success in many graph-based learning problem, such as credit risk assessment in financial networks and fake news detection in social networks. However, the trained GNNs still make errors and these errors may cause serious negative impact on society. \\textit{Model editing}, which corrects the model behavior on wrongly predicted target samples while leaving model predictions unchanged on unrelated samples, has garnered significant interest in the fields of computer vision and natural language processing. However, model editing for graph neural networks (GNNs) is rarely explored, despite GNNs' widespread applicability. To fill the gap, we first observe that existing model editing methods significantly deteriorate prediction accuracy (up to $50\\%$ accuracy drop) in GNNs while a slight accuracy drop in multi-layer perception (MLP). The rationale behind this observation is that the node aggregation in GNNs will spread the editing effect throughout the whole graph. This propagation pushes the node representation far from its original one. Motivated by this observation, we propose \\underline{E}ditable \\underline{G}raph \\underline{N}eural \\underline{N}etworks (EGNN), a neighbor propagation-free approach to correct the model prediction on misclassified nodes. Specifically, EGNN simply stitches an MLP to the underlying GNNs, where the weights of GNNs are frozen during model editing. In this way, EGNN disables the propagation during editing while still utilizing the neighbor propagation scheme for node prediction to obtain satisfactory results. Experiments demonstrate that EGNN outperforms existing baselines in terms of effectiveness (correcting wrong predictions with lower accuracy drop), generalizability (correcting wrong predictions for other similar nodes), and efficiency (low training time and memory) on various graph datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47781070",
                        "name": "Zirui Liu"
                    },
                    {
                        "authorId": "47653902",
                        "name": "Zhimeng Jiang"
                    },
                    {
                        "authorId": "2181946372",
                        "name": "Shaochen Zhong"
                    },
                    {
                        "authorId": "3364022",
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "authorId": "2156060357",
                        "name": "Li Li"
                    },
                    {
                        "authorId": "1562383795",
                        "name": "Rui Chen"
                    },
                    {
                        "authorId": "2108645988",
                        "name": "Soo-Hyun Choi"
                    },
                    {
                        "authorId": "2148950326",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "096fce503b711e88fe4df9103159ab4cae306a4f",
                "externalIds": {
                    "DOI": "10.1101/2023.05.16.541011",
                    "CorpusId": 258788047,
                    "PubMed": "37798250"
                },
                "corpusId": 258788047,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/096fce503b711e88fe4df9103159ab4cae306a4f",
                "title": "Interpretable modeling of time-resolved single-cell gene-protein expression using CrossmodalNet",
                "abstract": "Cell-surface proteins play a critical role in cell function and are primary targets for therapeutics. CITE-seq is a single-cell technique that enables simultaneous measurement of gene and surface protein expression. It is powerful but costly and technically challenging. Computational methods have been developed to predict surface protein expression using gene expression information such as from single-cell RNA sequencing (scRNA-seq) data. Existing methods however are computationally demanding and lack the interpretability to reveal underlying biological processes. We propose CrossmodalNet, an interpretable machine learning model, to predict surface protein expression from scRNA-seq data. Our model with a customized adaptive loss accurately predicts surface protein abundances. When samples from multiple time points are given, our model encodes temporal information into an easy-to-interpret time embedding to make prediction in a time point-specific manner able to uncover noise-free causal gene-protein relationships. Using two publicly available time-resolved CITE-seq data sets, we validate the performance of our model by comparing it to benchmarking methods and evaluate its interpretability. Together, we show our method accurately and interpretably profiles surface protein expression using scRNA-seq data, thereby expanding the capacity of CITE-seq experiments for investigating molecular mechanisms involving surface proteins.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2142995466",
                        "name": "Yongjian Yang"
                    },
                    {
                        "authorId": "2145832241",
                        "name": "Yu-Te Lin"
                    },
                    {
                        "authorId": "2108252872",
                        "name": "Guanxun Li"
                    },
                    {
                        "authorId": "2087596580",
                        "name": "Yan Zhong"
                    },
                    {
                        "authorId": "2149106377",
                        "name": "Qian Xu"
                    },
                    {
                        "authorId": "2090487688",
                        "name": "James J. Cai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "73d53dc75df83610483d2ac8dd1764c20ba89627",
                "externalIds": {
                    "ArXiv": "2305.12087",
                    "DBLP": "journals/corr/abs-2305-12087",
                    "DOI": "10.1145/3580305.3599497",
                    "CorpusId": 258833666
                },
                "corpusId": 258833666,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/73d53dc75df83610483d2ac8dd1764c20ba89627",
                "title": "Semi-Supervised Graph Imbalanced Regression",
                "abstract": "Data imbalance is easily found in annotated data when the observations of certain continuous label values are difficult to collect for regression tasks. When they come to molecule and polymer property predictions, the annotated graph datasets are often small because labeling them requires expensive equipment and effort. To address the lack of examples of rare label values in graph regression tasks, we propose a semi-supervised framework to progressively balance training data and reduce model bias via self-training. The training data balance is achieved by (1) pseudo-labeling more graphs for under-represented labels with a novel regression confidence measurement and (2) augmenting graph examples in latent space for remaining rare labels after data balancing with pseudo-labels. The former is to identify quality examples from unlabeled data whose labels are confidently predicted and sample a subset of them with a reverse distribution from the imbalanced annotated data. The latter collaborates with the former to target a perfect balance using a novel label-anchored mixup algorithm. We perform experiments in seven regression tasks on graph datasets. Results demonstrate that the proposed framework significantly reduces the error of predicted graph properties, especially in under-represented label areas.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146562364",
                        "name": "Gang Liu"
                    },
                    {
                        "authorId": "1742573",
                        "name": "Tong Zhao"
                    },
                    {
                        "authorId": "2218636086",
                        "name": "Eric Inae"
                    },
                    {
                        "authorId": "2068287187",
                        "name": "Te Luo"
                    },
                    {
                        "authorId": "144812586",
                        "name": "Meng Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, we cannot use Mixup directly because it is suitable for regular, Euclidean data [54], while the user\u2019s rating is discrete and non-interpolative, and there is no label for supervised learning."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b3d46206f727bb732871a8a4ca5597f972c67c02",
                "externalIds": {
                    "DBLP": "journals/pami/WuLGZC23",
                    "DOI": "10.1109/TPAMI.2023.3274759",
                    "CorpusId": 258617914,
                    "PubMed": "37163407"
                },
                "corpusId": 258617914,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b3d46206f727bb732871a8a4ca5597f972c67c02",
                "title": "Influence-Driven Data Poisoning for Robust Recommender Systems",
                "abstract": "Recent studies have shown that recommender systems are vulnerable, and it is easy for attackers to inject well-designed malicious profiles into the system, resulting in biased recommendations. We cannot deprive these data's injection right and deny their existence's rationality, making it imperative to study recommendation robustness. Despite impressive emerging work, threat assessment of the bi-level poisoning problem and the imperceptibility of poisoning users remain key challenges to be solved. To this end, we propose Infmix, an efficient poisoning attack strategy. Specifically, Infmix consists of an influence-based threat estimator and a user generator, Usermix. First, the influence-based estimator can efficiently evaluate the user's harm to the recommender system without retraining, which is challenging for existing attacks. Second, Usermix, a distribution-agnostic generator, can generate unnoticeable fake data even with a few known users. Under the guidance of the threat estimator, Infmix can select the users with large attacking impacts from the quasi-real candidates generated by Usermix. Extensive experiments demonstrate Infmix's superiority by attacking six recommendation systems with four real datasets. Additionally, we propose a novel defense strategy, adversarial poisoning training (APT). It mimics the poisoning process by injecting fake users (ERM users) committed to minimizing empirical risk to build a robust system. Similar to Infmix, we also utilize the influence function to solve the bi-level optimization challenge of generating ERM users. Although the idea of \u201cfighting fire with fire\u201d in APT seems counterintuitive, we prove its effectiveness in improving recommendation robustness through theoretical analysis and empirical experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2000383268",
                        "name": "Chenwang Wu"
                    },
                    {
                        "authorId": "1862782",
                        "name": "Defu Lian"
                    },
                    {
                        "authorId": "2068905418",
                        "name": "Yong Ge"
                    },
                    {
                        "authorId": "2118353327",
                        "name": "Zhihao Zhu"
                    },
                    {
                        "authorId": "2113754294",
                        "name": "Enhong Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "G-Mixup[11] employs graphon to augment graphs and improve graph classifcation task."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bb241315bad727e1df4f8965ea06fda918e4f1cd",
                "externalIds": {
                    "DBLP": "conf/www/ZhouCLSZLYWFW23",
                    "DOI": "10.1145/3543507.3583208",
                    "CorpusId": 258333751
                },
                "corpusId": 258333751,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bb241315bad727e1df4f8965ea06fda918e4f1cd",
                "title": "Multi-Aspect Heterogeneous Graph Augmentation",
                "abstract": "Data augmentation has been widely studied as it can be used to improve the generalizability of graph representation learning models. However, existing works focus only on the data augmentation on homogeneous graphs. Data augmentation for heterogeneous graphs remains under-explored. Considering that heterogeneous graphs contain different types of nodes and links, ignoring the type information and directly applying the data augmentation methods of homogeneous graphs to heterogeneous graphs will lead to suboptimal results. In this paper, we propose a novel Multi-Aspect Heterogeneous Graph Augmentation framework named MAHGA. Specifically, MAHGA consists of two core augmentation strategies: structure-level augmentation and metapath-level augmentation. Structure-level augmentation pays attention to network schema aspect and designs a relation-aware conditional variational auto-encoder that can generate synthetic features of neighbors to augment the nodes and the node types with scarce links. Metapath-level augmentation concentrates on metapath aspect, which constructs metapath reachable graphs for different metapaths and estimates the graphons of them. By sampling and mixing up based on the graphons, MAHGA yields intra-metapath and inter-metapath augmentation. Finally, we conduct extensive experiments on multiple benchmarks to validate the effectiveness of MAHGA. Experimental results demonstrate that our method improves the performances across a set of heterogeneous graph learning models and datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2121226686",
                        "name": "Yuchen Zhou"
                    },
                    {
                        "authorId": "9310727",
                        "name": "Yanan Cao"
                    },
                    {
                        "authorId": "2916386",
                        "name": "Yongchao Liu"
                    },
                    {
                        "authorId": "2031232",
                        "name": "Yanmin Shang"
                    },
                    {
                        "authorId": "47243067",
                        "name": "P. Zhang"
                    },
                    {
                        "authorId": "1390641501",
                        "name": "Zheng Lin"
                    },
                    {
                        "authorId": "2215400626",
                        "name": "Yun Yue"
                    },
                    {
                        "authorId": "2183062007",
                        "name": "Baokun Wang"
                    },
                    {
                        "authorId": "2119032506",
                        "name": "Xingbo Fu"
                    },
                    {
                        "authorId": "2127983409",
                        "name": "Weiqiang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Data enhancement is derived on existing annotated data pictures, mainly using data augmentation methods and advanced data augmentation methods, commonly used data augmentation methods mainly include geometric transformation and color transformation, and advanced data augmentation methods mainly include Mixup [10], Cutout [11], Cutmix [12] and Mosaic [13]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fe82ff3503227057bbfcc010e0eb67fb3d766c11",
                "externalIds": {
                    "DBLP": "conf/ccoms/DongXLL23",
                    "DOI": "10.1109/ICCCS57501.2023.10150849",
                    "CorpusId": 259278731
                },
                "corpusId": 259278731,
                "publicationVenue": {
                    "id": "da18ef0b-933e-4678-b2fa-4290baf6970a",
                    "name": "International Conference on Communication, Computing & Security",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Commun Comput  Secur",
                        "International Conference Communication and Computing Systems",
                        "ICCCS",
                        "International Conference on Computer and Communication Systems",
                        "Int Conf Comput Commun Syst",
                        "Int Conf Commun Comput Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fe82ff3503227057bbfcc010e0eb67fb3d766c11",
                "title": "Object Detection for Rare Birds on the Plateau",
                "abstract": "Object detection has always been a hot research direction in the field of computer vision. At present, most methods are supervised learning methods, but this algorithm requires a large amount of image labeled data, which not only takes time to manually label, but also takes a lot of time when training data. In this paper, the improved object detection network based on yolov3 network is studied, due to the fast inference speed of yolov3, high cost performance and strong versatility, the improved object detection network can identify and locate specific class objects by extracting features by algorithms. In order to improve the performance of detection, before training, the labeled pictures of rare birds on the plateau were augmented to expand the data, and attention mechanisms were added to the last three effective output layers of the backbone network. Finally, the experimental results show that the obtained model has a certain improvement in the picture detection effect of rare birds on the plateau.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220774101",
                        "name": "Deqi Dong"
                    },
                    {
                        "authorId": "2116552347",
                        "name": "Zhijie Xiao"
                    },
                    {
                        "authorId": "2220802642",
                        "name": "Lulian Liu"
                    },
                    {
                        "authorId": "2144442860",
                        "name": "Xiao-Di Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Graph augmentation modifies the overall structure of the graph, and can be seen as a combination of the previous methods [Han et al., 2022]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "23cc9e97df98173eeb966c6d69809ff43e7fee3a",
                "externalIds": {
                    "ArXiv": "2304.10045",
                    "DBLP": "journals/corr/abs-2304-10045",
                    "DOI": "10.48550/arXiv.2304.10045",
                    "CorpusId": 258236333
                },
                "corpusId": 258236333,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/23cc9e97df98173eeb966c6d69809ff43e7fee3a",
                "title": "ID-MixGCL: Identity Mixup for Graph Contrastive Learning",
                "abstract": "Recently developed graph contrastive learning (GCL) approaches compare two different\"views\"of the same graph in order to learn node/graph representations. The core assumption of these approaches is that by graph augmentation, it is possible to generate several structurally different but semantically similar graph structures, and therefore, the identity labels of the original and augmented graph/nodes should be identical. However, in this paper, we observe that this assumption does not always hold, for example, any perturbation to nodes or edges in a molecular graph will change the graph labels to some degree. Therefore, we believe that augmenting the graph structure should be accompanied by an adaptation of the labels used for the contrastive loss. Based on this idea, we propose ID-MixGCL, which allows for simultaneous modulation of both the input graph and the corresponding identity labels, with a controllable degree of change, leading to the capture of fine-grained representations from unlabeled graphs. Experimental results demonstrate that ID-MixGCL improves performance on graph classification and node classification tasks, as demonstrated by significant improvements on the Cora, IMDB-B, and IMDB-M datasets compared to state-of-the-art techniques, by 3-29% absolute points.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2151251543",
                        "name": "Ge Zhang"
                    },
                    {
                        "authorId": "48613402",
                        "name": "Yu Bowen"
                    },
                    {
                        "authorId": "2115871859",
                        "name": "Jiangxia Cao"
                    },
                    {
                        "authorId": "49468918",
                        "name": "Xinghua Zhang"
                    },
                    {
                        "authorId": "121698165",
                        "name": "Tingwen Liu"
                    },
                    {
                        "authorId": "1857210",
                        "name": "Chuan Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[17,2,4] used mixup based techniques to augment the graph data so as to improve the training performance."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "cf70b057c1b43e7c38b7627b5d9f931a555b6a29",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-05749",
                    "ArXiv": "2304.05749",
                    "DOI": "10.48550/arXiv.2304.05749",
                    "CorpusId": 258078827
                },
                "corpusId": 258078827,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cf70b057c1b43e7c38b7627b5d9f931a555b6a29",
                "title": "Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation",
                "abstract": "This study focuses on long-term forecasting (LTF) on continuous-time dynamic graph networks (CTDGNs), which is important for real-world modeling. Existing CTDGNs are effective for modeling temporal graph data due to their ability to capture complex temporal dependencies but perform poorly on LTF due to the substantial requirement for historical data, which is not practical in most cases. To relieve this problem, a most intuitive way is data augmentation. In this study, we propose \\textbf{\\underline{U}ncertainty \\underline{M}asked \\underline{M}ix\\underline{U}p (UmmU)}: a plug-and-play module that conducts uncertainty estimation to introduce uncertainty into the embedding of intermediate layer of CTDGNs, and perform masked mixup to further enhance the uncertainty of the embedding to make it generalize to more situations. UmmU can be easily inserted into arbitrary CTDGNs without increasing the number of parameters. We conduct comprehensive experiments on three real-world dynamic graph datasets, the results demonstrate that UmmU can effectively improve the long-term forecasting performance for CTDGNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152948436",
                        "name": "Yu Tian"
                    },
                    {
                        "authorId": "49716252",
                        "name": "Mingjie Zhu"
                    },
                    {
                        "authorId": "2214242897",
                        "name": "Jiachi Luo"
                    },
                    {
                        "authorId": "2214164425",
                        "name": "Song Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Techniques like random graph data augmentations (e.g., edge and node dropping) (Han et al., 2022; Liu et al., 2022) and large-scale pre-training on diverse graphs (You et al., 2020a;b; 2021; Hou et al., 2022) have been widely adopted to augment the diversity of training graph structures.",
                ", edge and node dropping) (Han et al., 2022; Liu et al., 2022) and large-scale pre-training on diverse graphs (You et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "737b5bbbc22913949c7d31157ac2dedfb039003a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-02806",
                    "ArXiv": "2304.02806",
                    "DOI": "10.48550/arXiv.2304.02806",
                    "CorpusId": 257985095
                },
                "corpusId": 257985095,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/737b5bbbc22913949c7d31157ac2dedfb039003a",
                "title": "Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling",
                "abstract": "Graph neural networks (GNNs) have been widely applied to learning over graph data. Yet, real-world graphs commonly exhibit diverse graph structures and contain heterogeneous nodes and edges. Moreover, to enhance the generalization ability of GNNs, it has become common practice to further increase the diversity of training graph structures by incorporating graph augmentations and/or performing large-scale pre-training on more graphs. Therefore, it becomes essential for a GNN to simultaneously model diverse graph structures. Yet, naively increasing the GNN model capacity will suffer from both higher inference costs and the notorious trainability issue of GNNs. This paper introduces the Mixture-of-Expert (MoE) idea to GNNs, aiming to enhance their ability to accommodate the diversity of training graph structures, without incurring computational overheads. Our new Graph Mixture of Expert (GMoE) model enables each node in the graph to dynamically select its own optimal \\textit{information aggregation experts}. These experts are trained to model different subgroups of graph structures in the training set. Additionally, GMoE includes information aggregation experts with varying aggregation hop sizes, where the experts with larger hop sizes are specialized in capturing information over longer ranges. The effectiveness of GMoE is verified through experimental results on a large variety of graph, node, and link prediction tasks in the OGB benchmark. For instance, it enhances ROC-AUC by $1.81\\%$ in ogbg-molhiv and by $1.40\\%$ in ogbg-molbbbp, as compared to the non-MoE baselines. Our code is available at https://github.com/VITA-Group/Graph-Mixture-of-Experts.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "113727681",
                        "name": "Haotao Wang"
                    },
                    {
                        "authorId": "152420547",
                        "name": "Ziyu Jiang"
                    },
                    {
                        "authorId": "2153216777",
                        "name": "Yan Han"
                    },
                    {
                        "authorId": "2156070723",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Literaturelly, graphon has been studied from two perspectives: as limit of graph sequence, and as graph generators[1, 11, 24].",
                "[3] mentioned the same motifs were also found from bacteria [11] to yeast [29], animal [32] to plants."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "dbf615e93b6a4dc7cdfa9fe551a64a65f20324bd",
                "externalIds": {
                    "ArXiv": "2303.16458",
                    "DBLP": "conf/kdd/CaoXYWZ0C023",
                    "DOI": "10.1145/3580305.3599548",
                    "CorpusId": 258840938
                },
                "corpusId": 258840938,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/dbf615e93b6a4dc7cdfa9fe551a64a65f20324bd",
                "title": "When to Pre-Train Graph Neural Networks? From Data Generation Perspective!",
                "abstract": "In recent years, graph pre-training has gained significant attention, focusing on acquiring transferable knowledge from unlabeled graph data to improve downstream performance. Despite these recent endeavors, the problem of negative transfer remains a major concern when utilizing graph pre-trained models to downstream tasks. Previous studies made great efforts on the issue of what to pre-train and how to pre-train by designing a variety of graph pre-training and fine-tuning strategies. However, there are cases where even the most advanced \"pre-train and fine-tune\" paradigms fail to yield distinct benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN first fits the pre-training data into graphon bases, each element of graphon basis (i.e., a graphon) identifies a fundamental transferable pattern shared by a collection of pre-training graphs. All convex combinations of graphon bases give rise to a generator space, from which graphs generated form the solution space for those downstream data that can benefit from pre-training. In this manner, the feasibility of pre-training can be quantified as the generation probability of the downstream data from any generator in the generator space. W2PGNN offers three broad applications: providing the application scope of graph pre-trained models, quantifying the feasibility of pre-training, and assistance in selecting pre-training data to enhance downstream performance. We provide a theoretically sound solution for the first application and extensive empirical justifications for the latter two applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144149886",
                        "name": "Yu Cao"
                    },
                    {
                        "authorId": "49394731",
                        "name": "Jiarong Xu"
                    },
                    {
                        "authorId": "1390553618",
                        "name": "Carl Yang"
                    },
                    {
                        "authorId": "2118328782",
                        "name": "Jiaan Wang"
                    },
                    {
                        "authorId": "2120330528",
                        "name": "Yunchao Zhang"
                    },
                    {
                        "authorId": "2201480815",
                        "name": "Chunping Wang"
                    },
                    {
                        "authorId": "144595980",
                        "name": "L. Chen"
                    },
                    {
                        "authorId": "2152915866",
                        "name": "Yang Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026learning methods including self-training with selected unlabeled graphs (ST-REAL) and generated graphs (ST-GEN) and INFOGRAPH (Sun et al., 2020), and (3) graph data augmentation (GDA) methods including FLAG (Kong et al., 2022), GREA (Liu et al., 2022), and G-MIXUP (Han et al., 2022).",
                "Generation models (Antoniou et al., 2017; Bowles et al., 2018; Han et al., 2022) create in-class examples.",
                "GDA (GREA and G-MIXUP) methods outperform self-training in most classification tasks except ogbg-SIDER, because they are often designed to exploit categorical labeled data and remain underexplored for regression.",
                "They learn to create new examples that preserve the properties of original graphs (Kong et al., 2022; Han et al., 2022; Luo et al., 2022).",
                "Baselines and implementation: Besides GIN, there are three lines of baseline methods: (1) selfsupervised learing methods including EDGEPRED, ATTRMASK, CONTEXTPRED in (Hu et al., 2019), INFOMAX (Velickovic et al., 2019), JOAO (You et al., 2021), GRAPHLOG (Xu et al., 2021), and D-SLA (Kim et al., 2022), (2) semi-supervised learning methods including self-training with selected unlabeled graphs (ST-REAL) and generated graphs (ST-GEN) and INFOGRAPH (Sun et al., 2020), and (3) graph data augmentation (GDA) methods including FLAG (Kong et al., 2022), GREA (Liu et al., 2022), and G-MIXUP (Han et al., 2022).",
                "\u2191 ogbg-HIV ogbg-ToxCast ogbg-Tox21 ogbg-BBBP ogbg-BACE ogbg-ClinTox ogbg-SIDER\n# Training Graphs 32,901 6,860 6,264 1,631 1,210 1,181 1,141\nGIN 77.4(1.2) 66.9(0.2) 76.0(0.6) 67.5(2.7) 77.5(2.8) 88.8(3.8) 58.1(0.9)\nSe lf-\nSu pe\nrv is\ned EDGEPRED 78.1(1.3) 63.9(0.4) 75.5(0.4) 69.9(0.5) 79.5(1.0) 62.9(2.3) 59.7(0.8) ATTRMASK 77.1(1.7) 64.2(0.5) 76.6(0.4) 63.9(1.2) 79.3(0.7) 70.4(1.1) 60.7(0.4) CONTEXTPRED 78.4(0.1) 63.7(0.3) 75.0(0.1) 68.8(1.6) 75.7(1.0) 63.2(6.5) 60.7(0.8) INFOMAX 75.4(1.8) 61.7(1.0) 75.5(0.4) 69.2(0.5) 76.8(0.2) 73.0(0.2) 58.6(0.5) JOAO 76.2(0.2) 64.8(0.3) 74.8(0.5) 69.3(2.5) 75.9(3.9) 69.4(4.5) 60.8(0.6) GRAPHLOG 74.8(1.1) 63.2(0.8) 75.4(0.8) 67.5(2.3) 80.4(3.6) 69.0(6.6) 57.0(0.9) D-SLA 76.9(0.9) 60.8(1.2) 76.1(0.1) 62.6(1.0) 80.3(0.6) 78.3(2.4) 55.1(1.0)\nSe m\niSL INFOGRAPH 73.3(0.7) 61.5(1.1) 67.6(0.9) 61.6(4.4) 75.9(1.8) 62.2(5.5) 56.3(2.3)\nST-REAL 78.3(0.6) 64.5(1.0) 76.2(0.5) 66.7(1.9) 77.4(1.8) 82.2(2.4) 60.8(1.2) ST-GEN 77.9(1.6) 65.1(1.0) 75.8(0.9) 66.3(1.5) 78.4(3.0) 87.3(1.3) 59.3(1.3)\nG D A FLAG 74.6(1.7) 59.9(1.6) 76.9(0.7) 66.6(1.0) 79.1(1.2) 85.1(3.4) 57.6(2.3) GREA 79.3(0.9) 67.5(0.7) 77.2(1.2) 69.7(1.3) 82.4(2.4) 87.9(3.7) 60.1(2.0) G-MIXUP 77.1(1.1) 55.6(1.1) 64.6(0.4) 70.2(1.0) 77.8(3.3) 60.2(7.5) 56.8(3.5)\nDCT (Ours) 79.5(1.0) 68.1(0.2) 78.2(0.2) 70.8(0.5) 85.6(0.6) 92.1(0.8) 63.9(0.3) Molecule Regression: MAE \u2193 Polymer Regression: MAE \u2193 Bio: AUC (%)\u2191\nogbg-Lipo ogbg-ESOL ogbg-FreeSolv GlassTemp MeltingTemp ThermCond O2Perm PPI # Training Graphs 3,360 902 513 4,303 2,189 455 356 60,715\nGIN 0.545(0.019) 0.766(0.016) 1.639(0.146) 26.4(0.2) 40.9(2.2) 3.25(0.19) 201.3(45.0) 69.1(0.0)\nSe lf-\nSu pe\nrv is\ned EDGEPRED 0.585(0.008) 1.062(0.066) 2.249(0.150) 27.6(1.4) 47.4(2.8) 3.69(0.50) 207.3(41.7) 63.7(1.1) ATTRMASK 0.573(0.009) 1.041(0.041) 1.952(0.088) 27.7(0.8) 45.8(2.6) 3.17(0.32) 179.9(30.8) 64.1(1.8) CONTEXTPRED 0.592(0.007) 0.971(0.027) 2.193(0.151) 27.6(0.3) 46.7(1.9) 3.15(0.24) 191.2(35.2) 62.0(1.2) INFOMAX 0.581(0.009) 0.935(0.018) 2.197(0.129) 27.5(0.8) 46.5(2.8) 3.31(0.25) 231.0(52.6) 63.3(1.2) JOAO 0.596(0.016) 1.098(0.037) 2.465(0.095) 27.5(0.2) 46.0(0.2) 3.55(0.26) 207.7(43.7) 61.5(1.2) GRAPHLOG 0.577(0.010) 1.109(0.059) 2.373(0.283) 29.5(1.3) 50.3(3.3) 3.01(0.17) 229.7(48.3) 62.1(0.6) D-SLA 0.563(0.004) 1.064(0.030) 2.190(0.149) 27.5(1.0) 51.7(2.5) 2.71(0.08) 257.8(30.2) 65.0(1.2)\nSe m\niSL INFOGRAPH 0.793(0.094) 1.285(0.093) 3.710(0.418) 30.8(1.2) 51.2(5.1) 2.75(0.15) 207.2(21.8) 67.7(0.4)\nST-REAL 0.526(0.009) 0.788(0.070) 1.770(0.251) 26.6(0.3) 42.3(1.2) 2.64(0.07) 256.0(17.5) 68.9(0.1) ST-GEN 0.531(0.031) 0.724(0.082) 1.547(0.082) 26.8(0.3) 42.0(0.9) 2.70(0.03) 262.2(10.1) 68.6(0.6)\nG D A FLAG 0.528(0.012) 0.755(0.039) 1.565(0.098) 26.6(1.3) 44.2(2.0) 3.05(0.10) 177.7(60.7) 69.2(0.2) GREA 0.586(0.036) 0.805(0.135) 1.829(0.368) 26.7(1.0) 41.1(0.8) 3.23(0.18) 194.0(45.5) 68.8(0.2)\nDCT (Ours) 0.516(0.071) 0.717(0.020) 1.339(0.075) 23.7(0.2) 38.0(0.8) 2.59(0.11) 165.6(24.3) 69.5(0.2)\n(ogbg-HIV, ogbg-ToxCast, ogbg-Tox21, ogbg-BBBP, ogbg-BACE, ogbg-ClinTox, ogbg-SIDER), three molecule regression tasks (ogbg-Lipo, ogbg-ESOL, ogbg-FreeSolv) from open graph benchmarks (Hu et al., 2020), four polymer regression tasks (GlassTemp, MeltingTemp, O2Perm, and thermal conductivity prediction ThermCond), and also protein function prediction (PPI) (Hu et al., 2019).",
                "The learning to augment approaches learn from labeled graphs to perturb graph structures (Luo et al., 2022), to estimate graphons for different classes (Han et al., 2022), or to split the latent space for augmentation (Liu et al., 2022)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "313dc05e245b31438fd5c18702beac6c7ad39e98",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-10108",
                    "ArXiv": "2303.10108",
                    "DOI": "10.48550/arXiv.2303.10108",
                    "CorpusId": 257622605
                },
                "corpusId": 257622605,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/313dc05e245b31438fd5c18702beac6c7ad39e98",
                "title": "Data-Centric Learning from Unlabeled Graphs with Diffusion Model",
                "abstract": "Graph property prediction tasks are important and numerous. While each task offers a small size of labeled examples, unlabeled graphs have been collected from various sources and at a large scale. A conventional approach is training a model with the unlabeled graphs on self-supervised tasks and then fine-tuning the model on the prediction tasks. However, the self-supervised task knowledge could not be aligned or sometimes conflicted with what the predictions needed. In this paper, we propose to extract the knowledge underlying the large set of unlabeled graphs as a specific set of useful data points to augment each property prediction model. We use a diffusion model to fully utilize the unlabeled graphs and design two new objectives to guide the model's denoising process with each task's labeled data to generate task-specific graph examples and their labels. Experiments demonstrate that our data-centric approach performs significantly better than fourteen existing various methods on fifteen tasks. The performance improvement brought by unlabeled data is visible as the generated labeled examples unlike self-supervised learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146562364",
                        "name": "Gang Liu"
                    },
                    {
                        "authorId": "2186434931",
                        "name": "Eric Inae"
                    },
                    {
                        "authorId": "1742573",
                        "name": "Tong Zhao"
                    },
                    {
                        "authorId": "2110506035",
                        "name": "Jiaxin Xu"
                    },
                    {
                        "authorId": "2068287187",
                        "name": "Te Luo"
                    },
                    {
                        "authorId": "144812586",
                        "name": "Meng Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Augmentation Basic manipulation Automation Programmatic [42, 93, 250, 282, 282, 288].",
                "For example, compared to image data, graph data is irregular and not well-aligned, and thus the vanilla Mixup strategy can not be directly applied [93].",
                "Beyond image data, basic manipulation often needs to be tailored for the other data types, such as permutation and jittering in time-series data [250], mixing data in the hidden space for text data to retain semantic meanings [42], and mixing graphon for graph data [93]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "12c6be503e4e5b7c9cb1810152d4364f26628a8d",
                "externalIds": {
                    "ArXiv": "2303.10158",
                    "DBLP": "journals/corr/abs-2303-10158",
                    "DOI": "10.48550/arXiv.2303.10158",
                    "CorpusId": 257622614
                },
                "corpusId": 257622614,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/12c6be503e4e5b7c9cb1810152d4364f26628a8d",
                "title": "Data-centric Artificial Intelligence: A Survey",
                "abstract": "Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1759658",
                        "name": "D. Zha"
                    },
                    {
                        "authorId": "2122929218",
                        "name": "Zaid Pervaiz Bhat"
                    },
                    {
                        "authorId": "51238382",
                        "name": "Kwei-Herng Lai"
                    },
                    {
                        "authorId": "47829900",
                        "name": "Fan Yang"
                    },
                    {
                        "authorId": "47653902",
                        "name": "Zhimeng Jiang"
                    },
                    {
                        "authorId": "2181946372",
                        "name": "Shaochen Zhong"
                    },
                    {
                        "authorId": "2109724398",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mixup Improves Generalization After the initial work of Zhang et al. (2018), a series of the Mixup\u2019s variants have been proposed (Guo et al., 2019a; Verma et al., 2019; Yun et al., 2019; Kim et al., 2020; Greenewald et al., 2021; Han et al., 2022; Sohn et al., 2022).",
                "(2018), a series of the Mixup\u2019s variants have been proposed (Guo et al., 2019a; Verma et al., 2019; Yun et al., 2019; Kim et al., 2020; Greenewald et al., 2021; Han et al., 2022; Sohn et al., 2022)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "99486d35169e1937b8ff9e5abd7227b6d1e901b9",
                "externalIds": {
                    "ArXiv": "2303.01475",
                    "DBLP": "conf/iclr/LiuWGM23",
                    "DOI": "10.48550/arXiv.2303.01475",
                    "CorpusId": 254198988
                },
                "corpusId": 254198988,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/99486d35169e1937b8ff9e5abd7227b6d1e901b9",
                "title": "Over-training with Mixup May Hurt Generalization",
                "abstract": "Mixup, which creates synthetic training instances by linearly interpolating random sample pairs, is a simple and yet effective regularization technique to boost the performance of deep models trained with SGD. In this work, we report a previously unobserved phenomenon in Mixup training: on a number of standard datasets, the performance of Mixup-trained models starts to decay after training for a large number of epochs, giving rise to a U-shaped generalization curve. This behavior is further aggravated when the size of original dataset is reduced. To help understand such a behavior of Mixup, we show theoretically that Mixup training may introduce undesired data-dependent label noises to the synthesized data. Via analyzing a least-square regression problem with a random feature model, we explain why noisy labels may cause the U-shaped curve to occur: Mixup improves generalization through fitting the clean patterns at the early training stage, but as training progresses, Mixup becomes over-fitting to the noise in the synthetic data. Extensive experiments are performed on a variety of benchmark datasets, validating this explanation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1645199888",
                        "name": "Zixuan Liu"
                    },
                    {
                        "authorId": "2117428102",
                        "name": "Ziqiao Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More specifically, inspired by theMixup technology employed in the field of computer vision [35] [5], we first generate new negative samples\u2019 representation by mixing positive sample embeddings into the negative sample embedding."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2c97e98000c559f396f3b300ca7f65e6aaa699a7",
                "externalIds": {
                    "DBLP": "conf/wsdm/ChenGJH23",
                    "DOI": "10.1145/3539597.3570424",
                    "CorpusId": 257079704
                },
                "corpusId": 257079704,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/2c97e98000c559f396f3b300ca7f65e6aaa699a7",
                "title": "Relation Preference Oriented High-order Sampling for Recommendation",
                "abstract": "The introduction of knowledge graphs (KG) into recommendation systems (RS) has been proven to be effective because KG introduces a variety of relations between items. In fact, users have different relation preferences depending on the relationship in KG. Existing GNN-based models largely adopt random neighbor sampling strategies to process propagation; however, these models cannot aggregate biased relation preference local information for a specific user, and thus cannot effectively reveal the internal relationship between users' preferences. This will reduce the accuracy of recommendations, while also limiting the interpretability of the results. Therefore, we propose a Relation Preference oriented High-order Sampling (RPHS) model to dynamically sample subgraphs based on relation preference and hard negative samples for user-item pairs. We design a path sampling strategy based on relation preference, which can encode the critical paths between specific user-item pairs to sample the paths in the high-order message passing subgraphs. Next, we design a mixed sampling strategy and define a new propagation operation to further enhance RPHS's ability to distinguish negative signals. Through the above sampling strategies, our model can better aggregate local relation preference information and reveal the internal relationship between users' preferences. Experiments show that our model outperforms the state-of-the-art models on three datasets by 14.98%, 5.31%, and 8.65%, and also performs well in terms of interpretability. The codes are available at https://github.com/RPHS/RPHS.git",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108227988",
                        "name": "Mukun Chen"
                    },
                    {
                        "authorId": "2007190",
                        "name": "Xiuwen Gong"
                    },
                    {
                        "authorId": "2209348709",
                        "name": "YH Jin"
                    },
                    {
                        "authorId": "2146226874",
                        "name": "Wenbin Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some strong baselines such as G-MIXUP (Han et al., 2022) were excluded because they require labels during the pre-training phase."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "58810ee6ea9f9d40816419a2f0ec3ed3c45a8458",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-02909",
                    "ArXiv": "2302.02909",
                    "DOI": "10.48550/arXiv.2302.02909",
                    "CorpusId": 256615731
                },
                "corpusId": 256615731,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/58810ee6ea9f9d40816419a2f0ec3ed3c45a8458",
                "title": "Spectral Augmentations for Graph Contrastive Learning",
                "abstract": "Contrastive learning has emerged as a premier method for learning representations with or without supervision. Recent studies have shown its utility in graph representation learning for pre-training. Despite successes, the understanding of how to design effective graph augmentations that can capture structural properties common to many different types of downstream graphs remains incomplete. We propose a set of well-motivated graph transformation operations derived via graph spectral analysis to provide a bank of candidates when constructing augmentations for a graph contrastive objective, enabling contrastive learning to capture useful structural representation from pre-training graph datasets. We first present a spectral graph cropping augmentation that involves filtering nodes by applying thresholds to the eigenvalues of the leading Laplacian eigenvectors. Our second novel augmentation reorders the graph frequency components in a structural Laplacian-derived position graph embedding. Further, we introduce a method that leads to improved views of local subgraphs by performing alignment via global random walk embeddings. Our experimental results indicate consistent improvements in out-of-domain graph data transfer compared to state-of-the-art graph contrastive learning methods, shedding light on how to design a graph learner that is able to learn structural properties common to diverse graph types.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "81407699",
                        "name": "Amur Ghose"
                    },
                    {
                        "authorId": "2135319291",
                        "name": "Yingxue Zhang"
                    },
                    {
                        "authorId": "2069718816",
                        "name": "Jianye Hao"
                    },
                    {
                        "authorId": "2150349871",
                        "name": "Mark Coates"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is worth noting that the drop edge technique we use here is different to the standard data augmentation techniques such as DropEdge (Rong et al., 2019), and G-Mixup (Han et al., 2022b), which either add slightly modified copies of existing data or generate synthetic based on existing data.",
                "\u2026et al., 2015; Gilmer et al., 2017), physics (Cranmer et al., 2019; Bapst et al., 2020), transportation (Derrow-Pinion et al., 2021), vision (Han et al., 2022a), natural language processing (NLP) (Wu et al., 2021a), knowledge graphs (Schlichtkrull et al., 2018), drug design (Stokes et al.,\u2026",
                "It is worth noting that the drop edge technique we use here is different to the standard data augmentation techniques such as DropEdge [77], and G-Mixup [78], which either add slightly modified copies of existing data or generate synthetic based on existing data."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "517802b9381246dff16756fe5299fa62bb29e228",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-13350",
                    "ArXiv": "2212.13350",
                    "DOI": "10.48550/arXiv.2212.13350",
                    "CorpusId": 255186280
                },
                "corpusId": 255186280,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/517802b9381246dff16756fe5299fa62bb29e228",
                "title": "A Generalization of ViT/MLP-Mixer to Graphs",
                "abstract": "Graph Neural Networks (GNNs) have shown great potential in the field of graph representation learning. Standard GNNs define a local message-passing mechanism which propagates information over the whole graph domain by stacking multiple layers. This paradigm suffers from two major limitations, over-squashing and poor long-range dependencies, that can be solved using global attention but significantly increases the computational cost to quadratic complexity. In this work, we propose an alternative approach to overcome these structural limitations by leveraging the ViT/MLP-Mixer architectures introduced in computer vision. We introduce a new class of GNNs, called Graph ViT/MLP-Mixer, that holds three key properties. First, they capture long-range dependency and mitigate the issue of over-squashing as demonstrated on Long Range Graph Benchmark and TreeNeighbourMatch datasets. Second, they offer better speed and memory efficiency with a complexity linear to the number of nodes and edges, surpassing the related Graph Transformer and expressive GNN models. Third, they show high expressivity in terms of graph isomorphism as they can distinguish at least 3-WL non-isomorphic graphs. We test our architecture on 4 simulated datasets and 7 real-world benchmarks, and show highly competitive results on all of them. The source code is available for reproducibility at: \\url{https://github.com/XiaoxinHe/Graph-ViT-MLPMixer}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154302372",
                        "name": "Xiaoxin He"
                    },
                    {
                        "authorId": "2019961",
                        "name": "Bryan Hooi"
                    },
                    {
                        "authorId": "81634721",
                        "name": "T. Laurent"
                    },
                    {
                        "authorId": "2198488555",
                        "name": "Adam Perold"
                    },
                    {
                        "authorId": "1688882",
                        "name": "Yann LeCun"
                    },
                    {
                        "authorId": "2549032",
                        "name": "X. Bresson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Graph [218] Cora/Citeseer/Pubmed [219] [220], [221], [222] [223], [224], [225] Cora-Full [226] [220], [221] Coauthor [227], [227] [220], [221], [222] IMDB/Reddit [228] [222], [229] BlogCatalog [230] [223], [225]",
                "G-Mixup [229] generates synthetic graphs by interpolating sampled graphons in the Euclidean space, which is a generator estimated for each class."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0d110d199d74a6fae14c9edb3492bd02bf0a517f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-10888",
                    "ArXiv": "2212.10888",
                    "DOI": "10.48550/arXiv.2212.10888",
                    "CorpusId": 254926498
                },
                "corpusId": 254926498,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0d110d199d74a6fae14c9edb3492bd02bf0a517f",
                "title": "A Survey of Mix-based Data Augmentation: Taxonomy, Methods, Applications, and Explainability",
                "abstract": "Data augmentation (DA) is indispensable in modern machine learning and deep neural networks. The basic idea of DA is to construct new training data to improve the model's generalization by adding slightly disturbed versions of existing data or synthesizing new data. In this work, we review a small but essential subset of DA -- Mix-based Data Augmentation (MixDA) that generates novel samples by mixing multiple examples. Unlike conventional DA approaches based on a single-sample operation or requiring domain knowledge, MixDA is more general in creating a broad spectrum of new data and has received increasing attention in the community. We begin with proposing a new taxonomy classifying MixDA into, Mixup-based, Cutmix-based, and hybrid approaches according to a hierarchical view of the data mix. Various MixDA techniques are then comprehensively reviewed in a more fine-grained way. Owing to its generalization, MixDA has penetrated a variety of applications which are also completely reviewed in this work. We also examine why MixDA works from different aspects of improving model performance, generalization, and calibration while explaining the model behavior based on the properties of MixDA. Finally, we recapitulate the critical findings and fundamental challenges of current MixDA studies, and outline the potential directions for future works. Different from previous related works that summarize the DA approaches in a specific domain (e.g., images or natural language processing) or only review a part of MixDA studies, we are the first to provide a systematical survey of MixDA in terms of its taxonomy, methodology, applications, and explainability. This work can serve as a roadmap to MixDA techniques and application reviews while providing promising directions for researchers interested in this exciting area.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "121135550",
                        "name": "Chengtai Cao"
                    },
                    {
                        "authorId": null,
                        "name": "Fan Zhou"
                    },
                    {
                        "authorId": "2143178343",
                        "name": "Yurou Dai"
                    },
                    {
                        "authorId": "2110324733",
                        "name": "Jianping Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "com/gasteigerjo/gdc [13] G-mixup ICML 2022 GI https://github."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5562a6d5d72e4b576123c6848d02afa00dcae0ac",
                "externalIds": {
                    "ArXiv": "2212.09970",
                    "CorpusId": 257037997
                },
                "corpusId": 257037997,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5562a6d5d72e4b576123c6848d02afa00dcae0ac",
                "title": "Data Augmentation on Graphs: A Technical Survey",
                "abstract": "In recent years, graph representation learning has achieved remarkable success while suffering from low-quality data problems. As a mature technology to improve data quality in computer vision, data augmentation has also attracted increasing attention in graph domain. For promoting the development of this emerging research direction, in this survey, we comprehensively review and summarize the existing graph data augmentation (GDAug) techniques. Specifically, we first summarize a variety of feasible taxonomies, and then classify existing GDAug studies based on fine-grained graph elements. Furthermore, for each type of GDAug technique, we formalize the general definition, discuss the technical details, and give schematic illustration. In addition, we also summarize common performance metrics and specific design metrics for constructing a GDAug evaluation system. Finally, we summarize the applications of GDAug from both data and model levels, as well as future directions. Latest advances in GDAug are summarized in a GitHub repository: https://github.com/jjzhou012/GDAug-Survey.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146367628",
                        "name": "Jiajun Zhou"
                    },
                    {
                        "authorId": "2197076698",
                        "name": "Chenxuan Xie"
                    },
                    {
                        "authorId": "1788122",
                        "name": "Z. Wen"
                    },
                    {
                        "authorId": "2197532318",
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "authorId": "2159202690",
                        "name": "Qi Xuan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is important to note that variations of mixup for Graph Neural Networks have been applied in previous papers [6], but they deal with graph datasets that have fluctuating numbers of nodes that come in different order that require far more complex implementations.",
                "G-Mixup [6] applies the principles of mixup to probability matrices with individual points representing the likelihood of an edge existing between two nodes."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "e49569b4ecfb522bb96b310fd5f7534cfb4fe651",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/Li22a",
                    "DOI": "10.1109/BigData55660.2022.10020662",
                    "CorpusId": 253965682
                },
                "corpusId": 253965682,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e49569b4ecfb522bb96b310fd5f7534cfb4fe651",
                "title": "BrainMixup: Data Augmentation for GNN-based Functional Brain Network Analysis",
                "abstract": "Different data augmentation techniques such as mixup have been applied to Graph Networks to reduce overfitting. Many graph mixup techniques have complicated implementations to deal with graph data\u2019s non-Euclidian data structure. However, not many papers have explored applying traditional mixup techniques to brain data that has the unique capacity in that nodes are defined in place by their ROIs. Thus, this paper proposes BrainGMixup, applying previously proposed mixup techniques meant for 1-D feature vectors to 2-D graph node features and edge index matrices. The results and experiments in this paper demonstrate that applying this simple modified mixup increases a model like EdgeConv\u2019 s ability to generalize on the training data and boosts its performance on a variety of metrics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2180546292",
                        "name": "Alex J. Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another study (Han et al., 2022) have adapted the mixup technique and introduced GMixup to improve graph classification robustness on GNNs.",
                "Another study (Han et al., 2022) have adapted the mixup technique and introduced GMixup to improve graph classification robustness on GNNs. G-Mixup makes use of graphons of a specific class as generators."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bca6b2c5ab058b55b3cf7efd5acb509d56214762",
                "externalIds": {
                    "ArXiv": "2212.07790",
                    "DBLP": "journals/corr/abs-2212-07790",
                    "DOI": "10.48550/arXiv.2212.07790",
                    "CorpusId": 254685921
                },
                "corpusId": 254685921,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bca6b2c5ab058b55b3cf7efd5acb509d56214762",
                "title": "Population Template-Based Brain Graph Augmentation for Improving One-Shot Learning Classification",
                "abstract": "The challenges of collecting medical data on neurological disorder diagnosis problems paved the way for learning methods with scarce number of samples. Due to this reason, one-shot learning still remains one of the most challenging and trending concepts of deep learning as it proposes to simulate the human-like learning approach in classification problems. Previous studies have focused on generating more accurate fingerprints of the population using graph neural networks (GNNs) with connectomic brain graph data. Thereby, generated population fingerprints named connectional brain template (CBTs) enabled detecting discriminative bio-markers of the population on classification tasks. However, the reverse problem of data augmentation from single graph data representing brain connectivity has never been tackled before. In this paper, we propose an augmentation pipeline in order to provide improved metrics on our binary classification problem. Divergently from the previous studies, we examine augmentation from a single population template by utilizing graph-based generative adversarial network (gGAN) architecture for a classification problem. We benchmarked our proposed solution on AD/LMCI dataset consisting of brain connectomes with Alzheimer's Disease (AD) and Late Mild Cognitive Impairment (LMCI). In order to evaluate our model's generalizability, we used cross-validation strategy and randomly sampled the folds multiple times. Our results on classification not only provided better accuracy when augmented data generated from one sample is introduced, but yields more balanced results on other metrics as well.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2199124362",
                        "name": "Oben \u00d6zg\u00fcr"
                    },
                    {
                        "authorId": "2158276430",
                        "name": "Arwa Rekik"
                    },
                    {
                        "authorId": "3243720",
                        "name": "I. Rekik"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The sub-graph perturbation could be categorized as the graph-level data augmentation strategy and is frequently used for graph-level tasks such as graph classiication[11, 16, 43, 49], while the node perturbation and edge perturbation are frequently adopted for node-level or edge-level tasks."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "90e5c92b4fef9ff96b4f8c3b64a4b49aa8ff6e51",
                "externalIds": {
                    "DOI": "10.1145/3572835",
                    "CorpusId": 254151780
                },
                "corpusId": 254151780,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/90e5c92b4fef9ff96b4f8c3b64a4b49aa8ff6e51",
                "title": "Disentangled Representations Learning for Multi-target Cross-domain Recommendation",
                "abstract": "Data sparsity has been a long-standing issue for accurate and trustworthy recommendation systems (RS). To alleviate the problem, many researchers pay much attention to cross-domain recommendation (CDR), which aims at transferring rich knowledge from related source domains to enhance the recommendation performance of sparse target domain. To reach the knowledge transferring purpose, recent CDR works always focus on designing different pairwise directed or undirected information transferring strategies between source and target domains. However, such pairwise transferring idea is difficult to adapt to multi-target CDR scenarios directly, e.g., transferring knowledge between multiple domains and improving their performance simultaneously, as such strategies may lead the following issues: (1) When the number of domains increases, the number of transferring modules will grow exponentially, which causes heavy computation complexity. (2) A single pairwise transferring module could only capture the relevant information of two domains, but ignores the correlated information of other domains, which may limit the transferring effectiveness. (3) When a sparse domain serves as the source domain during the pairwise transferring, it would easily leads the negative transfer problem, and the untrustworthy information may hurt the target domain recommendation performance. In this article, we consider the key challenge of the multi-target CDR task: How to identify the most valuable trustworthy information over multiple domains and transfer such information efficiently to avoid the negative transfer problem? To fulfill the above challenge, we propose a novel end-to-end model termed as DR-MTCDR, standing for Disentangled Representations learning for Multi-Target CDR. DR-MTCDR aims at transferring the trustworthy domain-shared information across domains, which has the two major advantages in both efficiency and effectiveness: (1) For efficiency, DR-MTCDR utilizes a unified module on all domains to capture disentangled domain-shared information and domain-specific information, which could support all domain recommendation and be insensitive to the number of domains. (2) For effectiveness, based on the disentangled domain-shared and domain-specific information, DR-MTCDR has the capability to lead positive effect and make trustworthy recommendation for each domain. Empirical evaluations on datasets from both public datasets and real-world large-scale financial datasets have shown that the proposed framework outperforms other state-of-the-art baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46814159",
                        "name": "Xiaobo Guo"
                    },
                    {
                        "authorId": "2120158964",
                        "name": "Shaoshuai Li"
                    },
                    {
                        "authorId": "1699467744",
                        "name": "Naicheng Guo"
                    },
                    {
                        "authorId": "2115871859",
                        "name": "Jiangxia Cao"
                    },
                    {
                        "authorId": "2193118561",
                        "name": "Xiaolei Liu"
                    },
                    {
                        "authorId": "2117132334",
                        "name": "Q. Ma"
                    },
                    {
                        "authorId": "2527505",
                        "name": "Runsheng Gan"
                    },
                    {
                        "authorId": "2140037198",
                        "name": "Yunan Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c33958e35f63be8acc719ede42b6a7af04f97751",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-16631",
                    "ArXiv": "2211.16631",
                    "DOI": "10.48550/arXiv.2211.16631",
                    "CorpusId": 254096403
                },
                "corpusId": 254096403,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c33958e35f63be8acc719ede42b6a7af04f97751",
                "title": "Every Node Counts: Improving the Training of Graph Neural Networks on Node Classification",
                "abstract": "Graph Neural Networks (GNNs) are prominent in handling sparse and unstructured data efficiently and effectively. Specifically, GNNs were shown to be highly effective for node classification tasks, where labelled information is available for only a fraction of the nodes. Typically, the optimization process, through the objective function, considers only labelled nodes while ignoring the rest. In this paper, we propose novel objective terms for the training of GNNs for node classification, aiming to exploit all the available data and improve accuracy. Our first term seeks to maximize the mutual information between node and label features, considering both labelled and unlabelled nodes in the optimization process. Our second term promotes anisotropic smoothness in the prediction maps. Lastly, we propose a cross-validating gradients approach to enhance the learning from labelled data. Our proposed objectives are general and can be applied to various GNNs and require no architectural modifications. Extensive experiments demonstrate our approach using popular GNNs like GCN, GAT and GCNII, reading a consistent and significant accuracy improvement on 10 real-world node classification datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "104172497",
                        "name": "Moshe Eliasof"
                    },
                    {
                        "authorId": "145761835",
                        "name": "E. Haber"
                    },
                    {
                        "authorId": "2784349",
                        "name": "Eran Treister"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 Graph Data Augmentation: DropEdge (Rong et al., 2020), GREA (Liu et al., 2022), FLAG (Kong et al., 2022), M-Mixup (Wang et al., 2021), G-Mixup (Han et al., 2022).",
                "G-Mixup significantly increases Aug-Train by generating OOD samples, while it cannot guarantee a decrease in Aug-Test.",
                "We choose three data augmentation baselines, DropEdge, FLAG and G-Mixup, which augment graphs from different views.",
                ", 2020), and graph-level (Wang et al., 2021; Han et al., 2022) with random (You et al.",
                "It can be roughly divided into node-level (Kong et al., 2022), edge-level (Rong et al., 2020), and graph-level (Wang et al., 2021; Han et al., 2022) with random (You et al., 2020) or adversarial strategies (Suresh et al., 2021)."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6c6bc72dfa934c29b8044e19d023712d6e51967d",
                "externalIds": {
                    "ArXiv": "2211.02843",
                    "DBLP": "journals/corr/abs-2211-02843",
                    "DOI": "10.48550/arXiv.2211.02843",
                    "CorpusId": 253384147
                },
                "corpusId": 253384147,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6c6bc72dfa934c29b8044e19d023712d6e51967d",
                "title": "Adversarial Causal Augmentation for Graph Covariate Shift",
                "abstract": "Out-of-distribution (OOD) generalization on graphs is drawing widespread attention. However, existing efforts mainly focus on the OOD issue of correlation shift. While another type, covariate shift, remains largely unexplored but is the focus of this work. From a data generation view, causal features are stable substructures in data, which play key roles in OOD generalization. While their complementary parts, environments, are unstable features that often lead to various distribution shifts. Correlation shift establishes spurious statistical correlations between environments and labels. In contrast, covariate shift means that there exist unseen environmental features in test data. Existing strategies of graph invariant learning and data augmentation suffer from limited environments or unstable causal features, which greatly limits their generalization ability on covariate shift. In view of that, we propose a novel graph augmentation strategy: Adversarial Causal Augmentation (AdvCA), to alleviate the covariate shift. Specifically, it adversarially augments the data to explore diverse distributions of the environments. Meanwhile, it keeps the causal features invariant across diverse environments. It maintains the environmental diversity while ensuring the invariance of the causal features, thereby effectively alleviating the covariate shift. Extensive experimental results with in-depth analyses demonstrate that AdvCA can outperform 14 baselines on synthetic and real-world datasets with various covariate shifts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2003767516",
                        "name": "Yongduo Sui"
                    },
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "1491035012",
                        "name": "Jiancan Wu"
                    },
                    {
                        "authorId": "2153659066",
                        "name": "An Zhang"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that G-Mixup [15] has gfeat as (10) and glabel as (1b)\ncp (2\u2212 \u03b1) for \u03b1 \u2208 [1, 2].",
                "For mixup of graph data gfeat, we compare GraphMAD\u2019s clusterpath data mixup (7) with linear graphon mixup [15].",
                "Similarly to [15], our graph descriptors are SBM graphon approximations, where Wi \u2208 [0, 1]D\u00d7D is obtained for each graph Gi by sorting and smoothing (SAS) [22] with D denoting the fineness of the graphon estimate.",
                "An attractive alternative is mixing graph data in a latent space, which includes mixup of learned graph representations [13, 14] or graph models [15].",
                "Note that G-Mixup [15] has gfeat as (10) and glabel as (1b) Fig.",
                "In this work, similar to [15], we adopt the graphon, a bounded symmetric function W : [0, 1](2) \u2192 [0, 1] that can be interpreted as a random graph model associated with a family of graphs with similar structural characteristics [18\u201320].",
                "[15] X. Han, Z. Jiang, N. Liu, and X. Hu, \u201cG-Mixup: Graph data augmentation for graph classification,\u201d arXiv preprint arXiv:2202.07179, 2022.",
                "Authors in [15] present the closest work to our own."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "ceee4ace2516fcbebf0e3cdae787aa840a76205b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-15721",
                    "ArXiv": "2210.15721",
                    "DOI": "10.48550/arXiv.2210.15721",
                    "CorpusId": 253223884
                },
                "corpusId": 253223884,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/ceee4ace2516fcbebf0e3cdae787aa840a76205b",
                "title": "GraphMAD: Graph Mixup for Data Augmentation using Data-Driven Convex Clustering",
                "abstract": "We develop a novel data-driven nonlinear mixup mechanism for graph data augmentation and present different mixup functions for sample pairs and their labels. Mixup is a data augmentation method to create new training data by linearly interpolating between pairs of data samples and their labels. Mixup of graph data is challenging since the interpolation between graphs of potentially different sizes is an ill-posed operation. Hence, a promising approach for graph mixup is to first project the graphs onto a common latent feature space and then explore linear and nonlinear mixup strategies in this latent space. In this context, we propose to (i) project graphs onto the latent space of continuous random graph models known as graphons, (ii) leverage convex clustering in this latent space to generate nonlinear data-driven mixup functions, and (iii) investigate the use of different mixup functions for labels and data samples. We evaluate our graph data augmentation performance on benchmark datasets and demonstrate that nonlinear data-driven mixup functions can significantly improve graph classification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1936823928",
                        "name": "Madeline Navarro"
                    },
                    {
                        "authorId": "2262891",
                        "name": "Santiago Segarra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Data augmentation [20, 27] is wildly adopted in the graph learning for improving model generalization, including dropping nodes [14], dropping edges [32], and graph mixup [20]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "727a91b687903d02c35e00d3ca94f9e728b4be36",
                "externalIds": {
                    "ArXiv": "2210.10737",
                    "DBLP": "journals/corr/abs-2210-10737",
                    "DOI": "10.48550/arXiv.2210.10737",
                    "CorpusId": 252993034
                },
                "corpusId": 252993034,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/727a91b687903d02c35e00d3ca94f9e728b4be36",
                "title": "RSC: Accelerating Graph Neural Networks Training via Randomized Sparse Computations",
                "abstract": "The training of graph neural networks (GNNs) is extremely time consuming because sparse graph-based operations are hard to be accelerated by hardware. Prior art explores trading off the computational precision to reduce the time complexity via sampling-based approximation. Based on the idea, previous works successfully accelerate the dense matrix based operations (e.g., convolution and linear) with negligible accuracy drop. However, unlike dense matrices, sparse matrices are stored in the irregular data format such that each row/column may have different number of non-zero entries. Thus, compared to the dense counterpart, approximating sparse operations has two unique challenges (1) we cannot directly control the efficiency of approximated sparse operation since the computation is only executed on non-zero entries; (2) sub-sampling sparse matrices is much more inefficient due to the irregular data format. To address the issues, our key idea is to control the accuracy-efficiency trade off by optimizing computation resource allocation layer-wisely and epoch-wisely. Specifically, for the first challenge, we customize the computation resource to different sparse operations, while limit the total used resource below a certain budget. For the second challenge, we cache previous sampled sparse matrices to reduce the epoch-wise sampling overhead. Finally, we propose a switching mechanisms to improve the generalization of GNNs trained with approximated operations. To this end, we propose Randomized Sparse Computation, which for the first time demonstrate the potential of training GNNs with approximated operations. In practice, rsc can achieve up to $11.6\\times$ speedup for a single sparse operation and a $1.6\\times$ end-to-end wall-clock time speedup with negligible accuracy drop.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47781070",
                        "name": "Zirui Liu"
                    },
                    {
                        "authorId": "2118530137",
                        "name": "Sheng-Wei Chen"
                    },
                    {
                        "authorId": "3364022",
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "authorId": "1759658",
                        "name": "D. Zha"
                    },
                    {
                        "authorId": "97620379",
                        "name": "Xiao Huang"
                    },
                    {
                        "authorId": "2109724398",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The mixup on graphs is regarded as challenging due to the irregularity and connectivity, and existing mixup methods for GNNs aim to mix hidden embedding [11, 36]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4004a6304d4e576671d73b4eb44400728396c32f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09609",
                    "ArXiv": "2210.09609",
                    "DOI": "10.48550/arXiv.2210.09609",
                    "CorpusId": 252967712
                },
                "corpusId": 252967712,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4004a6304d4e576671d73b4eb44400728396c32f",
                "title": "SA-MLP: Distilling Graph Knowledge from GNNs into Structure-Aware MLP",
                "abstract": "The message-passing mechanism helps Graph Neural Networks (GNNs) achieve remarkable results on various node classification tasks. Nevertheless, the recursive nodes fetching and aggregation in message-passing cause inference latency when deploying GNNs to large-scale graphs. One promising inference acceleration direction is to distill the GNNs into message-passing-free student multi-layer perceptrons (MLPs). However, the MLP student cannot fully learn the structure knowledge due to the lack of structure inputs, which causes inferior performance in the heterophily and inductive scenarios. To address this, we intend to inject structure information into MLP-like students in low-latency and interpretable ways. Specifically, we first design a Structure-Aware MLP (SA-MLP) student that encodes both features and structures without message-passing. Then, we introduce a novel structure-mixing knowledge distillation strategy to enhance the learning ability of MLPs for structure information. Furthermore, we design a latent structure embedding approximation technique with two-stage distillation for inductive scenarios. Extensive experiments on eight benchmark datasets under both transductive and inductive settings show that our SA-MLP can consistently outperform the teacher GNNs, while maintaining faster inference as MLPs. The source code of our work can be found in https://github.com/JC-202/SA-MLP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2186735792",
                        "name": "Jie Chen"
                    },
                    {
                        "authorId": "2107977562",
                        "name": "Shouzhen Chen"
                    },
                    {
                        "authorId": "8677944",
                        "name": "Mingyuan Bai"
                    },
                    {
                        "authorId": "32278515",
                        "name": "Junbin Gao"
                    },
                    {
                        "authorId": "2144127151",
                        "name": "Junping Zhang"
                    },
                    {
                        "authorId": "2058905282",
                        "name": "Jian Pu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "88f97cd4260e331235e1ba9cc18e4ea05894cf07",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-07921",
                    "ArXiv": "2209.07921",
                    "DOI": "10.48550/arXiv.2209.07921",
                    "CorpusId": 252355075
                },
                "corpusId": 252355075,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/88f97cd4260e331235e1ba9cc18e4ea05894cf07",
                "title": "ImDrug: A Benchmark for Deep Imbalanced Learning in AI-aided Drug Discovery",
                "abstract": "The last decade has witnessed a prosperous development of computational methods and dataset curation for AI-aided drug discovery (AIDD). However, real-world pharmaceutical datasets often exhibit highly imbalanced distribution, which is overlooked by the current literature but may severely compromise the fairness and generalization of machine learning applications. Motivated by this observation, we introduce ImDrug, a comprehensive benchmark with an open-source Python library which consists of 4 imbalance settings, 11 AI-ready datasets, 54 learning tasks and 16 baseline algorithms tailored for imbalanced learning. It provides an accessible and customizable testbed for problems and solutions spanning a broad spectrum of the drug discovery pipeline such as molecular modeling, drug-target interaction and retrosynthesis. We conduct extensive empirical studies with novel evaluation metrics, to demonstrate that the existing algorithms fall short of solving medicinal and pharmaceutical challenges in the data imbalance scenario. We believe that ImDrug opens up avenues for future research and development, on real-world challenges at the intersection of AIDD and deep imbalanced learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117007545",
                        "name": "Lanqing Li"
                    },
                    {
                        "authorId": "2114138366",
                        "name": "Li Zeng"
                    },
                    {
                        "authorId": "120267993",
                        "name": "Zi-Chao Gao"
                    },
                    {
                        "authorId": "2185356496",
                        "name": "Shen Yuan"
                    },
                    {
                        "authorId": "2419616",
                        "name": "Yatao Bian"
                    },
                    {
                        "authorId": "2152564746",
                        "name": "Bing Wu"
                    },
                    {
                        "authorId": "145124989",
                        "name": "Heng Zhang"
                    },
                    {
                        "authorId": "2149468676",
                        "name": "Chan Lu"
                    },
                    {
                        "authorId": "2152847316",
                        "name": "Yang Yu"
                    },
                    {
                        "authorId": "2157220990",
                        "name": "Wei Liu"
                    },
                    {
                        "authorId": "2146235527",
                        "name": "Hongteng Xu"
                    },
                    {
                        "authorId": "2118373495",
                        "name": "Jia Li"
                    },
                    {
                        "authorId": "144259957",
                        "name": "P. Zhao"
                    },
                    {
                        "authorId": "1714602",
                        "name": "P. Heng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These generate new instances by feature or graph structure interpolation (Verma et al., 2021; Wang et al., 2021; 2020; Han et al., 2022)."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "927c9b5afffc7b6b2d4474047e3964c9d5594f0f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-06589",
                    "ArXiv": "2209.06589",
                    "DOI": "10.48550/arXiv.2209.06589",
                    "CorpusId": 252220900
                },
                "corpusId": 252220900,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/927c9b5afffc7b6b2d4474047e3964c9d5594f0f",
                "title": "Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs improve the OOD generalization on a variety of inference tasks in the direction of diverse structural features.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2203023360",
                        "name": "Hyungeun Lee"
                    },
                    {
                        "authorId": "2155823102",
                        "name": "Hyunmok Park"
                    },
                    {
                        "authorId": "47639846",
                        "name": "Kijung Yoon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "94 3 [3], [9], [11], [51], [52] Proteins Interaction OGB-ppa [12] 158,100 243.",
                "Social Node/Edge Facebook [23] 4,039 88,234 2 [37], [65] Node BlogCatalog [27] 5,196 1,71,743 6 [42], [66] Node Flickr [27] 7,575 2,39,738 9 [42], [52], [66] Node/Edge Reddit [2] 232,965 11,606,919 41 [32], [44], [50], [68] Node Yelp [25] 716,847 6,977,410 100 [52] Node Polblogs [1] 1,222 16,714 2 [13] Drugs Interaction Edge OGB-DDI [12] 4,267 1,334,889 - [11], [19], [65] Proteins Interaction Node OGB-Proteins [12] 132,534 39,561,252 [19] Node PPI [10] 10,076 157,213 121 [44], [66], [68] Collaboration Edge OGB-Collab [12] 235,868 1,285,465 - [19]",
                "53 2 [9], [11], [51], [60] IMDB-M [2] 1500 13.",
                "0 2 [11], [40], [60] sider [20] 1,427 33.",
                "5 2 [11], [19], [60] bace [38] 1,513 34.",
                "9 2 [11], [40], [60] pcba [12] 437,929 26.",
                "75 2 [3], [11], [51], [60], [61] Reddit-5K [2] 4999 508.",
                "87 5 [11], [50], [51], [60] Reddit-12K [2] 11929 391.",
                "82 2 [3], [9], [11], [51], [52], [60], [61] Collaboration Collab [59] 5000 74.",
                "Graph Classification Edge Manipulation [70] Node Manipulation [52] Feature Manipulation [19] Sub-Graph Manipulation [3], [9], [11], [30], [40], [51] Hybrid Manipulation [60], [61]"
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4f2b263ac4391e77a0fc4e77982dba014311486c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-11973",
                    "ArXiv": "2208.11973",
                    "DOI": "10.48550/arXiv.2208.11973",
                    "CorpusId": 251800247
                },
                "corpusId": 251800247,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4f2b263ac4391e77a0fc4e77982dba014311486c",
                "title": "Data Augmentation for Graph Data: Recent Advancements",
                "abstract": "Graph Neural Network (GNNs) based methods have recently become a popular tool to deal with graph data because of their ability to incorporate structural information. The only hurdle in the performance of GNNs is the lack of labeled data. Data Augmentation techniques for images and text data can not be used for graph data because of the complex and non-euclidean structure of graph data. This gap has forced researchers to shift their focus towards the development of data augmentation techniques for graph data. Most of the proposed Graph Data Augmentation (GDA) techniques are task-specific. In this paper, we survey the existing GDA techniques based on different graph tasks. This survey not only provides a reference to the research community of GDA but also provides the necessary information to the researchers of other domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182686708",
                        "name": "Maria Marrium"
                    },
                    {
                        "authorId": "81223085",
                        "name": "Arif Mahmood"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In GNNs, many GCL methods are arisen for graph representation learning, such as GraphCL (You et al., 2020), GRACE (Zhu et al., 2020), AD-GCL (Suresh et al., 2021) and G-Mixup (Han et al., 2022)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "56209a9198a9badff3fbbbc4db8c3d065fc1de41",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-09126",
                    "ArXiv": "2208.09126",
                    "DOI": "10.48550/arXiv.2208.09126",
                    "CorpusId": 251710375
                },
                "corpusId": 251710375,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/56209a9198a9badff3fbbbc4db8c3d065fc1de41",
                "title": "GraphTTA: Test Time Adaptation on Graph Neural Networks",
                "abstract": "Recently, test time adaptation (TTA) has attracted increasing attention due to its power of handling the distribution shift issue in the real world. Unlike what has been developed for convolutional neural networks (CNNs) for image data, TTA is less explored for Graph Neural Networks (GNNs). There is still a lack of efficient algorithms tailored for graphs with irregular structures. In this paper, we present a novel test time adaptation strategy named Graph Adversarial Pseudo Group Contrast (GAPGC), for graph neural networks TTA, to better adapt to the Out Of Distribution (OOD) test data. Specifically, GAPGC employs a contrastive learning variant as a self-supervised task during TTA, equipped with Adversarial Learnable Augmenter and Group Pseudo-Positive Samples to enhance the relevance between the self-supervised task and the main task, boosting the performance of the main task. Furthermore, we provide theoretical evidence that GAPGC can extract minimal sufficient information for the main task from information theory perspective. Extensive experiments on molecular scaffold OOD dataset demonstrated that the proposed approach achieves state-of-the-art performance on GNNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "115151240",
                        "name": "Guan-Wun Chen"
                    },
                    {
                        "authorId": "2107966843",
                        "name": "Jiying Zhang"
                    },
                    {
                        "authorId": "2158760113",
                        "name": "Xi Xiao"
                    },
                    {
                        "authorId": "98177814",
                        "name": "Y. Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To this end, [43; 44; 45; 46] propose to augment graph data directly in the data space, while [47] interpolates latent representations to create novel ones."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a1b297edacc60891a9a3fd7f369dbb8f92c1c257",
                "externalIds": {
                    "DBLP": "conf/log/CrisostomiAMMMR22",
                    "ArXiv": "2206.03695",
                    "DOI": "10.48550/arXiv.2206.03695",
                    "CorpusId": 249461588
                },
                "corpusId": 249461588,
                "publicationVenue": {
                    "id": "50534c12-f4ba-4c64-806b-01647d1baacf",
                    "name": "LOG IN",
                    "type": "journal",
                    "alternate_names": [
                        "Log in",
                        "Log",
                        "LOG"
                    ],
                    "issn": "0720-8642",
                    "alternate_issns": [
                        "1547-4690",
                        "0024-5798"
                    ],
                    "url": "https://www.log-in-verlag.de/informatische_bildung/",
                    "alternate_urls": [
                        "https://www.anycorp.com/log/about",
                        "https://www.jstor.org/journal/log",
                        "https://www.anycorp.com/",
                        "http://www.jstor.org/action/showPublication?journalCode=log",
                        "http://www.anycorp.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a1b297edacc60891a9a3fd7f369dbb8f92c1c257",
                "title": "Metric Based Few-Shot Graph Classification",
                "abstract": "Many modern deep-learning techniques do not work without enormous datasets. At the same time, several fields demand methods working in scarcity of data. This problem is even more complex when the samples have varying structures, as in the case of graphs. Graph representation learning techniques have recently proven successful in a variety of domains. Nevertheless, the employed architectures perform miserably when faced with data scarcity. On the other hand, few-shot learning allows employing modern deep learning models in scarce data regimes without waiving their effectiveness. In this work, we tackle the problem of few-shot graph classification, showing that equipping a simple distance metric learning baseline with a state-of-the-art graph embedder allows to obtain competitive results on the task.While the simplicity of the architecture is enough to outperform more complex ones, it also allows straightforward additions. To this end, we show that additional improvements may be obtained by encouraging a task-conditioned embedding space. Finally, we propose a MixUp-based online data augmentation technique acting in the latent space and show its effectiveness on the task.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156124648",
                        "name": "Donato Crisostomi"
                    },
                    {
                        "authorId": "2156124699",
                        "name": "Simone Antonelli"
                    },
                    {
                        "authorId": "2140400495",
                        "name": "Valentino Maiorca"
                    },
                    {
                        "authorId": "23991573",
                        "name": "Luca Moschella"
                    },
                    {
                        "authorId": "2053857470",
                        "name": "R. Marin"
                    },
                    {
                        "authorId": "1796150",
                        "name": "E. Rodol\u00e0"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, graph mixup methods (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) synthesize a new graph or graph representation from two input graphs.",
                "Some studies (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) propose interpolation-based mixup methods for graph augmentations, and Kong et al. (2022) propose to augment node features through adversarial learning.",
                ", 2021), G-Mixup (Han et al., 2022), and FLAG (Kong et al.",
                "In addition to the baselines in Section 4.1, we also compare with previous graph augmentation methods, including DropEdge (Rong et al., 2020), M-Mixup (Wang et al., 2021), G-Mixup (Han et al., 2022), and FLAG (Kong et al., 2022).",
                "Some studies (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) propose interpolation-based mixup methods for graph augmentations, and Kong et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "6c76ae1dee7f76f83549a37f59cb4237fa1cac2d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-13248",
                    "ArXiv": "2202.13248",
                    "CorpusId": 247158749
                },
                "corpusId": 247158749,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6c76ae1dee7f76f83549a37f59cb4237fa1cac2d",
                "title": "Automated Data Augmentations for Graph Classification",
                "abstract": "Data augmentations are effective in improving the invariance of learning machines. We argue that the core challenge of data augmentations lies in designing data transformations that preserve labels. This is relatively straightforward for images, but much more challenging for graphs. In this work, we propose GraphAug, a novel automated data augmentation method aiming at computing label-invariant augmentations for graph classification. Instead of using uniform transformations as in existing studies, GraphAug uses an automated augmentation model to avoid compromising critical label-related information of the graph, thereby producing label-invariant augmentations at most times. To ensure label-invariance, we develop a training method based on reinforcement learning to maximize an estimated label-invariance probability. Experiments show that GraphAug outperforms previous graph augmentation methods on various graph classification tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2004524780",
                        "name": "Youzhi Luo"
                    },
                    {
                        "authorId": "2909646",
                        "name": "Michael McThrow"
                    },
                    {
                        "authorId": "121768478",
                        "name": "Wing Yee Au"
                    },
                    {
                        "authorId": "2057096289",
                        "name": "Tao Komikado"
                    },
                    {
                        "authorId": "2156583777",
                        "name": "Kanji Uchino"
                    },
                    {
                        "authorId": "2197887",
                        "name": "Koji Maruhashi"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ff424f1125f68c03a0e27d3ff1033fa565abe1eb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-08871",
                    "ArXiv": "2202.08871",
                    "CorpusId": 246996967
                },
                "corpusId": 246996967,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ff424f1125f68c03a0e27d3ff1033fa565abe1eb",
                "title": "Graph Data Augmentation for Graph Machine Learning: A Survey",
                "abstract": "Data augmentation has recently seen increased interest in graph machine learning given its demonstrated ability to improve model performance and generalization by added training data. Despite this recent surge, the area is still relatively under-explored, due to the challenges brought by complex, non-Euclidean structure of graph data, which limits the direct analogizing of traditional augmentation operations on other types of image, video or text data. Our work aims to give a necessary and timely overview of existing graph data augmentation methods; notably, we present a comprehensive and systematic survey of graph data augmentation approaches, summarizing the literature in a structured manner. We first introduce three different taxonomies for categorizing graph data augmentation methods from the data, task, and learning perspectives, respectively. Next, we introduce recent advances in graph data augmentation, differentiated by their methodologies and applications. We conclude by outlining currently unsolved challenges and directions for future research. Overall, our work aims to clarify the landscape of existing literature in graph data augmentation and motivates additional work in this area, providing a helpful resource for researchers and practitioners in the broader graph machine learning domain. Additionally, we provide a continuously updated reading list at https://github.com/zhao-tong/graph-data-augmentation-papers.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1742573",
                        "name": "Tong Zhao"
                    },
                    {
                        "authorId": "2146562364",
                        "name": "Gang Liu"
                    },
                    {
                        "authorId": "51249380",
                        "name": "Stephan Gunnemann"
                    },
                    {
                        "authorId": "144812586",
                        "name": "Meng Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4ec859621b48ef6738b33b7e31ff06c57d7d2c33",
                "externalIds": {
                    "DBLP": "journals/sigkdd/DingXTL22",
                    "ArXiv": "2202.08235",
                    "DOI": "10.1145/3575637.3575646",
                    "CorpusId": 246867109
                },
                "corpusId": 246867109,
                "publicationVenue": {
                    "id": "e3c9b2eb-d870-43b6-811e-7b451583a8df",
                    "name": "SIGKDD Explorations",
                    "type": "journal",
                    "alternate_names": [
                        "Sigkdd Explor",
                        "SIGKDD Explor",
                        "Sigkdd Explorations"
                    ],
                    "issn": "1931-0145",
                    "url": "http://dl.acm.org/toc.cfm?id=J721",
                    "alternate_urls": [
                        "http://www.sigkdd.org/explorations/archive.php",
                        "http://portal.acm.org/sigkdd/newsletter/",
                        "http://www.sigkdd.org/explorations/about.php"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4ec859621b48ef6738b33b7e31ff06c57d7d2c33",
                "title": "Data Augmentation for Deep Graph Learning",
                "abstract": "Graph neural networks, a powerful deep learning tool to model graph-structured data, have demonstrated remarkable performance on numerous graph learning tasks. To address the data noise and data scarcity issues in deep graph learning, the research on graph data augmentation has intensified lately. However, conventional data augmentation methods can hardly handle graph-structured data which is defined in non-Euclidean space with multi-modality. In this survey, we formally formulate the problem of graph data augmentation and further review the representative techniques and their applications in different deep graph learning problems. Specifically, we first propose a taxonomy for graph data augmentation techniques and then provide a structured review by categorizing the related work based on the augmented information modalities. Moreover, we summarize the applications of graph data augmentation in two representative problems in data-centric deep graph learning: (1) reliable graph learning which focuses on enhancing the utility of input graph as well as the model capacity via graph data augmentation; and (2) low-resource graph learning which targets on enlarging the labeled training data scale through graph data augmentation. For each problem, we also provide a hierarchical problem taxonomy and review the existing literature related to graph data augmentation. Finally, we point out promising research directions and the challenges in future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "66807781",
                        "name": "Kaize Ding"
                    },
                    {
                        "authorId": "2149237236",
                        "name": "Zhe Xu"
                    },
                    {
                        "authorId": "8163721",
                        "name": "Hanghang Tong"
                    },
                    {
                        "authorId": "2155337763",
                        "name": "Huan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The similar strategies are also applied in graphs [78, 79, 80, 81, 82, 83].",
                "G-Mixup [83] tackles the key challenges when mixing up directly on the graph data, as graph data is irregular and not well-aligned, and graph topology between classes is divergent."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "2a3349c9f48b322400cd1d2d720fc42a42d19d5f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-07987",
                    "ArXiv": "2202.07987",
                    "CorpusId": 246867220
                },
                "corpusId": 246867220,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2a3349c9f48b322400cd1d2d720fc42a42d19d5f",
                "title": "Out-Of-Distribution Generalization on Graphs: A Survey",
                "abstract": "\u2014Graph machine learning has been extensively studied in both academia and industry. Although booming with a vast number of emerging methods and techniques, most of the literature is built on the in-distribution hypothesis, i.e., testing and training graph data are identically distributed. However, this in-distribution hypothesis can hardly be satis\ufb01ed in many real-world graph scenarios where the model performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this critical problem, out-of-distribution (OOD) generalization on graphs, which goes beyond the in-distribution hypothesis, has made great progress and attracted ever-increasing attention from the research community. In this paper, we comprehensively survey OOD generalization on graphs and present a detailed review of recent advances in this area. First, we provide a formal problem de\ufb01nition of OOD generalization on graphs. Second, we categorize existing methods into three classes from conceptually different perspectives, i.e., data, model, and learning strategy, based on their positions in the graph machine learning pipeline, followed by detailed discussions for each category. We also review the theories related to OOD generalization on graphs and introduce the commonly used graph datasets for thorough evaluations. Finally, we share our insights on future research directions. This paper is the \ufb01rst systematic and comprehensive review of OOD generalization on graphs, to the best of our knowledge.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144911687",
                        "name": "Haoyang Li"
                    },
                    {
                        "authorId": "153316152",
                        "name": "Xin Wang"
                    },
                    {
                        "authorId": "2116460208",
                        "name": "Ziwei Zhang"
                    },
                    {
                        "authorId": "145583986",
                        "name": "Wenwu Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides, an advanced data augmentation strategy namely Mixup is recently applied to DGL and is proved to be effective for several graph-related tasks [Han et al., 2022]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "082b03b461eb7607d9d8c9ef804142b8a9a614fb",
                "externalIds": {
                    "ArXiv": "2202.07114",
                    "CorpusId": 246864091
                },
                "corpusId": 246864091,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/082b03b461eb7607d9d8c9ef804142b8a9a614fb",
                "title": "Recent Advances in Reliable Deep Graph Learning: Inherent Noise, Distribution Shift, and Adversarial Attack",
                "abstract": "Deep graph learning (DGL) has achieved remarkable progress in both business and scientific areas ranging from finance and e-commerce to drug and advanced material discovery. Despite the progress, applying DGL to real-world applications faces a series of reliability threats including inherent noise, distribution shift, and adversarial attacks. This survey aims to provide a comprehensive review of recent advances for improving the reliability of DGL algorithms against the above threats. In contrast to prior related surveys which mainly focus on adversarial attacks and defense, our survey covers more reliability-related aspects of DGL, i.e., inherent noise and distribution shift. Additionally, we discuss the relationships among above aspects and highlight some important issues to be explored in future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115953829",
                        "name": "Jintang Li"
                    },
                    {
                        "authorId": "27055880",
                        "name": "Bingzhe Wu"
                    },
                    {
                        "authorId": "144549366",
                        "name": "Chengbin Hou"
                    },
                    {
                        "authorId": null,
                        "name": "Guoji Fu"
                    },
                    {
                        "authorId": "2419616",
                        "name": "Yatao Bian"
                    },
                    {
                        "authorId": "1853048147",
                        "name": "Liang Chen"
                    },
                    {
                        "authorId": "1768190",
                        "name": "Junzhou Huang"
                    },
                    {
                        "authorId": "144291579",
                        "name": "Zibin Zheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026a graph pair, through mixing the graph representation resulting from passing the graph through the GNNs. Similarly, a concurrent work G-Mixup (Han et al. 2022) first interpolates represented graph generators (i.e., graphons) of different classes, and then leverages the mixed graphons for\u2026",
                "Similarly, a concurrent work G-Mixup (Han et al. 2022) first interpolates represented graph generators (i.",
                "Manifold Intrusion The manifold intrusion degrades the performance of Mixup-like methods (Guo, Mao, and Zhang 2019b; Han et al. 2022).",
                "MixupGraph (Wang et al. 2021b) also leverages a simple way to avoid dealing with the arbitrary structure in the input space for mixing a graph pair, through mixing the graph representation resulting from passing the graph through the GNNs. Similarly, a concurrent work G-Mixup (Han et al. 2022) first interpolates represented graph generators (i.e., graphons) of different classes, and then leverages the mixed graphons for sampling to generate synthetic graphs."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "def2c67f039009752915913b3bfddf586dad11cc",
                "externalIds": {
                    "ArXiv": "2110.09344",
                    "DBLP": "conf/aaai/GuoM23",
                    "DOI": "10.1609/aaai.v37i6.25941",
                    "CorpusId": 254017928
                },
                "corpusId": 254017928,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/def2c67f039009752915913b3bfddf586dad11cc",
                "title": "Interpolating Graph Pair to Regularize Graph Classification",
                "abstract": "We present a simple and yet effective interpolation-based regularization technique, aiming to improve the generalization of Graph Neural Networks (GNNs) on supervised graph classification. We leverage Mixup, an effective regularizer for vision, where random sample pairs and their labels are interpolated to create synthetic images for training. Unlike images with grid-like coordinates, graphs have arbitrary structure and topology, which can be very sensitive to any modification that alters the graph's semantic meanings. This posts two unanswered questions for Mixup-like regularization schemes: Can we directly mix up a pair of graph inputs? If so, how well does such mixing strategy regularize the learning of GNNs? To answer these two questions, we propose ifMixup, which first adds dummy nodes to make two graphs have the same input size and then simultaneously performs linear interpolation between the aligned node feature vectors and the aligned edge representations of the two graphs. We empirically show that such simple mixing schema can effectively regularize the classification learning, resulting in superior predictive accuracy to popular graph augmentation and GNN methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1694050",
                        "name": "Hongyu Guo"
                    },
                    {
                        "authorId": "2047889",
                        "name": "Yongyi Mao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "G-Mixup (Han et al., 2022) uses graphons as a surrogate to apply mixup techniques to graph data."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "34cce045b2106decb208e25197619628859fa3c0",
                "externalIds": {
                    "ArXiv": "2101.11525",
                    "CorpusId": 231718924
                },
                "corpusId": 231718924,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/34cce045b2106decb208e25197619628859fa3c0",
                "title": "Calibrating and Improving Graph Contrastive Learning",
                "abstract": "Graph contrastive learning algorithms have demonstrated remarkable success in various applications such as node classification, link prediction, and graph clustering. However, in unsupervised graph contrastive learning, some contrastive pairs may contradict the truths in downstream tasks and thus the decrease of losses on these pairs undesirably harms the performance in the downstream tasks. To assess the discrepancy between the prediction and the ground-truth in the downstream tasks for these contrastive pairs, we adapt the expected calibration error (ECE) to graph contrastive learning. The analysis of ECE motivates us to propose a novel regularization method, Contrast-Reg, to ensure that decreasing the contrastive loss leads to better performance in the downstream tasks. As a plug-in regularizer, Contrast-Reg effectively improves the performance of existing graph contrastive learning algorithms. We provide both theoretical and empirical results to demonstrate the effectiveness of Contrast-Reg in enhancing the generalizability of the Graph Neural Network(GNN) model and improving the performance of graph contrastive algorithms with different similarity definitions and encoder backbones across various downstream tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47737190",
                        "name": "Kaili Ma"
                    },
                    {
                        "authorId": "2118530836",
                        "name": "Haochen Yang"
                    },
                    {
                        "authorId": "2109713653",
                        "name": "Han Yang"
                    },
                    {
                        "authorId": "35295263",
                        "name": "Tatiana Jin"
                    },
                    {
                        "authorId": "2158171944",
                        "name": "Pengfei Chen"
                    },
                    {
                        "authorId": "2108962670",
                        "name": "Yongqiang Chen"
                    },
                    {
                        "authorId": "1739876059",
                        "name": "Barakeel Fanseu Kamhoua"
                    },
                    {
                        "authorId": "1717691",
                        "name": "James Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Literaturelly, graphon has been studied from two perspectives: as limit of graph sequence, and as graph generators[1, 11, 24].",
                "[3] mentioned the same motifs were also found from bacteria [11] to yeast [29], animal [32] to plants."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8bff4090c463f1880d4e36ef7f868b77c7657089",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-16458",
                    "DOI": "10.48550/arXiv.2303.16458",
                    "CorpusId": 257804792
                },
                "corpusId": 257804792,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8bff4090c463f1880d4e36ef7f868b77c7657089",
                "title": "When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!",
                "abstract": "Recently, graph pre-training has attracted wide research attention, which aims to learn transferable knowledge from unlabeled graph data so as to improve downstream performance. Despite these recent attempts, the negative transfer is a major issue when applying graph pre-trained models to downstream tasks. Existing works made great efforts on the issue of what to pre-train and how to pretrain by designing a number of graph pre-training and fine-tuning strategies. However, there are indeed cases where no matter how advanced the strategy is, the \u201cpre-train and fine-tune\u201d paradigm still cannot achieve clear benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (i.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN first fits the pre-training data into graphon bases, where each element of graphon basis (i.e., a graphon) identifies a fundamental transferable pattern shared by a collection of pre-training graphs. All convex combinations of graphon bases give rise to a generator space, from which graphs generated form the solution space for those downstream data that can benefit from pre-training. In this manner, the feasibility of pre-training can be \u2217Both authors contributed equally to this research. \u2020 Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym , XXXX, XX \u00a9 XXXX Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX quantified as the highest generation probability of the downstream data from any generator in the generator space. W2PGNN provides three broad applications, including providing the application scope of graph pre-trained models, quantifying the feasibility of performing pre-training, and helping select pre-training data to enhance downstream performance. We give a theoretically sound solution for the first application and extensive empirical justifications for the latter two applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144149886",
                        "name": "Yu Cao"
                    },
                    {
                        "authorId": "49394731",
                        "name": "Jiarong Xu"
                    },
                    {
                        "authorId": "1390553618",
                        "name": "Carl Yang"
                    },
                    {
                        "authorId": "2118328782",
                        "name": "Jiaan Wang"
                    },
                    {
                        "authorId": "2120330528",
                        "name": "Yunchao Zhang"
                    },
                    {
                        "authorId": "2201480815",
                        "name": "Chunping Wang"
                    },
                    {
                        "authorId": null,
                        "name": "Lei Chen"
                    },
                    {
                        "authorId": "2152915866",
                        "name": "Yang Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The graph augmentation methods combat the distributional shifts by increasing the data diversity (Zhao et al., 2021; Wang et al., 2021; Han et al., 2022)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cd6b91a76654cf951b98358badd8879ee0847327",
                "externalIds": {
                    "DBLP": "conf/icml/ChuJWZW0023",
                    "CorpusId": 259321098
                },
                "corpusId": 259321098,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/cd6b91a76654cf951b98358badd8879ee0847327",
                "title": "Wasserstein Barycenter Matching for Graph Size Generalization of Message Passing Neural Networks",
                "abstract": "Graph size generalization is hard for Message passing neural networks (MPNNs). The graphlevel classification performance of MPNNs degrades across various graph sizes. Recently, theoretical studies reveal that a slow uncontrollable convergence rate w.r.t. graph size could adversely affect the size generalization. To address the uncontrollable convergence rate caused by correlations across nodes in the underlying dimensional signal-generating space, we propose to use Wasserstein barycenters as graph-level consensus to combat node-level correlations. Methodologically, we propose a Wasserstein barycenter matching (WBM) layer that represents an input graph by Wasserstein distances between its MPNN-filtered node embeddings versus some learned class-wise barycenters. Theoretically, we show that the convergence rate of an MPNN with a WBM layer is controllable and independent to the dimensionality of the signal-generating space. Thus MPNNs with WBM layers are less susceptible to slow uncontrollable convergence rate and size variations. Empirically, the WBM layer improves the size generalization over vanilla MPNNs with different backbones (e.g., GCN, GIN, and PNA) significantly on real-world graph datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2056598391",
                        "name": "Xu Chu"
                    },
                    {
                        "authorId": "2175350283",
                        "name": "Yujie Jin"
                    },
                    {
                        "authorId": "153316152",
                        "name": "Xin Wang"
                    },
                    {
                        "authorId": "2437353",
                        "name": "Shanghang Zhang"
                    },
                    {
                        "authorId": "2108738446",
                        "name": "Yasha Wang"
                    },
                    {
                        "authorId": "2156154955",
                        "name": "Wenwu Zhu"
                    },
                    {
                        "authorId": "2175277699",
                        "name": "Hong Mei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite recent advances in graph representation learning (Grover & Leskovec, 2016; Kipf & Welling, 2017; 2016; Gilmer et al., 2017; Han et al., 2022b), these GNN models may inherit or even amplify bias from training data (Dai & Wang, 2021), thereby introducing prediction discrimination against\u2026",
                "\u2026their remarkable performance (Gao et al., 2021; Gao & Ji, 2019; Liu et al., 2021a;b; Yuan et al., 2021) in many applications, such as knowledge graphs (Hamaguchi et al., 2017), molecular property prediction (Liu et al., 2022; 2020; Han et al., 2022a) and social media mining (Hamilton et al., 2017)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "10b0313fac132e2fa2395257c83271c9d2d596b2",
                "externalIds": {
                    "CorpusId": 259935204
                },
                "corpusId": 259935204,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/10b0313fac132e2fa2395257c83271c9d2d596b2",
                "title": "AUTOMATED DATA AUGMENTATIONS",
                "abstract": "We consider fair graph representation learning via data augmentations. While this direction has been explored previously, existing methods invariably rely on certain assumptions on the properties of fair graph data in order to design fixed strategies on data augmentations. Nevertheless, the exact properties of fair graph data may vary significantly in different scenarios. Hence, heuristically designed augmentations may not always generate fair graph data in different application scenarios. In this work, we propose a method, known as Graphair, to learn fair representations based on automated graph data augmentations. Such fairness-aware augmentations are themselves learned from data. Our Graphair is designed to automatically discover fairness-aware augmentations from input graphs in order to circumvent sensitive information while preserving other useful information. Experimental results demonstrate that our Graphair consistently outperforms many baselines on multiple node classification datasets in terms of fairness-accuracy trade-off performance. In addition, results indicate that Graphair can automatically learn to generate fair graph data without prior knowledge on fairness-relevant graph properties. Our code is publicly available as part of the DIG package (https://github.com/divelab/DIG).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47653902",
                        "name": "Zhimeng Jiang"
                    },
                    {
                        "authorId": "2004524780",
                        "name": "Youzhi Luo"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    },
                    {
                        "authorId": "49648991",
                        "name": "Na Zou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other approaches focus on augmenting specific graph model-based parameters, such as graphons [18, 41] or contextual stochastic block models (CSBMs) [55], based on the explicit downstream data generation assumptions."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d96a202ab4f61ce1db1d9e34fbdf20c9cf1018de",
                "externalIds": {
                    "CorpusId": 260725551
                },
                "corpusId": 260725551,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d96a202ab4f61ce1db1d9e34fbdf20c9cf1018de",
                "title": "Graph Contrastive Learning: An Odyssey towards Generalizable, Scalable and Principled Representation Learning on Graphs",
                "abstract": "Graph Contrastive Learning (GCL), an uprising regime of learning representations of graph-structured data, has gained significant attention in recent years. At its core, GCL leverages the idea of comparing different views of a graph to learn representations that capture desirable characteristics of the graph structures. GCL has been applied to a wide range of graph-structured data, including attributed graphs, multi-relational graphs, temporal graphs, hierarchical graphs, heterogeneous graphs, and hypergraphs. The learned graph representations yield predictive performance that generalizes well in various downstream tasks at the node, link, and graph levels, and can scale up to graphs with millions of nodes. In this paper, we present a review of representative GCL approaches with a major emphasis on our own recent efforts. Beginning with the original GCL approach with ad-hoc view generation and simple homogeneous graphs, we demonstrate how the framework can be further extended to more complex heterogeneous graphs and hypergraphs, as well as improved via principled view generation towards generalizability, fairness, interpretability, and other aspects. Theoretical explorations are covered at the end. In conclusion, we discuss the future prospects and ongoing challenges in the field of GCL.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108807305",
                        "name": "Yan Han"
                    },
                    {
                        "authorId": "89197162",
                        "name": "Yuning You"
                    },
                    {
                        "authorId": "2152934619",
                        "name": "Wenqing Zheng"
                    },
                    {
                        "authorId": "2229759886",
                        "name": "Scott Hoang"
                    },
                    {
                        "authorId": "2068207615",
                        "name": "Tianxin Wei"
                    },
                    {
                        "authorId": "2114798575",
                        "name": "Majdi Hassan"
                    },
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "119663804",
                        "name": "Ying Ding"
                    },
                    {
                        "authorId": "1705610299",
                        "name": "Yang Shen"
                    },
                    {
                        "authorId": "2156070723",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2e327de390ba5d75e41b048936bd18a648b157e0",
                "externalIds": {
                    "DBLP": "conf/icml/LiuCZZ0H23",
                    "CorpusId": 260927431
                },
                "corpusId": 260927431,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2e327de390ba5d75e41b048936bd18a648b157e0",
                "title": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations",
                "abstract": "Training graph neural networks (GNNs) is extremely time-consuming because sparse graphbased operations are hard to be accelerated by community hardware. Prior art successfully reduces the computation cost of dense matrix based operations (e.g., convolution and linear) via sampling-based approximation. However, unlike dense matrices, sparse matrices are stored in an irregular data format such that each row/column may have a different number of non-zero entries. Thus, compared to the dense counterpart, approximating sparse operations has two unique challenges (1) we cannot directly control the efficiency of approximated sparse operation since the computation is only executed on non-zero entries; (2) sampling sparse matrices is much more inefficient due to the irregular data format. To address the issues, our key idea is to control the accuracyefficiency trade-off by optimizing computation resource allocation layer-wisely and epoch-wisely. For the first challenge, we customize the computation resource to different sparse operations, while limiting the total used resource below a certain budget. For the second challenge, we cache previously sampled sparse matrices to reduce the epoch-wise sampling overhead. To this end, we propose Randomized Sparse Computation. In practice, RSC can achieve up to 11.6\u00d7 speedup for a single sparse operation and 1.6\u00d7 end-toend wall-clock time speedup with almost no accuracy drop. Codes are available at https:// github.com/warai-0toko/RSC-ICML. Department of Computer Science, Rice University, Houston, TX, USA Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong SAR. Correspondence to: Xia Hu <xia.hu@rice.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47781070",
                        "name": "Zirui Liu"
                    },
                    {
                        "authorId": "2118530137",
                        "name": "Sheng-Wei Chen"
                    },
                    {
                        "authorId": "3364022",
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "authorId": "1759658",
                        "name": "D. Zha"
                    },
                    {
                        "authorId": "97620379",
                        "name": "Xiao Huang"
                    },
                    {
                        "authorId": "2109724398",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6b44b7b59a0eedef0a6d3f3edbecf150687d63af",
                "externalIds": {
                    "CorpusId": 263764876
                },
                "corpusId": 263764876,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6b44b7b59a0eedef0a6d3f3edbecf150687d63af",
                "title": "[\u00acRe] G-Mixup: Graph Data Augmentation for Graph Classification",
                "abstract": "Scope of Reproducibility \u2014We study the graph data mixup method G\u2010Mixup developed by Han, Jiang, Liu, and Hu [1]. We investigate their claims that G\u2010Mixup theoretically pro\u2010 duces synthetic graphs that contain a mixture of key graph topologies of source graphs, experimentally improves the performance of graph neural networks (GNNs), and exper\u2010 imentally performs better than another graph data augmentation methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2256399941",
                        "name": "Dylan Cordaro"
                    },
                    {
                        "authorId": "2256389833",
                        "name": "Shelby Cox"
                    },
                    {
                        "authorId": "2256771851",
                        "name": "Yiman Ren"
                    },
                    {
                        "authorId": "2256865506",
                        "name": "Teresa Yu"
                    },
                    {
                        "authorId": "2256275271",
                        "name": "Koustuv Sinha"
                    },
                    {
                        "authorId": "1452678770",
                        "name": "Maurits J. R. Bleeker"
                    },
                    {
                        "authorId": "81679142",
                        "name": "Samarth Bhargav"
                    }
                ]
            }
        },
        {
            "contexts": [
                "com/gasteigerjo/gdc [13] G-mixup ICML 2022 GI https://github.",
                "G-Mixup [13] first estimates a graphon for each class of graphs, then performs interpolation in Euclidean space to generate mixed graphons, and finally augments synthetic graphs by sampling from the mixed graphons."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b57b1ef7108338561b4126c537854c24f20e1423",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-09970",
                    "DOI": "10.48550/arXiv.2212.09970",
                    "CorpusId": 254877079
                },
                "corpusId": 254877079,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b57b1ef7108338561b4126c537854c24f20e1423",
                "title": "Data Augmentation on Graphs: A Survey",
                "abstract": "In recent years, graph representation learning has achieved remarkable success while suffering from low-quality data problems. As a mature technology to improve data quality in computer vision, data augmentation has also attracted increasing attention in graph domain. For promoting the development of this emerging research direction, in this survey, we comprehensively review and summarize the existing graph data augmentation (GDAug) techniques. Specifically, we first summarize a variety of feasible taxonomies, and then classify existing GDAug studies based on fine-grained graph elements. Furthermore, for each type of GDAug technique, we formalize the general definition, discuss the technical details, and give schematic illustration. In addition, we also summarize common performance metrics and specific design metrics for constructing a GDAug evaluation system. Finally, we summarize the applications of GDAug from both data and model levels, as well as future directions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146367628",
                        "name": "Jiajun Zhou"
                    },
                    {
                        "authorId": "2197076698",
                        "name": "Chenxuan Xie"
                    },
                    {
                        "authorId": "1788122",
                        "name": "Z. Wen"
                    },
                    {
                        "authorId": "2197532318",
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "authorId": "2159202690",
                        "name": "Qi Xuan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f43e2a64358e2f0f420183f317c636f7df3fdffd",
                "externalIds": {
                    "DBLP": "conf/log/ZhaoTZJ0SASYJ22",
                    "CorpusId": 256942528
                },
                "corpusId": 256942528,
                "publicationVenue": {
                    "id": "50534c12-f4ba-4c64-806b-01647d1baacf",
                    "name": "LOG IN",
                    "type": "journal",
                    "alternate_names": [
                        "Log in",
                        "Log",
                        "LOG"
                    ],
                    "issn": "0720-8642",
                    "alternate_issns": [
                        "1547-4690",
                        "0024-5798"
                    ],
                    "url": "https://www.log-in-verlag.de/informatische_bildung/",
                    "alternate_urls": [
                        "https://www.anycorp.com/log/about",
                        "https://www.jstor.org/journal/log",
                        "https://www.anycorp.com/",
                        "http://www.jstor.org/action/showPublication?journalCode=log",
                        "http://www.anycorp.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f43e2a64358e2f0f420183f317c636f7df3fdffd",
                "title": "AutoGDA: Automated Graph Data Augmentation for Node Classification",
                "abstract": "Graph data augmentation has been used to improve generalizability of graph machine learning. However, by only applying \ufb01xed augmentation operations on entire graphs, existing methods overlook the unique characteristics of communities which naturally exist in the graphs. For example, different communities can have various degree distributions and homophily ratios. Ignoring such discrepancy with uni\ufb01ed augmentation strategies on the entire graph could lead to sub-optimal performance for graph data augmentation methods. In this paper, we study a novel problem of automated graph data augmentation for node classi\ufb01cation from the localized perspective of communities. We formulate it as a bilevel optimization problem: \ufb01nding a set of augmentation strategies for each community, which maximizes the performance of graph neural networks on node classi\ufb01cation. As the bilevel optimization is hard to solve directly and the search space for community-customized augmentations strategy is huge, we propose a reinforcement learning framework AutoGDA that learns the local-optimal augmentation strategy for each community sequentially. Our proposed approach outperforms established and popular baselines on public node classi\ufb01cation benchmarks as well as real industry e-commerce networks by up to +12.5% accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1742573",
                        "name": "Tong Zhao"
                    },
                    {
                        "authorId": "48784944",
                        "name": "Xianfeng Tang"
                    },
                    {
                        "authorId": "46334890",
                        "name": "Danqing Zhang"
                    },
                    {
                        "authorId": "5795999",
                        "name": "Haoming Jiang"
                    },
                    {
                        "authorId": "145850291",
                        "name": "Nikhil S. Rao"
                    },
                    {
                        "authorId": "2115714424",
                        "name": "Yiwei Song"
                    },
                    {
                        "authorId": "2155993722",
                        "name": "Pallav Agrawal"
                    },
                    {
                        "authorId": "2691095",
                        "name": "Karthik Subbian"
                    },
                    {
                        "authorId": "2021632793",
                        "name": "Bing Yin"
                    },
                    {
                        "authorId": "144812586",
                        "name": "Meng Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "097ec5407871f88517ed2521e2ab0e811a670b1b",
                "externalIds": {
                    "DBLP": "conf/nips/DengLWC22",
                    "CorpusId": 258509339
                },
                "corpusId": 258509339,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/097ec5407871f88517ed2521e2ab0e811a670b1b",
                "title": "Graph Convolution Network based Recommender Systems: Learning Guarantee and Item Mixture Powered Strategy",
                "abstract": "Inspired by their powerful representation ability on graph-structured data, Graph Convolution Networks (GCNs) have been widely applied to recommender systems, and have shown superior performance. Despite their empirical success, there is a lack of theoretical explorations such as generalization properties. In this paper, we take a first step towards establishing a generalization guarantee for GCN-based recommendation models under inductive and transductive learning. We mainly investigate the roles of graph normalization and non-linear activation, providing some theoretical understanding, and construct extensive experiments to further verify these findings empirically. Furthermore, based on the proven generalization bound and the challenge of existing models in discrete data learning, we propose Item Mixture (IMix) to enhance recommendation. It models discrete spaces in a continuous manner by mixing the embeddings of positive-negative item pairs, and its effectiveness can be strictly guaranteed from empirical and theoretical aspects.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152092068",
                        "name": "Leyan Deng"
                    },
                    {
                        "authorId": "1862782",
                        "name": "Defu Lian"
                    },
                    {
                        "authorId": "2000383268",
                        "name": "Chenwang Wu"
                    },
                    {
                        "authorId": "2113754294",
                        "name": "Enhong Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "259933538",
                "publicationVenue": null,
                "url": null,
                "title": "L EARNING F AIR G RAPH R EPRESENTATIONS VIA A UTOMATED D ATA A UGMENTATIONS",
                "abstract": null,
                "year": null,
                "authors": []
            }
        }
    ]
}