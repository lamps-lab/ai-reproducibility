{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4ca6a711b7fc314d9b111f2001d434267d2d3df6",
                "externalIds": {
                    "DOI": "10.4108/eetsis.3907",
                    "CorpusId": 262951579
                },
                "corpusId": 262951579,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4ca6a711b7fc314d9b111f2001d434267d2d3df6",
                "title": "Deep spectral network for time series clustering",
                "abstract": "Deep clustering is an approach that uses deep learning to cluster data, since it involves training a neural network model to become familiar with a data representation that is suitable for clustering. Deep clustering has been applied to a wide range of data types, including images, texts, time series and has the advantage of being able to automatically learn features from the data, which can be more effective than using hand-crafted features. It is also able to handle high-dimensional data, such as time series with many variables, which can be challenging for traditional clustering techniques. In this paper, we introduce a novel deep neural network type to improve the performance of the auto-encoder part by ignoring the unnecessary extra-noises and labelling the input data. Our approach is helpful when just a limited amount of labelled data is available, but labelling a big amount of data would be costly or time-consuming. It also applies for the data in high-dimensional and difficult to define a good set of features for clustering.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "102497891",
                        "name": "Duc-Trung Hoang"
                    },
                    {
                        "authorId": "2247652083",
                        "name": "Mahdi Achache"
                    },
                    {
                        "authorId": "2174389083",
                        "name": "Vinay Kumar Jain"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "of representing learning on the UTS and the MTS datasets Franceschi et al. (2019). D.",
                "Srlt Franceschi et al. (2019): The abbreviation Srlt comes from the paper title \u2014 Unsupervised Scalable Representation Learning for Multivariate Time Series. Inspired by how word2vec Mikolov et al. (2013) is trained, this method proposes a novel triplet loss for modeling the time series data.",
                "of representing learning on the UTS and the MTS datasets Franceschi et al. (2019). D.Conv consists of layers of dilated convolutions. Compared to the previous work Franceschi et al. (2019), we do not use causal convolutions, so as to incorporate information from both before and after time step i when conducting convolution operations. Time-Series Transformer (TsTransformer): The Time-Series Transformer has proved a success in representing the MTS data type Zerveas et al. (2021). It has the same structure as the original transformer encoder Vaswani et al. (2017), except that it replaces the Layer Normalization layer with the Batch Normalization layer and the embedding layer with the linear projection layer.",
                "of representing learning on the UTS and the MTS datasets Franceschi et al. (2019). D.Conv consists of layers of dilated convolutions. Compared to the previous work Franceschi et al. (2019), we do not use causal convolutions, so as to incorporate information from both before and after time step i when conducting convolution operations. Time-Series Transformer (TsTransformer): The Time-Series Transformer has proved a success in representing the MTS data type Zerveas et al. (2021). It has the same structure as the original transformer encoder Vaswani et al.",
                "Effects on optimization Non-convex optimization in high-dimensional space has been a major challenge in deep learning since the overall process is affected by many factors Goodfellow et al. (2016), such as the initial parameters, the choice of the optimizer, the model structure, etc.",
                "(2020); Franceschi et al. (2019). Second, under limited computational resources, we chose to do experiments on as many datasets as possible to make conclusions informative and credible.",
                "Srlt Franceschi et al. (2019): The abbreviation Srlt comes from the paper title \u2014 Unsupervised Scalable Representation Learning for Multivariate Time Series.",
                "of representing learning on the UTS and the MTS datasets Franceschi et al. (2019). D.Conv consists of layers of dilated convolutions. Compared to the previous work Franceschi et al. (2019), we do not use causal convolutions, so as to incorporate information from both before and after time step i when conducting convolution operations.",
                "(2020); Franceschi et al. (2019). Second, under limited computational resources, we chose to do experiments on as many datasets as possible to make conclusions informative and credible. It is worth noting that the model structures presented below all model the time series in both directions. LSTM: Previous work Sagheer and Kotb (2019) shows the usefulness of unsupervised pretraining of LSTM-based autoencoder for MTS prediction tasks.",
                "(2020); Franceschi et al. (2019). Second, under limited computational resources, we chose to do experiments on as many datasets as possible to make conclusions informative and credible. It is worth noting that the model structures presented below all model the time series in both directions. LSTM: Previous work Sagheer and Kotb (2019) shows the usefulness of unsupervised pretraining of LSTM-based autoencoder for MTS prediction tasks. Here we have simplified the model by constructing a vanilla bidirectional LSTM of two layers. Dilated Convolutional Neural Network (D.Conv): The Convolutional Neural Network performs well on time series forecasting Yue et al. (2022) and demonstrates its strengths"
            ],
            "citingPaper": {
                "paperId": "094f5c1e0cd4a5d22a637c65cf6afd6c7313f124",
                "externalIds": {
                    "ArXiv": "2309.05256",
                    "CorpusId": 261682048
                },
                "corpusId": 261682048,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/094f5c1e0cd4a5d22a637c65cf6afd6c7313f124",
                "title": "Examining the Effect of Pre-training on Time Series Classification",
                "abstract": "Although the pre-training followed by fine-tuning paradigm is used extensively in many fields, there is still some controversy surrounding the impact of pre-training on the fine-tuning process. Currently, experimental findings based on text and image data lack consensus. To delve deeper into the unsupervised pre-training followed by fine-tuning paradigm, we have extended previous research to a new modality: time series. In this study, we conducted a thorough examination of 150 classification datasets derived from the Univariate Time Series (UTS) and Multivariate Time Series (MTS) benchmarks. Our analysis reveals several key conclusions. (i) Pre-training can only help improve the optimization process for models that fit the data poorly, rather than those that fit the data well. (ii) Pre-training does not exhibit the effect of regularization when given sufficient training time. (iii) Pre-training can only speed up convergence if the model has sufficient ability to fit the data. (iv) Adding more pre-training data does not improve generalization, but it can strengthen the advantage of pre-training on the original data volume, such as faster convergence. (v) While both the pre-training task and the model structure determine the effectiveness of the paradigm on a given dataset, the model structure plays a more significant role.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34260749",
                        "name": "Jiashu Pu"
                    },
                    {
                        "authorId": "2000896394",
                        "name": "Shiwei Zhao"
                    },
                    {
                        "authorId": "2239055014",
                        "name": "Ling Cheng"
                    },
                    {
                        "authorId": "2152554888",
                        "name": "Yongzhu Chang"
                    },
                    {
                        "authorId": "2239054423",
                        "name": "Runze Wu"
                    },
                    {
                        "authorId": "2238952248",
                        "name": "Tangjie Lv"
                    },
                    {
                        "authorId": "48263731",
                        "name": "Rongsheng Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "22c0e041e205364a67b318a00f949b9d60c3da11",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-05305",
                    "ArXiv": "2309.05305",
                    "DOI": "10.48550/arXiv.2309.05305",
                    "CorpusId": 261682449
                },
                "corpusId": 261682449,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/22c0e041e205364a67b318a00f949b9d60c3da11",
                "title": "Fully-Connected Spatial-Temporal Graph for Multivariate Time Series Data",
                "abstract": "Multivariate Time-Series (MTS) data is crucial in various application fields. With its sequential and multi-source (multiple sensors) properties, MTS data inherently exhibits Spatial-Temporal (ST) dependencies, involving temporal correlations between timestamps and spatial correlations between sensors in each timestamp. To effectively leverage this information, Graph Neural Network-based methods (GNNs) have been widely adopted. However, existing approaches separately capture spatial dependency and temporal dependency and fail to capture the correlations between Different sEnsors at Different Timestamps (DEDT). Overlooking such correlations hinders the comprehensive modelling of ST dependencies within MTS data, thus restricting existing GNNs from learning effective representations. To address this limitation, we propose a novel method called Fully-Connected Spatial-Temporal Graph Neural Network (FC-STGNN), including two key components namely FC graph construction and FC graph convolution. For graph construction, we design a decay graph to connect sensors across all timestamps based on their temporal distances, enabling us to fully model the ST dependencies by considering the correlations between DEDT. Further, we devise FC graph convolution with a moving-pooling GNN layer to effectively capture the ST dependencies for learning effective representations. Extensive experiments show the effectiveness of FC-STGNN on multiple MTS datasets compared to SOTA methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2204444421",
                        "name": "Yucheng Wang"
                    },
                    {
                        "authorId": "9734988",
                        "name": "Yuecong Xu"
                    },
                    {
                        "authorId": "2562263",
                        "name": "Jianfei Yang"
                    },
                    {
                        "authorId": "1390606776",
                        "name": "Min Wu"
                    },
                    {
                        "authorId": "2108674591",
                        "name": "Xiaoli Li"
                    },
                    {
                        "authorId": "2152786430",
                        "name": "Lihua Xie"
                    },
                    {
                        "authorId": "48354147",
                        "name": "Zhenghua Chen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "The evaluation results presented in the aforementioned paper demonstrate that TST outperforms other methods, such as ROCKET [18], dilation-CNN [24], XGBoost [10], and LSTM [39], on a range of datasets, achieving the best average rank across multiple dimensions."
            ],
            "citingPaper": {
                "paperId": "ea2cff09153e1d2f15bcb4880ba050db20aa53b8",
                "externalIds": {
                    "DOI": "10.1145/3604277",
                    "CorpusId": 261706615
                },
                "corpusId": 261706615,
                "publicationVenue": {
                    "id": "425553d6-e479-478a-8dd7-ae59d3f32b72",
                    "name": "Proceedings of the ACM on Human-Computer Interaction",
                    "alternate_names": [
                        "Proc ACM Human-computer Interact"
                    ],
                    "issn": "2573-0142",
                    "url": "https://dl.acm.org/citation.cfm?id=3120954",
                    "alternate_urls": [
                        "https://dl.acm.org/citation.cfm?id=J1598&picked=prox"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ea2cff09153e1d2f15bcb4880ba050db20aa53b8",
                "title": "UnifiedSense: Enabling Without-Device Gesture Interactions Using Over-the-shoulder Training Between Redundant Wearable Sensors",
                "abstract": "Wearable devices allow quick and convenient interactions for controlling mobile computers. However, these interactions are often device-dependent, and users cannot control devices in a way they are familiar with if they do not wear the same wearable device. This paper proposes a new method, UnifiedSense, to enable device-dependent gestures even when the device that detects such gestures is missing by utilizing sensors on other wearable devices. UnifiedSense achieves this without explicit gesture training for different devices, by training its recognition model while users naturally perform gestures. The recognizer uses the gestures detected on the primary device (i.e., a device that reliably detects gestures) as labels for training samples and collects sensor data from all other available devices on the user. We conducted a technical evaluation with data collected from 15 participants with four types of wearable devices. It showed that UnifiedSense could correctly recognize 5 gestures (5 gestures \u00d7 5 configurations) with an accuracy of 90.9% (SD = 1.9%) without the primary device present.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9281163",
                        "name": "Md. Aashikur Rahman Azim"
                    },
                    {
                        "authorId": "2115322908",
                        "name": "Adil Rahman"
                    },
                    {
                        "authorId": "1724588",
                        "name": "Seongkook Heo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Using the time series embeddings from [24], it learns embeddings that seamlessly blend with the local context.",
                "Specifically, for DS and PS, we adopt two layers of LSTM; for C-FID, we adopt ts2vec [24] as the backbone."
            ],
            "citingPaper": {
                "paperId": "406f9cda15bc6f078d2b6f31a0ad2aad15a8021a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-03755",
                    "ArXiv": "2309.03755",
                    "DOI": "10.48550/arXiv.2309.03755",
                    "CorpusId": 261582354
                },
                "corpusId": 261582354,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/406f9cda15bc6f078d2b6f31a0ad2aad15a8021a",
                "title": "TSGBench: Time Series Generation Benchmark",
                "abstract": "Synthetic Time Series Generation (TSG) is crucial in a range of applications, including data augmentation, anomaly detection, and privacy preservation. Although significant strides have been made in this field, existing methods exhibit three key limitations: (1) They often benchmark against similar model types, constraining a holistic view of performance capabilities. (2) The use of specialized synthetic and private datasets introduces biases and hampers generalizability. (3) Ambiguous evaluation measures, often tied to custom networks or downstream tasks, hinder consistent and fair comparison. To overcome these limitations, we introduce \\textsf{TSGBench}, the inaugural TSG Benchmark, designed for a unified and comprehensive assessment of TSG methods. It comprises three modules: (1) a curated collection of publicly available, real-world datasets tailored for TSG, together with a standardized preprocessing pipeline; (2) a comprehensive evaluation measures suite including vanilla measures, new distance-based assessments, and visualization tools; (3) a pioneering generalization test rooted in Domain Adaptation (DA), compatible with all methods. We have conducted extensive experiments across ten real-world datasets from diverse domains, utilizing ten advanced TSG methods and twelve evaluation measures, all gauged through \\textsf{TSGBench}. The results highlight its remarkable efficacy and consistency. More importantly, \\textsf{TSGBench} delivers a statistical breakdown of method rankings, illuminating performance variations across different datasets and measures, and offering nuanced insights into the effectiveness of each method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2238206194",
                        "name": "Yihao Ang"
                    },
                    {
                        "authorId": "2111287212",
                        "name": "Qiang Huang"
                    },
                    {
                        "authorId": "2238210777",
                        "name": "Yifan Bao"
                    },
                    {
                        "authorId": "2238206423",
                        "name": "Anthony K. H. Tung"
                    },
                    {
                        "authorId": "2225237094",
                        "name": "Zhiyong Huang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "972c2e86da0fefe2b716d46716b1e018f07949d6",
                "externalIds": {
                    "DOI": "10.1016/j.engappai.2023.106543",
                    "CorpusId": 259795092
                },
                "corpusId": 259795092,
                "publicationVenue": {
                    "id": "1a24ea21-4c37-41d8-9e76-ab802d4afb3e",
                    "name": "Engineering applications of artificial intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Eng appl artif intell",
                        "Eng Appl Artif Intell",
                        "Engineering Applications of Artificial Intelligence"
                    ],
                    "issn": "0952-1976",
                    "url": "http://www.sciencedirect.com/science/journal/09521976"
                },
                "url": "https://www.semanticscholar.org/paper/972c2e86da0fefe2b716d46716b1e018f07949d6",
                "title": "GTSNet: Flexible architecture under budget constraint for real-time human activity recognition from wearable sensor",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109107330",
                        "name": "Jaegyun Park"
                    },
                    {
                        "authorId": "2166546888",
                        "name": "W. Lim"
                    },
                    {
                        "authorId": "1875093",
                        "name": "Dae-Won Kim"
                    },
                    {
                        "authorId": "46664325",
                        "name": "Jaesung Lee"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "39f496fb661e9fafe229aa289105112ee1f76f95",
                "externalIds": {
                    "DOI": "10.1016/j.patcog.2023.109943",
                    "CorpusId": 261649447
                },
                "corpusId": 261649447,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/39f496fb661e9fafe229aa289105112ee1f76f95",
                "title": "Multi-scale self-supervised representation learning with temporal alignment for multi-rate time series modeling",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1452347263",
                        "name": "Jiawei Chen"
                    },
                    {
                        "authorId": "2057359684",
                        "name": "Pengyu Song"
                    },
                    {
                        "authorId": "2238906417",
                        "name": "Chunhui Zhao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b00b98067f84dae982c43752f95002d18374e31c",
                "externalIds": {
                    "DOI": "10.1016/j.ress.2023.109602",
                    "CorpusId": 261769993
                },
                "corpusId": 261769993,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b00b98067f84dae982c43752f95002d18374e31c",
                "title": "Incorporating prior knowledge into self-supervised representation learning for long PHM signal",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143444772",
                        "name": "Yi-Lin Wang"
                    },
                    {
                        "authorId": "2156574151",
                        "name": "Yuanxiang Li"
                    },
                    {
                        "authorId": "46868456",
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "authorId": "2239973161",
                        "name": "Jia Lei"
                    },
                    {
                        "authorId": "2119042044",
                        "name": "Yifei Yu"
                    },
                    {
                        "authorId": "2240201281",
                        "name": "Tongtong Zhang"
                    },
                    {
                        "authorId": "2164838249",
                        "name": "Yongshen Yang"
                    },
                    {
                        "authorId": "1471640469",
                        "name": "Honghua Zhao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9ea883e5ec2806f60f3126958444026f67cdcd92",
                "externalIds": {
                    "ArXiv": "2308.11200",
                    "DBLP": "journals/corr/abs-2308-11200",
                    "DOI": "10.48550/arXiv.2308.11200",
                    "CorpusId": 261064627
                },
                "corpusId": 261064627,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9ea883e5ec2806f60f3126958444026f67cdcd92",
                "title": "SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting",
                "abstract": "RNN-based methods have faced challenges in the Long-term Time Series Forecasting (LTSF) domain when dealing with excessively long look-back windows and forecast horizons. Consequently, the dominance in this domain has shifted towards Transformer, MLP, and CNN approaches. The substantial number of recurrent iterations are the fundamental reasons behind the limitations of RNNs in LTSF. To address these issues, we propose two novel strategies to reduce the number of iterations in RNNs for LTSF tasks: Segment-wise Iterations and Parallel Multi-step Forecasting (PMF). RNNs that combine these strategies, namely SegRNN, significantly reduce the required recurrent iterations for LTSF, resulting in notable improvements in forecast accuracy and inference speed. Extensive experiments demonstrate that SegRNN not only outperforms SOTA Transformer-based models but also reduces runtime and memory usage by more than 78%. These achievements provide strong evidence that RNNs continue to excel in LTSF tasks and encourage further exploration of this domain with more RNN-based approaches. The source code is coming soon.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2230088607",
                        "name": "Shengsheng Lin"
                    },
                    {
                        "authorId": "2154072278",
                        "name": "Weiwei Lin"
                    },
                    {
                        "authorId": "3414155",
                        "name": "Wentai Wu"
                    },
                    {
                        "authorId": "2091850372",
                        "name": "Feiyu Zhao"
                    },
                    {
                        "authorId": "1398103308",
                        "name": "Ruichao Mo"
                    },
                    {
                        "authorId": "2214826143",
                        "name": "Haotong Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "To overcome these limitations, methods such as TNC [56] and T-Loss [17] have been proposed."
            ],
            "citingPaper": {
                "paperId": "7d7d856790d9f03dfcd2bb4437c2022dd62dc7eb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-09296",
                    "ArXiv": "2308.09296",
                    "DOI": "10.48550/arXiv.2308.09296",
                    "CorpusId": 261030943
                },
                "corpusId": 261030943,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7d7d856790d9f03dfcd2bb4437c2022dd62dc7eb",
                "title": "CARLA: A Self-supervised Contrastive Representation Learning Approach for Time Series Anomaly Detection",
                "abstract": "We introduce a Self-supervised Contrastive Representation Learning Approach for Time Series Anomaly Detection (CARLA), an innovative end-to-end self-supervised framework carefully developed to identify anomalous patterns in both univariate and multivariate time series data. By taking advantage of contrastive representation learning, We introduce an innovative end-to-end self-supervised deep learning framework carefully developed to identify anomalous patterns in both univariate and multivariate time series data. By taking advantage of contrastive representation learning, CARLA effectively generates robust representations for time series windows. It achieves this by 1) learning similar representations for temporally close windows and dissimilar representations for windows and their equivalent anomalous windows and 2) employing a self-supervised approach to classify normal/anomalous representations of windows based on their nearest/furthest neighbours in the representation space. Most of the existing models focus on learning normal behaviour. The normal boundary is often tightly defined, which can result in slight deviations being classified as anomalies, resulting in a high false positive rate and limited ability to generalise normal patterns. CARLA's contrastive learning methodology promotes the production of highly consistent and discriminative predictions, thereby empowering us to adeptly address the inherent challenges associated with anomaly detection in time series data. Through extensive experimentation on 7 standard real-world time series anomaly detection benchmark datasets, CARLA demonstrates F1 and AU-PR superior to existing state-of-the-art results. Our research highlights the immense potential of contrastive representation learning in advancing the field of time series anomaly detection, thus paving the way for novel applications and in-depth exploration in this domain.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2117009",
                        "name": "Zahra Zamanzadeh Darban"
                    },
                    {
                        "authorId": "1726660",
                        "name": "Geoffrey I. Webb"
                    },
                    {
                        "authorId": "2585415",
                        "name": "Shirui Pan"
                    },
                    {
                        "authorId": "39044014",
                        "name": "Mahsa Salehi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d84cf745c534c010b8e55e5a4a04878906848dc3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-08241",
                    "ArXiv": "2308.08241",
                    "DOI": "10.48550/arXiv.2308.08241",
                    "CorpusId": 260926650
                },
                "corpusId": 260926650,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d84cf745c534c010b8e55e5a4a04878906848dc3",
                "title": "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
                "abstract": "This work summarizes two strategies for completing time-series (TS) tasks using today's language model (LLM): LLM-for-TS, design and train a fundamental large model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS data. Considering the insufficient data accumulation, limited resources, and semantic context requirements, this work focuses on TS-for-LLM methods, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed them by instance-wise, feature-wise, and text-prototype-aligned contrast, and then creates prompts to make LLM more open to embeddings, and finally implements TS tasks. Experiments are carried out on TS classification and forecasting tasks using 8 LLMs with different structures and sizes. Although its results cannot significantly outperform the current SOTA models customized for TS tasks, by treating LLM as the pattern machine, it can endow LLM's ability to process TS data without compromising the language ability. This paper is intended to serve as a foundational work that will inspire further research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144102247",
                        "name": "Chenxi Sun"
                    },
                    {
                        "authorId": "2110479359",
                        "name": "Yaliang Li"
                    },
                    {
                        "authorId": "2115263944",
                        "name": "Hongyan Li"
                    },
                    {
                        "authorId": "2317297",
                        "name": "linda Qiao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In the recent, deep neural networks begin to automatically learn complex features in TSC, including supervised feature mining [24], unsupervised feature learning [23], transformer methods [15, 48, 49, 68], etc."
            ],
            "citingPaper": {
                "paperId": "c2e57a1926217f67a72c617d09fa12ec8e667d0e",
                "externalIds": {
                    "DBLP": "conf/kdd/WangYJ023",
                    "DOI": "10.1145/3580305.3599549",
                    "CorpusId": 260499799
                },
                "corpusId": 260499799,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/c2e57a1926217f67a72c617d09fa12ec8e667d0e",
                "title": "WHEN: A Wavelet-DTW Hybrid Attention Network for Heterogeneous Time Series Analysis",
                "abstract": "Given its broad applications, time series analysis has gained substantial research attention but remains a very challenging task. Recent years have witnessed the great success of deep learning methods, eg., CNN and RNN, in time series classification and forecasting, but heterogeneity as the very nature of time series has not yet been addressed adequately and remains the performance \"treadstone.\" In this light, we argue that the intra-sequence non-stationarity and inter-sequence asynchronism are two types of heterogeneities widely existed in multiple times series, and propose a hybrid attention network called WHEN as deep learning solution. WHEN features in two attention mechanisms in two different modules. In the WaveAtt module, we propose a novel data-dependent wavelet function and integrate it into the BiLSTM network as the wavelet attention, for the purpose of analyzing dynamic frequency components in nonstationary time series. In the DTWAtt module, we transform the dynamic time warping (DTW) technique into the form as the DTW attention, where all input sequences are synchronized with a universal parameter sequence to overcome the time distortion problem in multiple time series. WHEN with the hybrid attentions is then formulated as task-dependent neural network for either classification or forecasting tasks. Extensive experiments on 30 UEA datasets and 3 real-world datasets with rich competitive baselines demonstrate the excellent performance of our model. The ability of WHEN in dealing with time series heterogeneities is also elaborately explored via specially designed analysis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48093287",
                        "name": "Jingyuan Wang"
                    },
                    {
                        "authorId": "2154171453",
                        "name": "Chen Yang"
                    },
                    {
                        "authorId": "2144809917",
                        "name": "Xiaohan Jiang"
                    },
                    {
                        "authorId": "144637541",
                        "name": "Junjie Wu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[19] have lower performance on one or more datasets."
            ],
            "citingPaper": {
                "paperId": "7a2b0e9a62c4b63a5e265fb35ff1142fb22f0039",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-04637",
                    "ArXiv": "2308.04637",
                    "DOI": "10.1145/3580305.3599508",
                    "CorpusId": 260499900
                },
                "corpusId": 260499900,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/7a2b0e9a62c4b63a5e265fb35ff1142fb22f0039",
                "title": "Sparse Binary Transformers for Multivariate Time Series Modeling",
                "abstract": "Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attention mask to allow computation only at the current time step. Together, each compression technique and attention modification substantially reduces the number of non-zero operations necessary in the Transformer. We measure the computational savings of our approach over a range of metrics including parameter count, bit size, and floating point operation (FLOPs) count, showing up to a 53\u00d7 reduction in storage size and up to 10.5\u00d7 reduction in FLOPs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1512264892",
                        "name": "Matt Gorbett"
                    },
                    {
                        "authorId": "1988829",
                        "name": "H. Shirazi"
                    },
                    {
                        "authorId": "144039860",
                        "name": "I. Ray"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "e) T-Loss [50]: T-Loss learns scalable general-purpose representations by considering inherent characteristics of time series, including highly variable lengths and sparse labeling.",
                "Existing temporal-level contrastive learning models either consider the temporal dependencies by leveraging temporal contrasting modules [33, 50, 51], or focus on capturing multi-scale contextual information across different granularities [52, 61, 112, 113].",
                "The model is trained through a multinomial logistic regression classifier, which aims to accurately discriminate all time segments in a time series by utilizing the segment indices as labels for the data points.\ne) T-Loss [50]: T-Loss learns scalable general-purpose representations by considering inherent characteristics of time series, including highly variable lengths and sparse labeling.",
                "inherent in time series data, researchers have explored the feasibility of distinguishing contextual information at a finegrained temporal level [50, 51, 52].",
                "advantages in representation learning for various types of data, including image [43, 44, 45, 46], video [47, 48, 49] and time series [50, 51, 52, 53].",
                "T-Loss [50] T-Loss learns scalable representations by taking highly variable lengths and sparse labeling properties of time series data into account.",
                "Temporal-Level TS2Vec [52] TS-TCC [51] T-Loss [50]",
                "From the results, it can be observed that contrastive learning models such as TS2Vec [52], TSTCC [51] and T-Loss [50], which emphasize the impact at the temporal level, achieve better results compared to methods that focus on instance-level or prototype-level contrast.",
                "Approaches such as T-Loss [50] and TNC [33] utilize the information from the neighborhood to construct positive and negative samples for contrastive learning."
            ],
            "citingPaper": {
                "paperId": "916bc9ed62113ad7a5bb8096051593df4ebb2061",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-01578",
                    "ArXiv": "2308.01578",
                    "DOI": "10.48550/arXiv.2308.01578",
                    "CorpusId": 260438737
                },
                "corpusId": 260438737,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/916bc9ed62113ad7a5bb8096051593df4ebb2061",
                "title": "Unsupervised Representation Learning for Time Series: A Review",
                "abstract": "Unsupervised representation learning approaches aim to learn discriminative feature representations from unlabeled data, without the requirement of annotating every sample. Enabling unsupervised representation learning is extremely crucial for time series data, due to its unique annotation bottleneck caused by its complex characteristics and lack of visual cues compared with other data modalities. In recent years, unsupervised representation learning techniques have advanced rapidly in various domains. However, there is a lack of systematic analysis of unsupervised representation learning approaches for time series. To fill the gap, we conduct a comprehensive literature review of existing rapidly evolving unsupervised representation learning approaches for time series. Moreover, we also develop a unified and standardized library, named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast implementations and unified evaluations on various models. With ULTS, we empirically evaluate state-of-the-art approaches, especially the rapidly evolving contrastive learning methods, on 9 diverse real-world datasets. We further discuss practical considerations as well as open research challenges on unsupervised representation learning for time series to facilitate future research in this field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "103236149",
                        "name": "Qianwen Meng"
                    },
                    {
                        "authorId": "1732549",
                        "name": "Hangwei Qian"
                    },
                    {
                        "authorId": "2144384782",
                        "name": "Yong Liu"
                    },
                    {
                        "authorId": "153018970",
                        "name": "Yonghui Xu"
                    },
                    {
                        "authorId": "2111639168",
                        "name": "Zhiqi Shen"
                    },
                    {
                        "authorId": "101457473",
                        "name": "Li-zhen Cui"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Finally, we follow the same protocol as [10], where a decoder is trained on top of the",
                "[10] encouraged representations that closely resemble sampled subseries, while Tonekaboni et al.",
                "We perform comprehensive experiments on time series classification to assess the classification performance of our approach, in comparison to other unsupervised time series representation models, namely T-Loss [10], TS-TCC [9], TST [49], and TNC [32]."
            ],
            "citingPaper": {
                "paperId": "a2d16169c571ec32bf4df13f26e8c1630a6b607b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-01011",
                    "ArXiv": "2308.01011",
                    "DOI": "10.48550/arXiv.2308.01011",
                    "CorpusId": 260379161
                },
                "corpusId": 260379161,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a2d16169c571ec32bf4df13f26e8c1630a6b607b",
                "title": "Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach",
                "abstract": "Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, forecasting, and anomaly detection tasks to demonstrate the effectiveness of Floss. We incorporate Floss into several representative deep learning solutions to justify our design choices and demonstrate that it is capable of automatically discovering periodic dynamics and improving state-of-the-art deep learning models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2067957",
                        "name": "Chunwei Yang"
                    },
                    {
                        "authorId": "2155631234",
                        "name": "Xiaoxu Chen"
                    },
                    {
                        "authorId": "1490866355",
                        "name": "Lijun Sun"
                    },
                    {
                        "authorId": "2134725367",
                        "name": "Hongyu Yang"
                    },
                    {
                        "authorId": "1641939256",
                        "name": "Yuankai Wu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[56], is a deep learning architecture developed to model causal relationships in sequential data."
            ],
            "citingPaper": {
                "paperId": "9af4c980f6964da45b80e253b8f18969ff9373f1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-02412",
                    "ArXiv": "2308.02412",
                    "DOI": "10.48550/arXiv.2308.02412",
                    "CorpusId": 260610947
                },
                "corpusId": 260610947,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9af4c980f6964da45b80e253b8f18969ff9373f1",
                "title": "Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study",
                "abstract": "Recently, with the advancement of the Internet of Things (IoT), WiFi CSI-based HAR has gained increasing attention from academic and industry communities. By integrating the deep learning technology with CSI-based HAR, researchers achieve state-of-the-art performance without the need of expert knowledge. However, the scarcity of labeled CSI data remains the most prominent challenge when applying deep learning models in the context of CSI-based HAR due to the privacy and incomprehensibility of CSI-based HAR data. On the other hand, SSL has emerged as a promising approach for learning meaningful representations from data without heavy reliance on labeled examples. Therefore, considerable efforts have been made to address the challenge of insufficient data in deep learning by leveraging SSL algorithms. In this paper, we undertake a comprehensive inventory and analysis of the potential held by different categories of SSL algorithms, including those that have been previously studied and those that have not yet been explored, within the field. We provide an in-depth investigation of SSL algorithms in the context of WiFi CSI-based HAR. We evaluate four categories of SSL algorithms using three publicly available CSI HAR datasets, each encompassing different tasks and environmental settings. To ensure relevance to real-world applications, we design performance metrics that align with specific requirements. Furthermore, our experimental findings uncover several limitations and blind spots in existing work, highlighting the barriers that need to be addressed before SSL can be effectively deployed in real-world WiFi-based HAR applications. Our results also serve as a practical guideline for industry practitioners and provide valuable insights for future research endeavors in this field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2190019169",
                        "name": "Ke Xu"
                    },
                    {
                        "authorId": "2118443494",
                        "name": "Jiangtao Wang"
                    },
                    {
                        "authorId": "7296648",
                        "name": "Hongyuan Zhu"
                    },
                    {
                        "authorId": "2055037499",
                        "name": "Dingchang Zheng"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "To overcome the limitations of RNNs and CNNs and their variants, there have been roughly twomain streams of research: Transformer-based models [13, 17, 19, 23] and time-series representation learning models [4, 5, 21]."
            ],
            "citingPaper": {
                "paperId": "b23b506a1922c6b6a7fe725fcce2af4f071b21d2",
                "externalIds": {
                    "DBLP": "conf/sigir/KimC23",
                    "DOI": "10.1145/3539618.3592013",
                    "CorpusId": 259949794
                },
                "corpusId": 259949794,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/b23b506a1922c6b6a7fe725fcce2af4f071b21d2",
                "title": "Look Ahead: Improving the Accuracy of Time-Series Forecasting by Previewing Future Time Features",
                "abstract": "Time-series forecasting has been actively studied and adopted in various real-world domains. Recently there have been two research mainstreams in this area: building Transformer-based architectures such as Informer, Autoformer and Reformer, and developing time-series representation learning frameworks based on contrastive learning such as TS2Vec and CoST. Both efforts have greatly improved the performance of time series forecasting. In this paper, we investigate a novel direction towards improving the forecasting performance even more, which is orthogonal to the aforementioned mainstreams as a model-agnostic scheme. We focus on time stamp embeddings that has been less-focused in the literature. Our idea is simple-yet-effective: based on given current time stamp, we predict embeddings of its near future time stamp and utilize the predicted embeddings in the time-series (value) forecasting task. We believe that if such future time information can be previewed at the time of prediction, they can be utilized by any time-series forecasting models as useful additional information. Our experimental results confirmed that our method consistently and significantly improves the accuracy of the recent Transformer-based models and time-series representation learning frameworks. Our code is available at: https://github.com/sunsunmin/Look_Ahead",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223803407",
                        "name": "Seonmin Kim"
                    },
                    {
                        "authorId": "3257314",
                        "name": "Dong-Kyu Chae"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "The work in [25] adopts subsequences to generate positive and negative pairs.",
                "Similar to the augmentation network, we utilize a neural network with two components, a fully connected layer, and a multi-layer dilated CNN module [25], [61] as the feature extraction encoder.",
                "Compared to other forms of data, the time series domain has seen less research on contrastive learning [22], [23], [25], [51].",
                "During our experiments, we utilize a grid search algorithm to explore the search space of [1, 2, 3, 4, 5, 10, 15, 20, 25, 30] for optimizing the anomaly threshold proportion r.",
                "Recently, efforts have been made to utilize self-supervised learning in the context of time series data [22], [23], [25], [39], [51], [61]."
            ],
            "citingPaper": {
                "paperId": "d0d3bb0a46dca097deca000737356f6fd0942f70",
                "externalIds": {
                    "DBLP": "conf/eurosp/ZhengWCSL23",
                    "DOI": "10.1109/EuroSPW59978.2023.00046",
                    "CorpusId": 260387117
                },
                "corpusId": 260387117,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d0d3bb0a46dca097deca000737356f6fd0942f70",
                "title": "Unsafe Behavior Detection with Adaptive Contrastive Learning in Industrial Control Systems",
                "abstract": "Unsafe behavior detection is crucial for maintaining safe and reliable operations in various industrial control systems. However, the scarcity of labeled samples for model training poses significant challenges for existing methods. Self-supervised learning, particularly contrastive learning, offers a promising solution due to its ability to learn from unlabelled data. In this paper, we present AdaTCL, a contrastive learning framework with adaptive augmentations, to detect unsafe behavior in industrial control systems. By dividing instances into task-irrelevant and informative parts and applying lossless transform functions, AdaTCL prevents ad-hoc decisions and laborious trial-and-error tuning for augmentation selection, which improves the generalization capability of contrastive learning. Our experiments demonstrate that AdaTCL significantly outperforms classic and recent baselines highlighting the potential of state-of-the-art self-supervised learning techniques for industrial control systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226672788",
                        "name": "Xu Zheng"
                    },
                    {
                        "authorId": "40606845",
                        "name": "Tianchun Wang"
                    },
                    {
                        "authorId": "2226526095",
                        "name": "S. Y. Chowdhury"
                    },
                    {
                        "authorId": "2068173144",
                        "name": "Ruimin Sun"
                    },
                    {
                        "authorId": "2226519756",
                        "name": "Dongsheng Luo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "671fd4179acdd25a2b88f108e7db4090d8ae2605",
                "externalIds": {
                    "DBLP": "journals/tii/CastangiaBCQMP23",
                    "DOI": "10.1109/TII.2022.3217495",
                    "CorpusId": 253366593
                },
                "corpusId": 253366593,
                "publicationVenue": {
                    "id": "2135230a-3b24-4b71-9583-60624389377a",
                    "name": "IEEE Transactions on Industrial Informatics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Ind Informatics"
                    ],
                    "issn": "1551-3203",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=9424",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9424"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/671fd4179acdd25a2b88f108e7db4090d8ae2605",
                "title": "Clustering Appliance Operation Modes With Unsupervised Deep Learning Techniques",
                "abstract": "In smart grids, consumers can be involved in demand response programs to reduce the total power consumption of their households during the peak hours of the day. Unfortunately, nowadays, utility companies are facing important challenges in the implementation of demand response programs because of their negative impact on the comfort of end-users. In this article, we cluster the different operation modes of household appliances based on the analysis of their power signatures. For this purpose, we implement an autoencoder neural network to create a better data representation of the power signatures. Then, we cluster the different operational programs by using a K-means algorithm fitted to the new data representation. To test our methodology, we study the operation modes of some washing machines and dishwashers whose power signatures were derived from both submeters and nonintrusive load monitoring techniques. Our clustering analysis reveals the existence of multiple working programs showing well-defined features in terms of both average energy consumption and duration. Our results can then be used to improve demand response programs by reducing their impact on the comfort of end-users. Furthermore, end-users can rely on our framework to favor lighter operation modes and reduce their overall energy consumption.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2087399051",
                        "name": "M. Castangia"
                    },
                    {
                        "authorId": "2190010070",
                        "name": "Nicola Barletta"
                    },
                    {
                        "authorId": "39603467",
                        "name": "Christian Camarda"
                    },
                    {
                        "authorId": "8038281",
                        "name": "S. Quer"
                    },
                    {
                        "authorId": "1708440",
                        "name": "E. Macii"
                    },
                    {
                        "authorId": "2315556",
                        "name": "E. Patti"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b460d27829f6aa9e66772cf0b36ecf4d312d349d",
                "externalIds": {
                    "DBLP": "journals/isci/HuHLD23",
                    "DOI": "10.1016/j.ins.2023.03.143",
                    "CorpusId": 257850945
                },
                "corpusId": 257850945,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/b460d27829f6aa9e66772cf0b36ecf4d312d349d",
                "title": "A contrastive learning based universal representation for time series forecasting",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145815852",
                        "name": "Jie Hu"
                    },
                    {
                        "authorId": "2213992386",
                        "name": "Zhanao Hu"
                    },
                    {
                        "authorId": "2118910985",
                        "name": "Tianrui Li"
                    },
                    {
                        "authorId": "18103812",
                        "name": "Shengdong Du"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6e71d5a4b54cf7969012c30ca8b999981feadd22",
                "externalIds": {
                    "DBLP": "journals/kbs/KimCK23",
                    "DOI": "10.1016/j.knosys.2023.110790",
                    "CorpusId": 259561637
                },
                "corpusId": 259561637,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6e71d5a4b54cf7969012c30ca8b999981feadd22",
                "title": "FEAT: A general framework for feature-aware multivariate time-series representation learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2138995567",
                        "name": "Subin Kim"
                    },
                    {
                        "authorId": "2147282623",
                        "name": "Euisuk Chung"
                    },
                    {
                        "authorId": "2994930",
                        "name": "Pilsung Kang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7fc235c4e62a7078260f6ecef0d54bbb1532c503",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-00066",
                    "ArXiv": "2307.00066",
                    "DOI": "10.48550/arXiv.2307.00066",
                    "CorpusId": 259316167
                },
                "corpusId": 259316167,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7fc235c4e62a7078260f6ecef0d54bbb1532c503",
                "title": "Improving the Transferability of Time Series Forecasting with Decomposition Adaptation",
                "abstract": "Due to effective pattern mining and feature representation, neural forecasting models based on deep learning have achieved great progress. The premise of effective learning is to collect sufficient data. However, in time series forecasting, it is difficult to obtain enough data, which limits the performance of neural forecasting models. To alleviate the data scarcity limitation, we design Sequence Decomposition Adaptation Network (SeDAN) which is a novel transfer architecture to improve forecasting performance on the target domain by aligning transferable knowledge from cross-domain datasets. Rethinking the transferability of features in time series data, we propose Implicit Contrastive Decomposition to decompose the original features into components including seasonal and trend features, which are easier to transfer. Then we design the corresponding adaptation methods for decomposed features in different domains. Specifically, for seasonal features, we perform joint distribution adaptation and for trend features, we design an Optimal Local Adaptation. We conduct extensive experiments on five benchmark datasets for multivariate time series forecasting. The results demonstrate the effectiveness of our SeDAN. It can provide more efficient and stable knowledge transfer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145971530",
                        "name": "Yan-hong Gao"
                    },
                    {
                        "authorId": "2152544561",
                        "name": "Yan Wang"
                    },
                    {
                        "authorId": "2145418728",
                        "name": "Qiang Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Scalable Representation Learning (SRL) (Franceschi, Dieuleveut, and Jaggi 2019) employs negative sampling techniques with an encoder-based architecture to learn the representation."
            ],
            "citingPaper": {
                "paperId": "86f260abb52cea53b4dbf3f5c2a5669450983374",
                "externalIds": {
                    "DBLP": "conf/aaai/ZuoLCBMW23",
                    "DOI": "10.1609/aaai.v37i9.26359",
                    "CorpusId": 259642048
                },
                "corpusId": 259642048,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/86f260abb52cea53b4dbf3f5c2a5669450983374",
                "title": "SVP-T: A Shape-Level Variable-Position Transformer for Multivariate Time Series Classification",
                "abstract": "Multivariate time series classi\ufb01cation (MTSC), one of the most fundamental time series applications, has not only gained substantial research attentions but has also emerged in many real-life applications. Recently, using transformers to solve MTSC has been reported. However, current transformer-based methods take data points of individual timestamps as inputs (timestamp-level), which only capture the temporal dependencies, not the dependencies among variables. In this\npaper, we propose a novel method, called SVP-T. Specifically, we \ufb01rst propose to take time series subsequences, which can be from different variables and positions (time interval), as the inputs (shape-level). The temporal and variable dependencies are both handled by capturing the long- and short-term dependencies among shapes. Second, we propose a variable-position encoding layer (VP-layer) to utilize both the variable and position information of each shape. Third, we introduce a novel VP-based (Variable-Position) self-attention mechanism to allow the enhancing the attention weights of overlapping shapes. We evaluate our method on all UEA MTS datasets. SVP-T achieves the best accuracy rank when compared with several competitive state-of-the-art methods. Furthermore, we demonstrate the effectiveness of the VP-layer and the VP-based self-attention mechanism. Finally, we present one case study to interpret the result of SVP-T.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "123736753",
                        "name": "Rundong Zuo"
                    },
                    {
                        "authorId": "1998950772",
                        "name": "Guozhong Li"
                    },
                    {
                        "authorId": "2151008964",
                        "name": "Byron Choi"
                    },
                    {
                        "authorId": "1730344",
                        "name": "S. Bhowmick"
                    },
                    {
                        "authorId": "98747747",
                        "name": "D. Mah"
                    },
                    {
                        "authorId": "2055941366",
                        "name": "Grace L.H. Wong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We encode each sequence with unsupervised representations learned by (Franceschi, Dieuleveut, and Jaggi 2019)."
            ],
            "citingPaper": {
                "paperId": "7a084f14b45e83fd2d870d0cdbe38560cd2c07c9",
                "externalIds": {
                    "DBLP": "conf/aaai/Wang0L23",
                    "DOI": "10.1609/aaai.v37i8.26208",
                    "CorpusId": 259731154
                },
                "corpusId": 259731154,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7a084f14b45e83fd2d870d0cdbe38560cd2c07c9",
                "title": "AEC-GAN: Adversarial Error Correction GANs for Auto-Regressive Long Time-Series Generation",
                "abstract": "Large-scale high-quality data is critical for training modern deep neural networks. However, data acquisition can be costly or time-consuming for many time-series applications, thus researchers turn to generative models for generating synthetic time-series data. In particular, recent generative adversarial networks (GANs) have achieved remarkable success in time-series generation. Despite their success, existing GAN models typically generate the sequences in an auto-regressive manner, and we empirically observe that they suffer from severe distribution shifts and bias amplification, especially when generating long sequences. To resolve this problem, we propose Adversarial Error Correction GAN (AEC-GAN), which is capable of dynamically correcting the bias in the past generated data to alleviate the risk of distribution shifts and thus can generate high-quality long sequences. AEC-GAN contains two main innovations: (1) We develop an error correction module to mitigate the bias. In the training phase, we adversarially perturb the realistic time-series data and then optimize this module to reconstruct the original data. In the generation phase, this module can act as an efficient regulator to detect and mitigate the bias. (2) We propose an augmentation method to facilitate GAN's training by introducing adversarial examples. Thus, AEC-GAN can generate high-quality sequences of arbitrary lengths, and the synthetic data can be readily applied to downstream tasks to boost their performance. We conduct extensive experiments on six widely used datasets and three state-of-the-art time-series forecasting models to evaluate the quality of our synthetic time-series data in different lengths and downstream tasks. Both the qualitative and quantitative experimental results demonstrate the superior performance of AEC-GAN over other deep generative models for time-series generation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152509145",
                        "name": "Lei Wang"
                    },
                    {
                        "authorId": "2107063481",
                        "name": "Liang Zeng"
                    },
                    {
                        "authorId": "2151968158",
                        "name": "Jian Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "70e5e9bd353d1b1e54b4eebd3a67b58026a13dff",
                "externalIds": {
                    "DBLP": "conf/aaai/ChowdhuryL0H0S23",
                    "DOI": "10.1609/aaai.v37i6.25876",
                    "CorpusId": 259722039
                },
                "corpusId": 259722039,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/70e5e9bd353d1b1e54b4eebd3a67b58026a13dff",
                "title": "PrimeNet: Pre-training for Irregular Multivariate Time Series",
                "abstract": "Real-world applications often involve irregular time series, for which the time intervals between successive observations are non-uniform. Irregularity across multiple features in a multi-variate time series further results in a different subset of features at any given time (i.e., asynchronicity). Existing pre-training schemes for time-series, however, often assume regularity of time series and make no special treatment of irregularity. We argue that such irregularity offers insight about domain property of the data\u2014for example, frequency of hospital visits may signal patient health condition\u2014that can guide representation learning. In this work, we propose PrimeNet to learn a self-supervised representation for irregular multivariate time-series. Specifically, we design a time sensitive contrastive learning and data reconstruction task to pre-train a model. Irregular time-series exhibits considerable variations in sampling density over time. Hence, our triplet generation strategy follows the density of the original data points, preserving its native irregularity. Moreover, the sampling density variation over time makes data reconstruction difficult for different regions. Therefore, we design a data masking technique that always masks a constant time duration to accommodate reconstruction for regions of different sampling density. We learn with these tasks using unlabeled data to build a pre-trained model and fine-tune on a downstream task with limited labeled data, in contrast with existing fully supervised approach for irregular time-series, requiring large amounts of labeled data. Experiment results show that PrimeNet significantly outperforms state-of-the-art methods on naturally irregular and asynchronous data from Healthcare and IoT applications for several downstream tasks, including classification, interpolation, and regression.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "134629191",
                        "name": "Ranak Roy Chowdhury"
                    },
                    {
                        "authorId": "97483167",
                        "name": "Jiacheng Li"
                    },
                    {
                        "authorId": "2108217022",
                        "name": "Xiyuan Zhang"
                    },
                    {
                        "authorId": "2505157",
                        "name": "Dezhi Hong"
                    },
                    {
                        "authorId": "2110343779",
                        "name": "Rajesh K. Gupta"
                    },
                    {
                        "authorId": "2163679367",
                        "name": "Jingbo Shang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a2056f5affe509b50e41612057ca9cca143ef97a",
                "externalIds": {
                    "DBLP": "conf/aaai/ChenGWW23",
                    "DOI": "10.1609/aaai.v37i6.25863",
                    "CorpusId": 259734465
                },
                "corpusId": 259734465,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a2056f5affe509b50e41612057ca9cca143ef97a",
                "title": "Supervised Contrastive Few-Shot Learning for High-Frequency Time Series",
                "abstract": "Significant progress has been made in representation learning, especially with recent success on self-supervised contrastive learning. However, for time series with less intuitive or semantic meaning, sampling bias may be inevitably encountered in unsupervised approaches. Although supervised contrastive learning has shown superior performance by leveraging label information, it may also suffer from class collapse. In this study, we consider a realistic scenario in industry with limited annotation information available. A supervised contrastive framework is developed for high-frequency time series representation and classification, wherein a novel variant of supervised contrastive loss is proposed to include multiple augmentations while induce spread within each class. Experiments on four mainstream public datasets as well as a series of sensitivity and ablation analyses demonstrate that the learned representations are effective and robust compared with the direct supervised learning and self-supervised learning, notably under the minimal few-shot situation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1683647",
                        "name": "X. Chen"
                    },
                    {
                        "authorId": "2175412701",
                        "name": "Cheng Ge"
                    },
                    {
                        "authorId": "2115270654",
                        "name": "Mingxing Wang"
                    },
                    {
                        "authorId": "2143719046",
                        "name": "Jin Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "131819cd0495e02bc2e62be869d07a8c2316a71d",
                "externalIds": {
                    "DBLP": "conf/aaai/LiuMMW23",
                    "DOI": "10.1609/aaai.v37i7.26072",
                    "CorpusId": 259675699
                },
                "corpusId": 259675699,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/131819cd0495e02bc2e62be869d07a8c2316a71d",
                "title": "Temporal-Frequency Co-training for Time Series Semi-supervised Learning",
                "abstract": "Semi-supervised learning (SSL) has been actively studied due to its ability to alleviate the reliance of deep learning models on labeled data. Although existing SSL methods based on pseudo-labeling strategies have made great progress, they rarely consider time-series data's intrinsic properties (e.g., temporal dependence). Learning representations by mining the inherent properties of time series has recently gained much attention. Nonetheless, how to utilize feature representations to design SSL paradigms for time series has not been explored. To this end, we propose a Time Series SSL framework via Temporal-Frequency Co-training (TS-TFC), leveraging the complementary information from two distinct views for unlabeled data learning. In particular, TS-TFC employs time-domain and frequency-domain views to train two deep neural networks simultaneously, and each view's pseudo-labels generated by label propagation in the representation space are adopted to guide the training of the other view's classifier. To enhance the discriminative of representations between categories, we propose a temporal-frequency supervised contrastive learning module, which integrates the learning difficulty of categories to improve the quality of pseudo-labels. Through co-training the pseudo-labels obtained from temporal-frequency representations, the complementary information in the two distinct views is exploited to enable the model to better learn the distribution of categories. Extensive experiments on 106 UCR datasets show that TS-TFC outperforms state-of-the-art methods, demonstrating the effectiveness and robustness of our proposed model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118415962",
                        "name": "Zhen Liu"
                    },
                    {
                        "authorId": "2153709283",
                        "name": "Qianli Ma"
                    },
                    {
                        "authorId": "2220290902",
                        "name": "Peitian Ma"
                    },
                    {
                        "authorId": "2094560916",
                        "name": "Linghao Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "a) Encoder: Although the instance-level representation learning models like TNC [18] and T-Loss [19] have shown great success in clustering and classification tasks, the methods fail to capture multi-scale features that provide different levels of semantics and improve the generalization capability of learned representations [23] .",
                "The encoder maps the input time series X to its representation R, and the decoder maps R to the output text sequence.\na) Encoder: Although the instance-level representation learning models like TNC [18] and T-Loss [19] have shown great success in clustering and classification tasks, the methods fail to capture multi-scale features that provide different levels of semantics and improve the generalization capability of learned representations [23] ."
            ],
            "citingPaper": {
                "paperId": "b60b70f5b7b3d4100e506161008601e580a3a4c1",
                "externalIds": {
                    "DBLP": "conf/ijcnn/LiGCZSL23",
                    "DOI": "10.1109/IJCNN54540.2023.10191421",
                    "CorpusId": 260386030
                },
                "corpusId": 260386030,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/b60b70f5b7b3d4100e506161008601e580a3a4c1",
                "title": "Repr2Seq: A Data-to-Text Generation Model for Time Series",
                "abstract": "Data-to-text generation takes structured data as input and produces text that sufficiently describes the data as output. Recently, it has received a lot of attention from both research field and industry. However, as a critical data form, time series is less discussed in this domain. This paper proposes Repr2Seq, a data-to-text generation model for time series. To better capture the structure and core information of time series, Repr2Seq obtains representation vectors using time series representation learning methods, which are then fed into a neural network-based model to generate text sequences. To demonstrate the effectiveness of Repr2Seq, a dataset consisting of stock price series and corresponding comments is proposed. Experiments show that Repr2Seq achieves significant improvement over standard approaches and leads to satisfactory results. We also conduct experiments to investigate the effect of hyperparameters on the model and detect performance improvements in various settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153683836",
                        "name": "Yi Li"
                    },
                    {
                        "authorId": "1390781864",
                        "name": "Yuxuan Gao"
                    },
                    {
                        "authorId": "2226720623",
                        "name": "Jianyi Cai"
                    },
                    {
                        "authorId": "2226522556",
                        "name": "Guoxiang Zheng"
                    },
                    {
                        "authorId": "145308926",
                        "name": "Hanlin Shi"
                    },
                    {
                        "authorId": "2110755755",
                        "name": "Xiping Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Then, we use the triplet loss that is used in [31] as the loss function of the similarity prediction task:"
            ],
            "citingPaper": {
                "paperId": "ecce6ecf289b9247700ad1cc254564cc1c8a282a",
                "externalIds": {
                    "DBLP": "conf/ijcnn/LuoGL23",
                    "DOI": "10.1109/IJCNN54540.2023.10191261",
                    "CorpusId": 260387827
                },
                "corpusId": 260387827,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/ecce6ecf289b9247700ad1cc254564cc1c8a282a",
                "title": "PT3: A Transformer-based Model for Sepsis Death Risk Prediction via Vital Signs Time Series",
                "abstract": "Sepsis is a life-threatening systemic syndrome with a high mortality rate. It is critical for doctors to identify sepsis patients at high risk of death in real time, saving patients and reducing in-hospital mortality. However, current clinical methods use traditional machine learning models to predict the death risk of sepsis patients based on the vital signs and lab test results, which is difficult to achieve real-time prediction. In this work, we propose a novel Transformer-based model named PT3 to predict the death risk of sepsis patients within $k$ hours (k = 6, 24, 48) in the future based only on the vital signs time series that can be collected in real time. In clinical settings, the collection intervals of vital signs time series are usually irregular. To address this challenge, we design a time-aware mechanism by using a time decay function to explore temporal correlations between records at different moments. We further introduce an auto-imputing mechanism to our model by using the masked prediction pre-training task. To enhance the representation learning ability, we propose a similarity prediction task, a self-supervised pre-training method, to pre-train our model with triplet-loss function. We validate the effectiveness of PT3 on two public clinical databases, MIMIC-IV and eICU. Experiments results show that our model has effective prediction performance, whose AUC on MIMIC-IV and eICU datasets achieve 0.9067 and 0.8733 respectively, especially in the next 6 hours, outperforming other state-of-the-art deep learning methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34330295",
                        "name": "R. Luo"
                    },
                    {
                        "authorId": "2052305062",
                        "name": "Minghui Gong"
                    },
                    {
                        "authorId": "2109640170",
                        "name": "Chunping Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "As mentioned in [102], suppose one anchor x , one positive sample x, and K negative samples x k , k\u22081,2,\u00b7\u00b7\u00b7 ,K are chosen, we expect to assimilate x ref and x and to distinguish between x and x k , i."
            ],
            "citingPaper": {
                "paperId": "1725ad1d8cc0e539ac5d0a85657d5c95b4538c5e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-10125",
                    "ArXiv": "2306.10125",
                    "DOI": "10.48550/arXiv.2306.10125",
                    "CorpusId": 259203853
                },
                "corpusId": 259203853,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1725ad1d8cc0e539ac5d0a85657d5c95b4538c5e",
                "title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects",
                "abstract": "Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages and disadvantages. To facilitate the experiments and validation of time series SSL methods, we also summarize datasets commonly used in time series forecasting, classification, anomaly detection, and clustering tasks. Finally, we present the future directions of SSL for time series analysis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119059780",
                        "name": "Kexin Zhang"
                    },
                    {
                        "authorId": "3308963",
                        "name": "Qingsong Wen"
                    },
                    {
                        "authorId": "2152737103",
                        "name": "Chaoli Zhang"
                    },
                    {
                        "authorId": "2136700261",
                        "name": "Rongyao Cai"
                    },
                    {
                        "authorId": "2072905592",
                        "name": "Ming Jin"
                    },
                    {
                        "authorId": "2144384782",
                        "name": "Yong Liu"
                    },
                    {
                        "authorId": "2108020140",
                        "name": "James Zhang"
                    },
                    {
                        "authorId": "72322304",
                        "name": "Y. Liang"
                    },
                    {
                        "authorId": "3224619",
                        "name": "Guansong Pang"
                    },
                    {
                        "authorId": "2451800",
                        "name": "Dongjin Song"
                    },
                    {
                        "authorId": "2585415",
                        "name": "Shirui Pan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We compare MBrain with state-of-the-art models including one supervised classification model MiniRocket [12] and several self-supervised and unsupervised models: CPC [28], SimCLR [7], Triplet-Loss (T-Loss) [17], Time Series Transformer (TST) [45], GTS [36], TS-TCC [16] and TS2Vec [43]."
            ],
            "citingPaper": {
                "paperId": "0c11e74e80360dfa31cd4bd303ad13f7d2078673",
                "externalIds": {
                    "ArXiv": "2306.13102",
                    "DBLP": "journals/corr/abs-2306-13102",
                    "DOI": "10.1145/3580305.3599426",
                    "CorpusId": 259243749
                },
                "corpusId": 259243749,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/0c11e74e80360dfa31cd4bd303ad13f7d2078673",
                "title": "MBrain: A Multi-channel Self-Supervised Learning Framework for Brain Signals",
                "abstract": "Brain signals are important quantitative data for understanding physiological activities and diseases of human brain. Meanwhile, rapidly developing deep learning methods offer a wide range of opportunities for better modeling brain signals, which has attracted considerable research efforts recently. Most existing studies pay attention to supervised learning methods, which, however, require high-cost clinical labels. In addition, the huge difference in the clinical patterns of brain signals measured by invasive (e.g., SEEG) and non-invasive (e.g., EEG) methods leads to the lack of a unified method. To handle the above issues, in this paper, we propose to study the self-supervised learning (SSL) framework for brain signals that can be applied to pre-train either SEEG or EEG data. Intuitively, brain signals, generated by the firing of neurons, are transmitted among different connecting structures in human brain. Inspired by this, we propose MBrain to learn implicit spatial and temporal correlations between different channels (i.e., contacts of the electrode, corresponding to different brain areas) as the cornerstone for uniformly modeling different types of brain signals. Specifically, we represent the spatial correlation by a graph structure, which is built with proposed multi-channel CPC. We theoretically prove that optimizing the goal of multi-channel CPC can lead to a better predictive representation and apply the instantaneou-time-shift prediction task based on it. Then we capture the temporal correlation by designing the delayed-time-shift prediction task. Finally, replace-discriminative-learning task is proposed to preserve the characteristics of each channel. Extensive experiments of seizure detection on both EEG and SEEG large-scale real-world datasets demonstrate that our model outperforms several state-of-the-art time series SSL and unsupervised models, and has the ability to be deployed to clinical practice.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1930919381",
                        "name": "Donghong Cai"
                    },
                    {
                        "authorId": "2128680092",
                        "name": "Junru Chen"
                    },
                    {
                        "authorId": "2172713048",
                        "name": "Yang Yang"
                    },
                    {
                        "authorId": "2110033454",
                        "name": "Te-Chun Liu"
                    },
                    {
                        "authorId": "2155132333",
                        "name": "Yafeng Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Thirdly, all the contrast-based methods [13]\u2013[16] construct positive pairs or negative samples based on prior knowledge of the time series dataset or make strong assumptions of the data distribution of time series.",
                "Firstly, most recent studies [12], [13] only learn instancelevel representations, which is unsuitable for point-wise tasks, e.",
                "One of the earliest works, T-Loss [13], following Word2Vec [23], attempts to learn scalable representations by randomly selecting time segments via triplet loss."
            ],
            "citingPaper": {
                "paperId": "48b14f4b8a497d5d0db8ca7627047162b4fcd091",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-06994",
                    "ArXiv": "2306.06994",
                    "DOI": "10.1109/CASE56687.2023.10260640",
                    "CorpusId": 259138815
                },
                "corpusId": 259138815,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/48b14f4b8a497d5d0db8ca7627047162b4fcd091",
                "title": "Correlated Time Series Self-Supervised Representation Learning via Spatiotemporal Bootstrapping",
                "abstract": "Correlated time series analysis plays an important role in many real-world industries. Learning an efficient representation of this large-scale data for further downstream tasks is necessary but challenging. In this paper, we propose a time-step-level representation learning framework for individual instances via bootstrapped spatiotemporal representation prediction. We evaluated the effectiveness and flexibility of our representation learning framework on correlated time series forecasting and cold-start transferring the forecasting model to new instances with limited data. A linear regression model trained on top of the learned representations demonstrates our model performs best in most cases. Especially compared to representation learning models, we reduce the RMSE, MAE, and MAPE by 37%, 49%, and 48% on the PeMS-BAY dataset, respectively. Furthermore, in real-world metro passenger flow data, our framework demonstrates the ability to transfer to infer future information of new cold-start instances, with gains of 15%, 19%, and 18%. The source code will be released under the GitHub https://github.com/bonaldli/Spatiotemporal-TS-Representation-Learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220660224",
                        "name": "Luxuan Wang"
                    },
                    {
                        "authorId": "50010487",
                        "name": "Lei Bai"
                    },
                    {
                        "authorId": "2051629352",
                        "name": "Ziyue Li"
                    },
                    {
                        "authorId": "2153291399",
                        "name": "Rui Zhao"
                    },
                    {
                        "authorId": "2173360",
                        "name": "F. Tsung"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "For the classification task, we follow the settings in [18] and train an RBF SVM classifier on segment-level representations generated by our baselines.",
                "Causal Convolution [3, 59, 18] or Dilated Convolution [18, 65]).",
                "Similar to [65], we incorporate both instance-wise loss [18] (Lins) and temporal loss [55] (Ltemp) to model the distance within a couplet.",
                "Recognizing the significance of the task and the existing gaps in current literature [18, 55, 17, 64, 65], this study dedicates to addressing the time series representation learning problem.",
                "The existing sampling strategies mainly center around the time series\u2019 invariance characteristics, such as temporal invariance [26, 55, 18], transformation and augmentation invariance [54, 64, 68], and contextual invariance [17, 65].",
                "In this way, our CoInception framework can be seen as a set of multiple dilated convolution experts, with much shallower depth and equivalent receptive fields compared with ordinary stacked Dilated Convolution networks [18, 65].",
                "Although ensuring contextual consistency has been demonstrated to be more robust than previous consistencies [17, 18, 55], we recognize",
                "To further strengthen our empirical evidence, we additionally implement a K-nearest neighbor classifier equipped with DTW [9] metric, along with T-Loss [18] and TST [66] beside the aforementioned SOTA approaches.",
                "Prior research on representation learning in time series data has predominantly focused on employing the self-supervised contrastive learning technique [18, 55, 17, 64, 65], which consists of two main components: sampling strategy and encoder architecture."
            ],
            "citingPaper": {
                "paperId": "dd6d5479aa4071286498a4bd7dbe39024dd7bfe5",
                "externalIds": {
                    "ArXiv": "2306.06579",
                    "DBLP": "journals/corr/abs-2306-06579",
                    "DOI": "10.48550/arXiv.2306.06579",
                    "CorpusId": 259137376
                },
                "corpusId": 259137376,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dd6d5479aa4071286498a4bd7dbe39024dd7bfe5",
                "title": "Learning Robust and Consistent Time Series Representations: A Dilated Inception-Based Approach",
                "abstract": "Representation learning for time series has been an important research area for decades. Since the emergence of the foundation models, this topic has attracted a lot of attention in contrastive self-supervised learning, to solve a wide range of downstream tasks. However, there have been several challenges for contrastive time series processing. First, there is no work considering noise, which is one of the critical factors affecting the efficacy of time series tasks. Second, there is a lack of efficient yet lightweight encoder architectures that can learn informative representations robust to various downstream tasks. To fill in these gaps, we initiate a novel sampling strategy that promotes consistent representation learning with the presence of noise in natural time series. In addition, we propose an encoder architecture that utilizes dilated convolution within the Inception block to create a scalable and robust network architecture with a wide receptive field. Experiments demonstrate that our method consistently outperforms state-of-the-art methods in forecasting, classification, and abnormality detection tasks, e.g. ranks first over two-thirds of the classification UCR datasets, with only $40\\%$ of the parameters compared to the second-best approach. Our source code for CoInception framework is accessible at https://github.com/anhduy0911/CoInception.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48720997",
                        "name": "Anh Duy Nguyen"
                    },
                    {
                        "authorId": "2072581996",
                        "name": "Trang H. Tran"
                    },
                    {
                        "authorId": "143950636",
                        "name": "Hieu Pham"
                    },
                    {
                        "authorId": "2143967163",
                        "name": "Phi-Le Nguyen"
                    },
                    {
                        "authorId": "144274166",
                        "name": "Lam M. Nguyen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "To evaluate the effectiveness of the proposed loss function, we compare it to state-of-the-art baselines, which are Triplet-Loss [13], Temporal Neighborhood Coding (TNC) [33], Contrastive Predictive Coding (CPC) [21, 34] and TS2Vec [40].",
                "For the choice of the encoder, we directly adopt the encoder network used in [13].",
                "Among those methods that consider segmentation as a downstream task, Triplet-Loss [13] and TNC [33] are two state-of-the-art methods.",
                "As stated in the original paper [13, 33], all of these baselines are independent of the architecture of the encoder.",
                "There are dozens of time series representation learning methods [10, 12, 13, 22, 31, 33, 40], but very few consider segmentation as a downstream task.",
                "4 Effectiveness of LSE-Loss To evaluate the effectiveness of the proposed loss function, we compare it to state-of-the-art baselines, which are Triplet-Loss [13], Temporal Neighborhood Coding (TNC) [33], Contrastive Predictive Coding (CPC) [21, 34] and TS2Vec [40].",
                "For a fair comparison and to ensure that the difference in performance is not caused by the differences in the encoders\u2019 architecture, we followed the convention in [13, 33] and used the same encoder network across all compared baselines.",
                "f\u03b8 (xi )\u22ba f\u03b8 (x j ) is the similarity, which uses the widely-adopted dot product to measure the similarity [13, 17, 24, 35].",
                "In this paper, we implement the encoder as a causal convolution network [13] because it has been proved to be efficient for time series data, and can alleviate the disadvantages of recurrent neural networks (e.",
                "For a detailed introduction of these loss functions, refer to the original paper [13, 21, 33, 34, 40].",
                "Triplet-Loss trains the encoder by maximizing the distance between an anchor sample and negative samples and minimizing the distance between the anchor samples and positive samples."
            ],
            "citingPaper": {
                "paperId": "dbe4e024b6a5531cb4e8381b50f8784911efce5b",
                "externalIds": {
                    "DBLP": "journals/pacmmod/00080ZC23",
                    "DOI": "10.1145/3588697",
                    "CorpusId": 259077121
                },
                "corpusId": 259077121,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dbe4e024b6a5531cb4e8381b50f8784911efce5b",
                "title": "Time2State: An Unsupervised Framework for Inferring the Latent States in Time Series Data",
                "abstract": "Time series data from monitoring applications reflect the physical or logical states of the objects, which may produce time series of distinguishable characteristics in different states. Thus, time series data can usually be split into different segments, each reflecting a state of the objects. These states carry rich high-level semantic information, e.g., run, walk, or jump, which helps people better understand the behaviour of the monitored objects. Nevertheless, these states are latent and hard to discover, because the characteristic of time series is complicated and the computational cost is high. This paper develops an efficient and effective unsupervised approach for inferring the latent states of massive multivariate time data. To reduce the computational cost, we present Time2State, a scalable framework that utilizes a sliding window and an encoder to greatly reduce the length of raw time series. To train the encoder, we propose a novel unsupervised loss function, LSE-Loss. Extensive experiments show that compared to the state-of-the-art time series representation learning methods of the same kind, LSE-Loss brings a performance improvement of up to 15% in accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2128761358",
                        "name": "Chengyu Wang"
                    },
                    {
                        "authorId": "152322677",
                        "name": "Kui Wu"
                    },
                    {
                        "authorId": "2053854565",
                        "name": "Tongqing Zhou"
                    },
                    {
                        "authorId": "143942560",
                        "name": "Zhiping Cai"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Although several studies have attempted to fill this gap by considering some of the key characteristics of time series, such as the temporal dynamics [44] and the multi-scale semantics [9, 56], existing approaches can still be weak in learning well-performed representations partly due to the following reasons.",
                "Second, some existing approaches rely on domain-specific assumptions, such as the neighbor similarity [9, 44] and the contextual consistency [56], thus are difficult to generalize to various scenarios.",
                "[9] adapts the triplet loss to time series to achieve URL.",
                "We compare our CSL with 5 URL baselines specially designed for time series, including TS2Vec [56], T-Loss [9], Table 2: Statistics of used anomaly detection datasets.",
                "Specifically, convolutional neural network (CNN) [14, 47] and Transformer [48] are commonly-used encoders in recent studies [8, 9, 44, 53, 56, 57].",
                "self-supervised) representation learning (URL) for MTS [8, 9, 37, 44, 53, 56, 57].",
                "We believe that our method is more general as it does not depend on task-specific assumptions like [9, 44, 56]."
            ],
            "citingPaper": {
                "paperId": "2d6bdff34abb045d7e928ebb4fca87a5ac76c709",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-18888",
                    "ArXiv": "2305.18888",
                    "DOI": "10.48550/arXiv.2305.18888",
                    "CorpusId": 258967715
                },
                "corpusId": 258967715,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2d6bdff34abb045d7e928ebb4fca87a5ac76c709",
                "title": "Contrastive Shapelet Learning for Unsupervised Multivariate Time Series Representation Learning",
                "abstract": "Recent studies have shown great promise in unsupervised representation learning (URL) for multivariate time series, because URL has the capability in learning generalizable representation for many downstream tasks without using inaccessible labels. However, existing approaches usually adopt the models originally designed for other domains (e.g., computer vision) to encode the time series data and rely on strong assumptions to design learning objectives, which limits their ability to perform well. To deal with these problems, we propose a novel URL framework for multivariate time series by learning time-series-specific shapelet-based representation through a popular contrasting learning paradigm. To the best of our knowledge, this is the first work that explores the shapelet-based embedding in the unsupervised general-purpose representation learning. A unified shapelet-based encoder and a novel learning objective with multi-grained contrasting and multi-scale alignment are particularly designed to achieve our goal, and a data augmentation library is employed to improve the generalization. We conduct extensive experiments using tens of real-world datasets to assess the representation quality on many downstream tasks, including classification, clustering, and anomaly detection. The results demonstrate the superiority of our method against not only URL competitors, but also techniques specially designed for downstream tasks. Our code has been made publicly available at https://github.com/real2fish/CSL.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3027405",
                        "name": "Zhiyu Liang"
                    },
                    {
                        "authorId": "2107968621",
                        "name": "Jianfeng Zhang"
                    },
                    {
                        "authorId": "2154311465",
                        "name": "Cheng Liang"
                    },
                    {
                        "authorId": "2180333371",
                        "name": "Hongzhi Wang"
                    },
                    {
                        "authorId": "2113514295",
                        "name": "Zheng Liang"
                    },
                    {
                        "authorId": "2288532",
                        "name": "Lujia Pan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In recent years, various deep learning models, including RNNs [55, 43, 45], multi-layer perceptrons (MLP) [62, 63], CNNs [24], and Temporal convolution networks (TCN) [11] are utilized to perform time series forecasting [14, 41, 4, 21].",
                "[11] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi."
            ],
            "citingPaper": {
                "paperId": "60e9ee92833ac232a99462a8202562759894ec43",
                "externalIds": {
                    "ArXiv": "2305.18382",
                    "DBLP": "journals/corr/abs-2305-18382",
                    "DOI": "10.48550/arXiv.2305.18382",
                    "CorpusId": 258967419
                },
                "corpusId": 258967419,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/60e9ee92833ac232a99462a8202562759894ec43",
                "title": "Adaptive Sparsity Level during Training for Efficient Time Series Forecasting with Transformers",
                "abstract": "Efficient time series forecasting has become critical for real-world applications, particularly with deep neural networks (DNNs). Efficiency in DNNs can be achieved through sparse connectivity and reducing the model size. However, finding the sparsity level automatically during training remains a challenging task due to the heterogeneity in the loss-sparsity tradeoffs across the datasets. In this paper, we propose \\enquote{\\textbf{P}runing with \\textbf{A}daptive \\textbf{S}parsity \\textbf{L}evel} (\\textbf{PALS}), to automatically seek an optimal balance between loss and sparsity, all without the need for a predefined sparsity level. PALS draws inspiration from both sparse training and during-training methods. It introduces the novel\"expand\"mechanism in training sparse neural networks, allowing the model to dynamically shrink, expand, or remain stable to find a proper sparsity level. In this paper, we focus on achieving efficiency in transformers known for their excellent time series forecasting performance but high computational cost. Nevertheless, PALS can be applied directly to any DNN. In the scope of these arguments, we demonstrate its effectiveness also on the DLinear model. Experimental results on six benchmark datasets and five state-of-the-art transformer variants show that PALS substantially reduces model size while maintaining comparable performance to the dense model. More interestingly, PALS even outperforms the dense model, in 12 and 14 cases out of 30 cases in terms of MSE and MAE loss, respectively, while reducing 65% parameter count and 63% FLOPs on average. Our code will be publicly available upon acceptance of the paper.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1768130522",
                        "name": "Zahra Atashgahi"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "2154952601",
                        "name": "Raymond N. J. Veldhuis"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To address this problem, T-Loss [26] employs time-based negative sampling and a triplet loss to learn scalable representations for multivariate time series.",
                "address this problem, T-Loss [26] employs time-based negative"
            ],
            "citingPaper": {
                "paperId": "53e46e599cc58948fb5e19301a0b8680bc4ce768",
                "externalIds": {
                    "DBLP": "conf/cscwd/ChenLLY23",
                    "DOI": "10.1109/CSCWD57460.2023.10152835",
                    "CorpusId": 259235701
                },
                "corpusId": 259235701,
                "publicationVenue": {
                    "id": "a966c5e8-76dc-45c0-942a-3c7e41ac9b1a",
                    "name": "International Conference on Computer Supported Cooperative Work in Design",
                    "type": "conference",
                    "alternate_names": [
                        "Computer Supported Cooperative Work in Design",
                        "Int Conf Comput Support Cooperative Work Des",
                        "CSCWD",
                        "Comput Support Cooperative Work Des"
                    ],
                    "url": "http://www.cscwd.org/"
                },
                "url": "https://www.semanticscholar.org/paper/53e46e599cc58948fb5e19301a0b8680bc4ce768",
                "title": "Representation-Based Time Series Label Propagation for Active Learning",
                "abstract": "Time series data are ubiquitous and informative nowadays, but the labels are difficult to obtain. Active learning is one way to reduce labeling efforts. The label-continuous property of time series data implies the possibility of duplicating labels and thus improving the performance of active learning. This paper proposes Representation-based time series Label Propagation (RLP), a new method based on contrastive representation learning, which can automatically copy the manually labeled labels to adjacent data points and then expand the labeled dataset. To the best of our knowledge, we are the first to apply representation learning to label propagation and achieve good performance in cold-start scenario. Comprehensive experimental results show that our method achieves the best under most active learning query strategies compared with other label propagation methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2179529123",
                        "name": "Dingquan Chen"
                    },
                    {
                        "authorId": "2167477858",
                        "name": "Xin-Yi Li"
                    },
                    {
                        "authorId": "2112839281",
                        "name": "Ang Li"
                    },
                    {
                        "authorId": "2020008",
                        "name": "Yubin Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "Experimental results on UCR and UEA time series archives indicated that the representations obtained by T-Loss can be beneficial for the downstream classification task.",
                "first [134] employed a sufficiently long and non-stationary subseries in a time series sample as the context.",
                "T-Loss: Franceschi et al. [134] proposed an unsupervised time series representation learning method using TCN and a novel Triplet Loss (T-Loss).",
                "Experimental results on multiple time series datasets demonstrate that TNC performs better on downstream classification and clustering tasks compared with the T-Loss [134].",
                "Further, the authors [134] employed the Triplet Loss (T-Loss) to keep the context and positive subsequences close, while making the context and negative subsequences far away for representation learning of time series.",
                "Then, we analyze the classification performance of T-Loss, SelfTime, TS-TCC, TST, and TS2Vec after pre-training on 128 UCR and 30 UEA time series datasets.",
                "T-loss [134], Selftime [135], TS-TCC [38], and TS2Vec [68] are consistency-based PTMs.",
                "For the time-series classification task, we select T-Loss [134], SelfTime [135], TS-TCC [38], TST [29], and TS2Vec [68] to analyze the performance of TS-PTMs and compare them with the supervised FCN [65] model."
            ],
            "citingPaper": {
                "paperId": "e5dff0d39324dd0bb3fa323f2d256f801043ba4a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-10716",
                    "ArXiv": "2305.10716",
                    "DOI": "10.48550/arXiv.2305.10716",
                    "CorpusId": 258762293
                },
                "corpusId": 258762293,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e5dff0d39324dd0bb3fa323f2d256f801043ba4a",
                "title": "A Survey on Time-Series Pre-Trained Models",
                "abstract": "Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, Pre-Trained Models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments are conducted to analyze the advantages and disadvantages of transfer learning strategies, Transformer-based models, and representative TS-PTMs. Finally, we point out some potential directions of TS-PTMs for future work.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144598359",
                        "name": "Qianli Ma"
                    },
                    {
                        "authorId": "46270580",
                        "name": "Z. Liu"
                    },
                    {
                        "authorId": "1576033404",
                        "name": "Zhenjing Zheng"
                    },
                    {
                        "authorId": "2109596395",
                        "name": "Ziyang Huang"
                    },
                    {
                        "authorId": "2110018948",
                        "name": "Siying Zhu"
                    },
                    {
                        "authorId": "2116679739",
                        "name": "Zhongzhong Yu"
                    },
                    {
                        "authorId": "145193332",
                        "name": "J. Kwok"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8ff2964bbbc4d11e95584e211b1589ed719cdf82",
                "externalIds": {
                    "DOI": "10.1109/DDCLS58216.2023.10166246",
                    "CorpusId": 259364128
                },
                "corpusId": 259364128,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8ff2964bbbc4d11e95584e211b1589ed719cdf82",
                "title": "Contrastive Representation Learning for Time Series via Compound Consistency and Hierarchical Contrasting",
                "abstract": "In this paper, a novel contrastive representation learning framework for time series data is proposed. The framework is designed to learn general representations of time series at various semantic levels and is capable of transferring across different datasets. The framework incorporates two key components. Firstly, a hierarchical contrasting method is used to consider both the temporal and instance dimensions of the time series and captures information at different levels through maximum pooling at corresponding timestamps, enabling the model to learn fine-grained and multi-scale time-stamped representations for time series prediction tasks. Secondly, a compound consistency constraint is leveraged, which combines transformation consistency and temporal-frequency consistency, to effectively learn a universal representation of the time series, thereby ensuring its transferability across different datasets. Additionally, the framework considers both the temporal and frequency information of the time series, and uses an adaptive wavelet transform to obtain the frequency domain representation while maintaining temporal alignment, facilitating the contrast of temporal-frequency consistency. Finally, the proposed framework is evaluated through extensive experiments on time series prediction tasks and compared with existing models on four public datasets. The results show that the linear regressor trained with the representations learned by the proposed model outperforms existing time series prediction models in terms of prediction accuracy and transferability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2221252049",
                        "name": "Teng Zheng"
                    },
                    {
                        "authorId": "2221253487",
                        "name": "Guanghao Cao"
                    },
                    {
                        "authorId": "144962309",
                        "name": "Lei Chen"
                    },
                    {
                        "authorId": "39414900",
                        "name": "K. Hao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "(All samples are selected following the negative sampling approach in [9])",
                "Since the focus of our work is on the novel negative sampling technique introduced above, we rely on an existing proven encoder architecture as described in [9,2] based on exponential dilated convolutional neural networks.",
                "3: (3a) shows the idea of an exponentially dilated causal convolution, repainted from [9].",
                "Triplet networks have been successful in computer vision [5,21] and natural language processing [14] but were only recently introduced for time-series clustering and classification [9].",
                "For more details on the architecture, we refer to [9].",
                "This is similar to [9] who applied a classifier on the learned representation for classification purposes.",
                "Previous work for contrastive learning in time-series [9] assumes sufficient variety in the data such that x i will be substantially different from x i and x pos i by random sampling (similar to word2vec [14]).",
                "Formally, this results in algorithm 1 (for one epoch), inspired by [9].",
                "[9], we normalize the data by substracting the mean and dividing by the variance of the entire dataset.",
                "We use these as input to the triplet network with the following objective loss function [9,14]:"
            ],
            "citingPaper": {
                "paperId": "eb47de961c8b6f86cbef2bfd26494f140d04d5aa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-05495",
                    "ArXiv": "2305.05495",
                    "DOI": "10.48550/arXiv.2305.05495",
                    "CorpusId": 258564556
                },
                "corpusId": 258564556,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eb47de961c8b6f86cbef2bfd26494f140d04d5aa",
                "title": "Self-Supervised Anomaly Detection of Rogue Soil Moisture Sensors",
                "abstract": "IoT data is a central element in the successful digital transformation of agriculture. However, IoT data comes with its own set of challenges. E.g., the risk of data contamination due to rogue sensors. A sensor is considered rogue when it provides incorrect measurements over time. To ensure correct analytical results, an essential preprocessing step when working with IoT data is the detection of such rogue sensors. Existing methods assume that well-behaving sensors are known or that a large majority of the sensors is well-behaving. However, real-world data is often completely unlabeled and voluminous, calling for self-supervised methods that can detect rogue sensors without prior information. We present a self-supervised anomalous sensor detector based on a neural network with a contrastive loss, followed by DBSCAN. A core contribution of our paper is the use of Dynamic Time Warping in the negative sampling for the triplet loss. This novelty makes the use of triplet networks feasible for anomalous sensor detection. Our method shows promising results on a challenging dataset of soil moisture sensors deployed in multiple pear orchards.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2193791694",
                        "name": "Boje Deforce"
                    },
                    {
                        "authorId": "1720683",
                        "name": "B. Baesens"
                    },
                    {
                        "authorId": "113962381",
                        "name": "Janie Diels"
                    },
                    {
                        "authorId": "24294537",
                        "name": "E. S. Asensio"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Many recently emerged time series embeddings transform one time series into one vector, cf. Kazemi et al. (2019); Franceschi, Dieuleveut, and Jaggi (2019); Nalmpantis and Vrakas (2019); Kim, Hong, and Cha (2020); Tonekaboni, Eytan, and Goldenberg (2021); Yue et al. (2022)."
            ],
            "citingPaper": {
                "paperId": "220485a60e0b8d5b298340225960c0030223e67b",
                "externalIds": {
                    "DBLP": "conf/flairs/SchwenkeA23",
                    "DOI": "10.32473/flairs.36.133107",
                    "CorpusId": 258585232
                },
                "corpusId": 258585232,
                "publicationVenue": {
                    "id": "546d164a-fc31-4aee-99a5-879e03ff7d36",
                    "name": "The Florida AI Research Society",
                    "type": "conference",
                    "alternate_names": [
                        "Fla AI Res Soc",
                        "FLAIRS"
                    ],
                    "url": "http://www.flairs.com/"
                },
                "url": "https://www.semanticscholar.org/paper/220485a60e0b8d5b298340225960c0030223e67b",
                "title": "Making Time Series Embeddings More Interpretable in Deep Learning - Extracting Higher-Level Features via Symbolic Approximation Representations",
                "abstract": "With the success of language models in deep learning, multiple new time series embeddings have been proposed. However, the interpretability of those representations is often still lacking compared to word embeddings. This paper tackles this issue, aiming to present some criteria for making time series embeddings applied in deep learning models more interpretable using higher-level features in symbolic form. For that, we investigate two different approaches for extracting symbolic approximation representations regarding the frequency and the trend information, i.e. the Symbolic Fourier Approximation (SFA) and the Symbolic Aggregate approXimation (SAX). In particular, we analyze and discuss the impact of applying the different representation approaches. Furthermore, in our experimentation, we apply a state-of-the-art Transformer model to demonstrate the efficacy of the proposed approach regarding explainability in a comprehensive evaluation using a large set of time series datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2097670994",
                        "name": "Leonid Schwenke"
                    },
                    {
                        "authorId": "2191921580",
                        "name": "Martin Atzmueller"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Triplet Loss: the triplet loss [31] as a concept was first adopted and introduced in [13] in MTS modeling and soon adopted in [32].",
                "In this study, we evaluate three state-of-the-art unsupervised representation learning techniques, namely Temporal Neighborhood Coding (TNC) [12], Triplet Loss [13], and Contrastive Predictive Coding (CPC) [14], [15].",
                "Triplet Loss [13] learns scalable MTS representations with a time-based negative sampling and triplet loss.",
                "Each of our baselines selections ( [12], [13], and [14]) offer a unique approach to generalization, resulting in different methods for extracting latent states in MTS problems."
            ],
            "citingPaper": {
                "paperId": "ce63ca8edeb0a09a7ad43869e8d71dcd2d2ef0af",
                "externalIds": {
                    "DOI": "10.1109/PHM58589.2023.00049",
                    "CorpusId": 259280736
                },
                "corpusId": 259280736,
                "publicationVenue": {
                    "id": "aa8a6448-d948-46a6-a4e3-ee66794017ce",
                    "name": "IEEE Conference on Prognostics and Health Management",
                    "type": "conference",
                    "alternate_names": [
                        "PHM",
                        "IEEE Conf Progn Health Manag"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ce63ca8edeb0a09a7ad43869e8d71dcd2d2ef0af",
                "title": "Unsupervised Representation Learning in Multivariate Time Series with Simulated Data",
                "abstract": "Multivariate time-series data contains valuable information, but are challenging to analyze and model. This study uses unsupervised representation learning approaches to extract meaningful representations from unlabeled data, which are used to identify patterns of underlying states. These representations can be later utilized as inputs in downstream tasks for prognosis and health management (PHM) of complex physical systems, with the aim of quantifying system\u2019s reliability and efficiency, and measuring the potential for failure, reducing downtime, and improving overall safety. We evaluate the performance of three advanced methods, namely Temporal Neighborhood Coding (TNC), Triplet Loss, and Contrastive Predictive Coding (CPC), on three simulated scenarios that mimic real-world situations. Key Performance Indicators are used as evaluation metrics for clustering and classification tasks. Our objective is to demonstrate the practicality of these approaches in multiple scenarios while remaining domain agnostic.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051712381",
                        "name": "Thabang Lebese"
                    },
                    {
                        "authorId": "2272703",
                        "name": "C. Mattrand"
                    },
                    {
                        "authorId": "2220782130",
                        "name": "David Clair"
                    },
                    {
                        "authorId": "31007327",
                        "name": "Jean-Marc Bourinet"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, self-supervised learning (SSL) techniques have been developed that can generalize to various tasks, data domains, and input structures [10], [21], [24]."
            ],
            "citingPaper": {
                "paperId": "53759c0ec3fcb03a39b7bdc8243d74a3c78b7d8e",
                "externalIds": {
                    "ArXiv": "2305.00619",
                    "DBLP": "conf/mdm/LiuDXNS23",
                    "DOI": "10.1109/MDM58254.2023.00019",
                    "CorpusId": 258426474
                },
                "corpusId": 258426474,
                "publicationVenue": {
                    "id": "187dde4a-2b59-491b-b6cf-1a8618878cdc",
                    "name": "International Conference on Mobile Data Management",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Mob Data Manag",
                        "MDM",
                        "Mobile Data Management",
                        "Mob Data Manag"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/53759c0ec3fcb03a39b7bdc8243d74a3c78b7d8e",
                "title": "Self-supervised Activity Representation Learning with Incremental Data: An Empirical Study",
                "abstract": "In the context of mobile sensing environments, various sensors on mobile devices continually generate a vast amount of data. Analyzing this ever-increasing data presents several challenges, including limited access to annotated data and a constantly changing environment. Recent advancements in self-supervised learning have been utilized as a pre-training step to enhance the performance of conventional supervised models to address the absence of labelled datasets. This research examines the impact of using a self-supervised representation learning model for time series classification tasks in which data is incrementally available. We proposed and evaluated a workflow in which a model learns to extract informative features using a corpus of unlabeled time series data and then conducts classification on labelled data using features extracted by the model. We analyzed the effect of varying the size, distribution, and source of the unlabeled data on the final classification performance across four public datasets, including various types of sensors in diverse applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108387652",
                        "name": "Jason Liu"
                    },
                    {
                        "authorId": "1864633127",
                        "name": "Shohreh Deldari"
                    },
                    {
                        "authorId": "1560895396",
                        "name": "Hao Xue"
                    },
                    {
                        "authorId": "2196642410",
                        "name": "Van-Hau Nguyen"
                    },
                    {
                        "authorId": "144954586",
                        "name": "Flora D. Salim"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "RNNs model successive time points based on the Markov assumption [5, 16, 32], while CNNs extract variation information along the temporal dimension using techniques such as temporal convolutional networks (TCNs) [2, 12]."
            ],
            "citingPaper": {
                "paperId": "830020aa9f71ffccdef3ca3782a9739f07b3a64c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-05206",
                    "ArXiv": "2304.05206",
                    "DOI": "10.48550/arXiv.2304.05206",
                    "CorpusId": 258060249
                },
                "corpusId": 258060249,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/830020aa9f71ffccdef3ca3782a9739f07b3a64c",
                "title": "The Capacity and Robustness Trade-off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting",
                "abstract": "Multivariate time series data comprises various channels of variables. The multivariate forecasting models need to capture the relationship between the channels to accurately predict future values. However, recently, there has been an emergence of methods that employ the Channel Independent (CI) strategy. These methods view multivariate time series data as separate univariate time series and disregard the correlation between channels. Surprisingly, our empirical results have shown that models trained with the CI strategy outperform those trained with the Channel Dependent (CD) strategy, usually by a significant margin. Nevertheless, the reasons behind this phenomenon have not yet been thoroughly explored in the literature. This paper provides comprehensive empirical and theoretical analyses of the characteristics of multivariate time series datasets and the CI/CD strategy. Our results conclude that the CD approach has higher capacity but often lacks robustness to accurately predict distributionally drifted time series. In contrast, the CI approach trades capacity for robust prediction. Practical measures inspired by these analyses are proposed to address the capacity and robustness dilemma, including a modified CD method called Predict Residuals with Regularization (PRReg) that can surpass the CI strategy. We hope our findings can raise awareness among researchers about the characteristics of multivariate time series and inspire the construction of better forecasting models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112708270",
                        "name": "Lu Han"
                    },
                    {
                        "authorId": "2151459740",
                        "name": "Han-Jia Ye"
                    },
                    {
                        "authorId": "1721819",
                        "name": "De-chuan Zhan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "(1) Feature Extraction [28], [35], [79], [87]: Deep learning models are designed to extract latent features from input data.",
                "In recent years, due to the prevalence of deep neural networks, many unsupervised deep learning-based methods have been proposed for anomaly detection [3], [9], [26], [28], [35], [49], [60], [77], [79], [86], [87]."
            ],
            "citingPaper": {
                "paperId": "ef1d8c6bbe6459251665c2f0f8add5fb1951ebb3",
                "externalIds": {
                    "DBLP": "conf/icde/AngHTH23",
                    "DOI": "10.1109/ICDE55515.2023.00143",
                    "CorpusId": 260171813
                },
                "corpusId": 260171813,
                "publicationVenue": {
                    "id": "764e3630-ddac-4c21-af4b-9d32ffef082e",
                    "name": "IEEE International Conference on Data Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "ICDE",
                        "Int Conf Data Eng",
                        "IEEE Int Conf Data Eng",
                        "International Conference on Data Engineering"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1331"
                },
                "url": "https://www.semanticscholar.org/paper/ef1d8c6bbe6459251665c2f0f8add5fb1951ebb3",
                "title": "A Stitch in Time Saves Nine: Enabling Early Anomaly Detection with Correlation Analysis",
                "abstract": "Early detection of anomalies from sensor-based Multivariate Time Series (MTS) is vital for timely response to the signs of operation failures and errors. While many interesting works have been done toward solving this problem, existing methods typically detect such anomalies as outliers by making certain assumptions that allow efficient and easily understandable solutions to be used but might not be applicable to time series. Meanwhile, unsupervised deep learning-based methods might be highly accurate but often lead to challenges for real-time industrial scenarios, e.g., requiring a large amount of training data and producing unstable output.In this paper, we propose a new approach, CAD, to detect anomalies from sensor-based MTS. We aim to leverage the latent correlations between sensors by first converting the MTS into a sequence of Time-Series Graphs (TSGs) that connect sensors to their highly correlated neighbors within a certain time period. Then, we track the unusual correlation variations between sensors on the sequence of TSGs. By analyzing the correlation variations with a theoretical guarantee, CAD can detect the time of occurrence for the anomalies simultaneously with the sensors that are affected as early as possible.Extensive experiments over eight real-world datasets show that CAD is effective, scalable, yet stable compared to nine state-of-the-art methods while keeping comparable efficiency. Moreover, it maintains above 85% accuracy on large-scale datasets with over 1,000 sensors. Notably, CAD can determine relevant sensors in a very early stage of the anomaly so that timely predictive maintenance can be done. The code is available at https://github.com/YihaoAng/CAD.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35323457",
                        "name": "Y. Ang"
                    },
                    {
                        "authorId": "2111287212",
                        "name": "Qiang Huang"
                    },
                    {
                        "authorId": "1699730",
                        "name": "A. Tung"
                    },
                    {
                        "authorId": "2225237094",
                        "name": "Zhiyong Huang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, (Franceschi et al., 2019) learns scalable representations for various time series lengths using contrasting positive, negative, and reference pairs with an innovative triplet loss."
            ],
            "citingPaper": {
                "paperId": "a071d4a95b4f7f2561c0490d51560bfef7667eac",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-18205",
                    "ArXiv": "2303.18205",
                    "DOI": "10.48550/arXiv.2303.18205",
                    "CorpusId": 257901174
                },
                "corpusId": 257901174,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a071d4a95b4f7f2561c0490d51560bfef7667eac",
                "title": "SimTS: Rethinking Contrastive Representation Learning for Time Series Forecasting",
                "abstract": "Contrastive learning methods have shown an impressive ability to learn meaningful representations for image or time series classification. However, these methods are less effective for time series forecasting, as optimization of instance discrimination is not directly applicable to predicting the future state from the history context. Moreover, the construction of positive and negative pairs in current technologies strongly relies on specific time series characteristics, restricting their generalization across diverse types of time series data. To address these limitations, we propose SimTS, a simple representation learning approach for improving time series forecasting by learning to predict the future from the past in the latent space. SimTS does not rely on negative pairs or specific assumptions about the characteristics of the particular time series. Our extensive experiments on several benchmark time series forecasting datasets show that SimTS achieves competitive performance compared to existing contrastive learning methods. Furthermore, we show the shortcomings of the current contrastive learning framework used for time series forecasting through a detailed ablation study. Overall, our work suggests that SimTS is a promising alternative to other contrastive learning approaches for time series forecasting.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2000933950",
                        "name": "Xiaochen Zheng"
                    },
                    {
                        "authorId": "2118654037",
                        "name": "Xing-Yu Chen"
                    },
                    {
                        "authorId": "1811146394",
                        "name": "Manuel Schurch"
                    },
                    {
                        "authorId": "9628283",
                        "name": "Amina Mollaysa"
                    },
                    {
                        "authorId": "153045787",
                        "name": "A. Allam"
                    },
                    {
                        "authorId": "2318427",
                        "name": "M. Krauthammer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "the whole-series level[10], the sub-sequence level [2], and the timestamp level [8]."
            ],
            "citingPaper": {
                "paperId": "eeb959d282c24a720c7b2f630c80c7a6d3bd6404",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-13804",
                    "ArXiv": "2303.13804",
                    "DOI": "10.48550/arXiv.2303.13804",
                    "CorpusId": 257756837
                },
                "corpusId": 257756837,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eeb959d282c24a720c7b2f630c80c7a6d3bd6404",
                "title": "UniTS: A Universal Time Series Analysis Framework with Self-supervised Representation Learning",
                "abstract": "Machine learning has emerged as a powerful tool for time series analysis. Existing methods are usually customized for different analysis tasks and face challenges in tackling practical problems such as partial labeling and domain shift. To achieve universal analysis and address the aforementioned problems, we develop UniTS, a novel framework that incorporates self-supervised representation learning (or pre-training). The components of UniTS are designed using sklearn-like APIs to allow flexible extensions. We demonstrate how users can easily perform an analysis task using the user-friendly GUIs, and show the superior performance of UniTS over the traditional task-specific methods without self-supervised pre-training on five mainstream tasks and two practical settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3027405",
                        "name": "Zhiyu Liang"
                    },
                    {
                        "authorId": "2154311465",
                        "name": "Cheng Liang"
                    },
                    {
                        "authorId": "2113514295",
                        "name": "Zheng Liang"
                    },
                    {
                        "authorId": "2180333371",
                        "name": "Hongzhi Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Although some recent works [15, 56] train supervised classifiers using these learned features on temporal data as input, to the best of our knowledge, no method designed for time series performs classification in a fully unsupervised manner.",
                "time series either use pseudo-labels to train neural networks in a supervised fashion [19, 21] or focus on learning deep representations on which clustering can be performed with standards algorithms [15, 56]."
            ],
            "citingPaper": {
                "paperId": "4c1ca2e2c3f37040e3d6cc4849eff302f145b42d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-12533",
                    "ArXiv": "2303.12533",
                    "DOI": "10.48550/arXiv.2303.12533",
                    "CorpusId": 257663601
                },
                "corpusId": 257663601,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4c1ca2e2c3f37040e3d6cc4849eff302f145b42d",
                "title": "Pixel-wise Agricultural Image Time Series Classification: Comparisons and a Deformable Prototype-based Approach",
                "abstract": "Improvements in Earth observation by satellites allow for imagery of ever higher temporal and spatial resolution. Leveraging this data for agricultural monitoring is key for addressing environmental and economic challenges. Current methods for crop segmentation using temporal data either rely on annotated data or are heavily engineered to compensate the lack of supervision. In this paper, we present and compare datasets and methods for both supervised and unsupervised pixel-wise segmentation of satellite image time series (SITS). We also introduce an approach to add invariance to spectral deformations and temporal shifts to classical prototype-based methods such as K-means and Nearest Centroid Classifier (NCC). We show this simple and highly interpretable method leads to meaningful results in both the supervised and unsupervised settings and significantly improves the state of the art for unsupervised classification of agricultural time series on four recent SITS datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2086971830",
                        "name": "Elliot Vincent"
                    },
                    {
                        "authorId": "144189388",
                        "name": "J. Ponce"
                    },
                    {
                        "authorId": "48582897",
                        "name": "Mathieu Aubry"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Following the previous setting, we evaluate the quality of representations on time series classification in a standard supervised manner (Franceschi, Dieuleveut, and Jaggi 2019; Yue et al. 2022).",
                "Despite being effective and prevalent, contrastive learning has been less explored in the time series domain (Eldele et al. 2021; Franceschi, Dieuleveut, and Jaggi 2019; Fan, Zhang, and Gao 2020; Tonekaboni, Eytan, and Goldenberg 2021).",
                "Meta-learner Network Previous time series contrastive learning methods (Franceschi, Dieuleveut, and Jaggi 2019; Fan, Zhang, and Gao 2020; Eldele et al. 2021; Tonekaboni, Eytan, and Goldenberg 2021) generate augmentations with either rule of thumb guided by prefabricated human priors or tedious\u2026",
                "Recently, some efforts have been devoted to applying contrastive learning to the time series domain (Oord, Li, and Vinyals 2018; Franceschi, Dieuleveut, and Jaggi 2019; Fan, Zhang, and Gao 2020; Eldele et al. 2021; Tonekaboni, Eytan, and Goldenberg 2021; Yue et al. 2022).",
                "We compare InfoTS with baselines including TS2Vec (Yue et al. 2022), T-Loss (Franceschi, Dieuleveut, and Jaggi 2019), TSTCC (Eldele et al. 2021), TST (Zerveas et al. 2021), and DTW (Franceschi, Dieuleveut, and Jaggi 2019).",
                "Franceschi et.al. propose to extract subsequences for data augmentation (Franceschi, Dieuleveut, and Jaggi 2019).",
                "In (Franceschi, Dieuleveut, and Jaggi 2019), Franceschi et.al. generate positive and negative pairs based on subsequences.",
                "Architecture The adopted encoder f\u03b8(x) : RT\u00d7F \u2192 RD consists of two components, a fully connected layer, and a 10-layer dilated CNN module (Franceschi, Dieuleveut, and Jaggi 2019; Yue et al. 2022)."
            ],
            "citingPaper": {
                "paperId": "6e70a2b7512fde9d25176c508f9cad35e47f66ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-11911",
                    "ArXiv": "2303.11911",
                    "DOI": "10.48550/arXiv.2303.11911",
                    "CorpusId": 257636777
                },
                "corpusId": 257636777,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6e70a2b7512fde9d25176c508f9cad35e47f66ad",
                "title": "Time Series Contrastive Learning with Information-Aware Augmentations",
                "abstract": "Various contrastive learning approaches have been proposed in recent years and achieve significant empirical success. While effective and prevalent, contrastive learning has been less explored for time series data. A key component of contrastive learning is to select appropriate augmentations imposing some priors to construct feasible positive samples, such that an encoder can be trained to learn robust and discriminative representations. Unlike image and language domains where \"desired'' augmented samples can be generated with the rule of thumb guided by prefabricated human priors, the ad-hoc manual selection of time series augmentations is hindered by their diverse and human-unrecognizable temporal structures. How to find the desired augmentations of time series data that are meaningful for given contrastive learning tasks and datasets remains an open question. In this work, we address the problem by encouraging both high fidelity and variety based on information theory. A theoretical analysis leads to the criteria for selecting feasible data augmentations. On top of that, we propose a new contrastive learning approach with information-aware augmentations, InfoTS, that adaptively selects optimal augmentations for time series representation learning. Experiments on various datasets show highly competitive performance with up to a 12.0% reduction in MSE on forecasting tasks and up to 3.7% relative improvement in accuracy on classification tasks over the leading baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "145859270",
                        "name": "Wei Cheng"
                    },
                    {
                        "authorId": "2107962435",
                        "name": "Yingheng Wang"
                    },
                    {
                        "authorId": "2116459424",
                        "name": "Dongkuan Xu"
                    },
                    {
                        "authorId": "2090567",
                        "name": "Jingchao Ni"
                    },
                    {
                        "authorId": "3007026",
                        "name": "Wenchao Yu"
                    },
                    {
                        "authorId": "2048981220",
                        "name": "Xuchao Zhang"
                    },
                    {
                        "authorId": "3215702",
                        "name": "Yanchi Liu"
                    },
                    {
                        "authorId": "47557891",
                        "name": "Yuncong Chen"
                    },
                    {
                        "authorId": "2145225543",
                        "name": "Haifeng Chen"
                    },
                    {
                        "authorId": "2190288679",
                        "name": "Xiang Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Each dataset is normalized such that it has zero mean and unit variance, following (Franceschi et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "e2a9b9f13182ba604a8dcf560fbdf21086b0c520",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-04743",
                    "ArXiv": "2303.04743",
                    "DOI": "10.48550/arXiv.2303.04743",
                    "CorpusId": 257405229
                },
                "corpusId": 257405229,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e2a9b9f13182ba604a8dcf560fbdf21086b0c520",
                "title": "Vector Quantized Time Series Generation with a Bidirectional Prior Model",
                "abstract": "Time series generation (TSG) studies have mainly focused on the use of Generative Adversarial Networks (GANs) combined with recurrent neural network (RNN) variants. However, the fundamental limitations and challenges of training GANs still remain. In addition, the RNN-family typically has difficulties with temporal consistency between distant timesteps. Motivated by the successes in the image generation (IMG) domain, we propose TimeVQVAE, the first work, to our knowledge, that uses vector quantization (VQ) techniques to address the TSG problem. Moreover, the priors of the discrete latent spaces are learned with bidirectional transformer models that can better capture global temporal consistency. We also propose VQ modeling in a time-frequency domain, separated into low-frequency (LF) and high-frequency (HF). This allows us to retain important characteristics of the time series and, in turn, generate new synthetic signals that are of better quality, with sharper changes in modularity, than its competing TSG methods. Our experimental evaluation is conducted on all datasets from the UCR archive, using well-established metrics in the IMG literature, such as Fr\\'echet inception distance and inception scores. Our implementation on GitHub: \\url{https://github.com/ML4ITS/TimeVQVAE}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "96694307",
                        "name": "Daesoo Lee"
                    },
                    {
                        "authorId": "2121354424",
                        "name": "Sara Malacarne"
                    },
                    {
                        "authorId": "2150775",
                        "name": "Erlend Aune"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A long-standing line of research has thus focused on efforts in learning informative time series representations, such as simple vectors, that are capable of capturing local and global structure in such data (Franceschi et al., 2019; Gu et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "be30fc0627babcc50974c940847e8ba6f796e632",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-01841",
                    "ArXiv": "2303.01841",
                    "DOI": "10.48550/arXiv.2303.01841",
                    "CorpusId": 257353697
                },
                "corpusId": 257353697,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/be30fc0627babcc50974c940847e8ba6f796e632",
                "title": "Anamnesic Neural Differential Equations with Orthogonal Polynomial Projections",
                "abstract": "Neural ordinary differential equations (Neural ODEs) are an effective framework for learning dynamical systems from irregularly sampled time series data. These models provide a continuous-time latent representation of the underlying dynamical system where new observations at arbitrary time points can be used to update the latent representation of the dynamical system. Existing parameterizations for the dynamics functions of Neural ODEs limit the ability of the model to retain global information about the time series; specifically, a piece-wise integration of the latent process between observations can result in a loss of memory on the dynamic patterns of previously observed data points. We propose PolyODE, a Neural ODE that models the latent continuous-time process as a projection onto a basis of orthogonal polynomials. This formulation enforces long-range memory and preserves a global representation of the underlying dynamical system. Our construction is backed by favourable theoretical guarantees and in a series of experiments, we demonstrate that it outperforms previous works in the reconstruction of past and future data, and in downstream prediction tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51457192",
                        "name": "E. Brouwer"
                    },
                    {
                        "authorId": "145253891",
                        "name": "R. G. Krishnan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "T-Loss [13] proposed an unsupervised triplet loss employing time-based negative sampling.",
                "However, only a few researches have been proposed for time-series analysis [13, 14, 15, 16, 17].",
                "In this study, we compared our method with the benchmark models, including DTW [45], T-Loss [13], TNC[14], TS-TCC[16], TST [15], and TS2Vec [17] for 30 classification datasets in the UEA archive.",
                "T-Loss [13] mainly pursued the subseries consistency that encourages representations of the input time segment and its sampled sub-series to be close to each other."
            ],
            "citingPaper": {
                "paperId": "3c8cc085f6cff1fcbb1ec223ab0aea8060422ecb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-01034",
                    "ArXiv": "2303.01034",
                    "DOI": "10.48550/arXiv.2303.01034",
                    "CorpusId": 257279888
                },
                "corpusId": 257279888,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3c8cc085f6cff1fcbb1ec223ab0aea8060422ecb",
                "title": "Multi-Task Self-Supervised Time-Series Representation Learning",
                "abstract": "Time-series representation learning can extract representations from data with temporal dynamics and sparse labels. When labeled data are sparse but unlabeled data are abundant, contrastive learning, i.e., a framework to learn a latent space where similar samples are close to each other while dissimilar ones are far from each other, has shown outstanding performance. This strategy can encourage varied consistency of time-series representations depending on the positive pair selection and contrastive loss. We propose a new time-series representation learning method by combining the advantages of self-supervised tasks related to contextual, temporal, and transformation consistency. It allows the network to learn general representations for various downstream tasks and domains. Specifically, we first adopt data preprocessing to generate positive and negative pairs for each self-supervised task. The model then performs contextual, temporal, and transformation contrastive learning and is optimized jointly using their contrastive losses. We further investigate an uncertainty weighting approach to enable effective multi-task learning by considering the contribution of each consistency. We evaluate the proposed framework on three downstream tasks: time-series classification, forecasting, and anomaly detection. Experimental results show that our method not only outperforms the benchmark models on these downstream tasks, but also shows efficiency in cross-domain transfer learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111432940",
                        "name": "Heejeong Choi"
                    },
                    {
                        "authorId": "2994930",
                        "name": "Pilsung Kang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For instance in T-Loss [42], time-based negative sampling with triple loss is utilized, simultaneously."
            ],
            "citingPaper": {
                "paperId": "89983c7d510ccf76362a4a2881f0c605b5b5d9b3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-00320",
                    "ArXiv": "2303.00320",
                    "DOI": "10.48550/arXiv.2303.00320",
                    "CorpusId": 257255150
                },
                "corpusId": 257255150,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/89983c7d510ccf76362a4a2881f0c605b5b5d9b3",
                "title": "TimeMAE: Self-Supervised Representations of Time Series with Decoupled Masked Autoencoders",
                "abstract": "Enhancing the expressive capacity of deep learning-based time series models with self-supervised pre-training has become ever-increasingly prevalent in time series classification. Even though numerous efforts have been devoted to developing self-supervised models for time series data, we argue that the current methods are not sufficient to learn optimal time series representations due to solely unidirectional encoding over sparse point-wise input units. In this work, we propose TimeMAE, a novel self-supervised paradigm for learning transferrable time series representations based on transformer networks. The distinct characteristics of the TimeMAE lie in processing each time series into a sequence of non-overlapping sub-series via window-slicing partitioning, followed by random masking strategies over the semantic units of localized sub-series. Such a simple yet effective setting can help us achieve the goal of killing three birds with one stone, i.e., (1) learning enriched contextual representations of time series with a bidirectional encoding scheme; (2) increasing the information density of basic semantic units; (3) efficiently encoding representations of time series using transformer networks. Nevertheless, it is a non-trivial to perform reconstructing task over such a novel formulated modeling paradigm. To solve the discrepancy issue incurred by newly injected masked embeddings, we design a decoupled autoencoder architecture, which learns the representations of visible (unmasked) positions and masked ones with two different encoder modules, respectively. Furthermore, we construct two types of informative targets to accomplish the corresponding pretext tasks. One is to create a tokenizer module that assigns a codeword to each masked region, allowing the masked codeword classification (MCC) task to be completed effectively...",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1491233507",
                        "name": "Mingyue Cheng"
                    },
                    {
                        "authorId": "50384171",
                        "name": "Qi Liu"
                    },
                    {
                        "authorId": "2163529740",
                        "name": "Zhiding Liu"
                    },
                    {
                        "authorId": "145140331",
                        "name": "Haotong Zhang"
                    },
                    {
                        "authorId": "2110255993",
                        "name": "Rujiao Zhang"
                    },
                    {
                        "authorId": "2173129111",
                        "name": "Enhong Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We follow (Zerveas et al., 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilationCNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al., 2020), and a transformer-based TST (Zerveas et\u2026",
                ", 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilationCNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al."
            ],
            "citingPaper": {
                "paperId": "668fe862cbfcab26b67dd81b4cefa6c0cf11adab",
                "externalIds": {
                    "ArXiv": "2303.12799",
                    "DBLP": "journals/corr/abs-2303-12799",
                    "DOI": "10.48550/arXiv.2303.12799",
                    "CorpusId": 257687717
                },
                "corpusId": 257687717,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/668fe862cbfcab26b67dd81b4cefa6c0cf11adab",
                "title": "Time Series as Images: Vision Transformer for Irregularly Sampled Time Series",
                "abstract": "Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance improvement is up to 54.0\\% in absolute F1 score points. Our code and data are available at \\url{https://github.com/Leezekun/ViTST}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2168519132",
                        "name": "Zekun Li"
                    },
                    {
                        "authorId": "50341591",
                        "name": "SHIYANG LI"
                    },
                    {
                        "authorId": "1740249",
                        "name": "Xifeng Yan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "applies triplet loss to unsupervised learning through specially designed sampling method [7].",
                "The exponential dilated causal convolution inspired by WaveNet is also used for universal time-series data [7]."
            ],
            "citingPaper": {
                "paperId": "aa41a3a5b439604987adc25ab8d5cfe20d1f3ddb",
                "externalIds": {
                    "DBLP": "journals/sensors/ZhaoFZTZFYLFM23",
                    "PubMedCentral": "10056920",
                    "DOI": "10.3390/s23063345",
                    "CorpusId": 257707684,
                    "PubMed": "36992056"
                },
                "corpusId": 257707684,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aa41a3a5b439604987adc25ab8d5cfe20d1f3ddb",
                "title": "Time-Series Representation Learning in Topology Prediction for Passive Optical Network of Telecom Operators",
                "abstract": "The passive optical network (PON) is widely used in optical fiber communication thanks to its low cost and low resource consumption. However, the passiveness brings about a critical problem that it requires manual work to identify the topology structure, which is costly and prone to bringing noise to the topology logs. In this paper, we provide a base solution firstly introducing neural networks for such problems, and based on that solution we propose a complete methodology (PT-Predictor) for predicting PON topology through representation learning on its optical power data. Specifically, we design useful model ensembles (GCE-Scorer) to extract the features of optical power with noise-tolerant training techniques integrated. We further implement a data-based aggregation algorithm (MaxMeanVoter) and a novel Transformer-based voter (TransVoter) to predict the topology. Compared with previous model-free methods, PT-Predictor is able to improve prediction accuracy by 23.1% in scenarios where data provided by telecom operators is sufficient, and by 14.8% in scenarios where data is temporarily insufficient. Besides, we identify a class of scenarios where PON topology does not follow a strict tree structure, and thus topology prediction cannot be effectively performed by relying on optical power data alone, which will be studied in our future work.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112672549",
                        "name": "Haoran Zhao"
                    },
                    {
                        "authorId": "10283826",
                        "name": "Yuchen Fang"
                    },
                    {
                        "authorId": "2180221539",
                        "name": "Yuxiang Zhao"
                    },
                    {
                        "authorId": "2212096868",
                        "name": "Zheng Tian"
                    },
                    {
                        "authorId": "2108309275",
                        "name": "Weinan Zhang"
                    },
                    {
                        "authorId": "2212014490",
                        "name": "Xidong Feng"
                    },
                    {
                        "authorId": "2217578189",
                        "name": "Li Yu"
                    },
                    {
                        "authorId": "2157337709",
                        "name": "Wei Li"
                    },
                    {
                        "authorId": "2212010696",
                        "name": "Hulei Fan"
                    },
                    {
                        "authorId": "2212108391",
                        "name": "Tiema Mu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "004616ad8810aa56844d05fdf5ffa88545b705fb",
                "externalIds": {
                    "ArXiv": "2302.11939",
                    "CorpusId": 258741419
                },
                "corpusId": 258741419,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/004616ad8810aa56844d05fdf5ffa88545b705fb",
                "title": "One Fits All:Power General Time Series Analysis by Pretrained LM",
                "abstract": "Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time series. Our results demonstrate that pre-trained models on natural language or images can lead to a comparable or state-of-the-art performance in all main time series analysis tasks, as illustrated in Figure 1. We also found both theoretically and empirically that the self-attention module behaviors similarly to principle component analysis (PCA), an observation that helps explains how transformer bridges the domain gap and a crucial step towards understanding the universality of a pre-trained transformer.The code is publicly available at https://github.com/DAMO-DI-ML/One_Fits_All.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153423819",
                        "name": "Tian Zhou"
                    },
                    {
                        "authorId": "2117218646",
                        "name": "Peisong Niu"
                    },
                    {
                        "authorId": "2118294665",
                        "name": "Xue Wang"
                    },
                    {
                        "authorId": "2110940896",
                        "name": "Liang Sun"
                    },
                    {
                        "authorId": "2152101871",
                        "name": "Rong Jin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", state-space models, stochastic recurrent neural networks, etc [37, 38, 21]."
            ],
            "citingPaper": {
                "paperId": "a3b24456ae6abdae5ec2a3d764efc019b583d62a",
                "externalIds": {
                    "ArXiv": "2302.11078",
                    "DBLP": "journals/corr/abs-2302-11078",
                    "DOI": "10.48550/arXiv.2302.11078",
                    "CorpusId": 254183692
                },
                "corpusId": 254183692,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a3b24456ae6abdae5ec2a3d764efc019b583d62a",
                "title": "Learning Mixture Structure on Multi-Source Time Series for Probabilistic Forecasting",
                "abstract": "In many data-driven applications, collecting data from different sources is increasingly desirable for enhancing performance. In this paper, we are interested in the problem of probabilistic forecasting with multi-source time series. We propose a neural mixture structure-based probability model for learning different predictive relations and their adaptive combinations from multi-source time series. We present the prediction and uncertainty quantification methods that apply to different distributions of target variables. Additionally, given the imbalanced and unstable behaviors observed during the direct training of the proposed mixture model, we develop a phased learning method and provide a theoretical analysis. In experimental evaluations, the mixture model trained by the phased learning exhibits competitive performance on both point and probabilistic prediction metrics. Meanwhile, the proposed uncertainty conditioned error suggests the potential of the mixture model's uncertainty score as a reliability indicator of predictions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144792678",
                        "name": "Tianli Guo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Graphs in Figure 6 show that despite the influence of ShapeWord Length LSW , the overlapping optimal interval of three datasets for NSW is approximately [5, 12].",
                "In our scheme, we choose the deep dilated causal convolutional neural network [12] as the encoder backbone given its high efficiency and outstanding excellence in capturing long-range dependencies [5]."
            ],
            "citingPaper": {
                "paperId": "093badd49ff7c0aee94bd109ffdcf8173f578f2d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-05021",
                    "ArXiv": "2302.05021",
                    "DOI": "10.48550/arXiv.2302.05021",
                    "CorpusId": 256808198
                },
                "corpusId": 256808198,
                "publicationVenue": {
                    "id": "8107ca1c-f651-4769-86dc-3d94a7b5ac26",
                    "name": "International Conference on Database Systems for Advanced Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Database Syst Adv Appl",
                        "Database Syst Adv Appl",
                        "Database Systems for Advanced Applications",
                        "DASFAA"
                    ],
                    "url": "http://www.dasfaa.org/"
                },
                "url": "https://www.semanticscholar.org/paper/093badd49ff7c0aee94bd109ffdcf8173f578f2d",
                "title": "ShapeWordNet: An Interpretable Shapelet Neural Network for Physiological Signal Classification",
                "abstract": "Physiological signals are high-dimensional time series of great practical values in medical and healthcare applications. However, previous works on its classification fail to obtain promising results due to the intractable data characteristics and the severe label sparsity issues. In this paper, we try to address these challenges by proposing a more effective and interpretable scheme tailored for the physiological signal classification task. Specifically, we exploit the time series shapelets to extract prominent local patterns and perform interpretable sequence discretization to distill the whole-series information. By doing so, the long and continuous raw signals are compressed into short and discrete token sequences, where both local patterns and global contexts are well preserved. Moreover, to alleviate the label sparsity issue, a multi-scale transformation strategy is adaptively designed to augment data and a cross-scale contrastive learning mechanism is accordingly devised to guide the model training. We name our method as ShapeWordNet and conduct extensive experiments on three real-world datasets to investigate its effectiveness. Comparative results show that our proposed scheme remarkably outperforms four categories of cutting-edge approaches. Visualization analysis further witnesses the good interpretability of the sequence discretization idea based on shapelets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2107123679",
                        "name": "W. He"
                    },
                    {
                        "authorId": "1491233507",
                        "name": "Mingyue Cheng"
                    },
                    {
                        "authorId": "2157149882",
                        "name": "Qi Liu"
                    },
                    {
                        "authorId": "2118214458",
                        "name": "Zhi Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, following previous contrastive learning and masked modeling paradigms, some self-supervised pre-training methods for time series have been proposed (Franceschi et al., 2019; Sarkar & Etemad, 2020; Rebjock et al., 2021; Sun et al., 2021; Yang & Hong, 2022)."
            ],
            "citingPaper": {
                "paperId": "581921989e50637b1f9930670e623fbd18e42c5d",
                "externalIds": {
                    "ArXiv": "2302.00861",
                    "DBLP": "journals/corr/abs-2302-00861",
                    "DOI": "10.48550/arXiv.2302.00861",
                    "CorpusId": 256503994
                },
                "corpusId": 256503994,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/581921989e50637b1f9930670e623fbd18e42c5d",
                "title": "SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling",
                "abstract": "Time series analysis is widely used in extensive areas. Recently, to reduce labeling expenses and benefit various tasks, self-supervised pre-training has attracted immense interest. One mainstream paradigm is masked modeling, which successfully pre-trains deep models by learning to reconstruct the masked content based on the unmasked part. However, since the semantic information of time series is mainly contained in temporal variations, the standard way of randomly masking a portion of time points will seriously ruin vital temporal variations of time series, making the reconstruction task too difficult to guide representation learning. We thus present SimMTM, a Simple pre-training framework for Masked Time-series Modeling. By relating masked modeling to manifold learning, SimMTM proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold, which eases the reconstruction task by assembling ruined but complementary temporal variations from multiple masked series. SimMTM further learns to uncover the local structure of the manifold, which is helpful for masked modeling. Experimentally, SimMTM achieves state-of-the-art fine-tuning performance compared to the most advanced time series pre-training methods in two canonical time series analysis tasks: forecasting and classification, covering both in- and cross-domain settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "13090675",
                        "name": "Jiaxiang Dong"
                    },
                    {
                        "authorId": "2051867856",
                        "name": "Haixu Wu"
                    },
                    {
                        "authorId": "2135691006",
                        "name": "Haoran Zhang"
                    },
                    {
                        "authorId": "48571183",
                        "name": "Li Zhang"
                    },
                    {
                        "authorId": "2116460725",
                        "name": "Jianmin Wang"
                    },
                    {
                        "authorId": "35776445",
                        "name": "Mingsheng Long"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "work, takes the hierarchical and sequential structure of time series data into account [62], but otherwise does not specifically exploit potential skew product structure of the underlying time series.",
                "We also consider artificial neural networks in the form of a causal convolutional network (cCNN), an established architecture for time series representation learning [62]; and Latent Factor Analysis via Dynamical Systems (LFADS), a recurrent variational autoencoder [63]."
            ],
            "citingPaper": {
                "paperId": "02aa422b282eb22c419e4805ced1176e92073447",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-13516",
                    "ArXiv": "2301.13516",
                    "DOI": "10.48550/arXiv.2301.13516",
                    "CorpusId": 256416401
                },
                "corpusId": 256416401,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/02aa422b282eb22c419e4805ced1176e92073447",
                "title": "Recurrences reveal shared causal drivers of complex time series",
                "abstract": "Many experimental time series measurements share unobserved causal drivers. Examples include genes targeted by transcription factors, ocean flows influenced by large-scale atmospheric currents, and motor circuits steered by descending neurons. Reliably inferring this unseen driving force is necessary to understand the intermittent nature of top-down control schemes in diverse biological and engineered systems. Here, we introduce a new unsupervised learning algorithm that uses recurrences in time series measurements to gradually reconstruct an unobserved driving signal. Drawing on the mathematical theory of skew-product dynamical systems, we identify recurrence events shared across response time series, which implicitly define a recurrence graph with glass-like structure. As the amount or quality of observed data improves, this recurrence graph undergoes a percolation transition manifesting as weak ergodicity breaking for random walks on the induced landscape -- revealing the shared driver's dynamics, even in the presence of strongly corrupted or noisy measurements. Across several thousand random dynamical systems, we empirically quantify the dependence of reconstruction accuracy on the rate of information transfer from a chaotic driver to the response systems, and we find that effective reconstruction proceeds through gradual approximation of the driver's dominant orbit topology. Through extensive benchmarks against classical and neural-network-based signal processing techniques, we demonstrate our method's strong ability to extract causal driving signals from diverse real-world datasets spanning ecology, genomics, fluid dynamics, and physiology.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2067095315",
                        "name": "W. Gilpin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Lei et al. (2019); Franceschi et al. (2019) used loss function of metric learning to preserve pairwise similarities in the time domain.",
                "To evaluate the performance of models on classification, we follow the same protocol Franceschi et al. (2019), where an SVM classifier with RBF kernel is trained on obtained instance-level representations.",
                "For time series classification tasks, we include more competitive unsupervised representation learning methods: TS2Vec, T-Loss (Franceschi et al., 2019), TS-TCC (Eldele et al., 2021), TST (Zerveas et al., 2021), TNC (Tonekaboni et al., 2021) and DTW (Chen et al., 2013).",
                "For time series classification tasks, we include more competitive unsupervised representation learning methods: TS2Vec, T-Loss (Franceschi et al., 2019), TS-TCC (Eldele et al."
            ],
            "citingPaper": {
                "paperId": "644b14c253bc76f0914b1645d7af59e6042d59f9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-08871",
                    "ArXiv": "2301.08871",
                    "DOI": "10.48550/arXiv.2301.08871",
                    "CorpusId": 256105702
                },
                "corpusId": 256105702,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/644b14c253bc76f0914b1645d7af59e6042d59f9",
                "title": "Ti-MAE: Self-Supervised Masked Time Series Autoencoders",
                "abstract": "Multivariate Time Series forecasting has been an increasingly popular topic in various applications and scenarios. Recently, contrastive learning and Transformer-based models have achieved good performance in many long-term series forecasting tasks. However, there are still several issues in existing methods. First, the training paradigm of contrastive learning and downstream prediction tasks are inconsistent, leading to inaccurate prediction results. Second, existing Transformer-based models which resort to similar patterns in historical time series data for predicting future values generally induce severe distribution shift problems, and do not fully leverage the sequence information compared to self-supervised methods. To address these issues, we propose a novel framework named Ti-MAE, in which the input time series are assumed to follow an integrate distribution. In detail, Ti-MAE randomly masks out embedded time series data and learns an autoencoder to reconstruct them at the point-level. Ti-MAE adopts mask modeling (rather than contrastive learning) as the auxiliary task and bridges the connection between existing representation learning and generative Transformer-based methods, reducing the difference between upstream and downstream forecasting tasks while maintaining the utilization of original time series data. Experiments on several public real-world datasets demonstrate that our framework of masked autoencoding could learn strong representations directly from the raw data, yielding better performance in time series forecasting and classification tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144241452",
                        "name": "Zhe Li"
                    },
                    {
                        "authorId": "2202227635",
                        "name": "Zhongwen Rao"
                    },
                    {
                        "authorId": "2288532",
                        "name": "Lujia Pan"
                    },
                    {
                        "authorId": "2108815759",
                        "name": "Pengyun Wang"
                    },
                    {
                        "authorId": "1683510",
                        "name": "Zenglin Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2022 Deep-learning based methods USRL-FordA [7] is an unsupervised method to learn\nuniversal embeddings of time series and achieves stateof-the-art performance in time series classification.",
                "\u2022 Deep-learning based methods USRL-FordA [7] is an unsupervised method to learn universal embeddings of time series and achieves stateof-the-art performance in time series classification.",
                "The results show that our mthod is superior to RISE, SAXVFSEQL, FS, LRS and USRL-FordA because the 1-to-1 wins numbers for these methods are all more than half of all the datasets.",
                "Thus, we can conclude the performance of our method on classification accuracy is at the same level with COTE and significantly better than RISE, SAX-VFSEQL, FS, LRS and USRL-FordA."
            ],
            "citingPaper": {
                "paperId": "885b456241ff9dbb8ed120d7490c8ec8d29f5253",
                "externalIds": {
                    "DBLP": "journals/apin/ChenW23",
                    "DOI": "10.1007/s10489-022-04422-2",
                    "CorpusId": 256189117
                },
                "corpusId": 256189117,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/885b456241ff9dbb8ed120d7490c8ec8d29f5253",
                "title": "Localized shapelets selection for interpretable time series classification",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2202247544",
                        "name": "Jiahui Chen"
                    },
                    {
                        "authorId": "2075389455",
                        "name": "Yuan Wan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "to construct different views of the input data, then the feature representation is learned by maximizing the similarity between different views of the same sample and minimizing the similarity of different sample views [31,32]."
            ],
            "citingPaper": {
                "paperId": "ec0ab98cac24038bb8ad679ab6cc859d50b2ac31",
                "externalIds": {
                    "DOI": "10.1007/s40747-022-00945-w",
                    "CorpusId": 255700762
                },
                "corpusId": 255700762,
                "publicationVenue": {
                    "id": "d30c9917-b233-46f4-a644-8e5cdf6d6c5e",
                    "name": "Complex & Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Complex  Intell Syst"
                    ],
                    "issn": "2199-4536",
                    "url": "https://link.springer.com/journal/40747",
                    "alternate_urls": [
                        "http://link.springer.com/journal/40747"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ec0ab98cac24038bb8ad679ab6cc859d50b2ac31",
                "title": "A bidirectional trajectory contrastive learning model for driving intention prediction",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2157399331",
                        "name": "Yi Zhou"
                    },
                    {
                        "authorId": "2145330096",
                        "name": "Huxiao Wang"
                    },
                    {
                        "authorId": "51221884",
                        "name": "Nianwen Ning"
                    },
                    {
                        "authorId": "2190592654",
                        "name": "Zhangyun Wang"
                    },
                    {
                        "authorId": "2108382019",
                        "name": "Yanyu Zhang"
                    },
                    {
                        "authorId": "2190583120",
                        "name": "Fuqiang Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", [15, 23, 34]) have shown that different states of a system (measured by a multivariate time series) can be learned for each time step in a self-",
                ", [15, 34]), we first encode MOTS in short time windows with a neural network to a spatio-temporal voxel embedding."
            ],
            "citingPaper": {
                "paperId": "0469298dd78c4211d2d920f9a3fb17d52c65c649",
                "externalIds": {
                    "ArXiv": "2212.14750",
                    "DBLP": "journals/corr/abs-2212-14750",
                    "DOI": "10.1109/WACV56688.2023.00169",
                    "CorpusId": 255341143
                },
                "corpusId": 255341143,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/0469298dd78c4211d2d920f9a3fb17d52c65c649",
                "title": "Unsupervised 4D LiDAR Moving Object Segmentation in Stationary Settings with Multivariate Occupancy Time Series",
                "abstract": "In this work, we address the problem of unsupervised moving object segmentation (MOS) in 4D LiDAR data recorded from a stationary sensor, where no ground truth annotations are involved. Deep learning-based state-of-the-art methods for LiDAR MOS strongly depend on annotated ground truth data, which is expensive to obtain and scarce in existence. To close this gap in the stationary setting, we propose a novel 4D LiDAR representation based on multivariate time series that relaxes the problem of unsupervised MOS to a time series clustering problem. More specifically, we propose modeling the change in occupancy of a voxel by a multivariate occupancy time series (MOTS), which captures spatio-temporal occupancy changes on the voxel level and its surrounding neighborhood. To perform unsupervised MOS, we train a neural network in a self-supervised manner to encode MOTS into voxel-level feature representations, which can be partitioned by a clustering algorithm into moving or stationary. Experiments on stationary scenes from the Raw KITTI dataset show that our fully unsupervised approach achieves performance that is comparable to that of supervised state-of-the-art approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "15274050",
                        "name": "T. Kreutz"
                    },
                    {
                        "authorId": "1725964",
                        "name": "M. M\u00fchlh\u00e4user"
                    },
                    {
                        "authorId": "1792693",
                        "name": "Alejandro S\u00e1nchez Guinea"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "[17] presented an unsupervised learning model with convolutional kernels for time series feature transformation, in which the dilation factors of kernels increased exponentially layer by layer."
            ],
            "citingPaper": {
                "paperId": "6de57c70a2b661f4c8171e26b5f53b5782d937d1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-09840",
                    "ArXiv": "2212.09840",
                    "DOI": "10.48550/arXiv.2212.09840",
                    "CorpusId": 254103020
                },
                "corpusId": 254103020,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6de57c70a2b661f4c8171e26b5f53b5782d937d1",
                "title": "Dynamic Sparse Network for Time Series Classification: Learning What to \"see\"",
                "abstract": "The receptive field (RF), which determines the region of time series to be ``seen'' and used, is critical to improve the performance for time series classification (TSC). However, the variation of signal scales across and within time series data, makes it challenging to decide on proper RF sizes for TSC. In this paper, we propose a dynamic sparse network (DSN) with sparse connections for TSC, which can learn to cover various RF without cumbersome hyper-parameters tuning. The kernels in each sparse layer are sparse and can be explored under the constraint regions by dynamic sparse training, which makes it possible to reduce the resource cost. The experimental results show that the proposed DSN model can achieve state-of-art performance on both univariate and multivariate TSC datasets with less than 50\\% computational cost compared with recent baseline methods, opening the path towards more accurate resource-aware methods for time series analyses. Our code is publicly available at: https://github.com/QiaoXiao7282/DSN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056427208",
                        "name": "Qiao Xiao"
                    },
                    {
                        "authorId": "46791907",
                        "name": "Boqian Wu"
                    },
                    {
                        "authorId": null,
                        "name": "Yu Zhang"
                    },
                    {
                        "authorId": "47130544",
                        "name": "Shiwei Liu"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "authorId": "146634864",
                        "name": "Elena Mocanu"
                    },
                    {
                        "authorId": "2571038",
                        "name": "D. Mocanu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In particular, the work in [11] constructs an encoder-only architecture using TCN with triple loss and negative sampling to generate representation",
                "5) TCN [11] constructs an encoder-only architecture using temporal convolutional networks with triple loss and negative sampling to generate representation embeddings."
            ],
            "citingPaper": {
                "paperId": "ef269de07d24b3a525bb543de951542c0bf15cb1",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/HuangSY22",
                    "DOI": "10.1109/BigData55660.2022.10020260",
                    "CorpusId": 256322877
                },
                "corpusId": 256322877,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ef269de07d24b3a525bb543de951542c0bf15cb1",
                "title": "Deep Time Series Sketching and Its Application on Industrial Time Series Clustering",
                "abstract": "Today, voluminous multivariate time series data collected from sensors provides tremendous benefit for understanding of modern industrial systems such as power plants, wind turbines and aircrafts. However, the dynamic and complex nature of these systems, as well as the lack of prior knowledge impose challenges in perceiving different system behaviors from the time series data. To handle these issues, time series clustering has become one of the key analysis techniques. Nevertheless, the data nonlinearity, varying lengths and high dimensions of industrial time series could hinder the quality of clustering. To deal with these challenges, we propose Deep Time Series Sketching (DTSS) model. This model is a representation learning model based on temporal convolutional networks that perform on a sliding window basis along time series to learn the windows\u2019 embeddings. The sequence of embeddings is then fed into embedding sketching to obtain its sketch. Such sketch is a descriptor of the whole time series and will be fed into K-means for clustering. Our model is a novel end-to-end hybrid model that incorporates both local and global contextual features. It is able to project multivariate time series with varying lengths into the same latent space. Moreover, we show that our model is able to perform early clustering as it can assign real-time label without seeing the whole time series. We test our model on both benchmark and real world industrial datasets, and experiments show that our proposed method outperforms popular time series clustering baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143995146",
                        "name": "Hao Huang"
                    },
                    {
                        "authorId": "2053683973",
                        "name": "Tapan Shah"
                    },
                    {
                        "authorId": "2282774",
                        "name": "Shinjae Yoo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "GRU [22], LSTM [23], [24], dilated convolutions [25], [26] and ResNet [27].",
                "Most existing methods are based on some comparison functions [33] or contrasting learning [26], [34], [35].",
                "implementation of [26] and ResNet follows [77].",
                "T-Loss [26] adopts a triplet loss on randomly cropped subseries to enhance",
                "Following previous works [16], [26], [36], we evaluate the",
                "The 1D-convolution module denotes a multi-layer dilated convolution [26], where the first layer has a dilation s = 1; the second layer has a dilation s = 2, and the n-th layer has a dilation s = 2."
            ],
            "citingPaper": {
                "paperId": "df6e42e06d77ebc28b50492b1fd26b39c1bd4277",
                "externalIds": {
                    "ArXiv": "2212.08330",
                    "DBLP": "journals/corr/abs-2212-08330",
                    "DOI": "10.1109/TPAMI.2023.3236725",
                    "CorpusId": 254823112,
                    "PubMed": "37018677"
                },
                "corpusId": 254823112,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/df6e42e06d77ebc28b50492b1fd26b39c1bd4277",
                "title": "Convolution-Enhanced Evolving Attention Networks",
                "abstract": "Attention-based neural networks, such as Transformers, have become ubiquitous in numerous applications, including computer vision, natural language processing, and time-series analysis. In all kinds of attention networks, the attention maps are crucial as they encode semantic dependencies between input tokens. However, most existing attention networks perform modeling or reasoning based on representations, wherein the attention maps of different layers are learned separately without explicit interactions. In this paper, we propose a novel and generic evolving attention mechanism, which directly models the evolution of inter-token relationships through a chain of residual convolutional modules. The major motivations are twofold. On the one hand, the attention maps in different layers share transferable knowledge, thus adding a residual connection can facilitate the information flow of inter-token relationships across layers. On the other hand, there is naturally an evolutionary trend among attention maps at different abstraction levels, so it is beneficial to exploit a dedicated convolution-based module to capture this process. Equipped with the proposed mechanism, the convolution-enhanced evolving attention networks achieve superior performance in various applications, including time-series representation, natural language understanding, machine translation, and image classification. Especially on time-series representation tasks, Evolving Attention-enhanced Dilated Convolutional (EA-DC-) Transformer outperforms state-of-the-art models significantly, achieving an average of 17% improvement compared to the best SOTA. To the best of our knowledge, this is the first work that explicitly models the layer-wise evolution of attention maps. Our implementation is available at https://github.com/pkuyym/EvolvingAttention.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115657798",
                        "name": "Yujing Wang"
                    },
                    {
                        "authorId": "2152661290",
                        "name": "Yaming Yang"
                    },
                    {
                        "authorId": "80389349",
                        "name": "Zhuowan Li"
                    },
                    {
                        "authorId": "152770689",
                        "name": "Jiangang Bai"
                    },
                    {
                        "authorId": "2129402731",
                        "name": "Mingliang Zhang"
                    },
                    {
                        "authorId": "92385001",
                        "name": "Xiangtai Li"
                    },
                    {
                        "authorId": "119883573",
                        "name": "J. Yu"
                    },
                    {
                        "authorId": "2109269339",
                        "name": "Ce Zhang"
                    },
                    {
                        "authorId": "2115218570",
                        "name": "Gao Huang"
                    },
                    {
                        "authorId": "2054671931",
                        "name": "Yu Tong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "We compare MHCCL with eight state-of-the-art approaches in two categories: 1) instance-wise approaches including SimCLR (Chen et al. 2020), BYOL (Grill et al. 2020), TLoss (Franceschi, Dieuleveut, and Jaggi 2019), TSTCC (Eldele et al. 2021) and TS2Vec (Yue et al. 2022), and 2) cluster-wise approaches including SwAV (Caron et al. 2020), PCL (Li et al. 2021a) and CCL (Sharma et al. 2020).",
                "T-Loss (Franceschi, Dieuleveut, and Jaggi 2019) employs an efficient triplet loss that uses timebased negative sampling to distinguish anchors from negative instances, and assimilate anchors and positive instances.",
                "\u2026has achieved remarkable advantages in diverse applications such as image (Chen et al. 2020; Grill et al. 2020; He et al. 2020; Chen et al. 2021; Dave et al. 2022) and time series classification (Franceschi, Dieuleveut, and Jaggi 2019; Eldele et al. 2021; Yue et al. 2022; Bagnall et al. 2017).",
                "\u2026state-of-the-art approaches in two categories: 1) instance-wise approaches including SimCLR (Chen et al. 2020), BYOL (Grill et al. 2020), TLoss (Franceschi, Dieuleveut, and Jaggi 2019), TSTCC (Eldele et al. 2021) and TS2Vec (Yue et al. 2022), and 2) cluster-wise approaches including SwAV\u2026"
            ],
            "citingPaper": {
                "paperId": "08a103c145772c83f9544c8798dbcc292bdff762",
                "externalIds": {
                    "ArXiv": "2212.01141",
                    "DBLP": "journals/corr/abs-2212-01141",
                    "DOI": "10.48550/arXiv.2212.01141",
                    "CorpusId": 254220847
                },
                "corpusId": 254220847,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/08a103c145772c83f9544c8798dbcc292bdff762",
                "title": "MHCCL: Masked Hierarchical Cluster-wise Contrastive Learning for Multivariate Time Series",
                "abstract": "Learning semantic-rich representations from raw unlabeled time series data is critical for downstream tasks such as classification and forecasting. Contrastive learning has recently shown its promising representation learning capability in the absence of expert annotations. However, existing contrastive approaches generally treat each instance independently, which leads to false negative pairs that share the same semantics. To tackle this problem, we propose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model, which exploits semantic information obtained from the hierarchical structure consisting of multiple latent partitions for multivariate time series. Motivated by the observation that fine-grained clustering preserves higher purity while coarse-grained one reflects higher-level semantics, we propose a novel downward masking strategy to filter out fake negatives and supplement positives by incorporating the multi-granularity information from the clustering hierarchy. In addition, a novel upward masking strategy is designed in MHCCL to remove outliers of clusters at each partition to refine prototypes, which helps speed up the hierarchical clustering process and improves the clustering quality. We conduct experimental evaluations on seven widely-used multivariate time series datasets. The results demonstrate the superiority of MHCCL over the state-of-the-art approaches for unsupervised time series representation learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "103236149",
                        "name": "Qianwen Meng"
                    },
                    {
                        "authorId": "1732549",
                        "name": "Hangwei Qian"
                    },
                    {
                        "authorId": "2144386151",
                        "name": "Yong Liu"
                    },
                    {
                        "authorId": "153018970",
                        "name": "Yonghui Xu"
                    },
                    {
                        "authorId": "2111639168",
                        "name": "Zhiqi Shen"
                    },
                    {
                        "authorId": "101457473",
                        "name": "Li-zhen Cui"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "205fd9413b159e9f1d5e5cd8fa69960c0a3a31bd",
                "externalIds": {
                    "DOI": "10.1051/wujns/2022276521",
                    "CorpusId": 255930023
                },
                "corpusId": 255930023,
                "publicationVenue": {
                    "id": "bec67739-611a-4a92-a592-5c6bf70bdffc",
                    "name": "Wuhan University Journal of Natural Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Wuhan Univ J Nat Sci"
                    ],
                    "issn": "1007-1202",
                    "url": "http://www.springer.com/west/home?SGWID=4-102-70-173677603-0&changeHeader=true",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11859"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/205fd9413b159e9f1d5e5cd8fa69960c0a3a31bd",
                "title": "Self-Supervised Time Series Classification Based on LSTM and Contrastive Transformer",
                "abstract": "Time series data has attached extensive attention as multi-domain data, but it is difficult to analyze due to its high dimension and few labels. Self-supervised representation learning provides an effective way for processing such data. Considering the frequency domain features of the time series data itself and the contextual feature in the classification task, this paper proposes an unsupervised Long Short-Term Memory (LSTM) and contrastive transformer-based time series representation model using contrastive learning. Firstly, transforming data with frequency domain-based augmentation increases the ability to represent features in the frequency domain. Secondly, the encoder module with three layers of LSTM and convolution maps the augmented data to the latent space and calculates the temporal loss with a contrastive transformer module and contextual loss. Finally, after self-supervised training, the representation vector of the original data can be got from the pre-trained encoder. Our model achieves satisfied performances on Human Activity Recognition (HAR) and sleepEDF real-life datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2194105126",
                        "name": "Yuanhao Zou"
                    },
                    {
                        "authorId": "2145056370",
                        "name": "Yufei Zhang"
                    },
                    {
                        "authorId": "2111200005",
                        "name": "Xiaodong Zhao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "There are many non-Transformer-based models proposed in recent years to learn representations in time series (Franceschi et al., 2019; Tonekaboni et al., 2021; Yang & Hong, 2022; Yue et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "dad15404d372a23b4b3bf9a63b3124693df3c85e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-14730",
                    "ArXiv": "2211.14730",
                    "DOI": "10.48550/arXiv.2211.14730",
                    "CorpusId": 254044221
                },
                "corpusId": 254044221,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/dad15404d372a23b4b3bf9a63b3124693df3c85e",
                "title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers",
                "abstract": "We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy. Code is available at: https://github.com/yuqinie98/PatchTST.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152972535",
                        "name": "Yuqi Nie"
                    },
                    {
                        "authorId": "144547425",
                        "name": "Nam H. Nguyen"
                    },
                    {
                        "authorId": "40913517",
                        "name": "Phanwadee Sinthong"
                    },
                    {
                        "authorId": "1682581",
                        "name": "J. Kalagnanam"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "It can be seen that our model performed best on 3 out of 4 datasets, achieving an average rank of 1.5, followed by Causal CNN.",
                "There are two sequentially stacked sub-layers in an MCAT layer: (1) a multi-head self-attention and (2) a multi-scale CNN.",
                "Causal CNN [27]: Causal Convolutional Neural Network (Causal CNN) combines an encoder based on causal dilated convolutions with a triplet loss.",
                "Both Rocket and Causal CNN achieve SOTA performance on other multivariate time series datasets."
            ],
            "citingPaper": {
                "paperId": "e624c652f094e4478f31325b7286b028da402228",
                "externalIds": {
                    "DBLP": "journals/titb/XuWZZZ23",
                    "DOI": "10.1109/JBHI.2022.3219640",
                    "CorpusId": 253303702,
                    "PubMed": "36331630"
                },
                "corpusId": 253303702,
                "publicationVenue": {
                    "id": "eac74c9c-a5c0-417d-8088-8164a6a8bfb3",
                    "name": "IEEE journal of biomedical and health informatics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Journal of Biomedical and Health Informatics",
                        "IEEE j biomed health informatics",
                        "IEEE J Biomed Health Informatics"
                    ],
                    "issn": "2168-2194",
                    "url": "https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=6221020",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221020"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e624c652f094e4478f31325b7286b028da402228",
                "title": "Dual-Stream Contrastive Learning for Channel State Information Based Human Activity Recognition",
                "abstract": "WiFi-based human activity recognition (HAR) has been extensively studied due to its far-reaching applications in health domains, including elderly monitoring, exercise supervision and rehabilitation monitoring, etc. Although existing supervised deep learning techniques have achieved remarkable performances for these tasks, they are however data-hungry and hence are notoriously difficult due to the privacy and incomprehensibility of WiFi-based HAR data. Existing contrastive learning models, mainly designed for computer vision, cannot guarantee their performance on channel state information (CSI) data. To this end, we propose a new dual-stream contrastive learning model that can process and learn the raw WiFi CSI data in a self-supervised manner. More specifically, our proposed method, coined as DualConFi, takes raw WiFI CSI data as input and incorporates channel and temporal streams to learn highly-discriminative spatiotemporal features under a mutual information constraint using unlabeled data. We exhibit the effectiveness of our model on three publicly available CSI data sets in various experiment settings, including linear evaluation, semi-supervised, and transfer learning. We show that DualConFi is able to perform favourably against challenging baselines in each setting. Moreover, by studying the effects of different transform functions on CSI data, we finally verify the effectiveness of highly-discriminative features.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2190019169",
                        "name": "Ke Xu"
                    },
                    {
                        "authorId": "2118443494",
                        "name": "Jiangtao Wang"
                    },
                    {
                        "authorId": "2191208842",
                        "name": "Le Zhang"
                    },
                    {
                        "authorId": "7296648",
                        "name": "Hongyuan Zhu"
                    },
                    {
                        "authorId": "2055037499",
                        "name": "Dingchang Zheng"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "197fa0df563c1a1fc0684ea4f9f17c1c36951c47",
                "externalIds": {
                    "DBLP": "journals/ijis/XingXZLDL22",
                    "DOI": "10.1002/int.22957",
                    "CorpusId": 252533075
                },
                "corpusId": 252533075,
                "publicationVenue": {
                    "id": "05528bac-d212-46a6-9c84-314d4bd77368",
                    "name": "International Journal of Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Intell Syst"
                    ],
                    "issn": "0884-8173",
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/36062",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/1098111X"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/197fa0df563c1a1fc0684ea4f9f17c1c36951c47",
                "title": "SelfMatch: Robust semisupervised time\u2010series classification with self\u2010distillation",
                "abstract": "Over the years, a number of semisupervised deep\u2010learning algorithms have been proposed for time\u2010series classification (TSC). In semisupervised deep learning, from the point of view of representation hierarchy, semantic information extracted from lower levels is the basis of that extracted from higher levels. The authors wonder if high\u2010level semantic information extracted is also helpful for capturing low\u2010level semantic information. This paper studies this problem and proposes a robust semisupervised model with self\u2010distillation (SD) that simplifies existing semisupervised learning (SSL) techniques for TSC, called SelfMatch. SelfMatch hybridizes supervised learning, unsupervised learning, and SD. In unsupervised learning, SelfMatch applies pseudolabeling to feature extraction on labeled data. A weakly augmented sequence is used as a target to guide the prediction of a Timecut\u2010augmented version of the same sequence. SD promotes the knowledge flow from higher to lower levels, guiding the extraction of low\u2010level semantic information. This paper designs a feature extractor for TSC, called ResNet\u2013LSTMaN, responsible for feature and relation extraction. The experimental results show that SelfMatch achieves excellent SSL performance on 35 widely adopted UCR2018 data sets, compared with a number of state\u2010of\u2010the\u2010art semisupervised and supervised algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "38092176",
                        "name": "Huanlai Xing"
                    },
                    {
                        "authorId": "50479843",
                        "name": "Zhiwen Xiao"
                    },
                    {
                        "authorId": "49294642",
                        "name": "Dawei Zhan"
                    },
                    {
                        "authorId": "2962874",
                        "name": "Shouxi Luo"
                    },
                    {
                        "authorId": "39301921",
                        "name": "Penglin Dai"
                    },
                    {
                        "authorId": "2149142438",
                        "name": "Ke Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[15] create an unsupervised machine learning task to generate a generic representation vector for time series data and improve the representation quality, portability, and practicability."
            ],
            "citingPaper": {
                "paperId": "c6ac0a507dd4b86429b4767916744a67059394fc",
                "externalIds": {
                    "DBLP": "journals/sensors/WangCF22",
                    "PubMedCentral": "9658096",
                    "DOI": "10.3390/s22218450",
                    "CorpusId": 253326016,
                    "PubMed": "36366148"
                },
                "corpusId": 253326016,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c6ac0a507dd4b86429b4767916744a67059394fc",
                "title": "Synergy Masks of Domain Attribute Model DaBERT: Emotional Tracking on Time-Varying Virtual Space Communication",
                "abstract": "Emotional tracking on time-varying virtual space communication aims to identify sentiments and opinions expressed in a piece of user-generated content. However, the existing research mainly focuses on the user\u2019s single post, despite the fact that social network data are sequential. In this article, we propose a sentiment analysis model based on time series prediction in order to understand and master the chronological evolution of the user\u2019s point of view. Specifically, with the help of a domain-knowledge-enhanced pre-trained encoder, the model embeds tokens for each moment in the text sequence. We then propose an attention-based temporal prediction model to extract rich timing information from historical posting records, which improves the prediction of the user\u2019s current state and personalizes the analysis of user\u2019s sentiment changes in social networks. The experiments show that the proposed model improves on four kinds of sentiment tasks and significantly outperforms the strong baseline.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115739005",
                        "name": "Ye Wang"
                    },
                    {
                        "authorId": "2117202111",
                        "name": "Zhenghan Chen"
                    },
                    {
                        "authorId": "1519525807",
                        "name": "Changzeng Fu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Following the success of unsupervised learning in computer vision [23], [24] and natural language processing [25], different unsupervised learning frameworks for time series data have been proposed that try to make efficient use of large amounts of unlabeled time series data and produce powerful representations [14], [26], [27].",
                "In one of the earliest applications of unsupervised learning [26] propose a scalable representation learning framework for time series by applying a triplet loss to positive samples from a timeseries\u2019 subseries and negative samples from other instances."
            ],
            "citingPaper": {
                "paperId": "8013150e6e57c14cc3c4bc72eb2ecf46d9b4e309",
                "externalIds": {
                    "DBLP": "conf/icdm/ParkKOLK22",
                    "DOI": "10.1109/ICDM54844.2022.00048",
                    "CorpusId": 256462989
                },
                "corpusId": 256462989,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/8013150e6e57c14cc3c4bc72eb2ecf46d9b4e309",
                "title": "A Large-Scale Ensemble Learning Framework for Demand Forecasting",
                "abstract": "Demand forecasting is a crucial component of supply chain management for revenue optimization and inventory planning. Traditional time series forecasting methods, however, have resulted in small models with limited expressive power because they have difficulty in scaling their model size up while maintaining high accuracy. In this paper, we propose Forecasting orchestra (Forchestra), a simple but powerful ensemble framework capable of accurately predicting future demand for a diverse range of items. Forchestra consists of two parts: 1) base predictors and 2) a neural conductor. For a given time series, each base predictor outputs its respective forecast based on historical observations. On top of the base predictors, the neural conductor adaptively assigns the importance weight for each predictor by looking at the representation vector provided by a representation module. Finally, Forchestra aggregates the predictions by the weights and constructs a final prediction. In contrast to previous ensemble approaches, the neural conductor and all base predictors of Forchestra are trained in an end-to-end manner; this allows each base predictor to modify its reaction to different inputs, while supporting other predictors and constructing a final prediction jointly. We empirically show that the model size is scalable to up to 0.8 billion parameters ($\\approx$400-layer LSTM). The proposed method is evaluated on our proprietary E-Commerce (100K) and the public M5(30K) datasets, and it outperforms existing forecasting models with a significant margin. In addition, we observe that our framework generalizes well to unseen data points when evaluated in a zeroshot fashion on downstream datasets. Last but not least, we present extensive qualitative and quantitative studies to analyze how the proposed model outperforms baseline models and differs from conventional ensemble approaches. The code is available at https://github.com/young-j-parld22-ICDM-Forchestra.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115210131",
                        "name": "Young-Jin Park"
                    },
                    {
                        "authorId": "2145183608",
                        "name": "Donghyun Kim"
                    },
                    {
                        "authorId": "2203816721",
                        "name": "Fr\u00e9d\u00e9ric Odermatt"
                    },
                    {
                        "authorId": "2108550899",
                        "name": "Juho Lee"
                    },
                    {
                        "authorId": "2109351321",
                        "name": "KyungHyun Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Extracting features by deep learning techniques has been practiced (Franceschi et al. 2019) and applied to tasks like classification (Ismail Fawaz et al. 2020).",
                "FDJNet (Franceschi et al. 2019) combines an encoder based on causal dilated convolutions with triplet loss to embed variable-length and multivariate time series.",
                "Extracting features by deep learning techniques has been practiced (Franceschi et al. 2019) and applied to tasks like classification (Ismail Fawaz et al."
            ],
            "citingPaper": {
                "paperId": "9016ba4fdad2dc292bc645640693b39ebdacc1e0",
                "externalIds": {
                    "DBLP": "journals/jvis/ZhouJQH23",
                    "DOI": "10.1007/s12650-022-00890-3",
                    "CorpusId": 253231406
                },
                "corpusId": 253231406,
                "publicationVenue": {
                    "id": "c3faa921-3f7d-4435-906f-25cdb7d6a885",
                    "name": "Journal of Vision",
                    "type": "journal",
                    "alternate_names": [
                        "J Vis",
                        "Journal of Visualization"
                    ],
                    "issn": "1534-7362",
                    "alternate_issns": [
                        "1343-8875"
                    ],
                    "url": "http://www.journalofvision.org/4/6/",
                    "alternate_urls": [
                        "https://link.springer.com/journal/12650",
                        "http://journalofvision.org/",
                        "https://www.iospress.nl/html/13438875.php"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9016ba4fdad2dc292bc645640693b39ebdacc1e0",
                "title": "Representation and analysis of time-series data via deep embedding and visual exploration",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110330318",
                        "name": "Yixuan Zhou"
                    },
                    {
                        "authorId": "2189324647",
                        "name": "Runfeng Jiang"
                    },
                    {
                        "authorId": "2369566",
                        "name": "Hongxing Qin"
                    },
                    {
                        "authorId": "2166199319",
                        "name": "Haibo Hu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c9b3c30fdc346cf54d70e5b23e340d6e3ca57dac",
                "externalIds": {
                    "DOI": "10.36001/phmconf.2022.v14i1.3187",
                    "CorpusId": 253342273
                },
                "corpusId": 253342273,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c9b3c30fdc346cf54d70e5b23e340d6e3ca57dac",
                "title": "Partially Supervised Classification for Industrial System using Deep Neural Network",
                "abstract": "A general classification setting requires prior knowledge (i.e. labeled samples) to cover all classes. However, in many industrial problems, prior knowledge usually does not describe all the classes, and the generation of a complete training set that cover all classes often is a time-consuming, expensive and difficult (if not impossible) task. Our target of this work is, given labeled samples from only a subset of classes, how to assign label to any sample that potentially come from either known or unknown classes in real time data. We test our algorithm on industrial failure classification and experiments show that our method outperforms existing popular baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143569237",
                        "name": "Hao Huang"
                    },
                    {
                        "authorId": "2053683874",
                        "name": "Tapan Shah"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "efd8243e417bcbe9eebeb18d622bec3076a5d012",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-13757",
                    "ArXiv": "2210.13757",
                    "DOI": "10.48550/arXiv.2210.13757",
                    "CorpusId": 253107663
                },
                "corpusId": 253107663,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/efd8243e417bcbe9eebeb18d622bec3076a5d012",
                "title": "Workload Similarity Analysis using Machine Learning Techniques",
                "abstract": "\u2014Finding the similarity between two workload be- haviours is helpful in 1. creating proxy workloads 2. char-acterising an unknown workload\u2019s behavior by matching its behavior against known workloads. In this article, we propose a method to measure the similarity between two workloads using machine learning based analysis of the performance telemetry data collected for the execution runs of the two workloads. We also demonstrate the accuracy of the technique by measuring the similarity between a variety of know benchmark workloads.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2188780885",
                        "name": "Deepak Mishra"
                    },
                    {
                        "authorId": "2188814950",
                        "name": "Vineet Singh"
                    },
                    {
                        "authorId": "2188780125",
                        "name": "Ashish Ledalla"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "35dcba87de73476dd87779c0ec884df03aa3ec38",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09817",
                    "ArXiv": "2210.09817",
                    "DOI": "10.48550/arXiv.2210.09817",
                    "CorpusId": 252968231
                },
                "corpusId": 252968231,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/35dcba87de73476dd87779c0ec884df03aa3ec38",
                "title": "Universal hidden monotonic trend estimation with contrastive learning",
                "abstract": "In this paper, we describe a universal method for extracting the underlying monotonic trend factor from time series data. We propose an approach related to the Mann-Kendall test, a standard monotonic trend detection method and call it contrastive trend estimation (CTE). We show that the CTE method identifies any hidden trend underlying temporal data while avoiding the standard assumptions used for monotonic trend identification. In particular, CTE can take any type of temporal data (vector, images, graphs, time series, etc.) as input. We finally illustrate the interest of our CTE method through several experiments on different types of data and problems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50988081",
                        "name": "Edouard Pineau"
                    },
                    {
                        "authorId": "3072865",
                        "name": "S. Razakarivony"
                    },
                    {
                        "authorId": "2148272487",
                        "name": "Mauricio Gonzalez"
                    },
                    {
                        "authorId": "1748843717",
                        "name": "A. Schrapffer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Time-series Meta Features: There are prior works that generated standard time-series features [17], tsfresh [12] (that we used for generating part of our meta-features)."
            ],
            "citingPaper": {
                "paperId": "bc0bd9b325a66dce77e925b62912db7febbc4ca2",
                "externalIds": {
                    "DBLP": "conf/cikm/AbdallahRMKZB22",
                    "DOI": "10.1145/3511808.3557241",
                    "CorpusId": 252587492
                },
                "corpusId": 252587492,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bc0bd9b325a66dce77e925b62912db7febbc4ca2",
                "title": "AutoForecast: Automatic Time-Series Forecasting Model Selection",
                "abstract": "In this work, we develop techniques for fast automatic selection of the best forecasting model for a new unseen time-series dataset, without having to first train (or evaluate) all the models on the new time-series data to select the best one. In particular, we develop a forecasting meta-learning approach called AutoForecast that allows for the quick inference of the best time-series forecasting model for an unseen dataset. Our approach learns both forecasting models performances over time horizon of same dataset and task similarity across different datasets. The experiments demonstrate the effectiveness of the approach over state-of-the-art (SOTA) single and ensemble methods and several SOTA meta-learners (adapted to our problem) in terms of selecting better forecasting models (i.e., 2X gain) for unseen tasks for univariate and multivariate testbeds.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144912195",
                        "name": "Mustafa Abdallah"
                    },
                    {
                        "authorId": "2066337266",
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "authorId": "39015414",
                        "name": "K. Mahadik"
                    },
                    {
                        "authorId": "2187905618",
                        "name": "Sungchul Kim"
                    },
                    {
                        "authorId": "7574699",
                        "name": "Handong Zhao"
                    },
                    {
                        "authorId": "1679009",
                        "name": "S. Bagchi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Unfortunately, they are all designed to model one trajectory independently without considering the spatial distance between objects, leading to the overfitting problem[8, 37]."
            ],
            "citingPaper": {
                "paperId": "1d3771be40e3a0f80c33267fb3908c279969ecd3",
                "externalIds": {
                    "DBLP": "conf/cikm/FengPF0Z022",
                    "DOI": "10.1145/3511808.3557239",
                    "CorpusId": 252904867
                },
                "corpusId": 252904867,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1d3771be40e3a0f80c33267fb3908c279969ecd3",
                "title": "Aries: Accurate Metric-based Representation Learning for Fast Top-k Trajectory Similarity Query",
                "abstract": "With the prevalence of location-based services (LBS), trajectories are being generated rapidly. As is widely used in LBS, top-k trajectory similarity query serves as a key operation, deeply empowering applications such as travel route recommendation and carpooling. Given the rise of deep learning, trajectory representation has been well-proven to speed up this operator. However, existing representation-based computing modes remain two major problems understudied: the low quality of trajectory representation and insufficient support for various trajectory similarity metrics, which make them difficult to apply in practice. Therefore, we propose an Accurate metric-based representation learning approach for fast top-k trajectory similarity query, named Aries. Specifically, Aries has two sophisticated modules: (1) An novel trajectory embedding strategy enhanced by the bidirectional LSTM encoder and spatial attention mechanism, which can extract more precise and comprehensive knowledge. (2) A deep metric learning network aggregating multiple measures for better top-k query. Extensive experiments conducted on real trajectory dataset show that Aries achieves both impressive accuracy and lower training time compared with state-of-the-art solutions. In particular, it achieves 5x-10x speedup and 10%-20% accuracy improvement over Euclidean, Hausdorff, DTW, and EDR measures. Besides, our method can maintain stable performance when handling various scenarios, without repeated training in order to adapt to diverse similarity metrics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2187856868",
                        "name": "Chunhui Feng"
                    },
                    {
                        "authorId": "50209953",
                        "name": "Zhicheng Pan"
                    },
                    {
                        "authorId": "2375706",
                        "name": "Junhua Fang"
                    },
                    {
                        "authorId": "3757313",
                        "name": "Jiajie Xu"
                    },
                    {
                        "authorId": "2927967",
                        "name": "Pengpeng Zhao"
                    },
                    {
                        "authorId": "98756666",
                        "name": "Lei Zhao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The diverse learning tasks that can be defined in a supervised manner directly from the samples only have been proposed and validated their efficacy to learn general-purpose representations [6]."
            ],
            "citingPaper": {
                "paperId": "2fb7202bb9750bcb0752e30889098b4ecb964162",
                "externalIds": {
                    "DBLP": "conf/cikm/KoS22",
                    "DOI": "10.1145/3511808.3557589",
                    "CorpusId": 252904744
                },
                "corpusId": 252904744,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/2fb7202bb9750bcb0752e30889098b4ecb964162",
                "title": "EEG-Oriented Self-Supervised Learning and Cluster-Aware Adaptation",
                "abstract": "Recently, deep learning-based electroencephalogram (EEG) analysis and decoding have gained widespread attention to monitor a user's clinical condition or identify his/her intention/emotion. Nevertheless, the existing methods mostly model EEG signals with limited viewpoints or restricted concerns about the characteristics of the EEG signals, thus suffering from representing complex spatio-spectro-temporal patterns as well as inter-subject variability. In this work, we propose novel EEG-oriented self-supervised learning methods to discover complex and diverse patterns of spatio-spectral characteristics and spatio-temporal dynamics. Combined with the proposed self-supervised representation learning, we also devise a feature normalization strategy to resolve an inter-subject variability problem via clustering. We demonstrated the validity of the proposed framework on three publicly available datasets by comparing with state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47106040",
                        "name": "Wonjun Ko"
                    },
                    {
                        "authorId": "143802908",
                        "name": "Heung-Il Suk"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "applied to human activity recognition (HAR) data [13], electroencephalography (EEG) data [15], or household consumption data [14], among others.",
                ", [14], [15]) motivate to research the applicability of contrastive SSL for vehicle CAN-data.",
                "As contrastive learning methods, we consider Temporal Neighborhood Coding (TNC) [15] and a triplet loss approach (T-Loss) [14].",
                ", HAR data [15], T-Loss for Household Consumption data [14], and an AE in [17] for CAN-data.",
                "Regarding the neural networks, we follow [14] and do not perform any hyperparameter optimization on the network parameters."
            ],
            "citingPaper": {
                "paperId": "bf1b619404132dba0e832ba154a0544591f50976",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-04988",
                    "ArXiv": "2301.04988",
                    "DOI": "10.1109/ITSC55140.2022.9922158",
                    "CorpusId": 253252765
                },
                "corpusId": 253252765,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bf1b619404132dba0e832ba154a0544591f50976",
                "title": "Unsupervised Driving Event Discovery Based on Vehicle CAN-data",
                "abstract": "The data collected from a vehicle's Controller Area Network (CAN) can quickly exceed human analysis or annotation capabilities when considering fleets of vehicles, which stresses the importance of unsupervised machine learning methods. This work presents a simultaneous clustering and segmentation approach for vehicle CAN-data that identifies common driving events in an unsupervised manner. The approach builds on self-supervised learning (SSL) for multivariate time series to distinguish different driving events in the learned latent space. We evaluate our approach with a dataset of real Tesla Model 3 vehicle CAN-data and a two-hour driving session that we annotated with different driving events. With our approach, we evaluate the applicability of recent time series-related contrastive and generative SSL techniques to learn representations that distinguish driving events. Compared to state-of-the-art (SOTA) generative SSL methods for driving event discovery, we find that contrastive learning approaches reach similar performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2189469969",
                        "name": "Thomas Kreutz"
                    },
                    {
                        "authorId": "147733826",
                        "name": "O. Esbel"
                    },
                    {
                        "authorId": "1725964",
                        "name": "M. M\u00fchlh\u00e4user"
                    },
                    {
                        "authorId": "1792693",
                        "name": "Alejandro S\u00e1nchez Guinea"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "In recent years, many deep models have been proposed for temporal modeling, such as MLP, TCN, RNN-based models (Hochreiter & Schmidhuber, 1997; Lai et al., 2018; Franceschi et al., 2019).",
                "to extract the variation information (Franceschi et al., 2019; He & Zhao, 2019).",
                ", 2021) and classification of trajectories for action recognition (Franceschi et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "47696145b3f88c4cc3f3c22035286b5d7ebce09d",
                "externalIds": {
                    "DBLP": "conf/iclr/WuHLZ0L23",
                    "ArXiv": "2210.02186",
                    "DOI": "10.48550/arXiv.2210.02186",
                    "CorpusId": 252715491
                },
                "corpusId": 252715491,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/47696145b3f88c4cc3f3c22035286b5d7ebce09d",
                "title": "TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis",
                "abstract": "Time series analysis is of immense importance in extensive applications, such as weather forecasting, anomaly detection, and action recognition. This paper focuses on temporal variation modeling, which is the common key problem of extensive analysis tasks. Previous methods attempt to accomplish this directly from the 1D time series, which is extremely challenging due to the intricate temporal patterns. Based on the observation of multi-periodicity in time series, we ravel out the complex temporal variations into the multiple intraperiod- and interperiod-variations. To tackle the limitations of 1D time series in representation capability, we extend the analysis of temporal variations into the 2D space by transforming the 1D time series into a set of 2D tensors based on multiple periods. This transformation can embed the intraperiod- and interperiod-variations into the columns and rows of the 2D tensors respectively, making the 2D-variations to be easily modeled by 2D kernels. Technically, we propose the TimesNet with TimesBlock as a task-general backbone for time series analysis. TimesBlock can discover the multi-periodicity adaptively and extract the complex temporal variations from transformed 2D tensors by a parameter-efficient inception block. Our proposed TimesNet achieves consistent state-of-the-art in five mainstream time series analysis tasks, including short- and long-term forecasting, imputation, classification, and anomaly detection. Code is available at this repository: https://github.com/thuml/TimesNet.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2051867856",
                        "name": "Haixu Wu"
                    },
                    {
                        "authorId": "2112912801",
                        "name": "Teng Hu"
                    },
                    {
                        "authorId": "2144386151",
                        "name": "Yong Liu"
                    },
                    {
                        "authorId": "2171667749",
                        "name": "Hang Zhou"
                    },
                    {
                        "authorId": "2144499343",
                        "name": "Jianmin Wang"
                    },
                    {
                        "authorId": "2054275000",
                        "name": "Mingsheng Long"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For representation function, a neural network with exponentially dilated causal convolutions [7] with a depth of 10, 40 channels, and embedding size of 320 is used.",
                "(1) Temporal contrastive loss: Similar to [7], we employ triplet loss as the temporal contrastive loss for the ith sample, which is formulated as"
            ],
            "citingPaper": {
                "paperId": "971af97de097930ed2240df93c0ccd7cf505bac6",
                "externalIds": {
                    "DBLP": "conf/dasc/PanKZW22",
                    "DOI": "10.1109/DASC/PiCom/CBDCom/Cy55231.2022.9927927",
                    "CorpusId": 254639525
                },
                "corpusId": 254639525,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/971af97de097930ed2240df93c0ccd7cf505bac6",
                "title": "Contrastive Representation based Active Learning for Time Series",
                "abstract": "Active Learning designs query strategies to select the most representative samples to be labeled by an oracle in an attempt to maximize the model\u2019s performance while minimizing the labeling workload. We propose REAL, a new pooling-based active learning algorithm for time series data that learns the query strategy and optimizes the representation model in a contrastive manner. To initialize the process, a cluster module is employed to select the first sample set for labeling. Subsequent samples are selected through a contrastive loss function from three complementary perspectives, self-consistency, attraction to similar samples, and repulsion of disparate samples. Concurrently, the contrastive loss is also used to update the representation model. We evaluate our method on various time series classification tasks against state-of-the-art algorithms and demonstrate gains or comparable performance for an equal number of labeled samples.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2288532",
                        "name": "Lujia Pan"
                    },
                    {
                        "authorId": "152238016",
                        "name": "Marcus Kalander"
                    },
                    {
                        "authorId": "2108472459",
                        "name": "Yuchao Zhang"
                    },
                    {
                        "authorId": "2152206172",
                        "name": "Pinghui Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "employ a triplet loss in [3], which strives to ensure that a reference time series has a representation that is close to any one of its subseries (a positive sample) but far from negative series (chosen at random)."
            ],
            "citingPaper": {
                "paperId": "7ec9a05f120c5c8a0beabef617267da1a177d8e6",
                "externalIds": {
                    "ArXiv": "2209.10662",
                    "DBLP": "conf/eusipco/ZhangRVC22",
                    "DOI": "10.48550/arXiv.2209.10662",
                    "CorpusId": 252438887
                },
                "corpusId": 252438887,
                "publicationVenue": {
                    "id": "ebcbaf26-62a6-4cb2-b637-b29091ca04d6",
                    "name": "European Signal Processing Conference",
                    "type": "conference",
                    "alternate_names": [
                        "EUSIPCO",
                        "Eur Signal Process Conf"
                    ],
                    "issn": "2076-1465",
                    "alternate_issns": [
                        "2219-5491"
                    ],
                    "url": "https://www.eurasip.org/index.php?Itemid=89&id=80&option=com_content&view=article",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conhome/1801907/all-proceedings"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7ec9a05f120c5c8a0beabef617267da1a177d8e6",
                "title": "Contrastive Learning for Time Series on Dynamic Graphs",
                "abstract": "There have been several recent efforts towards developing representations for multivariate time-series in an unsupervised learning framework. Such representations can prove beneficial in tasks such as activity recognition, health monitoring, and anomaly detection. In this paper, we consider a setting where we observe time-series at each node in a dynamic graph. We propose a framework called GraphTNC for unsupervised learning of joint representations of the graph and the time-series. Our approach employs a contrastive learning strategy. Based on an assumption that the time-series and graph evolution dynamics are piecewise smooth, we identify local windows of time where the signals exhibit approximate stationarity. We then train an encoding that allows the distribution of signals within a neighborhood to be distinguished from the distribution of non-neighboring signals. We first demonstrate the performance of our proposed framework using synthetic data, and subsequently we show that it can prove beneficial for the classification task with real-world datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108284444",
                        "name": "Yitian Zhang"
                    },
                    {
                        "authorId": "1388386548",
                        "name": "Florence Regol"
                    },
                    {
                        "authorId": "1419478649",
                        "name": "Antonios Valkanas"
                    },
                    {
                        "authorId": "2150349871",
                        "name": "Mark Coates"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[28], for example, proposed measuring the similarity in time for time-series data in an unsupervised setup."
            ],
            "citingPaper": {
                "paperId": "63eac0c86307801703edc7d610e0c9b758fd4860",
                "externalIds": {
                    "ArXiv": "2208.13288",
                    "DBLP": "journals/corr/abs-2208-13288",
                    "MAG": "3203577903",
                    "DOI": "10.3850/978-981-18-2016-8_706-cd",
                    "CorpusId": 244320175
                },
                "corpusId": 244320175,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/63eac0c86307801703edc7d610e0c9b758fd4860",
                "title": "Contrastive Feature Learning for Fault Detection and Diagnostics in Railway Applications",
                "abstract": "A railway is a complex system comprising multiple infrastructure and rolling stock assets. To operate the system safely, reliably, and efficiently, the condition many components needs to be monitored. To automate this process, data-driven fault detection and diagnostics models can be employed. In practice, however, the performance of data-driven models can be compromised if the training dataset is not representative of all possible future conditions. We propose to approach this problem by learning a feature representation that is, on the one hand, invariant to operating or environmental factors but, on the other hand, sensitive to changes in the asset's health condition. We evaluate how contrastive learning can be employed on supervised and unsupervised fault detection and diagnostics tasks given real condition monitoring datasets within a railway system - one image dataset from infrastructure assets and one time-series dataset from rolling stock assets. First, we evaluate the performance of supervised contrastive feature learning on a railway sleeper defect classification task given a labeled image dataset. Second, we evaluate the performance of unsupervised contrastive feature learning without access to faulty samples on an anomaly detection task given a railway wheel dataset. Here, we test the hypothesis of whether a feature encoder's sensitivity to degradation is also sensitive to novel fault patterns in the data. Our results demonstrate that contrastive feature learning improves the performance on the supervised classification task regarding sleepers compared to a state-of-the-art method. Moreover, on the anomaly detection task concerning the railway wheels, the detection of shelling defects is improved compared to state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51132870",
                        "name": "Katharina Rombach"
                    },
                    {
                        "authorId": "3393868",
                        "name": "Gabriel Michau"
                    },
                    {
                        "authorId": "2183214873",
                        "name": "Kajan Ratnasabapathy"
                    },
                    {
                        "authorId": "103196033",
                        "name": "L. Ancu"
                    },
                    {
                        "authorId": "2183604490",
                        "name": "Wilfried B\u00fcrzle"
                    },
                    {
                        "authorId": "2067087623",
                        "name": "Stefan Koller"
                    },
                    {
                        "authorId": "2757308",
                        "name": "Olga Fink"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[31] also applied contrastive learning successfully to time series data."
            ],
            "citingPaper": {
                "paperId": "9df2de1856a165a16b657309a6359e7baf632202",
                "externalIds": {
                    "DBLP": "journals/sensors/AhmadAK22",
                    "PubMedCentral": "9460177",
                    "DOI": "10.3390/s22176448",
                    "CorpusId": 251968300,
                    "PubMed": "36080907"
                },
                "corpusId": 251968300,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9df2de1856a165a16b657309a6359e7baf632202",
                "title": "A Centrifugal Pump Fault Diagnosis Framework Based on Supervised Contrastive Learning",
                "abstract": "A novel intelligent centrifugal pump (CP) fault diagnosis method is proposed in this paper. The method is based on the contrast in vibration data obtained from a centrifugal pump (CP) under several operating conditions. The vibration signals data obtained from a CP are non-stationary because of the impulses caused by different faults; thus, traditional time domain and frequency domain analyses such as fast Fourier transform and Walsh transform are not the best option to pre-process the non-stationary signals. First, to visualize the fault-related impulses in vibration data, we computed the kurtogram images of time series vibration sequences. To extract the discriminant features related to faults from the kurtogram images, we used a deep learning tool convolutional encoder (CE) with a supervised contrastive loss. The supervised contrastive loss pulls together samples belonging to the same class, while pushing apart samples belonging to a different class. The convolutional encoder was pretrained on the kurtograms with the supervised contrastive loss to infer the contrasting features belonging to different CP data classes. After pretraining with the supervised contrastive loss, the learned representations of the convolutional encoder were kept as obtained, and a linear classifier was trained above the frozen convolutional encoder, which completed the fault identification. The proposed model was validated with data collected from a real industrial testbed, yielding a high classification accuracy of 99.1% and an error of less than 1%. Furthermore, to prove the proposed model robust, it was validated on CP data with 3.0 and 3.5 bar inlet pressure.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151676245",
                        "name": "Sajjad Ahmad"
                    },
                    {
                        "authorId": "1961263837",
                        "name": "Zahoor Ahmad"
                    },
                    {
                        "authorId": "46453824",
                        "name": "Jong-Myon Kim"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e02f4e0e35358aea8f880e16cebc17d24aa71f00",
                "externalIds": {
                    "DBLP": "conf/case/WangTY22",
                    "DOI": "10.1109/CASE49997.2022.9926721",
                    "CorpusId": 253186817
                },
                "corpusId": 253186817,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e02f4e0e35358aea8f880e16cebc17d24aa71f00",
                "title": "Attention-based Representation Learning for Time Series with Principal and Residual Space Monitoring",
                "abstract": "The encoder-decoder network is one of the most common deep learning models for time series representation learning and anomaly detection. However, it is hard to reconstruct time series, which is complex, correlated, and lacking in common patterns. In this paper, we apply the attention mechanism to rescale convolution layers and learn representation in the principal and the residual space. To avoid the reconstruction process, we define the residual space by the omitted segments according to the attention score in the encoder. We introduce the temporal information inside the token level and use sparse penalty to improve representation learning. We apply the proposed model to anomaly classification and fault detection experiments on two datasets, i.e. multivariate bearing fault dataset and UCRArchive profile dataset. The result shows that the representation learned by the proposed model is more likely to cluster by category, especially in the residual space. Compared to the baselines and state-of-the-art models, the proposed model has higher accuracy and recall in the limited-labeled situation, which illustrates the stability of the learned representation and its superiority in the downstream tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1901691",
                        "name": "Botao Wang"
                    },
                    {
                        "authorId": "2173360",
                        "name": "F. Tsung"
                    },
                    {
                        "authorId": "2152208710",
                        "name": "Hao Yan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "Existing works for multivariate time series clustering (MTC) can be roughly divided into three groups, dimension reduction-based methods [22], [23], classical distance-based methods [24], and deep learning-based methods [25], [26].",
                "Moreover, the deep learning-based method is an important technique for multivariate time series clustering, such as USRL [25] and DeTSEC [26].",
                "We compare MUSLA with ten representative multivariate time series clustering methods: 1) dimension reductionbased methods, including MC2PCA, SWMDFC, and TCK; 2) classical distance-based methods, including m-kAVG +ED, m-kDBA, m-kShape, and m-KSC; 3) deep learningbased methods, such as USRL and DeTSEC; 4) multiview learning methods, such as multi-view spectral clustering via integrating Nonnegative Embedding and Spectral Embedding (NESE) [53].",
                ", USRL) on Epilepsy and PenDigits, where the results of USRL on RI are reported according to the previous work [25].",
                "USRL developed an encoder-only architecture and a triplet loss to train the model, which could admit variable-length inputs and obtain stable and high-quality representation.",
                "3) MUSLA is only inferior to the deep learning-based methods (e.g., USRL) on Epilepsy and PenDigits, where the results of USRL on RI are reported according to the previous work [25].",
                "In addition, the performance of MUSLA only on\nTABLE 3 Performance Comparisons of MUSLA and Contrast Algorithms in Terms of RI\nData sets n Algorithms MC2PCA SWMDFC TCK m-kAVG+ED m-kDBA m-kShape m-kSC USRL DeTSEC NESE MUSLA ArticularyWordRecognition 0.9891 0.8939 0.9734 0.9522 0.9336 0.7582 0.9510 0.9730 0.9718 0.9756 0.9768 AtrialFibrilation 0.5143 0.7429 0.5524 0.7048 0.6857 0.3810 0.6571 0.2000 0.6286 0.6190 0.7238 BasicMotions 0.7910 0.7013 0.8679 0.7718 0.7487 0.5244 0.7718 1.0000 0.7165 0.7449 1.0000 Epilepsy 0.6126 0.6666 0.7856 0.7684 0.7771 0.5136 0.6044 0.9710 0.8397 0.8897 0.8157 ERing 0.7563 0.7724 0.7724 0.8046 0.7747 0.7701 0.7494 0.1330 0.7701 0.7540 0.8414 HandMovementDirection 0.6272 0.6527 0.6353 0.6968 0.6853 0.5728 0.6920 0.3510 0.6275 0.5920 0.7194 Libras 0.8920 0.8611 0.9171 0.9111 0.9133 0.6605 0.9227 0.8830 0.9070 0.9087 0.9412 NATOPS 0.8818 0.7610 0.8334 0.8525 0.8755 0.6534 0.8348 0.9170 0.7143 0.7637 0.9760 PEMS-SF 0.4239 0.7814 0.1909 0.8172 0.7546 0.7303 0.8039 0.6880 0.8058 0.7842 0.8920 PenDigits 0.9288 0.9110 0.9219 0.9345 0.8807 0.8655 0.9208 0.9850 0.8850 0.9064 0.9455 StandWalkJump 0.5905 0.7238 0.7619 0.7333 0.6952 0.3485 0.6571 0.4020 0.7333 0.6476 0.7714 UWaveGestureLibrary 0.8828 0.8246 0.9130 0.9204 0.8934 0.8015 0.9259 0.8840 0.8790 0.8553 0.9129\nArithmetic Mean \" 0.7409 0.7744 0.7604 0.8223 0.8015 0.6316 0.7909 0.6989 0.7899 0.7868 0.8763 Geometric Mean \" 0.7183 0.7702 0.7115 0.8177 0.7966 0.6092 0.7822 0.5883 0.7827 0.7775 0.8709 Absolute Wins \" 1.00 1.00 0.00 0.00 0.00 0.00 1.00 2.50 0.00 0.00 6.50 MUSLA 1-to-1a 11/0/1 11/0/1 11/1/0 11/0/1 12/0/0 12/0/0 11/0/1 9/1/2 11/0/1 11/0/1 - Wilcoxon-Holmb # 6.6667 7.1250 5.1250 3.9167 5.7500 10.2083 5.6250 6.4583 6.5000 6.8333 1.7917 Rank Mean Rank Std # 6.67 2.78 7.17 2.73 5.08 2.47 4.00 1.87 5.75 2.0910.25 1.305.58 2.756.50 3.976.42 2.336.83 2.271.75 1.09 a \u201cMUSLA 1-to-1\u201d indicates the number of MUSLA 1-versus-1 wins/draws/losses. b \u201cWilcoxon-Holm\u201d indicates the Wilcoxon rank test and Holm\u2019s alpha correction."
            ],
            "citingPaper": {
                "paperId": "992c78dc9c6fae25b291f98979ee4e21526bef9d",
                "externalIds": {
                    "DBLP": "journals/pami/ZhangS23",
                    "DOI": "10.1109/TPAMI.2022.3198411",
                    "CorpusId": 251565359,
                    "PubMed": "35969573"
                },
                "corpusId": 251565359,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/992c78dc9c6fae25b291f98979ee4e21526bef9d",
                "title": "Multiview Unsupervised Shapelet Learning for Multivariate Time Series Clustering",
                "abstract": "Multivariate time series clustering has become an important research topic in the time series learning task, which aims to discover the correlation among multiple sequences and partition multivariate time series data into several subsets. Although there are currently some methods that can handle this task, most of them fail to discover informative subsequences from multivariate time series instances. In this paper, we first propose a novel unsupervised shapelet learning with adaptive neighbors (USLA) model for learning salient multivariate subsequences (i.e., multivariate shapelets), where the importance of each variate can be auto-determined when given a candidate multivariate shapelet. USLA performs multivariate shapelet-transformed representation learning and local structure learning simultaneously, but the performance of USLA with multivariate shapelets of different lengths is comparable to that of isometric multivariate shapelets. In fact, the shapelet-transformed representations learned from multivariate shapelets of different lengths can all represent multivariate time series instances separately and often contain complementary information to each other. Therefore, we develop a novel multiview USLA (MUSLA) model which treats shapelet-transformed representations learned from shapelets of different lengths as different views. In this way, MUSLA learns the importance of each view and the neighbor graph matrix among multiview representations when candidate multivariate shapelets of different lengths are determined. Experimental results show that MUSLA outperforms other state-of-the-art multivariate time series algorithms on real-world multivariate time series datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "36856144",
                        "name": "N. Zhang"
                    },
                    {
                        "authorId": "32484584",
                        "name": "S. Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "As for the patient-level epileptic wave detection task, we use the following multivariate time series classification models as baselines: EEGNet [23], TapNet [40], MLSTM-FCN [20] and NS [15]."
            ],
            "citingPaper": {
                "paperId": "f9a03b6cb8b7570caf7ac80c0e58cc2c0eb59bcd",
                "externalIds": {
                    "ArXiv": "2306.13101",
                    "DBLP": "conf/kdd/Chen0YFMY22",
                    "DOI": "10.1145/3534678.3539178",
                    "CorpusId": 249917123
                },
                "corpusId": 249917123,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/f9a03b6cb8b7570caf7ac80c0e58cc2c0eb59bcd",
                "title": "BrainNet: Epileptic Wave Detection from SEEG with Hierarchical Graph Diffusion Learning",
                "abstract": "Epilepsy is one of the most serious neurological diseases, affecting 1-2% of the world's population. The diagnosis of epilepsy depends heavily on the recognition of epileptic waves, i.e., disordered electrical brainwave activity in the patient's brain. Existing works have begun to employ machine learning models to detect epileptic waves via cortical electroencephalogram (EEG), which refers to brain data obtained from a noninvasive examination performed on the patient's scalp surface to record electrical activity in the brain. However, the recently developed stereoelectrocorticography (SEEG) method provides information in stereo that is more precise than conventional EEG, and has been broadly applied in clinical practice. Therefore, in this paper, we propose the first data-driven study to detect epileptic waves in a real-world SEEG dataset. While offering new opportunities, SEEG also poses several challenges. In clinical practice, epileptic wave activities are considered to propagate between different regions in the brain. These propagation paths, also known as the epileptogenic network, are deemed to be a key factor in the context of epilepsy surgery. However, the question of how to extract an exact epileptogenic network for each patient remains an open problem in the field of neuroscience. Moreover, the nature of epileptic waves and SEEG data inevitably leads to extremely imbalanced labels and severe noise. To address these challenges, we propose a novel model (BrainNet) that jointly learns the dynamic diffusion graphs and models the brain wave diffusion patterns. In addition, our model effectively aids in resisting label imbalance and severe noise by employing several self-supervised learning tasks and a hierarchical framework. By experimenting with the extensive real SEEG dataset obtained from multiple patients, we find that BrainNet outperforms several latest state-of-the-art baselines derived from time-series analysis.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118057152",
                        "name": "Junru Chen"
                    },
                    {
                        "authorId": "2172713048",
                        "name": "Yang Yang"
                    },
                    {
                        "authorId": "48881008",
                        "name": "Tao Yu"
                    },
                    {
                        "authorId": "2181309072",
                        "name": "Yingying Fan"
                    },
                    {
                        "authorId": "2181270678",
                        "name": "Xiaolong Mo"
                    },
                    {
                        "authorId": "2695365",
                        "name": "Carl Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Unsupervised representation learning for time-series uses triplet loss with negative sampling [14], hierarchical contrastive loss [36], temporal and contextual contrasting [11], local smoothness to define neighborhoods in time [28], and reprogramming acoustic models [32].",
                "(3) Negative samples (NS) [14] generates negative samples and trains a dilated causal convolution encoder with triplet loss."
            ],
            "citingPaper": {
                "paperId": "3848daf38c39983650a3f6ecb4ccd11dbfab757a",
                "externalIds": {
                    "DBLP": "conf/kdd/Chowdhury0S0H22",
                    "DOI": "10.1145/3534678.3539329",
                    "CorpusId": 251518236
                },
                "corpusId": 251518236,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/3848daf38c39983650a3f6ecb4ccd11dbfab757a",
                "title": "TARNet: Task-Aware Reconstruction for Time-Series Transformer",
                "abstract": "Time-series data contains temporal order information that can guide representation learning for predictive end tasks (e.g., classification, regression). Recently, there are some attempts to leverage such order information to first pre-train time-series models by reconstructing time-series values of randomly masked time segments, followed by an end-task fine-tuning on the same dataset, demonstrating improved end-task performance. However, this learning paradigm decouples data reconstruction from the end task. We argue that the representations learnt in this way are not informed by the end task and may, therefore, be sub-optimal for the end-task performance. In fact, the importance of different timestamps can vary significantly in different end tasks. We believe that representations learnt by reconstructing important timestamps would be a better strategy for improving end-task performance. In this work, we propose TARNet, Task-Aware Reconstruction Network, a new model using Transformers to learn task-aware data reconstruction that augments end-task performance. Specifically, we design a data-driven masking strategy that uses self-attention score distribution from end-task training to sample timestamps deemed important by the end task. Then, we mask out data at those timestamps and reconstruct them, thereby making the reconstruction task-aware. This reconstruction task is trained alternately with the end task at every epoch, sharing parameters in a single model, allowing the representation learnt through reconstruction to improve end-task performance. Extensive experiments on tens of classification and regression datasets show that TARNet significantly outperforms state-of-the-art baseline models across all evaluation metrics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "134629191",
                        "name": "Ranak Roy Chowdhury"
                    },
                    {
                        "authorId": "2108217022",
                        "name": "Xiyuan Zhang"
                    },
                    {
                        "authorId": "2884976",
                        "name": "Jingbo Shang"
                    },
                    {
                        "authorId": "2110343779",
                        "name": "Rajesh K. Gupta"
                    },
                    {
                        "authorId": "2505157",
                        "name": "Dezhi Hong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "First, unlike images, where its features are mostly spatial, we find time-series data are mainly characterised by the temporal dependencies [8]."
            ],
            "citingPaper": {
                "paperId": "2d914e3d648e63fd32b42befe4ced8756b7407b9",
                "externalIds": {
                    "ArXiv": "2208.06616",
                    "DBLP": "journals/corr/abs-2208-06616",
                    "DOI": "10.1109/TPAMI.2023.3308189",
                    "CorpusId": 251564369,
                    "PubMed": "37639415"
                },
                "corpusId": 251564369,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2d914e3d648e63fd32b42befe4ced8756b7407b9",
                "title": "Self-supervised Contrastive Representation Learning for Semi-supervised Time-Series Classification",
                "abstract": "Learning time-series representations when only unlabeled data or few labeled samples are available can be a challenging task. Recently, contrastive self-supervised learning has shown great improvement in extracting useful representations from unlabeled data via contrasting different augmented views of data. In this work, we propose a novel Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC) that learns representations from unlabeled data with contrastive learning. Specifically, we propose time-series-specific weak and strong augmentations and use their views to learn robust temporal relations in the proposed temporal contrasting module, besides learning discriminative representations by our proposed contextual contrasting module. Additionally, we conduct a systematic study of time-series data augmentation selection, which is a key part of contrastive learning. We also extend TS-TCC to the semi-supervised learning settings and propose a Class-Aware TS-TCC (CA-TCC) that benefits from the available few labeled data to further improve representations learned by TS-TCC. Specifically, we leverage the robust pseudo labels produced by TS-TCC to realize a class-aware contrastive loss. Extensive experiments show that the linear evaluation of the features learned by our proposed framework performs comparably with the fully supervised training. Additionally, our framework shows high efficiency in few labeled data and transfer learning scenarios. The code is publicly available at https://github.com/emadeldeen24/CA-TCC.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2086836960",
                        "name": "Emadeldeen Eldele"
                    },
                    {
                        "authorId": "122101015",
                        "name": "Mohamed Ragab"
                    },
                    {
                        "authorId": "48354147",
                        "name": "Zhenghua Chen"
                    },
                    {
                        "authorId": "1390606776",
                        "name": "Min Wu"
                    },
                    {
                        "authorId": "145367091",
                        "name": "C. Kwoh"
                    },
                    {
                        "authorId": "2108674591",
                        "name": "Xiaoli Li"
                    },
                    {
                        "authorId": "2081050342",
                        "name": "Cuntai Guan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Contrastive self-supervised learning techniques have been successfully applied as pretraining tasks in numerous deep learning field, from speech [15], time series [16], [17], structured language models [18]"
            ],
            "citingPaper": {
                "paperId": "3cba9690c2d70143e7ef62fa32eea2ed86d98bae",
                "externalIds": {
                    "DBLP": "journals/tits/KubinBASTS22",
                    "DOI": "10.1109/tits.2021.3114816",
                    "CorpusId": 243368834
                },
                "corpusId": 243368834,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3cba9690c2d70143e7ef62fa32eea2ed86d98bae",
                "title": "Deep Crash Detection From Vehicular Sensor Data With Multimodal Self-Supervision",
                "abstract": "The ability to detect vehicle accidents from on-board sensor data is of the utmost importance to provide prompt assistance to prevent injuries and fatalities. In this article, we present a novel deep learning method capable of analyzing time series recorded from Inertial Measurement Units (IMU) and GPS devices to recognize the presence of an accident along with its severity. We propose a neural architecture capable of exploiting the different sensor streams (i.e., acceleration, gyroscope, and GPS speed), a multimodal contrastive self-supervised training procedure, and an ad-hoc stack of data augmentation techniques, specifically designed to counteract the extreme class imbalance and to improve the generalization capabilities of the whole pipeline. The proposed method has been validated against several state-of-the-art methods on a large and highly imbalanced dataset, composed of more than 200 thousand time series collected from US vehicles, with different vehicle sizes and traveling on different types of road. Our method achieves an average-precision score (AP) of 0.9 in the detection of crashes and 0.76 in the detection of severe crashes, significantly outperforming all the other approaches, and has small footprint and latency, so that it can easily be deployed on embedded devices.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2042693230",
                        "name": "Luca Kubin"
                    },
                    {
                        "authorId": "2093102",
                        "name": "Tommaso Bianconcini"
                    },
                    {
                        "authorId": "2124744013",
                        "name": "Douglas Coimbra de Andrade"
                    },
                    {
                        "authorId": "2533756",
                        "name": "Matteo Simoncini"
                    },
                    {
                        "authorId": "2616063",
                        "name": "L. Taccari"
                    },
                    {
                        "authorId": "1443777687",
                        "name": "Francesco Sambo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Authors of [24] investigate representation learning on multivariate time-series data [24], However, their method depends on triplet losse and require explicit mining of negative pairs to train the model, which is quite challenging in practice.",
                "Contrastive learning methods mostly have been proposed for single modality data across a variety of applications including computer vision [10, 12, 32, 37], audio processing [32, 62, 90], natural language processing [23, 28], sensor data analytics [13, 18, 22, 24, 47, 64, 65, 78]."
            ],
            "citingPaper": {
                "paperId": "569ba1ef5db91bc5ecb49c8beb2a14633f4e3159",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-00467",
                    "ArXiv": "2208.00467",
                    "DOI": "10.1145/3550316",
                    "CorpusId": 251224140
                },
                "corpusId": 251224140,
                "publicationVenue": {
                    "id": "4c51a870-1809-485b-8c20-3c1326b3fe16",
                    "name": "Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies",
                    "alternate_names": [
                        "Proc ACM Interact Mob Wearable Ubiquitous Technol"
                    ],
                    "issn": "2474-9567",
                    "url": "https://dl.acm.org/journal/imwut",
                    "alternate_urls": [
                        "http://imwut.acm.org/",
                        "https://dl.acm.org/pub.cfm?id=J1566"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/569ba1ef5db91bc5ecb49c8beb2a14633f4e3159",
                "title": "COCOA",
                "abstract": "Self-Supervised Learning (SSL) is a new paradigm for learning discriminative representations without labeled data, and has reached comparable or even state-of-the-art results in comparison to supervised counterparts. Contrastive Learning (CL) is one of the most well-known approaches in SSL that attempts to learn general, informative representations of data. CL methods have been mostly developed for applications in computer vision and natural language processing where only a single sensor modality is used. A majority of pervasive computing applications, however, exploit data from a range of different sensor modalities. While existing CL methods are limited to learning from one or two data sources, we propose COCOA (Cross mOdality COntrastive leArning), a self-supervised model that employs a novel objective function to learn quality representations from multisensor data by computing the cross-correlation between different data modalities and minimizing the similarity between irrelevant instances. We evaluate the effectiveness of COCOA against eight recently introduced state-of-the-art self-supervised models, and two supervised baselines across five public datasets. We show that COCOA achieves superior classification performance to all other approaches. Also, COCOA is far more label-efficient than the other baselines including the fully supervised model using only one-tenth of available labeled data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1864633127",
                        "name": "Shohreh Deldari"
                    },
                    {
                        "authorId": "2099127073",
                        "name": "Hao Xue"
                    },
                    {
                        "authorId": "9261711",
                        "name": "Aaqib Saeed"
                    },
                    {
                        "authorId": "2143623845",
                        "name": "Daniel V. Smith"
                    },
                    {
                        "authorId": "144954586",
                        "name": "Flora D. Salim"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Baselines we select contain end-to-end models, contrastive learning based models (CoST [18], TS2Vec [22], TNC [24], MoCo [51], Triplet [52], CPC [53], TST [54], TCC [55]) and a feature engineered model (TSFresh package).",
                "The first stage of model is designed to map input sequences into a latent space and extract representations of the input sequences through contrastive loss function [14], [18], [24], [52].",
                "Starting from RNN [3], [4], [5], [24], [57], [68], [69], popular networks which are successful in other research fields are successively applied to time series forecasting, like CNN [1], [2], [18], [22], [52], GNN [9], [10], [70] and Transformer [6], [7], [31], [36], [37], [41], [71]."
            ],
            "citingPaper": {
                "paperId": "b2324bc58d1c86ec1ba3e235ef6120d2d84f9fc0",
                "externalIds": {
                    "ArXiv": "2207.10941",
                    "DBLP": "journals/corr/abs-2207-10941",
                    "DOI": "10.48550/arXiv.2207.10941",
                    "CorpusId": 251018689
                },
                "corpusId": 251018689,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b2324bc58d1c86ec1ba3e235ef6120d2d84f9fc0",
                "title": "Respecting Time Series Properties Makes Deep Time Series Forecasting Perfect",
                "abstract": "How to handle time features shall be the core question of any time series forecasting model. Ironically, it is often ignored or misunderstood by deep-learning based models, even those baselines which are state-of-the-art. This behavior makes their inefficient, untenable and unstable. In this paper, we rigorously analyze three prevalent but deficient/unfounded deep time series forecasting mechanisms or methods from the view of time series properties, including normalization methods, multivariate forecasting and input sequence length. Corresponding corollaries and solutions are given on both empirical and theoretical basis. We thereby propose a novel time series forecasting network, i.e. RTNet, on the basis of aforementioned analysis. It is general enough to be combined with both supervised and self-supervised forecasting format. Thanks to the core idea of respecting time series properties, no matter in which forecasting format, RTNet shows obviously superior forecasting performances compared with dozens of other SOTA time series forecasting baselines in three real-world benchmark datasets. By and large, it even occupies less time complexity and memory usage while acquiring better forecasting accuracy. The source code is available at https://github.com/OrigamiSL/RTNet.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144035136",
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "19208107",
                        "name": "Yuning Wei"
                    },
                    {
                        "authorId": "30551358",
                        "name": "Yangzhu Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A wide range of time-series contrastive learing methods crudely choose positive samples from temporal neighbors of the given anchor sample, which includes some false positive samples when temporally neighboring samples are of different semantic information [12], [26], [27]."
            ],
            "citingPaper": {
                "paperId": "f823be224a3293197237b286950bad7335e56fb5",
                "externalIds": {
                    "DBLP": "conf/ijcnn/ZhangWXDGL22",
                    "DOI": "10.1109/IJCNN55064.2022.9892840",
                    "CorpusId": 252626325
                },
                "corpusId": 252626325,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/f823be224a3293197237b286950bad7335e56fb5",
                "title": "Expert Knowledge Inspired Contrastive Learning for Sleep Staging",
                "abstract": "Although supervised deep learning methods achieve favorable performance in automatic sleep staging, they are limited in clinical situations due to the heavy reliance on massive labeled multi-channel polysomnogram (PSG) recordings. Accordingly, to alleviate the reliance on labeled PSG, we present SleepECL, a self-supervised learning framework based on electroencephalogram (EEG) signals taking advantage of contrastive learning, which learns efficient representations by contrasting semantically consistent and inconsistent samples (a.k.a. positive samples and negative samples). Specifically, SleepECL conducts contrastive learning upon local representations (i.e., intra-epoch EEG decoding) as well as contextual representations (i.e., interepoch dependency) and incorporates sleep expert knowledge to discover more accurate positive samples in contrastive learning, leading to more effective representations. Experimental results on two publicly available datasets demonstrate that our SleepECL outperforms state-of-the-art self-supervised methods. Moreover, the pre-trained model achieves acceptable performance using only a few label single-channel EEG recordings, which contributes to a more convenient application of automatic sleep staging in clinical situations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108699823",
                        "name": "Hongjun Zhang"
                    },
                    {
                        "authorId": "2152438043",
                        "name": "Jing Wang"
                    },
                    {
                        "authorId": "2186560990",
                        "name": "Jiahong Xiong"
                    },
                    {
                        "authorId": "2186624462",
                        "name": "Yuxuan Ding"
                    },
                    {
                        "authorId": "2186559156",
                        "name": "Zhenliang Gan"
                    },
                    {
                        "authorId": "2624174",
                        "name": "Youfang Lin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Multiple pretext losses exist in the literature, such as the reconstruction loss for autoencoders [6] or its variation for the denoising autoencoders, or the triplet loss proposed for times series in [7].",
                "8 pretext losses : the classical reconstruction loss (rec) - the ELBO loss for Variational AutoEncoders (vae) - the triplet loss [7] with K equals to 1,2,5, 10 and combined (tripletKxxx)- a joint reconstruction loss [9] (multi rec)."
            ],
            "citingPaper": {
                "paperId": "3709e0c1fb6c7c07bdae1bac5eaf517c09a58e27",
                "externalIds": {
                    "DBLP": "conf/igarss/LafabreguePWF22",
                    "DOI": "10.1109/IGARSS46834.2022.9884322",
                    "CorpusId": 252590205
                },
                "corpusId": 252590205,
                "publicationVenue": {
                    "id": "a47b9394-c5c7-4bc8-b8fc-b08f96954278",
                    "name": "IEEE International Geoscience and Remote Sensing Symposium",
                    "type": "conference",
                    "alternate_names": [
                        "Int Geosci Remote Sens Symp",
                        "IGARSS",
                        "International Geoscience and Remote Sensing Symposium",
                        "IEEE Int Geosci Remote Sens Symp"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000307/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/3709e0c1fb6c7c07bdae1bac5eaf517c09a58e27",
                "title": "Deep Clustering Methods Study Applied to Satellite Images Time Series",
                "abstract": "Clustering is an essential tool for data analysis and visualization. It is particularly useful in case of a lack of labels, which prevent the use of supervised methods. The analysis of satellite images is particularly prone to this problem, especially when studied as time series, because the access to this type of data is still recent. Among all clustering methods, the ones based on Deep Neural Networks (DNNs) have seen an increasing interest lately, but only a few works have been conducted on time series yet. This paper aims to give more insight on how current clustering methods based on DNNs can be applied to Satellite Images Time Series (SITS) and it shows that with a proper configuration they can perform better compared to classical non-deep methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51289468",
                        "name": "Baptiste Lafabregue"
                    },
                    {
                        "authorId": "2050456759",
                        "name": "A. Puissant"
                    },
                    {
                        "authorId": "152947675",
                        "name": "J. Weber"
                    },
                    {
                        "authorId": "2318564",
                        "name": "G. Forestier"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "632841ebaf485cd55e225ce8fb7e03fae6dedd3a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-04812",
                    "ArXiv": "2207.04812",
                    "DOI": "10.48550/arXiv.2207.04812",
                    "CorpusId": 250426684,
                    "PubMed": "37207397"
                },
                "corpusId": 250426684,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/632841ebaf485cd55e225ce8fb7e03fae6dedd3a",
                "title": "A clinically motivated self-supervised approach for content-based image retrieval of CT liver images",
                "abstract": "Deep learning-based approaches for content-based image retrieval (CBIR) of computed tomography (CT) liver images is an active field of research, but suffer from some critical limitations. First, they are heavily reliant on labeled data, which can be challenging and costly to acquire. Second, they lack transparency and explainability, which limits the trustworthiness of deep CBIR systems. We address these limitations by: (1) Proposing a self-supervised learning framework that incorporates domain-knowledge into the training procedure, and, (2) by providing the first representation learning explainability analysis in the context of CBIR of CT liver images. Results demonstrate improved performance compared to the standard self-supervised approach across several metrics, as well as improved generalization across datasets. Further, we conduct the first representation learning explainability analysis in the context of CBIR, which reveals new insights into the feature extraction process. Lastly, we perform a case study with cross-examination CBIR that demonstrates the usability of our proposed framework. We believe that our proposed framework could play a vital role in creating trustworthy deep CBIR systems that can successfully take advantage of unlabeled data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1387887318",
                        "name": "Kristoffer Wickstr\u00f8m"
                    },
                    {
                        "authorId": "2175653689",
                        "name": "Eirik Agnalt Ostmo"
                    },
                    {
                        "authorId": "2175653015",
                        "name": "Keyur Radiya"
                    },
                    {
                        "authorId": "1700727",
                        "name": "Karl \u00d8yvind Mikalsen"
                    },
                    {
                        "authorId": "8199702",
                        "name": "Michael C. Kampffmeyer"
                    },
                    {
                        "authorId": "1747567",
                        "name": "R. Jenssen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "SSL has been widely exploited in different domains, including computer vision [21\u201324], audio/speech processing [25], and time-series analysis [26,27]."
            ],
            "citingPaper": {
                "paperId": "aa8a1ee5324e1e87e6203523915b3d66356cb324",
                "externalIds": {
                    "DOI": "10.3390/electronics11142146",
                    "CorpusId": 250403583
                },
                "corpusId": 250403583,
                "publicationVenue": {
                    "id": "ccd8e532-73c6-414f-bc91-271bbb2933e2",
                    "name": "Electronics",
                    "type": "journal",
                    "issn": "1450-5843",
                    "alternate_issns": [
                        "2079-9292",
                        "0883-4989"
                    ],
                    "url": "http://www.electronics.etfbl.net/",
                    "alternate_urls": [
                        "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-247562",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-247562",
                        "https://www.mdpi.com/journal/electronics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aa8a1ee5324e1e87e6203523915b3d66356cb324",
                "title": "Self-Supervised Learning for Time-Series Anomaly Detection in Industrial Internet of Things",
                "abstract": "Industrial sensors have presently emerged as a very important device for monitoring environmental conditions in the manufacturing system. However, abnormal behavior of these smart sensors may cause some failure or potential risk during system operation, thereby increasing the high availability of the entire manufacturing process. An anomaly detection tool in industrial monitoring system must detect any abnormal behavior in advance. Recently, self-supervised learning demonstrated comparable performance with other methods while eliminating manually labeled processes in training. Moreover, this technique decreases the complexity of the training model in lightweight devices to increase the processing time and detect accurately the health of equipment assets. Therefore, this paper proposes an anomaly detection method using a self-supervised learning framework in a time-series dataset to improve the model performance in terms of high accuracy and lightweight method. With the consideration of time-series data augmentation for generating pseudo-label, a classifier using one-dimension convolutional neural network (1DCNN) is applied to learn the characteristics of normal data. This classification model output will effectively measure the degree of abnormality. The experimental results indicate that our proposed method outperforms classic anomaly detection methods. Furthermore, the model deployment in a real testbed is performed to illustrate the efficiency of the self-supervised learning method for time-series anomaly detection.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "146557894",
                        "name": "Duc Hoang Tran"
                    },
                    {
                        "authorId": "2175545673",
                        "name": "Van Linh Nguyen"
                    },
                    {
                        "authorId": "146917845",
                        "name": "Huy Nguyen"
                    },
                    {
                        "authorId": "1816029",
                        "name": "Y. Jang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Moreover, there is convincing evidence that dilated convolutional operations can further enhance the performance of sequence modeling such as forecasting, generation, and representation learning, even outperforming sequence-tosequence models [21, 22, 23]."
            ],
            "citingPaper": {
                "paperId": "04d0fa75c9971dcf4d35af10168f7ae20637fcdf",
                "externalIds": {
                    "DBLP": "conf/icufn/ParkPK22",
                    "DOI": "10.1109/icufn55119.2022.9829692",
                    "CorpusId": 250932458
                },
                "corpusId": 250932458,
                "publicationVenue": {
                    "id": "5ba115b2-444a-476a-ae16-66bf1b83372f",
                    "name": "International Conference on Ubiquitous and Future Networks",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Ubiquitous Future Netw",
                        "ICUFN"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/04d0fa75c9971dcf4d35af10168f7ae20637fcdf",
                "title": "TCAE: Temporal Convolutional Autoencoders for Time Series Anomaly Detection",
                "abstract": "Prevalent recurrent autoencoders for time series anomaly detection often fail to model time series since they have information bottlenecks from the fixed-length latent vectors. In this paper, we propose a conceptually simple yet experimentally effective time series anomaly detection framework called temporal convolutional autoencoder (TCAE). Our model imposes dilated causal convolutional neural networks to capture temporal features while avoiding inefficient recurrent models. Also, we utilize bypassing residual connections in encoded vectors to enhance the temporal features and train the entire model efficiently. Extensive evaluation on several real-world datasets demonstrates that the proposed method outperforms strong anomaly detection baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109202360",
                        "name": "Jinuk Park"
                    },
                    {
                        "authorId": "2145790295",
                        "name": "Yong-Suk Park"
                    },
                    {
                        "authorId": "2178449221",
                        "name": "Chang-Il Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2022 The SSL-based methods include SleepDPC [Xiao et al., 2021], CoSleep [Ye et al., 2021], Time-Series representation learning via Temporal and Contextual Contrasting (TS-TCC) [Eldele et al., 2021b], Relative Positioning (RP) [Banville et al., 2021], Temporal Shuffling (TS) [Banville et al., 2021], Momentum Contrast (MoCo) [He et al., 2020], Contrastive Predictive Coding (CPC) [Oord et al., 2018], Dense Predictive Coding (DPC) [Han et al., 2019] and Triplet Loss [Franceschi et al., 2019].",
                "\u2026Relative Positioning (RP) [Banville et al., 2021], Temporal Shuffling (TS) [Banville et al., 2021], Momentum Contrast (MoCo) [He et al., 2020], Contrastive Predictive Coding (CPC) [Oord et al., 2018], Dense Predictive Coding (DPC) [Han et al., 2019] and Triplet Loss [Franceschi et al., 2019]."
            ],
            "citingPaper": {
                "paperId": "9a260cc14669aeb312936b212d45db89952908bc",
                "externalIds": {
                    "DBLP": "conf/ijcai/LeeSC22",
                    "DOI": "10.24963/ijcai.2022/537",
                    "CorpusId": 250632063
                },
                "corpusId": 250632063,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/9a260cc14669aeb312936b212d45db89952908bc",
                "title": "Self-Supervised Learning with Attention-based Latent Signal Augmentation for Sleep Staging with Limited Labeled Data",
                "abstract": "Sleep staging is an important task that enables sleep quality assessment and disorder diagnosis. Due to dependency on manually labeled data, many researches have turned from supervised approaches to self-supervised learning (SSL) for sleep staging. While existing SSL methods have made significant progress in terms of its comparable performance to supervised methods, there are still some limitations. Contrastive learning could potentially lead to false negative pair assignments in sleep signal data. Moreover, existing data augmentation techniques directly modify the original signal data, making it likely to lose important information. To mitigate these issues, we propose Self-Supervised Learning with Attention-aided Positive Pairs (SSLAPP). Instead of the contrastive learning, SSLAPP carefully draws high-quality positive pairs and exploits them in representation learning. Here, we propose attention-based latent signal augmentation, which plays a key role by capturing important features without losing valuable signal information. Experimental results show that our proposed method achieves state-of-the-art performance in sleep stage classification with limited labeled data. The code is available at: https://github.com/DILAB-HYU/SSLAPP",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2048702994",
                        "name": "Harim Lee"
                    },
                    {
                        "authorId": "2176808474",
                        "name": "Eunseon Seong"
                    },
                    {
                        "authorId": "3257314",
                        "name": "Dong-Kyu Chae"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Similar to anomaly detection for time series, representation learning for time series has a rich body of literature, e.g., Franceschi et al. (2019); Zerveas et al. (2021); Lubba et al. (2019); Christ et al. (2017).",
                "Time series representation learning techniques such as (Franceschi et al., 2019) aim to provide generalpurpose representations that are independent of the downstream task.",
                "1 Contrastive Losses and Anomaly Detection Time series representation learning techniques such as (Franceschi et al., 2019) aim to provide generalpurpose representations that are independent of the downstream task.",
                "In particular, we propose a novel adaptation of unsupervised time series representations (Franceschi et al., 2019) yielding environment-invariant embeddings.",
                "While we extend Franceschi et al. (2019) for practical reasons such as speed of experimentation and general robustness, we remark our approach carries over also to other architectures, such as Zerveas et al. (2021).",
                "The basic building block of our embedding network architecture consists of stacked temporal dilated causal convolutions (Bai et al., 2018) following (Franceschi et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "363d88aa10012b4469670a1d7ba7c8602e6b8e61",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-14342",
                    "ArXiv": "2206.14342",
                    "DOI": "10.48550/arXiv.2206.14342",
                    "CorpusId": 250113764
                },
                "corpusId": 250113764,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/363d88aa10012b4469670a1d7ba7c8602e6b8e61",
                "title": "Intrinsic Anomaly Detection for Multi-Variate Time Series",
                "abstract": "We introduce a novel, practically relevant variation of the anomaly detection problem in multi-variate time series: intrinsic anomaly detection. It appears in diverse practical scenarios ranging from DevOps to IoT, where we want to recognize failures of a system that operates under the influence of a surrounding environment. Intrinsic anomalies are changes in the functional dependency structure between time series that represent an environment and time series that represent the internal state of a system that is placed in said environment. We formalize this problem, provide under-studied public and new purpose-built data sets for it, and present methods that handle intrinsic anomaly detection. These address the short-coming of existing anomaly detection methods that cannot differentiate between expected changes in the system's state and unexpected ones, i.e., changes in the system that deviate from the environment's influence. Our most promising approach is fully unsupervised and combines adversarial learning and time series representation learning, thereby addressing problems such as label sparsity and subjectivity, while allowing to navigate and improve notoriously problematic anomaly detection data sets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "29938263",
                        "name": "Stephan Rabanser"
                    },
                    {
                        "authorId": "2166235",
                        "name": "Tim Januschowski"
                    },
                    {
                        "authorId": "4565995",
                        "name": "Kashif Rasul"
                    },
                    {
                        "authorId": "2101840120",
                        "name": "Oliver Borchert"
                    },
                    {
                        "authorId": "4708988",
                        "name": "Richard Kurle"
                    },
                    {
                        "authorId": "2113062",
                        "name": "Jan Gasthaus"
                    },
                    {
                        "authorId": "1405223072",
                        "name": "Michael Bohlke-Schneider"
                    },
                    {
                        "authorId": "1967156",
                        "name": "Nicolas Papernot"
                    },
                    {
                        "authorId": "2067154581",
                        "name": "Valentin Flunkert"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Specifically, the actor \u03c0\u03d5(st) adopts the dilated causal convolutions (Franceschi, Dieuleveut, and Jaggi 2019) as the basic encoder structure to extract latent time series features, and uses a rank embedding table to extract the base model features."
            ],
            "citingPaper": {
                "paperId": "3e72db97e20366bf52bf809b285b3f8746d2d90f",
                "externalIds": {
                    "DBLP": "conf/aaai/FuWB22",
                    "DOI": "10.1609/aaai.v36i6.20618",
                    "CorpusId": 250289706
                },
                "corpusId": 250289706,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3e72db97e20366bf52bf809b285b3f8746d2d90f",
                "title": "Reinforcement Learning Based Dynamic Model Combination for Time Series Forecasting",
                "abstract": "Time series data appears in many real-world fields such as energy, transportation, communication systems. Accurate modelling and forecasting of time series data can be of significant importance to improve the efficiency of these systems. Extensive research efforts have been taken for time series problems. Different types of approaches, including both statistical-based methods and machine learning-based methods, have been investigated. Among these methods, ensemble learning has shown to be effective and robust. However, it is still an open question that how we should determine weights for base models in the ensemble. Sub-optimal weights may prevent the final model from reaching its full potential. To deal with this challenge, we propose a reinforcement learning (RL) based model combination (RLMC) framework for determining model weights in an ensemble for time series forecasting tasks. By formulating model selection as a sequential decision-making problem, RLMC learns a deterministic policy to output dynamic model weights for non-stationary time series data. RLMC further leverages deep learning to learn hidden features from raw time series data to adapt fast to the changing data distribution. Extensive experiments on multiple real-world datasets have been implemented to showcase the effectiveness of the proposed method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118637906",
                        "name": "Yuwei Fu"
                    },
                    {
                        "authorId": "92148538",
                        "name": "Di Wu"
                    },
                    {
                        "authorId": "2500923",
                        "name": "B. Boulet"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More recent works try to combine classical CL approaches with time-series specific training objectives and augmentations such as slicing (Tonekaboni et al., 2020; Franceschi et al., 2019; Zheng et al., 2021), forecasting (Eldele et al., 2021) and neural processes (Kallidromitis et al., 2021).",
                "More recent works try to combine classical CL approaches with time-series specific training objectives and augmentations such as slicing (Tonekaboni et al., 2020; Franceschi et al., 2019; Zheng et al., 2021), forecasting (Eldele et al."
            ],
            "citingPaper": {
                "paperId": "81d465aa946bae514f373757f60f6a070ae011c7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-11517",
                    "ArXiv": "2206.11517",
                    "DOI": "10.48550/arXiv.2206.11517",
                    "CorpusId": 249953735
                },
                "corpusId": 249953735,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/81d465aa946bae514f373757f60f6a070ae011c7",
                "title": "Utilizing Expert Features for Contrastive Learning of Time-Series Representations",
                "abstract": "We present an approach that incorporates expert knowledge for time-series representation learning. Our method employs expert features to replace the commonly used data transformations in previous contrastive learning approaches. We do this since time-series data frequently stems from the industrial or medical field where expert features are often available from domain experts, while transformations are generally elusive for time-series data. We start by proposing two properties that useful time-series representations should fulfill and show that current representation learning approaches do not ensure these properties. We therefore devise ExpCLR, a novel contrastive learning approach built on an objective that utilizes expert features to encourage both properties for the learned representation. Finally, we demonstrate on three real-world time-series datasets that ExpCLR surpasses several state-of-the-art methods for both unsupervised and semi-supervised representation learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40899276",
                        "name": "Manuel Nonnenmacher"
                    },
                    {
                        "authorId": "2172403710",
                        "name": "Lukas Oldenburg"
                    },
                    {
                        "authorId": "2120820891",
                        "name": "Ingo Steinwart"
                    },
                    {
                        "authorId": "8612587",
                        "name": "D. Reeb"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026et al., 2020b; Dwibedi et al., 2021; He et al., 2020), and several of which are tailored to unsupervised representation learning of time series (Franceschi et al., 2019; Y\u00e8che et al., 2021; Yue et al., 2022; Tonekaboni et al., 2021; Kiyasseh et al., 2021; Eldele et al., 2021; Yang & Hong,\u2026",
                "As a result, several methods emerged: Scalable representation learning (SRL) (Franceschi et al., 2019), neighborhood contrastive learning (NCL) (Y\u00e8che et al., 2021), TS2Vec (Yue et al., 2022), and temporal neighborhood coding (TNC) (Tonekaboni et al., 2021) treat the neighboring windows of the time series as positive pairs and use other windows to construct negative pairs.",
                "For this, SRL, NCL, and TS2Vec minimize the triplet loss, contrastive loss, and hierarchical contrastive loss, respectively, while TNC trains a discriminator network to predict neighborhood information.",
                "As a result, several methods emerged: Scalable representation learning (SRL) (Franceschi et al., 2019), neighborhood contrastive learning (NCL) (Y\u00e8che et al.",
                ", 2020), and several of which are tailored to unsupervised representation learning of time series (Franceschi et al., 2019; Y\u00e8che et al., 2021; Yue et al., 2022; Tonekaboni et al., 2021; Kiyasseh et al., 2021; Eldele et al., 2021; Yang & Hong, 2022; Zhang et al., 2022).",
                "As a result, several methods emerged: Scalable representation learning (SRL) (Franceschi et al., 2019), neighborhood contrastive learning (NCL) (Y\u00e8che et al., 2021), TS2Vec (Yue et al., 2022), and temporal neighborhood coding (TNC) (Tonekaboni et al., 2021) treat the neighboring windows of the time\u2026",
                "Another research stream has developed time series methods for transfer learning from the source domain to the target domain (Eldele et al., 2021; Franceschi et al., 2019; Kiyasseh et al., 2021; Tonekaboni et al., 2021; Yang & Hong, 2022; Y\u00e8che et al., 2021; Yue et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "c8cb6aa0a02cc1a94f2065921a9346d72273e590",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-06243",
                    "ArXiv": "2206.06243",
                    "DOI": "10.48550/arXiv.2206.06243",
                    "CorpusId": 249625545
                },
                "corpusId": 249625545,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c8cb6aa0a02cc1a94f2065921a9346d72273e590",
                "title": "Contrastive Learning for Unsupervised Domain Adaptation of Time Series",
                "abstract": "Unsupervised domain adaptation (UDA) aims at learning a machine learning model using a labeled source domain that performs well on a similar yet different, unlabeled target domain. UDA is important in many applications such as medicine, where it is used to adapt risk scores across different patient cohorts. In this paper, we develop a novel framework for UDA of time series data, called CLUDA. Specifically, we propose a contrastive learning framework to learn contextual representations in multivariate time series, so that these preserve label information for the prediction task. In our framework, we further capture the variation in the contextual representations between source and target domain via a custom nearest-neighbor contrastive learning. To the best of our knowledge, ours is the first framework to learn domain-invariant, contextual representation for UDA of time series data. We evaluate our framework using a wide range of time series datasets to demonstrate its effectiveness and show that it achieves state-of-the-art performance for time series UDA.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2048031965",
                        "name": "Yilmazcan Ozyurt"
                    },
                    {
                        "authorId": "3207649",
                        "name": "S. Feuerriegel"
                    },
                    {
                        "authorId": "1776014",
                        "name": "Ce Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Although there are several approaches to SSRL from multivariate sensor data [41, 55], they are not listed here as they do not consider"
            ],
            "citingPaper": {
                "paperId": "55e31baa3ae5f32fb5e695761892319e26dbc639",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-02353",
                    "ArXiv": "2206.02353",
                    "DOI": "10.48550/arXiv.2206.02353",
                    "CorpusId": 249395333
                },
                "corpusId": 249395333,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/55e31baa3ae5f32fb5e695761892319e26dbc639",
                "title": "Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data",
                "abstract": "Recently, Self-Supervised Representation Learning (SSRL) has attracted much attention in the field of computer vision, speech, natural language processing (NLP), and recently, with other types of modalities, including time series from sensors. The popularity of self-supervised learning is driven by the fact that traditional models typically require a huge amount of well-annotated data for training. Acquiring annotated data can be a difficult and costly process. Self-supervised methods have been introduced to improve the efficiency of training data through discriminative pre-training of models using supervisory signals that have been freely obtained from the raw data. Unlike existing reviews of SSRL that have pre-dominately focused upon methods in the fields of CV or NLP for a single modality, we aim to provide the first comprehensive review of multimodal self-supervised learning methods for temporal data. To this end, we 1) provide a comprehensive categorization of existing SSRL methods, 2) introduce a generic pipeline by defining the key components of a SSRL framework, 3) compare existing models in terms of their objective function, network architecture and potential applications, and 4) review existing multimodal techniques in each category and various modalities. Finally, we present existing weaknesses and future opportunities. We believe our work develops a perspective on the requirements of SSRL in domains that utilise multimodal and/or temporal data",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1864633127",
                        "name": "Shohreh Deldari"
                    },
                    {
                        "authorId": "1560895396",
                        "name": "Hao Xue"
                    },
                    {
                        "authorId": "9261711",
                        "name": "Aaqib Saeed"
                    },
                    {
                        "authorId": "150147667",
                        "name": "Jiayuan He"
                    },
                    {
                        "authorId": "2143623845",
                        "name": "Daniel V. Smith"
                    },
                    {
                        "authorId": "144954586",
                        "name": "Flora D. Salim"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Popular methods are normally discriminative approaches that first extract useful temporal representations followed by clustering in the embedding space (Franceschi et al., 2019; Ma et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "ef27b661fae3abedd7172d3be207ad78aa306094",
                "externalIds": {
                    "ArXiv": "2205.14104",
                    "DBLP": "journals/corr/abs-2205-14104",
                    "DOI": "10.48550/arXiv.2205.14104",
                    "CorpusId": 249151924
                },
                "corpusId": 249151924,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ef27b661fae3abedd7172d3be207ad78aa306094",
                "title": "Efficient Forecasting of Large Scale Hierarchical Time Series via Multilevel Clustering",
                "abstract": "We propose a novel approach to the problem of clustering hierarchically aggregated time-series data, which has remained an understudied problem though it has several commercial applications. We first group time series at each aggregated level, while simultaneously leveraging local and global information. The proposed method can cluster hierarchical time series (HTS) with different lengths and structures. For common two-level hierarchies, we employ a combined objective for local and global clustering over spaces of discrete probability measures, using Wasserstein distance coupled with Soft-DTW divergence. For multi-level hierarchies, we present a bottom-up procedure that progressively leverages lower-level information for higher-level clustering. Our final goal is to improve both the accuracy and speed of forecasts for a larger number of HTS needed for a real-world application. To attain this goal, each time series is first assigned the forecast for its cluster representative, which can be considered as a\"shrinkage prior\"for the set of time series it represents. Then this base forecast can be quickly fine-tuned to adjust to the specifics of that time series. We empirically show that our method substantially improves performance in terms of both speed and accuracy for large-scale forecasting tasks involving much HTS.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110566959",
                        "name": "Xing Han"
                    },
                    {
                        "authorId": "32453998",
                        "name": "Tongzheng Ren"
                    },
                    {
                        "authorId": "2219075402",
                        "name": "Jing Hu"
                    },
                    {
                        "authorId": "2068500758",
                        "name": "J. Ghosh"
                    },
                    {
                        "authorId": "3526349",
                        "name": "Nhat Ho"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Contrast-based methods [12]\u2013[14] are the main-stream approach of self-supervised representation learning for time series.",
                "One baseline named Scalable Representation Learning [12] is not included in our results, as it requires a much longer running time and we failed to produce its results in several days.",
                "named Scalable Representation Learning [12] is not included in our results, as it requires a much longer running time and we failed to produce its results in several days.",
                "Unsupervised Scalable Representation Learning [12] introduces a novel unsupervised loss with timebased negative sampling to train a scalable encoder, shaped as a deep convolutional neural network with dilated convolutions [43]."
            ],
            "citingPaper": {
                "paperId": "26645c9dcb5bcc67f6b1a648937677c6331fac32",
                "externalIds": {
                    "ArXiv": "2205.09928",
                    "DOI": "10.1109/TNNLS.2023.3292066",
                    "CorpusId": 259375670,
                    "PubMed": "37478042"
                },
                "corpusId": 259375670,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/26645c9dcb5bcc67f6b1a648937677c6331fac32",
                "title": "Self-Supervised Time Series Representation Learning via Cross Reconstruction Transformer.",
                "abstract": "Since labeled samples are typically scarce in real-world scenarios, self-supervised representation learning in time series is critical. Existing approaches mainly employ the contrastive learning framework, which automatically learns to understand similar and dissimilar data pairs. However, they are constrained by the request for cumbersome sampling policies and prior knowledge of constructing pairs. Also, few works have focused on effectively modeling temporal-spectral correlations to improve the capacity of representations. In this article, we propose the cross reconstruction transformer (CRT) to solve the aforementioned issues. CRT achieves time series representation learning through a cross-domain dropping-reconstruction task. Specifically, we obtain the frequency domain of the time series via the fast Fourier transform (FFT) and randomly drop certain patches in both time and frequency domains. Dropping is employed to maximally preserve the global context while masking leads to the distribution shift. Then a Transformer architecture is utilized to adequately discover the cross-domain correlations between temporal and spectral information through reconstructing data in both domains, which is called Dropped Temporal-Spectral Modeling. To discriminate the representations in global latent space, we propose instance discrimination constraint (IDC) to reduce the mutual information between different time series samples and sharpen the decision boundaries. Additionally, a specified curriculum learning (CL) strategy is employed to improve the robustness during the pretraining phase, which progressively increases the dropping ratio in the training process. We conduct extensive experiments to evaluate the effectiveness of the proposed method on multiple real-world datasets. Results show that CRT consistently achieves the best performance over existing methods by 2%-9%. The code is publicly available at https://github.com/BobZwr/Cross-Reconstruction-Transformer.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "117636048",
                        "name": "Wen-Rang Zhang"
                    },
                    {
                        "authorId": "2155557947",
                        "name": "Ling Yang"
                    },
                    {
                        "authorId": "37953292",
                        "name": "Shijia Geng"
                    },
                    {
                        "authorId": "2145267154",
                        "name": "Shenda Hong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Classical representation learning methods for time series data[1][2], which focuses on learning a function to automatically transform the raw time series into feature representations, has received more attention."
            ],
            "citingPaper": {
                "paperId": "7a9bc29912ba9f0c38ed5884317cdc58d604610e",
                "externalIds": {
                    "DOI": "10.1088/1742-6596/2232/1/012008",
                    "CorpusId": 248744672
                },
                "corpusId": 248744672,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7a9bc29912ba9f0c38ed5884317cdc58d604610e",
                "title": "A multi-level representation learning method for the classification with emerging new classes on power event monitoring data",
                "abstract": "For the classification problem of power event monitoring data, manual rules are mainly used to recognize the known classes of events, and new classes of events cannot be extracted by using existing manual rules. But new classes may emerge with the upgrading of power equipment and traditional representation of this time series data is not proper for the task. To solve this issue, we introduce an effective approach that includes three parts: i) embed the raw object of power event in semantic space ii) learn the representation of power event with multi-level. iii) detect new classes, classify old classes, and update models to classify both new classes and old classes. Experiments on real power event monitoring dataset demonstrate that the proposed method outperforms the state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151989695",
                        "name": "Xiaohui Pan"
                    },
                    {
                        "authorId": "2153629247",
                        "name": "Yi Liu"
                    },
                    {
                        "authorId": "2086783801",
                        "name": "Fan Meng"
                    },
                    {
                        "authorId": "2165041034",
                        "name": "Shuai Xiang"
                    },
                    {
                        "authorId": "2165050138",
                        "name": "Hang Zhou"
                    },
                    {
                        "authorId": "145082678",
                        "name": "G. Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Clustering in latent space using variational autoencoders and contrastive learning [8, 2, 16] will also be evaluated in future work."
            ],
            "citingPaper": {
                "paperId": "85be64e9e165c273df25940e75610991f1df2e44",
                "externalIds": {
                    "DOI": "10.1088/1742-6596/2265/3/032089",
                    "CorpusId": 249308506
                },
                "corpusId": 249308506,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/85be64e9e165c273df25940e75610991f1df2e44",
                "title": "Identifying evolving leading edge erosion by tracking clusters of lift coefficients",
                "abstract": "This work proposes an approach to identify Leading Edge Erosion (LEE) of a wind turbine blade by tracking evolving and emerging clusters of lift coefficients CL time-series signals under uncertain inflow conditions. Most diagnostic techniques today rely on direct visual inspection, image processing, and statistical analysis, e.g. data mining or regression on SCADA output signals. We claim that probabilistic multivariate spatio-temporal techniques could play an eminent role in the diagnostics of LEE specifically leveraging CL time-series signals form multiple sections along the span of the blade. The proposed method extracts clusters\u2019 features based on Variational Bayesian Gaussian Mixture Models (VBGMM) and tracks their spatial and temporal changes, as well as interpret the evolution of the clusters through prior physics-based assumptions. The parameters of the VBGMM are the mean, the eigenvalues and eigenvectors of the covariance matrix, and the angle of orientation of the eigenvectors. We show that the distribution of the CL data may not show statistically separable clusters, however, the parameters of the VBGMM clusters fitted to the CL data, allows to discriminate moving clusters primarily due to varying inflow and operating conditions, versus emerging clusters primarily due to evolving severity of the blade LEE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "33985386",
                        "name": "I. Abdallah"
                    },
                    {
                        "authorId": "2134243541",
                        "name": "G. Duth\u00e9"
                    },
                    {
                        "authorId": "81862050",
                        "name": "S. Barber"
                    },
                    {
                        "authorId": "66773557",
                        "name": "E. Chatzi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4b65c76f7f28e7ba3ea44ebb1b3e4355a148d0de",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-11291",
                    "ArXiv": "2204.11291",
                    "DOI": "10.48550/arXiv.2204.11291",
                    "CorpusId": 248377251
                },
                "corpusId": 248377251,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4b65c76f7f28e7ba3ea44ebb1b3e4355a148d0de",
                "title": "Large Scale Time-Series Representation Learning via Simultaneous Low and High Frequency Feature Bootstrapping",
                "abstract": "Learning representation from unlabeled time series data is a challenging problem. Most existing self-supervised and unsupervised approaches in the time-series domain do not capture low and high-frequency features at the same time. Further, some of these methods employ large scale models like transformers or rely on computationally expensive techniques such as contrastive learning. To tackle these problems, we propose a non-contrastive self-supervised learning approach efficiently captures low and high-frequency time-varying features in a cost-effective manner. Our method takes raw time series data as input and creates two different augmented views for two branches of the model, by randomly sampling the augmentations from same family. Following the terminology of BYOL, the two branches are called online and target network which allows bootstrapping of the latent representation. In contrast to BYOL, where a backbone encoder is followed by multilayer perceptron (MLP) heads, the proposed model contains additional temporal convolutional network (TCN) heads. As the augmented views are passed through large kernel convolution blocks of the encoder, the subsequent combination of MLP and TCN enables an effective representation of low as well as high-frequency time-varying features due to the varying receptive fields. The two modules (MLP and TCN) act in a complementary manner. We train an online network where each module learns to predict the outcome of the respective module of target network branch. To demonstrate the robustness of our model we performed extensive experiments and ablation studies on five real-world time-series datasets. Our method achieved state-of-art performance on all five real-world datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2163451552",
                        "name": "Vandan Gorade"
                    },
                    {
                        "authorId": "2110018045",
                        "name": "Azad Singh"
                    },
                    {
                        "authorId": "2082316146",
                        "name": "Deepak Mishra"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "However, the previous works usually focus on time-wise features and need to continuously obtain the features of several time steps [15,12,34], which makes them difficult to be applied to the novel MTS representation.",
                "(4) TS-TCC+: It applies TaT as the encoder like W2V+ while training with the pretext task of TS-TCC.",
                "W2V and our framework are suitable for both short and long time length datasets, and our performances can significantly surpass W2V.",
                "W2V is a method which pays more attention on the local information, so the causal dilated convolutions which only focuses on previous information is\nmore suitable than W2V+ which encodes the global information.",
                "To name a few, [15] employs the idea of word2vec [23] which regards part of the time series as word, the rest as context, and part of other time series as negative samples for training.",
                "Many research efforts have been devoted to the self-supervised representation learning of time series [15,12,34] and promising results have been achieved.",
                "(2) W2V+: It applies our proposed two tower Transformer-based model as the encoder while training with the pretext task of W2V.",
                "The detailed information and the reason why we choose these methods are as followed: (1) W2V [15]: This method employs the idea of word2vec.",
                "(6) TST+: It applies TaT as the encoder like W2V+ while training with the\npretext task of TST. (7) NVP+CS: To compare with the regression, in our framework we replace Next Trend Prediction with Next Value Predict (NVP) and regard it as a new strong baseline."
            ],
            "citingPaper": {
                "paperId": "95a253865b53d4a44f4e7496e199f52796039128",
                "externalIds": {
                    "DBLP": "conf/dasfaa/ChenZXLX22",
                    "ArXiv": "2203.04298",
                    "DOI": "10.48550/arXiv.2203.04298",
                    "CorpusId": 247318538
                },
                "corpusId": 247318538,
                "publicationVenue": {
                    "id": "8107ca1c-f651-4769-86dc-3d94a7b5ac26",
                    "name": "International Conference on Database Systems for Advanced Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Database Syst Adv Appl",
                        "Database Syst Adv Appl",
                        "Database Systems for Advanced Applications",
                        "DASFAA"
                    ],
                    "url": "http://www.dasfaa.org/"
                },
                "url": "https://www.semanticscholar.org/paper/95a253865b53d4a44f4e7496e199f52796039128",
                "title": "CaSS: A Channel-aware Self-supervised Representation Learning Framework for Multivariate Time Series Classification",
                "abstract": "Self-supervised representation learning of Multivariate Time Series (MTS) is a challenging task and attracts increasing research interests in recent years. Many previous works focus on the pretext task of self-supervised learning and usually neglect the complex problem of MTS encoding, leading to unpromising results. In this paper, we tackle this challenge from two aspects: encoder and pretext task, and propose a unified channel-aware self-supervised learning framework CaSS. Specifically, we first design a new Transformer-based encoder Channel-aware Transformer (CaT) to capture the complex relationships between different time channels of MTS. Second, we combine two novel pretext tasks Next Trend Prediction (NTP) and Contextual Similarity (CS) for the self-supervised representation learning with our proposed encoder. Extensive experiments are conducted on several commonly used benchmark datasets. The experimental results show that our framework achieves new state-of-the-art comparing with previous self-supervised MTS representation learning methods (up to +7.70\\% improvement on LSST dataset) and can be well applied to the downstream MTS classification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109359431",
                        "name": "Yijiang Chen"
                    },
                    {
                        "authorId": "2116447265",
                        "name": "Xiangdong Zhou"
                    },
                    {
                        "authorId": "2099118205",
                        "name": "Zhen Xing"
                    },
                    {
                        "authorId": "2144292765",
                        "name": "Zhidan Liu"
                    },
                    {
                        "authorId": "2153557642",
                        "name": "Minyang Xu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "68b095c7186db7f565d085ea178ac53558e44f7f",
                "externalIds": {
                    "ArXiv": "2203.03991",
                    "DBLP": "journals/corr/abs-2203-03991",
                    "DOI": "10.48550/arXiv.2203.03991",
                    "CorpusId": 247315324
                },
                "corpusId": 247315324,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/68b095c7186db7f565d085ea178ac53558e44f7f",
                "title": "Sparsification and Filtering for Spatial-temporal GNN in Multivariate Time-series",
                "abstract": "We propose an end-to-end architecture for multivariate time-series prediction that integrates a spatial-temporal graph neural network with a matrix filtering module. This module generates filtered (inverse) correlation graphs from multivariate time series before inputting them into a GNN. In contrast with existing sparsification methods adopted in graph neural network, our model explicitly leverage time-series filtering to overcome the low signal-to-noise ratio typical of complex systems data. We present a set of experiments, where we predict future sales from a synthetic time-series sales dataset. The proposed spatial-temporal graph neural network displays superior performances with respect to baseline approaches, with no graphical information, and with fully connected, disconnected graphs and unfiltered graphs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146016329",
                        "name": "Yuanrong Wang"
                    },
                    {
                        "authorId": "8903071",
                        "name": "T. Aste"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bd6e9b0452e27c7fcdc5d9cb3ad45797fc49099e",
                "externalIds": {
                    "DBLP": "journals/eswa/EftimovPCKIKB22",
                    "DOI": "10.1016/j.eswa.2022.116871",
                    "CorpusId": 247555187
                },
                "corpusId": 247555187,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd6e9b0452e27c7fcdc5d9cb3ad45797fc49099e",
                "title": "Less is more: Selecting the right benchmarking set of data for time series classification",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35604507",
                        "name": "T. Eftimov"
                    },
                    {
                        "authorId": "73349536",
                        "name": "G. Petelin"
                    },
                    {
                        "authorId": "2123076219",
                        "name": "Gjorgjina Cenikj"
                    },
                    {
                        "authorId": "151474485",
                        "name": "A. Kostovska"
                    },
                    {
                        "authorId": "35381534",
                        "name": "Gordana Ispirova"
                    },
                    {
                        "authorId": "1866510",
                        "name": "P. Koro\u0161ec"
                    },
                    {
                        "authorId": "150139636",
                        "name": "Jasmin Bogatinovski"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "31b6e52d39713eb310c08baa0dbc73c2fb702d8e",
                "externalIds": {
                    "DBLP": "journals/kbs/YangZC22",
                    "DOI": "10.1016/j.knosys.2022.108606",
                    "CorpusId": 247610202
                },
                "corpusId": 247610202,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31b6e52d39713eb310c08baa0dbc73c2fb702d8e",
                "title": "TimeCLR: A self-supervised contrastive learning framework for univariate time series representation",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2150439354",
                        "name": "Xinyu Yang"
                    },
                    {
                        "authorId": "50316555",
                        "name": "Zhenguo Zhang"
                    },
                    {
                        "authorId": "37205707",
                        "name": "Rong-yi Cui"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
                "externalIds": {
                    "ArXiv": "2202.06258",
                    "DBLP": "journals/corr/abs-2202-06258",
                    "CorpusId": 246822433
                },
                "corpusId": 246822433,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
                "title": "Flowformer: Linearizing Transformers with Conservation Flows",
                "abstract": "Transformers based on the attention mechanism have achieved impressive success in various areas. However, the attention mechanism has a quadratic complexity, significantly impeding Transformers from dealing with numerous tokens and scaling up to bigger models. Previous methods mainly utilize the similarity decomposition and the associativity of matrix multiplication to devise linear-time attention mechanisms. They avoid degeneration of attention to a trivial distribution by reintroducing inductive biases such as the locality, thereby at the expense of model generality and expressiveness. In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory. We cast attention as the information flow aggregated from the sources (values) to the sinks (results) through the learned flow capacities (attentions). Within this framework, we apply the property of flow conservation into attention and propose the Flow-Attention mechanism of linear complexity. By respectively conserving the incoming flow of sinks for source competition and the outgoing flow of sources for sink allocation, Flow-Attention inherently generates informative attentions without using specific inductive biases. Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning. The code and settings are available at this repository: https://github.com/thuml/Flowformer.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2051867856",
                        "name": "Haixu Wu"
                    },
                    {
                        "authorId": "2154707054",
                        "name": "Jialong Wu"
                    },
                    {
                        "authorId": "2111064536",
                        "name": "Jiehui Xu"
                    },
                    {
                        "authorId": "2144499343",
                        "name": "Jianmin Wang"
                    },
                    {
                        "authorId": "2054275000",
                        "name": "Mingsheng Long"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "54f866e75b83e7dfb8240f0c2eba7337dca0efd3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-05610",
                    "ArXiv": "2202.05610",
                    "DOI": "10.1103/PhysRevAccelBeams.25.104601",
                    "CorpusId": 246822408
                },
                "corpusId": 246822408,
                "publicationVenue": {
                    "id": "f0b25a48-865d-41bf-8487-1394ac41a63f",
                    "name": "Physical Review Accelerators and Beams",
                    "type": "journal",
                    "alternate_names": [
                        "Phys rev accel beam",
                        "Phys Rev Accel Beam",
                        "Physical review accelerators and beams"
                    ],
                    "issn": "2469-9888",
                    "url": "https://journals.aps.org/prab/"
                },
                "url": "https://www.semanticscholar.org/paper/54f866e75b83e7dfb8240f0c2eba7337dca0efd3",
                "title": "Explainable Machine Learning for Breakdown Prediction in High Gradient RF Cavities",
                "abstract": ",",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8208406",
                        "name": "Christoph Obermair"
                    },
                    {
                        "authorId": "1397029595",
                        "name": "T. Cartier-Michaud"
                    },
                    {
                        "authorId": "73680784",
                        "name": "A. Apollonio"
                    },
                    {
                        "authorId": "46603375",
                        "name": "W. Millar"
                    },
                    {
                        "authorId": "51212658",
                        "name": "L. Felsberger"
                    },
                    {
                        "authorId": "2154411691",
                        "name": "Lorenz Fischl"
                    },
                    {
                        "authorId": "2144250332",
                        "name": "H. S. Bovbjerg"
                    },
                    {
                        "authorId": "14531138",
                        "name": "D. Wollmann"
                    },
                    {
                        "authorId": "2574955",
                        "name": "W. Wuensch"
                    },
                    {
                        "authorId": "94377792",
                        "name": "N. Lasheras"
                    },
                    {
                        "authorId": "153746217",
                        "name": "M. Boronat"
                    },
                    {
                        "authorId": "1691836",
                        "name": "F. Pernkopf"
                    },
                    {
                        "authorId": "8288732",
                        "name": "G. Burt"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Inspired by Word2Vec (Mikolov et al., 2013), Scalable Representation Learning (SRL) (Franceschi et al., 2019) proposes a novel triplet loss and tries to learn scalable representations via randomly sampling time segments.",
                "Data Preprocessing Following Franceschi et al. (2019); Zhou et al. (2021), for univariate time series classification task, we normalize datasets using z-score so that the set of observations for each dataset has zero mean and unit variance.",
                "\u2026Learning (TCL) (Hyvarinen & Morioka, 2016), Contrastive Predictive Coding (CPC) (Oord et al., 2018), Scalable RepresentationLearning (SRL) (Franceschi et al., 2019), Temporal and Contextual Contrasting (TS-TCC) (Eldele et al., 2021b) and Temporal Neighborhood Coding(TNC) (Tonekaboni et\u2026",
                ", 2018), Scalable RepresentationLearning (SRL) (Franceschi et al., 2019), Temporal and Contextual Contrasting (TS-TCC) (Eldele et al.",
                ", 2013), Scalable Representation Learning (SRL) (Franceschi et al., 2019) proposes a novel triplet loss and tries to learn scalable representations via randomly sampling time segments."
            ],
            "citingPaper": {
                "paperId": "3bc63b8ebe8195525e7a6a2c06fe129814789928",
                "externalIds": {
                    "ArXiv": "2202.04770",
                    "DBLP": "conf/icml/YangH22a",
                    "CorpusId": 246706116
                },
                "corpusId": 246706116,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3bc63b8ebe8195525e7a6a2c06fe129814789928",
                "title": "Unsupervised Time-Series Representation Learning with Iterative Bilinear Temporal-Spectral Fusion",
                "abstract": "Unsupervised/self-supervised time series representation learning is a challenging problem because of its complex dynamics and sparse annotations. Existing works mainly adopt the framework of contrastive learning with the time-based augmentation techniques to sample positives and negatives for contrastive training. Nevertheless, they mostly use segment-level augmentation derived from time slicing, which may bring about sampling bias and incorrect optimization with false negatives due to the loss of global context. Besides, they all pay no attention to incorporate the spectral information in feature representation. In this paper, we propose a unified framework, namely Bilinear Temporal-Spectral Fusion (BTSF). Specifically, we firstly utilize the instance-level augmentation with a simple dropout on the entire time series for maximally capturing long-term dependencies. We devise a novel iterative bilinear temporal-spectral fusion to explicitly encode the affinities of abundant time-frequency pairs, and iteratively refines representations in a fusion-and-squeeze manner with Spectrum-to-Time (S2T) and Time-to-Spectrum (T2S) Aggregation modules. We firstly conducts downstream evaluations on three major tasks for time series including classification, forecasting and anomaly detection. Experimental results shows that our BTSF consistently significantly outperforms the state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155557947",
                        "name": "Ling Yang"
                    },
                    {
                        "authorId": "2317297",
                        "name": "linda Qiao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026on reconstruction (Yuan et al., 2019; Fortuin et al., 2018, 2020; Chorowski et al., 2019), clustering (Ma et al., 2019; Lei et al., 2019), contrastive objectives (Oord et al., 2018; Franceschi et al., 2019; Hyvarinen et al., 2019; Tonekaboni et al., 2020; Hyvarinen and Morioka, 2016), and others.",
                "This approach is commonly used for evaluating the quality of representations (Oord et al., 2018; Franceschi et al., 2019; Fortuin et al., 2020).",
                ", 2019), contrastive objectives (Oord et al., 2018; Franceschi et al., 2019; Hyvarinen et al., 2019; Tonekaboni et al., 2020; Hyvarinen and Morioka, 2016), and others."
            ],
            "citingPaper": {
                "paperId": "2d7cd80c10cfe85af82a5432b48a743a431654fe",
                "externalIds": {
                    "DBLP": "conf/aistats/TonekaboniLAGP22",
                    "ArXiv": "2202.02262",
                    "CorpusId": 246608063
                },
                "corpusId": 246608063,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2d7cd80c10cfe85af82a5432b48a743a431654fe",
                "title": "Decoupling Local and Global Representations of Time Series",
                "abstract": "Real-world time series data are often generated from several sources of variation. Learning representations that capture the factors contributing to this variability enables a better understanding of the data via its underlying generative process and improves performance on downstream machine learning tasks. This paper proposes a novel generative approach for learning representations for the global and local factors of variation in time series. The local representation of each sample models non-stationarity over time with a stochastic process prior, and the global representation of the sample encodes the time-independent characteristics. To encourage decoupling between the representations, we introduce counterfactual regularization that minimizes the mutual information between the two variables. In experiments, we demonstrate successful recovery of the true local and global variability factors on simulated data, and show that representations learned using our method yield superior performance on downstream tasks on real-world datasets. We believe that the proposed way of defining representations is beneficial for data modelling and yields better insights into the complexity of real-world data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "23152217",
                        "name": "S. Tonekaboni"
                    },
                    {
                        "authorId": "2116729195",
                        "name": "Chun-Liang Li"
                    },
                    {
                        "authorId": "2676352",
                        "name": "Sercan \u00d6. Arik"
                    },
                    {
                        "authorId": "49800482",
                        "name": "A. Goldenberg"
                    },
                    {
                        "authorId": "1945962",
                        "name": "Tomas Pfister"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Data augmentations used are described in Appendix C.\nTriplet (Franceschi et al., 2020) Triplet proposes a time series self-supervised learning approach by taking positive samples to be substrings of the anchor, and negative samples to be randomly sampled from the dataset.",
                "While recent work in time series representation learning focused on various aspects of representation learning such how to sample contrastive pairs (Franceschi et al., 2020; Tonekaboni et al., 2021), taking a Transformer based approach (Zerveas et al.",
                "While recent work in time series representation learning focused on various aspects of representation learning such how to sample contrastive pairs (Franceschi et al., 2020; Tonekaboni et al., 2021), taking a Transformer based approach (Zerveas et al., 2021), exploring complex contrastive learning\u2026"
            ],
            "citingPaper": {
                "paperId": "c090ff3dec01e06f46735b7b9ab133a5db8c73c3",
                "externalIds": {
                    "DBLP": "conf/iclr/WooLSKH22",
                    "ArXiv": "2202.01575",
                    "CorpusId": 246485738
                },
                "corpusId": 246485738,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c090ff3dec01e06f46735b7b9ab133a5db8c73c3",
                "title": "CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting",
                "abstract": "Deep learning has been actively studied for time series forecasting, and the mainstream paradigm is based on the end-to-end training of neural network architectures, ranging from classical LSTM/RNNs to more recent TCNs and Transformers. Motivated by the recent success of representation learning in computer vision and natural language processing, we argue that a more promising paradigm for time series forecasting, is to first learn disentangled feature representations, followed by a simple regression fine-tuning step -- we justify such a paradigm from a causal perspective. Following this principle, we propose a new time series representation learning framework for time series forecasting named CoST, which applies contrastive learning methods to learn disentangled seasonal-trend representations. CoST comprises both time domain and frequency domain contrastive losses to learn discriminative trend and seasonal representations, respectively. Extensive experiments on real-world datasets show that CoST consistently outperforms the state-of-the-art methods by a considerable margin, achieving a 21.3% improvement in MSE on multivariate benchmarks. It is also robust to various choices of backbone encoders, as well as downstream regressors. Code is available at https://github.com/salesforce/CoST.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151488390",
                        "name": "Gerald Woo"
                    },
                    {
                        "authorId": "2039481",
                        "name": "Chenghao Liu"
                    },
                    {
                        "authorId": "36187119",
                        "name": "Doyen Sahoo"
                    },
                    {
                        "authorId": "40305195",
                        "name": "Akshat Kumar"
                    },
                    {
                        "authorId": "1741126",
                        "name": "S. Hoi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7f36d87c89afa1eb39554bc21d125b4b2609262b",
                "externalIds": {
                    "DBLP": "journals/prl/WickstromKMJ22",
                    "ArXiv": "2203.09270",
                    "DOI": "10.1016/j.patrec.2022.02.007",
                    "CorpusId": 246851526
                },
                "corpusId": 246851526,
                "publicationVenue": {
                    "id": "f35e3e87-9df4-497b-aa0d-bb8584197290",
                    "name": "Pattern Recognition Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit Lett"
                    ],
                    "issn": "0167-8655",
                    "url": "https://www.journals.elsevier.com/pattern-recognition-letters/",
                    "alternate_urls": [
                        "http://www.journals.elsevier.com/pattern-recognition-letters/",
                        "http://www.sciencedirect.com/science/journal/01678655"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7f36d87c89afa1eb39554bc21d125b4b2609262b",
                "title": "Mixing Up Contrastive Learning: Self-Supervised Representation Learning for Time Series",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1387887318",
                        "name": "Kristoffer Wickstr\u00f8m"
                    },
                    {
                        "authorId": "8199702",
                        "name": "Michael C. Kampffmeyer"
                    },
                    {
                        "authorId": "1700727",
                        "name": "Karl \u00d8yvind Mikalsen"
                    },
                    {
                        "authorId": "1747567",
                        "name": "R. Jenssen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "on the foundation of random triplet selection in [7] and applied the same network architecture, which consists of layers of dilated causal convolutions.",
                "We use a loss function that is close to [7] while allowing forK positive and negative samples for each anchor.",
                "With respect to time series representation learning, recent work has considered sequential techniques like Long Short Term Memory (LSTM) autoencoders for time series classification [15], similarity preserving low-rank factorization for time series clustering [13], as well as contrastive learning [7] that uses a triplet loss along with a dilational temporal convolutional network architecture [3, 19].",
                "Software implementation was extended from code published by [7] with Python 3.",
                "These issues haven\u2019t received much attention for multivariate time series, where the state of the art [7] simply uses plain random sampling.",
                "In a special case which we generalize in this paper, xpos is a subinterval within xref [7].",
                "combining anchor selection with similarity based triplet mining outperforms the random triplet selection [7], which itself was shown to be better or comparable against the best non-representation learning based time series classification methods including HIVE-COTE [14], RWS [21], ResNet [20], as well as TimeNet [15] among others."
            ],
            "citingPaper": {
                "paperId": "95be484b7a922e2de86a4dcbbd33da1a8d77d31d",
                "externalIds": {
                    "DBLP": "conf/comad/ChangSPD22",
                    "DOI": "10.1145/3493700.3493711",
                    "CorpusId": 245802924
                },
                "corpusId": 245802924,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/95be484b7a922e2de86a4dcbbd33da1a8d77d31d",
                "title": "Time Series Representation Learning with Contrastive Triplet Selection",
                "abstract": "Representation learning, with its proven appeal and advantage in visual and textual modalities, has seen extensions to numerical time series. Recent work proposed a triplet loss formulation based on random triplet sampling to derive a fixed length embedding for time series. Unlike images and text however, statistical and distance measures can be readily computed from numerical time series data to quantitatively contrast differences or cluster based on similarity. This paper investigates the triplet mining problem through contrastive identification methods to select anchors, positive and negative samples in the raw signal space as well as in the embedded vector space. Selected triplets are then learned by a causal temporal neural network to minimize anchor\u2019s distance to positives and maximize its distance to negatives. Experimental results along with an ablation study to compare these methods measured in classification accuracy and variance demonstrated notable improvement over random triplet selection. We also investigate and report performance improvement when sampling avoids label contamination, demonstrating advantage of algorithms proposed.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "11579204",
                        "name": "Yuan Chang"
                    },
                    {
                        "authorId": "1762215",
                        "name": "D. Subramanian"
                    },
                    {
                        "authorId": "2969297",
                        "name": "Raju Pavuluri"
                    },
                    {
                        "authorId": "117334157",
                        "name": "Timothy J. Dinger"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "1) Distance-based methods include derivative DTW(DDDT W) [12] and derivative transform distance (DTDC) [13]; 2) Feature-based methods include bag of SFA symbols(BOSS) [28], time series fores (TSF) [8], time series bag of features (TSBF) [5] and learned pattern similarity(LPS) [4]; 3) Ensemble-based methods include elastic ensembles (EE) [21] and collection of transformation ensembles (COTE) [2]; 4) Deep learning methods include multilayer perceptrons (MLP) [32] and unsupervised scalable representation (FordA) [11]."
            ],
            "citingPaper": {
                "paperId": "05f2635f9c74286ee723770bd90402912ba69113",
                "externalIds": {
                    "DBLP": "journals/apin/ChenWWX22",
                    "DOI": "10.1007/s10489-021-03009-7",
                    "CorpusId": 249523978
                },
                "corpusId": 249523978,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/05f2635f9c74286ee723770bd90402912ba69113",
                "title": "Learning-based shapelets discovery by feature selection for time series classification",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2169180860",
                        "name": "Jiahui Chen"
                    },
                    {
                        "authorId": "50501478",
                        "name": "Yuan Wan"
                    },
                    {
                        "authorId": "2155610963",
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "authorId": "2191928896",
                        "name": "Y. Xuan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "337a0734d9f1b2d5d0d27cb5aa4bca5c3db2a1f9",
                "externalIds": {
                    "DBLP": "journals/isci/ParkPCP22",
                    "DOI": "10.1016/j.ins.2022.01.015",
                    "CorpusId": 245939904
                },
                "corpusId": 245939904,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/337a0734d9f1b2d5d0d27cb5aa4bca5c3db2a1f9",
                "title": "DeepGate: Global-local decomposition for multivariate time series modeling",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109202338",
                        "name": "Jinuk Park"
                    },
                    {
                        "authorId": "2115223984",
                        "name": "Chanhee Park"
                    },
                    {
                        "authorId": "29877537",
                        "name": "Jonghwan Choi"
                    },
                    {
                        "authorId": "2148574733",
                        "name": "Sanghyun Park"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "We reported the performance of the baseline results from the original papers [8], [9], [10], [21], [22], [24], [44] and [11], respectively.",
                "In the UEA archive experiment, we compared our proposed method with eight different methods, including three benchmarks [44], a bag-of-pattern based approach [8], and deep learning-based methods [9], [10], [11], [21], [22], [24].",
                "Dataset EDI DTWI DTWD MLSTM WEASEL NS TapNet ShapeNet TST TS2Vec ODE-RGRU [44] [44] [44] -FCNs [21] +MUSE [8] [22] [9] [10] [24] [11] (Ours)",
                "Also, compared with distance-based methods such as NS [22], TapNet [9], and ShapeNet [10], it showed sufficiently high performance even on small-sized datasets (e.",
                "Negative samples (NS) [22] is an unsupervised learning method with triplet loss through several negative samples.",
                "These approaches use convolutional neural networks (CNNs) or recurrent neural networks (RNNs) to capture spatial or temporal features of time-series data [9], [10], [20], [21], [22].",
                "In [22], an unsupervised method is proposed to solve the variable lengths and sparse labeling problems of time-series data."
            ],
            "citingPaper": {
                "paperId": "51dd9135ca151112408f21ea9a9be1aa15879dd4",
                "externalIds": {
                    "ArXiv": "2112.03379",
                    "DOI": "10.1109/TPAMI.2023.3320125",
                    "CorpusId": 244920600,
                    "PubMed": "37768794"
                },
                "corpusId": 244920600,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/51dd9135ca151112408f21ea9a9be1aa15879dd4",
                "title": "Deep Efficient Continuous Manifold Learning for Time Series Modeling.",
                "abstract": "Modeling non-Euclidean data is drawing extensive attention along with the unprecedented successes of deep neural networks in diverse fields. Particularly, a symmetric positive definite matrix is being actively studied in computer vision, signal processing, and medical image analysis, due to its ability to learn beneficial statistical representations. However, owing to its rigid constraints, it remains challenging to optimization problems and inefficient computational costs, especially, when incorporating it with a deep learning framework. In this paper, we propose a framework to exploit a diffeomorphism mapping between Riemannian manifolds and a Cholesky space, by which it becomes feasible not only to efficiently solve optimization problems but also to greatly reduce computation costs. Further, for dynamic modeling of time-series data, we devise a continuous manifold learning method by systematically integrating a manifold ordinary differential equation\u00a0and a gated recurrent neural network. It is worth noting that due to the nice parameterization of matrices in a Cholesky space, training our proposed network equipped with Riemannian geometric metrics is straightforward. We demonstrate through experiments over regular and irregular time-series datasets that our proposed model can be efficiently and reliably trained and outperforms existing manifold methods and state-of-the-art methods in various time-series tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2570228",
                        "name": "Seungwoo Jeong"
                    },
                    {
                        "authorId": "47106040",
                        "name": "Wonjun Ko"
                    },
                    {
                        "authorId": "71723808",
                        "name": "A. Mulyadi"
                    },
                    {
                        "authorId": "143802908",
                        "name": "Heung-Il Suk"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Unlike the usual supervision-based triplet selection [16,7], we propose a novel unsupervised triplet selection for 2-D time-series images.",
                "estimation of the extent of resemblance between the samples utilizing characteristics such as their shape [14], shapelets [7], alignment [3] and structure [13].",
                "Once trained, a model is used as a feature extractor [7] for the end task.",
                "It facilitates representing time-series from a visual perspective inspired by human visual cognition, involving 2-D convolutions, unlike the 1-D convolutional approaches popular in time-series domain [7].",
                "Unsupervised triplet loss training has been proposed for time-series [7] where representations are learned and evaluated for time-series classification using 1-D dilated causal convolutions."
            ],
            "citingPaper": {
                "paperId": "dc0cc1bd442d107e19f3d26e929a09e30ca82291",
                "externalIds": {
                    "DBLP": "conf/iconip/AnandN20",
                    "MAG": "3102655019",
                    "ArXiv": "2111.10309",
                    "DOI": "10.1007/978-3-030-63823-8_94",
                    "CorpusId": 227075845
                },
                "corpusId": 227075845,
                "publicationVenue": {
                    "id": "bc5a5118-8f5c-49c7-806e-fb8d44c10ae7",
                    "name": "International Conference on Neural Information Processing",
                    "type": "conference",
                    "alternate_names": [
                        "ICONIP",
                        "Int Conf Neural Inf Process"
                    ],
                    "url": "https://link.springer.com/conference/iconip"
                },
                "url": "https://www.semanticscholar.org/paper/dc0cc1bd442d107e19f3d26e929a09e30ca82291",
                "title": "Unsupervised Visual Time-Series Representation Learning and Clustering",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "33195661",
                        "name": "G. Anand"
                    },
                    {
                        "authorId": "143658054",
                        "name": "R. Nayak"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[25] proposed the time-contrastive learning framework, which combines triplet loss, causal convolution, and hole convolution network through positive and negative sampling of time series data."
            ],
            "citingPaper": {
                "paperId": "28f9aac6e26e5a6db95acde15a0d7088934da924",
                "externalIds": {
                    "DBLP": "journals/ijdsa/ZhangWZJZ23",
                    "DOI": "10.1007/s41060-021-00290-0",
                    "CorpusId": 244061731
                },
                "corpusId": 244061731,
                "publicationVenue": {
                    "id": "c3875580-2cce-4a95-a094-fff60bb381b9",
                    "name": "International Journal of Data Science and Analysis",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Data Sci Anal",
                        "International Journal of Data Science and Analytics"
                    ],
                    "issn": "2575-1883",
                    "alternate_issns": [
                        "2364-4168"
                    ],
                    "url": "http://www.sciencepublishinggroup.com/journal/index?journalid=367",
                    "alternate_urls": [
                        "https://link.springer.com/journal/41060"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/28f9aac6e26e5a6db95acde15a0d7088934da924",
                "title": "Exploring unsupervised multivariate time series representation learning for chronic disease diagnosis",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115464862",
                        "name": "Xu Zhang"
                    },
                    {
                        "authorId": "2143467929",
                        "name": "Ya-ming Wang"
                    },
                    {
                        "authorId": "2146642142",
                        "name": "Liang Zhang"
                    },
                    {
                        "authorId": "1504367017",
                        "name": "Bo Jin"
                    },
                    {
                        "authorId": "2108854022",
                        "name": "Hongzhe Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "31bfb15cfe0d91ad6bc8289bb454e9b46dc7cc7f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-03418",
                    "ArXiv": "2111.03418",
                    "CorpusId": 243832718
                },
                "corpusId": 243832718,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/31bfb15cfe0d91ad6bc8289bb454e9b46dc7cc7f",
                "title": "Meta-Forecasting by combining Global Deep Representations with Local Adaptation",
                "abstract": "While classical time series forecasting considers individual time series in isolation, recent advances based on deep learning showed that jointly learning from a large pool of related time series can boost the forecasting accuracy. However, the accuracy of these methods suffers greatly when modeling out-of-sample time series, significantly limiting their applicability compared to classical forecasting methods. To bridge this gap, we adopt a meta-learning view of the time series forecasting problem. We introduce a novel forecasting method, called Meta Global-Local Auto-Regression (Meta-GLAR), that adapts to each time series by learning in closed-form the mapping from the representations produced by a recurrent neural network (RNN) to one-step-ahead forecasts. Crucially, the parameters ofthe RNN are learned across multiple time series by backpropagating through the closed-form adaptation mechanism. In our extensive empirical evaluation we show that our method is competitive with the state-of-the-art in out-of-sample forecasting accuracy reported in earlier work.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51004518",
                        "name": "Riccardo Grazzi"
                    },
                    {
                        "authorId": "2067154581",
                        "name": "Valentin Flunkert"
                    },
                    {
                        "authorId": "144607961",
                        "name": "David Salinas"
                    },
                    {
                        "authorId": "2166235",
                        "name": "Tim Januschowski"
                    },
                    {
                        "authorId": "1680574",
                        "name": "M. Seeger"
                    },
                    {
                        "authorId": "2824663",
                        "name": "C. Archambeau"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ef4af4e2d021ebf173af1383e499cb4f71452b91",
                "externalIds": {
                    "ArXiv": "2111.02599",
                    "DBLP": "conf/aistats/AgrawalLOGS22",
                    "CorpusId": 242757803
                },
                "corpusId": 242757803,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ef4af4e2d021ebf173af1383e499cb4f71452b91",
                "title": "Leveraging Time Irreversibility with Order-Contrastive Pre-training",
                "abstract": "Label-scarce, high-dimensional domains such as healthcare present a challenge for modern machine learning techniques. To overcome the difficulties posed by a lack of labeled data, we explore an\"order-contrastive\"method for self-supervised pre-training on longitudinal data. We sample pairs of time segments, switch the order for half of them, and train a model to predict whether a given pair is in the correct order. Intuitively, the ordering task allows the model to attend to the least time-reversible features (for example, features that indicate progression of a chronic disease). The same features are often useful for downstream tasks of interest. To quantify this, we study a simple theoretical setting where we prove a finite-sample guarantee for the downstream error of a representation learned with order-contrastive pre-training. Empirically, in synthetic and longitudinal healthcare settings, we demonstrate the effectiveness of order-contrastive pre-training in the small-data regime over supervised learning and other self-supervised pre-training baselines. Our results indicate that pre-training methods designed for particular classes of distributions and downstream tasks can improve the performance of self-supervised learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2056898702",
                        "name": "Monica Agrawal"
                    },
                    {
                        "authorId": "30155522",
                        "name": "Hunter Lang"
                    },
                    {
                        "authorId": "5880067",
                        "name": "M. Offin"
                    },
                    {
                        "authorId": "10428733",
                        "name": "L. Gazit"
                    },
                    {
                        "authorId": "1746662",
                        "name": "D. Sontag"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "While image [31, 32], video [33], language [34, 35], and speech [36] representations have benefited from contrastive learning, research on learning physiological signals has been limited [37, 38]."
            ],
            "citingPaper": {
                "paperId": "acfeea47264dd89948853201191332e1780bea1c",
                "externalIds": {
                    "ArXiv": "2110.15278",
                    "CorpusId": 240070586
                },
                "corpusId": 240070586,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/acfeea47264dd89948853201191332e1780bea1c",
                "title": "Self-supervised EEG Representation Learning for Automatic Sleep Staging",
                "abstract": "Background: Deep learning models have shown great success in automating tasks in sleep medicine by learning from carefully annotated Electroencephalogram (EEG) data. However, effectively utilizing a large amount of raw EEG remains a challenge. Objective: In this paper, we aim to learn robust vector representations from massive unlabeled EEG signals, such that the learned vectorized features (1) are expressive enough to replace the raw signals in the sleep staging task; and (2) provide better predictive performance than supervised models in scenarios of fewer labels and noisy samples. Methods: We propose a self-supervised model, named Contrast with the World Representation (ContraWR), for EEG signal representation learning, which uses global statistics from the dataset to distinguish signals associated with different sleep stages. The ContraWR model is evaluated on three real-world EEG datasets that include both at-home and in-lab EEG recording settings. Results: ContraWR outperforms 4 recent self-supervised learning methods on the sleep staging task across 3 large EEG datasets. ContraWR also beats supervised learning when fewer training labels are available (e.g., 4% accuracy improvement when less than 2% data is labeled). Moreover, the model provides informative representative feature structures in 2D projection. Conclusions: We show that ContraWR is robust to noise and can provide high-quality EEG representations for downstream prediction tasks. The proposed model can be generalized to other unsupervised physiological signal learning tasks. Future directions include exploring task-specific data augmentations and combining self-supervised with supervised methods, building upon the initial success of self-supervised learning in this paper.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51166038",
                        "name": "Chaoqi Yang"
                    },
                    {
                        "authorId": "2233479993",
                        "name": "Danica Xiao"
                    },
                    {
                        "authorId": "144293787",
                        "name": "M. Westover"
                    },
                    {
                        "authorId": "1738536",
                        "name": "Jimeng Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "The T-loss model (Franceschi et al., 2019) uses a triplet loss, which maximizes the distance between",
                "Recent self-supervised methods for time series data (Franceschi et al., 2019; van den Oord et al., 2018; Tonekaboni et al., 2021) learn representations using forecasting or distance-based metrics in order to employ conventional contrastive loss functions.",
                "The T-loss model (Franceschi et al., 2019) uses a triplet loss, which maximizes the distance between\nnegative examples that are chosen independently at random while minimizing the distance between the reference sequence and its subsets.",
                "We make the logical assumption similar to other time series approaches (Franceschi et al., 2019; Tonekaboni et al., 2021) that in the input D the smaller the distance (in time) is between two points Dt and Dt\u2032 , the more related they are."
            ],
            "citingPaper": {
                "paperId": "3f20f3b72be3b57f375bc5116a34f750554ec2a2",
                "externalIds": {
                    "ArXiv": "2110.13623",
                    "DBLP": "journals/corr/abs-2110-13623",
                    "CorpusId": 239885540
                },
                "corpusId": 239885540,
                "publicationVenue": {
                    "id": "2486528b-036c-4f3c-953f-c574eb381d12",
                    "name": "Asian Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Mach Learn",
                        "ACML"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=40"
                },
                "url": "https://www.semanticscholar.org/paper/3f20f3b72be3b57f375bc5116a34f750554ec2a2",
                "title": "Contrastive Neural Processes for Self-Supervised Learning",
                "abstract": "Recent contrastive methods show significant improvement in self-supervised learning in several domains. In particular, contrastive methods are most effective where data augmentation can be easily constructed e.g. in computer vision. However, they are less successful in domains without established data transformations such as time series data. In this paper, we propose a novel self-supervised learning framework that combines contrastive learning with neural processes. It relies on recent advances in neural processes to perform time series forecasting. This allows to generate augmented versions of data by employing a set of various sampling functions and, hence, avoid manually designed augmentations. We extend conventional neural processes and propose a new contrastive loss to learn times series representations in a self-supervised setup. Therefore, unlike previous self-supervised methods, our augmentation pipeline is task-agnostic, enabling our method to perform well across various applications. In particular, a ResNet with a linear classifier trained using our approach is able to outperform state-of-the-art techniques across industrial, medical and audio datasets improving accuracy over 10% in ECG periodic data. We further demonstrate that our self-supervised representations are more efficient in the latent space, improving multiple clustering indexes and that fine-tuning our method on 10% of labels achieves results competitive to fully-supervised learning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2134886789",
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "authorId": "3040891",
                        "name": "Denis A. Gudovskiy"
                    },
                    {
                        "authorId": "52280238",
                        "name": "Kozuka Kazuki"
                    },
                    {
                        "authorId": "66398167",
                        "name": "Ohama Iku"
                    },
                    {
                        "authorId": "2526461",
                        "name": "Luca Rigazio"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "art in a variety of diverse benchmarks a suite of benchmarks in diverse application areas [86]."
            ],
            "citingPaper": {
                "paperId": "aa62d5e43cb151cd574e4df058b4c6a509d62644",
                "externalIds": {
                    "DBLP": "journals/spm/EricssonGLH22",
                    "ArXiv": "2110.09327",
                    "DOI": "10.1109/MSP.2021.3134634",
                    "CorpusId": 239017006
                },
                "corpusId": 239017006,
                "publicationVenue": {
                    "id": "f62e5eab-173a-4e0a-a963-ed8de9835d22",
                    "name": "IEEE Signal Processing Magazine",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Signal Process Mag"
                    ],
                    "issn": "1053-5888",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=79"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aa62d5e43cb151cd574e4df058b4c6a509d62644",
                "title": "Self-Supervised Representation Learning: Introduction, advances, and challenges",
                "abstract": "Self-supervised representation learning (SSRL) methods aim to provide powerful, deep feature learning without the requirement of large annotated data sets, thus alleviating the annotation bottleneck\u2014one of the main barriers to the practical deployment of deep learning today. These techniques have advanced rapidly in recent years, with their efficacy approaching and sometimes surpassing fully supervised pretraining alternatives across a variety of data modalities, including image, video, sound, text, and graphs. This article introduces this vibrant area, including key concepts, the four main families of approaches and associated state-of-the-art techniques, and how self-supervised methods are applied to diverse modalities of data. We further discuss practical considerations including workflows, representation transferability, and computational cost. Finally, we survey major open challenges in the field, that provide fertile ground for future work.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "37151799",
                        "name": "Linus Ericsson"
                    },
                    {
                        "authorId": "2319565",
                        "name": "H. Gouk"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "1697755",
                        "name": "Timothy M. Hospedales"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "24): \u2013 Triplet loss (triplet): This is the loss proposed in Franceschi et al. (2019) that aims to obtain similar representation between a time subseries and its neighborhood (see Eq.",
                "\u2013 Dilated-CNN (DCNN): This architecture was proposed in Franceschi et al. (2019).",
                "Another loss has been proposed to train an encoder specific to time series, called triplet loss (Franceschi et al. 2019).",
                "Besides offering a variety of datasets, these two archives have been subject to numerous use (Fawaz et al. 2019; Franceschi et al. 2019; Ma et al. 2019; Madiraju et al. 2018; Xiao et al. 2020; Zhang et al. 2018).",
                "The authors in Franceschi et al. (2019) proposed to solve this problem by using a time-based sampling strategy.",
                "For all configurations, we use the Adam optimizer with a learning rate of 0.001 (Bo et al. 2020; Franceschi et al. 2019; Madiraju et al. 2018; Mukherjee et al. 2019) at the exception of theDRNN architecture wherewe use the SGDoptimizer with exponential decay, a learning rate of 0.1, and a decay\u2026"
            ],
            "citingPaper": {
                "paperId": "7cdc279cea2d6ffb973bbe71e5e92d5de5ce24bb",
                "externalIds": {
                    "MAG": "3205581618",
                    "DBLP": "journals/datamine/LafabregueWGF22",
                    "DOI": "10.1007/s10618-021-00796-y",
                    "CorpusId": 244581459
                },
                "corpusId": 244581459,
                "publicationVenue": {
                    "id": "d263025a-9eaf-443f-9bbf-72377e8d22a6",
                    "name": "Data mining and knowledge discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Data Mining and Knowledge Discovery",
                        "Data Min Knowl Discov",
                        "Data min knowl discov"
                    ],
                    "issn": "1384-5810",
                    "url": "https://www.springer.com/computer/database+management+&+information+retrieval/journal/10618",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10618",
                        "http://www.springer.com/computer/database+management+&+information+retrieval/journal/10618"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7cdc279cea2d6ffb973bbe71e5e92d5de5ce24bb",
                "title": "End-to-end deep representation learning for time series clustering: a comparative study",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51289468",
                        "name": "Baptiste Lafabregue"
                    },
                    {
                        "authorId": "152947675",
                        "name": "J. Weber"
                    },
                    {
                        "authorId": "1808290",
                        "name": "P. Gan\u00e7arski"
                    },
                    {
                        "authorId": "2318564",
                        "name": "G. Forestier"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2022 T-Loss (Franceschi, Dieuleveut, and Jaggi 2019): Unsupervised scalable representation learning for multivariate time series.",
                "Recent progress of self-supervised learning (SSL) (Jing and Tian 2019; He et al. 2020; Chen et al. 2020) has gained promising performance for physiological time series, with competitive performance compared with supervised methods (Franceschi, Dieuleveut, and Jaggi 2019; Xiao et al. 2021).",
                "For time series, a line of SSL methods sample positives from temporally neighbors (Oord, Li, and Vinyals 2018; Franceschi, Dieuleveut, and Jaggi 2019; Tonekaboni, Eytan, and Goldenberg 2021; Eldele et al. 2021)."
            ],
            "citingPaper": {
                "paperId": "bad27305a92d45ffe7097ad152501eb079662899",
                "externalIds": {
                    "ArXiv": "2110.09966",
                    "DBLP": "journals/corr/abs-2110-09966",
                    "CorpusId": 239024620
                },
                "corpusId": 239024620,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bad27305a92d45ffe7097ad152501eb079662899",
                "title": "SleepPriorCL: Contrastive Representation Learning with Prior Knowledge-based Positive Mining and Adaptive Temperature for Sleep Staging",
                "abstract": "The objective of this paper is to learn semantic representations for sleep stage classification from raw physiological time series. Although supervised methods have gained remarkable performance, they are limited in clinical situations due to the requirement of fully labeled data. Self-supervised learning (SSL) based on contrasting semantically similar (positive) and dissimilar (negative) pairs of samples have achieved promising success. However, existing SSL methods suffer the problem that many semantically similar positives are still uncovered and even treated as negatives. In this paper, we propose a novel SSL approach named SleepPriorCL to alleviate the above problem. Advances of our approach over existing SSL methods are two-fold: 1) by incorporating prior domain knowledge into the training regime of SSL, more semantically similar positives are discovered without accessing ground-truth labels; 2) via investigating the influence of the temperature in contrastive loss, an adaptive temperature mechanism for each sample according to prior domain knowledge is further proposed, leading to better performance. Extensive experiments demonstrate that our method achieves state-of-the-art performance and consistently outperforms baselines.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108699823",
                        "name": "Hongjun Zhang"
                    },
                    {
                        "authorId": "2152438043",
                        "name": "Jing Wang"
                    },
                    {
                        "authorId": "2113517543",
                        "name": "Qinfeng Xiao"
                    },
                    {
                        "authorId": "2153628358",
                        "name": "Jiaoxue Deng"
                    },
                    {
                        "authorId": "2624174",
                        "name": "Youfang Lin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "On certain datasets, our results approach other recent unsupervised approaches in which a simple linear classifier is trained on top of a complex unsupervised feature extractor [43, 70, 71].",
                "Our encoder is a one layer causal dilated encoder with skip connections, an architecture recently shown to provide strong time series classification performance [71]."
            ],
            "citingPaper": {
                "paperId": "9c1ceb96ef4b0cbf46d01b89d40da0c9ff52c675",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-05266",
                    "ArXiv": "2110.05266",
                    "CorpusId": 238583409
                },
                "corpusId": 238583409,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9c1ceb96ef4b0cbf46d01b89d40da0c9ff52c675",
                "title": "Chaos as an interpretable benchmark for forecasting and data-driven modelling",
                "abstract": "The striking fractal geometry of strange attractors underscores the generative nature of chaos: like probability distributions, chaotic systems can be repeatedly measured to produce arbitrarily-detailed information about the underlying attractor. Chaotic systems thus pose a unique challenge to modern statistical learning techniques, while retaining quantifiable mathematical properties that make them controllable and interpretable as benchmarks. Here, we present a growing database currently comprising 131 known chaotic dynamical systems spanning fields such as astrophysics, climatology, and biochemistry. Each system is paired with precomputed multivariate and univariate time series. Our dataset has comparable scale to existing static time series databases; however, our systems can be re-integrated to produce additional datasets of arbitrary length and granularity. Our dataset is annotated with known mathematical properties of each system, and we perform feature analysis to broadly categorize the diverse dynamics present across the collection. Chaotic systems inherently challenge forecasting models, and across extensive benchmarks we correlate forecasting performance with the degree of chaos present. We also exploit the unique generative properties of our dataset in several proof-of-concept experiments: surrogate transfer learning to improve time series classification, importance sampling to accelerate model training, and benchmarking symbolic regression algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49738547",
                        "name": "W. Gilpin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2019s [42] unsupervised representation learning algorithm, hereby noted as \u201cRL\u201d, learns representations of time-series elements using an encoder architecture based on causal dilated convolutions with a triplet"
            ],
            "citingPaper": {
                "paperId": "6b8ef926e486079c0eed877dfc863cbc896f8914",
                "externalIds": {
                    "ArXiv": "2110.05633",
                    "DBLP": "conf/icdm/SharmaBR21",
                    "DOI": "10.1109/ICDM51629.2021.00165",
                    "CorpusId": 246288417
                },
                "corpusId": 246288417,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/6b8ef926e486079c0eed877dfc863cbc896f8914",
                "title": "T3: Domain-Agnostic Neural Time-series Narration",
                "abstract": "The task of generating rich and fluent narratives that aptly describe the characteristics, trends, and anomalies of time-series data is invaluable to the sciences (geology, meteorology, epidemiology) or finance (trades, stocks). The efforts for time-series narration hitherto are domain-specific and use predefined templates that offer consistency but lead to mechanical narratives. We present $\\mathrm{T}^{3}$ (Time-series-To-Text), a domain-agnostic neural framework for time-series narration, that couples the representation of essential time-series elements in the form of a dense knowledge graph and the translation of said knowledge graph into rich and fluent narratives through the transfer-learning capabilities of PLMs (Pre-trained Language Models). To the best of our knowledge, $\\mathrm{T}^{3}$ is the first investigation of the use of neural strategies for time-series narration. We showcase that $\\mathrm{T}^{3}$ can improve the lexical diversity of the generated narratives by up to 65.38% while still maintaining grammatical integrity. The performance and practicality of $\\mathrm{T}^{3}$ is further validated through an expert review $(n=21)$ where 76.2% of participating experts wary of auto-generated narratives favored $\\mathrm{T}^{3}$ as a deployable system for time-series narration due to its rich and diverse narratives. Our code-base and the datasets used with detailed instructions for reproducibility is publicly hosted 1.1https://github.com/Mandar-Sharma/TCube",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1928141171",
                        "name": "Mandar Sharma"
                    },
                    {
                        "authorId": "144252495",
                        "name": "J. Brownstein"
                    },
                    {
                        "authorId": "1755938",
                        "name": "Naren Ramakrishnan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "This conclusion is also demonstrated in EEG/MEG applications (SelfRegulationSCP1), with 10% labeled samples, SMATE obtained a higher accuracy (0.781) than fully supervised 1NNDTW-D (0.775), USRL (0.771), Semi-TapNet (0.739) and MTL (0.730).",
                ", from 10% labeled training set to fully labeled one, the accuracy of SMATE varies only by 0.046, compared to INN-DTW-D (0.264), USRL (0.286), Semi-TapNet (0.151) and MTL(0.225), showing that SMATE is capable of learning a class-separable representation under weak supervision.",
                "For comparison, we applied one classic model 1NN-DTW-D [3] and three recently proposed semi-supervised deep learning models: USRL [15], Semi-TapNet [20] and MTL [21].",
                "USRL [15]; TapNet [20]; MLSTM-FCN [12]; CA-SFCN [6]; SMATENR: SMATE without supervised Regularization, instead, a Softmax layer is applied on the embedding.",
                "1We use the term spatial in this paper for the variable axis The recent research turns to Representation Learning [14] when handling weakly labeled MTS, which allows learning low dimensional embeddings in an unsupervised manner, such as using triplet loss [15] to form the embedding space, then even an SVM classifier is powerful enough on the learned representation [15].",
                "USRL [15]; TapNet\n[20]; MLSTM-FCN [12]; CA-SFCN [6]; SMATENR: SMATE without supervised Regularization, instead, a Softmax layer is applied on the embedding.",
                "Unsupervised Scalable Representation Learning (USRL) described in [15] combines causal dilated convolutions with triplet loss for contrastive learning.",
                "Besides, USRL and SMATENR perform much worse than SMATE with the same SVM classifier, confirming the reliability of our supervised regularization on the embedding space.",
                "As no label information is utilized to learn the representation [15], there is a risk that it deviates from the true features, thus affecting the classifier performance."
            ],
            "citingPaper": {
                "paperId": "652922ab117fb58b22803c4087615a36ebd25373",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-00578",
                    "ArXiv": "2110.00578",
                    "DOI": "10.1109/ICDM51629.2021.00206",
                    "CorpusId": 238253191
                },
                "corpusId": 238253191,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/652922ab117fb58b22803c4087615a36ebd25373",
                "title": "SMATE: Semi-Supervised Spatio-Temporal Representation Learning on Multivariate Time Series",
                "abstract": "Learning from Multivariate Time Series (MTS) has attracted widespread attention in recent years. In particular, label shortage is a real challenge for the classification task on MTS, considering its complex dimensional and sequential data structure. Unlike self-training and positive unlabeled learning that rely on distance-based classifiers, in this paper, we propose SMATE, a novel semi-supervised model for learning the interpretable Spatio-Temporal representation from weakly labeled MTS. We validate empirically the learned representation on 30 public datasets from the UEA MTS archive. We compare it with 13 state-of-the-art baseline methods for fully supervised tasks and four baselines for semi-supervised tasks. The results show the reliability and efficiency of our proposed method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2057461656",
                        "name": "Jingwei Zuo"
                    },
                    {
                        "authorId": "1734676",
                        "name": "K. Zeitouni"
                    },
                    {
                        "authorId": "2671962",
                        "name": "Y. Taher"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Some works have studied self-supervision in time series data (Yue et al. 2021; Mehari and Strodthoff 2021; Spathis et al. 2020; Banville et al. 2019; Ma et al. 2019; Franceschi, Dieuleveut, and Jaggi 2019; Hyvarinen and Morioka 2016)."
            ],
            "citingPaper": {
                "paperId": "bf10bec6c7127df551f5431fa83e4a2f853d2515",
                "externalIds": {
                    "DBLP": "conf/aaai/0004NHF22",
                    "ArXiv": "2109.08908",
                    "DOI": "10.1609/aaai.v36i4.20376",
                    "CorpusId": 237571745
                },
                "corpusId": 237571745,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bf10bec6c7127df551f5431fa83e4a2f853d2515",
                "title": "Intra-Inter Subject Self-supervised Learning for Multivariate Cardiac Signals",
                "abstract": "Learning information-rich and generalizable representations effectively from unlabeled multivariate cardiac signals to identify abnormal heart rhythms (cardiac arrhythmias) is valuable in real-world clinical settings but often challenging due to its complex temporal dynamics. Cardiac arrhythmias can vary significantly in temporal patterns even for the same patient (i.e., intra subject difference). Meanwhile, the same type of cardiac arrhythmia can show different temporal patterns among different patients due to different cardiac structures (i.e., inter subject difference). In this paper, we address the challenges by proposing an Intra-Inter Subject Self-Supervised Learning (ISL) model that is customized for multivariate cardiac signals. Our proposed ISL model integrates medical knowledge into self-supervision to effectively learn from intra-inter subject differences. In intra subject self-supervision, ISL model first extracts heartbeat-level features from each subject using a channel-wise attentional CNN-RNN encoder. Then a stationarity test module is employed to capture the temporal dependencies between heartbeats. In inter subject self-supervision, we design a set of data augmentations according to the clinical characteristics of cardiac signals and perform contrastive learning among subjects to learn distinctive representations for various types of patients. Extensive experiments on three real-world datasets were conducted. In a semi-supervised transfer learning scenario, our pre-trained ISL model leads about 10% improvement over supervised training when only 1% labeled data is available, suggesting strong generalizability and robustness of the model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2056436683",
                        "name": "Xiang Lan"
                    },
                    {
                        "authorId": "2016341822",
                        "name": "Dianwen Ng"
                    },
                    {
                        "authorId": "2317297",
                        "name": "linda Qiao"
                    },
                    {
                        "authorId": "2067622344",
                        "name": "Mengling Feng"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "(b) composition of the i-th layer of the chosen architecture [10]",
                "We use an algorithm [10] that outperforms the previously mentioned methods, also tested on many standard datasets for unsupervised representation learning for time series.",
                "Overall, the training procedure consists of traveling through the training dataset for several epochs (possibly using mini-batches), picking tuples (x, x,(xk neg )k) at random as detailed in Algorithm 1 of franceschi [10], and performing a minimization step on the",
                "Franceschi Algorithm 1- for time series embedding [10]"
            ],
            "citingPaper": {
                "paperId": "ad48c5e51191970cffa2dd153d84ba4b0b6d5e00",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-07570",
                    "ArXiv": "2109.07570",
                    "CorpusId": 237532680
                },
                "corpusId": 237532680,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ad48c5e51191970cffa2dd153d84ba4b0b6d5e00",
                "title": "Predicting the outcome of team movements - Player time series analysis using fuzzy and deep methods for representation learning",
                "abstract": "We extract and use player position time-series data, tagged along with the action types, to build a competent model for representing team tactics behavioral patterns and use this representation to predict the outcome of arbitrary movements. We provide a framework for the useful encoding of short tactics and space occupations in a more extended sequence of movements or tactical plans. We investigate game segments during a match in which the team in possession of the ball regularly attempts to reach a position where they can take a shot at goal for a single game. A carefully designed and efficient kernel is employed using a triangular fuzzy membership function to create multiple time series for players' potential of presence at different court regions. Unsupervised learning is then used for time series using triplet loss and deep neural networks with exponentially dilated causal convolutions for the derived multivariate time series. This works key contribution lies in its approach to model how short scenes contribute to other longer ones and how players occupies and creates new spaces in-game court. We discuss the effectiveness of the proposed approach for prediction and recognition tasks on the professional basketball SportVU dataset for the 2015-16 half-season. The proposed system demonstrates descent functionality even with relatively small data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2123318008",
                        "name": "Omid Shokrollahi"
                    },
                    {
                        "authorId": "2127066788",
                        "name": "Bahman Rohani"
                    },
                    {
                        "authorId": "2125029",
                        "name": "A. Nobakhti"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d9372fb78e1f6db3a1f171bdf0c2d563376e7385",
                "externalIds": {
                    "ArXiv": "2109.00783",
                    "CorpusId": 250626479
                },
                "corpusId": 250626479,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d9372fb78e1f6db3a1f171bdf0c2d563376e7385",
                "title": "Computer Vision Self-supervised Learning Methods on Time Series",
                "abstract": "Self-supervised learning (SSL) has had great success in both computer vision and natural language processing. These approaches often rely on cleverly crafted loss functions and training setups to avoid feature collapse. In this study, the effectiveness of mainstream SSL frameworks from computer vision and some SSL frameworks for time series are evaluated on the UCR, UEA and PTB-XL datasets, and we show that computer vision SSL frameworks can be effective for time series. In addition, we propose a new method that improves on the recently proposed VICReg method. Our method improves on a \\textit{covariance} term proposed in VICReg, and in addition we augment the head of the architecture by an IterNorm layer that accelerates the convergence of the model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "96694307",
                        "name": "Daesoo Lee"
                    },
                    {
                        "authorId": "2176789998",
                        "name": "Erlend Aune Norwegian University of Science"
                    },
                    {
                        "authorId": "2133449244",
                        "name": "Technology"
                    },
                    {
                        "authorId": "2176778846",
                        "name": "BI Norwegian Business School"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "1a2ac489fd45da5e235095b39adc9e36dc91af22",
                "externalIds": {
                    "DOI": "10.31224/osf.io/mcg75",
                    "CorpusId": 239343811
                },
                "corpusId": 239343811,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1a2ac489fd45da5e235095b39adc9e36dc91af22",
                "title": "Modelling and Monitoring Erosion of the Leading Edge of Wind Turbine Blades",
                "abstract": "Leading edge surface erosion is an emerging issue in wind turbine blade reliability, causing reduction in power performance, aerodynamic loads imbalance, increased noise emission and ultimately additional maintenance costs, and if left untreated, leads to the compromise of the functionality of the blade. In this work, we first propose an empirical spatio-temporal stochastic model for simulating leading edge erosion, to be used in conjunction with aeroelastic simulations, and subsequently propose a deep learning model trained on simulated data, which aims to monitor leading edge erosion by detecting and classifying the degradation severity. The main ingredients of the model include a damage process that progresses at random times, across multiple discrete states characterized by a non-homogeneous compound Poisson process, which is used to describe the random and time-dependent degradation of the blade surface, thus implicitly affecting its aerodynamic properties. The model allows for one, or more, zones along the span of the blades to be independently affected by erosion. The proposed model accounts for uncertainties in the local airfoil aerodynamics via parameterization of the lift and drag coefficients curves. The proposed model is used to generate a stochastic ensemble of degrading airfoil aerodynamic polars, for use in forward aero-servo-elastic simulations, where we compute the effect of leading edge erosion degradation on the dynamic response of a wind turbine under varying turbulent input inflow conditions. The dynamic response is chosen a defining output as this relates to the output variable that is most commonly monitored under a Structural Health Monitoring (SHM) regime. In this context, we further propose an approach for spatio-temporal dependent diagnostics of leading erosion, namely, a deep learning attention-based Transformer, which we modify for classification tasks on slow degradation processes with long sequence multivariate time-series as inputs. We perform multiple sets of numerical experiments, aiming to evaluate the Transformer for diagnostics and assess its limitations. The results reveal Transformers as a potent method for diagnosis of such degradation processes. The attention-based mechanism allows the network to focus on different features at different time intervals for better prediction accuracy, especially for long time-series sequences representing a slow degradation process.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2134243541",
                        "name": "G. Duth\u00e9"
                    },
                    {
                        "authorId": "33985386",
                        "name": "I. Abdallah"
                    },
                    {
                        "authorId": "81862050",
                        "name": "S. Barber"
                    },
                    {
                        "authorId": "66773557",
                        "name": "E. Chatzi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "01f835dbd5c615c530a946961116174088930b27",
                "externalIds": {
                    "MAG": "3197712158",
                    "DOI": "10.32913/mic-ict-research.v2021.n2.973",
                    "CorpusId": 239669642
                },
                "corpusId": 239669642,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/01f835dbd5c615c530a946961116174088930b27",
                "title": "Runoff Prediction Based on Deep Belief Networks",
                "abstract": "\n \n \nRunoff prediction has recently become an essential task with respect to assessing the impact of climate change to people\u2019s livelihoods and production. However, the runoff time series always exhibits nonlinear and non-stationary features, which makes it very difficult to be accurately predicted. Machine learning have been recently proved to be a powerful tool in helping society adapt to a changing climate and its subfield, deep learning, showed the power in approximate nonlinear functions. In this study, we propose a method based on deep belief networks (DBN) for runoff prediction. In order to evaluate the proposed method, we collected runoff datasets from Srepok and Dak Nong rivers located in mountain regions of the Central Highland of Vietnam in the periods of 2001-2007 at Dak Nong hydrology station and 1990-2011 at Buon Don hydrology station, respectively. Experimental results show that DBN outperforms, respectively, LSTM, BiLSTM, multi-layer perceptron (MLP) trained by particle swarm optimization (PSO) and MLP trained by stochastic gradient descent (SGD) in which gradients are computed using the backpropagation (BP) procedure. The results also confirm that DBN is suitable to employ for the task of runoff prediction. \n \n \n",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2140461799",
                        "name": "Thanh Hi\u00ean Nguy\u1ec5n"
                    },
                    {
                        "authorId": "2000136759",
                        "name": "Thi T. T. Tran"
                    },
                    {
                        "authorId": "3025333",
                        "name": "H. Duong"
                    },
                    {
                        "authorId": "2149581631",
                        "name": "Hoai Tran"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Negative sampling and triplet loss are frequently used in various tasks [15, 16, 17, 18].",
                "These methods were also applied for multivariate time series representation learning and image similarity learning [17, 18]."
            ],
            "citingPaper": {
                "paperId": "358e9fd8ee370db0459d28602c12cbb1a0add253",
                "externalIds": {
                    "DBLP": "conf/interspeech/ChoiKKK21",
                    "MAG": "3196553401",
                    "DOI": "10.21437/interspeech.2021-885",
                    "CorpusId": 239703992
                },
                "corpusId": 239703992,
                "publicationVenue": {
                    "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
                    "name": "Interspeech",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Int Speech Commun Assoc",
                        "INTERSPEECH",
                        "Conference of the International Speech Communication Association"
                    ],
                    "issn": "2308-457X",
                    "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
                    "alternate_urls": [
                        "http://www.isca-speech.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/358e9fd8ee370db0459d28602c12cbb1a0add253",
                "title": "Label Embedding for Chinese Grapheme-to-Phoneme Conversion",
                "abstract": "Chinese grapheme-to-phoneme (G2P) conversion plays a significant role in text-to-speech systems by generating pronunciations corresponding to Chinese input characters. The main challenge in Chinese G2P conversion is polyphone disambiguation, which requires selecting the appropriate pronunciation among several candidates. In polyphone disambiguation, calculating probabilities for the entire pronunciations is unnecessary since each Chinese character has only a few (mostly two or three) candidate pronunciations. In this study, we introduce a label embedding approach that matches the character embedding with the closest label embedding among the possible candidates. Specifically, negative sampling and triplet loss were applied to maximize the difference between the correct embedding and the other candidate embeddings. Experimental results show that the label embedding approach improved the polyphone disambiguation accuracy by 4.50% and 1.74% on two datasets compared to the one-hot label classification approach. Moreover, the bidirectional long short-term memory model with the label embedding approach outperformed the previous most advanced model, BERT, demonstrating outstanding performance in polyphone disambiguation. Lastly, we discuss the effect of contextual information in character embeddings on the G2P conversion task.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2146892797",
                        "name": "E. Choi"
                    },
                    {
                        "authorId": "82608837",
                        "name": "Hwa-Yeon Kim"
                    },
                    {
                        "authorId": "2175564388",
                        "name": "Jong-Hwan Kim"
                    },
                    {
                        "authorId": "2125028067",
                        "name": "Jae-Min Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[21] proposed an unsupervised scalable representation learning model (USRL) for multivariate time series, which utilized a deep encoder network formed by dilated convolutions to generate informative features.",
                "It is known that there are many works that can reduce dimensionality of multivariate time series, such as CPCA [20], VPCA [17], and deep encoder networks [21].",
                "For example, two-dimensional singular value decomposition [15], variable-based principal component analysis (VPCA) [17], common principal component analysis (CPCA) [18\u201320], and deep encoder networks [21, 22] are used to reduce dimensionality of multivariate time series and learn informative features for clustering."
            ],
            "citingPaper": {
                "paperId": "af5fc2a3fc7fa23bb8780527ff098525f4d91c36",
                "externalIds": {
                    "MAG": "3193456254",
                    "DOI": "10.1155/2021/4310417",
                    "CorpusId": 238722656
                },
                "corpusId": 238722656,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/af5fc2a3fc7fa23bb8780527ff098525f4d91c36",
                "title": "Robust Graph Factorization for Multivariate Electricity Consumption Series Clustering",
                "abstract": "Multivariate electricity consumption series clustering can reflect trends of power consumption changes in the past time period, which can provide reliable guidance for electricity production. However, there are some abnormal series in the past multivariate electricity consumption series data, while outliers will affect the discovery of electricity consumption trends in different time periods. To address this problem, we propose a robust graph factorization model for multivariate electricity consumption clustering (RGF-MEC), which performs graph factorization and outlier discovery simultaneously. RGF-MEC first obtains a similarity graph by calculating distance among multivariate electricity consumption series data and then performs robust matrix factorization on the similarity graph. Meanwhile, the similarity graph is decomposed into a class-related embedding and a spectral embedding, where the class-related embedding directly reveals the final clustering results. Experimental results on realistic multivariate time-series datasets and multivariate electricity consumption series datasets demonstrate effectiveness of the proposed RGF-MEC model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "101499494",
                        "name": "Zheng Kaihong"
                    },
                    {
                        "authorId": "49844453",
                        "name": "Honghao Liang"
                    },
                    {
                        "authorId": "101781253",
                        "name": "Zeng Lukun"
                    },
                    {
                        "authorId": "2135093601",
                        "name": "Xiaowei Chen"
                    },
                    {
                        "authorId": "2053947123",
                        "name": "Li Sheng"
                    },
                    {
                        "authorId": "2044335199",
                        "name": "HeFang Jiang"
                    },
                    {
                        "authorId": "2135935884",
                        "name": "Qihang Gong"
                    },
                    {
                        "authorId": "2143205099",
                        "name": "Sijian Li"
                    },
                    {
                        "authorId": "66277677",
                        "name": "Y. Jingfeng"
                    },
                    {
                        "authorId": "9094534",
                        "name": "Zhou Shangli"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[25] proposed a metric-learning-based pre-text task for time series."
            ],
            "citingPaper": {
                "paperId": "61c9bc8451d803f700e1db8b158263a1ff5f2dee",
                "externalIds": {
                    "ArXiv": "2108.08721",
                    "DBLP": "journals/corr/abs-2108-08721",
                    "DOI": "10.36001/ijphm.2022.v13i1.3096",
                    "CorpusId": 237213314
                },
                "corpusId": 237213314,
                "publicationVenue": {
                    "id": "423a93aa-9cad-4bb5-8756-fb1a2bfaaec4",
                    "name": "International Journal of Prognostics and Health Management",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Progn Health Manag"
                    ],
                    "issn": "2153-2648",
                    "url": "http://www.ijphm.org/",
                    "alternate_urls": [
                        "http://www.phmsociety.org/journal"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/61c9bc8451d803f700e1db8b158263a1ff5f2dee",
                "title": "Improving Semi-Supervised Learning for Remaining Useful Lifetime Estimation Through Self-Supervision",
                "abstract": "RUL estimation plays a vital role in effectively scheduling maintenance operations. Unfortunately, it suffers from a severe data imbalance where data from machines near their end of life is rare. Additionally, the data produced by a machine can only be labeled after the machine failed. Both of these points make using data-driven methods for RUL estimation difficult. Semi-Supervised Learning (SSL) can incorporate the unlabeled data produced by machines that did not yet fail into data-driven methods. Previous work on SSL evaluated approaches under unrealistic conditions where the data near failure was still available. Even so, only moderate improvements were made. This paper defines more realistic evaluation conditions and proposes a novel SSL approach based on self-supervised pre-training. The method can outperform two competing approaches from the literature and the supervised baseline on the NASA Commercial Modular Aero-Propulsion System Simulation dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1455840375",
                        "name": "Tilman Krokotsch"
                    },
                    {
                        "authorId": "32696265",
                        "name": "M. Knaak"
                    },
                    {
                        "authorId": "1735865",
                        "name": "C. G\u00fchmann"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "Comprehensive experiments verify that, compared to PAA and DEA generated by other SOTA architectures, including FDJNet [17], TimeNet [31] and InceptionTime [15], theDEA generated by SEAnet is more effective in preserving the original pairwise distances in the lower-dimensional summarized space.",
                "Unlike most existing encoders with linear final layers [17], the SEAnet encoder is finalized by LayerNorm2, which is specifically designed using the SoS preservation principle.",
                "TimeNet [31] and FDJNet [17] are two SOTA architectures for data series representation learning.",
                "[Methods] We evaluated the SEAnet-generated DEA and its applications in data series similarity search against PAA and DEA generated by SEAnet-nD (a simplified version of SEAnet), and our adaptations of FDJNet [17], TimeNet [31], and InceptionTime [15].",
                "On the other hand, few recent works [17, 31] focus on data series representation learning, none of which targets similarity search.",
                "Unlike FDJNet [17] and other SOTA convolutional architectures for series embedding, SEAnet is composed of both an encoder and a decoder.",
                "Although encoder-only architectures is the popular choice [17], we argue (and experimentally verify) that the decoder is necessary in similarity search applications in order to regularize the DEAs, so that they are distinguishable among each other.",
                "Its success in data series has also been reported for speech recognition [27], data series classification [17] and many other applications.",
                "In contrast to existing convolutional autoencoders for series embedding [17], SEAnet comprises both an encoder and a decoder."
            ],
            "citingPaper": {
                "paperId": "49b45a191d4e4301d851fd2c894831abaad28135",
                "externalIds": {
                    "DBLP": "conf/kdd/WangP21",
                    "DOI": "10.1145/3447548.3467317",
                    "CorpusId": 236980002
                },
                "corpusId": 236980002,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/49b45a191d4e4301d851fd2c894831abaad28135",
                "title": "Deep Learning Embeddings for Data Series Similarity Search",
                "abstract": "A key operation for the (increasingly large) data series collection analysis is similarity search. According to recent studies, SAX-based indexes offer state-of-the-art performance for similarity search tasks. However, their performance lags under high-frequency, weakly correlated, excessively noisy, or other dataset-specific properties. In this work, we propose Deep Embedding Approximation (DEA), a novel family of data series summarization techniques based on deep neural networks. Moreover, we describe SEAnet, a novel architecture especially designed for learning DEA, that introduces the Sum of Squares preservation property into the deep network design. Finally, we propose a new sampling strategy, SEASam, that allows SEAnet to effectively train on massive datasets. Comprehensive experiments on 7 diverse synthetic and real datasets verify the advantages of DEA learned using SEAnet, when compared to other state-of-the-art traditional and DEA solutions, in providing high-quality data series summarizations and similarity search results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116068893",
                        "name": "Qitong Wang"
                    },
                    {
                        "authorId": "1725167",
                        "name": "Themis Palpanas"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "In the past years, methods such as (Salinas et al., 2020; Franceschi et al., 2019; Kurle et al., 2020; de B\u00e9zenac et al., 2020; Oreshkin et al., 2020a; Rasul et al., 2021; Cui et al., 2016; Wang et al., 2017) have consistently showcased the effectiveness of deep learning in time series analysis tasks.",
                ", 2016), ContextFID, leveraging unsupervised time series embeddings (Franceschi et al., 2019).",
                "Additionally, we suggest a Frechet Inception distance-like score that is based on unsupervised time series embeddings (Franceschi et al., 2019).",
                "Convolutional architectures are able to learn relevant features from the raw time series data (van den Oord et al., 2016; Bai et al., 2018; Franceschi et al., 2019), but are ultimately limited to local receptive fields and can only capture long-range dependencies via many stacks of convolutional layers.",
                "One way to achieve longer realistic synthetic time series is by employing convolutional (van den Oord et al., 2016; Bai et al., 2018; Franceschi et al., 2019) and self-attention architectures (Vaswani et al."
            ],
            "citingPaper": {
                "paperId": "68043cccd2b620d44d6ffd4983b55060966acb23",
                "externalIds": {
                    "ArXiv": "2108.00981",
                    "CorpusId": 236777112
                },
                "corpusId": 236777112,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/68043cccd2b620d44d6ffd4983b55060966acb23",
                "title": "PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series",
                "abstract": "Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in two downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet-Inception Distance-like score, Context-FID, assessing the quality of synthetic time series samples. In our downstream tasks, we find that the lowest scoring models correspond to the best-performing ones. Therefore, Context-FID could be a useful tool to develop time series GAN models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2123124547",
                        "name": "Jeha Paul"
                    },
                    {
                        "authorId": "2122366053",
                        "name": "Bohlke-Schneider Michael"
                    },
                    {
                        "authorId": "2122184015",
                        "name": "Mercado Pedro"
                    },
                    {
                        "authorId": "2122364729",
                        "name": "Kapoor Shubham"
                    },
                    {
                        "authorId": "2122365235",
                        "name": "Singh Nirwan Rajbir"
                    },
                    {
                        "authorId": "2122347487",
                        "name": "Flunkert Valentin"
                    },
                    {
                        "authorId": "2122353127",
                        "name": "Gasthaus Jan"
                    },
                    {
                        "authorId": "2122348317",
                        "name": "Januschowski Tim"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ef6cfc0c0f929409d6068ed009cbf9a8c652033d",
                "externalIds": {
                    "MAG": "3202864310",
                    "DOI": "10.18429/JACOW-IPAC2021-MOPAB344",
                    "CorpusId": 244236869
                },
                "corpusId": 244236869,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ef6cfc0c0f929409d6068ed009cbf9a8c652033d",
                "title": "Machine Learning Models for Breakdown Prediction in RF Cavities for Accelerators",
                "abstract": "Radio Frequency (RF) breakdowns are one of the most prevalent limits in RF cavities for particle accelerators. During a breakdown, field enhancement associated with small deformations on the cavity surface results in electrical arcs. Such arcs degrade a passing beam and if they occur frequently, they can cause irreparable damage to the RF cavity surface. In this paper, we propose a machine learning approach to predict the occurrence of breakdowns in CERN\u2019s Compact LInear Collider (CLIC) accelerating structures. We discuss state-of-the-art algorithms for data exploration with unsupervised machine learning, breakdown prediction with supervised machine learning, and result validation with Explainable-Artificial Intelligence (Explainable AI). By interpreting the model parameters of various approaches, we go further in addressing opportunities to elucidate the physics of a breakdown and improve accelerator reliability and operation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8208406",
                        "name": "Christoph Obermair"
                    },
                    {
                        "authorId": "73680784",
                        "name": "A. Apollonio"
                    },
                    {
                        "authorId": "1397029595",
                        "name": "T. Cartier-Michaud"
                    },
                    {
                        "authorId": "94377792",
                        "name": "N. Lasheras"
                    },
                    {
                        "authorId": "51212658",
                        "name": "L. Felsberger"
                    },
                    {
                        "authorId": "46603375",
                        "name": "W. Millar"
                    },
                    {
                        "authorId": "1691836",
                        "name": "F. Pernkopf"
                    },
                    {
                        "authorId": "2574955",
                        "name": "W. Wuensch"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Similarly, the work [7] builds a temporal convolutional network based encoder-only structure using negative sampling and triple loss for representation learning.",
                "TCN is orginally proposed in work [15] and popularly applied in various sequence modeling tasks in works [3], [7].",
                "1) Our model is different from traditional TCN in works [3], [7] in two aspects.",
                "Different from works [3], [7] that take the whole MTS X(\u2217, t1 : tn\u22121) \u2208 Rm\u00d7(n\u22121) as input, our TCN takes X(i, t1 : tn\u22121) \u2208 R1\u00d7(n\u22121) for i = 1, 2, .",
                "Recent deep learning methods, such as [7], [11], explore similarities on low-dimensional embeddings through autoencoder structures.",
                "5) TCN [7] constructs an encoder-only architecture using TCN with triple loss and negative sampling to generate representation embeddings."
            ],
            "citingPaper": {
                "paperId": "ad1ed64da9cae902a36f1f434a20bcccb808c52a",
                "externalIds": {
                    "DBLP": "conf/ijcnn/XuHY21",
                    "DOI": "10.1109/IJCNN52387.2021.9533427",
                    "CorpusId": 237598629
                },
                "corpusId": 237598629,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/ad1ed64da9cae902a36f1f434a20bcccb808c52a",
                "title": "A Deep Neural Network for Multivariate Time Series Clustering with Result Interpretation",
                "abstract": "In today's industrial and scientific arenas, large quantities of multivariate time series data are generated without labels. Clustering such data is an important but challenging task due to complex variable associations. Unlike other previous efforts, this work explicitly explores variable associations through learning variable association graphs for each cluster. This is achieved through time series autoregression by a multi-path neural network, where each path corresponds to one cluster. The learned variable association graphs can be used to interpret how one cluster differs from another. Experiments demonstrate our framework's effectiveness on clustering and result interpretability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1999184471",
                        "name": "Chenxiao Xu"
                    },
                    {
                        "authorId": "143995146",
                        "name": "Hao Huang"
                    },
                    {
                        "authorId": "2282774",
                        "name": "Shinjae Yoo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "sequence to a sequence of the same length such that the i output sequence is calculated using the values up till i element of the input [2].",
                "Recent work proposes different methods to learn useful representations of the time series data in an unsupervised way that can be leveraged to perform well on downstream tasks such as classification [2], [3]."
            ],
            "citingPaper": {
                "paperId": "7af9eb636ebe9da4aac10314e43da234e31ad535",
                "externalIds": {
                    "DBLP": "conf/ijcnn/BansalBMA21",
                    "DOI": "10.1109/IJCNN52387.2021.9534469",
                    "CorpusId": 237598009
                },
                "corpusId": 237598009,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/7af9eb636ebe9da4aac10314e43da234e31ad535",
                "title": "Systematic Generalization in Neural Networks-based Multivariate Time Series Forecasting Models",
                "abstract": "Systematic generalization aims to evaluate reasoning about novel combinations from known components, an intrinsic property of human cognition. In this work, we study systematic generalization of Neural Networks (NNs) in forecasting future time series of dependent variables in a dynamical system, conditioned on past time series of dependent variables, and past and future control variables. We focus on systematic generalization wherein the NN-based forecasting model should perform well on previously unseen combinations or regimes of control variables after being trained on a limited set of the possible regimes. For NNs to depict such out-of-distribution generalization, they should be able to disentangle the various dependencies between control variables and dependent variables. We hypothesize that a modular NN architecture guided by the readily-available knowledge of independence of control variables as a potentially useful inductive bias to this end. Through extensive empirical evaluation on a toy dataset and a simulated electric motor dataset, we show that our proposed modular NN architecture serves as a simple yet highly effective inductive bias that enabling better forecasting of the dependent variables up to large horizons in contrast to standard NNs, and indeed capture the true dependency relations between the dependent and the control variables.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "103404553",
                        "name": "Hritik Bansal"
                    },
                    {
                        "authorId": "1702999721",
                        "name": "Gantavya Bhatt"
                    },
                    {
                        "authorId": "143898225",
                        "name": "Pankaj Malhotra"
                    },
                    {
                        "authorId": "2127763983",
                        "name": "Prathosh Ap"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "46bdaaedc16ad932f9fac0642d94817bb0e4df09",
                "externalIds": {
                    "ArXiv": "2107.07702",
                    "DBLP": "conf/ijcai/CarmonaAFG22",
                    "DOI": "10.24963/ijcai.2022/394",
                    "CorpusId": 236034302
                },
                "corpusId": 236034302,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/46bdaaedc16ad932f9fac0642d94817bb0e4df09",
                "title": "Neural Contextual Anomaly Detection for Time Series",
                "abstract": "We introduce Neural Contextual Anomaly Detection (NCAD), a framework for anomaly detection on time series that scales seamlessly from the unsupervised to supervised setting, and is applicable to both univariate and multivariate time series. This is achieved by combining recent developments in representation learning for multivariate time series, with techniques for deep anomaly detection originally developed for computer vision that we tailor to the time series setting. Our window-based approach facilitates learning the boundary between normal and anomalous classes by injecting generic synthetic anomalies into the available data. NCAD can effectively take advantage of domain knowledge and of any available training labels. We demonstrate empirically on standard benchmark datasets that our approach obtains a state-of-the-art performance in the supervised, semi-supervised, and unsupervised settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40341648",
                        "name": "Chris U. Carmona"
                    },
                    {
                        "authorId": "150098869",
                        "name": "Franccois-Xavier Aubet"
                    },
                    {
                        "authorId": "2067154581",
                        "name": "Valentin Flunkert"
                    },
                    {
                        "authorId": "2113062",
                        "name": "Jan Gasthaus"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "47e0dab08c920b589a9ddd11643c694d47ccd1c4",
                "externalIds": {
                    "ArXiv": "2106.14112",
                    "DBLP": "conf/ijcai/Eldele0C000G21",
                    "DOI": "10.24963/ijcai.2021/324",
                    "CorpusId": 235658361
                },
                "corpusId": 235658361,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/47e0dab08c920b589a9ddd11643c694d47ccd1c4",
                "title": "Time-Series Representation Learning via Temporal and Contextual Contrasting",
                "abstract": "Learning decent representations from unlabeled time-series data with temporal dynamics is a very challenging task. In this paper, we propose an unsupervised Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC), to learn time-series representation from unlabeled data. First, the raw time-series data are transformed into two different yet correlated views by using weak and strong augmentations. Second, we propose a novel temporal contrasting module to learn robust temporal representations by designing a tough cross-view prediction task. Last, to further learn discriminative representations, we propose a contextual contrasting module built upon the contexts from the temporal contrasting module. It attempts to maximize the similarity among different contexts of the same sample while minimizing similarity among contexts of different samples. Experiments have been carried out on three real-world time-series datasets. The results manifest that training a linear classifier on top of the features learned by our proposed TS-TCC performs comparably with the supervised training. Additionally, our proposed TS-TCC shows high efficiency in few-labeled data and transfer learning scenarios. The code is publicly available at https://github.com/emadeldeen24/TS-TCC.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2086836960",
                        "name": "Emadeldeen Eldele"
                    },
                    {
                        "authorId": "122101015",
                        "name": "Mohamed Ragab"
                    },
                    {
                        "authorId": "48354147",
                        "name": "Zhenghua Chen"
                    },
                    {
                        "authorId": "1390606776",
                        "name": "Min Wu"
                    },
                    {
                        "authorId": "145367091",
                        "name": "C. Kwoh"
                    },
                    {
                        "authorId": "2108674591",
                        "name": "Xiaoli Li"
                    },
                    {
                        "authorId": "2081050342",
                        "name": "Cuntai Guan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, ANNs such as the RNN and CNN, have been used to learn supervised [17] or unsupervised representation [18] for time series analysis."
            ],
            "citingPaper": {
                "paperId": "73723e9a76f093a8c95e2383bc7c502a3e7fa655",
                "externalIds": {
                    "MAG": "3174381582",
                    "DBLP": "journals/algorithms/OuyangHZZ21",
                    "DOI": "10.3390/a14070192",
                    "CorpusId": 237156204
                },
                "corpusId": 237156204,
                "publicationVenue": {
                    "id": "e95c8d18-09be-464f-a3cf-5b2637f0eff6",
                    "name": "Algorithms",
                    "type": "journal",
                    "issn": "1999-4893",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-150910",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-150910",
                        "http://www.mdpi.com/journal/algorithms",
                        "http://www.mdpi.com/journal/algorithms/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/73723e9a76f093a8c95e2383bc7c502a3e7fa655",
                "title": "Convolutional Neural Network with an Elastic Matching Mechanism for Time Series Classification",
                "abstract": "Recently, some researchers adopted the convolutional neural network (CNN) for time series classification (TSC) and have achieved better performance than most hand-crafted methods in the University of California, Riverside (UCR) archive. The secret to the success of the CNN is weight sharing, which is robust to the global translation of the time series. However, global translation invariance is not the only case considered for TSC. Temporal distortion is another common phenomenon besides global translation in time series. The scale and phase changes due to temporal distortion bring significant challenges to TSC, which is out of the scope of conventional CNNs. In this paper, a CNN architecture with an elastic matching mechanism, which is named Elastic Matching CNN (short for EM-CNN), is proposed to address this challenge. Compared with the conventional CNN, EM-CNN allows local time shifting between the time series and convolutional kernels, and a matching matrix is exploited to learn the nonlinear alignment between time series and convolutional kernels of the CNN. Several EM-CNN models are proposed in this paper based on diverse CNN models. The results for 85 UCR datasets demonstrate that the elastic matching mechanism effectively improves CNN performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3407127",
                        "name": "Kewei Ouyang"
                    },
                    {
                        "authorId": "144459059",
                        "name": "Yi Hou"
                    },
                    {
                        "authorId": null,
                        "name": "Shilin Zhou"
                    },
                    {
                        "authorId": "2115893561",
                        "name": "Ye Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "For example, T-Loss (Franceschi, Dieuleveut, and Jaggi 2019) performs instance-wise contrasting only at the instance level; TS-TCC (Eldele et al. 2021) applies instance-wise contrasting only at the timestamp level; TNC (Tonekaboni, Eytan, and Goldenberg 2021) encourages temporal local smoothness in\u2026",
                "As mentioned in section and , T-Loss, TS-TCC and TNC perform contrastive learning at only a certain level and impose strong inductive bias, such as transformation-invariance, to select positive pairs.",
                "TLoss (Franceschi, Dieuleveut, and Jaggi 2019) uses random sub-series from the original time series as positive samples.",
                "For example, T-Loss (Franceschi, Dieuleveut, and Jaggi 2019) performs instance-wise contrasting only at the instance level; TS-TCC (Eldele et al. 2021) applies instance-wise contrasting only at the timestamp level; TNC (Tonekaboni, Eytan, and Goldenberg 2021) encourages temporal local smoothness in a specific level of granularity.",
                "We replace our proposed contextual consistency, including the timestamp masking\nand random cropping, into temporal consistency (Tonekaboni, Eytan, and Goldenberg 2021) and subseries consistency (Franceschi, Dieuleveut, and Jaggi 2019).",
                "Previous works have adopted various selection strategies (Figure 2), which are summarized as follows:\n\u2022 Subseries consistency (Franceschi, Dieuleveut, and Jaggi 2019) encourages the representation of a time series to be closer to its sampled subseries.",
                "We then follow the same protocol\nas T-Loss (Franceschi, Dieuleveut, and Jaggi 2019) where an SVM classifier with RBF kernel is trained on top of the instance-level representations to make predictions.",
                "In addition, recent works (Eldele et al. 2021; Franceschi, Dieuleveut, and Jaggi 2019) employed the contrastive loss to learn the inherent structure of time series.",
                "Many studies (Tonekaboni, Eytan, and Goldenberg 2021; Franceschi, Dieuleveut, and Jaggi 2019; Wu et al. 2018) focused on learning instance-level representations, which described the whole segment of the input time series and have showed great success in tasks like clustering and classification.",
                "We conduct extensive experiments on time series classification to evaluate the instance-level representations, compared with other SOTAs of unsupervised time series representation, including T-Loss, TS-TCC (Eldele et al. 2021), TST (Zerveas et al. 2021) and TNC (Tonekaboni, Eytan, and Goldenberg 2021)."
            ],
            "citingPaper": {
                "paperId": "f79a319413b5c014e4a98bcd223e6f65e3f7901b",
                "externalIds": {
                    "ArXiv": "2106.10466",
                    "DBLP": "conf/aaai/YueWDYHTX22",
                    "DOI": "10.1609/aaai.v36i8.20881",
                    "CorpusId": 237497421
                },
                "corpusId": 237497421,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f79a319413b5c014e4a98bcd223e6f65e3f7901b",
                "title": "TS2Vec: Towards Universal Representation of Time Series",
                "abstract": "This paper presents TS2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, TS2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classification tasks to evaluate the quality of time series representations. As a result, TS2Vec achieves significant improvement over existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous SOTAs of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes SOTA results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113956163",
                        "name": "Zhihan Yue"
                    },
                    {
                        "authorId": "2115657798",
                        "name": "Yujing Wang"
                    },
                    {
                        "authorId": "2104164",
                        "name": "Juanyong Duan"
                    },
                    {
                        "authorId": "40047504",
                        "name": "Tianmeng Yang"
                    },
                    {
                        "authorId": "3261801",
                        "name": "Congrui Huang"
                    },
                    {
                        "authorId": "2054671931",
                        "name": "Yu Tong"
                    },
                    {
                        "authorId": "1388682473",
                        "name": "Bixiong Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Two particularly notable models here are Triplet loss [15] and Temporal Neighborhood Coding [34].",
                "Three were unsupervised time series models: Triplet Loss (tloss) [15], Temporal Neighborhood coding (tnc) [34], and Contrastive Predictive Coding (cpc) [36]."
            ],
            "citingPaper": {
                "paperId": "7a7145a21e51e6773fb7ecc90e1a978fa50bd5ea",
                "externalIds": {
                    "ArXiv": "2106.07369",
                    "DBLP": "journals/corr/abs-2106-07369",
                    "CorpusId": 235422065
                },
                "corpusId": 235422065,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7a7145a21e51e6773fb7ecc90e1a978fa50bd5ea",
                "title": "A Self-Supervised Framework for Function Learning and Extrapolation",
                "abstract": "Understanding how agents learn to generalize -- and, in particular, to extrapolate -- in high-dimensional, naturalistic environments remains a challenge for both machine learning and the study of biological agents. One approach to this has been the use of function learning paradigms, which allow peoples' empirical patterns of generalization for smooth scalar functions to be described precisely. However, to date, such work has not succeeded in identifying mechanisms that acquire the kinds of general purpose representations over which function learning can operate to exhibit the patterns of generalization observed in human empirical studies. Here, we present a framework for how a learner may acquire such representations, that then support generalization -- and extrapolation in particular -- in a few-shot fashion. Taking inspiration from a classic theory of visual processing, we construct a self-supervised encoder that implements the basic inductive bias of invariance under topological distortions. We show the resulting representations outperform those from other models for unsupervised time series learning in several downstream function learning tasks, including extrapolation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "72610330",
                        "name": "Simon Segert"
                    },
                    {
                        "authorId": "153564781",
                        "name": "J. Cohen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "We chose to consider as neighbors samples from a patient that are close in time motivated by (Banville et al., 2020) and (Franceschi et al., 2019) works.",
                "Both Banville et al. (2020) and Franceschi et al. (2019) approach this problem by enforcing temporal smoothness between contiguous samples, similar in spirit to Mikolov et al. (2013)."
            ],
            "citingPaper": {
                "paperId": "5c0afb8d45e7105a4f736d0eb1ba26cc021f602c",
                "externalIds": {
                    "DBLP": "conf/icml/YecheDLHR21",
                    "MAG": "3168006077",
                    "ArXiv": "2106.05142",
                    "CorpusId": 235376998
                },
                "corpusId": 235376998,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5c0afb8d45e7105a4f736d0eb1ba26cc021f602c",
                "title": "Neighborhood Contrastive Learning Applied to Online Patient Monitoring",
                "abstract": "Intensive care units (ICU) are increasingly looking towards machine learning for methods to provide online monitoring of critically ill patients. In machine learning, online monitoring is often formulated as a supervised learning problem. Recently, contrastive learning approaches have demonstrated promising improvements over competitive supervised benchmarks. These methods rely on well-understood data augmentation techniques developed for image data which do not apply to online monitoring. In this work, we overcome this limitation by supplementing time-series data augmentation techniques with a novel contrastive learning objective which we call neighborhood contrastive learning (NCL). Our objective explicitly groups together contiguous time segments from each patient while maintaining state-specific information. Our experiments demonstrate a marked improvement over existing work applying contrastive methods to medical time-series.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1390025042",
                        "name": "Hugo Yeche"
                    },
                    {
                        "authorId": "25927447",
                        "name": "Gideon Dresdner"
                    },
                    {
                        "authorId": "9557137",
                        "name": "Francesco Locatello"
                    },
                    {
                        "authorId": "2007577652",
                        "name": "Matthias Huser"
                    },
                    {
                        "authorId": "2097712597",
                        "name": "Gunnar Ratsch"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Franceschi et al. (2019) employs time-based negative sampling and a triplet loss to learn scalable representations for multivariate time series.",
                "Triplet-Loss (T-Loss), introduced in (Franceschi et al., 2019), which employs time-based negative sampling and a triplet loss to learn representations for time series windows."
            ],
            "citingPaper": {
                "paperId": "e19ca3c11fd45dfc9bc6e43ddf9b03b6c798e66d",
                "externalIds": {
                    "DBLP": "conf/iclr/TonekaboniEG21",
                    "ArXiv": "2106.00750",
                    "CorpusId": 235293778
                },
                "corpusId": 235293778,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e19ca3c11fd45dfc9bc6e43ddf9b03b6c798e66d",
                "title": "Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding",
                "abstract": "Time series are often complex and rich in information but sparsely labeled and therefore challenging to model. In this paper, we propose a self-supervised framework for learning generalizable representations for non-stationary time series. Our approach, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal's generative process to define neighborhoods in time with stationary properties. Using a debiased contrastive objective, our framework learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from the distribution of non-neighboring signals. Our motivation stems from the medical field, where the ability to model the dynamic nature of time series data is especially valuable for identifying, tracking, and predicting the underlying patients' latent states in settings where labeling data is practically impossible. We compare our method to recently developed unsupervised representation learning approaches and demonstrate superior performance on clustering and classification tasks for multiple datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "23152217",
                        "name": "S. Tonekaboni"
                    },
                    {
                        "authorId": "1954568",
                        "name": "D. Eytan"
                    },
                    {
                        "authorId": "49800482",
                        "name": "A. Goldenberg"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "76caa7edab7acbf503f6e3f657f010b437ccf485",
                "externalIds": {
                    "DOI": "10.1016/j.jfoodeng.2021.110477",
                    "CorpusId": 241961419
                },
                "corpusId": 241961419,
                "publicationVenue": {
                    "id": "4cf56510-7e32-4cbd-899d-bea584f9cad1",
                    "name": "Journal of Food Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "J Food Eng"
                    ],
                    "issn": "0260-8774",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/405862/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/journal-of-food-engineering/",
                        "http://www.sciencedirect.com/science/journal/02608774"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/76caa7edab7acbf503f6e3f657f010b437ccf485",
                "title": "Statistical and temporal analysis of a novel multivariate time series data for food engineering",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "94208019",
                        "name": "Alla Abdella"
                    },
                    {
                        "authorId": "89535221",
                        "name": "J. Brecht"
                    },
                    {
                        "authorId": "2237262",
                        "name": "Ismail Uysal"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Table 2 shows the results of our proposed method Pattern Discovery with Byte Pair Encoding (PD-BPE), time series embedding proposed in [10] shown here as TS-Embed, and Rocket [7], for IGTB constructs.",
                "Another notable work is an unsupervised embedding approach proposed in [10]."
            ],
            "citingPaper": {
                "paperId": "9da25b947a76fb7fbbcba8bc6592edb2c0a8bb0b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-00614",
                    "ArXiv": "2106.00614",
                    "CorpusId": 235265824
                },
                "corpusId": 235265824,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9da25b947a76fb7fbbcba8bc6592edb2c0a8bb0b",
                "title": "Pattern Discovery in Time Series with Byte Pair Encoding",
                "abstract": "The growing popularity of wearable sensors has generated large quantities of temporal physiological and activity data. Ability to analyze this data offers new opportunities for real-time health monitoring and forecasting. However, temporal physiological data presents many analytic challenges: the data is noisy, contains many missing values, and each series has a different length. Most methods proposed for time series analysis and classification do not handle datasets with these characteristics nor do they offer interpretability and explainability, a critical requirement in the health domain. We propose an unsupervised method for learning representations of time series based on common patterns identified within them. The patterns are, interpretable, variable in length, and extracted using Byte Pair Encoding compression technique. In this way the method can capture both long-term and short-term dependencies present in the data. We show that this method applies to both univariate and multivariate time series and beats state-of-the-art approaches on a real world dataset collected from wearable sensors.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3362066",
                        "name": "N. Tavabi"
                    },
                    {
                        "authorId": "1782658",
                        "name": "Kristina Lerman"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026\u03b8, \u03b8\u0303) 12: \u03b8 \u2032 \u2190 \u03b8 \u2212 \u03b7\u2207\u03b8J (\u03b8) 13: end while\nlize an original sampling strategy combined with the consistency training [Bachman et al., 2014; Laine and Aila, 2017; Franceschi et al., 2019; Xie et al., 2020] on numerous unlabeled time series to constrain model predictions to be invariant to\u2026"
            ],
            "citingPaper": {
                "paperId": "08c1e516852c15d6160ea0d3038139b3ade63707",
                "externalIds": {
                    "DBLP": "conf/aaai/ChenZ022",
                    "ArXiv": "2105.08643",
                    "DOI": "10.1609/aaai.v36i6.20584",
                    "CorpusId": 234763295
                },
                "corpusId": 234763295,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/08c1e516852c15d6160ea0d3038139b3ade63707",
                "title": "ASM2TV: An Adaptive Semi-Supervised Multi-Task Multi-View Learning Framework",
                "abstract": "Many real-world scenarios, such as human activity recognition (HAR) in IoT, can be formalized as a multi-task multi-view learning problem. Each specific task consists of multiple shared feature views collected from multiple sources, either homogeneous or heterogeneous. Common among recent approaches is to employ a typical hard/soft sharing strategy at the initial phase separately for each view across tasks to uncover common knowledge, underlying the assumption that all views are conditionally independent. On the one hand, multiple views across tasks possibly relate to each other under practical situations. On the other hand, supervised methods might be insufficient when labeled data is scarce. To tackle these challenges, we introduce a novel framework ASM2TV for semi-supervised multi-task multi-view learning. We present a new perspective named gating control policy, a learnable task-view-interacted sharing policy that adaptively selects the most desirable candidate shared block for any view across any task, which uncovers more fine-grained task-view-interacted relatedness and improves inference efficiency. Significantly, our proposed gathering consistency adaption procedure takes full advantage of large amounts of unlabeled fragmented time-series, making it a general framework that accommodates a wide range of applications. Experiments on two diverse real-world HAR benchmark datasets collected from various subjects and sources demonstrate our framework's superiority over other state-of-the-arts. Anonymous codes are available at https://github.com/zachstarkk/ASM2TV.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40834657",
                        "name": "Zekai Chen"
                    },
                    {
                        "authorId": "2093924502",
                        "name": "Maiwang Shi"
                    },
                    {
                        "authorId": "2115478851",
                        "name": "Xiao Zhang"
                    },
                    {
                        "authorId": "36887453",
                        "name": "Haochao Ying"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "We recall that Franceschi et al. (Franceschi, Dieuleveut, and Jaggi 2019) followed the principle from word2vec (Mikolov et al. 2013), which makes the assumption that the representation of a word should meet two requirements: (i) the representation should be close to those near its context (Goldberg and Levy 2014), and (ii) it should be distant from those in a randomly chosen context, since they are probably different from the original word\u2019s context.",
                "Figure 4 shows the loss in using the original triplet loss (Franceschi, Dieuleveut, and Jaggi 2019) to learn shapelet representation.",
                "In (Chechik et al. 2010) and (Schroff, Kalenichenko, and Philbin 2015), only one positive sample and one negative\nsample are considered, whereas, in (Franceschi, Dieuleveut, and Jaggi 2019) and (Mikolov et al. 2013), one positive and several negative samples are considered.",
                "Comparison with other methods The experimental accuracies of the baseline results are all taken from the original papers (Bagnall et al. 2018), (Franceschi, Dieuleveut, and Jaggi 2019) and (Zhang et al. 2020), respectively.",
                "We recall that Franceschi et al. (Franceschi, Dieuleveut, and Jaggi 2019) followed the principle from word2vec (Mikolov et al. 2013), which makes the assumption that the representation of a word should meet two requirements: (i) the representation should be close to those near its context (Goldberg\u2026",
                "(Franceschi, Dieuleveut, and Jaggi 2019) applies one positive sample and several negative samples when training their neural network, then SVM is utilized to do the final classification.",
                "WEASEL-MUSE is a bag-of-pattern based approach with statistical feature selection, variable window lengths and SAX for MTSC.\n\u2022 Negative samples (NS) (Franceschi, Dieuleveut, and Jaggi 2019)."
            ],
            "citingPaper": {
                "paperId": "be818bd3db1928dcfe24f23ee7afaa09545f9a61",
                "externalIds": {
                    "DBLP": "conf/aaai/LiCXBCW21",
                    "DOI": "10.1609/aaai.v35i9.17018",
                    "CorpusId": 235306095
                },
                "corpusId": 235306095,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/be818bd3db1928dcfe24f23ee7afaa09545f9a61",
                "title": "ShapeNet: A Shapelet-Neural Network Approach for Multivariate Time Series Classification",
                "abstract": "Time series shapelets are short discriminative subsequences that recently have been found not only to be accurate but also interpretable for the classification problem of univariate time series (UTS). However, existing work on shapelets selection cannot be applied to multivariate time series classification (MTSC) since the candidate shapelets of MTSC may come from different variables of different lengths and thus cannot be directly compared. To address this challenge, in this paper, we propose a novel model called ShapeNet, which embeds shapelet candidates of different lengths into a unified space for shapelet selection. The network is trained using cluster-wise triplet loss, which considers the distance between anchor and multiple positive (negative) samples and the distance between positive (negative) samples, which are important for convergence. We compute representative and diversified final shapelets rather than directly using all the embeddings for model building to avoid a large fraction of non-discriminative shapelet candidates. We have conducted experiments on ShapeNet with competitive state-of-the-art and benchmark methods using UEA MTS datasets. The results show that the accuracy of ShapeNet is the best of all the methods compared. Furthermore, we illustrate the shapelets\u2019 interpretability with two case studies.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1998950772",
                        "name": "Guozhong Li"
                    },
                    {
                        "authorId": "2110585930",
                        "name": "Byron Choi"
                    },
                    {
                        "authorId": "9097705",
                        "name": "Jianliang Xu"
                    },
                    {
                        "authorId": "1730344",
                        "name": "S. Bhowmick"
                    },
                    {
                        "authorId": "1489467846",
                        "name": "Kwok-Pan Chun"
                    },
                    {
                        "authorId": "145803455",
                        "name": "G. Wong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "applied contrastive learning also successfully to timeseries data [14]."
            ],
            "citingPaper": {
                "paperId": "ba7f1eb2087958cc9b5bd9ac1e7168ff1765558c",
                "externalIds": {
                    "PubMedCentral": "8161334",
                    "DBLP": "journals/sensors/RombachMF21",
                    "DOI": "10.3390/s21103550",
                    "CorpusId": 235240164,
                    "PubMed": "34065164"
                },
                "corpusId": 235240164,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ba7f1eb2087958cc9b5bd9ac1e7168ff1765558c",
                "title": "Contrastive Learning for Fault Detection and Diagnostics in the Context of Changing Operating Conditions and Novel Fault Types",
                "abstract": "Reliable fault detection and diagnostics are crucial in order to ensure efficient operations in industrial assets. Data-driven solutions have shown great potential in various fields but pose many challenges in Prognostics and Health Management (PHM) applications: Changing external in-service factors and operating conditions cause variations in the condition monitoring (CM) data resulting in false alarms. Furthermore, novel types of faults can also cause variations in CM data. Since faults occur rarely in complex safety critical systems, a training dataset typically does not cover all possible fault types. To enable the detection of novel fault types, the models need to be sensitive to novel variations. Simultaneously, to decrease the false alarm rate, invariance to variations in CM data caused by changing operating conditions is required. We propose contrastive learning for the task of fault detection and diagnostics in the context of changing operating conditions and novel fault types. In particular, we evaluate how a feature representation trained by the triplet loss is suited to fault detection and diagnostics under the aforementioned conditions. We showcase that classification and clustering based on the learned feature representations are (1) invariant to changing operating conditions while also being (2) suited to the detection of novel fault types. Our evaluation is conducted on the bearing benchmark dataset provided by the Case Western Reserve University (CWRU).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51132870",
                        "name": "Katharina Rombach"
                    },
                    {
                        "authorId": "3393868",
                        "name": "Gabriel Michau"
                    },
                    {
                        "authorId": "2757308",
                        "name": "Olga Fink"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e49a9e1e3be36263d6ed2d5224d485cf3b9c5511",
                "externalIds": {
                    "DBLP": "journals/isci/ChenCYS21",
                    "MAG": "3114077264",
                    "DOI": "10.1016/j.ins.2020.12.062",
                    "CorpusId": 232060213
                },
                "corpusId": 232060213,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/e49a9e1e3be36263d6ed2d5224d485cf3b9c5511",
                "title": "A deep multi-task representation learning method for time series classification and retrieval",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1409961430",
                        "name": "Ling Chen"
                    },
                    {
                        "authorId": "145976145",
                        "name": "Donghui Chen"
                    },
                    {
                        "authorId": "2158029818",
                        "name": "Fan Yang"
                    },
                    {
                        "authorId": "4346236",
                        "name": "Jianling Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Exponentially-dilated convolutions were first proposed for semantic segmentation of images in [33] and have since seen success in other domains like audio synthesis [34] and time series classification [35, 36]."
            ],
            "citingPaper": {
                "paperId": "b726c4fe2d78cb903170b1db6a670969dd7d6de8",
                "externalIds": {
                    "DBLP": "journals/tbbis/LohrGK22",
                    "ArXiv": "2104.10489",
                    "DOI": "10.1109/tbiom.2022.3167633",
                    "CorpusId": 233324250
                },
                "corpusId": 233324250,
                "publicationVenue": {
                    "id": "7aeea00c-043e-48c2-99e1-1528d26d0032",
                    "name": "IEEE Transactions on Biometrics Behavior and Identity Science",
                    "alternate_names": [
                        "IEEE Trans Biom Behav Identity Sci"
                    ],
                    "issn": "2637-6407",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8423754&punumber=8423754"
                },
                "url": "https://www.semanticscholar.org/paper/b726c4fe2d78cb903170b1db6a670969dd7d6de8",
                "title": "Eye Know You: Metric Learning for End-to-End Biometric Authentication Using Eye Movements From a Longitudinal Dataset",
                "abstract": "The permanence of eye movements as a biometric modality remains largely unexplored in the literature. The present study addresses this limitation by evaluating a novel exponentially-dilated convolutional neural network for eye movement authentication using a recently proposed longitudinal dataset known as GazeBase. The network is trained using multi-similarity loss, which directly enables the enrollment and authentication of out-of-sample users. In addition, this study includes an exhaustive analysis of the effects of evaluating on various tasks and downsampling from 1000 Hz to several lower sampling rates. Our results reveal that reasonable authentication accuracy may be achieved even during both a low-cognitive-load task and at low sampling rates. Moreover, we find that eye movements are quite resilient against template aging after as long as 3 years.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "33476957",
                        "name": "D. Lohr"
                    },
                    {
                        "authorId": "145896428",
                        "name": "Henry K. Griffith"
                    },
                    {
                        "authorId": "1778350",
                        "name": "Oleg V. Komogortsev"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "36779e9b82fb8a880b2e98aa3b3c969f8b336be4",
                "externalIds": {
                    "DBLP": "conf/chil/ManduchiHFVRF21",
                    "DOI": "10.1145/3450439.3451872",
                    "CorpusId": 232154854
                },
                "corpusId": 232154854,
                "publicationVenue": {
                    "id": "67d171e0-fd12-4512-a35d-c4d7af1bd5b3",
                    "name": "ACM Conference on Health, Inference, and Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CHIL",
                        "ACM Conf Health Inference Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/36779e9b82fb8a880b2e98aa3b3c969f8b336be4",
                "title": "T-DPSOM: an interpretable clustering method for unsupervised learning of patient health states",
                "abstract": "Generating interpretable visualizations of multivariate time series in the intensive care unit is of great practical importance. Clinicians seek to condense complex clinical observations into intuitively understandable critical illness patterns, like failures of different organ systems. They would greatly benefit from a low-dimensional representation in which the trajectories of the patients' pathology become apparent and relevant health features are highlighted. To this end, we propose to use the latent topological structure of Self-Organizing Maps (SOMs) to achieve an interpretable latent representation of ICU time series and combine it with recent advances in deep clustering. Specifically, we (a) present a novel way to fit SOMs with probabilistic cluster assignments (PSOM), (b) propose a new deep architecture for probabilistic clustering (DPSOM) using a VAE, and (c) extend our architecture to cluster and forecast clinical states in time series (T-DPSOM). We show that our model achieves superior clustering performance compared to state-of-the-art SOM-based clustering methods while maintaining the favorable visualization properties of SOMs. On the eICU data-set, we demonstrate that T-DPSOM provides interpretable visualizations of patient state trajectories and uncertainty estimation. We show that our method rediscovers well-known clinical patient characteristics, such as a dynamic variant of the Acute Physiology And Chronic Health Evaluation (APACHE) score. Moreover, we illustrate how it can disentangle individual organ dysfunctions on disjoint regions of the two-dimensional SOM map.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "11180167",
                        "name": "L. Manduchi"
                    },
                    {
                        "authorId": "50988033",
                        "name": "Matthias H\u00fcser"
                    },
                    {
                        "authorId": "51232733",
                        "name": "M. Faltys"
                    },
                    {
                        "authorId": "8258126",
                        "name": "Julia E. Vogt"
                    },
                    {
                        "authorId": "2414086",
                        "name": "G. R\u00e4tsch"
                    },
                    {
                        "authorId": "41031794",
                        "name": "Vincent Fortuin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "[3] use a pure encoderbased network in combination with a so-called triplet loss function for classification on various reference time series.",
                "Methods of representation learning have recently emerged in the field of computer vision and speech recognition, enabling unsupervised feature extraction that outperforms the prediction performance of common manual and automated feature extraction methods in the case of only a small amount of existing labeled data [3]."
            ],
            "citingPaper": {
                "paperId": "cd9091ede5911ff2e1a412a8dc0565a676d24bc1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-02784",
                    "ArXiv": "2104.02784",
                    "CorpusId": 233168753
                },
                "corpusId": 233168753,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cd9091ede5911ff2e1a412a8dc0565a676d24bc1",
                "title": "Autoencoder-based Representation Learning from Heterogeneous Multivariate Time Series Data of Mechatronic Systems",
                "abstract": "Sensor and control data of modern mechatronic systems are often available as heterogeneous time series with different sampling rates and value ranges. Suitable classification and regression methods from the field of supervised machine learning already exist for predictive tasks, for example in the context of condition monitoring, but their performance scales strongly with the number of labeled training data. Their provision is often associated with high effort in the form of person-hours or additional sensors. In this paper, we present a method for unsupervised feature extraction using autoencoder networks that specifically addresses the heterogeneous nature of the database and reduces the amount of labeled training data required compared to existing methods. Three public datasets of mechatronic systems from different application domains are used to validate the results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1387974601",
                        "name": "Karl-Philipp Kortmann"
                    },
                    {
                        "authorId": "134391093",
                        "name": "M. Fehsenfeld"
                    },
                    {
                        "authorId": "3099334",
                        "name": "M. Wielitzka"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In order to learn these representations for driving time series, we have calculated a triplet loss function inspired by [21], [22] for driving time series, which is fully unsupervised."
            ],
            "citingPaper": {
                "paperId": "75a15da46a63a30e355debda6bbdeefd06645d50",
                "externalIds": {
                    "DBLP": "conf/wcnc/AzadaniB21",
                    "DOI": "10.1109/WCNC49053.2021.9417463",
                    "CorpusId": 233875102
                },
                "corpusId": 233875102,
                "publicationVenue": {
                    "id": "27235614-bd3e-4d6b-be38-5ede18f4e209",
                    "name": "IEEE Wireless Communications and Networking Conference",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Wirel Commun Netw Conf",
                        "WCNC",
                        "Wireless Communications and Networking Conference",
                        "Wirel Commun Netw Conf"
                    ],
                    "url": "http://www.ieee-wcnc.org/"
                },
                "url": "https://www.semanticscholar.org/paper/75a15da46a63a30e355debda6bbdeefd06645d50",
                "title": "Driver Identification Using Vehicular Sensing Data: A Deep Learning Approach",
                "abstract": "Driver identification plays a pivotal role in the design of advanced driver assistant systems. The continued development of in-vehicle networking systems, CAN-bus technology, and the ubiquitous presence of smartphones as well as the broad range of state-of-the-art sensors have paved the way to collect huge amount of data from both vehicles and drivers. This paper addresses the necessity of having a large volume of labeled data for driver identification and presents a novel methodology to identify drivers based on their driving behavior analysis. The proposed architecture benefits from triplet loss training for driving time series in an unsupervised approach. An encoder architecture based on exponentially dilated causal convolutions is employed to obtain the representations. An SVM classifier is then trained on top of the representations to predict the person behind the wheel. The experiment results demonstrated higher performance of the proposed methodology when compared to benchmark methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9244695",
                        "name": "Mozhgan Nasr Azadani"
                    },
                    {
                        "authorId": "1710379",
                        "name": "A. Boukerche"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9311bff96a6c59b2783cb881f12740b6c2185b84",
                "externalIds": {
                    "DOI": "10.1109/JSEN.2020.3046575",
                    "CorpusId": 232043612
                },
                "corpusId": 232043612,
                "publicationVenue": {
                    "id": "b210fd3d-11d7-478e-a0aa-7e3d2a4f482d",
                    "name": "IEEE Sensors Journal",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Sens J"
                    ],
                    "issn": "1530-437X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7361",
                    "alternate_urls": [
                        "http://ieee-sensors.org/sensors-journal/",
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?puNumber=7361",
                        "http://www.ieee-sensors.org/journals",
                        "https://ieee-sensors.org/sensors-journal/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9311bff96a6c59b2783cb881f12740b6c2185b84",
                "title": "Sense2Vec: Representation and Visualization of Multivariate Sensory Time Series Data",
                "abstract": "Processing multivariate sensory time-series with variable lengths is a challenging problem across different application domains due to the naturally complex, high-dimensional, and often non-stationary nature of the data. An excellent example can be found in the temperature-controlled transportation of goods where sensors could be placed in different locations along the supply chain and data could be coming from different shipments with different numbers of observations across time. In this paper, we propose a new approach (Sense2Vec) for processing variable-length sensory time-series data leveraging various similarity metrics between different time-series temperature profiles. The proposed algorithm is shown to be independent of the distance similarity measure (like dynamic time warping or Pearson\u2019s correlation coefficient) and provides better visualization and summarization of the multivariate raw time-series through representations that are robust to noise and outliers. Specifically, a moving clipping mechanism is used to create uniform sets of disjoint sensory recordings across multiple groups to calculate normalized similarity distances followed by a weighted fusion and concatenation to create a representative vector for each sensor group. The proposed algorithm is tested on a novel food transportation dataset which consists of temperature recordings from wireless sensor networks implemented on different shipments of perishable commodities across the United States. Graphical results show that the algorithm facilitates discovering similarities and discrepancies across the transportation process to find proper representations while reducing the time-series dimensionality for back-end applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "94208019",
                        "name": "Alla Abdella"
                    },
                    {
                        "authorId": "2237262",
                        "name": "Ismail Uysal"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "There is much interest in effective methods for classification of MTS data [3] across a broad range of application domains including finance [58], metereology [8], graph mining [55, 60], audio representation learning [17, 54], healthcare [13, 34], human activity recognition [38, 57], among others."
            ],
            "citingPaper": {
                "paperId": "9aaacc905b6bd3f76ffb3952a3a68449d06a9952",
                "externalIds": {
                    "DBLP": "conf/wsdm/HsiehWSH21",
                    "DOI": "10.1145/3437963.3441815",
                    "CorpusId": 231995040
                },
                "corpusId": 231995040,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/9aaacc905b6bd3f76ffb3952a3a68449d06a9952",
                "title": "Explainable Multivariate Time Series Classification: A Deep Neural Network Which Learns to Attend to Important Variables As Well As Time Intervals",
                "abstract": "Many real-world applications, e.g., healthcare, present multi-variate time series prediction problems. In such settings, in addition to the predictive accuracy of the models, model transparency and explainability are paramount. We consider the problem of building explainable classifiers from multi-variate time series data. A key criterion to understand such predictive models involves elucidating and quantifying the contribution of time varying input variables to the classification. Hence, we introduce a novel, modular, convolution-based feature extraction and attention mechanism that simultaneously identifies the variables as well as time intervals which determine the classifier output. We present results of extensive experiments with several benchmark data sets that show that the proposed method outperforms the state-of-the-art baseline methods on multi-variate time series classification task. The results of our case studies demonstrate that the variables and time intervals identified by the proposed method make sense relative to available domain knowledge.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2058059560",
                        "name": "Tsung-Yu Hsieh"
                    },
                    {
                        "authorId": "2893721",
                        "name": "Suhang Wang"
                    },
                    {
                        "authorId": "1474741004",
                        "name": "Yiwei Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Besides, feature subset selected by AgnoS impaired the results on all six models, which suggested that AgnoS might not work well on high-dimensional MTS dataset (Face Detection is 144-dimensional).",
                "Noticeably, combination of NFS and MCDCNN achieved an accuracy of 0.636 on the Face Detection dataset, exceeding the most recent work by more than 10%.",
                "Note that MCNN didn\u2019t converge on Face Detection dataset, thus it should be ignored.",
                "After ignoring MCNN on Face Detection dataset, the average Accuracy score of NFS is 0.595, exceeding that of AgnoS by 0.05.",
                "As for Face Detection, the highest accuracy of 0.636 was obtained by NFS+MCDCNN model trained with NFS feature subset.",
                "Face Detection This dataset is from the UEA MTS classification archive4.",
                "Besides the above supervised MTS classification/regression methods, unsupervised MTS representation learning has also been explored [20].",
                "2) Experimental results on four publicly-available datasets (OhioT1DM [5], Favorita [2], PhysioNet 2012 [3] and Face Detection [20] ) show that NFS can boost the performance of state-of-the-art MTS models including Resnet, MCNN, MCDCNN, T-leNet and Transformer by joint training and selecting discriminative features.",
                "Table IV summarizes the results obtained on PhysioNet 2012 and Face Detection.",
                "To validate the general learning capability of NSF, we consider four public MTS datasets, namely OhioT1DM, Favorita, PhysioNet 2012 and Face Detection, from varying domains."
            ],
            "citingPaper": {
                "paperId": "9b67d3eb9fbdf943fe4d1dac623d78b07cc59ea5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-06024",
                    "ArXiv": "2102.06024",
                    "DOI": "10.1109/ICDMW53433.2021.00132",
                    "CorpusId": 231880031
                },
                "corpusId": 231880031,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9b67d3eb9fbdf943fe4d1dac623d78b07cc59ea5",
                "title": "Feature Selection for Multivariate Time Series via Network Pruning",
                "abstract": "In recent years, there has been an ever increasing amount of multivariate time series (MTS) data in various domains, typically generated by a large family of sensors such as wearable devices. This has led to the development of novel learning methods on MTS data, with deep learning models dominating the most recent advancements. Prior literature has primarily focused on designing new network architectures for modeling temporal dependencies within MTS. However, a less studied challenge is associated with high dimensionality of MTS data. In this paper, we propose a novel neural component, namely Neural Feature Selector (NFS), as an end-2-end solution for feature selection in MTS data. Specifically, NFS is based on decomposed convolution design and includes two modules: firstly each feature stream1 within MTS is processed by a temporal CNN independently; then an aggregating CNN combines the processed streams to produce input for other downstream networks. We evaluated the proposed NFS model on four real-world MTS datasets and found that it achieves comparable results with state-of-the-art methods while providing the benefit of feature selection. Our paper also highlights the robustness and effectiveness of feature selection with NFS compared to using recent autoencoder-based methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144145089",
                        "name": "Kang Gu"
                    },
                    {
                        "authorId": "1918441",
                        "name": "Soroush Vosoughi"
                    },
                    {
                        "authorId": "10409573",
                        "name": "T. Prioleau"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Causal convolutions map a sequence to a sequence of the same length such that the i output sequence is calculated using the values up till i element of the input [2].",
                "Recent work proposes different methods to learn useful representations of the time series data in an unsupervised way that can be leveraged to perform well on downstream tasks such as classification [2], [3]."
            ],
            "citingPaper": {
                "paperId": "02dac573bff1d0620bb7412ed68681d21aee7e12",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-05602",
                    "ArXiv": "2102.05602",
                    "CorpusId": 231861659
                },
                "corpusId": 231861659,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/02dac573bff1d0620bb7412ed68681d21aee7e12",
                "title": "Systematic Generalization for Predictive Control in Multivariate Time Series",
                "abstract": "Prior work has focused on evaluating the ability of neural networks to reason about novel combinations from known components, an intrinsic property of human cognition. In this work, we aim to study systematic generalization in predicting future state trajectories of a dynamical system, conditioned on past states' trajectory (dependent variables), past and future actions (control variables). In our context, systematic generalization implies that a good model should perform well on all new combinations of future actions after being trained on all of them, but only on a limited set of their combinations. For models to generalize out-of-distribution to unseen action combinations, they should reason about the states and their dependency relation with the applied actions. We conduct a rigorous study of useful inductive biases that learn to predict the trajectories up to large horizons well, and capture true dependency relations between the states and the controls through our synthetic setup, and simulated data from electric motors.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "103404553",
                        "name": "Hritik Bansal"
                    },
                    {
                        "authorId": "1702999721",
                        "name": "Gantavya Bhatt"
                    },
                    {
                        "authorId": "143898225",
                        "name": "Pankaj Malhotra"
                    },
                    {
                        "authorId": "1941435689",
                        "name": "Prathosh A.P."
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "An overall visualization of important ECG segments was constructed with the following steps: (1) all median ECG beats and their corresponding per-patient normalized Guided Grad-CAM maps were aligned temporally by normalizing the PQ and QT intervals, (2) the mean and standard deviations of the ECG signal were derived within each group, (3) the proportion of the per-patient Guided Grad-CAM"
            ],
            "citingPaper": {
                "paperId": "7803250c405896a37ed88e80117564ce5fd7a466",
                "externalIds": {
                    "PubMedCentral": "7892204",
                    "DOI": "10.1161/CIRCEP.120.009056",
                    "CorpusId": 230782267,
                    "PubMed": "33401921"
                },
                "corpusId": 230782267,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7803250c405896a37ed88e80117564ce5fd7a466",
                "title": "Discovering and Visualizing Disease-Specific Electrocardiogram Features Using Deep Learning",
                "abstract": "Supplemental Digital Content is available in the text.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "83443172",
                        "name": "R. R. van de Leur"
                    },
                    {
                        "authorId": "145911739",
                        "name": "K. Taha"
                    },
                    {
                        "authorId": "2045206604",
                        "name": "M. N. Bos"
                    },
                    {
                        "authorId": "2322009",
                        "name": "J. F. van der Heijden"
                    },
                    {
                        "authorId": "144786228",
                        "name": "D. Gupta"
                    },
                    {
                        "authorId": "2131637",
                        "name": "M. Cramer"
                    },
                    {
                        "authorId": "4899169",
                        "name": "R. Hassink"
                    },
                    {
                        "authorId": "7269555",
                        "name": "P. van der Harst"
                    },
                    {
                        "authorId": "144507277",
                        "name": "P. Doevendans"
                    },
                    {
                        "authorId": "39458739",
                        "name": "F. Asselbergs"
                    },
                    {
                        "authorId": "5442699",
                        "name": "R. van Es"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2020) and time-series (Franceschi et al., 2019).",
                "\u2026methods show also strong success in natural language processing (Logeswaran & Lee, 2018; Mikolov et al., 2013; Devlin et al., 2018; van den Oord et al., 2018), video classification (Sun et al., 2019), reinforcement learning (Srinivas et al., 2020) and time-series (Franceschi et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "85813dc2c27f9feeb1f4aee6436d9ca853e151a7",
                "externalIds": {
                    "ArXiv": "2012.01064",
                    "MAG": "3107704755",
                    "DBLP": "journals/corr/abs-2012-01064",
                    "CorpusId": 227247954
                },
                "corpusId": 227247954,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/85813dc2c27f9feeb1f4aee6436d9ca853e151a7",
                "title": "About contrastive unsupervised representation learning for classification and its convergence",
                "abstract": "Contrastive representation learning has been recently proved to be very efficient for self-supervised training. These methods have been successfully used to train encoders which perform comparably to supervised training on downstream classification tasks. A few works have started to build a theoretical framework around contrastive learning in which guarantees for its performance can be proven. We provide extensions of these results to training with multiple negative samples and for multiway classification. Furthermore, we provide convergence guarantees for the minimization of the contrastive training error with gradient descent of an overparametrized deep neural encoder, and provide some numerical experiments that complement our theoretical findings",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1396994645",
                        "name": "Ibrahim Merad"
                    },
                    {
                        "authorId": "150353622",
                        "name": "Yiyang Yu"
                    },
                    {
                        "authorId": "1977383",
                        "name": "E. Bacry"
                    },
                    {
                        "authorId": "3172127",
                        "name": "St\u00e9phane Ga\u00efffas"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "In [9], an unsupervised scalable time series representation (USTR) is proposed using the notion of triplet loss.",
                "We do not find consistent representations with USTR.",
                "In all experiments, we compare Seq2Graph to three time series representation methods: a sequential autoencoder (SAE) [17], to the unsupervised scalable time series representation (USTR) [9] and to the sequence-to-VAR (Seq2VAR) [22].",
                "In particular, SAE and USTR completely miss the consistent ageing information, as expected from pattern-based approaches.",
                "From top to bottom: USTR [9], SAE [17], Seq2VAR [22], Seq2Graph.",
                "We see that Seq2Graph outperforms both SAE, USTR and Seq2VAR for unsupervised representation learning, when meaningful information is fully contained in the causality.",
                "In the current paper, we use SAE, USTR and Seq2VAR as comparative models in the experiment part.",
                "In [9], an unsupervised scalable time series"
            ],
            "citingPaper": {
                "paperId": "44ee628db6b804bde0771e8689017807299edcc0",
                "externalIds": {
                    "DBLP": "conf/icmla/PineauRB20",
                    "DOI": "10.1109/ICMLA51294.2020.00052",
                    "CorpusId": 232061816
                },
                "corpusId": 232061816,
                "publicationVenue": {
                    "id": "f6752838-f268-4a1b-87e7-c5f30a36713c",
                    "name": "International Conference on Machine Learning and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Mach Learn Appl",
                        "ICMLA"
                    ],
                    "url": "http://www.icmla-conference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/44ee628db6b804bde0771e8689017807299edcc0",
                "title": "Unsupervised ageing detection of mechanical systems on a causality graph",
                "abstract": "Multivariate time series (MTS) have specific features that complicate their analysis: interactions in space and time between the MTS components, variable length, absence of trivial alignment between samples and high dimensionality. Hence, finding a representation of MTS from which we can extract meaningful information is a challenging task. In general, specific assumptions are needed to obtain a valuable representation. In this paper, we assume that a dataset of MTS samples has an underlying causal structure that we can exploit to represent samples. Our contribution is a new representation framework that consists of first finding the overall causality graph in a studied dataset and then mapping each sample onto G to obtain a causality-based representation. Since causality isG an important feature underlying MTS data, we claim and show that representating samples on G is meaningful. We name this method Sequence-to-Graph (Seq2Graph). We apply Seq2Graph on health monitoring tasks, using two MTS datasets coming from ageing mechanical systems.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50988081",
                        "name": "Edouard Pineau"
                    },
                    {
                        "authorId": "3072865",
                        "name": "S. Razakarivony"
                    },
                    {
                        "authorId": "1774132",
                        "name": "T. Bonald"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "sentation Learning In recent years, self-supervised representation learning has been used to capture informative and compact representations of video [1, 33], image [8, 21], text[35], and time series [17, 31, 39, 40] data. 2.2.1 Contrastive Learning. Contrastive learning is an approach used to formulate what makes the samples in a dataset similar or dissimilar using a set of training instances composed of positiv",
                "s the first time contrastive learning has been used for change point detection. There is a few works that investigates the use of representation learning with multivariate time series. The authors of [17] proposed a general-purpose approach to learn representations of variable length time series using a deep dilated convolutional network (WaveNet [34]) and an unsupervised triplet loss function based o"
            ],
            "citingPaper": {
                "paperId": "854c93fdb747f1ed4905d0211e0419e91db27b45",
                "externalIds": {
                    "ArXiv": "2011.14097",
                    "DBLP": "journals/corr/abs-2011-14097",
                    "MAG": "3109518304",
                    "DOI": "10.1145/3442381.3449903",
                    "CorpusId": 227227883
                },
                "corpusId": 227227883,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/854c93fdb747f1ed4905d0211e0419e91db27b45",
                "title": "Time Series Change Point Detection with Self-Supervised Contrastive Predictive Coding",
                "abstract": "Change Point Detection (CPD) methods identify the times associated with changes in the trends and properties of time series data in order to describe the underlying behaviour of the system. For instance, detecting the changes and anomalies associated with web service usage, application usage or human behaviour can provide valuable insights for downstream modelling tasks. We propose a novel approach for self-supervised Time Series Change Point detection method based on Contrastive Predictive coding (TS \u2212 CP2). TS \u2212 CP2 is the first approach to employ a contrastive learning strategy for CPD by learning an embedded representation that separates pairs of embeddings of time adjacent intervals from pairs of interval embeddings separated across time. Through extensive experiments on three diverse, widely used time series datasets, we demonstrate that our method outperforms five state-of-the-art CPD methods, which include unsupervised and semi-supervised approaches. TS \u2212 CP2 is shown to improve the performance of methods that use either handcrafted statistical or temporal features by 79.4% and deep learning-based methods by 17.0% with respect to the F1-score averaged across the three datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1864633127",
                        "name": "Shohreh Deldari"
                    },
                    {
                        "authorId": "2143623845",
                        "name": "Daniel V. Smith"
                    },
                    {
                        "authorId": "1560895396",
                        "name": "Hao Xue"
                    },
                    {
                        "authorId": "2142031887",
                        "name": "F. Salim"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "For time series data such as audio and ECG, the metric learning based methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al., 2019; Saeed et al., 2020), or multi-task learning based methods that predict different handcrafted features such as MFCCs, prosody,\u2026",
                "Also, in Triplet Loss, a time-based negative sampling is used to capture the inter-sample temporal relation among time pieces sampled from the different time series, which is cannot directly and efficiently\ncapture the intra-sample temporal pattern of time series.",
                "More recently, in the time series analysis domain, some metric learning based self-supervised methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al., 2019; Saeed et al., 2020), or multi-task learning based self-supervised methods that predict different\u2026",
                "More recently, in the time series analysis domain, some metric learning based self-supervised methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al.",
                "Among those baselines, either global features (Deep InfoMax, Transformation, SimCLR, Relation) or local features (Triplet Loss, Deep InfoMax, Forecast) are considered during representation learning, they neglect the essential temporal information of time series except Triplet Loss and Forecast.",
                "Triplet Loss4 (Franceschi et al., 2019) We download the authors\u2019 official source code and use the same backbone as SelfTime, and set the number of negative samples as 10.",
                "For time series data such as audio and ECG, the metric learning based methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al.",
                "\u2022 Triplet Loss (Franceschi et al., 2019) is an unsupervised time series representation learning model that uses triplet loss to push a subsequence of time series close to its context and distant from a randomly chosen time series."
            ],
            "citingPaper": {
                "paperId": "8f80a30dbfffde78b0bd13f696b6a2cb3f7134f2",
                "externalIds": {
                    "MAG": "3109603638",
                    "DBLP": "journals/corr/abs-2011-13548",
                    "ArXiv": "2011.13548",
                    "CorpusId": 227210405
                },
                "corpusId": 227210405,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8f80a30dbfffde78b0bd13f696b6a2cb3f7134f2",
                "title": "Self-Supervised Time Series Representation Learning by Inter-Intra Relational Reasoning",
                "abstract": "Self-supervised learning achieves superior performance in many domains by extracting useful representations from the unlabeled data. However, most of traditional self-supervised methods mainly focus on exploring the inter-sample structure while less efforts have been concentrated on the underlying intra-temporal structure, which is important for time series data. In this paper, we present SelfTime: a general self-supervised time series representation learning framework, by exploring the inter-sample relation and intra-temporal relation of time series to learn the underlying structure feature on the unlabeled time series. Specifically, we first generate the inter-sample relation by sampling positive and negative samples of a given anchor sample, and intra-temporal relation by sampling time pieces from this anchor. Then, based on the sampled relation, a shared feature extraction backbone combined with two separate relation reasoning heads are employed to quantify the relationships of the sample pairs for inter-sample relation reasoning, and the relationships of the time piece pairs for intra-temporal relation reasoning, respectively. Finally, the useful representations of time series are extracted from the backbone under the supervision of relation reasoning heads. Experimental results on multiple real-world time series datasets for time series classification task demonstrate the effectiveness of the proposed method. Code and data are publicly available at https://haoyfan.github.io/.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "87646177",
                        "name": "Haoyi Fan"
                    },
                    {
                        "authorId": "2854772",
                        "name": "Fengbin Zhang"
                    },
                    {
                        "authorId": "35350470",
                        "name": "Yue Gao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "3d696143e7559adc2d8f0eb4035a787c4ee962af",
                "externalIds": {
                    "MAG": "3106597262",
                    "DOI": "10.1159/000512166",
                    "CorpusId": 228084586,
                    "PubMed": "33442581"
                },
                "corpusId": 228084586,
                "publicationVenue": {
                    "id": "fe4ce33d-a793-466b-a457-cfeb55706388",
                    "name": "Digital Biomarkers",
                    "alternate_names": [
                        "Digit Biomarkers"
                    ],
                    "issn": "2504-110X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1021963",
                    "alternate_urls": [
                        "https://www.karger.com/Journal/Home/271954",
                        "http://www.karger.com/?ISSN=2504-110X",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-1021963"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3d696143e7559adc2d8f0eb4035a787c4ee962af",
                "title": "Assessment of Fatigue Using Wearable Sensors: A Pilot Study",
                "abstract": "Background: Fatigue is a broad, multifactorial concept encompassing feelings of reduced physical and mental energy levels. Fatigue strongly impacts patient health-related quality of life across a huge range of conditions, yet, to date, tools available to understand fatigue are severely limited. Methods: After using a recurrent neural network-based algorithm to impute missing time series data form a multisensor wearable device, we compared supervised and unsupervised machine learning approaches to gain insights on the relationship between self-reported non-pathological fatigue and multimodal sensor data. Results: A total of 27 healthy subjects and 405 recording days were analyzed. Recorded data included continuous multimodal wearable sensor time series on physical activity, vital signs, and other physiological parameters, and daily questionnaires on fatigue. The best results were obtained when using the causal convolutional neural network model for unsupervised representation learning of multivariate sensor data, and random forest as a classifier trained on subject-reported physical fatigue labels (weighted precision of 0.70 \u00b1 0.03 and recall of 0.73 \u00b1 0.03). When using manually engineered features on sensor data to train our random forest (weighted precision of 0.70 \u00b1 0.05 and recall of 0.72 \u00b1 0.01), both physical activity (energy expenditure, activity counts, and steps) and vital signs (heart rate, heart rate variability, and respiratory rate) were important parameters to measure. Furthermore, vital signs contributed the most as top features for predicting mental fatigue compared to physical ones. These results support the idea that fatigue is a highly multimodal concept. Analysis of clusters from sensor data highlighted a digital phenotype indicating the presence of fatigue (95% of observations) characterized by a high intensity of physical activity. Mental fatigue followed similar trends but was less predictable. Potential future directions could focus on anomaly detection assuming longer individual monitoring periods. Conclusion: Taken together, these results are the first demonstration that multimodal digital data can be used to inform, quantify, and augment subjectively captured non-pathological fatigue measures.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2116665720",
                        "name": "Hongyu Luo"
                    },
                    {
                        "authorId": "2111249529",
                        "name": "P. Lee"
                    },
                    {
                        "authorId": "4889245",
                        "name": "I. Clay"
                    },
                    {
                        "authorId": "2034183898",
                        "name": "M. Jaggi"
                    },
                    {
                        "authorId": "2133553328",
                        "name": "Valeria De Luca"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Dataset Existing SOTA USRLFordA[13] InceptionTime[12] Combined (1NN)[13] OSCNN[53] Best: fcn-lstm[19] Vanilla:RNTransformer [18, 55] ResNetTransformer1 [18] ResNetTransformer2 [18] ResNetTransformer3 [18] Ours",
                "Dataset Classes SeriesLength Existing SOTA USRLFordA[13] InceptionTime[12] Combined (1NN)[13] OSCNN[53] Best: lstm-fcn [19] Vanilla:RNTransformer [18, 55] ResNetTransformer1 [18] ResNetTransformer2 [18] ResNetTransformer3 [18] Ours"
            ],
            "citingPaper": {
                "paperId": "056f51e121d1b461dc0e62694096be1695625607",
                "externalIds": {
                    "DBLP": "journals/isci/XiaoXXLDZ21",
                    "MAG": "3107468544",
                    "ArXiv": "2011.11829",
                    "DOI": "10.1016/J.INS.2021.04.053",
                    "CorpusId": 227151250
                },
                "corpusId": 227151250,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/056f51e121d1b461dc0e62694096be1695625607",
                "title": "RTFN: A Robust Temporal Feature Network for Time Series Classification",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50479843",
                        "name": "Zhiwen Xiao"
                    },
                    {
                        "authorId": "2152775867",
                        "name": "Xin Xu"
                    },
                    {
                        "authorId": "38092176",
                        "name": "Huanlai Xing"
                    },
                    {
                        "authorId": "2962874",
                        "name": "Shouxi Luo"
                    },
                    {
                        "authorId": "39301921",
                        "name": "Penglin Dai"
                    },
                    {
                        "authorId": "49294642",
                        "name": "Dawei Zhan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "There is much interest in effective methods for classification of MTS data [3] across a broad range of application domains including finance [60], metereology [8], graph mining [57, 62], audio representation learning [17, 56], healthcare [13, 33, 35], human activity recognition [28, 38, 59], among others."
            ],
            "citingPaper": {
                "paperId": "1fef3d26b37149b5095d8b415f1830faebbe5675",
                "externalIds": {
                    "ArXiv": "2011.11631",
                    "MAG": "3107504545",
                    "DBLP": "journals/corr/abs-2011-11631",
                    "CorpusId": 227151651
                },
                "corpusId": 227151651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1fef3d26b37149b5095d8b415f1830faebbe5675",
                "title": "Explainable Multivariate Time Series Classification: A Deep Neural Network Which Learns To Attend To Important Variables As Well As Informative Time Intervals",
                "abstract": "Time series data is prevalent in a wide variety of real-world applications and it calls for trustworthy and explainable models for people to understand and fully trust decisions made by AI solutions. We consider the problem of building explainable classifiers from multi-variate time series data. A key criterion to understand such predictive models involves elucidating and quantifying the contribution of time varying input variables to the classification. Hence, we introduce a novel, modular, convolution-based feature extraction and attention mechanism that simultaneously identifies the variables as well as time intervals which determine the classifier output. We present results of extensive experiments with several benchmark data sets that show that the proposed method outperforms the state-of-the-art baseline methods on multi-variate time series classification task. The results of our case studies demonstrate that the variables and time intervals identified by the proposed method make sense relative to available domain knowledge.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2058059560",
                        "name": "Tsung-Yu Hsieh"
                    },
                    {
                        "authorId": "2893721",
                        "name": "Suhang Wang"
                    },
                    {
                        "authorId": "1474741004",
                        "name": "Yiwei Sun"
                    },
                    {
                        "authorId": "145513516",
                        "name": "Vasant G Honavar"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "1561f38e65bc525ab12d31022244aaa7d5751f15",
                "externalIds": {
                    "DBLP": "journals/kbs/AnandN21",
                    "MAG": "3100594686",
                    "DOI": "10.1016/j.knosys.2020.106551",
                    "CorpusId": 228856383
                },
                "corpusId": 228856383,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1561f38e65bc525ab12d31022244aaa7d5751f15",
                "title": "DeLTa: Deep local pattern representation for time-series clustering and classification using visual perception",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "33195661",
                        "name": "G. Anand"
                    },
                    {
                        "authorId": "143658054",
                        "name": "R. Nayak"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "A multi-layer perceptron (MLP) with two hidden layers as well as temporal convolutional network (TCN) blocks, which have been shown to outperform RNNbased models in sequence modeling tasks (Bai, Kolter, and Koltun 2018; Franceschi, Dieuleveut, and Jaggi 2019)."
            ],
            "citingPaper": {
                "paperId": "4b29170f70a3a4bd72190b2f03f905742ff05d81",
                "externalIds": {
                    "MAG": "3097428244",
                    "DBLP": "conf/spaca/HeitzFFMRH21",
                    "ArXiv": "2011.00865",
                    "CorpusId": 226226701
                },
                "corpusId": 226226701,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4b29170f70a3a4bd72190b2f03f905742ff05d81",
                "title": "WRSE - a non-parametric weighted-resolution ensemble for predicting individual survival distributions in the ICU",
                "abstract": "Dynamic assessment of mortality risk in the intensive care unit (ICU) can be used to stratify patients, inform about treatment effectiveness or serve as part of an early-warning system. Static risk scoring systems, such as APACHE or SAPS, have recently been supplemented with data-driven approaches that track the dynamic mortality risk over time. Recent works have focused on enhancing the information delivered to clinicians even further by producing full survival distributions instead of point predictions or fixed horizon risks. In this work, we propose a non-parametric ensemble model, Weighted Resolution Survival Ensemble (WRSE), tailored to estimate such dynamic individual survival distributions. Inspired by the simplicity and robustness of ensemble methods, the proposed approach combines a set of binary classifiers spaced according to a decay function reflecting the relevance of short-term mortality predictions. Models and baselines are evaluated under weighted calibration and discrimination metrics for individual survival distributions which closely reflect the utility of a model in ICU practice. We show competitive results with state-of-the-art probabilistic models, while greatly reducing training time by factors of 2-9x.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2007566052",
                        "name": "Jonathan Heitz"
                    },
                    {
                        "authorId": "2091755593",
                        "name": "Joanna Ficek"
                    },
                    {
                        "authorId": "51232733",
                        "name": "M. Faltys"
                    },
                    {
                        "authorId": "145794857",
                        "name": "T. Merz"
                    },
                    {
                        "authorId": "152597562",
                        "name": "Gunnar R\u00e4tsch"
                    },
                    {
                        "authorId": "2007577652",
                        "name": "Matthias Huser"
                    }
                ]
            }
        },
        {
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "We can see the results in a comparison against two baselines and a novel SOTA deep-learning architecture by Franceschi, [8]."
            ],
            "citingPaper": {
                "paperId": "a5764c04be3bb6a0b418eac9243d67d4c8b54168",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-05056",
                    "ArXiv": "2010.05056",
                    "MAG": "3092421482",
                    "CorpusId": 222290695
                },
                "corpusId": 222290695,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a5764c04be3bb6a0b418eac9243d67d4c8b54168",
                "title": "TOTOPO: Classifying univariate and multivariate time series with Topological Data Analysis",
                "abstract": "This work is devoted to a comprehensive analysis of topological data analysis fortime series classification. Previous works have significant shortcomings, such aslack of large-scale benchmarking or missing state-of-the-art methods. In this work,we propose TOTOPO for extracting topological descriptors from different types ofpersistence diagrams. The results suggest that TOTOPO significantly outperformsexisting baselines in terms of accuracy. TOTOPO is also competitive with thestate-of-the-art, being the best on 20% of univariate and 40% of multivariate timeseries datasets. This work validates the hypothesis that TDA-based approaches arerobust to small perturbations in data and are useful for cases where periodicity andshape help discriminate between classes.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "138609337",
                        "name": "Polina Pilyugina"
                    },
                    {
                        "authorId": "1419470286",
                        "name": "Rodrigo Rivera-Castro"
                    },
                    {
                        "authorId": "1410503828",
                        "name": "Eugeny Burnaev"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Finally, Jansen et al. (2018) rely on a triplet loss and the idea of temporal proximity (the loss rewards similarity of representations between proximal segments and penalizes similarity between distal segments of the time series) for unsupervised representation learning of non-speech audio data.",
                "The dilation-CNN (Franceschi et al., 2019) and XGBoost, which performed best on the remaining 1 dataset, tied and on average ranked 3."
            ],
            "citingPaper": {
                "paperId": "2051548f7681c96d603de932ee23406c525276f9",
                "externalIds": {
                    "MAG": "3092172514",
                    "ArXiv": "2010.02803",
                    "DBLP": "conf/kdd/ZerveasJPBE21",
                    "DOI": "10.1145/3447548.3467401",
                    "CorpusId": 222142244
                },
                "corpusId": 222142244,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/2051548f7681c96d603de932ee23406c525276f9",
                "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning",
                "abstract": "We present a novel framework for multivariate time series representation learning based on the transformer encoder architecture. The framework includes an unsupervised pre-training scheme, which can offer substantial performance benefits over fully supervised learning on downstream tasks, both with but even without leveraging additional unlabeled data, i.e., by reusing the existing data samples. Evaluating our framework on several public multivariate time series datasets from various domains and with diverse characteristics, we demonstrate that it performs significantly better than the best currently available methods for regression and classification, even for datasets which consist of only a few hundred training samples. Given the pronounced interest in unsupervised learning for nearly all domains in the sciences and in industry, these findings represent an important landmark, presenting the first unsupervised method shown to push the limits of state-of-the-art performance for multivariate time series regression and classification.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "30647302",
                        "name": "George Zerveas"
                    },
                    {
                        "authorId": "1988817745",
                        "name": "Srideepika Jayaraman"
                    },
                    {
                        "authorId": "2059011837",
                        "name": "Dhaval Patel"
                    },
                    {
                        "authorId": "1804908",
                        "name": "A. Bhamidipaty"
                    },
                    {
                        "authorId": "30044743",
                        "name": "Carsten Eickhoff"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2019), USRLT (Franceschi et al., 2019) and BeatGAN (Zhou et al.",
                "\u2026of comparisons with related work We next provide a comprehensive comparison between the proposed framework and other state-of-the-art methods, including (WaRTEm (Mathew et al., 2019), DTCR (Ma et al., 2019), USRLT (Franceschi et al., 2019) and BeatGAN (Zhou et al., 2019)), as shown in Table 1.",
                "Summary of comparisons with related work We next provide a comprehensive comparison between the proposed framework and other state-of-the-art methods, including (WaRTEm (Mathew et al., 2019), DTCR (Ma et al., 2019), USRLT (Franceschi et al., 2019) and BeatGAN (Zhou et al., 2019)), as shown in Table 1."
            ],
            "citingPaper": {
                "paperId": "60773c01dd3c76be0e1d3bc429cf474e5f9d8519",
                "externalIds": {
                    "ArXiv": "2010.01596",
                    "MAG": "3091277188",
                    "DBLP": "journals/corr/abs-2010-01596",
                    "CorpusId": 222133265
                },
                "corpusId": 222133265,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/60773c01dd3c76be0e1d3bc429cf474e5f9d8519",
                "title": "TimeAutoML: Autonomous Representation Learning for Multivariate Irregularly Sampled Time Series",
                "abstract": "Multivariate time series (MTS) data are becoming increasingly ubiquitous in diverse domains, e.g., IoT systems, health informatics, and 5G networks. To obtain an effective representation of MTS data, it is not only essential to consider unpredictable dynamics and highly variable lengths of these data but also important to address the irregularities in the sampling rates of MTS. Existing parametric approaches rely on manual hyperparameter tuning and may cost a huge amount of labor effort. Therefore, it is desirable to learn the representation automatically and efficiently. To this end, we propose an autonomous representation learning approach for multivariate time series (TimeAutoML) with irregular sampling rates and variable lengths. As opposed to previous works, we first present a representation learning pipeline in which the configuration and hyperparameter optimization are fully automatic and can be tailored for various tasks, e.g., anomaly detection, clustering, etc. Next, a negative sample generation approach and an auxiliary classification task are developed and integrated within TimeAutoML to enhance its representation capability. Extensive empirical studies on real-world datasets demonstrate that the proposed TimeAutoML outperforms competing approaches on various tasks by a large margin. In fact, it achieves the best anomaly detection performance among all comparison algorithms on 78 out of all 85 UCR datasets, acquiring up to 20% performance improvement in terms of AUC score.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2055181125",
                        "name": "Yang Jiao"
                    },
                    {
                        "authorId": "1637401447",
                        "name": "Kai Yang"
                    },
                    {
                        "authorId": "10675825",
                        "name": "Shaoyu Dou"
                    },
                    {
                        "authorId": "2052216790",
                        "name": "Pan Luo"
                    },
                    {
                        "authorId": "8602668",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "2451800",
                        "name": "Dongjin Song"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c1e9efc95462e5d391ba6ec70cd52b987e1a0e64",
                "externalIds": {
                    "ArXiv": "2010.00567",
                    "DBLP": "phd/hal/Fawaz20",
                    "MAG": "3090683883",
                    "CorpusId": 222090378
                },
                "corpusId": 222090378,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c1e9efc95462e5d391ba6ec70cd52b987e1a0e64",
                "title": "Deep learning for time series classification",
                "abstract": "Time series analysis is a field of data science which is interested in analyzing sequences of numerical values ordered in time. Time series are particularly interesting because they allow us to visualize and understand the evolution of a process over time. Their analysis can reveal trends, relationships and similarities across the data. There exists numerous fields containing data in the form of time series: health care (electrocardiogram, blood sugar, etc.), activity recognition, remote sensing, finance (stock market price), industry (sensors), etc. Time series classification consists of constructing algorithms dedicated to automatically label time series data. The sequential aspect of time series data requires the development of algorithms that are able to harness this temporal property, thus making the existing off-the-shelf machine learning models for traditional tabular data suboptimal for solving the underlying task. In this context, deep learning has emerged in recent years as one of the most effective methods for tackling the supervised classification task, particularly in the field of computer vision. The main objective of this thesis was to study and develop deep neural networks specifically constructed for the classification of time series data. We thus carried out the first large scale experimental study allowing us to compare the existing deep methods and to position them compared other non-deep learning based state-of-the-art methods. Subsequently, we made numerous contributions in this area, notably in the context of transfer learning, data augmentation, ensembling and adversarial attacks. Finally, we have also proposed a novel architecture, based on the famous Inception network (Google), which ranks among the most efficient to date.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "19302914",
                        "name": "Hassan Ismail Fawaz"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Classical approaches in time series forecasting can roughly be split into three main categories: short-term predictors, often relying on historical tools [2], mid-term predictors exploiting deep learning to extract automatically robust features from a large period of time [8] and, finally, longterm predictors that correspond mainly to seasonality [9], [10]."
            ],
            "citingPaper": {
                "paperId": "65e6dabf04bd49ea92135498a7bf7d52618b9056",
                "externalIds": {
                    "DBLP": "conf/itsc/Cribier-Delande20",
                    "DOI": "10.1109/ITSC45102.2020.9294267",
                    "CorpusId": 229703200
                },
                "corpusId": 229703200,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/65e6dabf04bd49ea92135498a7bf7d52618b9056",
                "title": "Time series prediction & generation from disentangled latent factors: new opportunities for smart cities",
                "abstract": "The acceleration of urbanisation has brought many new challenges to cities around the world. Application range is wide, from air pollution to public transportation modelling. The availability of data pertaining to these issues has been growing fast in the last years, offering many opportunities to tackle those applications with machine learning algorithms. We propose an elegant and general architecture that is able to provide state of the art forecasting in several different domains. Our idea is the following: for many time-series, a number of factors, that often relate to the context they were created in, can influence the observed values, such as day or location. In this paper, we present a machine learning model that learns to represent and disentangle such factors. Our contribution is to provide an approach that works at different scales: on a short term basis (30 minutes to few hours) our deep neural network architecture delivers competitive forecasting in a classical setting; at the day/week/month level, we show that we can generate relevant time series associated with unknown contexts. To the best of our knowledge, this ambitious application has not been investigated until now.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1399324742",
                        "name": "P. Cribier-Delande"
                    },
                    {
                        "authorId": "2342900",
                        "name": "Raphael Puget"
                    },
                    {
                        "authorId": "2140018458",
                        "name": "C. No\u00fbs"
                    },
                    {
                        "authorId": "2232776",
                        "name": "V. Guigue"
                    },
                    {
                        "authorId": "8905591",
                        "name": "Ludovic Denoyer"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[8], we built an architecture composed of several causal convolutional blocks, transforming the 12\u00d7 L-sized ECG data to 216 L-dimensional feature maps, where each point in a feature map is based on a history of 383 sample points, including itself.",
                "However, the application of exponentially dilated causal convolutions is more sensible for time series, such as ECGs, as they take the temporal nature of the data into account and use increasing receptive fields from which ECG features could be extracted [8]."
            ],
            "citingPaper": {
                "paperId": "2b263df679c8f49121e1d3046dd66da3c5080781",
                "externalIds": {
                    "DBLP": "conf/cinc/BosLVGHDE20",
                    "DOI": "10.22489/CinC.2020.253",
                    "CorpusId": 227990816
                },
                "corpusId": 227990816,
                "publicationVenue": {
                    "id": "a95b95d8-e19b-4ae4-9b8e-e2e26a773a64",
                    "name": "Computers in cardiology",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Cardiol",
                        "Computing in cardiology",
                        "Computing in Cardiology",
                        "Comput cardiol"
                    ],
                    "issn": "0276-6574",
                    "alternate_issns": [
                        "2325-8861",
                        "2325-887X",
                        "2325-8853"
                    ],
                    "url": "http://www.cinc.org/",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000157"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2b263df679c8f49121e1d3046dd66da3c5080781",
                "title": "Automated Comprehensive Interpretation of 12-lead Electrocardiograms Using Pre-trained Exponentially Dilated Causal Convolutional Neural Networks",
                "abstract": "Correct interpretation of the electrocardiogram (ECG) is critical for the diagnosis of many cardiac diseases, and current computerized algorithms are not accurate enough to provide automated comprehensive interpretation of the ECG. This study aimed to develop and validate the use of a pre-trained exponentially dilated causal convolutional neural network for interpretation of the ECG as part of the 2020 Physionet/Computing in Cardiology Challenge. The network was pre-trained on a physician-annotated dataset of 254,044 12-lead ECGs. The weights of the pre-trained network were partially frozen, and the others were finetuned on the challenge dataset of 42,511 ECGs. 10-fold cross-validation was applied and the best performing model in each fold was selected and used to construct an ensemble. The proposed method yielded a cross-validated area under the receiver operating curve (AU-ROC) of 0.939 \u00b1 0.004 and a challenge score of 0.565 \u00b1 0.005. Evaluation on the hidden test set resulted in a score of 0.417, placing us 7th out of 41 in the official ranking (team name UMCUVA). We demonstrated that an ensemble of exponentially dilated causal convolutional networks and pre-training on a large dataset of ECGs from a different country and device manufacturer performs excellent for interpretation of ECGs.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2045206604",
                        "name": "M. N. Bos"
                    },
                    {
                        "authorId": "2051252014",
                        "name": "R. V. D. Leur"
                    },
                    {
                        "authorId": "2049089668",
                        "name": "Jeroen F. Vranken"
                    },
                    {
                        "authorId": "144786228",
                        "name": "D. Gupta"
                    },
                    {
                        "authorId": "4445582",
                        "name": "P. Harst"
                    },
                    {
                        "authorId": "144507277",
                        "name": "P. Doevendans"
                    },
                    {
                        "authorId": "2611355",
                        "name": "R. V. Es"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "TS shares similarities with the unsupervised metric learning approach of [40], however the sampling procedure and loss function both differ."
            ],
            "citingPaper": {
                "paperId": "fdde034d0d19be6fae4d768157e7ed53ec265741",
                "externalIds": {
                    "MAG": "3101658985",
                    "DBLP": "journals/corr/abs-2007-16104",
                    "ArXiv": "2007.16104",
                    "DOI": "10.1088/1741-2552/abca18",
                    "CorpusId": 220919935,
                    "PubMed": "33181507"
                },
                "corpusId": 220919935,
                "publicationVenue": {
                    "id": "aa06d038-4db2-4d34-a660-be35ff62d392",
                    "name": "Journal of Neural Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "J Neural Eng"
                    ],
                    "issn": "1741-2552",
                    "url": "http://iopscience.iop.org/1741-2552/"
                },
                "url": "https://www.semanticscholar.org/paper/fdde034d0d19be6fae4d768157e7ed53ec265741",
                "title": "Uncovering the structure of clinical EEG signals with self-supervised learning",
                "abstract": "Objective. Supervised learning paradigms are often limited by the amount of labeled data that is available. This phenomenon is particularly problematic in clinically-relevant data, such as electroencephalography (EEG), where labeling can be costly in terms of specialized expertise and human processing time. Consequently, deep learning architectures designed to learn on EEG data have yielded relatively shallow models and performances at best similar to those of traditional feature-based approaches. However, in most situations, unlabeled data is available in abundance. By extracting information from this unlabeled data, it might be possible to reach competitive performance with deep neural networks despite limited access to labels. Approach. We investigated self-supervised learning (SSL), a promising technique for discovering structure in unlabeled data, to learn representations of EEG signals. Specifically, we explored two tasks based on temporal context prediction as well as contrastive predictive coding on two clinically-relevant problems: EEG-based sleep staging and pathology detection. We conducted experiments on two large public datasets with thousands of recordings and performed baseline comparisons with purely supervised and hand-engineered approaches. Main results. Linear classifiers trained on SSL-learned features consistently outperformed purely supervised deep neural networks in low-labeled data regimes while reaching competitive performance when all labels were available. Additionally, the embeddings learned with each method revealed clear latent structures related to physiological and clinical phenomena, such as age effects. Significance. We demonstrate the benefit of SSL approaches on EEG data. Our results suggest that self-supervision may pave the way to a wider use of deep learning models on EEG data.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2154685",
                        "name": "Hubert J. Banville"
                    },
                    {
                        "authorId": "1471117518",
                        "name": "O. Chehab"
                    },
                    {
                        "authorId": "1791548",
                        "name": "Aapo Hyv\u00e4rinen"
                    },
                    {
                        "authorId": "3184164",
                        "name": "D. Engemann"
                    },
                    {
                        "authorId": "1797840",
                        "name": "Alexandre Gramfort"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In order to overcome the shortcomings of conventional autoregressive methods about the limitation of numbers to be simultaneously forecasted, matrix factorizationbased method [17], unsupervised scalable representation learning [18], DeepGLO that combines a global matrix factorization and a local temporal network [19] have been proposed."
            ],
            "citingPaper": {
                "paperId": "968e5c3b33265ee8541a78b6fb82c3c397211cc1",
                "externalIds": {
                    "MAG": "3046210600",
                    "DOI": "10.1007/s42452-020-03225-9",
                    "CorpusId": 225452144
                },
                "corpusId": 225452144,
                "publicationVenue": {
                    "id": "4766beb3-bec6-4b4a-a5fc-cf12ba2339b6",
                    "name": "SN Applied Sciences",
                    "alternate_names": [
                        "SN Appl Sci"
                    ],
                    "issn": "2523-3963",
                    "url": "https://www.springer.com/engineering/journal/42452"
                },
                "url": "https://www.semanticscholar.org/paper/968e5c3b33265ee8541a78b6fb82c3c397211cc1",
                "title": "Time series forecasting of agricultural product prices based on recurrent neural networks and its evaluation method",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2450175",
                        "name": "K. Kurumatani"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Convolutional architectures are able to learn relevant features from the raw time series data [11, 62, 183], but are ultimately limited to local receptive fields and can only capture long-range dependencies via many stacks of convolutional layers.",
                "One way to achieve longer realistic synthetic time series is by employing convolutional [11, 62, 183] and self-attention architectures [184]."
            ],
            "citingPaper": {
                "paperId": "2c2be8306911e0464365d9b4576445d833afddc1",
                "externalIds": {
                    "ArXiv": "2004.10240",
                    "DBLP": "journals/csur/BenidisRFWMTGBS23",
                    "DOI": "10.1145/3533382",
                    "CorpusId": 248892945
                },
                "corpusId": 248892945,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2c2be8306911e0464365d9b4576445d833afddc1",
                "title": "Deep Learning for Time Series Forecasting: Tutorial and Literature Survey",
                "abstract": "Deep learning based forecasting methods have become the methods of choice in many applications of time series prediction or forecasting often outperforming other approaches. Consequently, over the last years, these methods are now ubiquitous in large-scale industrial forecasting applications and have consistently ranked among the best entries in forecasting competitions (e.g., M4 and M5). This practical success has further increased the academic interest to understand and improve deep forecasting methods. In this article we provide an introduction and overview of the field: We present important building blocks for deep forecasting in some depth; using these building blocks, we then survey the breadth of the recent deep forecasting literature.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1915597",
                        "name": "Konstantinos Benidis"
                    },
                    {
                        "authorId": "26320653",
                        "name": "Syama Sundar Rangapuram"
                    },
                    {
                        "authorId": "2067154581",
                        "name": "Valentin Flunkert"
                    },
                    {
                        "authorId": "49416149",
                        "name": "Bernie Wang"
                    },
                    {
                        "authorId": "40792304",
                        "name": "Danielle C. Maddix"
                    },
                    {
                        "authorId": "2155891002",
                        "name": "Caner Turkmen"
                    },
                    {
                        "authorId": "2113062",
                        "name": "Jan Gasthaus"
                    },
                    {
                        "authorId": "1405223072",
                        "name": "Michael Bohlke-Schneider"
                    },
                    {
                        "authorId": "144607961",
                        "name": "David Salinas"
                    },
                    {
                        "authorId": "2066263165",
                        "name": "Lorenzo Stella"
                    },
                    {
                        "authorId": "51055581",
                        "name": "Fran\u00e7ois-Xavier Aubet"
                    },
                    {
                        "authorId": "11788397",
                        "name": "Laurent Callot"
                    },
                    {
                        "authorId": "2166235",
                        "name": "Tim Januschowski"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "To this end, we investigated the utility of an unsupervised representation learning model proposed by (Franceschi, Dieuleveut, and Jaggi 2019), which can be trained on a large amount of unlabeled data to learn potentially useful feature representations.",
                "Very few studies have focused on unsupervised representation learning for time series and (Franceschi, Dieuleveut, and Jaggi 2019) is amongst the few general-purpose representation learning algorithms for time series without any structural assumptions on nontemporal data."
            ],
            "citingPaper": {
                "paperId": "a5a01357ecc2ac930f7263ea5c2796b377a97bb3",
                "externalIds": {
                    "MAG": "3033928195",
                    "DBLP": "conf/aaai/GoswamiCD20",
                    "DOI": "10.1609/aaai.v34i01.5378",
                    "CorpusId": 219327043
                },
                "corpusId": 219327043,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a5a01357ecc2ac930f7263ea5c2796b377a97bb3",
                "title": "Discriminating Cognitive Disequilibrium and Flow in Problem Solving: A Semi-Supervised Approach Using Involuntary Dynamic Behavioral Signals",
                "abstract": "Problem solving is one of the most important 21st century skills. However, effectively coaching young students in problem solving is challenging because teachers must continuously monitor their cognitive and affective states, and make real-time pedagogical interventions to maximize their learning outcomes. It is an even more challenging task in social environments with limited human coaching resources. To lessen the cognitive load on a teacher and enable affect-sensitive intelligent tutoring, many researchers have investigated automated cognitive and affective detection methods. However, most of the studies use culturally-sensitive indices of affect that are prone to social editing such as facial expressions, and only few studies have explored involuntary dynamic behavioral signals such as gross body movements. In addition, most current methods rely on expensive labelled data from trained annotators for supervised learning. In this paper, we explore a semi-supervised learning framework that can learn low-dimensional representations of involuntary dynamic behavioral signals (mainly gross-body movements) from a modest number of short time series segments. Experiments on a real-world dataset reveal a significant advantage of these representations in discriminating cognitive disequilibrium and flow, as compared to traditional complexity measures from dynamical systems literature, and demonstrate their potential in transferring learned models to previously unseen subjects.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "147478215",
                        "name": "Mononito Goswami"
                    },
                    {
                        "authorId": "2115385407",
                        "name": "Lujie Chen"
                    },
                    {
                        "authorId": "144292541",
                        "name": "A. Dubrawski"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "To this end, we investigated the utility of an unsupervised representation learning model proposed by (Franceschi, Dieuleveut, and Jaggi 2019), which can be trained on a large amount of unlabeled data to learn potentially useful feature representations."
            ],
            "citingPaper": {
                "paperId": "05073a6b9762d25b47e8da7575e90d2218792b4c",
                "externalIds": {
                    "MAG": "3037621479",
                    "DBLP": "conf/aaai/GoswamiCGD20",
                    "DOI": "10.1609/aaai.v34i10.7171",
                    "CorpusId": 219182377
                },
                "corpusId": 219182377,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/05073a6b9762d25b47e8da7575e90d2218792b4c",
                "title": "Modeling Involuntary Dynamic Behaviors to Support Intelligent Tutoring (Student Abstract)",
                "abstract": "Problem solving is one of the most important 21st century skills. However, effectively coaching young students in problem solving is challenging because teachers must continuously monitor their cognitive and affective states and make real-time pedagogical interventions to maximize students' learning outcomes. It is an even more challenging task in social environments with limited human coaching resources. To lessen the cognitive load on a teacher and enable affect-sensitive intelligent tutoring, many researchers have investigated automated cognitive and affective detection methods. However, most of the studies use culturally-sensitive indices of affect that are prone to social editing such as facial expressions, and only few studies have explored involuntary dynamic behavioral signals such as gross body movements. In addition, most current methods rely on expensive labelled data from trained annotators for supervised learning. In this paper, we explore a semi-supervised learning framework that can learn low-dimensional representations of involuntary dynamic behavioral signals (mainly gross-body movements) from a modest number of short time series segments. Experiments on a real-world dataset reveal a significant utility of these representations in discriminating cognitive disequilibrium and flow and demonstrate their potential in transferring learned models to previously unseen subjects.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "147478215",
                        "name": "Mononito Goswami"
                    },
                    {
                        "authorId": "2115385407",
                        "name": "Lujie Chen"
                    },
                    {
                        "authorId": "75181061",
                        "name": "Chufan Gao"
                    },
                    {
                        "authorId": "144292541",
                        "name": "A. Dubrawski"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "time series analysis [30], we reduced each feature of a clip"
            ],
            "citingPaper": {
                "paperId": "38e4ef60ca11634e29565bf6e50e6df93375025c",
                "externalIds": {
                    "DBLP": "conf/aaai/AgarwalM20",
                    "MAG": "3037385973",
                    "DOI": "10.1609/aaai.v34i09.7057",
                    "CorpusId": 219182233
                },
                "corpusId": 219182233,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/38e4ef60ca11634e29565bf6e50e6df93375025c",
                "title": "Semi-Supervised Learning to Perceive Children's Affective States in a Tablet Tutor",
                "abstract": "Like good human tutors, intelligent tutoring systems should detect and respond to students' affective states. However, accuracy in detecting affective states automatically has been limited by the time and expense of manually labeling training data for supervised learning. To combat this limitation, we use semi-supervised learning to train an affective state detector on a sparsely labeled, culturally novel, authentic data set in the form of screen capture videos from a Swahili literacy and numeracy tablet tutor in Tanzania that shows the face of the child using it. We achieved 88% leave-1-child-out cross-validated accuracy in distinguishing pleasant, unpleasant, and neutral affective states, compared to only 61% for the best supervised learning method we tested. This work contributes toward using automated affect detection both off-line to improve the design of intelligent tutors, and at runtime to respond to student affect based on input from a user-facing tablet camera or webcam.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1630294533",
                        "name": "Mansi Agarwal"
                    },
                    {
                        "authorId": "1695106",
                        "name": "Jack Mostow"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The encoder is able to obtain meaningful embeddings that perform well on time series classification and regression tasks and trains significantly faster than a traditional RNN encoder-decoder model [11].",
                "In this paper, we propose an embedding algorithm using a state-of-the-art deep unsupervised dilated, causal convolutional encoder model [11] to find informative embeddings from continuous vital sign time series hemorrhage data of six vital signs."
            ],
            "citingPaper": {
                "paperId": "3c35176d0392b4a9e21598339397adb9719970c8",
                "externalIds": {
                    "ArXiv": "1911.05121",
                    "DBLP": "journals/corr/abs-1911-05121",
                    "MAG": "2984102052",
                    "CorpusId": 207930657
                },
                "corpusId": 207930657,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3c35176d0392b4a9e21598339397adb9719970c8",
                "title": "Detecting Patterns of Physiological Response to Hemodynamic Stress via Unsupervised Deep Learning",
                "abstract": "Monitoring physiological responses to hemodynamic stress can help in determining appropriate treatment and ensuring good patient outcomes. Physicians' intuition suggests that the human body has a number of physiological response patterns to hemorrhage which escalate as blood loss continues, however the exact etiology and phenotypes of such responses are not well known or understood only at a coarse level. Although previous research has shown that machine learning models can perform well in hemorrhage detection and survival prediction, it is unclear whether machine learning could help to identify and characterize the underlying physiological responses in raw vital sign data. We approach this problem by first transforming the high-dimensional vital sign time series into a tractable, lower-dimensional latent space using a dilated, causal convolutional encoder model trained purely unsupervised. Second, we identify informative clusters in the embeddings. By analyzing the clusters of latent embeddings and visualizing them over time, we hypothesize that the clusters correspond to the physiological response patterns that match physicians' intuition. Furthermore, we attempt to evaluate the latent embeddings using a variety of methods, such as predicting the cluster labels using explainable features.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "75181061",
                        "name": "Chufan Gao"
                    },
                    {
                        "authorId": "79665376",
                        "name": "Fabian Falck"
                    },
                    {
                        "authorId": "147478215",
                        "name": "Mononito Goswami"
                    },
                    {
                        "authorId": "51149834",
                        "name": "A. Wertz"
                    },
                    {
                        "authorId": "2910412",
                        "name": "M. Pinsky"
                    },
                    {
                        "authorId": "144292541",
                        "name": "A. Dubrawski"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Two other recent techniques, [13] and [14], use convolutional neural"
            ],
            "citingPaper": {
                "paperId": "40035aabbc15a8fa1c924e107a66d6b0a8190f04",
                "externalIds": {
                    "MAG": "3003910104",
                    "DOI": "10.1109/ISAP48318.2019.9065991",
                    "CorpusId": 213367998
                },
                "corpusId": 213367998,
                "publicationVenue": {
                    "id": "a974eaab-748d-4fd3-8b41-72831ed1df6b",
                    "name": "International Symposium on Antennas and Propagation",
                    "type": "conference",
                    "alternate_names": [
                        "ISAP",
                        "Int Symp Antenna Propag"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/40035aabbc15a8fa1c924e107a66d6b0a8190f04",
                "title": "Time Series Representation Learning Applications for Power Analytics",
                "abstract": "The uptake of solar power generation is on the rise. This necessitates more research into developing data-driven intelligent methods that can perform effective analytics over power generation data to inform strategies to improve solar power generation systems. In this paper, we consider the utility of time series representation learning for analytics over power generation data. WaRTEm, a representation learning method that focuses on learning time series representations that are invariant to local phase shifts, is the focus of our investigations in this paper. We identify two metadata attributes for power generation sequences, month and CellID, as attributes that embed useful notions of semantic similarity between time series sequences. We evaluate the effectiveness of WaRTEm representations, as against using the raw time series sequences, in alignment to the month and CellID labellings, using accuracy over 1NN retrieval as an evaluation framework. Through empirical evaluations, we identify that WaRTEm embeddings are consistently able to achieve better representations when evaluated on 1NN accuracy. We also identify some features of WaRTEm that are more suited for time series representation learning, which provides promising directions for future work.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "73451750",
                        "name": "Anish K. Mathew"
                    },
                    {
                        "authorId": "1803913519",
                        "name": "D. P."
                    },
                    {
                        "authorId": "35139562",
                        "name": "Sahely Bhadra"
                    },
                    {
                        "authorId": "145910809",
                        "name": "N. Pindoriya"
                    },
                    {
                        "authorId": "40257436",
                        "name": "A. Kiprakis"
                    },
                    {
                        "authorId": "2145868495",
                        "name": "S. N. Singh"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "1ce1d287e825c78d381a95c0134e080bf1310611",
                "externalIds": {
                    "MAG": "2994759459",
                    "DBLP": "journals/corr/abs-1911-03584",
                    "ArXiv": "1911.03584",
                    "CorpusId": 207852415
                },
                "corpusId": 207852415,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1ce1d287e825c78d381a95c0134e080bf1310611",
                "title": "On the Relationship between Self-Attention and Convolutional Layers",
                "abstract": "Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "51440515",
                        "name": "Jean-Baptiste Cordonnier"
                    },
                    {
                        "authorId": "1966031",
                        "name": "Andreas Loukas"
                    },
                    {
                        "authorId": "2456863",
                        "name": "Martin Jaggi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026use of dilation in convolutional neural networks, where dilation increases exponentially with depth (e.g., Yu and Koltun 2016; Bai et al. 2018; Franceschi et al. 2019), we sample dilation randomly for each kernel, producing a huge variety of kernel dilation, capturing patterns at different\u2026",
                "Franceschi et al. (2019) present a method for unsupervised learning of convolutional kernels for a feature transform for time series input, based on a multilayer convolutional architecture with dilation increasing exponentially in each successive layer.",
                "In contrast to the typical use of dilation in convolutional neural networks, where dilation increases exponentially with depth (e.g., Yu and Koltun 2016; Bai et al. 2018; Franceschi et al. 2019), we sample dilation randomly for each kernel, producing a huge variety of kernel dilation, capturing patterns at different frequencies and scales, which is critical to the performance of the method (see section 4."
            ],
            "citingPaper": {
                "paperId": "2deafb11372f085d504db87fd626e478d8e965aa",
                "externalIds": {
                    "ArXiv": "1910.13051",
                    "MAG": "3042807565",
                    "DBLP": "journals/corr/abs-1910-13051",
                    "DOI": "10.1007/s10618-020-00701-z",
                    "CorpusId": 204949593
                },
                "corpusId": 204949593,
                "publicationVenue": {
                    "id": "d263025a-9eaf-443f-9bbf-72377e8d22a6",
                    "name": "Data mining and knowledge discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Data Mining and Knowledge Discovery",
                        "Data Min Knowl Discov",
                        "Data min knowl discov"
                    ],
                    "issn": "1384-5810",
                    "url": "https://www.springer.com/computer/database+management+&+information+retrieval/journal/10618",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10618",
                        "http://www.springer.com/computer/database+management+&+information+retrieval/journal/10618"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2deafb11372f085d504db87fd626e478d8e965aa",
                "title": "ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1388781269",
                        "name": "Angus Dempster"
                    },
                    {
                        "authorId": "153827036",
                        "name": "Franccois Petitjean"
                    },
                    {
                        "authorId": "1726660",
                        "name": "Geoffrey I. Webb"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Although there exist several methods tailored to representation learning on time series [8, 10, 11], only few models present extensions of the SOM optimized for temporal data."
            ],
            "citingPaper": {
                "paperId": "92df00d420928b3e97f2f68ff0dd662af29785d9",
                "externalIds": {
                    "MAG": "3006228137",
                    "CorpusId": 213431618
                },
                "corpusId": 213431618,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/92df00d420928b3e97f2f68ff0dd662af29785d9",
                "title": "DPSOM: Deep Probabilistic Clustering with Self-Organizing Maps",
                "abstract": "Generating visualizations and interpretations from high-dimensional data is a common problem in many applications. Two key approaches for tackling this problem are clustering and representation learning. On the one hand, there are very performant deep clustering models, such as DEC and IDEC. On the other hand, there are interpretable representation learning techniques, often relying on latent topological structures such as self-organizing maps. However, current methods do not yet successfully combine these two approaches. We present a novel way to fit self-organizing maps with probabilistic cluster assignments, PSOM, a new deep architecture for probabilistic clustering, DPSOM, and its extension to time series data, T-DPSOM. We show that they achieve superior clustering performance compared to current deep clustering methods on static MNIST/Fashion-MNIST data as well as medical time series, while also inducing an interpretable representation. Moreover, on medical time series, T-DPSOM successfully predicts future trajectories in the original data space.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "11180167",
                        "name": "L. Manduchi"
                    },
                    {
                        "authorId": "50988033",
                        "name": "Matthias H\u00fcser"
                    },
                    {
                        "authorId": "2414086",
                        "name": "G. R\u00e4tsch"
                    },
                    {
                        "authorId": "41031794",
                        "name": "Vincent Fortuin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "There exist several methods tailored to representation learning on time series, among them [7, 8, 9], which are however not based on SOMs."
            ],
            "citingPaper": {
                "paperId": "a81655d54b7a7b4b4bab095eee5e568de1438a1e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1910-01590",
                    "MAG": "2977866976",
                    "ArXiv": "1910.01590",
                    "CorpusId": 203641795
                },
                "corpusId": 203641795,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a81655d54b7a7b4b4bab095eee5e568de1438a1e",
                "title": "Variational PSOM: Deep Probabilistic Clustering with Self-Organizing Maps",
                "abstract": "Generating visualizations and interpretations from high-dimensional data is a common problem in many fields. Two key approaches for tackling this problem are clustering and representation learning. There are very performant deep clustering models on the one hand and interpretable representation learning techniques, often relying on latent topological structures such as self-organizing maps, on the other hand. However, current methods do not yet successfully combine these two approaches. We present a new deep architecture for probabilistic clustering, VarPSOM, and its extension to time series data, VarTPSOM. We show that they achieve superior clustering performance compared to current deep clustering methods on static MNIST/Fashion-MNIST data as well as medical time series, while inducing an interpretable representation. Moreover, on the medical time series, VarTPSOM successfully predicts future trajectories in the original data space.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "11180167",
                        "name": "L. Manduchi"
                    },
                    {
                        "authorId": "50988033",
                        "name": "Matthias H\u00fcser"
                    },
                    {
                        "authorId": "2414086",
                        "name": "G. R\u00e4tsch"
                    },
                    {
                        "authorId": "41031794",
                        "name": "Vincent Fortuin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "CNNs trained using triplet loss for TSC have been very recently proposed for unsupervised learning in [13] and for supervised learning in [3]."
            ],
            "citingPaper": {
                "paperId": "9a9968e65ad4c38a0afffa7b01f75ffc7062bf5d",
                "externalIds": {
                    "MAG": "2999767092",
                    "ArXiv": "1909.07155",
                    "DBLP": "journals/corr/abs-1909-07155",
                    "DOI": "10.1145/3371158.3371162",
                    "CorpusId": 202577942
                },
                "corpusId": 202577942,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9a9968e65ad4c38a0afffa7b01f75ffc7062bf5d",
                "title": "Meta-Learning for Few-Shot Time Series Classification",
                "abstract": "Deep neural networks (DNNs) have achieved state-of-the-art results on time series classification (TSC) tasks. In this work, we focus on leveraging DNNs in the often-encountered practical scenario where access to labeled training data is difficult, and where DNNs would be prone to overfitting. We leverage recent advancements in gradient-based meta-learning, and propose an approach to train a residual neural network with convolutional layers as a meta-learning agent for few-shot TSC. The network is trained on a diverse set of few-shot tasks sampled from various domains (e.g. healthcare, activity recognition, etc.) such that it can solve a target task from another domain using only a small number of training samples from the target task. Most existing meta-learning approaches are limited in practice as they assume a fixed number of target classes across tasks. We overcome this limitation in order to train a common agent across domains with each domain having different number of target classes, we utilize a triplet-loss based learning procedure that does not require any constraints to be enforced on the number of classes for the few-shot TSC tasks. To the best of our knowledge, we are the first to use meta-learning based pre-training for TSC. Our approach sets a new benchmark for few-shot TSC, outperforming several strong baselines on few-shot tasks sampled from 41 datasets in UCR TSC Archive. We observe that pre-training under the meta-learning paradigm allows the network to quickly adapt to new unseen tasks with small number of labeled instances.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "90519310",
                        "name": "Jyoti Narwariya"
                    },
                    {
                        "authorId": "143898225",
                        "name": "Pankaj Malhotra"
                    },
                    {
                        "authorId": "3213990",
                        "name": "L. Vig"
                    },
                    {
                        "authorId": "143725466",
                        "name": "Gautam M. Shroff"
                    },
                    {
                        "authorId": "38951388",
                        "name": "T. Vishnu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Another recent work (Franceschi et al., 2019) makes use of convolutional neural networks in a framework heavily inspired by word2vec (Mikolov et al.",
                "Another recent work (Franceschi et al., 2019) makes use of convolutional neural networks in a framework heavily inspired by word2vec (Mikolov et al., 2013)."
            ],
            "citingPaper": {
                "paperId": "b569995d6139c3b17bfa642cf1764119aaf48141",
                "externalIds": {
                    "MAG": "2950105934",
                    "DBLP": "journals/corr/abs-1906-05205",
                    "CorpusId": 186206670
                },
                "corpusId": 186206670,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b569995d6139c3b17bfa642cf1764119aaf48141",
                "title": "Warping Resilient Time Series Embeddings",
                "abstract": "Time series are ubiquitous in real world problems and computing distance between two time series is often required in several learning tasks. Computing similarity between time series by ignoring variations in speed or warping is often encountered and dynamic time warping (DTW) is the state of the art. However DTW is not applicable in algorithms which require kernel or vectors. In this paper, we propose a mechanism named WaRTEm to generate vector embeddings of time series such that distance measures in the embedding space exhibit resilience to warping. Therefore, WaRTEm is more widely applicable than DTW. WaRTEm is based on a twin auto-encoder architecture and a training strategy involving warping operators for generating warping resilient embeddings for time series datasets. We evaluate the performance of WaRTEm and observed more than $20\\%$ improvement over DTW in multiple real-world datasets.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "73451750",
                        "name": "Anish K. Mathew"
                    },
                    {
                        "authorId": "145791689",
                        "name": "P Deepak"
                    },
                    {
                        "authorId": "35139562",
                        "name": "Sahely Bhadra"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2022], T-Loss [Franceschi et al., 2019], TNC [Tonekaboni et al.",
                "Compared to other forms of data, the time series domain has seen less research on contrastive learning [Eldele et al., 2021a; Franceschi et al., 2019; Fan et al., 2020; Tonekaboni et al., 2021].",
                "Recently, there have been efforts to apply contrastive learning to the time series domain [Oord et al., 2018; Franceschi et al., 2019; Fan et al., 2020; Eldele et al., 2021a; Tonekaboni et al., 2021; Yue et al., 2021].",
                "We first utilized the same input layer and a stacked dilated CNN module [Franceschi et al., 2019; Yue et al., 2021] for both g(x) and h(x), respectively.",
                "[Franceschi et al., 2019] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi.",
                "In [Franceschi et al., 2019], Franceschi et.al. utilize subsequences to generate positive and negative pairs.",
                "We compare our method with 8 state-of-the-art baselines, including TS2Vec [Yue et al., 2022], T-Loss [Franceschi et al., 2019], TNC [Tonekaboni et al., 2021], TS-TCC [Eldele et al., 2021b], TST [Zerveas et al., 2021], DTW [Chen et al., 2013], TF-C [Zhang et al., 2022] and InfoTS [Luo et al., 2023]."
            ],
            "citingPaper": {
                "paperId": "291c2699fa74b1186b216103884abe1653583c0a",
                "externalIds": {
                    "CorpusId": 260341220
                },
                "corpusId": 260341220,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/291c2699fa74b1186b216103884abe1653583c0a",
                "title": "AutoTCL: Automated Time Series Contrastive Learning with Adaptive Augmentations",
                "abstract": "Modern techniques like contrastive learning have been effectively used in many areas, including computer vision, natural language processing, and graph-structured data. Creating positive examples that assist the model in learning robust and discriminative representations is a crucial stage in contrastive learning approaches. Usually, preset human intuition directs the selection of relevant data augmentations. Due to patterns that are easily recognized by humans, this rule of thumb works well in the vision and language domains. However, it is impractical to visually inspect the temporal structures in time series. The diversity of time series augmentations at both the dataset and instance levels makes it difficult to choose meaningful augmen-tations on the fly. Thus, although prevalent, contrastive learning with data augmentation has been less studied in the time series domain. In this study, we analyze data augmentation for time series based on information theory and summarize the most adopted transformations in a unified form. On top of that, we generalize it to a parameterized augmentation method to support adaptive usage in time series representation learning. Experiments on benchmark datasets demonstrate the highly competitive results of our method, AutoTCL, with an average 10.3% reduction in MSE and 7.0% in MAE over the leading baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110301001",
                        "name": "Xu Zheng"
                    },
                    {
                        "authorId": "40606845",
                        "name": "Tianchun Wang"
                    },
                    {
                        "authorId": "145859270",
                        "name": "Wei Cheng"
                    },
                    {
                        "authorId": "2226185279",
                        "name": "Aitian Ma"
                    },
                    {
                        "authorId": "2145225543",
                        "name": "Haifeng Chen"
                    },
                    {
                        "authorId": "2061470522",
                        "name": "M. Sha"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6a4964d47b2314d0eb40fd4a21dc9b5df15c1c5e",
                "externalIds": {
                    "DBLP": "journals/tgrs/YangLTLW23",
                    "DOI": "10.1109/TGRS.2023.3240728",
                    "CorpusId": 256440788
                },
                "corpusId": 256440788,
                "publicationVenue": {
                    "id": "70628d6a-97aa-4571-9701-bc0eb3989c32",
                    "name": "IEEE Transactions on Geoscience and Remote Sensing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Geosci Remote Sens"
                    ],
                    "issn": "0196-2892",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=36",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6a4964d47b2314d0eb40fd4a21dc9b5df15c1c5e",
                "title": "Unsupervised Clustering of Microseismic Signals Using a Contrastive Learning Model",
                "abstract": "Distinguishing useful microseismic signals is a critical step in microseismic monitoring. Here, we present the time series contrastive clustering (TSCC) method, an end-to-end unsupervised model for clustering microseismic signals that uses a contrastive learning network and a centroidal-based clustering model. The TSCC framework consists of two successive phases: pretraining and fine-tuning. In the pretraining phase, two random cropping augmentations are used to transform the time series microseismic data into two distinct but correlated views. Then, the multiscale temporal and instance contrasting learning are used to discriminate between negative and positive views, thus motivating the encoder to capture microseismic signal contextual information from multiple perspectives and generate distinct representations from unlabeled data. During the fine-tuning phase, the encoder weights are iteratively fine-tuned by simultaneously performing contrast learning and clustering. The corresponding loss is a weighted combination of the contrastive and clustering loss functions, which induces the encoder to learn representations that improve the clustering performance. The test results demonstrate that the proposed method can achieve better clustering accuracy (ACC) than popular clustering methods, including $k$ -means, deep embedding clustering (DEC), unsupervised clustering with deep convolutional autoencoders (DCAs), and deep clustering with self-supervision (DCSS). Moreover, the TSCC model can produce results comparable to supervised deep learning approaches while requiring no labeled data, manual feature extraction, or large training datasets. In practice, the TSCC model has a clustering ACC of 98.07% and a normalized mutual information (NMI) of 86.26%.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2203911627",
                        "name": "Zhen Yang"
                    },
                    {
                        "authorId": "2109043090",
                        "name": "Huailiang Li"
                    },
                    {
                        "authorId": "2148572916",
                        "name": "Xianguo Tuo"
                    },
                    {
                        "authorId": "2139471538",
                        "name": "Linjia Li"
                    },
                    {
                        "authorId": "2203488157",
                        "name": "Junnan Wen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, some representation methods of learning time series based on deep learning have been proposed, such as DTCR [28] and USRLTS [12].",
                "USRLTS constructs a scalable network to learn the representation of time series with variable lengths."
            ],
            "citingPaper": {
                "paperId": "adb9bf7811723ba9198783017acf19a9494e7aaa",
                "externalIds": {
                    "DBLP": "journals/access/ZhangWLZW23",
                    "DOI": "10.1109/ACCESS.2022.3188393",
                    "CorpusId": 250304519
                },
                "corpusId": 250304519,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/adb9bf7811723ba9198783017acf19a9494e7aaa",
                "title": "Heterogeneous Feature Based Time Series Classification With Attention Mechanism",
                "abstract": "Time series classification (TSC) problem has been a significantly attractive research problem for decades. A large number of models with various types of features have been proposed. However, with the rapid development of new applications, like IoT and intelligent manufacturing, the time series data from different industries and applications are constantly emerging. To classify these data accurately, data scientists face the challenges of 1) how to select the optimal features and classification models and 2) how to interpret the results. To tackle these two challenges, in this paper, we propose a heterogeneous feature ensemble network, named FEnet. Multiple features, including time-domain, frequency-domain, and so on, are combined to build the model so that it can deal with the diversity of the data characteristics. Furthermore, to improve the interpretability, we propose a two-level attention mechanism. Finally, we propose two model optimization strategies to enhance classification accuracy and efficiency. Extensive experiments are conducted on real datasets and the results verify the accuracy, operation efficiency, and interpretability of FEnet.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119079859",
                        "name": "Hanbo Zhang"
                    },
                    {
                        "authorId": "2155302123",
                        "name": "Peng Wang"
                    },
                    {
                        "authorId": "1720848206",
                        "name": "Shen Liang"
                    },
                    {
                        "authorId": "2175443574",
                        "name": "Tongming Zhou"
                    },
                    {
                        "authorId": "145200778",
                        "name": "Wei Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Unsupervised time-series embedders have been proposed [29, 30] to deal with label scarcity.",
                "We used [29] for our stream data embedder since it is more flexible to different time-series lengths and generates good representations for anomaly data compared to [30].",
                "[29] use the triplet loss function to learn a representation of multidimensional time-series data."
            ],
            "citingPaper": {
                "paperId": "350224cfedb00967c1d88ffc13245efa86786d97",
                "externalIds": {
                    "DBLP": "conf/csw/KosanHALC23",
                    "CorpusId": 257584849
                },
                "corpusId": 257584849,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/350224cfedb00967c1d88ffc13245efa86786d97",
                "title": "AI Decision Systems with Feedback Loop Active Learner",
                "abstract": "Making precise decisions for high-stakes applications such as finance, health, and self-driving is critical for increasing the economy of an entity or the quality of life. In most scenarios, decision quickness is also as essential as accuracy. This is particularly true in the case of event detection problems, where late detection can cause financial or physical damage. While recent work focuses on combining fast unsupervised AI decision systems and precise human decisions to solve this problem, the quality of this cooperation remains questionable. A human can generate ground-truth labels for the AI decision systems for future improvements. However, having noisy ground truth can worsen the performance. To address this challenge, this paper proposes FLAL (Feedback Loop Active Learner), a novel bridge system between the AI decision system and human/s, designed to understand human expertise and interest using a recommender mechanism and improve AI system performance using an active learning mechanism. FLAL is able to identify human behavior and makes entity recommendations to users who can generate better ground-truth labels for these entities. Our experiments show that FLAL performs better than competing baselines and converges fast.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2134769449",
                        "name": "Mert Kosan"
                    },
                    {
                        "authorId": "2112502171",
                        "name": "Linyun He"
                    },
                    {
                        "authorId": "2112401388",
                        "name": "Shubham Agrawal"
                    },
                    {
                        "authorId": "2117922425",
                        "name": "Hongyi Liu"
                    },
                    {
                        "authorId": "46747446",
                        "name": "Chiranjeet Chetia"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We follow (Zerveas et al., 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilation-CNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al., 2020), and a transformer-based TST (Zerveas et\u2026",
                ", 2021) to use these baselines for comparison: DTWD which stands for dimension-Dependent DTW combined with dilation-CNN (Franceschi et al., 2019), LSTM (Graves, 2012), XGBoost (Chen & Guestrin, 2016), Rocket (Dempster et al."
            ],
            "citingPaper": {
                "paperId": "97e24e67aa8a4254f1778b8e469fcc3d9bb73425",
                "externalIds": {
                    "CorpusId": 259311557
                },
                "corpusId": 259311557,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/97e24e67aa8a4254f1778b8e469fcc3d9bb73425",
                "title": "IRREGULARLY SAMPLED TIME SERIES",
                "abstract": "Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Our code and data are available at https://github.com/Leezekun/ViTST.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2168519132",
                        "name": "Zekun Li"
                    },
                    {
                        "authorId": "50341591",
                        "name": "SHIYANG LI"
                    },
                    {
                        "authorId": "1740249",
                        "name": "Xifeng Yan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a7c2c3a2cbee726bc878de561b93d5feca9f136b",
                "externalIds": {
                    "DBLP": "journals/eswa/ZhuDWZ23",
                    "DOI": "10.1016/j.eswa.2023.120619",
                    "CorpusId": 259213447
                },
                "corpusId": 259213447,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a7c2c3a2cbee726bc878de561b93d5feca9f136b",
                "title": "A contrastive learning-based framework for wind power forecast",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "120435839",
                        "name": "Nanyang Zhu"
                    },
                    {
                        "authorId": "33570120",
                        "name": "Zemei Dai"
                    },
                    {
                        "authorId": "2155515061",
                        "name": "Ying Wang"
                    },
                    {
                        "authorId": "2104228",
                        "name": "Kaifeng Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "[32] [C2] Unsupervised representation for times series Neurips 2019"
            ],
            "citingPaper": {
                "paperId": "3d738b20bc50524c5db2d58af100c6cff000a06b",
                "externalIds": {
                    "CorpusId": 259839794
                },
                "corpusId": 259839794,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3d738b20bc50524c5db2d58af100c6cff000a06b",
                "title": "Habilitation \u00e0 Diriger des Recherches",
                "abstract": "The design of new algorithms for artificial intelligence is one of the major challenges of our time. The extraordinary progress made in recent years has been enabled by a combination of factors: on the one hand, the simultaneous growth of available data sets and computing power; on the other hand, algorithmic innovations that have played a decisive role in making it possible to move to the next scale. During the last years, the field of Federated Learning (FL) has gained a lot of importance. In Federated Learning, several organizations or devices seek to collaboratively train a model under the orchestration of a central server, while keeping individual datasets on their respective local storage. Federated Learning has emerged as a fundamental setting to tackle new societal and industrial challenges in machine learning. Indeed, privacy has become a major concern for both society (individuals participating into the training want to protect their privacy) and industries (facing legal constraints preventing them from exploiting valuable data). Overall, it becomes necessary to train the models without centralizing the data, either because of those privacy constraints, or sometimes because extremely large datasets have to be distributed over networks of storing devices. Consequently, new optimization challenges are arising, taking into account communication constraints, and new opportunities to improve the models to adapt to the users are being explored. Developing new algorithms for Federated Learning is thus a key challenge. It will simultaneously positively impact society, by protecting individual data and restoring public trust and confidence in machine learning technologies, and unlock countless novel opportunities of collaboration between entities willing to collaborate without centralizing their datasets; a crucial situation in medical applications, fraud detection, IOT, and many other domains. The design and analysis of algorithms and techniques for optimization and for optimization in FL have been at the core of my research interests over the last few years. Designing new techniques requires an in-depth understanding of both the (first-order) optimization techniques and the specificities of the federated context. In this manuscript, I summarize my research activities over the last years and provided a detailed description of some featured contributions, especially in the domain of Federated Learning with communication constraints, in the first part of the manuscript, and fundamental results on classical first-order optimization, in the second part.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223148689",
                        "name": "Aymeric Dieuleveut"
                    },
                    {
                        "authorId": "2223137109",
                        "name": "Dan Alistarh"
                    },
                    {
                        "authorId": "1398407593",
                        "name": "Michel Jordan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d1e086fb358c4298d7848d5ca8359ad9b14fbf62",
                "externalIds": {
                    "CorpusId": 260329565
                },
                "corpusId": 260329565,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d1e086fb358c4298d7848d5ca8359ad9b14fbf62",
                "title": "Self-Supervised Electroencephalogram Representation Learning for Automatic Sleep Staging: Model Development and Evaluation Study",
                "abstract": "Background: Deep learning models have shown great success in automating tasks in sleep medicine by learning from carefully annotated electroencephalogram (EEG) data. However, effectively using a large amount of raw EEG data remains a challenge. Objective: In this study, we aim to learn robust vector representations from massive unlabeled EEG signals, such that the learned vectorized features (1) are expressive enough to replace the raw signals in the sleep staging task, and (2) provide better predictive performance than supervised models in scenarios involving fewer labels and noisy samples. Methods: We propose a self-supervised model, Contrast with the World Representation (ContraWR), for EEG signal representation learning. Unlike previous models that use a set of negative samples, our model uses global statistics (ie, the average representation) from the data set to distinguish signals associated with different sleep stages. The ContraWR model is evaluated on 3 real-world EEG data sets that include both settings: at-home and in-laboratory EEG recording. Results: ContraWR outperforms 4 recently reported self-supervised learning methods on the sleep staging task across 3 large EEG data sets. ContraWR also supersedes supervised learning when fewer training labels are available (eg, 4% accuracy improvement when less than 2% of data are labeled on the Sleep EDF data set). Moreover, the model provides informative, representative feature structures in 2D projection. Conclusions: We show that ContraWR is robust to noise and can provide high-quality EEG representations for downstream prediction tasks. The proposed model can be generalized to other unsupervised physiological signal learning tasks. Future directions include exploring task-specific data augmentations and combining self-supervised methods with supervised methods, building upon the initial success of self-supervised learning reported in this study.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1928291861",
                        "name": "Chaoqi Yang"
                    },
                    {
                        "authorId": "2225956368",
                        "name": "MSc Cao Xiao"
                    },
                    {
                        "authorId": "2226004445",
                        "name": "P. M. B. Westover"
                    },
                    {
                        "authorId": "2225986723",
                        "name": "PhD Jimeng Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Franceschi et al.[5] introduces an unsupervised contrastive learning framework by introducing a novel triplet selection approach based on segment\u2019s context."
            ],
            "citingPaper": {
                "paperId": "0e47edbfe684d2d6986b1c3238bca7a15d3f0f21",
                "externalIds": {
                    "CorpusId": 260749561
                },
                "corpusId": 260749561,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0e47edbfe684d2d6986b1c3238bca7a15d3f0f21",
                "title": "Adaptive Resolution Loss: An Efficient and Effective Loss for Time Series Self-Supervised Learning Framework",
                "abstract": "Time series data is a crucial form of information that has vast opportunities. With the widespread use of sensor networks, large-scale time series data has become ubiquitous. One of the most prominent problems in time series data mining is representation learning. Recently, with the introduction of self-supervised learning frameworks (SSL), numerous amounts of research have focused on designing an effective SSL for time series data. One of the current state-of-the-art SSL frameworks in time series is called TS2Vec. TS2Vec specially designs a hierarchical contrastive learning framework that uses loss-based training, which performs outstandingly against benchmark testing. However, the computational cost for TS2Vec is often significantly greater than other SSL frameworks. In this paper, we present a new self-supervised learning loss named, adaptive resolution loss. The proposed solution reduces the number of resolutions used for training the model via score functions, leading to an efficient adaptive resolution learning algorithm. The proposed method preserves the original model\u2019s integrity while significantly enhancing its training time.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2228912475",
                        "name": "Kevin Garcia"
                    },
                    {
                        "authorId": "145616183",
                        "name": "Juan Manuel P\u00e9rez"
                    },
                    {
                        "authorId": "145225904",
                        "name": "Yifeng Gao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Causal convolutions have been shown to be effective in creating embeddings[16] for time series data, allowing for an easier interpretation of long sequences."
            ],
            "citingPaper": {
                "paperId": "92b4ad3394ea104939894f07567b0888c688cfb5",
                "externalIds": {
                    "CorpusId": 262937455
                },
                "corpusId": 262937455,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/92b4ad3394ea104939894f07567b0888c688cfb5",
                "title": "Master Computer Science Multivariate Time Series Classification In Radial Drilling Applications",
                "abstract": "The application of Radial Drilling is an emerging field in the geothermal, oil-and gas industry. By creating radial extensions from existing wells into existing reservoirs, the extraction of resources can be made easier - improving the yields of these wells at a relatively low cost. We aim to automatically classify the main steps in the Radial-Drilling process based on the previous outputs of 4 of the most important sensors. We have designed and implemented an open source machine learning pipeline for these kinds of multivariate time-series classification tasks. This pipeline consists of MVTS-analyzer, a platform for visualizing annotating and analyzing multivariate time-series data and Configurun, an app installable as a python package that can be used to (remotely) create, manage and run python configurations - on top of which we have implemented our machine-learning framework, making all (novel) classification methods available in an intuitive UI. MVTS-analyzer is used to annotate the raw Radial-Drilling data. Configurun and the implemented training-framework are then used to train and test the performance of 7 models on this novel classification-task and on 10 existing time-series classification tasks. By combining two state-of-the-art classification methods (Rocket and XGBoost) we create a new classifier that shows robust results on the tested datasets. Based on a state-of-the-art Time Series Transformer architecture (TST) we design and implement our own convolutional-transformer classification architecture (CTST). We expand upon this idea by applying multiple convolutional layers (MCTST), allowing for a higher degree of feature extraction before self-attention is applied. We train and test these and several state-of-the-art models and compare our found results to the author-reported performance. From our results, the average accuracy of TST is still slightly lower than that of the state-of-the-art methods. CTST shows a slight overall increase in accuracy over TST, while MCTST shows some improvement on select datasets, but does not improve the accuracy on average. On the novel Radial-Drilling classification dataset, we achieved a maximum prediction accuracy of 82% using our proposed method of combining Rocket and",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2247606416",
                        "name": "W. J. Stokman"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Despite the success of supervised learning, in particular that of recurrent architectures in the field of time-series forecasting, other machine learning branches have also proposed various models for time-series forecasting, such as, state spate models in (Franceschi et al., 2020), representation learning based models in (Rangapuram et al."
            ],
            "citingPaper": {
                "paperId": "abe885f34b8e57a560a1d3ed1d78bdfdac27c5fa",
                "externalIds": {
                    "DBLP": "conf/icaart/AiwansedoBB23",
                    "DOI": "10.5220/0011660100003393",
                    "CorpusId": 257360462
                },
                "corpusId": 257360462,
                "publicationVenue": {
                    "id": "f6b96a8f-dc43-4d21-99cf-0f2532b7f01f",
                    "name": "International Conference on Agents and Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Agent Artif Intell",
                        "ICAART"
                    ],
                    "url": "http://www.icaart.org/"
                },
                "url": "https://www.semanticscholar.org/paper/abe885f34b8e57a560a1d3ed1d78bdfdac27c5fa",
                "title": "Trade-off Clustering Approach for Multivariate Multi-Step Ahead Time-Series Forecasting",
                "abstract": ": Time-Series forecasting has gained a lot of steam in recent years. With the advent of Big Data, a considerable amount of data is more available across multiple \ufb01elds, thus providing an opportunity for processing historical business-oriented data in an attempt to predict trends, identify changes and inform strategic decision-making. The abundance of time-series data has prompted the development of state-of-the-art machine learning algorithms, such as neural networks, capable of forecasting both univariate and multivariate time-series data. Various time-series forecasting approaches can be implemented when leveraging the potential of deep neural networks. Determining the upsides and downsides of each approach when presented with univariate or multivariate time-series data, thus becomes a crucial matter. This evaluation focuses on three forecasting approaches: a single model forecasting approach (SMFA), a global model forecasting model (GMFA) and a cluster-based forecasting approach (CBFA). The study highlights the fact that the decision pertaining to the \ufb01nest forecasting approach often is a question of trade-off between accuracy, execution time and dataset size. In this study, we also compare the performance of 6 deep learning architectures when dealing with both uni-variate and multivariate time-series datasets for multi-step ahead time-series forecasting, across 6 benchmark datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210765347",
                        "name": "Konstandinos Aiwansedo"
                    },
                    {
                        "authorId": "2684478",
                        "name": "Wafa Badreddine"
                    },
                    {
                        "authorId": "1687894",
                        "name": "J. Bosche"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "rithms [29], [30], [31], [72], DCRLS-based classification uses a two-step method.",
                "Following the previous work in [72], we embed a one-nearestneighbor (1-NN) classifier into the DCRLS-based classifica-",
                "\u2022 T-Loss: a method based on random sub-series technique ResNet as its feature extractor [72]."
            ],
            "citingPaper": {
                "paperId": "4d8cd768cfa537ee373d79e80fa17b42bf640fe4",
                "externalIds": {
                    "DOI": "10.1109/tetci.2023.3304948",
                    "CorpusId": 261379992
                },
                "corpusId": 261379992,
                "publicationVenue": {
                    "id": "544cddb9-1149-469a-8377-d8c34f08d8b1",
                    "name": "IEEE Transactions on Emerging Topics in Computational Intelligence",
                    "alternate_names": [
                        "IEEE Trans Emerg Top Comput Intell"
                    ],
                    "issn": "2471-285X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7433297",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7433297"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4d8cd768cfa537ee373d79e80fa17b42bf640fe4",
                "title": "Deep Contrastive Representation Learning With Self-Distillation",
                "abstract": "\u2014Recently, contrastive learning (CL) is a promising way of learning discriminative representations from time series data. In the representation hierarchy, semantic information extracted from lower levels is the basis of that captured from higher levels. Low-level semantic information is essential and should be considered in the CL process. However, the existing CL algorithms mainly focus on the similarity of high-level semantic information. Considering the similarity of low-level semantic information may improve the performance of CL. To this end, we present a deep contrastive representation learning with self-distillation (DCRLS) for the time series domain. DCRLS gracefully combine data augmentation, deep contrastive learning, and self distillation. Our data augmentation provides different views from the same sample as the input of DCRLS. Unlike most CL algorithms that concentrate on high-level semantic information only, our deep contrastive learning also considers the contrast similarity of low-level semantic information between peer residual blocks. Our self distillation promotes knowledge \ufb02ow from high-level to low-level blocks to help regularize DCRLS in the knowledge transfer process. The experimental results demonstrate that the DCRLS-based structures achieve excellent performance on classi\ufb01cation and clustering on 36",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50479843",
                        "name": "Zhiwen Xiao"
                    },
                    {
                        "authorId": "38092176",
                        "name": "Huanlai Xing"
                    },
                    {
                        "authorId": "2143736546",
                        "name": "Bowen Zhao"
                    },
                    {
                        "authorId": "145036587",
                        "name": "Rong Qu"
                    },
                    {
                        "authorId": "2962874",
                        "name": "Shouxi Luo"
                    },
                    {
                        "authorId": "39301921",
                        "name": "Penglin Dai"
                    },
                    {
                        "authorId": "2149142438",
                        "name": "Ke Li"
                    },
                    {
                        "authorId": "40859893",
                        "name": "Zonghai Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b168a252c5043a11df02ca3ac7f49465488e92f9",
                "externalIds": {
                    "DOI": "10.1109/ACCESS.2023.3319395",
                    "CorpusId": 263197078
                },
                "corpusId": 263197078,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b168a252c5043a11df02ca3ac7f49465488e92f9",
                "title": "A New Hybrid Prediction Method of El Ni\u00f1o/La Ni\u00f1a Events by Combining TimesNet and ARIMA",
                "abstract": "El Ni\u00f1o/La Ni\u00f1a events significantly impact human society, often resulting in considerable monetary losses. Accurate prediction has become crucial with triple La Ni\u00f1a events in this century. This study applied TimesNet to El Ni\u00f1o/La Ni\u00f1a event prediction for the first time. We proposed a hybrid prediction method based on extracting features from time series data and initially decomposing the time series data (Ni\u00f1o3.4) into several Intrinsic Mode Functions (IMFs) using the Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN). Based on the characteristics of each IMF, we used a hybrid method of TimesNet and ARIMA to make adaptive forecasts for them. We selected monthly data from 1950 to 2022, with the first 63 years used for training and shifted 12 periods (12 months) ahead to forecast the Ni\u00f1o3.4 index values for the next ten years. The experimental results of this study show that: 1) The pre-processing method using CEEMDAN can effectively extract the original time series data features and significantly improve the prediction performance; 2) proposed approach achieved good performance in predicting El Ni\u00f1o/La Ni\u00f1a events, particularly during the transition from El Ni\u00f1o to La Ni\u00f1a events (e.g., 2016, 2019-2020); 3) evaluation results indicate that the proposed model exhibits better predictive power (stability and accuracy of prediction results) than the current best single-order predictor, the ConvLSTM model, on the validation set of the last ten years.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2249075581",
                        "name": "Yuhao Du"
                    },
                    {
                        "authorId": "2248191509",
                        "name": "Yihong Li"
                    },
                    {
                        "authorId": "2248952317",
                        "name": "Hui Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Inspired by Word2Vec [Mikolov et al., 2013], Scalable Representation Learning (SRL) [Franceschi et al., 2019] proposes a novel triplet loss and tries to learn scalable representations via randomly sampling time segments.",
                "\u2026Learning (TCL) [Hyvarinen and Morioka, 2016], Contrastive Predictive Coding (CPC) [Oord et al., 2018], Scalable RepresentationLearning (SRL) [Franceschi et al., 2019], Temporal and Contextual Contrasting (TS-TCC) [Eldele et al., 2021a] and Temporal Neighborhood Coding(TNC) [Tonekaboni et\u2026",
                ", 2013], Scalable Representation Learning (SRL) [Franceschi et al., 2019] proposes a novel triplet loss and tries to learn scalable representations via randomly sampling time segments.",
                "Time-Contrastive Learning (TCL) [Hyvarinen and Morioka, 2016], Contrastive Predictive Coding (CPC) [Oord et al., 2018], Scalable RepresentationLearning (SRL) [Franceschi et al., 2019], Temporal and Contextual Contrasting (TS-TCC) [Eldele et al., 2021a] and Temporal Neighborhood Coding(TNC) [Tonekaboni et al., 2021] are all segment-level methods which sample contrastive pairs along temporal axis.",
                "We compare our performances with stateof-the-art approaches CPC, SRL, TS-TCC and TNC.",
                ", 2018], Scalable RepresentationLearning (SRL) [Franceschi et al., 2019], Temporal and Contextual Contrasting (TS-TCC) [Eldele et al."
            ],
            "citingPaper": {
                "paperId": "cd702e9384c44436902c6a1585817c8ed269ba77",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-04770",
                    "CorpusId": 246867524
                },
                "corpusId": 246867524,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cd702e9384c44436902c6a1585817c8ed269ba77",
                "title": "Iterative Bilinear Temporal-Spectral Fusion for Unsupervised Time-Series Representation Learning",
                "abstract": "Unsupervised/self-supervised time series representation learning is a challenging problem because of its complex dynamics and sparse annotations. Existing works mainly adopt the framework of contrastive learning with the time-based augmentation techniques to sample positives and negatives for contrastive training. Nevertheless, they mostly use segment-level augmentation derived from time slicing, which may bring about sampling bias and incorrect optimization with false negatives due to the loss of global context. Besides, they all pay no attention to incorporate the spectral information in feature representation. In this paper, we propose a uni\ufb01ed framework, namely Bilinear Temporal-Spectral Fusion ( BTSF ). Speci\ufb01cally, we \ufb01rstly utilize the instance-level augmentation with a simple dropout on the entire time series for maximally capturing long-term dependencies. We devise a novel iterative bilinear temporal-spectral fusion to explicitly encode the af\ufb01nities of abundant time-frequency pairs, and iteratively re\ufb01nes representations in a fusion-and-squeeze manner with Spectrum-to-Time (S2T) and Time-to-Spectrum (T2S) Aggregation modules. We \ufb01rstly conducts downstream evaluations on three major tasks for time series including classi\ufb01cation, forecasting and anomaly detection. Experimental results shows that our BTSF consistently signi\ufb01cantly outperforms the state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155557947",
                        "name": "Ling Yang"
                    },
                    {
                        "authorId": "2317297",
                        "name": "linda Qiao"
                    },
                    {
                        "authorId": "2156146028",
                        "name": "Luxia Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Following [14, 30], we fit an SVM classifier on the learned representations for classification.",
                "Previous studies in [14, 30] show that the dilated CNN-based networks, such as the temporal convolutional network (TCN) [5], are better at capturing the long-term dependencies in time series than the Recurrent Networks or Transformer based networks.",
                "1, 3, 4, 8, 10, 13, 15 [14] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi.",
                "2 Positive and Negative Selection for Triplet Learning For any time series x, T-Loss [14] selects positive subseries xpos such that the positive subseries itself is part of the anchor subseries, i.",
                "Recent works [14, 28, 13, 30] employ self-supervised contrastive learning to learn time series representations.",
                "[14] presented T-Loss, a time series representation learning method based on triplet loss and time-based negative sampling.",
                "3 Triplet loss Our triplet loss formulation is based on word2vec [19] and T-Loss [14].",
                "We compare TS-Rep with the self-supervised methods [14, 13, 30], and a VAE-based method Incr-VAE [4].",
                "In recent years, self-supervised contrastive learning-based methods have gained attention for generalised time series representation learning [28, 13, 14, 30].",
                "1 Network Architecture Our network is identical to T-Loss [14], and further details about the network are given in Appendix D."
            ],
            "citingPaper": {
                "paperId": "0057e521c26d61779a34910e8e10e66792d35a02",
                "externalIds": {
                    "CorpusId": 259143877
                },
                "corpusId": 259143877,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0057e521c26d61779a34910e8e10e66792d35a02",
                "title": "TS-Rep: Self-supervised time series representation learning from robot sensor data",
                "abstract": "In this paper, we propose TS-Rep, a self-supervised method that learns representations from multi-modal varying-length time series sensor data from real robots. TS-Rep is based on a simple yet effective technique for triplet learning, where we randomly split the time series into two segments to form anchor and positive while selecting random subseries from the other time series in the mini-batch to construct negatives. We additionally use the nearest neighbour in the representation space to increase the diversity in the positives. For evaluation, we perform a clusterability analysis on representations of three heteroge-neous robotics datasets. Then learned representations are applied for anomaly detection, and our method consistently performs well. A classifier trained on TS-Rep learned representations outperforms unsupervised methods and performs close to the fully-supervised methods for terrain classification. Furthermore, we show that TS-Rep is, on average, the fastest method to train among the baselines. Our code is available at https://github.com/imprs/TS-Rep .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1723335184",
                        "name": "Pratik Somaiya"
                    },
                    {
                        "authorId": "3072183",
                        "name": "Harit Pandya"
                    },
                    {
                        "authorId": "20541968",
                        "name": "Riccardo Polvara"
                    },
                    {
                        "authorId": "1728609",
                        "name": "Marc Hanheide"
                    },
                    {
                        "authorId": "2763480",
                        "name": "Grzegorz Cielniak"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026methods use measure of similarity to train the encoders (Lei et al., 2017; Ma et al., 2019; Madiraju et al., 2018), and more recent methods use different types of contrastive objectives for training (Oord et al., 2016; Franceschi et al., 2019; Tonekaboni et al., 2020; Hyvarinen and Morioka, 2016).",
                ", 2018), and more recent methods use different types of contrastive objectives for training (Oord et al., 2016; Franceschi et al., 2019; Tonekaboni et al., 2020; Hyvarinen and Morioka, 2016)."
            ],
            "citingPaper": {
                "paperId": "f57810ba499cded59b323cc76b5f3a08051929d4",
                "externalIds": {
                    "DBLP": "conf/chil/WeatherheadGMME22",
                    "CorpusId": 248396656
                },
                "corpusId": 248396656,
                "publicationVenue": {
                    "id": "67d171e0-fd12-4512-a35d-c4d7af1bd5b3",
                    "name": "ACM Conference on Health, Inference, and Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CHIL",
                        "ACM Conf Health Inference Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f57810ba499cded59b323cc76b5f3a08051929d4",
                "title": "Learning Unsupervised Representations for ICU Timeseries",
                "abstract": "Medical time series like physiological signals provide a rich source of information about patients\u2019 underlying clinical states. Learning such states is a challenging problem for ML but has great utility for clinical applications. It allows us to identify patients with similar underlying conditions, track disease progression over time, and much more. The challenge with medical time series however, is the lack of well-defined labels for a given patient\u2019s state for extended periods of time. Collecting such labels is expensive and often requires substantial effort. In this work, we propose an unsupervised representation learning method, called TRACE, that allows us to learn meaningful patient representations from time series collected in the Intensive Care Unit (ICU). We show the utility and generalizability of these representations in identifying different downstream clinical conditions and also show how the trajectory of representations over time exhibits progression toward critical conditions such as cardiopulmonary arrest or circulatory failure.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2134694104",
                        "name": "A. Weatherhead"
                    },
                    {
                        "authorId": "49415411",
                        "name": "R. Greer"
                    },
                    {
                        "authorId": "145241812",
                        "name": "M. Moga"
                    },
                    {
                        "authorId": "13131345",
                        "name": "M. Mazwi"
                    },
                    {
                        "authorId": "1954568",
                        "name": "D. Eytan"
                    },
                    {
                        "authorId": "49800482",
                        "name": "A. Goldenberg"
                    },
                    {
                        "authorId": "23152217",
                        "name": "S. Tonekaboni"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "One baseline named Scalable Representation Learning [15] is not included in our results, as it requires much longer running time and we failed to produce its results in several days.",
                "Contrast based methods [15], [16] are mainly to apply a segment-level sampling policy to conduct the contrastive learning.",
                "Unsupervised Scalable Representation Learning [15] introduces a novel unsupervised loss with time-based negative sampling to train a scalable encoder, shaped as a deep convolutional neural network with dilated convolutions [30]."
            ],
            "citingPaper": {
                "paperId": "af3dfcecbf4d35820d039cfc31c80477374f49b2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-09928",
                    "DOI": "10.48550/arXiv.2205.09928",
                    "CorpusId": 248965084
                },
                "corpusId": 248965084,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/af3dfcecbf4d35820d039cfc31c80477374f49b2",
                "title": "Cross Reconstruction Transformer for Self-Supervised Time Series Representation Learning",
                "abstract": "\u2014Unsupervised/self-supervised representation learn- ing in time series is critical since labeled samples are usually scarce in real-world scenarios. Existing approaches mainly lever- age the contrastive learning framework, which automatically learns to understand the similar and dissimilar data pairs. Never- theless, they are restricted to the prior knowledge of constructing pairs, cumbersome sampling policy, and unstable performances when encountering sampling bias. Also, few works have focused on effectively modeling across temporal-spectral relations to extend the capacity of representations. In this paper, we aim at learning representations for time series from a new perspective and propose Cross Reconstruction Transformer (CRT) to solve the aforementioned problems in a uni\ufb01ed way. CRT achieves time series representation learning through a cross-domain dropping-reconstruction task. Speci\ufb01cally, we transform time series into the frequency domain and randomly drop certain parts in both time and frequency domains. Dropping can maximally preserve the global context compared to cropping and masking. Then a transformer architecture is utilized to adequately capture the cross-domain correlations between temporal and spectral information through reconstructing data in both domains, which is called Dropped Temporal-Spectral Modeling. To discriminate the representations in global latent space, we propose Instance Discrimination Constraint to reduce the mutual information between different time series and sharpen the decision boundaries. Additionally, we propose a speci\ufb01ed curriculum learning strategy to optimize the CRT, which progressively increases the dropping ratio in the training process. We conduct extensive experiments to evaluate the effectiveness of the proposed method on multiple real-world datasets. Results show that CRT consistently achieves the best performance over existing methods by 2% \u223c 9%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "117636048",
                        "name": "Wen-Rang Zhang"
                    },
                    {
                        "authorId": "2155557947",
                        "name": "Ling Yang"
                    },
                    {
                        "authorId": "37953292",
                        "name": "Shijia Geng"
                    },
                    {
                        "authorId": "2317297",
                        "name": "linda Qiao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "In summary, the results demonstrate that the SEAnet DEA is robust across various dataset properties and outperforms its competitors by better preserving original pairwise distances and nearest neighborhood structure, leading to better approximate similarity search results than traditional (PAA-based) and alternative deep learning (DEA-based using FDJNet [23], TimeNet [26], and InceptionTime [27]) approaches.",
                "Unlike most existing encoders with linear final layers [23], the SEAnet encoder is finalized by LayerNorm2, which is specifically designed using the SoS preservation principle."
            ],
            "citingPaper": {
                "paperId": "3bb7b1a6374e514c6e278b935e5da5968e87f5c0",
                "externalIds": {
                    "DBLP": "conf/vldb/Wang22",
                    "CorpusId": 251771004
                },
                "corpusId": 251771004,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3bb7b1a6374e514c6e278b935e5da5968e87f5c0",
                "title": "Data Series Similarity Search via Deep Learning",
                "abstract": "A key operation for the (increasingly large) data series collection analysis is similarity search. According to recent studies, SAX-based indexes offer state-of-the-art performance for similarity search tasks. However, their performance lags under high-frequency, weakly correlated, excessively noisy, or other dataset-specific properties. In this work, we propose to facilitate data series similarity search with deep learning techniques, involving both data series approximation and data series indexing. Our preliminary study focuses on developing Deep Embedding Approximation (DEA), a novel family of data series summarization techniques based on deep neural networks. Moreover, we describe SEAnet, a novel architecture specially designed for learning DEA, that introduces the Sum of Squares preservation property into the deep network design. Finally, we propose a new sampling strategy, SEASam, that allows SEAnet to effectively train on massive datasets. Comprehensive experiments verify the advantages of DEA learned using SEAnet. These preliminary results can lead to further progress in this area, by developing more customized architectures and training strategies, better integrating DEA with index structures, learning novel data series indexes, and facilitating faster model training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116068893",
                        "name": "Qitong Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d8ef762d782ed4b2a51b76cbcb116069faad5fc1",
                "externalIds": {
                    "CorpusId": 259143811
                },
                "corpusId": 259143811,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d8ef762d782ed4b2a51b76cbcb116069faad5fc1",
                "title": "Time Series Anomaly Detection using Skip-Step Contrastive Predictive Coding",
                "abstract": "Self-supervised learning (SSL) shows impressive performance in many tasks lack-ing sufficient labels. In this paper, we study SSL in time series anomaly detection (TSAD) by incorporating the characteristics of time series data. Specifically, we build an anomaly detection algorithm consisting of global pattern learning and local association learning. The global pattern learning module builds encoder and decoder to reconstruct the raw time series data to detect global anomalies. To complement the limitation of the global pattern learning that ignores local associations between anomaly points and their adjacent windows, we design a local association learning module, which leverages contrastive predictive coding (CPC) to transform the identification of anomaly points into positive pairs identification in contrastive learning. Motivated by the observation that adjusting the distance between the history window and the time point to be detected directly impacts the detection performance in the CPC framework, we further propose a skip-step CPC scheme in the local association learning module which adjusts the distance for better construction of the positive pairs and detection results. The experimental results show that the proposed algorithm achieves superior performance on multiple benchmark datasets in comparison with 11 state-of-the-art algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2119059780",
                        "name": "Kexin Zhang"
                    },
                    {
                        "authorId": "2219955133",
                        "name": "\u2020. QingsongWen"
                    },
                    {
                        "authorId": "2152737103",
                        "name": "Chaoli Zhang"
                    },
                    {
                        "authorId": "2110940043",
                        "name": "Liang Sun"
                    },
                    {
                        "authorId": "1679704",
                        "name": "Y. Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "T-Loss [6] uses random sub-series from the original time series as positive samples.",
                "We compare TS2Vec with other SOTAs of unsupervised time series representation, including T-Loss [6] and TNC [5], on time series classification tasks.",
                "We adopt the same evaluation protocol as [6], which trains a SVM with RBF kernel on top of the instance-level representations to predict the label of an instance.",
                "The representation dimensions of TS2Vec, T-Loss and TNC are all set to 320 and under SVM evaluation protocol [6] for fair comparison.",
                "To address this problem, T-Loss [6] employs time-based negative sampling and a triplet loss to learn scalable representations for multivariate time series.",
                "Many studies [4, 5, 6, 7] focused on learning instance-level representations, which described the whole segment of the input time series and have showed great success in downstream tasks like clustering and classification.",
                ", TLoss [6]) employed the contrastive loss to learn the inherent structure of time series.",
                "Following [6], the representation dimension is set to 320."
            ],
            "citingPaper": {
                "paperId": "c16bd53732f4f18e3bcf25fddf7b21a1d647bcb5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-10466",
                    "CorpusId": 235489858
                },
                "corpusId": 235489858,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c16bd53732f4f18e3bcf25fddf7b21a1d647bcb5",
                "title": "Learning Timestamp-Level Representations for Time Series with Hierarchical Contrastive Loss",
                "abstract": "This paper presents TS2Vec, a universal framework for learning timestamp-level representations of time series. Unlike existing methods, TS2Vec performs timestamp-wise discrimination, which learns a contextual representation vector directly for each timestamp. We \ufb01nd that the learned representations have superior predictive ability. A linear regression trained on top of the learned representations outperforms previous SOTAs for supervised time series forecasting. Also, the instance-level representations can be simply obtained by applying a max pooling layer on top of learned representations of all timestamps. We conduct extensive experiments on time series classi\ufb01cation tasks to evaluate the quality of instance-level representations. As a result, TS2Vec achieves signi\ufb01cant improvement compared with existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The source code is publicly available at https://github.com/yuezhihan/ts2vec .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113956163",
                        "name": "Zhihan Yue"
                    },
                    {
                        "authorId": "2115657798",
                        "name": "Yujing Wang"
                    },
                    {
                        "authorId": "2104164",
                        "name": "Juanyong Duan"
                    },
                    {
                        "authorId": "40047504",
                        "name": "Tianmeng Yang"
                    },
                    {
                        "authorId": "3261801",
                        "name": "Congrui Huang"
                    },
                    {
                        "authorId": "1388682473",
                        "name": "Bixiong Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[1] ein rein Encoder-basiertes Netzwerk in Kombination mit einer sogenannten Triplet-Loss-Funktion f\u00fcr die Klassifikation auf diversen Referenzzeitreihen nutzen.",
                "Methoden des Representation Learning haben sich in j\u00fcngster Zeit im Bereich der Bild- und Sprachverarbeitung etabliert und erm\u00f6glichen eine un\u00fcberwachte Featureextraktion, welche die Pr\u00e4diktionsg\u00fcte g\u00e4ngiger Verfahren der manuellen und automatisierten Featureextraktion im Fall von nur wenigen vorhandenen gelabelten Daten \u00fcbertrifft [1]."
            ],
            "citingPaper": {
                "paperId": "7be8c1c9c917842a32eb0993ef850ceeff697c41",
                "externalIds": {
                    "CorpusId": 235176323
                },
                "corpusId": 235176323,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7be8c1c9c917842a32eb0993ef850ceeff697c41",
                "title": "Featureextraktion aus heterogenen multivariaten Zeitreihendaten me- chatronischer Systeme mittels Autoencoder-Netzwerken Autoencoder-based Feature Extraction from Heterogeneous Multiva- riate Time Series Data of Mechatronic Systems",
                "abstract": "Sensor and control data of modern mechatronic systems are often available as heterogeneous time series with different sampling rates and value ranges. Suitable classification and regression methods from the field of supervised machine learning already exist for predictive tasks, for example in the context of condition monitoring, but their performance scales strongly with the number of labeled training data. Their provision is often associated with high effort in the form of personhours or additional sensors. In this paper, we present a method for unsupervised feature extraction using autoencoder networks that specifically addresses the heterogeneous nature of the database and reduces the amount of labeled training data required compared to existing methods. Three public datasets of mechatronic systems from different application domains are used to validate the results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1387974601",
                        "name": "Karl-Philipp Kortmann"
                    },
                    {
                        "authorId": "2104499461",
                        "name": "Moritz Fehsenfeld"
                    },
                    {
                        "authorId": "3099334",
                        "name": "M. Wielitzka"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Predicting neighboring time series segments, have also resulted in similarly powerful embeddings for time series clustering tasks(Franceschi et al., 2019).",
                "Negative sampling techniques have been used in the past in self-supervised learning for time series, in changepoint detection(Deldari et al., 2020) and clustering(Franceschi et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "99c34731bb9afaaa94b647d61f670b3c0e1f0d9f",
                "externalIds": {
                    "CorpusId": 236973347
                },
                "corpusId": 236973347,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/99c34731bb9afaaa94b647d61f670b3c0e1f0d9f",
                "title": "Changepoint Detection using Self Supervised Variational AutoEncoders",
                "abstract": "Changepoint Detection methods aim to find locations where a time series shows abrupt changes in properties, such as level and trend, which persist with time. Traditional parametric approaches assume specific generative models for each segment of the time series, but often, the complexities of real time series data are hard to capture with such models. To address these issues, in this paper, we propose VAE-CP, which uses a variational autoencoder with self supervised loss functions to learn informative latent representations of time series segments. We use traditional hypothesis test based and Bayesian changepoint methods in this latent space of normally distributed latent variables, thus combining the strength of self-supervised representation learning, with parametric changepoint modeling. This proposed approach outperforms traditional and previous deep learning based changepoint detection methods in synthetic and real datasets containing trend changes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47320875",
                        "name": "S. Chatterjee"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "32acfd0ddb254a2b15943acb9d5f91dc1a33565e",
                "externalIds": {
                    "CorpusId": 238863819
                },
                "corpusId": 238863819,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/32acfd0ddb254a2b15943acb9d5f91dc1a33565e",
                "title": "Generative Adversarial Networks used for Latent Space Optimization: A Comparative Study for Partial Discharge Analysis",
                "abstract": "Hydrogenerators are complex equipment with many components where more than 100 failure mechanisms can be active. During normal operation, the high voltage stator, one of the main components of hydrogenerators, is always subjected to Partial Discharge (PD) activity. Multiple sources of PD activity can be active simultaneously. Global PD signals which include all active PD sources are obtained from periodic measurements made on hydrogenerators while they are in operation. PD measurements are an effective diagnostic tool for evaluating the integrity of the stator winding, similar to signals coming from an electrocardiogram for the health status of a human. Quantifying PD activity is still a challenge in the industry since the recognition of the type of PD is not trivial and still requires expert judgement. Since the degradation rate of all active PD sources is different, automatic classification of PD source is thus essential to monitor their evolution. With that goal in mind, an extensive effort was initiated in 2019 to automatically recognize individual PD sources from 2D Partial Discharge Analyzer (PDA) files using Deep Learning (DL) techniques. In this context, this paper presents the use of a Generative Adversarial Network (GAN) in combination with A Variational Autoencoder (VAE) for increasing the representativeness of each PD sources in the VAE latent space.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50203462",
                        "name": "R. Zemouri"
                    },
                    {
                        "authorId": "39139820",
                        "name": "M. L\u00e9vesque"
                    },
                    {
                        "authorId": "1399714695",
                        "name": "O. Kokoko"
                    },
                    {
                        "authorId": "48136604",
                        "name": "C. Hudon"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "[13] combined an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining low-dimensional features for multivariate time series of variable length."
            ],
            "citingPaper": {
                "paperId": "ab760067b604656f5e8d8597b5bf784693a1ab8e",
                "externalIds": {
                    "DBLP": "journals/access/ZhengYGZZL21",
                    "DOI": "10.1109/ACCESS.2021.3124009",
                    "CorpusId": 240203667
                },
                "corpusId": 240203667,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ab760067b604656f5e8d8597b5bf784693a1ab8e",
                "title": "Multivariate Extreme Learning Machine Based AutoEncoder for Electricity Consumption Series Clustering",
                "abstract": "Multivariate electricity consumption series clustering can reflect the trend of power consumption changes in the past time period, which can provide reliable guidance for electricity production. The dimensionality reduction-based method is an effective technology to address this problem, which obtains the low-dimensional features of each variate or all variates for multivariate time series clustering. However, most existing dimensionality reduction-based methods ignore the joint learning of the common representations and the variable-based representations. In this paper, we build a multivariate extreme learning machine based autoencoder model for electricity consumption clustering (MELM-EC), which performs common representation learning and variable-based representation learning simultaneously. MELM-EC maps the common representation and multiple variable-based representations to the original multivariate time series and computes the common output weights within a few iterations. Experimental results on realistic multivariate time series datasets and multivariate electricity consumption series datasets demonstrate the effectiveness of the proposed MELM-EC model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2052687944",
                        "name": "Kaihong Zheng"
                    },
                    {
                        "authorId": "2144505598",
                        "name": "Jingfeng Yang"
                    },
                    {
                        "authorId": "2135935884",
                        "name": "Qihang Gong"
                    },
                    {
                        "authorId": "2111057051",
                        "name": "Shangli Zhou"
                    },
                    {
                        "authorId": "73189900",
                        "name": "Lukun Zeng"
                    },
                    {
                        "authorId": "2153701278",
                        "name": "Sheng Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "For time series data such as audio and ECG, the metric learning based methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al., 2019; Saeed et al., 2020), or multi-task learning based methods that predict different handcrafted features such as MFCCs, prosody,\u2026",
                "Triplet Loss5 (Franceschi et al., 2019) We download the authors\u2019 official source code and use the same backbone as SelfTime, and set the number of negative samples as 10.",
                "More recently, in the time series analysis domain, some metric learning based self-supervised methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al., 2019; Saeed et al., 2020), or multi-task learning based self-supervised methods that predict different\u2026",
                "More recently, in the time series analysis domain, some metric learning based self-supervised methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al.",
                "Among those baselines, either global features (Deep InfoMax, Transformation, SimCLR, Relation) or local features (Triplet Loss, Deep InfoMax, Forecast) are considered during representation learning, they neglect the essential temporal information of time series except Triplet Loss and Forecast.",
                "For time series data such as audio and ECG, the metric learning based methods such as triplet loss (Franceschi et al., 2019) and contrastive loss (Schneider et al.",
                "Also, in Triplet Loss, a time-based negative sampling is used to capture the inter-sample temporal relation among time pieces sampled from the different time series, which is cannot directly and efficiently capture the intra-sample temporal pattern of time series.",
                "\u2022 Triplet Loss (Franceschi et al., 2019) is an unsupervised time series representation learning model that uses triplet loss to push a subsequence of time series close to its context and distant from a randomly chosen time series."
            ],
            "citingPaper": {
                "paperId": "fbe31c21649697bd24ec48c562b1c70f1355e330",
                "externalIds": {
                    "CorpusId": 227211017
                },
                "corpusId": 227211017,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fbe31c21649697bd24ec48c562b1c70f1355e330",
                "title": "Short-term Middle-term Long-term Negative Positive Anchor Sample Inter-sample Relation Intra-temporal Relation Negative Sample Positive Sample Anchor Piece Temporal Relation : Sample Relation :",
                "abstract": "Self-supervised learning achieves superior performance in many domains by extracting useful representations from the unlabeled data. However, most of traditional self-supervised methods mainly focus on exploring the inter-sample structure while less efforts have been concentrated on the underlying intra-temporal structure, which is important for time series data. In this paper, we present SelfTime: a general Self-supervised Time series representation learning framework, by exploring the inter-sample relation and intra-temporal relation of time series to learn the underlying structure feature on the unlabeled time series. Specifically, we first generate the inter-sample relation by sampling positive and negative samples of a given anchor sample, and intra-temporal relation by sampling time pieces from this anchor. Then, based on the sampled relation, a shared feature extraction backbone combined with two separate relation reasoning heads are employed to quantify the relationships of the sample pairs for inter-sample relation reasoning, and the relationships of the time piece pairs for intra-temporal relation reasoning, respectively. Finally, the useful representations of time series are extracted from the backbone under the supervision of relation reasoning heads. Experimental results on multiple real-world time series datasets for time series classification task demonstrate the effectiveness of the proposed method. Code and data are publicly available 1.",
                "year": 2020,
                "authors": []
            }
        },
        {
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "To this end, we use the triplet loss introduced in [2] that follows the word2vec\u2019s intuition.",
                "[2] aims to classify time series using an unsupervised representation learning with causal Convolutional Neural Network (CNN).",
                "We will pre-train our encoder in a self-supervised manner using the so-called triplet loss defined in [2].",
                "Our results are consistent across all datasets that we trained on: while the benchmark performed just under the state-of-the-art results of Franceschi et al [2], the Transformer approach failed to leverage the representation learned in the pre-training and is overfitting early in the training process.",
                "Based on the recent success of Transformers [4] on machine translation, we try to combine the unsupervised representation learning of [2] with a Transformer to improve the accuracy on the standardized datasets of the UCR archive1.",
                "The work of Franceschi et al. [2] aims to classify time series using an unsupervised representation learning with causal Convolutional Neural Network (CNN).",
                "We observe that while our benchmark EmbConv performs just under the state-of-the art results of Franceschi et al. [2], the simplified BERT BenchBert fails to produce viable results and is prone to over-fitting, even when performing a pre-training.",
                "Notice that EmbConv reaches an accuracy of 69% on the testing set, where the state-ofthe-art method presented in [2] reaches an accuracy of 74%.",
                "[2], the simplified BERT BenchBert fails to produce viable results and is prone to over-fitting, even when performing a pre-training."
            ],
            "citingPaper": {
                "paperId": "071457b02ddba8a38b4d6af5fe962705ac018042",
                "externalIds": {
                    "CorpusId": 235488921
                },
                "corpusId": 235488921,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/071457b02ddba8a38b4d6af5fe962705ac018042",
                "title": "Self-supervised learning for time series classification",
                "abstract": "Time series classification (TSC) is an important and challenging problem in machine learning. In this work, we tackle the problem of TSC by first applying a Bidirectional Encoder Representations from Transformers (BERT) model, and then applying a convolutional neural network (CNN) for the classification. We report suboptimal results compared to the state-of-the-art, as the model is overfitting early in the training process. We believe that this issue might be overcome by tuning the hyperparameters more carefully.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2113892850",
                        "name": "Anne-Sophie Van de Velde"
                    },
                    {
                        "authorId": "2113892826",
                        "name": "Martin Jaggi Co-Supervisor"
                    },
                    {
                        "authorId": "51440515",
                        "name": "Jean-Baptiste Cordonnier"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "All rights reserved.\nby the application of deep neural networks, which learn appropriate representations of time series within their hidden layers (Franceschi, Dieuleveut, and Jaggi 2019)."
            ],
            "citingPaper": {
                "paperId": "37df5f586c8dae552401ad9f077b53c0ef53bbee",
                "externalIds": {
                    "DBLP": "conf/flairs/MohrWH0K20",
                    "MAG": "3037813365",
                    "CorpusId": 219324368
                },
                "corpusId": 219324368,
                "publicationVenue": {
                    "id": "546d164a-fc31-4aee-99a5-879e03ff7d36",
                    "name": "The Florida AI Research Society",
                    "type": "conference",
                    "alternate_names": [
                        "Fla AI Res Soc",
                        "FLAIRS"
                    ],
                    "url": "http://www.flairs.com/"
                },
                "url": "https://www.semanticscholar.org/paper/37df5f586c8dae552401ad9f077b53c0ef53bbee",
                "title": "New Approaches in Ordinal Pattern Representations for Multivariate Time Series",
                "abstract": "Many practical applications involve classi\ufb01cation tasks on time series data, e.g., the diagnosis of cardiac insuf\ufb01ciency by evaluating the recordings of an electrocardiogram. Since most machine learning algorithms for classi\ufb01cation are not capable of dealing with time series directly, mappings of time series to scalar values, also called representations, are applied before using these algorithms. Finding ef\ufb01cient mappings, which capture the characteristics of a time series is subject of the \ufb01eld of representation learning and especially valu- able in cases of few data samples. Time series representations based on information theoretic entropies are a proven and well-established approach. Since this approach assumes a total ordering it is only directly applicable to univariate time se- ries and thus rendering it dif\ufb01cult for many real-world applications dealing with multiple measurements at the same time. Some extensions were established which also cope with multivariate time series data, but none of the existing approaches take into account potential correlations between the movement of the variables. In this paper we propose two new ap- proaches, considering the correlation between multiple variables, which outperform state-of-the-art algorithms on real- world data sets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1576293632",
                        "name": "Marisa Mohr"
                    },
                    {
                        "authorId": "2052594289",
                        "name": "Florian Wilhelm"
                    },
                    {
                        "authorId": "40112877",
                        "name": "Mattis Hartwig"
                    },
                    {
                        "authorId": "113364585",
                        "name": "R. M\u00f6ller"
                    },
                    {
                        "authorId": "47466736",
                        "name": "K. Keller"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "3e94a97620036b863b133bc411754e4550931517",
                "externalIds": {
                    "CorpusId": 231718041
                },
                "corpusId": 231718041,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3e94a97620036b863b133bc411754e4550931517",
                "title": "Attention mechanism on disentangled contextual factor representations for time series generation",
                "abstract": "Learning & predicting sequence dynamics remain an outstanding research issue. Many application domains ranging from sales forecast to smart cities face challenging problems. Most of the approaches in the literature have weaknesses regarding the modelling of the time series context. We choose to tackle long-term forecasting with a focus on learning disentangled representations of contextual factors. We turned the forecasting problem into a generative problem : given spatial, temporal and other factors, we are able to generate the corresponding time series. Not only are we able to predict the sequence dynamics for a specific day or location, but we also show that our architecture is very robust to the introduction of outliers in the training set. We propose different variants of our approach, including an encoder-decoder architecture that enables us to extrapolate all contextual combinations corresponding to a new factor observed only once. This ambitious formulation raises theoretical questions about how to aggregate information associated with the different factors. This article examines several options and highlights the value of introducing an attention mechanism to improve the effectiveness of this critical step.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1399324742",
                        "name": "P. Cribier-Delande"
                    },
                    {
                        "authorId": "2342900",
                        "name": "Raphael Puget"
                    },
                    {
                        "authorId": "2232776",
                        "name": "V. Guigue"
                    },
                    {
                        "authorId": "8905591",
                        "name": "Ludovic Denoyer"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "We also experiment with a recent general-purpose unsupervised timeseries representation proposed in [5].",
                "Though USRL demonstrates encouraging performance on timeseries classification [5], it does not perform well on timeseries anomaly detection."
            ],
            "citingPaper": {
                "paperId": "fc36d50be4352afada233f55a59f0dca0a7a826b",
                "externalIds": {
                    "DBLP": "conf/nips/ShenLK20",
                    "MAG": "3099971460",
                    "CorpusId": 227276245
                },
                "corpusId": 227276245,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fc36d50be4352afada233f55a59f0dca0a7a826b",
                "title": "Timeseries Anomaly Detection using Temporal Hierarchical One-Class Network",
                "abstract": "Real-world timeseries have complex underlying temporal dynamics and the detection of anomalies is challenging. In this paper, we propose the Temporal Hierarchical One-Class (THOC) network, a temporal one-class classification model for timeseries anomaly detection. It captures temporal dynamics in multiple scales by using a dilated recurrent neural network with skip connections. Using multiple hyperspheres obtained with a hierarchical clustering process, a one-class objective called Multiscale Vector Data Description is defined. This allows the temporal dynamics to be well captured by a set of multi-resolution temporal clusters. To further facilitate representation learning, the hypersphere centers are encouraged to be orthogonal to each other, and a self-supervision task in the temporal domain is added. The whole model can be trained end-to-end. Extensive empirical studies on various real-world timeseries demonstrate that the proposed THOC network outperforms recent strong deep learning baselines on timeseries anomaly detection.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48491340",
                        "name": "Lifeng Shen"
                    },
                    {
                        "authorId": "2118394614",
                        "name": "Zhuocong Li"
                    },
                    {
                        "authorId": "145193332",
                        "name": "J. Kwok"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "The paper Unsupervised Scalable Representation Learning for Multivariate Time Series by [1] presents an unsupervised approach to learning representations for time series that can be used for subsequent classification.",
                "This is the case for these experiments since subseries are explicitly sampled at different lengths from the dataset (see Algorithm 1 in [1]).",
                "In this work, we have presented a replication study of the work by [1] and found that most of the results are replicable."
            ],
            "citingPaper": {
                "paperId": "ffee2d5ad221664a785ff0ea1fea04efc9261d05",
                "externalIds": {
                    "CorpusId": 211029415
                },
                "corpusId": 211029415,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ffee2d5ad221664a785ff0ea1fea04efc9261d05",
                "title": "[Re] Unsupervised Scalable Representation Learning for Multivariate Time Series",
                "abstract": "The paper Unsupervised Scalable Representation Learning for Multivariate Time Series by [1] presents an unsupervised approach to learning representations for time series that can be used for subsequent classification. An encoder architecture with dilated causal convolutional blocks is trained to minimize a triplet loss. The triplet loss is based on the idea that a subseries, xpos, of a time series, xref , should be closer to xref in representation space than the representation of a randomly sampled time series from the dataset, xneg. This type of loss was first introduced by [2]. To evaluate the strength of the learned representations, an SVM classifier is trained using the labels corresponding to these representations. The evaluation is performed on 159 datasets, together constituting themain benchmarking datasets for time series. The first group of datasets, the UCR Archive [3], contains univariate, short (between 15 and 2844 time steps) series; the second group, theUEAdataset [4], containsmultivariate time series of up to 1345 dimensions and varying sample lengths. The last dataset, the Individual Household Electric Power Consumption (IHEPC) dataset from the UCI Machine Learning Repository [5], is a single seven-dimensional time series with more than two million time steps. The authors\u02bc intention with using this variety of datasets is to show the universality of their method, which is one of their main claims. We have re-implemented the authors\u02bc method from scratch as best as we could by using only the paper as instruction. It should be noted that a code repository for the article was made public by the authors, but this was not employed for the purpose of investigating the replicability of the work starting from scratch. This means that we adhere to the Replication track of the Reproducibility challenge for NeurIPS 2019. Our implementation, as well as the authors ,\u0313 was made using the Pytorch [6] library and can be found at https://github.com/lilfelix/reproducibility_NeurIPS19. The main motivation to do a full replication study, avoiding the use of readily provided code, is to better be able to detect if there are parts of the implementation that are crucial for the results, but not presented as such in the original paper. This risk exists generally in machine learning [7], and has in particular been brought to light for deep learning (e.g. [8], [9]), due to the latter s\u0313 typically vast number of hyperparameters to tune.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2083683058",
                        "name": "Felix Liljefors"
                    }
                ]
            }
        }
    ]
}