{
    "offset": 0,
    "data": [
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "f131a32da42bfca149d90f86f0c6bee9aee5823d",
                "externalIds": {
                    "DOI": "10.3389/frai.2023.1124553",
                    "CorpusId": 260231750
                },
                "corpusId": 260231750,
                "publicationVenue": {
                    "id": "6a8c0041-d0b7-4e32-b52c-33adef005c7e",
                    "name": "Frontiers in Artificial Intelligence",
                    "alternate_names": [
                        "Front Artif Intell"
                    ],
                    "issn": "2624-8212",
                    "url": "https://www.frontiersin.org/journals/artificial-intelligence#"
                },
                "url": "https://www.semanticscholar.org/paper/f131a32da42bfca149d90f86f0c6bee9aee5823d",
                "title": "Decision trees: from efficient prediction to responsible AI",
                "abstract": "This article provides a birds-eye view on the role of decision trees in machine learning and data science over roughly four decades. It sketches the evolution of decision tree research over the years, describes the broader context in which the research is situated, and summarizes strengths and weaknesses of decision trees in this context. The main goal of the article is to clarify the broad relevance to machine learning and artificial intelligence, both practical and theoretical, that decision trees still have today.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1755851",
                        "name": "H. Blockeel"
                    },
                    {
                        "authorId": "1393693474",
                        "name": "Laurens Devos"
                    },
                    {
                        "authorId": "1786603",
                        "name": "Beno\u00eet Fr\u00e9nay"
                    },
                    {
                        "authorId": "30094568",
                        "name": "G\u00e9raldin Nanfack"
                    },
                    {
                        "authorId": "1702481",
                        "name": "Siegfried Nijssen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "6b88732c04d21ad0eed4431cb7da0dd20700c579",
                "externalIds": {
                    "DBLP": "journals/eswa/NaNL23",
                    "DOI": "10.2139/ssrn.4379466",
                    "CorpusId": 257393055
                },
                "corpusId": 257393055,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6b88732c04d21ad0eed4431cb7da0dd20700c579",
                "title": "Toward practical and plausible counterfactual explanation through latent adjustment in disentangled space",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210887063",
                        "name": "Seung-Hyup Na"
                    },
                    {
                        "authorId": "148354920",
                        "name": "Woo-Jeoung Nam"
                    },
                    {
                        "authorId": "2164853907",
                        "name": "Seong-Whan Lee"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In another direction, some works generate CEs for specific model categories, such as tree-based (Lucic et al., 2022; Tolomei et al., 2017; Parmentier & Vidal, 2021) or differentiable (Dhurandhar et al., 2018) models."
            ],
            "citingPaper": {
                "paperId": "a4b63425b1e583b4388030a5b29976b20fdca137",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-17021",
                    "ArXiv": "2305.17021",
                    "DOI": "10.48550/arXiv.2305.17021",
                    "CorpusId": 258947157
                },
                "corpusId": 258947157,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a4b63425b1e583b4388030a5b29976b20fdca137",
                "title": "GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations",
                "abstract": "Counterfactual explanations have been widely studied in explainability, with a range of application dependent methods prominent in fairness, recourse and model understanding. The major shortcoming associated with these methods, however, is their inability to provide explanations beyond the local or instance-level. While many works touch upon the notion of a global explanation, typically suggesting to aggregate masses of local explanations in the hope of ascertaining global properties, few provide frameworks that are both reliable and computationally tractable. Meanwhile, practitioners are requesting more efficient and interactive explainability tools. We take this opportunity to propose Global&Efficient Counterfactual Explanations (GLOBE-CE), a flexible framework that tackles the reliability and scalability issues associated with current state-of-the-art, particularly on higher dimensional datasets and in the presence of continuous features. Furthermore, we provide a unique mathematical analysis of categorical feature translations, utilising it in our method. Experimental evaluation with publicly available datasets and user studies demonstrate that GLOBE-CE performs significantly better than the current state-of-the-art across multiple metrics (e.g., speed, reliability).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2081992675",
                        "name": "D. Ley"
                    },
                    {
                        "authorId": "28265392",
                        "name": "Saumitra Mishra"
                    },
                    {
                        "authorId": "1738142",
                        "name": "D. Magazzeni"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Counterfactual: As a sub-class of contrastive, counterfactual explanations identify the required changes on the input side that would have significant impact on the output [14, 42, 44]."
            ],
            "citingPaper": {
                "paperId": "b80cebb67b2d157119a51097e48de1d55b66ee4e",
                "externalIds": {
                    "DOI": "10.3233/sw-233297",
                    "CorpusId": 258882619
                },
                "corpusId": 258882619,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b80cebb67b2d157119a51097e48de1d55b66ee4e",
                "title": "Engineering user-centered explanations to query answers in ontology-driven socio-technical systems",
                "abstract": "The role of explanations in intelligent systems has in the last few years entered the spotlight as AI-based solutions appear in an ever-growing set of applications. Though data-driven (or machine learning) techniques are often used as examples of how opaque (also called black box) approaches can lead to problems such as bias and general lack of explainability and interpretability, in reality these features are difficult to tame in general, even for approaches that are based on tools typically considered to be more amenable, like knowledge-based formalisms. In this paper, we continue a line of research and development towards building tools that facilitate the implementation of explainable and interpretable hybrid intelligent socio-technical systems, focusing on features that users can leverage to build explanations to their queries. In particular, we present the implementation of a recently-proposed application framework (and make available its source code) for developing such systems, and explore user-centered mechanisms for building explanations based both on the kinds of explanations required (such as counterfactual, contextual, etc.) and the inputs used for building them (coming from various sources, such as the knowledge base and lower-level data-driven modules). In order to validate our approach, we develop two use cases, one as a running example for detecting hate speech in social platforms and the other as an extension that also contemplates cyberbullying scenarios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2292875",
                        "name": "J. C. Teze"
                    },
                    {
                        "authorId": "145367381",
                        "name": "Jos\u00e9 Paredes"
                    },
                    {
                        "authorId": "1683358",
                        "name": "Maria Vanina Martinez"
                    },
                    {
                        "authorId": "1405183475",
                        "name": "Gerardo I. Simari"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "This issue may limit their deployment, especially in some critical domains, considering the increasing demand for explainable artificial intelligence (XAI) worldwide [4, 10, 12, 18, 19, 22, 24, 25]."
            ],
            "citingPaper": {
                "paperId": "afbd4b202dcd599a6906bfc0dd219b5abbf9f0b8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00574",
                    "ArXiv": "2305.00574",
                    "DOI": "10.1145/3539618.3592070",
                    "CorpusId": 258427017
                },
                "corpusId": 258427017,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/afbd4b202dcd599a6906bfc0dd219b5abbf9f0b8",
                "title": "The Dark Side of Explanations: Poisoning Recommender Systems with Counterfactual Examples",
                "abstract": "Deep learning-based recommender systems have become an integral part of several online platforms. However, their black-box nature emphasizes the need for explainable artificial intelligence (XAI) approaches to provide human-understandable reasons why a specific item gets recommended to a given user. One such method is counterfactual explanation (CF). While CFs can be highly beneficial for users and system designers, malicious actors may also exploit these explanations to undermine the system's security. In this work, we propose H-CARS, a novel strategy to poison recommender systems via CFs. Specifically, we first train a logical-reasoning-based surrogate model on training data derived from counterfactual explanations. By reversing the learning process of the recommendation model, we thus develop a proficient greedy algorithm to generate fabricated user profiles and their associated interaction records for the aforementioned surrogate model. Our experiments, which employ a well-known CF generation method and are conducted on two distinct datasets, show that H-CARS yields significant and successful attack performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2157281262",
                        "name": "Ziheng Chen"
                    },
                    {
                        "authorId": "2192306989",
                        "name": "Fabrizio Silvestri"
                    },
                    {
                        "authorId": "2144547216",
                        "name": "Jia Wang"
                    },
                    {
                        "authorId": "1591136873",
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "authorId": "2651748",
                        "name": "Gabriele Tolomei"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "\u2026of the existing CE methods can be categorized depending on their optimization methods; gradient-based [Wachter et al., 2018, Mothilal et al., 2020, Lucic et al., 2022], integer optimization [Ustun et al., 2019, Kanamori et al., 2020, Parmentier and Vidal, 2021], autoencoders [Pawelczyk et al.,\u2026"
            ],
            "citingPaper": {
                "paperId": "5885542d7c9bffc1c3ca17bb542f60870ef2974e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-14606",
                    "ArXiv": "2304.14606",
                    "DOI": "10.48550/arXiv.2304.14606",
                    "CorpusId": 258418240
                },
                "corpusId": 258418240,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5885542d7c9bffc1c3ca17bb542f60870ef2974e",
                "title": "Counterfactual Explanation with Missing Values",
                "abstract": "Counterfactual Explanation (CE) is a post-hoc explanation method that provides a perturbation for altering the prediction result of a classifier. Users can interpret the perturbation as an\"action\"to obtain their desired decision results. Existing CE methods require complete information on the features of an input instance. However, we often encounter missing values in a given instance, and the previous methods do not work in such a practical situation. In this paper, we first empirically and theoretically show the risk that missing value imputation methods affect the validity of an action, as well as the features that the action suggests changing. Then, we propose a new framework of CE, named Counterfactual Explanation by Pairs of Imputation and Action (CEPIA), that enables users to obtain valid actions even with missing values and clarifies how actions are affected by imputation of the missing values. Specifically, our CEPIA provides a representative set of pairs of an imputation candidate for a given incomplete instance and its optimal action. We formulate the problem of finding such a set as a submodular maximization problem, which can be solved by a simple greedy algorithm with an approximation guarantee. Experimental results demonstrated the efficacy of our CEPIA in comparison with the baselines in the presence of missing values.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "134252715",
                        "name": "Kentaro Kanamori"
                    },
                    {
                        "authorId": "2134977",
                        "name": "Takuya Takagi"
                    },
                    {
                        "authorId": "47891756",
                        "name": "Ken Kobayashi"
                    },
                    {
                        "authorId": "51916558",
                        "name": "Yuichi Ike"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Various techniques for generating counterfactual explanations have been developed, including modelagnostic [19], model-specific [20], and adversarial methods [21]."
            ],
            "citingPaper": {
                "paperId": "9fbab8ad61dfb78877d30f1a403154c767ec5bc4",
                "externalIds": {
                    "DOI": "10.1101/2023.04.16.23288633",
                    "CorpusId": 258299665
                },
                "corpusId": 258299665,
                "publicationVenue": {
                    "id": "d5e5b5e7-54b1-4f53-82fc-4853f3e71c58",
                    "name": "medRxiv",
                    "type": "journal",
                    "url": "https://www.medrxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/9fbab8ad61dfb78877d30f1a403154c767ec5bc4",
                "title": "SCGAN: Sparse CounterGAN for Counterfactual Explanations in Breast Cancer Prediction",
                "abstract": "Imaging phenotypes extracted via radiomics of magnetic resonance imaging have shown great potential in predicting the treatment response in breast cancer patients after administering neoadjuvant systemic therapy (NST). Understanding the causal relationships between Imaging phenotypes, Clinical information, and Molecular (ICM) features, and the treatment response are critical in guiding treatment strategies and management plans. Counterfactual explanations provide an interpretable approach to generating causal inference; however, existing approaches are either computationally prohibitive for high dimensional problems, generate unrealistic counterfactuals, or confound the effects of causal features. This paper proposes a new method called Sparse CounteRGAN (SCGAN) for generating counterfactual instances to establish causal relationships between ICM features and the treatment response after NST. The generative approach learns the distribution of the original instances and, therefore, ensures that the new instances are realistic. Further, we propose a loss function that regularizes the counterfactuals to minimize the distance between original instances and counterfactuals (to promote sparsity) and the distances among generated counterfactuals to promote diversity. We evaluate the proposed method on two publicly available datasets, followed by the breast cancer dataset, and compare their performance with existing methods in the literature. Finally, we demonstrate the causal relationships from generated counterfactual instances. Results show that SCGAN generates plausible and realistic counterfactual instances with small changes in only a few features, making it a valuable tool for understanding the causal relationships between ICM features and treatment response.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220181091",
                        "name": "S. Zhou"
                    },
                    {
                        "authorId": "2189148186",
                        "name": "U. J. Islam"
                    },
                    {
                        "authorId": "2068812163",
                        "name": "N. Pfeiffer"
                    },
                    {
                        "authorId": "2065282393",
                        "name": "I. Banerjee"
                    },
                    {
                        "authorId": "38966672",
                        "name": "B. Patel"
                    },
                    {
                        "authorId": "2215217071",
                        "name": "A. Iquebal"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Lucic et al.[10] proposed a differentiable tree ensemble model which approximates a non-differentiable tree ensemble model to which it is difficult to apply gradient-based method."
            ],
            "citingPaper": {
                "paperId": "1fca79c415aa74d89ac8baab16bdf3673b5e3c03",
                "externalIds": {
                    "DBLP": "conf/sac/HanL23",
                    "DOI": "10.1145/3555776.3577737",
                    "CorpusId": 259099380
                },
                "corpusId": 259099380,
                "publicationVenue": {
                    "id": "d80d58be-58fc-4181-a397-5ac6fd976a47",
                    "name": "ACM Symposium on Applied Computing",
                    "type": "conference",
                    "alternate_names": [
                        "Sel Area Cryptogr",
                        "Int Conf Sel area Cryptogr",
                        "International Conference on Selected areas in Cryptography",
                        "ACM Symp Appl Comput",
                        "Selected Areas in Cryptography",
                        "Symposium on Applied Computing",
                        "SAC",
                        "Symp Appl Comput"
                    ],
                    "url": "https://www.acm.org/publications",
                    "alternate_urls": [
                        "http://sacworkshop.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1fca79c415aa74d89ac8baab16bdf3673b5e3c03",
                "title": "Gradient-based Counterfactual Generation for Sparse and Diverse Counterfactual Explanations",
                "abstract": "Counterfactual generation has attracted attention as a technique that generates samples, called counterfactual explanations, which provide a guidance to modify an input instance for changing its class label in real-world applications. Generation of multiple counterfactual explanations gives people various options to change their input instance according to their preferences or capabilities. To generate multiple counterfactual explanations, this paper proposes a gradient-based method which dynamically selects some subsets of attributes of the given instance to be tweaked for diverse counterfactual searches. It also proposes a loss-based update rule for one-hot encoded categorical attributes which is used to produce feasible and effective counterfactual explanations for instances with both categorical and continuous features. We conducted some comparative experiments on the six public datasets to evaluate the performance of the proposed method. The experiment results showed that the proposed method generates valid and diverse counterfactual explanations with a smaller number of attribute value modifications compared with the existing works.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48020914",
                        "name": "C. Han"
                    },
                    {
                        "authorId": "2145118716",
                        "name": "K. Lee"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "12c6be503e4e5b7c9cb1810152d4364f26628a8d",
                "externalIds": {
                    "ArXiv": "2303.10158",
                    "DBLP": "journals/corr/abs-2303-10158",
                    "DOI": "10.48550/arXiv.2303.10158",
                    "CorpusId": 257622614
                },
                "corpusId": 257622614,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/12c6be503e4e5b7c9cb1810152d4364f26628a8d",
                "title": "Data-centric Artificial Intelligence: A Survey",
                "abstract": "Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1759658",
                        "name": "D. Zha"
                    },
                    {
                        "authorId": "2122929218",
                        "name": "Zaid Pervaiz Bhat"
                    },
                    {
                        "authorId": "51238382",
                        "name": "Kwei-Herng Lai"
                    },
                    {
                        "authorId": "47829900",
                        "name": "Fan Yang"
                    },
                    {
                        "authorId": "47653902",
                        "name": "Zhimeng Jiang"
                    },
                    {
                        "authorId": "2181946372",
                        "name": "Shaochen Zhong"
                    },
                    {
                        "authorId": "2109724398",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology",
                "result"
            ],
            "contexts": [
                "Finally, we also tried to compare with the code for FOCUS(7) [23].",
                "[23] use a gradient-based algorithm that approximates the splits of the decision trees with sigmoid functions."
            ],
            "citingPaper": {
                "paperId": "f7544bfd25ae5baefe62ce3973d368f1c4b2fa70",
                "externalIds": {
                    "ArXiv": "2303.02883",
                    "DBLP": "journals/corr/abs-2303-02883",
                    "DOI": "10.48550/arXiv.2303.02883",
                    "CorpusId": 257364839
                },
                "corpusId": 257364839,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f7544bfd25ae5baefe62ce3973d368f1c4b2fa70",
                "title": "Very fast, approximate counterfactual explanations for decision forests",
                "abstract": "We consider finding a counterfactual explanation for a classification or regression forest, such as a random forest. This requires solving an optimization problem to find the closest input instance to a given instance for which the forest outputs a desired value. Finding an exact solution has a cost that is exponential on the number of leaves in the forest. We propose a simple but very effective approach: we constrain the optimization to input space regions populated by actual data points. The problem reduces to a form of nearest-neighbor search using a certain distance on a certain dataset. This has two advantages: first, the solution can be found very quickly, scaling to large forests and high-dimensional data, and enabling interactive use. Second, the solution found is more likely to be realistic in that it is guided towards high-density areas of input space.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1400347466",
                        "name": "Miguel 'A. Carreira-Perpin'an"
                    },
                    {
                        "authorId": "72109609",
                        "name": "Suryabhan Singh Hada"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "67c53987427d784a8be428ca03e4138d3d264bb1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-03180",
                    "ArXiv": "2302.03180",
                    "DOI": "10.48550/arXiv.2302.03180",
                    "CorpusId": 256627670
                },
                "corpusId": 256627670,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/67c53987427d784a8be428ca03e4138d3d264bb1",
                "title": "Who wants what and how: a Mapping Function for Explainable Artificial Intelligence",
                "abstract": "The increasing complexity of AI systems has led to the growth of the field of explainable AI (XAI), which aims to provide explanations and justifications for the outputs of AI algorithms. These methods mainly focus on feature importance and identifying changes that can be made to achieve a desired outcome. Researchers have identified desired properties for XAI methods, such as plausibility, sparsity, causality, low run-time, etc. The objective of this study is to conduct a review of existing XAI research and present a classification of XAI methods. The study also aims to connect XAI users with the appropriate method and relate desired properties to current XAI approaches. The outcome of this study will be a clear strategy that outlines how to choose the right XAI method for a particular goal and user and provide a personalized explanation for users.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2072398266",
                        "name": "M. Hashemi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "1df0ab2c38f838815c55a5ddde5d588f6299d7ab",
                "externalIds": {
                    "ArXiv": "2301.10074",
                    "DBLP": "journals/corr/abs-2301-10074",
                    "DOI": "10.48550/arXiv.2301.10074",
                    "CorpusId": 256194144
                },
                "corpusId": 256194144,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1df0ab2c38f838815c55a5ddde5d588f6299d7ab",
                "title": "Explainable Data-Driven Optimization: From Context to Decision and Back Again",
                "abstract": "Data-driven optimization uses contextual information and machine learning algorithms to find solutions to decision problems with uncertain parameters. While a vast body of work is dedicated to interpreting machine learning models in the classification setting, explaining decision pipelines involving learning algorithms remains unaddressed. This lack of interpretability can block the adoption of data-driven solutions as practitioners may not understand or trust the recommended decisions. We bridge this gap by introducing a counterfactual explanation methodology tailored to explain solutions to data-driven problems. We introduce two classes of explanations and develop methods to find nearest explanations of random forest and nearest-neighbor predictors. We demonstrate our approach by explaining key problems in operations management such as inventory management and routing.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2166785815",
                        "name": "Alexandre Forel"
                    },
                    {
                        "authorId": "2171487",
                        "name": "Axel Parmentier"
                    },
                    {
                        "authorId": "27855049",
                        "name": "Thibaut Vidal"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026Pawelczyk, Broelemann, and Kasneci 2020) where explanations are model-agnostic explanations for structured datasets and (Van der Waa et al. 2018; Lucic et al. 2022) where the contrast of interest is designed to in policy-based reinforcement learning settings and nondifferentiable tree models\u2026",
                "2019; Pawelczyk, Broelemann, and Kasneci 2020) where explanations are model-agnostic explanations for structured datasets and (Van der Waa et al. 2018; Lucic et al. 2022) where the contrast of interest is designed to in policy-based reinforcement learning settings and nondifferentiable tree models respectively."
            ],
            "citingPaper": {
                "paperId": "38819202e99e5174d15be8521727d711bf1b4629",
                "externalIds": {
                    "ArXiv": "2301.07941",
                    "DBLP": "journals/corr/abs-2301-07941",
                    "DOI": "10.48550/arXiv.2301.07941",
                    "CorpusId": 255999810
                },
                "corpusId": 255999810,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/38819202e99e5174d15be8521727d711bf1b4629",
                "title": "CEnt: An Entropy-based Model-agnostic Explainability Framework to Contrast Classifiers' Decisions",
                "abstract": "Current interpretability methods focus on explaining a particular model's decision through present input features. Such methods do not inform the user of the sufficient conditions that alter these decisions when they are not desirable. Contrastive explanations circumvent this problem by providing explanations of the form\"If the feature $X>x$, the output $Y$ would be different''. While different approaches are developed to find contrasts; these methods do not all deal with mutability and attainability constraints. In this work, we present a novel approach to locally contrast the prediction of any classifier. Our Contrastive Entropy-based explanation method, CEnt, approximates a model locally by a decision tree to compute entropy information of different feature splits. A graph, G, is then built where contrast nodes are found through a one-to-many shortest path search. Contrastive examples are generated from the shortest path to reflect feature splits that alter model decisions while maintaining lower entropy. We perform local sampling on manifold-like distances computed by variational auto-encoders to reflect data density. CEnt is the first non-gradient-based contrastive method generating diverse counterfactuals that do not necessarily exist in the training data while satisfying immutability (ex. race) and semi-immutability (ex. age can only change in an increasing direction). Empirical evaluation on four real-world numerical datasets demonstrates the ability of CEnt in generating counterfactuals that achieve better proximity rates than existing methods without compromising latency, feasibility, and attainability. We further extend CEnt to imagery data to derive visually appealing and useful contrasts between class labels on MNIST and Fashion MNIST datasets. Finally, we show how CEnt can serve as a tool to detect vulnerabilities of textual classifiers.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "8160667",
                        "name": "Julia El Zini"
                    },
                    {
                        "authorId": "50245581",
                        "name": "Mohamad Mansour"
                    },
                    {
                        "authorId": "144707373",
                        "name": "M. Awad"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The most popular approaches can be categorized into model-specific [18], [20], [24], [25] and model-agnostic [7], [26]\u2013[29].",
                "Besides, it is also worth remarking that tree-based models are generally easier to interpret and explain than complex neural networks [18]\u2013[20]."
            ],
            "citingPaper": {
                "paperId": "8a5726339b07ab5a835879ea4cbc01a23fbdc2a7",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/BassettiCPT22",
                    "DOI": "10.1109/BigData55660.2022.10020920",
                    "CorpusId": 256322250
                },
                "corpusId": 256322250,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8a5726339b07ab5a835879ea4cbc01a23fbdc2a7",
                "title": "ISIDE: Proactively Assist University Students at Risk of Dropout",
                "abstract": "In this work, we present ISIDE, the prototype of a student dropout alert system integrated within Infostud, i.e., the online student portal of the Sapienza University of Rome. Our proposed solution is based on a student dropout prediction (SDP) module built from a large dataset of academic records using advanced machine learning techniques. Offline experiments show that the best-performing SDP model can detect students prone to leave the school with an F1 score of 0.92. To further validate our prototype online, we run a pilot study on a subset of students from our School of Information Engineering, Informatics, and Statistics. This study shows that our prototype can detect students who are most likely to drop out early, as it clearly separates them from those with higher key engagement indicators.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "146579508",
                        "name": "Enrico Bassetti"
                    },
                    {
                        "authorId": "153891445",
                        "name": "A. Conti"
                    },
                    {
                        "authorId": "2257917",
                        "name": "Emanuele Panizzi"
                    },
                    {
                        "authorId": "2651748",
                        "name": "Gabriele Tolomei"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "b1be4d9daf6bcd8fb28d234e06c34ace677449cf",
                "externalIds": {
                    "ArXiv": "2211.02151",
                    "DBLP": "journals/corr/abs-2211-02151",
                    "DOI": "10.48550/arXiv.2211.02151",
                    "CorpusId": 253370313
                },
                "corpusId": 253370313,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b1be4d9daf6bcd8fb28d234e06c34ace677449cf",
                "title": "Decomposing Counterfactual Explanations for Consequential Decision Making",
                "abstract": "The goal of algorithmic recourse is to reverse unfavorable decisions (e.g., from loan denial to approval) under automated decision making by suggesting actionable feature changes (e.g., reduce the number of credit cards). To generate low-cost recourse the majority of methods work under the assumption that the features are independently manipulable (IMF). To address the feature dependency issue the recourse problem is usually studied through the causal recourse paradigm. However, it is well known that strong assumptions, as encoded in causal models and structural equations, hinder the applicability of these methods in complex domains where causal dependency structures are ambiguous. In this work, we develop \\texttt{DEAR} (DisEntangling Algorithmic Recourse), a novel and practical recourse framework that bridges the gap between the IMF and the strong causal assumptions. \\texttt{DEAR} generates recourses by disentangling the latent representation of co-varying features from a subset of promising recourse features to capture the main practical recourse desiderata. Our experiments on real-world data corroborate our theoretically motivated recourse model and highlight our framework's ability to provide reliable, low-cost recourse in the presence of feature dependencies.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "89583148",
                        "name": "Martin Pawelczyk"
                    },
                    {
                        "authorId": "2166301365",
                        "name": "Lea Tiyavorabun"
                    },
                    {
                        "authorId": "1686448",
                        "name": "G. Kasneci"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "CA has been\napplied to random forests in Fern\u00e1ndez et al. (2020) and tree ensembles in Lucic et al. (2019)."
            ],
            "citingPaper": {
                "paperId": "3f219a1cbba98216d4e2c642b623efce11ebb8cf",
                "externalIds": {
                    "PubMedCentral": "9605715",
                    "DOI": "10.1016/j.ifacol.2022.09.479",
                    "CorpusId": 253182715
                },
                "corpusId": 253182715,
                "publicationVenue": {
                    "id": "af98f1eb-affb-4b55-b8ff-1964b29cf894",
                    "name": "IFAC-PapersOnLine",
                    "type": "journal",
                    "issn": "2405-8963",
                    "url": "https://www.journals.elsevier.com/ifac-papersonline/",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/24058963",
                        "https://www.journals.elsevier.com/ifac-papersonline"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3f219a1cbba98216d4e2c642b623efce11ebb8cf",
                "title": "Disruption evaluation in end-to-end semiconductor supply chains via interpretable machine learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155934079",
                        "name": "Friedrich-Maximilian Jaenichen"
                    },
                    {
                        "authorId": "123520796",
                        "name": "C. Liepold"
                    },
                    {
                        "authorId": "2155900207",
                        "name": "Abdelgafar Ismail"
                    },
                    {
                        "authorId": "25470144",
                        "name": "Maximilian Schiffer"
                    },
                    {
                        "authorId": "1784980",
                        "name": "H. Ehm"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "cd89345814ce3030cea4025fa7501cea06f6f3fd",
                "externalIds": {
                    "ArXiv": "2209.13446",
                    "DBLP": "conf/kdd/VoL00BHP23",
                    "DOI": "10.1145/3580305.3599343",
                    "CorpusId": 258999054
                },
                "corpusId": 258999054,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/cd89345814ce3030cea4025fa7501cea06f6f3fd",
                "title": "Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations",
                "abstract": "Interpretable machine learning seeks to understand the reasoning process of complex black-box systems that are long notorious for lack of explainability. One flourishing approach is through counterfactual explanations, which provide suggestions on what a user can do to alter an outcome. Not only must a counterfactual example counter the original prediction from the black-box classifier but it should also satisfy various constraints for practical applications. Diversity is one of the critical constraints that however remains less discussed. While diverse counterfactuals are ideal, it is computationally challenging to simultaneously address some other constraints. Furthermore, there is a growing privacy concern over the released counterfactual data. To this end, we propose a feature-based learning framework that effectively handles the counterfactual constraints and contributes itself to the limited pool of private explanation models. We demonstrate the flexibility and effectiveness of our method in generating diverse counterfactuals of actionability and plausibility. Our counterfactual engine is more efficient than counterparts of the same capacity while yielding the lowest re-identification risks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3369353",
                        "name": "Vy A. Vo"
                    },
                    {
                        "authorId": "145301586",
                        "name": "Trung Le"
                    },
                    {
                        "authorId": "2147319231",
                        "name": "Van-Anh Nguyen"
                    },
                    {
                        "authorId": "1643682004",
                        "name": "He Zhao"
                    },
                    {
                        "authorId": "30561807",
                        "name": "Edwin V. Bonilla"
                    },
                    {
                        "authorId": "2561045",
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "authorId": "1400659302",
                        "name": "Dinh Q. Phung"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "In the recent years, a large body of work on eXplainable Artificial Intelligence (XAI) have been proposed in the literature [45,44,28,18,20,25,38]."
            ],
            "citingPaper": {
                "paperId": "edbdec562cb1525eafe148b249b75480a284dc59",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-04222",
                    "ArXiv": "2208.04222",
                    "DOI": "10.48550/arXiv.2208.04222",
                    "CorpusId": 251402395
                },
                "corpusId": 251402395,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/edbdec562cb1525eafe148b249b75480a284dc59",
                "title": "GREASE: Generate Factual and Counterfactual Explanations for GNN-based Recommendations",
                "abstract": "Recently, graph neural networks (GNNs) have been widely used to develop successful recommender systems. Although powerful, it is very difficult for a GNN-based recommender system to attach tangible explanations of why a specific item ends up in the list of suggestions for a given user. Indeed, explaining GNN-based recommendations is unique, and existing GNN explanation methods are inappropriate for two reasons. First, traditional GNN explanation methods are designed for node, edge, or graph classification tasks rather than ranking, as in recommender systems. Second, standard machine learning explanations are usually intended to support skilled decision-makers. Instead, recommendations are designed for any end-user, and thus their explanations should be provided in user-understandable ways. In this work, we propose GREASE, a novel method for explaining the suggestions provided by any black-box GNN-based recommender system. Specifically, GREASE first trains a surrogate model on a target user-item pair and its $l$-hop neighborhood. Then, it generates both factual and counterfactual explanations by finding optimal adjacency matrix perturbations to capture the sufficient and necessary conditions for an item to be recommended, respectively. Experimental results conducted on real-world datasets demonstrate that GREASE can generate concise and effective explanations for popular GNN-based recommender models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157281262",
                        "name": "Ziheng Chen"
                    },
                    {
                        "authorId": "144925193",
                        "name": "F. Silvestri"
                    },
                    {
                        "authorId": "2144547216",
                        "name": "Jia Wang"
                    },
                    {
                        "authorId": "1591136873",
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "authorId": null,
                        "name": "Zhenhua Huang"
                    },
                    {
                        "authorId": "34609799",
                        "name": "H. Ahn"
                    },
                    {
                        "authorId": "2651748",
                        "name": "Gabriele Tolomei"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "METHOD FT FOCUS FACE NN\nVALIDITY 72.9% 72.8% 84.4% 92.5%",
                "\u2026after generating counterfactuals using any of the existing methods for tree-based ensembles (that we also refer to as the base method), e.g., Feature Tweaking (FT) (Tolomei et al., 2017), FOCUS (Lucic et al., 2022), Nearest Neighbor (NN) (Albini et al., 2022), FACE (Poyiadzi et al., 2020), etc.",
                "For tree-based ensembles, some existing approaches to find the closest counterfactuals include (Tolomei et al., 2017; Lucic et al., 2022).",
                "\u2022 FOCUS (Lucic et al., 2022) is another popular technique that approximates the tree-based models with sigmoid\n\u2022 FACE (Poyiadzi et al., 2020) attempts to find counterfactuals that are not only close (L1 or L2 cost), but also (i) lie on the data manifold; and (ii) are connected to the original data point via a path on a connectivity graph on the dataset S .",
                "Observations: The average cost (L1 or L2 cost) between the original data point and the counterfactual increases only slightly for base methods such as FT, FOCUS, and NN (which find counterfactuals by explicitly minimizing this cost); however our counterfactuals are significantly more robust (in terms of validity) and realistic (in terms of LOF).",
                "METHOD L1 COST VALIDITY LOF\nCCF 3.05 99.9% 1.0\nFT 0.08 56.4% 0.65 FT +ROBX 2.70 99.9% 1.0\nFOCUS 0.12 53.7% 0.71 FOCUS +ROBX 2.71 99.7% 1.0\nFACE 2.62 88.8% 0.82 FACE +ROBX 2.72 99.7% 1.0\nNN 0.80 84.4% 0.94 NN +ROBX 2.71 99.7% 1.0\nMETHOD L2 COST VALIDITY LOF\nCCF 1.36 97.4% 1.0\nFT 0.08 53.4 0.65 FT +ROBX 1.17 98.6 1.0\nFOCUS 0.11 53.2% 0.82 FOCUS +ROBX 1.2 100% 1.0\nFACE 1.25 88.7% 0.77 FACE +ROBX 1.18 98.4% 1.0\nNN 0.49 79.0% 0.88 NN +ROBX 1.18 99.0% 0.94\nC.3.",
                ", 2017), FOCUS (Lucic et al., 2022), Nearest Neighbor (NN) (Albini et al.",
                "Our proposed strategy is a post-processing one, i.e., it can be applied after generating counterfactuals using any of the existing methods for tree-based ensembles (that we also refer to as the base method), e.g., Feature Tweaking (FT) (Tolomei et al., 2017), FOCUS (Lucic et al., 2022), Nearest Neighbor (NN) (Albini et al., 2022), FACE (Poyiadzi et al., 2020), etc.",
                "\u2026attention in recent years (see Verma et al. (2020);\nKarimi et al. (2020); Wachter et al. (2017); Dandl et al. (2020); K\u00f6nig et al. (2021); Albini et al. (2022); Kanamori et al. (2020); Poyiadzi et al. (2020); Lucic et al. (2022); Pawelczyk et al. (2020) as well as the references therein).",
                "\u2022 FOCUS (Lucic et al., 2022) is another popular technique that approximates the tree-based models with sigmoid\n\u2022 FACE (Poyiadzi et al., 2020) attempts to find counterfactuals that are not only close (L1 or L2 cost), but also (i) lie on the data manifold; and (ii) are connected to the original data\u2026",
                "We believe our choice of these four base methods to be quite a diverse representation of the existing approaches, namely, search-based closest\ncounterfactual (FT), optimization-based closest counterfactual (FOCUS), graph-based data-support counterfactual (FACE), and closest-data-support counterfactual (NN).",
                "\u2022 FOCUS (Lucic et al., 2022) is another popular technique that approximates the tree-based models with sigmoid"
            ],
            "citingPaper": {
                "paperId": "9627efce9888bbf78538e6d259f49081e91f391a",
                "externalIds": {
                    "DBLP": "conf/icml/DuttaLMTM22",
                    "ArXiv": "2207.02739",
                    "DOI": "10.48550/arXiv.2207.02739",
                    "CorpusId": 250311162
                },
                "corpusId": 250311162,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9627efce9888bbf78538e6d259f49081e91f391a",
                "title": "Robust Counterfactual Explanations for Tree-Based Ensembles",
                "abstract": "Counterfactual explanations inform ways to achieve a desired outcome from a machine learning model. However, such explanations are not robust to certain real-world changes in the under-lying model (e.g., retraining the model, changing hyperparameters, etc.), questioning their reliability in several applications, e.g., credit lending. In this work, we propose a novel strategy \u2013 that we call RobX \u2013 to generate robust counterfactuals for tree-based ensembles, e.g., XGBoost. Tree-based ensembles pose additional challenges in robust counterfactual generation, e.g., they have a non-smooth and non-differentiable objective function, and they can change a lot in the parameter space under retraining on very similar data. We first introduce a novel metric \u2013 that we call Counterfactual Stability \u2013 that attempts to quantify how robust a counterfactual is going to be to model changes under retraining, and comes with desirable theoretical properties. Our proposed strategy RobX works with any counterfactual generation method (base method) and searches for robust counterfactuals by iteratively refining the counterfactual generated by the base method using our metric Counterfactual Stability . We compare the performance of RobX with popular counterfactual generation methods (for tree-based ensembles) across benchmark datasets. The results demonstrate that our strategy generates counterfactuals that are significantly more robust (nearly 100% validity after actual model changes) and also realistic (in terms of local outlier factor) over existing state-of-the-art methods. Generated Using State-Of-The-Art Techniques (with L 1 cost minimization) for XGBoost Models on German Credit Dataset (Dua & Graff, 2017): Models were retrained after dropping only a single data point. A large fraction of the counterfactuals for the previous model no longer remain valid for the new models obtained after retraining.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "14579807",
                        "name": "Sanghamitra Dutta"
                    },
                    {
                        "authorId": "2117316193",
                        "name": "Jason Long"
                    },
                    {
                        "authorId": "28265392",
                        "name": "Saumitra Mishra"
                    },
                    {
                        "authorId": "73657148",
                        "name": "Cecilia Tilli"
                    },
                    {
                        "authorId": "1738142",
                        "name": "D. Magazzeni"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "d8d9e7c8a609abd59d8689e738a8000a5827a662",
                "externalIds": {
                    "ArXiv": "2207.02726",
                    "DBLP": "journals/corr/abs-2207-02726",
                    "DOI": "10.48550/arXiv.2207.02726",
                    "CorpusId": 249916850
                },
                "corpusId": 249916850,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d8d9e7c8a609abd59d8689e738a8000a5827a662",
                "title": "Towards the Use of Saliency Maps for Explaining Low-Quality Electrocardiograms to End Users",
                "abstract": "When using medical images for diagnosis, either by clinicians or arti\ufb01cial intelligence (AI) systems, it is important that the images are of high quality. When an image is of low quality, the medical exam that produced the image often needs to be redone. In telemedicine, a common problem is that the quality issue is only \ufb02agged once the patient has left the clinic, meaning they must return in order to have the exam redone. This can be especially dif\ufb01cult for people living in remote regions, who make up a substantial portion of the patients at Portal Telemedicina, a digital healthcare organization based in Brazil. In this paper, we report on ongoing work regarding (i) the development of an AI system for \ufb02agging and explaining low-quality medical images in real-time, (ii) an interview study to understand the explanation needs of stakeholders using the AI system at Portal Telemedicina, and (iii) a longitudinal user study design to examine the effect of including explanations on the work\ufb02ow of the technicians in our clinics in the context of understanding low-quality medical exams. To the best of our knowledge, this would be the \ufb01rst longitudinal study on evaluating the effects of XAI methods on end-users \u2013 stakeholders that use AI systems but do not have AI-speci\ufb01c expertise. We welcome feedback and suggestions on our experimental setup. identifying stakeholders, (ii) engaging with each stakeholder, and (iii) understanding the purpose of the explanation. We report on work in progress on developing, deploying and evaluating an AI system for \ufb02agging and explaining low-quality medical images. We describe the outcomes of two critical studies in our development process, aimed at answering the following research questions:",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "38702106",
                        "name": "Ana Lucic"
                    },
                    {
                        "authorId": "38971221",
                        "name": "Sheeraz Ahmad"
                    },
                    {
                        "authorId": "1582303702",
                        "name": "Amanda Furtado Brinhosa"
                    },
                    {
                        "authorId": "144921048",
                        "name": "Q. Liao"
                    },
                    {
                        "authorId": "2171869662",
                        "name": "Himani Agrawal"
                    },
                    {
                        "authorId": "32326200",
                        "name": "Umang Bhatt"
                    },
                    {
                        "authorId": "1769861",
                        "name": "K. Kenthapadi"
                    },
                    {
                        "authorId": "4990825",
                        "name": "Alice Xiang"
                    },
                    {
                        "authorId": "1696030",
                        "name": "M. de Rijke"
                    },
                    {
                        "authorId": "1395625876",
                        "name": "N. Drabowski"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The majority of existing counterfactual methods modify the given sample until the target class is attained (see e.g., Tolomei et al., 2017; Lucic et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "7fc3a2111b7e37d7a99a88398b698f0b44ebef14",
                "externalIds": {
                    "ArXiv": "2205.14116",
                    "CorpusId": 256416553
                },
                "corpusId": 256416553,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7fc3a2111b7e37d7a99a88398b698f0b44ebef14",
                "title": "Don't Explain Noise: Robust Counterfactuals for Randomized Ensembles",
                "abstract": "Counterfactual explanations describe how to modify a feature vector in order to flip the outcome of a trained classifier. Obtaining robust counterfactual explanations is essential to provide valid algorithmic recourse and meaningful explanations. We study the robustness of explanations of randomized ensembles, which are always subject to algorithmic uncertainty even when the training data is fixed. We formalize the generation of robust counterfactual explanations as a probabilistic problem and show the link between the robustness of ensemble models and the robustness of base learners. We develop a practical method with good empirical performance and support it with theoretical guarantees for ensembles of convex base learners. Our results show that existing methods give surprisingly low robustness: the validity of naive counterfactuals is below $50\\%$ on most data sets and can fall to $20\\%$ on problems with many features. In contrast, our method achieves high robustness with only a small increase in the distance from counterfactual explanations to their initial observations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2166785815",
                        "name": "Alexandre Forel"
                    },
                    {
                        "authorId": "2171487",
                        "name": "Axel Parmentier"
                    },
                    {
                        "authorId": "27855049",
                        "name": "Thibaut Vidal"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In another direction, some works focused on generating CEs for specific model categories, such as tree-based models (Lucic et al., 2022; Tolomei et al., 2017; Parmentier & Vidal, 2021), or differentiable models (Mothilal et al.",
                "In another direction, some works focused on generating CEs for specific model categories, such as tree-based models (Lucic et al., 2022; Tolomei et al., 2017; Parmentier & Vidal, 2021), or differentiable models (Mothilal et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "5aaae0d2b3a1bdc4ee9e6bf4a3bf8b1a0b7f9ec3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-06917",
                    "ArXiv": "2204.06917",
                    "DOI": "10.48550/arXiv.2204.06917",
                    "CorpusId": 248178167
                },
                "corpusId": 248178167,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5aaae0d2b3a1bdc4ee9e6bf4a3bf8b1a0b7f9ec3",
                "title": "Global Counterfactual Explanations: Investigations, Implementations and Improvements",
                "abstract": "Counterfactual explanations have been widely studied in explainability, with a range of application dependent methods emerging in fairness, recourse and model understanding. However, the major shortcoming associated with these methods is their inability to provide explanations beyond the local or instance-level. While some works touch upon the notion of a global explanation, typically suggesting to aggregate masses of local explanations in the hope of ascertaining global properties, few provide frameworks that are either reliable or computationally tractable. Meanwhile, practitioners are requesting more efficient and interactive explainability tools. We take this opportunity to investigate existing global methods, with a focus on implementing and improving Actionable Recourse Summaries (AReS), the only known global counterfactual explanation framework for recourse.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2081992675",
                        "name": "D. Ley"
                    },
                    {
                        "authorId": "28265392",
                        "name": "Saumitra Mishra"
                    },
                    {
                        "authorId": "1738142",
                        "name": "D. Magazzeni"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "As a consequence, some recourse methods were developed to find recourses for tree ensembles (Tolomei et al., 2017; Lucic et al., 2022) where the non-differentiability prevents a direct application of the recourse objective in (1)."
            ],
            "citingPaper": {
                "paperId": "31c4ad6867aa4fccdf0df6c6d27b8d956cf8d1e4",
                "externalIds": {
                    "DBLP": "conf/iclr/PawelczykDHKL23",
                    "ArXiv": "2203.06768",
                    "CorpusId": 252716083
                },
                "corpusId": 252716083,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/31c4ad6867aa4fccdf0df6c6d27b8d956cf8d1e4",
                "title": "Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse",
                "abstract": "As machine learning models are increasingly being employed to make consequential decisions in real-world settings, it becomes critical to ensure that individuals who are adversely impacted (e.g., loan denied) by the predictions of these models are provided with a means for recourse. While several approaches have been proposed to construct recourses for affected individuals, the recourses output by these methods either achieve low costs (i.e., ease-of-implementation) or robustness to small perturbations (i.e., noisy implementations of recourses), but not both due to the inherent trade-offs between the recourse costs and robustness. Furthermore, prior approaches do not provide end users with any agency over navigating the aforementioned trade-offs. In this work, we address the above challenges by proposing the first algorithmic framework which enables users to effectively manage the recourse cost vs. robustness trade-offs. More specifically, our framework Probabilistically ROBust rEcourse (\\texttt{PROBE}) lets users choose the probability with which a recourse could get invalidated (recourse invalidation rate) if small changes are made to the recourse i.e., the recourse is implemented somewhat noisily. To this end, we propose a novel objective function which simultaneously minimizes the gap between the achieved (resulting) and desired recourse invalidation rates, minimizes recourse costs, and also ensures that the resulting recourse achieves a positive model prediction. We develop novel theoretical results to characterize the recourse invalidation rates corresponding to any given instance w.r.t. different classes of underlying models (e.g., linear models, tree based models etc.), and leverage these results to efficiently optimize the proposed objective. Experimental evaluation with multiple real world datasets demonstrate the efficacy of the proposed framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "89583148",
                        "name": "Martin Pawelczyk"
                    },
                    {
                        "authorId": "2158813275",
                        "name": "Teresa Datta"
                    },
                    {
                        "authorId": "2158815650",
                        "name": "Johannes van-den-Heuvel"
                    },
                    {
                        "authorId": "1686448",
                        "name": "G. Kasneci"
                    },
                    {
                        "authorId": "1892673",
                        "name": "Himabindu Lakkaraju"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The former include: FeatTweak and FOCUS (tree-specific); DeepFool and GRACE (NN-specific).",
                "Another approach that is conceived for explaining tree ensembles is called FOCUS [22].",
                "In particular, model-agnostic techniques (including both variants of our ReLAX) clearly apply to every setting, whereas model-specific approaches can be tested only when the target model matches (e.g., FOCUS can be used only in combination with tree-based models)."
            ],
            "citingPaper": {
                "paperId": "f83f48c423bd19438ca7e2e54a3c5ad98fd7dc81",
                "externalIds": {
                    "DBLP": "conf/cikm/ChenSW0AT22",
                    "ArXiv": "2110.11960",
                    "DOI": "10.1145/3511808.3557429",
                    "CorpusId": 251403144
                },
                "corpusId": 251403144,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f83f48c423bd19438ca7e2e54a3c5ad98fd7dc81",
                "title": "ReLAX: Reinforcement Learning Agent Explainer for Arbitrary Predictive Models",
                "abstract": "Counterfactual examples (CFs) are one of the most popular methods for attaching post-hoc explanations to machine learning (ML) models. However, existing CF generation methods either exploit the internals of specific models or depend on each sample's neighborhood, thus they are hard to generalize for complex models and inefficient for large datasets. This work aims to overcome these limitations and introduces ReLAX, a model-agnostic algorithm to generate optimal counterfactual explanations. Specifically, we formulate the problem of crafting CFs as a sequential decision-making task and then find the optimal CFs via deep reinforcement learning (DRL) with discrete-continuous hybrid action space. Extensive experiments conducted on several tabular datasets have shown that ReLAX outperforms existing CF generation baselines, as it produces sparser counterfactuals, is more scalable to complex target models to explain, and generalizes to both classification and regression tasks. Finally, to demonstrate the usefulness of our method in a real-world use case, we leverage CFs generated by ReLAX to suggest actions that a country should take to reduce the risk of mortality due to COVID-19. Interestingly enough, the actions recommended by our method correspond to the strategies that many countries have actually implemented to counter the COVID-19 pandemic.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2157281262",
                        "name": "Ziheng Chen"
                    },
                    {
                        "authorId": "144925193",
                        "name": "F. Silvestri"
                    },
                    {
                        "authorId": "2144547216",
                        "name": "Jia Wang"
                    },
                    {
                        "authorId": "2117693967",
                        "name": "He Zhu"
                    },
                    {
                        "authorId": "34609799",
                        "name": "H. Ahn"
                    },
                    {
                        "authorId": "2651748",
                        "name": "Gabriele Tolomei"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Then it uses differentiable approximations of tree ensembles to keep the convexity of the problem (Lucic et al, 2019)."
            ],
            "citingPaper": {
                "paperId": "845585b109f4d3780eb42f011ba8f2012fd0d146",
                "externalIds": {
                    "MAG": "3199470821",
                    "DOI": "10.21203/rs.3.rs-551661/v1",
                    "CorpusId": 240566616
                },
                "corpusId": 240566616,
                "publicationVenue": {
                    "id": "d263025a-9eaf-443f-9bbf-72377e8d22a6",
                    "name": "Data mining and knowledge discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Data Mining and Knowledge Discovery",
                        "Data Min Knowl Discov",
                        "Data min knowl discov"
                    ],
                    "issn": "1384-5810",
                    "url": "https://www.springer.com/computer/database+management+&+information+retrieval/journal/10618",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10618",
                        "http://www.springer.com/computer/database+management+&+information+retrieval/journal/10618"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/845585b109f4d3780eb42f011ba8f2012fd0d146",
                "title": "Mining Pareto-Optimal Counterfactual Antecedents With A Branch-And-Bound Model-Agnostic Algorithm",
                "abstract": "\n Mining counterfactual antecedents became a valuable tool to discover knowledge and explain machine learning models. It consists of generating synthetic samples from an original sample to achieve the desired outcome in a machine learning model thus helping to understand the prediction. An insightful methodology would explore a broader set of counterfactual antecedents to reveal multiple possibilities while operating on any classifier. Thus, we create a tree-based search that requires monotonicity from the objective functions (a.k.a. cost functions); it allows pruning branches that will not improve the objective functions. Since monotonicity is only required for the objective function, this method can be used for any family of classifiers (e.g., linear models, neural networks, decision trees). However, additional classifier properties speed up the tree-search when it foresees branches that will not result in feasible actions. Moreover, the proposed optimization generates a diverse set of Pareto-optimal counterfactual antecedents by relying on multi-objective concepts. The results show an algorithm with working guarantees that enumerates a wide range of counterfactual antecedents. It helps the decision-maker understand the machine learning decision and finds alternatives to achieve the desired outcome. The user can inspect these multiple counterfactual antecedents to find the most suitable one and have a broader understanding of the prediction.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35473883",
                        "name": "Marcos M. Raimundo"
                    },
                    {
                        "authorId": "2526506",
                        "name": "L. G. Nonato"
                    },
                    {
                        "authorId": "1998171",
                        "name": "Jorge Poco"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The Mahalanobis Distance (MD) [76], commonly used to find multivariate outliers, can take the correlation between features into account [62,77]."
            ],
            "citingPaper": {
                "paperId": "3ee94d9f73a8ba6b291e254cdb0df18acce58056",
                "externalIds": {
                    "ArXiv": "2107.04680",
                    "DBLP": "journals/corr/abs-2107-04680",
                    "DOI": "10.3390/app11167274",
                    "CorpusId": 235794770
                },
                "corpusId": 235794770,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3ee94d9f73a8ba6b291e254cdb0df18acce58056",
                "title": "A Framework and Benchmarking Study for Counterfactual Generating Methods on Tabular Data",
                "abstract": "Counterfactual explanations are viewed as an effective way to explain machine learning predictions. This interest is reflected by a relatively young literature with already dozens of algorithms aiming to generate such explanations. These algorithms are focused on finding how features can be modified to change the output classification. However, this rather general objective can be achieved in different ways, which brings about the need for a methodology to test and benchmark these algorithms. The contributions of this work are manifold: First, a large benchmarking study of 10 algorithmic approaches on 22 tabular datasets is performed, using nine relevant evaluation metrics; second, the introduction of a novel, first of its kind, framework to test counterfactual generation algorithms; third, a set of objective metrics to evaluate and compare counterfactual results; and, finally, insight from the benchmarking results that indicate which approaches obtain the best performance on what type of dataset. This benchmarking study and framework can help practitioners in determining which technique and building blocks most suit their context, and can help researchers in the design and evaluation of current and future counterfactual generation algorithms. Our findings show that, overall, there\u2019s no single best algorithm to generate counterfactual explanations as the performance highly depends on properties related to the dataset, model, score, and factual point specificities.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118958649",
                        "name": "Raphael Mazzine"
                    },
                    {
                        "authorId": "145147309",
                        "name": "David Martens"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Human interpretable CFEs should involve changes to only a few features, and is usually enforced by `1-regularisation [30, 19, 27, 38] or `0-regularisation [14].",
                "On the other, methods exist that require full introspection into the model\u2019s specification, notably for tree ensembles [50, 30, 19]."
            ],
            "citingPaper": {
                "paperId": "ff2f098e2ad415841836e985ee6dfa2fbad7e0c7",
                "externalIds": {
                    "ArXiv": "2106.15212",
                    "DBLP": "journals/corr/abs-2106-15212",
                    "CorpusId": 235669629
                },
                "corpusId": 235669629,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ff2f098e2ad415841836e985ee6dfa2fbad7e0c7",
                "title": "Counterfactual Explanations for Arbitrary Regression Models",
                "abstract": "We present a new method for counterfactual explanations (CFEs) based on Bayesian optimisation that applies to both classification and regression models. Our method is a globally convergent search algorithm with support for arbitrary regression models and constraints like feature sparsity and actionable recourse, and furthermore can answer multiple counterfactual questions in parallel while learning from previous queries. We formulate CFE search for regression models in a rigorous mathematical framework using differentiable potentials, which resolves robustness issues in threshold-based objectives. We prove that in this framework, (a) verifying the existence of counterfactuals is NP-complete; and (b) that finding instances using such potentials is CLS-complete. We describe a unified algorithm for CFEs using a specialised acquisition function that composes both expected improvement and an exponential-polynomial (EP) family with desirable properties. Our evaluation on real-world benchmark domains demonstrate high sample-efficiency and precision.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49306150",
                        "name": "Thomas Spooner"
                    },
                    {
                        "authorId": "22214449",
                        "name": "Danial Dervovic"
                    },
                    {
                        "authorId": "2117316193",
                        "name": "Jason Long"
                    },
                    {
                        "authorId": "2115954363",
                        "name": "Jon Shepard"
                    },
                    {
                        "authorId": "1720719982",
                        "name": "Jiahao Chen"
                    },
                    {
                        "authorId": "1738142",
                        "name": "D. Magazzeni"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "caa7aa54e908ecb2ce4bc94a4e153a7545a642a2",
                "externalIds": {
                    "ArXiv": "2106.06631",
                    "DBLP": "journals/corr/abs-2106-06631",
                    "CorpusId": 235422675
                },
                "corpusId": 235422675,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/caa7aa54e908ecb2ce4bc94a4e153a7545a642a2",
                "title": "Optimal Counterfactual Explanations in Tree Ensembles",
                "abstract": "Counterfactual explanations are usually generated through heuristics that are sensitive to the search's initial conditions. The absence of guarantees of performance and robustness hinders trustworthiness. In this paper, we take a disciplined approach towards counterfactual explanations for tree ensembles. We advocate for a model-based search aiming at\"optimal\"explanations and propose efficient mixed-integer programming approaches. We show that isolation forests can be modeled within our framework to focus the search on plausible explanations with a low outlier score. We provide comprehensive coverage of additional constraints that model important objectives, heterogeneous data types, structural constraints on the feature space, along with resource and actionability restrictions. Our experimental analyses demonstrate that the proposed search approach requires a computational effort that is orders of magnitude smaller than previous mathematical programming algorithms. It scales up to large data sets and tree ensembles, where it provides, within seconds, systematic explanations grounded on well-defined models solved to optimality.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2171487",
                        "name": "Axel Parmentier"
                    },
                    {
                        "authorId": "27855049",
                        "name": "Thibaut Vidal"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "a03299482ae1bfe4dc150acdaa5578ed10ea6ec1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-14820",
                    "ArXiv": "2105.14820",
                    "CorpusId": 235254715
                },
                "corpusId": 235254715,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a03299482ae1bfe4dc150acdaa5578ed10ea6ec1",
                "title": "An exact counterfactual-example-based approach to tree-ensemble models interpretability",
                "abstract": "Explaining the decisions of machine learning models is becoming a necessity in many areas where trust in ML models decision is key to their accreditation/adoption. The ability to explain models decisions also allows to provide diagnosis in addition to the model decision, which is highly valuable in scenarios such as fault detection. Unfortunately, high-performance models do not exhibit the necessary transparency to make their decisions fully understandable. And the black-boxes approaches, which are used to explain such model decisions, suffer from a lack of accuracy in tracing back the exact cause of a model decision regarding a given input. Indeed, they do not have the ability to explicitly describe the decision regions of the model around that input, which is necessary to determine what influences the model towards one decision or the other. We thus asked ourselves the question: is there a category of high-performance models among the ones currently used for which we could explicitly and exactly characterise the decision regions in the input feature space using a geometrical characterisation? Surprisingly we came out with a positive answer for any model that enters the category of tree ensemble models, which encompasses a wide range of high-performance models such as XGBoost, LightGBM, random forests ... We could derive an exact geometrical characterisation of their decision regions under the form of a collection of multidimensional intervals. This characterisation makes it straightforward to compute the optimal counterfactual (CF) example associated with a query point. We demonstrate several possibilities of the approach, such as computing the CF example based only on a subset of features. This allows to obtain more plausible explanations by adding prior knowledge about which variables the user can control. An adaptation to CF reasoning on regression problems is also envisaged.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2246821",
                        "name": "P. Blanchart"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                ", SHAP values [12], LIME feature importances [15], counterfactual examples [11, 17])."
            ],
            "citingPaper": {
                "paperId": "22e26fc776374f733f12a7905837e671b7eb7932",
                "externalIds": {
                    "ArXiv": "2103.14976",
                    "DBLP": "journals/corr/abs-2103-14976",
                    "CorpusId": 232403994
                },
                "corpusId": 232403994,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/22e26fc776374f733f12a7905837e671b7eb7932",
                "title": "A Multistakeholder Approach Towards Evaluating AI Transparency Mechanisms",
                "abstract": "Given that there are a variety of stakeholders involved in, and affected by, decisions from machine learning (ML) models, it is important to consider that different stakeholders have different transparency needs. Previous work found that the majority of deployed transparency mechanisms primarily serve technical stakeholders. In our work, we want to investigate how well transparency mechanisms might work in practice for a more diverse set of stakeholders by conducting a large-scale, mixed-methods user study across a range of organizations, within a particular industry such as health care, criminal justice, or content moderation. In this paper, we outline the setup for our study.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38702106",
                        "name": "Ana Lucic"
                    },
                    {
                        "authorId": "2049095005",
                        "name": "Madhulika Srikumar"
                    },
                    {
                        "authorId": "32326200",
                        "name": "Umang Bhatt"
                    },
                    {
                        "authorId": "2254578470",
                        "name": "Alice Xiang"
                    },
                    {
                        "authorId": "2254595551",
                        "name": "Ankur Taly"
                    },
                    {
                        "authorId": "2254437849",
                        "name": "Q. V. Liao"
                    },
                    {
                        "authorId": "1696030",
                        "name": "M. de Rijke"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "The resulting optimal CF explanation is \u2206\u2217 x = x\u2217 \u2212 x [15].",
                "[15], we generate CF examples by minimizing a loss function of the form:",
                "Adversarial attacks [27] are also related to CF examples, but there is a distinction in the intent: adversarial examples are meant to fool the model, while CF examples are meant to explain the prediction [15]."
            ],
            "citingPaper": {
                "paperId": "11b9f4729c8e355dec7122993076f6e2788c03c4",
                "externalIds": {
                    "DBLP": "conf/aistats/LucicHTRS22",
                    "ArXiv": "2102.03322",
                    "CorpusId": 231839528
                },
                "corpusId": 231839528,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/11b9f4729c8e355dec7122993076f6e2788c03c4",
                "title": "CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks",
                "abstract": "Given the increasing promise of graph neural networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. Existing methods for interpreting predictions from GNNs have primarily focused on generating subgraphs that are especially relevant for a particular prediction. However, such methods are not counterfactual (CF) in nature: given a prediction, we want to understand how the prediction can be changed in order to achieve an alternative outcome. In this work, we propose a method for generating CF explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. Using only edge deletions, we find that our method, CF-GNNExplainer, can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94\\% accuracy. This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38702106",
                        "name": "Ana Lucic"
                    },
                    {
                        "authorId": "41096186",
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "authorId": "2651748",
                        "name": "Gabriele Tolomei"
                    },
                    {
                        "authorId": "1696030",
                        "name": "M. de Rijke"
                    },
                    {
                        "authorId": "144925193",
                        "name": "F. Silvestri"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[202, 203] Complete Tree ensemble No No L1 No No No -"
            ],
            "citingPaper": {
                "paperId": "f9145d932ae9e454d9a45ae23fdb0ec0171e4ef4",
                "externalIds": {
                    "ArXiv": "2010.10596",
                    "CorpusId": 253510293
                },
                "corpusId": 253510293,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f9145d932ae9e454d9a45ae23fdb0ec0171e4ef4",
                "title": "Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review",
                "abstract": "Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1780214785",
                        "name": "Sahil Verma"
                    },
                    {
                        "authorId": "2190750501",
                        "name": "Varich Boonsanong"
                    },
                    {
                        "authorId": "2190750431",
                        "name": "Minh Hoang"
                    },
                    {
                        "authorId": "4634403",
                        "name": "Keegan E. Hines"
                    },
                    {
                        "authorId": "1718974",
                        "name": "John P. Dickerson"
                    },
                    {
                        "authorId": "2145672392",
                        "name": "Chirag Shah"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                ", 1999) or attempt to approximate such models as decision trees or random forests with a differentiable version (Yang et al., 2018; Lucic et al., 2022).",
                "could use policy gradient (Sutton et al., 1999) or attempt to approximate such models as decision trees or random forests with a differentiable version (Yang et al., 2018; Lucic et al., 2022).",
                "\u2026(Verma et al., 2020), clamping each one-hot column to be a specific categorical value (Wachter et al., 2017; Downs et al., 2020), relying on genetic algorithms or SMT solvers for automatic treatment (Karimi et al., 2020a; Schleich et al., 2021), or simply filtering them out (Lucic et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "ac8c3a274a8a08ddf16d70a82d5244c6ad674f0b",
                "externalIds": {
                    "CorpusId": 256358917
                },
                "corpusId": 256358917,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ac8c3a274a8a08ddf16d70a82d5244c6ad674f0b",
                "title": "S TOCHASTIC O PTIMIZATION FOR C OUNTERFACTUAL E XPLANATIONS",
                "abstract": "Explainable AI offers insights into what factors drive a certain prediction of a black-box AI system. One popular interpreting approach is through counterfactual explanations, which go beyond why a system arrives at a certain decision to further provide suggestions on what a user can do to alter the outcome. A counterfactual example must be satisfy various constraints to be useful for real-world applications. These constraints exist at trade-offs between one and another presenting radical challenges to existing works. To this end, we propose a stochastic learning-based framework that effectively balances the counterfactual trade-offs. The framework consists of a generation and a feature selection module with complementary roles: the former aims to model the distribution of valid counterfactuals whereas the latter serves to enforce additional constraints in a way that allows for differentiable training and amortized optimization. We demonstrate the effectiveness of our method in generating actionable and plausible counterfactuals that are more diverse than the existing methods and particularly more ef\ufb01cient than closest baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145301586",
                        "name": "Trung Le"
                    },
                    {
                        "authorId": "2147319231",
                        "name": "Van-Anh Nguyen"
                    },
                    {
                        "authorId": "1643682004",
                        "name": "He Zhao"
                    },
                    {
                        "authorId": "30561807",
                        "name": "Edwin V. Bonilla"
                    },
                    {
                        "authorId": "2561045",
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "authorId": "1400659302",
                        "name": "Dinh Q. Phung"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "As a consequence, some recourse methods were developed to find recourses for tree ensembles (Tolomei et al., 2017; Lucic et al., 2022) where the non-differentiability prevents a direct application of the recourse objective in (1)."
            ],
            "citingPaper": {
                "paperId": "f97884c43dc5435a6d0f58f3fd832bc33af2bbb9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-06768",
                    "DOI": "10.48550/arXiv.2203.06768",
                    "CorpusId": 247446810
                },
                "corpusId": 247446810,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f97884c43dc5435a6d0f58f3fd832bc33af2bbb9",
                "title": "Algorithmic Recourse in the Face of Noisy Human Responses",
                "abstract": "Lemma",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "89583148",
                        "name": "Martin Pawelczyk"
                    },
                    {
                        "authorId": "2158813275",
                        "name": "Teresa Datta"
                    },
                    {
                        "authorId": "2158815650",
                        "name": "Johannes van-den-Heuvel"
                    },
                    {
                        "authorId": "1686448",
                        "name": "G. Kasneci"
                    },
                    {
                        "authorId": "1892673",
                        "name": "Himabindu Lakkaraju"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "eba98e41c302c953a4449a9bdf96418292b7c688",
                "externalIds": {
                    "DBLP": "conf/icaart/ZhangML22",
                    "DOI": "10.5220/0010900300003116",
                    "CorpusId": 246915659
                },
                "corpusId": 246915659,
                "publicationVenue": {
                    "id": "f6b96a8f-dc43-4d21-99cf-0f2532b7f01f",
                    "name": "International Conference on Agents and Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Agent Artif Intell",
                        "ICAART"
                    ],
                    "url": "http://www.icaart.org/"
                },
                "url": "https://www.semanticscholar.org/paper/eba98e41c302c953a4449a9bdf96418292b7c688",
                "title": "Developing and Experimenting on Approaches to Explainability in AI Systems",
                "abstract": ": There has been a sharp rise in research activities on explainable arti\ufb01cial intelligence (XAI), especially in the context of machine learning (ML). However, there has been less progress in developing and implementing XAI techniques in AI-enabled environments involving non-expert stakeholders. This paper reports our investigations into providing explanations on the outcomes of ML algorithms to non-experts. We investigate the use of three explanation approaches (global, local, and counterfactual), considering decision trees as a use case ML model. We demonstrate the approaches with a sample dataset, and provide empirical results from a study involving over 200 participants. Our results show that most participants have a good understanding of the generated explanations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49889487",
                        "name": "Yuhao Zhang"
                    },
                    {
                        "authorId": "2385485",
                        "name": "Kevin McAreavey"
                    },
                    {
                        "authorId": "1500379841",
                        "name": "Weiru Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In another direction, some works focused on generating CEs for specific model categories, such as tree-based models (Lucic et al., 2022; Tolomei et al., 2017; Parmentier & Vidal, 2021), or differentiable models (Mothilal et al.",
                "In another direction, some works focused on generating CEs for specific model categories, such as tree-based models (Lucic et al., 2022; Tolomei et al., 2017; Parmentier & Vidal, 2021), or differentiable models (Mothilal et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "553b54b994295cae7fc8f097f84c88c09663d679",
                "externalIds": {
                    "CorpusId": 249059215
                },
                "corpusId": 249059215,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/553b54b994295cae7fc8f097f84c88c09663d679",
                "title": "G LOBAL C OUNTERFACTUAL E XPLANATIONS : I NVESTIGATIONS , I MPLEMENTATIONS AND I MPROVEMENTS",
                "abstract": "used in our experiments. Although German Credit includes continuous fea- tures, we find that they have limited effect on the model both during training and in the resulting explanations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "28265392",
                        "name": "Saumitra Mishra"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "As a consequence, some recourse methods were developed to find recourses for tree ensembles (Tolomei et al., 2017; Lucic et al., 2022) where the non-differentiability prevents a direct application of the recourse objective in (1)."
            ],
            "citingPaper": {
                "paperId": "2bb1b84f35cdd22cfd9550907b56591b44819539",
                "externalIds": {
                    "CorpusId": 250089451
                },
                "corpusId": 250089451,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2bb1b84f35cdd22cfd9550907b56591b44819539",
                "title": "L ET U SERS D ECIDE : N AVIGATING THE T RADE - OFFS BETWEEN C OSTS AND R OBUSTNESS IN A LGORITHMIC R ECOURSE",
                "abstract": "the the the prescribed recourses exactly . studies that individuals implement recourses in a noisy and inconsistent manner \u2013 their salary by $505 if the prescribed recourse suggested an increase of $500. Motivated by this, we introduce and study the problem of recourse invalidation in the face of noisy human responses. More speci\ufb01cally, we theoretically and empirically analyze the behavior of state-of-the-art algorithms, and demonstrate that the recourses generated by these algorithms are very likely to be invalidated if small changes are made to them. We further propose a novel framework, EXPECTing noisy responses ( EXPECT ), which addresses the aforementioned problem by explicitly minimizing the probability of recourse invalidation in the face of noisy responses. Experimental evaluation with multiple real world datasets demonstrates the ef\ufb01cacy of the proposed framework, and supports our theoretical \ufb01ndings. , etc.) the upper from Lemma The results show no \u03c3 2 , \u03c9 , etc.) into the upper Lemma 3. The results no violations of our",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "89583148",
                        "name": "Martin Pawelczyk"
                    },
                    {
                        "authorId": "2158813275",
                        "name": "Teresa Datta"
                    },
                    {
                        "authorId": "2158815650",
                        "name": "Johannes van-den-Heuvel"
                    },
                    {
                        "authorId": "1686448",
                        "name": "G. Kasneci"
                    },
                    {
                        "authorId": "1892673",
                        "name": "Himabindu Lakkaraju"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "To fit non-differentiable models in our framework, one could use policy gradient (Sutton et al., 1999) or attempt to approximate such models as decision trees or random forests with a differentiable version (Yang et al., 2018; Lucic et al., 2022).",
                ", 1999) or attempt to approximate such models as decision trees or random forests with a differentiable version (Yang et al., 2018; Lucic et al., 2022).",
                "\u2026(Verma et al., 2020), clamping each one-hot column to be a specific categorical value (Wachter et al., 2017; Downs et al., 2020), relying on genetic algorithms or SMT solvers for automatic treatment (Karimi et al., 2020a; Schleich et al., 2021), or simply filtering them out (Lucic et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "c61e79cc2105db80d52df90a11b2c4a29454d608",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-13446",
                    "DOI": "10.48550/arXiv.2209.13446",
                    "CorpusId": 252545036
                },
                "corpusId": 252545036,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c61e79cc2105db80d52df90a11b2c4a29454d608",
                "title": "Learning to Counter: Stochastic Feature-based Learning for Diverse Counterfactual Explanations",
                "abstract": "Interpretable machine learning seeks to understand the reasoning process of com-plex black-box systems that are long notorious for lack of explainability. One growing interpreting approach is through counterfactual explanations, which go beyond why a system arrives at a certain decision to further provide suggestions on what a user can do to alter the outcome. A counterfactual example must be able to counter the original prediction from the black-box classi\ufb01er, while also satisfying various constraints for practical applications. These constraints exist at trade-offs between one and another presenting radical challenges to existing works. To this end",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Vy Vo"
                    },
                    {
                        "authorId": "145301586",
                        "name": "Trung Le"
                    },
                    {
                        "authorId": "2147319231",
                        "name": "Van-Anh Nguyen"
                    },
                    {
                        "authorId": "1643682004",
                        "name": "He Zhao"
                    },
                    {
                        "authorId": "30561807",
                        "name": "Edwin V. Bonilla"
                    },
                    {
                        "authorId": "2561045",
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "authorId": "1400659302",
                        "name": "Dinh Q. Phung"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Model-specific methods are FEATTWEAK and FOCUS (tree-ensemble-specific), and DEEPFOOL and GRACE (NN-specific).",
                "To accommodate for non-differentiable models, such as tree ensembles, FOCUS (Lucic et al. 2019) frames the problem of finding counterfactual explanations as an optimization task and uses probabilistic model approximations in the optimization framework."
            ],
            "citingPaper": {
                "paperId": "df764f7996377b2c0095715ed74b405460da1e87",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-11960",
                    "CorpusId": 239769284
                },
                "corpusId": 239769284,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/df764f7996377b2c0095715ed74b405460da1e87",
                "title": "ReLACE: Reinforcement Learning Agent for Counterfactual Explanations of Arbitrary Predictive Models",
                "abstract": "The demand for explainable machine learning (ML) models has been growing rapidly in recent years. Amongst the methods proposed to associate ML model predictions with humanunderstandable rationale, counterfactual explanations are one of the most popular. They consist of post-hoc rules derived from counterfactual examples (CFs), i.e., modified versions of input samples that result in alternative output responses from the predictive model to be explained. However, existing CF generation strategies either exploit the internals of specific models (e.g., random forests or neural networks), or depend on each sample\u2019s neighborhood, which makes them hard to be generalized for more complex models and inefficient for larger datasets. In this work, we aim to overcome these limitations and introduce a model-agnostic algorithm to generate optimal counterfactual explanations. Specifically, we formulate the problem of crafting CFs as a sequential decisionmaking task and then find the optimal CFs via deep reinforcement learning (DRL) with discrete-continuous hybrid action space. Differently from other techniques, our method is easily applied to any black-box model, as this resembles the environment that the DRL agent interacts with. In addition, we develop an algorithm to extract explainable decision rules from the DRL agent\u2019s policy, so as to make the process of generating CFs itself transparent. Extensive experiments conducted on several datasets have shown that our method outperforms existing CF generation baselines.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "4026843",
                        "name": "Ziheng Chen"
                    },
                    {
                        "authorId": "144925193",
                        "name": "F. Silvestri"
                    },
                    {
                        "authorId": "2651748",
                        "name": "Gabriele Tolomei"
                    },
                    {
                        "authorId": "2117693967",
                        "name": "He Zhu"
                    },
                    {
                        "authorId": "2144547216",
                        "name": "Jia Wang"
                    },
                    {
                        "authorId": "34609799",
                        "name": "H. Ahn"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                ", SHAP values [12], LIME feature importances [15], counterfactual examples [11, 17])."
            ],
            "citingPaper": {
                "paperId": "acd5e76492c832689ff1f4f0e7c775cb8eca6832",
                "externalIds": {
                    "CorpusId": 262234746
                },
                "corpusId": 262234746,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/acd5e76492c832689ff1f4f0e7c775cb8eca6832",
                "title": "A Multistakeholder Approach Towards Evaluating AI Transparency Mechanisms",
                "abstract": "Disclaimer/Complaints regulations If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please let the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the material inaccessible and/or remove it from the website. Please Ask the Library: https://uba.uva.nl/en/contact, or a letter to: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. You will be contacted as soon as possible.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2244799638",
                        "name": "Ana Lucic"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                ", SHAP values [12], LIME feature importances [15], counterfactual examples [11, 17])."
            ],
            "citingPaper": {
                "paperId": "d10e02e0654788f8d866158d8870de8b063b0401",
                "externalIds": {
                    "CorpusId": 261896618
                },
                "corpusId": 261896618,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d10e02e0654788f8d866158d8870de8b063b0401",
                "title": "UvA-DARE (Digital Academic Repository) A Multistakeholder Approach Towards Evaluating AI Transparency Mechanisms",
                "abstract": "Given that there are a variety of stakeholders involved in, and affected by, decisions from machine learning (ML) models, it is important to consider that different stakeholders have different transparency needs [14]. Previous work found that the majority of deployed transparency mechanisms primarily serve technical stakeholders [2]. In our work, we want to investigate how well transparency mechanisms might work in practice for a more diverse set of stakeholders by conducting a large-scale, mixed-methods user study across a range of organizations, within a particular industry such as health care, criminal justice, or content moderation. In this paper, we outline the setup for our study.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2248874554",
                        "name": "de Rijke"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c310b03f709390bd30b310cbd70fb03261aacb58",
                "externalIds": {
                    "DOI": "10.1016/j.eswa.2023.121670",
                    "CorpusId": 262204573
                },
                "corpusId": 262204573,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c310b03f709390bd30b310cbd70fb03261aacb58",
                "title": "FS-SCF network: Neural network interpretability based on counterfactual generation and feature selection for fault diagnosis",
                "abstract": null,
                "year": null,
                "authors": [
                    {
                        "authorId": "2125913381",
                        "name": "Joaqu\u00edn Figueroa Barraza"
                    },
                    {
                        "authorId": "2244527967",
                        "name": "Enrique L\u00f3pez Droguett"
                    },
                    {
                        "authorId": "2244505262",
                        "name": "Marcelo Ramos Martins"
                    }
                ]
            }
        }
    ]
}