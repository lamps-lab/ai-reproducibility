{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2d9e604632ff7848aa92aae1a1f828b8f755ad2c",
                "externalIds": {
                    "ArXiv": "2310.02588",
                    "CorpusId": 263620450
                },
                "corpusId": 263620450,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2d9e604632ff7848aa92aae1a1f828b8f755ad2c",
                "title": "ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for Vision Transformer",
                "abstract": "This paper presents a novel approach to address the challenges of understanding the prediction process and debugging prediction errors in Vision Transformers (ViT), which have demonstrated superior performance in various computer vision tasks such as image classification and object detection. While several visual explainability techniques, such as CAM, Grad-CAM, Score-CAM, and Recipro-CAM, have been extensively researched for Convolutional Neural Networks (CNNs), limited research has been conducted on ViT. Current state-of-the-art solutions for ViT rely on class agnostic Attention-Rollout and Relevance techniques. In this work, we propose a new gradient-free visual explanation method for ViT, called ViT-ReciproCAM, which does not require attention matrix and gradient information. ViT-ReciproCAM utilizes token masking and generated new layer outputs from the target layer's input to exploit the correlation between activated tokens and network predictions for target classes. Our proposed method outperforms the state-of-the-art Relevance method in the Average Drop-Coherence-Complexity (ADCC) metric by $4.58\\%$ to $5.80\\%$ and generates more localized saliency maps. Our experiments demonstrate the effectiveness of ViT-ReciproCAM and showcase its potential for understanding and debugging ViT models. Our proposed method provides an efficient and easy-to-implement alternative for generating visual explanations, without requiring attention and gradient information, which can be beneficial for various applications in the field of computer vision.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2032811",
                        "name": "Seokhyun Byun"
                    },
                    {
                        "authorId": "2148961398",
                        "name": "Won-Jo Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[19] demonstrate the manipulability of attention-based explanations and Wang et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c6e056cfb10f23475f3e0f5ac541c8cd74447fdc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-14272",
                    "ArXiv": "2308.14272",
                    "DOI": "10.48550/arXiv.2308.14272",
                    "CorpusId": 261244021
                },
                "corpusId": 261244021,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c6e056cfb10f23475f3e0f5ac541c8cd74447fdc",
                "title": "Goodhart's Law Applies to NLP's Explanation Benchmarks",
                "abstract": "Despite the rising popularity of saliency-based explanations, the research community remains at an impasse, facing doubts concerning their purpose, efficacy, and tendency to contradict each other. Seeking to unite the community's efforts around common goals, several recent works have proposed evaluation metrics. In this paper, we critically examine two sets of metrics: the ERASER metrics (comprehensiveness and sufficiency) and the EVAL-X metrics, focusing our inquiry on natural language processing. First, we show that we can inflate a model's comprehensiveness and sufficiency scores dramatically without altering its predictions or explanations on in-distribution test inputs. Our strategy exploits the tendency for extracted explanations and their complements to be\"out-of-support\"relative to each other and in-distribution inputs. Next, we demonstrate that the EVAL-X metrics can be inflated arbitrarily by a simple method that encodes the label, even though EVAL-X is precisely motivated to address such exploits. Our results raise doubts about the ability of current metrics to guide explainability research, underscoring the need for a broader reassessment of what precisely these metrics are intended to capture.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2234372990",
                        "name": "Jennifer Hsia"
                    },
                    {
                        "authorId": "7880098",
                        "name": "Danish Pruthi"
                    },
                    {
                        "authorId": "2109423866",
                        "name": "Aarti Singh"
                    },
                    {
                        "authorId": "32219137",
                        "name": "Zachary Chase Lipton"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "94abb4a364c7572173f01b9657a1f5f11aa13608",
                "externalIds": {
                    "DBLP": "conf/icml/HouC23",
                    "ArXiv": "2308.05219",
                    "DOI": "10.48550/arXiv.2308.05219",
                    "CorpusId": 260775611
                },
                "corpusId": 260775611,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/94abb4a364c7572173f01b9657a1f5f11aa13608",
                "title": "Decoding Layer Saliency in Language Transformers",
                "abstract": "In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152023094",
                        "name": "Elizabeth M. Hou"
                    },
                    {
                        "authorId": "3355010",
                        "name": "Greg Casta\u00f1\u00f3n"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These approaches involve recording the self-attention maps generated by the self-attention heads of the last block in the ViT model during inference.",
                "Despite these advancements, there are few contributions exploring the explainability of the ViT series of models.",
                "B. ViT Explainability\nCurrently, there remain few studies focusing on the explainability of methods belonging to the ViT family.",
                "First, we extract tLS from ViT.",
                "This approach consists of two main parts: generating alternative activation maps M and calculating the class-aware weighting scores w to extract class-aware patch tokens tc.\nGenerating alternative activation maps M : As discussed in III-A, ViT utilizes discrete tokens to convey information.",
                "V iT (\u00b7) denotes the output vector of the ViT model.",
                "Most existing approaches only consider the direct use of the raw-attention map corresponding to the class token in the multi-head self-attention (MHSA) module to directly generate explainability maps in ViT [22], [23], [24].",
                "Some approaches have been proposed to generate explainability maps directly from the raw-attention map corresponding to the cls-token [22], [23], [24].",
                "We firstly generate the patch tokens tLS by removing the last layer class token tLcls from the output of the last layer normalization\n4 \u22ef Transform er layers \u22ef \u22ee\nViT\u2022\n\u22ef\ns\n\u22ef\n\u201czebra\u201d \u201celephant\u201d\nC classes\nM\n\ud835\udc95\ud835\udc95\ud835\udc7a\ud835\udc7a\ud835\udc73\ud835\udc73\nP\nG\nw\nViT(X)\nViT(P)\nX\n\u22ef\nProjection+ Position em\nbedding\nReshape normalization\nHadamard product\nCosine similarity\n\u00d7\nColumnwise\nGenerate graph\nGraph cut\n\ud835\udc95\ud835\udc95\ud835\udc84\ud835\udc84\nR-Out\nCut\nViT Share weights\nUpsampling\nFig.",
                "To compute the weight scores w for each perturbation map Pi, we input both the perturbation map matrix P and the original image X into the pre-trained ViT model.",
                "In the Vision Transformer (ViT) architecture, each transformer block follows a specific arrangement of components.",
                "2) Implementation Details: In our experiments, we used the same pre-trained ViT-base model as the backbone for our explainability maps tests to ensure fairness.",
                "This type of explainability is particularly relevant for models with complex structures, such as Convolutional Neural Networks (CNNs) [6], [7], [8], [9], [10] and Vision Transformers (ViTs) [11], [12], [13], [14], [15], [16].",
                ", raw-attention [22], [23], [24], rollout[25], grad-cam[30], and Hila\u2019s method[26].",
                "The emergence of Vision Transformers (ViTs) has revolutionized computer vision.",
                "A. Vision transformer (ViT)\nThe ViT model is a popular approach for image classification tasks that uses a transformer-based architecture."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "40c86b3f758a2021cbd364c5942ee87c3896d25b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-09050",
                    "ArXiv": "2307.09050",
                    "DOI": "10.48550/arXiv.2307.09050",
                    "CorpusId": 259951049
                },
                "corpusId": 259951049,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/40c86b3f758a2021cbd364c5942ee87c3896d25b",
                "title": "R-Cut: Enhancing Explainability in Vision Transformers with Relationship Weighted Out and Cut",
                "abstract": "Transformer-based models have gained popularity in the field of natural language processing (NLP) and are extensively utilized in computer vision tasks and multi-modal models such as GPT4. This paper presents a novel method to enhance the explainability of Transformer-based image classification models. Our method aims to improve trust in classification results and empower users to gain a deeper understanding of the model for downstream tasks by providing visualizations of class-specific maps. We introduce two modules: the ``Relationship Weighted Out\"and the ``Cut\"modules. The ``Relationship Weighted Out\"module focuses on extracting class-specific information from intermediate layers, enabling us to highlight relevant features. Additionally, the ``Cut\"module performs fine-grained feature decomposition, taking into account factors such as position, texture, and color. By integrating these modules, we generate dense class-specific visual explainability maps. We validate our method with extensive qualitative and quantitative experiments on the ImageNet dataset. Furthermore, we conduct a large number of experiments on the LRN dataset, specifically designed for automatic driving danger alerts, to evaluate the explainability of our method in complex backgrounds. The results demonstrate a significant improvement over previous methods. Moreover, we conduct ablation experiments to validate the effectiveness of each module. Through these experiments, we are able to confirm the respective contributions of each module, thus solidifying the overall effectiveness of our proposed approach.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217228356",
                        "name": "Yingjie Niu"
                    },
                    {
                        "authorId": "145573466",
                        "name": "Ming Ding"
                    },
                    {
                        "authorId": "2142714606",
                        "name": "Maoning Ge"
                    },
                    {
                        "authorId": "2066181657",
                        "name": "Robin Karlsson"
                    },
                    {
                        "authorId": "2108079411",
                        "name": "Yuxiao Zhang"
                    },
                    {
                        "authorId": "153621979",
                        "name": "K. Takeda"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "60078a276178ea7eef660638658f4f89688d1b9f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-11260",
                    "ArXiv": "2306.11260",
                    "DOI": "10.48550/arXiv.2306.11260",
                    "CorpusId": 259204099
                },
                "corpusId": 259204099,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/60078a276178ea7eef660638658f4f89688d1b9f",
                "title": "A novel Counterfactual method for aspect-based sentiment analysis",
                "abstract": "Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation task, which analyze the emotional polarity of the evaluation aspects. However, previous works only focus on the identification of opinion expressions, forget that the diversity of opinion expressions also has great impacts on the ABSA task. To mitigate this problem, we propose a novel counterfactual data augmentation method to generate opinion expression with reversed sentiment polarity. Specially, the integrated gradients are calculated to identify and mask the opinion expression. Then, a prompt with the reverse label is combined to the original text, and a pre-trained language model (PLM), T5, is finally employed to retrieve the masks. The experimental results show the proposed counterfactual data augmentation method perform better than current augmentation methods on three ABSA datasets, i.e. Laptop, Restaurant and MAMS.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2116668838",
                        "name": "Dongming Wu"
                    },
                    {
                        "authorId": "2220294009",
                        "name": "Lulu Wen"
                    },
                    {
                        "authorId": "2145763032",
                        "name": "Chao Chen"
                    },
                    {
                        "authorId": "2220762375",
                        "name": "Zhaoshu Shi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This sensitivity makes them susceptible to manipulations designed to hide the use of problematic features in the AI decision-making and deceive the human overseeing the system [7, 11, 28, 65, 92]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e4a6870741997ccbfad39837a9dfc2afe18a004c",
                "externalIds": {
                    "DBLP": "conf/fat/PaniguttiHHLYJS23",
                    "DOI": "10.1145/3593013.3594069",
                    "CorpusId": 259139673
                },
                "corpusId": 259139673,
                "publicationVenue": {
                    "id": "cdc83875-a82d-445c-b097-cbe91afe99a8",
                    "name": "Conference on Fairness, Accountability and Transparency",
                    "type": "conference",
                    "alternate_names": [
                        "FAccT",
                        "Conf Fairness Account Transpar"
                    ],
                    "url": "https://facctconference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e4a6870741997ccbfad39837a9dfc2afe18a004c",
                "title": "The role of explainable AI in the context of the AI Act",
                "abstract": "The proposed EU regulation for Artificial Intelligence (AI), the AI Act, has sparked some debate about the role of explainable AI (XAI) in high-risk AI systems. Some argue that black-box AI models will have to be replaced with transparent ones, others argue that using XAI techniques might help in achieving compliance. This work aims to bring some clarity as regards XAI in the context of the AI Act and focuses in particular on the AI Act requirements for transparency and human oversight. After outlining key points of the debate and describing the current limitations of XAI techniques, this paper carries out an interdisciplinary analysis of how the AI Act addresses the issue of opaque AI systems. In particular, we argue that neither does the AI Act mandate a requirement for XAI, which is the subject of intense scientific research and is not without technical limitations, nor does it ban the use of black-box AI systems. Instead, the AI Act aims to achieve its stated policy objectives with the focus on transparency (including documentation) and human oversight. Finally, in order to concretely illustrate our findings and conclusions, a use case on AI-based proctoring is presented.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "14644743",
                        "name": "Cecilia Panigutti"
                    },
                    {
                        "authorId": "2897267",
                        "name": "Ronan Hamon"
                    },
                    {
                        "authorId": "2215413727",
                        "name": "Isabelle Hupont"
                    },
                    {
                        "authorId": "1938560317",
                        "name": "David Fern\u00e1ndez Llorca"
                    },
                    {
                        "authorId": "2127104511",
                        "name": "Delia Fano Yela"
                    },
                    {
                        "authorId": "2288623",
                        "name": "H. Junklewitz"
                    },
                    {
                        "authorId": "2219930073",
                        "name": "Salvatore Scalzo"
                    },
                    {
                        "authorId": "100789945",
                        "name": "Gabriele Mazzini"
                    },
                    {
                        "authorId": "2058031324",
                        "name": "Ignacio Sanchez"
                    },
                    {
                        "authorId": "2219929912",
                        "name": "Josep Soler Garrido"
                    },
                    {
                        "authorId": "1740615089",
                        "name": "Emilia G\u00f3mez"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "432757ff34d1c46393204bd7b5ecdcb712f32beb",
                "externalIds": {
                    "ArXiv": "2306.05143",
                    "DBLP": "journals/corr/abs-2306-05143",
                    "DOI": "10.48550/arXiv.2306.05143",
                    "CorpusId": 259108697
                },
                "corpusId": 259108697,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/432757ff34d1c46393204bd7b5ecdcb712f32beb",
                "title": "Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer",
                "abstract": "Given the increasing volume and quality of genomics data, extracting new insights requires interpretable machine-learning models. This work presents Genomic Interpreter: a novel architecture for genomic assay prediction. This model outperforms the state-of-the-art models for genomic assay prediction tasks. Our model can identify hierarchical dependencies in genomic sites. This is achieved through the integration of 1D-Swin, a novel Transformer-based block designed by us for modelling long-range hierarchical data. Evaluated on a dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter demonstrates superior performance in chromatin accessibility and gene expression prediction and unmasks the underlying `syntax' of gene regulation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109967301",
                        "name": "Zehui Li"
                    },
                    {
                        "authorId": "9339041",
                        "name": "Akashaditya Das"
                    },
                    {
                        "authorId": "108630561",
                        "name": "W. Beardall"
                    },
                    {
                        "authorId": "2109919449",
                        "name": "Yiren Zhao"
                    },
                    {
                        "authorId": "144335262",
                        "name": "G. Stan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, using information from raw attention poses limitations to the complete use of the structural characteristics of vision transformers, which include multiple learning modules [24]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "33cc4d534349e88fd6d396a05c0aafbdb4502a99",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChoiJH23",
                    "DOI": "10.1109/CVPR52729.2023.01166",
                    "CorpusId": 260868804
                },
                "corpusId": 260868804,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/33cc4d534349e88fd6d396a05c0aafbdb4502a99",
                "title": "Adversarial Normalization: I Can visualize Everything (ICE)",
                "abstract": "Vision transformers use [CLS] tokens to predict image classes. Their explainability visualization has been studied using relevant information from [CLS] tokens or focusing on attention scores during self-attention. Such visualization, however, is challenging because of the dependence of the structure of a vision transformer on skip connections and attention operators, the instability of non-linearities in the learning process, and the limited reflection of self-attention scores on relevance. We argue that the output vectors for each input patch token in a vision transformer retain the image information of each patch location, which can facilitate the prediction of an image class. In this paper, we propose ICE (Adversarial Normalization: I Can visualize Everything), a novel method that enables a model to directly predict a class for each patch in an image; thus, advancing the effective visualization of the explainability of a vision transformer. Our method distinguishes background from foreground regions by predicting background classes for patches that do not determine image classes. We used the DeiT-S model, the most representative model employed in studies, on the explainability visualization of vision transformers. On the ImageNet-Segmentation dataset, ICE outperformed all explainability visualization methods for four cases depending on the model size. We also conducted quantitative and qualitative analyses on the tasks of weakly-supervised object localization and unsupervised object discovery. On the CUB-200-2011 and PASCALVOC07/12 datasets, ICE achieved comparable performance to the state-of-the-art methods. We incorporated ICE into the encoder of DeiT-S and improved efficiency by 44.01% on the ImageNet dataset over that achieved by the original DeiT-S model. We showed performance on the accuracy and efficiency comparable to EViT, the state-of-the-art pruning model, demonstrating the effectiveness of ICE. The code is available at https://github.com/Hanyang-HCC-Lab/ICE.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111528165",
                        "name": "H. Choi"
                    },
                    {
                        "authorId": "1666224664",
                        "name": "Seungwan Jin"
                    },
                    {
                        "authorId": "35655049",
                        "name": "Kyungsik Han"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4a2ee7e1272e7889ff8389e9d5702216235fc638",
                "externalIds": {
                    "ArXiv": "2305.17627",
                    "DBLP": "conf/acl/WangHYZC23",
                    "DOI": "10.48550/arXiv.2305.17627",
                    "CorpusId": 258959215
                },
                "corpusId": 258959215,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/4a2ee7e1272e7889ff8389e9d5702216235fc638",
                "title": "Robust Natural Language Understanding with Residual Attention Debiasing",
                "abstract": "Natural language understanding (NLU) models often suffer from unintended dataset biases. Among bias mitigation methods, ensemble-based debiasing methods, especially product-of-experts (PoE), have stood out for their impressive empirical success. However, previous ensemble-based debiasing methods typically apply debiasing on top-level logits without directly addressing biased attention patterns. Attention serves as the main media of feature interaction and aggregation in PLMs and plays a crucial role in providing robust prediction. In this paper, we propose REsidual Attention Debiasing (READ), an end-to-end debiasing method that mitigates unintended biases from attention. Experiments on three NLU tasks show that READ significantly improves the performance of BERT-based models on OOD data with shortcuts removed, including +12.9% accuracy on HANS, +11.0% accuracy on FEVER-Symmetric, and +2.7% F1 on PAWS. Detailed analyses demonstrate the crucial role of unbiased attention in robust NLU models and that READ effectively mitigates biases in attention. Code is available at https://github.com/luka-group/READ.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47939052",
                        "name": "Fei Wang"
                    },
                    {
                        "authorId": "2110302673",
                        "name": "James Y. Huang"
                    },
                    {
                        "authorId": "2059616596",
                        "name": "Tianyi Yan"
                    },
                    {
                        "authorId": "2203076",
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "authorId": "1998918",
                        "name": "Muhao Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2020), and on applying gradient-based methods (Li et al.",
                "This line of research includes a large body of work focusing on the analysis of the attention mechanism\n(Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2020), and on applying gradient-based methods (Li et al., 2016a; Sundararajan et al., 2017) to obtain input attribution scores."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "01a659968a6511e590f0e37a81eb25e53fa2c752",
                "externalIds": {
                    "ACL": "2023.acl-long.301",
                    "DBLP": "journals/corr/abs-2305-12535",
                    "ArXiv": "2305.12535",
                    "DOI": "10.48550/arXiv.2305.12535",
                    "CorpusId": 258832652
                },
                "corpusId": 258832652,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/01a659968a6511e590f0e37a81eb25e53fa2c752",
                "title": "Explaining How Transformers Use Context to Build Predictions",
                "abstract": "Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model\u2019s prediction, it is still unclear how prior words affect the model\u2019s decision throughout the layers. In this work, we leverage recent advances in explainability of the Transformer and present a procedure to analyze models for language generation. Using contrastive examples, we compare the alignment of our explanations with evidence of the linguistic phenomena, and show that our method consistently aligns better than gradient-based and perturbation-based baselines. Then, we investigate the role of MLPs inside the Transformer and show that they learn features that help the model predict words that are grammatically acceptable. Lastly, we apply our method to Neural Machine Translation models, and demonstrate that they generate human-like source-target alignments for building predictions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1751450782",
                        "name": "Javier Ferrando"
                    },
                    {
                        "authorId": "2003752849",
                        "name": "Gerard I. G\u00e1llego"
                    },
                    {
                        "authorId": "122891770",
                        "name": "Ioannis Tsiamas"
                    },
                    {
                        "authorId": "1398996347",
                        "name": "M. Costa-juss\u00e0"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "feef7ce527341e4427260cf15be6aa1ac9668c1c",
                "externalIds": {
                    "DOI": "10.3233/sw-233183",
                    "CorpusId": 258821957
                },
                "corpusId": 258821957,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/feef7ce527341e4427260cf15be6aa1ac9668c1c",
                "title": "Interpretable ontology extension in chemistry",
                "abstract": "Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction and annotation enables them to maintain high quality, allowing them to be widely accepted across their community. However, the manual ontology development process does not scale for large domains. We present a new methodology for automatic ontology extension for domains in which the ontology classes have associated graph-structured annotations, and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We train Transformer-based deep learning models on the leaf node structures from the ChEBI ontology and the classes to which they belong. The models are then able to automatically classify previously unseen chemical structures, resulting in automated ontology extension. The proposed models achieved an overall F1 scores of 0.80 and above, improvements of at least 6 percentage points over our previous results on the same dataset. In addition, the models are interpretable: we illustrate that visualizing the model\u2019s attention weights can help to explain the results by providing insight into how the model made its decisions. We also analyse the performance for molecules that have not been part of the ontology and evaluate the logical correctness of the resulting extension.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2453522",
                        "name": "Martin Glauer"
                    },
                    {
                        "authorId": "1994523246",
                        "name": "A. Memariani"
                    },
                    {
                        "authorId": "47498173",
                        "name": "F. Neuhaus"
                    },
                    {
                        "authorId": "1764365",
                        "name": "T. Mossakowski"
                    },
                    {
                        "authorId": "1715137",
                        "name": "Janna Hastings"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In doing so, we avoid the non-identifiability problem of transformer models (Pruthi et al., 2019).",
                "In spite of a number of supporters initially for this approach, there has been a recent wave of detractors of attentionbased explanations (Jain and Wallace, 2019; Pruthi et al., 2019; Serrano and Smith, 2019).",
                "Because we are factorizing h, we can generate explanations on embeddings without needing to deal with the complexities of attention layers (Pruthi et al., 2019); nor do we have to deal with the nonidentifiability of transformer models (Brunner et al.",
                "Recent debates have shown that attention maps and attribution methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a0c3da053af5d47b1eda747e120fd95267c28f75",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-06754",
                    "ArXiv": "2305.06754",
                    "DOI": "10.48550/arXiv.2305.06754",
                    "CorpusId": 258615238
                },
                "corpusId": 258615238,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/a0c3da053af5d47b1eda747e120fd95267c28f75",
                "title": "COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks",
                "abstract": "Transformer architectures are complex and their use in NLP, while it has engendered many successes, makes their interpretability or explainability challenging. Recent debates have shown that attention maps and attribution methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this paper, we present some of their limitations and introduce COCKATIEL, which successfully addresses some of them. COCKATIEL is a novel, post-hoc, concept-based, model-agnostic XAI technique that generates meaningful explanations from the last layer of a neural net model trained on an NLP classification task by using Non-Negative Matrix Factorization (NMF) to discover the concepts the model leverages to make predictions and by exploiting a Sensitivity Analysis to estimate accurately the importance of each of these concepts for the model. It does so without compromising the accuracy of the underlying model or requiring a new one to be trained. We conduct experiments in single and multi-aspect sentiment analysis tasks and we show COCKATIEL's superior ability to discover concepts that align with humans' on Transformer models without any supervision, we objectively verify the faithfulness of its explanations through fidelity metrics, and we showcase its ability to provide meaningful explanations in two different datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210068687",
                        "name": "Fanny Jourdan"
                    },
                    {
                        "authorId": "150916914",
                        "name": "Agustin Picard"
                    },
                    {
                        "authorId": "1935310411",
                        "name": "Thomas Fel"
                    },
                    {
                        "authorId": "144450552",
                        "name": "L. Risser"
                    },
                    {
                        "authorId": "144736569",
                        "name": "J. Loubes"
                    },
                    {
                        "authorId": "1916126",
                        "name": "Nicholas M. Asher"
                    }
                ]
            }
        },
        {
            "contexts": [
                "can be deceivable, determining the importance of the output only based on attention weights is not explainable [4]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c54125a07037eeb1b78be992325420b1913f200c",
                "externalIds": {
                    "DOI": "10.1109/ICAAIC56838.2023.10141005",
                    "CorpusId": 259122356
                },
                "corpusId": 259122356,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c54125a07037eeb1b78be992325420b1913f200c",
                "title": "Explainability to Business: Demystify Transformer Models with Attention-based Explanations",
                "abstract": "Recently, many companies are relying on Natural Language Processing (NLP) techniques to understand the text data generated daily. It has become very critical to deal with this data because finding the sentiments of text and summarizing them will help the company understand the pain points of the customers posting reviews on social media or understand the experience of the customer. These requirements have increasingly demanded many advanced algorithms to deal the text data. The introduction of Transformers led to businesses adopting NLP methods more and more to keep up with their needs. Models like Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformers (GPT), state-of-the-art results were achieved with billions of parameters learned. Although these advancements improved the accuracy and expanded the use of algorithms to a wide range of NLP tasks like language translation, text summarization, and language modeling. Businesses are more interested in the Explainability of the model compared to its accuracy. Explainable Artificial Intelligence (XAI) plays an important role to comprehend the complexities of the model as well as the influence of weights on predictions. In this paper, the complexities of the transformer model are unraveled by presenting a straightforward method for computing explainable predictions. The DistilBERT model is chosen as an example to implement the explainable system due to its lighter nature. Combining the strengths of a Posthoc expla-nation with those of a self-learning neural network, the method makes it simple to scale it to other algorithms to implement. With technologies like python, PyTorch, and Hugging Face, a detailed step-by-step algorithmic computation is demonstrated to explain the predictions from the attention-based explanations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219800366",
                        "name": "Rajasekhar Thiruthuvaraj"
                    },
                    {
                        "authorId": "74223762",
                        "name": "Ashly Ann Jo"
                    },
                    {
                        "authorId": "3092219",
                        "name": "Ebin Deni Raj"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, recent studies [14, 30, 42] showed that attention is not an explanation."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "002eda35e1c255024c5967f3d972fe2d864aea66",
                "externalIds": {
                    "DBLP": "conf/www/NguyenR23",
                    "DOI": "10.1145/3543507.3583861",
                    "CorpusId": 258333869
                },
                "corpusId": 258333869,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/002eda35e1c255024c5967f3d972fe2d864aea66",
                "title": "Learning Faithful Attention for Interpretable Classification of Crisis-Related Microblogs under Constrained Human Budget",
                "abstract": "The recent widespread use of social media platforms has created convenient ways to obtain and spread up-to-date information during crisis events such as disasters. Time-critical analysis of crisis data can help human organizations gain actionable information and plan for aid responses. Many existing studies have proposed methods to identify informative messages and categorize them into different humanitarian classes. Advanced neural network architectures tend to achieve state-of-the-art performance, but the model decisions are opaque. While attention heatmaps show insights into the model\u2019s prediction, some studies found that standard attention does not provide meaningful explanations. Alternatively, recent works proposed interpretable approaches for the classification of crisis events that rely on human rationales to train and extract short snippets as explanations. However, the rationale annotations are not always available, especially in real-time situations for new tasks and events. In this paper, we propose a two-stage approach to learn the rationales under minimal human supervision and derive faithful machine attention. Extensive experiments over four crisis events show that our model is able to obtain better or comparable classification performance (\u223c 86% Macro-F1) to baselines and faithful attention heatmaps using only 40-50% human-level supervision. Further, we employ a zero-shot learning setup to detect actionable tweets along with actionable word snippets as rationales.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118937837",
                        "name": "Thi-Huyen Nguyen"
                    },
                    {
                        "authorId": "2042376",
                        "name": "Koustav Rudra"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "65eb33d12d7d98bbe71cecb69912005f0b22c99d",
                "externalIds": {
                    "DBLP": "journals/tkde/ZhangQLCD23",
                    "DOI": "10.1109/TKDE.2021.3130171",
                    "CorpusId": 244552812
                },
                "corpusId": 244552812,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/65eb33d12d7d98bbe71cecb69912005f0b22c99d",
                "title": "Context-Aware and Time-Aware Attention-Based Model for Disease Risk Prediction With Interpretability",
                "abstract": "Thanks to the huge accumulation of Electronic Health Records (EHRs), numerous deep learning based predictive models were proposed for this task. Among them, most of the existing state-of-the-art (SOTA) models were built with recurrent neural networks (RNNs). Regardless of their success, RNN-based models mainly suffer from three limitations. (i) Accuracy: the prediction accuracy of RNN-based models drops quickly as the length of EHR sequences increases. (ii) Efficiency: the recurrence property of RNN-based models makes the computation parallelization impossible, and accordingly hurts the efficiency of such models in practice. (iii) Interpretability: the outputs of RNN-based models are difficult to explain due to the unexplainable nature of deep models. In this paper, we resort to the recently advanced attention mechanism to model the dependencies between inputs and outputs, which overcomes shortages of RNN-based models in accuracy and efficiency. As for interpretability, we model the relationships with two linear mappings from the input to the output, which account for two important factors\u2014one is for context-aware information and the other is for time-aware representation\u2014of capturing discriminative features in learning patient\u2019s representations. We empirically demonstrate the effectiveness of the proposed model in both accuracy and computational efficiency, meanwhile, analyze and discuss the reasonability of each explanation approach.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108079176",
                        "name": "Xianli Zhang"
                    },
                    {
                        "authorId": "39835284",
                        "name": "B. Qian"
                    },
                    {
                        "authorId": "2154901370",
                        "name": "Yang Li"
                    },
                    {
                        "authorId": "46387279",
                        "name": "Shilei Cao"
                    },
                    {
                        "authorId": "98603960",
                        "name": "Ian Davidson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Past work has used attention weights as a mechanism of providing explanations, but it has been observed that such explanations may not always be faithful to predictions [12, 17, 41]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3b81d518781e0aecfcd52f1c7226f6a98b8ee622",
                "externalIds": {
                    "DBLP": "conf/wsdm/VedulaCAR23",
                    "DOI": "10.1145/3539597.3570489",
                    "CorpusId": 257079741
                },
                "corpusId": 257079741,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/3b81d518781e0aecfcd52f1c7226f6a98b8ee622",
                "title": "Generating Explainable Product Comparisons for Online Shopping",
                "abstract": "An essential part of making shopping purchase decisions is to compare and contrast products based on key differentiating features, but doing this manually can be overwhelming. Prior methods offer limited product comparison capabilities, e.g., via pre-defined common attributes that may be difficult to understand, or irrelevant to a particular product or user. Automatically generating an informative, natural-sounding, and factually consistent comparative text for multiple product and attribute types is a challenging research problem. We describe HCPC (Human Centered Product Comparison), to tackle two kinds of comparisons for online shopping: (i) product-specific, to describe and compare products based on their key attributes; and (ii) attribute-specific comparisons, to compare similar products on a specific attribute. To ensure that comparison text is faithful to the input product data, we introduce a novel multi-decoder, multi-task generative language model. One decoder generates product comparison text, and a second one generates supportive, explanatory text in the form of product attribute names and values. The second task imitates a copy mechanism, improving the comparison generator, and its output is used to justify the factual accuracy of the generated comparison text, by training a factual consistency model to detect and correct errors in the generated comparative text. We release a new dataset (https://registry.opendata.aws/) of ~15K human generated sentences, comparing products on one or more attributes (the first such data we know of for product comparison). We demonstrate on this data that HCPC significantly outperforms strong baselines, by ~10% using automatic metrics, and ~5% using human evaluation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "19304470",
                        "name": "Nikhita Vedula"
                    },
                    {
                        "authorId": "2068537998",
                        "name": "M. Collins"
                    },
                    {
                        "authorId": "1685296",
                        "name": "Eugene Agichtein"
                    },
                    {
                        "authorId": "3046332",
                        "name": "O. Rokhlenko"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6d1ba0b3e6b6f84a73ffc0b41cb2722f2a079e76",
                "externalIds": {
                    "DBLP": "journals/jcal/LottridgeWYJO23",
                    "DOI": "10.1111/jcal.12784",
                    "CorpusId": 256711010
                },
                "corpusId": 256711010,
                "publicationVenue": {
                    "id": "2d5e093c-6946-4495-ab7e-c7b814a1730d",
                    "name": "Journal of Computer Assisted Learning",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Assist Learn"
                    ],
                    "issn": "0266-4909",
                    "url": "http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1365-2729",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/13652729"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6d1ba0b3e6b6f84a73ffc0b41cb2722f2a079e76",
                "title": "The use of annotations to explain labels: Comparing results from a human-rater approach to a deep learning approach",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "6676556",
                        "name": "Susan Lottridge"
                    },
                    {
                        "authorId": "2041781328",
                        "name": "S. Woolf"
                    },
                    {
                        "authorId": "47533682",
                        "name": "M. Young"
                    },
                    {
                        "authorId": "1409870137",
                        "name": "Amir Jafari"
                    },
                    {
                        "authorId": "8677028",
                        "name": "Christopher M. Ormerod"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0fcf60b828e4bf52a17ae04c8b0d4502dea2ba8b",
                "externalIds": {
                    "DOI": "10.3389/fmars.2023.1112065",
                    "CorpusId": 256599570
                },
                "corpusId": 256599570,
                "publicationVenue": {
                    "id": "1257031a-9c82-43ab-b4e5-1c8749f9dd94",
                    "name": "Frontiers in Marine Science",
                    "type": "journal",
                    "alternate_names": [
                        "Front Mar Sci"
                    ],
                    "issn": "2296-7745",
                    "url": "https://www.frontiersin.org/journals/marine-science",
                    "alternate_urls": [
                        "http://www.frontiersin.org/Marine_Science/archive",
                        "http://www.frontiersin.org/Marine_Science"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0fcf60b828e4bf52a17ae04c8b0d4502dea2ba8b",
                "title": "Fusion of ocean data from multiple sources using deep learning: Utilizing sea temperature as an example",
                "abstract": "For investigating ocean activities and comprehending the role of the oceans in global climate change, it is essential to gather high-quality ocean data. However, existing ocean observation data have deficiencies such as inconsistent spatial and temporal distribution, severe fragmentation, and restricted observation depth layers. Data assimilation is computationally intensive, and other conventional data fusion techniques offer poor fusion precision. This research proposes a novel multi-source ocean data fusion network (ODF-Net) based on deep learning as a solution for these issues. The ODF-Net comprises a number of one-dimensional residual blocks that can rapidly fuse conventional observations, satellite observations, and three-dimensional model output and reanalysis data. The model utilizes vertical ocean profile data as target constraints, integrating physics-based prior knowledge to improve the precision of the fusion. The network structure contains channel and spatial attention mechanisms that guide the network model\u2019s attention to the most crucial features, hence enhancing model performance and interpretability. Comparing multiple global sea temperature datasets reveals that the ODF-Net achieves the highest accuracy and correlation with observations. To evaluate the feasibility of the proposed method, a global monthly three-dimensional sea temperature dataset with a spatial resolution of 0.25\u00b0\u00d70.25\u00b0 is produced by fusing ocean data from multiple sources from 1994 to 2017. The rationality tests on the fusion dataset show that ODF-Net is reliable for integrating ocean data from various sources.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108937296",
                        "name": "Mingqing Wang"
                    },
                    {
                        "authorId": "2204613700",
                        "name": "Danni Wang"
                    },
                    {
                        "authorId": "2036280391",
                        "name": "Yanfei Xiang"
                    },
                    {
                        "authorId": "7668619",
                        "name": "Yishuang Liang"
                    },
                    {
                        "authorId": "144304984",
                        "name": "Rui Xia"
                    },
                    {
                        "authorId": "2004661587",
                        "name": "Jinkun Yang"
                    },
                    {
                        "authorId": "48101471",
                        "name": "Fanghua Xu"
                    },
                    {
                        "authorId": "2145054325",
                        "name": "Xiaomeng Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-13379",
                    "ArXiv": "2301.13379",
                    "DOI": "10.48550/arXiv.2301.13379",
                    "CorpusId": 256416127
                },
                "corpusId": 256416127,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",
                "title": "Faithful Chain-of-Thought Reasoning",
                "abstract": "While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1904906987",
                        "name": "QING LYU"
                    },
                    {
                        "authorId": "151207988",
                        "name": "Shreya Havaldar"
                    },
                    {
                        "authorId": "2161714960",
                        "name": "Adam Stein"
                    },
                    {
                        "authorId": "72436283",
                        "name": "Li Zhang"
                    },
                    {
                        "authorId": "48810734",
                        "name": "D. Rao"
                    },
                    {
                        "authorId": "2053678328",
                        "name": "Eric Wong"
                    },
                    {
                        "authorId": "2817917",
                        "name": "Marianna Apidianaki"
                    },
                    {
                        "authorId": "1763608",
                        "name": "Chris Callison-Burch"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d0b6c5820ae12bb42a82f5f56de37a70c8b3b98a",
                "externalIds": {
                    "DBLP": "journals/make/ClementKAA23",
                    "DOI": "10.3390/make5010006",
                    "CorpusId": 255902137
                },
                "corpusId": 255902137,
                "publicationVenue": {
                    "id": "472fe64a-ea91-4506-8d2d-4c9a9374e1ea",
                    "name": "Machine Learning and Knowledge Extraction",
                    "alternate_names": [
                        "Mach Learn Knowl Extr"
                    ],
                    "issn": "2504-4990",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1327032",
                    "alternate_urls": [
                        "https://nbn-resolving.org/urn/resolver.pl?urn=urn:nbn:ch:bel-1327032",
                        "https://www.mdpi.com/journal/make"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d0b6c5820ae12bb42a82f5f56de37a70c8b3b98a",
                "title": "XAIR: A Systematic Metareview of Explainable AI (XAI) Aligned to the Software Development Process",
                "abstract": "Currently, explainability represents a major barrier that Artificial Intelligence (AI) is facing in regard to its practical implementation in various application domains. To combat the lack of understanding of AI-based systems, Explainable AI (XAI) aims to make black-box AI models more transparent and comprehensible for humans. Fortunately, plenty of XAI methods have been introduced to tackle the explainability problem from different perspectives. However, due to the vast search space, it is challenging for ML practitioners and data scientists to start with the development of XAI software and to optimally select the most suitable XAI methods. To tackle this challenge, we introduce XAIR, a novel systematic metareview of the most promising XAI methods and tools. XAIR differentiates itself from existing reviews by aligning its results to the five steps of the software development process, including requirement analysis, design, implementation, evaluation, and deployment. Through this mapping, we aim to create a better understanding of the individual steps of developing XAI software and to foster the creation of real-world AI applications that incorporate explainability. Finally, we conclude with highlighting new directions for future research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2201191388",
                        "name": "Tobias Clement"
                    },
                    {
                        "authorId": "2201191857",
                        "name": "Nils Kemmerzell"
                    },
                    {
                        "authorId": "2201184990",
                        "name": "Mohamed Abdelaal"
                    },
                    {
                        "authorId": "1774815",
                        "name": "M. Amberg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, using attention mechanisms may highlight associations entirely unrelated to a model\u2019s output [44]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "99e5dbdd7f6ad76f1caaf00ced29000d0fd886ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-07508",
                    "ArXiv": "2212.07508",
                    "DOI": "10.1109/SaTML54575.2023.00049",
                    "CorpusId": 254223860
                },
                "corpusId": 254223860,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/99e5dbdd7f6ad76f1caaf00ced29000d0fd886ce",
                "title": "Tensions Between the Proxies of Human Values in AI",
                "abstract": "Motivated by mitigating potentially harmful impacts of technologies, the AI community has formulated and accepted mathematical definitions for certain pillars of accountability: e.g. privacy, fairness, and model transparency. Yet, we argue this is fundamentally misguided because these definitions are imperfect, siloed constructions of the human values they hope to proxy, while giving the guise that those values are sufficiently embedded in our technologies. Under popularized methods, tensions arise when practitioners attempt to achieve each pillar of fairness, privacy, and transparency in isolation or simultaneously. In this position paper, we push for redirection. We argue that the AI community needs to consider all the consequences of choosing certain formulations of these pillars-not just the technical incompatibilities, but also the effects within the context of deployment. We point towards sociotechnical research for frameworks for the latter, but push for broader efforts into implementing these in practice.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2158813275",
                        "name": "Teresa Datta"
                    },
                    {
                        "authorId": "1981198",
                        "name": "D. Nissani"
                    },
                    {
                        "authorId": "2197067317",
                        "name": "Max Cembalest"
                    },
                    {
                        "authorId": "29655022",
                        "name": "Akash Khanna"
                    },
                    {
                        "authorId": "1436024527",
                        "name": "Haley Massa"
                    },
                    {
                        "authorId": "1718974",
                        "name": "John P. Dickerson"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "733db09d456061b82b7d67ac07fce084861fe1d7",
                "externalIds": {
                    "DOI": "10.1109/ICFTIC57696.2022.10075196",
                    "CorpusId": 257792307
                },
                "corpusId": 257792307,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/733db09d456061b82b7d67ac07fce084861fe1d7",
                "title": "Research on Black-Box Adversarial Attack Based on Transformer",
                "abstract": "The classic adversarial samples of black-box attacks are all aimed at the models of Convolutional neural networks (CNNs), but they do not perform well on the new recognition networks based on Transformer. In this paper, we propose an adversarial sample generation algorithm based on Vision transformers (VITs)\u2018 self-attention mechanism and patch partition. We noticed that different blocks in the VIT have uneven attention distribution, so we first generated a patch-based attention map and performed threshold segmentation, which is used as a mask to perform data enhancement operations based on patches with high weight and patches with low weight, and then exchanged information between patches to generate adversarial samples. The experiment of simulating black-box attack shows that the adversarial samples generated by the algorithm in this paper have a high success rate in all kinds of models based on Transformer of attacks, and also perform well on CNNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152935477",
                        "name": "Wenjuan Zheng"
                    },
                    {
                        "authorId": "2212704680",
                        "name": "Hanxu Luo"
                    },
                    {
                        "authorId": "2212679859",
                        "name": "YuXin Ji"
                    },
                    {
                        "authorId": "2214325876",
                        "name": "Haipeng Li"
                    },
                    {
                        "authorId": "134291036",
                        "name": "Longjian Cong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[28] use explanation-supervised models to substitute human participants in artificial simulatability studies to assess the quality of explanations.",
                "Recently, there has been promising progress on the topic of explanation supervision in the domains of image processing [19,29,2] and natural language processing [10,28,37]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f1069c804886a294b1bc50a925318330dccf6d54",
                "externalIds": {
                    "ArXiv": "2211.13236",
                    "DBLP": "journals/corr/abs-2211-13236",
                    "DOI": "10.48550/arXiv.2211.13236",
                    "CorpusId": 254018143
                },
                "corpusId": 254018143,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f1069c804886a294b1bc50a925318330dccf6d54",
                "title": "MEGAN: Multi-Explanation Graph Attention Network",
                "abstract": "We propose a multi-explanation graph attention network (MEGAN). Unlike existing graph explainability methods, our network can produce node and edge attributional explanations along multiple channels, the number of which is independent of task specifications. This proves crucial to improve the interpretability of graph regression predictions, as explanations can be split into positive and negative evidence w.r.t to a reference value. Additionally, our attention-based network is fully differentiable and explanations can actively be trained in an explanation-supervised manner. We first validate our model on a synthetic graph regression dataset with known ground-truth explanations. Our network outperforms existing baseline explainability methods for the single- as well as the multi-explanation case, achieving near-perfect explanation accuracy during explanation supervision. Finally, we demonstrate our model's capabilities on multiple real-world datasets. We find that our model produces sparse high-fidelity explanations consistent with human intuition about those tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149076354",
                        "name": "Jonas Teufel"
                    },
                    {
                        "authorId": "12366131",
                        "name": "Luca Torresi"
                    },
                    {
                        "authorId": "47591654",
                        "name": "Patrick Reiser"
                    },
                    {
                        "authorId": "35323511",
                        "name": "Pascal Friederich"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The use of attention weights for explaining NLP models has been extensively debated and the general conclusion seems to point to the negative side (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020; Bastings and Filippova 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a5e787c54ef97d40744f9a95d41f7ce8ccae05cd",
                "externalIds": {
                    "DBLP": "conf/ijcai/Xie0CZ23",
                    "ArXiv": "2211.03064",
                    "DOI": "10.48550/arXiv.2211.03064",
                    "CorpusId": 253383736
                },
                "corpusId": 253383736,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a5e787c54ef97d40744f9a95d41f7ce8ccae05cd",
                "title": "ViT-CX: Causal Explanation of Vision Transformers",
                "abstract": "Despite the popularity of Vision Transformers (ViTs) and eXplainable AI (XAI), only a few explanation methods have been designed specially for ViTs thus far. They mostly use attention weights of the [CLS] token on patch embeddings and often produce unsatisfactory saliency maps. This paper proposes a novel method for explaining ViTs called ViT-CX. It is based on patch embeddings, rather than attentions paid to them, and their causal impacts on the model output. Other characteristics of ViTs such as causal overdetermination are considered in the design of ViT-CX. The empirical results show that ViT-CX produces more meaningful saliency maps and does a better job revealing all important evidence for the predictions than previous methods. The explanation generated by ViT-CX also shows significantly better faithfulness to the model. The codes and appendix are available at https://github.com/vaynexie/CausalX-ViT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "4868695",
                        "name": "Weiyan Xie"
                    },
                    {
                        "authorId": "2118890136",
                        "name": "Xiao-hui Li"
                    },
                    {
                        "authorId": "3151540",
                        "name": "Caleb Chen Cao"
                    },
                    {
                        "authorId": "2190105006",
                        "name": "Nevin L.Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Incorporating prior knowledge through selective attention is widely explored in natural language processing, especially in\nrecent NLP models with attention mechanism (Lin et al., 2016; Sukhbaatar et al., 2019; Pruthi et al., 2020; Beltagy et al., 2020; Wang et al., 2022)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6788a8a0148421cf482cb8caa23504ee1e36798e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-12330",
                    "ACL": "2022.emnlp-main.409",
                    "ArXiv": "2210.12330",
                    "DOI": "10.48550/arXiv.2210.12330",
                    "CorpusId": 253098395
                },
                "corpusId": 253098395,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/6788a8a0148421cf482cb8caa23504ee1e36798e",
                "title": "Salience Allocation as Guidance for Abstractive Summarization",
                "abstract": "Abstractive summarization models typically learn to capture the salient information from scratch implicitly.Recent literature adds extractive summaries as guidance for abstractive summarization models to provide hints of salient content and achieves better performance.However, extractive summaries as guidance could be over strict, leading to information loss or noisy signals.Furthermore, it cannot easily adapt to documents with various abstractiveness.As the number and allocation of salience content pieces varies, it is hard to find a fixed threshold deciding which content should be included in the guidance.In this paper, we propose a novel summarization approach with a flexible and reliable salience guidance, namely SEASON (SaliencE Allocation as Guidance for Abstractive SummarizatiON).SEASON utilizes the allocation of salience expectation to guide abstractive summarization and adapts well to articles in different abstractiveness.Automatic and human evaluations on two benchmark datasets show that the proposed method is effective and reliable.Empirical results on more than one million news articles demonstrate a natural fifteen-fifty salience split for news article sentences, providing a useful insight for composing news articles.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47939052",
                        "name": "Fei Wang"
                    },
                    {
                        "authorId": "50982080",
                        "name": "Kaiqiang Song"
                    },
                    {
                        "authorId": "49723569",
                        "name": "Hongming Zhang"
                    },
                    {
                        "authorId": "2936180",
                        "name": "Lifeng Jin"
                    },
                    {
                        "authorId": "2173531",
                        "name": "Sangwoo Cho"
                    },
                    {
                        "authorId": "2087264100",
                        "name": "Wenlin Yao"
                    },
                    {
                        "authorId": "48631781",
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "authorId": "1998918",
                        "name": "Muhao Chen"
                    },
                    {
                        "authorId": "144580027",
                        "name": "Dong Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "all modalities and textual data in particular (Pruthi et al., 2020; Ribeiro et al., 2016; Lundberg and Lee, 2017).",
                "ExAI is witnessing endeavors in\nPapers accepted at Findings of Empirical Methods in Natural Language Processing (EMNLP), 2022.\nall modalities and textual data in particular (Pruthi et al., 2020; Ribeiro et al., 2016; Lundberg and Lee, 2017)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3426e5413932631e5260490ce686ebf303a961e4",
                "externalIds": {
                    "DBLP": "conf/emnlp/ZiniA22",
                    "ArXiv": "2210.08902",
                    "DOI": "10.48550/arXiv.2210.08902",
                    "CorpusId": 252917738
                },
                "corpusId": 252917738,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/3426e5413932631e5260490ce686ebf303a961e4",
                "title": "Beyond Model Interpretability: On the Faithfulness and Adversarial Robustness of Contrastive Textual Explanations",
                "abstract": "Contrastive explanation methods go beyond transparency and address the contrastive aspect of explanations. Such explanations are emerging as an attractive option to provide actionable change to scenarios adversely impacted by classifiers' decisions. However, their extension to textual data is under-explored and there is little investigation on their vulnerabilities and limitations. This work motivates textual counterfactuals by laying the ground for a novel evaluation scheme inspired by the faithfulness of explanations. Accordingly, we extend the computation of three metrics, proximity,connectedness and stability, to textual data and we benchmark two successful contrastive methods, POLYJUICE and MiCE, on our suggested metrics. Experiments on sentiment analysis data show that the connectedness of counterfactuals to their original counterparts is not obvious in both models. More interestingly, the generated contrastive texts are more attainable with POLYJUICE which highlights the significance of latent representations in counterfactual search. Finally, we perform the first semantic adversarial attack on textual recourse methods. The results demonstrate the robustness of POLYJUICE and the role that latent input representations play in robustness and reliability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8160667",
                        "name": "Julia El Zini"
                    },
                    {
                        "authorId": "144707373",
                        "name": "M. Awad"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2014) are widely used as an interpretability tool, but these attention maps are often unfaithful and unreliable (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020).",
                "Explainable NLP Heat maps generated from attention values from the models (Bahdanau et al., 2014) are widely used as an interpretability tool, but these attention maps are often unfaithful and unreliable (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1b1b6e429c022fecb524f755e3cf36961566bcb7",
                "externalIds": {
                    "DBLP": "conf/eacl/HayatiPRUK23",
                    "ACL": "2023.eacl-main.208",
                    "ArXiv": "2210.07469",
                    "DOI": "10.18653/v1/2023.eacl-main.208",
                    "CorpusId": 258170532
                },
                "corpusId": 258170532,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/1b1b6e429c022fecb524f755e3cf36961566bcb7",
                "title": "StyLEx: Explaining Style Using Human Lexical Annotations",
                "abstract": "Large pre-trained language models have achieved impressive results on various style classification tasks, but they often learn spurious domain-specific words to make predictions (Hayati et al., 2021). While human explanation highlights stylistic tokens as important features for this task, we observe that model explanations often do not align with them. To tackle this issue, we introduce StyLEx, a model that learns from human annotated explanations of stylistic features and jointly learns to perform the task and predict these features as model explanations. Our experiments show that StyLEx can provide human like stylistic lexical explanations without sacrificing the performance of sentence-level style prediction on both in-domain and out-of-domain datasets. Explanations from StyLEx show significant improvements in explanation metrics (sufficiency, plausibility) and when evaluated with human annotations. They are also more understandable by human judges compared to the widely-used saliency-based explanation baseline.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31998283",
                        "name": "Shirley Anugrah Hayati"
                    },
                    {
                        "authorId": "2152042873",
                        "name": "Kyumin Park"
                    },
                    {
                        "authorId": "1801149",
                        "name": "Dheeraj Rajagopal"
                    },
                    {
                        "authorId": "1717822",
                        "name": "Lyle Ungar"
                    },
                    {
                        "authorId": "48493368",
                        "name": "Dongyeop Kang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While our study design does not explicitly account for this, even if perceptions vary at the instance level, our findings suggest that reliance would depend on the inclusion of sensitive features, which research has shown to be an unreliable signal for assessing algorithmic fairness [4, 26, 50, 54, 67, 73, 79].",
                ", through adversarial attacks on explanation methods [24, 54, 79, 95].",
                ", the exclusion of information that is evidently indicative of a person\u2019s demographics, is neither necessary nor sufficient for an algorithm to be procedurally fair [54, 67, 79] or to not display bias in terms of distributive fairness [4, 26, 50, 73]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "642e8614e81b95ff9f829965c1a0cf5ba09e3ead",
                "externalIds": {
                    "ArXiv": "2209.11812",
                    "DBLP": "journals/corr/abs-2209-11812",
                    "DOI": "10.48550/arXiv.2209.11812",
                    "CorpusId": 252531872
                },
                "corpusId": 252531872,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/642e8614e81b95ff9f829965c1a0cf5ba09e3ead",
                "title": "On Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making",
                "abstract": "Proponents of explainable AI have often argued that it constitutes an essential path towards algorithmic fairness. Prior works examining these claims have primarily evaluated explanations based on their effects on humans' perceptions, but there is scant research on the relationship between explanations and distributive fairness of AI-assisted decisions. In this paper, we conduct an empirical study to examine the relationship between feature-based explanations and distributive fairness, mediated by human perceptions and reliance on AI recommendations. Our findings show that explanations influence fairness perceptions, which, in turn, relate to humans' tendency to adhere to AI recommendations. However, our findings suggest that such explanations do not enable humans to discern correct and wrong AI recommendations. Instead, we show that they may affect reliance irrespective of the correctness of AI recommendations. Depending on which features an explanation highlights, this can foster or hinder distributive fairness: when explanations highlight features that are task-irrelevant and evidently associated with the sensitive attribute, this prompts overrides that counter stereotype-aligned AI recommendations. Meanwhile, if explanations appear task-relevant, this induces reliance behavior that reinforces stereotype-aligned errors. These results show that feature-based explanations are not a reliable mechanism to improve distributive fairness, as their ability to do so relies on a human-in-the-loop operationalization of the flawed notion of\"fairness through unawareness\". Finally, our study design provides a blueprint to evaluate the suitability of other explanations as pathways towards improved distributive fairness of AI-assisted decisions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2096373936",
                        "name": "Jakob Schoeffer"
                    },
                    {
                        "authorId": "1406443102",
                        "name": "Maria De-Arteaga"
                    },
                    {
                        "authorId": "8359709",
                        "name": "N. Kuehl"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pruthi et al. (2020) again refutes argument (b), showing that with a simple training objective, they successfully guide the model to learn intended adversarial attention\ndistributions.",
                "For example, Pruthi et al. (2020) show that it is possible to attention weights can be a deceiving explanation to end-users."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "285d13bf3cbe6a8a0f164f584d84f8b74067271f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-11326",
                    "ArXiv": "2209.11326",
                    "DOI": "10.48550/arXiv.2209.11326",
                    "CorpusId": 252519203
                },
                "corpusId": 252519203,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/285d13bf3cbe6a8a0f164f584d84f8b74067271f",
                "title": "Towards Faithful Model Explanation in NLP: A Survey",
                "abstract": "End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, i.e. an explanation should accurately represent the reasoning process behind the model's prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1904906987",
                        "name": "QING LYU"
                    },
                    {
                        "authorId": "2817917",
                        "name": "Marianna Apidianaki"
                    },
                    {
                        "authorId": "1763608",
                        "name": "Chris Callison-Burch"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This suggests that although attention signifies the importance of the input modalities, it alone may not be enough for predicting DTAs or potential mechanism of action [36], [37]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0786e2ea7df789969342ff132e16026f11f249cb",
                "externalIds": {
                    "DBLP": "conf/bibm/YellaGJ22",
                    "DOI": "10.1109/BIBM55620.2022.9995245",
                    "CorpusId": 252092140
                },
                "corpusId": 252092140,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0786e2ea7df789969342ff132e16026f11f249cb",
                "title": "GraMDTA: Multimodal Graph Neural Networks for Predicting Drug-Target Associations",
                "abstract": "Finding novel drug-target associations is vital for drug discovery. However, screening millions of small molecules for a select target protein is challenging. Several computational approaches have been developed in the past using Machine learning methods for computational drug-target association (DTA) prediction predominantly use structural data of drugs and proteins. Some of these approaches use knowledge graph networks and link prediction. To the best of our knowledge there have been no approaches that use both structural learning that offers molecular-based representations and knowledge graph-based learning which offers interaction-based representations for DTA discovery. Based on the premise that multimodal sources of information acting complimentarily could improve the robustness of DTA predictions, we developed GraMDTA, a multimodal graph neural network that learns both structural and knowledge graph representations utilizing multi-head attention to fuse the multimodal representations. We compare GraMDTA with other computational approaches for DTA prediction to demonstrate the power of multimodal fusion for discovery of DTA.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27088203",
                        "name": "Jaswanth K. Yella"
                    },
                    {
                        "authorId": "51318824",
                        "name": "S. Ghandikota"
                    },
                    {
                        "authorId": "1717648",
                        "name": "A. Jegga"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018, 2019), a lot of work questions attention explainability (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019; Serrano and Smith, 2019; Bastings and Filippova, 2020; Pruthi et al., 2020).",
                "\u2026is not reliable: although there is evidence that attention can play recognizable roles (Voita et al., 2018, 2019), a lot of work questions attention explainability (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019; Serrano and Smith, 2019; Bastings and Filippova, 2020; Pruthi et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "193fc3bc9f200ccb28892c02979e6c2068a85138",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-05309",
                    "ACL": "2023.eacl-main.75",
                    "ArXiv": "2208.05309",
                    "DOI": "10.48550/arXiv.2208.05309",
                    "CorpusId": 251468136
                },
                "corpusId": 251468136,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/193fc3bc9f200ccb28892c02979e6c2068a85138",
                "title": "Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation",
                "abstract": "Although the problem of hallucinations in neural machine translation (NMT) has received some attention, research on this highly pathological phenomenon lacks solid ground. Previous work has been limited in several ways: it often resorts to artificial settings where the problem is amplified, it disregards some (common) types of hallucinations, and it does not validate adequacy of detection heuristics. In this paper, we set foundations for the study of NMT hallucinations. First, we work in a natural setting, i.e., in-domain data without artificial noise neither in training nor in inference. Next, we annotate a dataset of over 3.4k sentences indicating different kinds of critical errors and hallucinations. Then, we turn to detection methods and both revisit methods used previously and propose using glass-box uncertainty-based detectors. Overall, we show that for preventive settings, (i) previously used methods are largely inadequate, (ii) sequence log-probability works best and performs on par with reference-based methods. Finally, we propose DeHallucinator, a simple method for alleviating hallucinations at test time that significantly reduces the hallucinatory rate.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144726818",
                        "name": "Nuno M. Guerreiro"
                    },
                    {
                        "authorId": "46235299",
                        "name": "Elena Voita"
                    },
                    {
                        "authorId": "145644643",
                        "name": "Andr\u00e9 F. T. Martins"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[83] manipulate the attention weights to whitewash problematic tokens in explanations that afect the model fairness or accountability.",
                "[91] (2015) Weights visualization on recognizing textual entailement X Dominance of the last output vector over attention in some cases [27] (2017) Layer-wise relevance propagation (LRP) [6] X Importance of LRP to further interpret the attention weights and the internal workings of transformers [116] (2019) Alignment between syntactic dependency and attention through visualization and aggregation X (1) Disproportionality between heads targeting POS, (2) Capturing of longer-distance relationships by deeper layers and (3) Moderate correlation between distance and entropy of attention [47] (2019) Correlation and counterfactuals X No frequent correlation between attention weights and gradient-based measures of feature importance [13] (2019) Aggregation of context into hidden tokens X Preservation of the token identiiability throughout the model and decrease of information identiiability with depth [83] (2020) Diminishing attention weights of impermissible tokens X Attention-based explanations can be deceived especially within the fairness context"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8af69a903a14657ec96b93c4b6e139771beec106",
                "externalIds": {
                    "ArXiv": "2210.06929",
                    "DBLP": "journals/corr/abs-2210-06929",
                    "DOI": "10.1145/3529755",
                    "CorpusId": 250624746
                },
                "corpusId": 250624746,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8af69a903a14657ec96b93c4b6e139771beec106",
                "title": "On the Explainability of Natural Language Processing Deep Models",
                "abstract": "Despite their success, deep networks are used as black-box models with outputs that are not easily explainable during the learning and the prediction phases. This lack of interpretability is significantly limiting the adoption of such models in domains where decisions are critical such as the medical and legal fields. Recently, researchers have been interested in developing methods that help explain individual decisions and decipher the hidden representations of machine learning models in general and deep networks specifically. While there has been a recent explosion of work on Explainable Artificial Intelligence (ExAI) on deep models that operate on imagery and tabular data, textual datasets present new challenges to the ExAI community. Such challenges can be attributed to the lack of input structure in textual data, the use of word embeddings that add to the opacity of the models and the difficulty of the visualization of the inner workings of deep models when they are trained on textual data. Lately, methods have been developed to address the aforementioned challenges and present satisfactory explanations on Natural Language Processing (NLP) models. However, such methods are yet to be studied in a comprehensive framework where common challenges are properly stated and rigorous evaluation practices and metrics are proposed. Motivated to democratize ExAI methods in the NLP field, we present in this work a survey that studies model-agnostic as well as model-specific explainability methods on NLP models. Such methods can either develop inherently interpretable NLP models or operate on pre-trained models in a post hoc manner. We make this distinction and we further decompose the methods into three categories according to what they explain: (1) word embeddings (input level), (2) inner workings of NLP models (processing level), and (3) models\u2019 decisions (output level). We also detail the different evaluation approaches interpretability methods in the NLP field. Finally, we present a case-study on the well-known neural machine translation in an appendix, and we propose promising future research directions for ExAI in the NLP field.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8160667",
                        "name": "Julia El Zini"
                    },
                    {
                        "authorId": "144707373",
                        "name": "M. Awad"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, attention is only a small part of the overall computation and can be easily manipulated to hide model biases [56]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b680f288e2bbebe5fb5c64552e1e009f93ff2377",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-00938",
                    "ArXiv": "2207.00938",
                    "DOI": "10.1109/TPAMI.2022.3225162",
                    "CorpusId": 250264698,
                    "PubMed": "36441893"
                },
                "corpusId": 250264698,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b680f288e2bbebe5fb5c64552e1e009f93ff2377",
                "title": "Interpretable by Design: Learning Predictors by Composing Interpretable Queries",
                "abstract": "There is a growing concern about typically opaque decision-making with high-performance machine learning algorithms. Providing an explanation of the reasoning process in domain-specific terms can be crucial for adoption in risk-sensitive domains such as healthcare. We argue that machine learning algorithms should be interpretable by design and that the language in which these interpretations are expressed should be domain- and task-dependent. Consequently, we base our model's prediction on a family of user-defined and task-specific binary functions of the data, each having a clear interpretation to the end-user. We then minimize the expected number of queries needed for accurate prediction on any given input. As the solution is generally intractable, following prior work, we choose the queries sequentially based on information gain. However, in contrast to previous work, we need not assume the queries are conditionally independent. Instead, we leverage a stochastic generative model (VAE) and an MCMC algorithm (Unadjusted Langevin) to select the most informative query about the input based on previous query-answers. This enables the online determination of a query chain of whatever depth is required to resolve prediction ambiguities. Finally, experiments on vision and NLP tasks demonstrate the efficacy of our approach and its superiority over post-hoc explanations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2054168194",
                        "name": "Aditya Chattopadhyay"
                    },
                    {
                        "authorId": "1382117611",
                        "name": "Stewart Slocum"
                    },
                    {
                        "authorId": "2663366",
                        "name": "B. Haeffele"
                    },
                    {
                        "authorId": "2151896230",
                        "name": "Ren\u00e9 Vidal"
                    },
                    {
                        "authorId": "1707642",
                        "name": "D. Geman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f10cb334949346fe3b51158bc7f3c25f1b625b92",
                "externalIds": {
                    "DBLP": "journals/linguamatica/EnguixGMSV22",
                    "DOI": "10.21814/lm.14.1.342",
                    "CorpusId": 250464443
                },
                "corpusId": 250464443,
                "publicationVenue": {
                    "id": "4a53ffb1-aae4-4e97-b4c7-2bb8de0c22ea",
                    "name": "Linguam\u00e1tica",
                    "type": "journal",
                    "alternate_names": [
                        "Linguama\u0301tica"
                    ],
                    "issn": "1647-0818",
                    "url": "http://linguamatica.com/",
                    "alternate_urls": [
                        "https://www.linguamatica.com/index.php/linguamatica/index",
                        "https://www.linguamatica.com/index.php/linguamatica",
                        "https://dialnet.unirioja.es/servlet/revista?codigo=24287"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f10cb334949346fe3b51158bc7f3c25f1b625b92",
                "title": "La #felicidad en Twitter: \u00bfqu\u00e9 representa realmente?",
                "abstract": "Existe un gran n\u00famero de trabajos que tienen por objeto la clasificaci\u00f3n de diversos tipos de documentos, desde textos literarios hasta interacciones informales en redes sociales como Twitter, de acuerdo a los sentimientos que pretenden evocar. Se pueden realizar clasificaciones muy variadas con base en los sentimientos que el autor considere. El objetivo de este art\u00edculo es clasificar una recopilaci\u00f3n de tuits en diferentes contextos en los que la palabra \"feliz\" o \"felicidad\" se pueden emplear; por ejemplo publicidad, felicitaciones o como un simple sarcasmo. Para esto se har\u00e1 uso de sistemas de aprendizaje supervisado y se emplear\u00e1n varios m\u00e9todos de procesamiento de lenguaje natural como tokenizaci\u00f3n, identificaci\u00f3n de palabras funcionales y n-gramas.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2137228023",
                        "name": "Gemma Bel Enguix"
                    },
                    {
                        "authorId": "1403763319",
                        "name": "Helena G\u00f3mez-Adorno"
                    },
                    {
                        "authorId": "120494681",
                        "name": "K. Mendoza"
                    },
                    {
                        "authorId": "89155972",
                        "name": "Grigori Sidorov"
                    },
                    {
                        "authorId": "152943254",
                        "name": "J. V\u00e1squez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Reliability of visualizations: There has been recent work examining the reliability of model interpretability methods for real-world practitioners (Pruthi et al., 2020; Srinivas and Fleuret, 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "76e53e20cf56f2ed38ed2a25fb61896d9e6e01a1",
                "externalIds": {
                    "ArXiv": "2207.00056",
                    "DBLP": "conf/iclr/LiangLCJD0MS23",
                    "CorpusId": 257039090
                },
                "corpusId": 257039090,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/76e53e20cf56f2ed38ed2a25fb61896d9e6e01a1",
                "title": "MultiViz: Towards Visualizing and Understanding Multimodal Models",
                "abstract": "The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing MultiViz, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. MultiViz is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "28130078",
                        "name": "P. Liang"
                    },
                    {
                        "authorId": "2066413750",
                        "name": "Yiwei Lyu"
                    },
                    {
                        "authorId": "1509809381",
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "authorId": "2146677401",
                        "name": "Nihal Jain"
                    },
                    {
                        "authorId": "4692365",
                        "name": "Zihao Deng"
                    },
                    {
                        "authorId": "50141732",
                        "name": "Xingbo Wang"
                    },
                    {
                        "authorId": "49933077",
                        "name": "Louis-Philippe Morency"
                    },
                    {
                        "authorId": "145124475",
                        "name": "R. Salakhutdinov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although its use as an interpretability method has been criticized (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020), using also the Euclidean norms of the vectors computed across each attention head (Kobayashi et al. 2020), the attention vector-norms method from now on, has been\u2026",
                "Although its use as an interpretability method has been criticized (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020), using also the Euclidean norms of the vectors computed across each attention head (Kobayashi et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "948c9489473b3bb149ed6ae198eafad3c609c867",
                "externalIds": {
                    "DBLP": "conf/aaai/Costa-jussaEBFB22",
                    "DOI": "10.1609/aaai.v36i11.21442",
                    "CorpusId": 250301132
                },
                "corpusId": 250301132,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/948c9489473b3bb149ed6ae198eafad3c609c867",
                "title": "Interpreting Gender Bias in Neural Machine Translation: Multilingual Architecture Matters",
                "abstract": "Multilingual neural machine translation architectures mainly differ in the number of sharing modules and parameters applied among languages. In this paper, and from an algorithmic perspective, we explore whether the chosen architecture, when trained with the same data, influences the level of gender bias. Experiments conducted in three language pairs show that language-specific encoder-decoders exhibit less bias than the shared architecture. We propose two methods for interpreting and studying gender bias in machine translation based on source embeddings and attention. Our analysis shows that, in the language-specific case, the embeddings encode more gender information, and their attention is more diverted. Both behaviors help in mitigating gender bias.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1398996347",
                        "name": "M. Costa-juss\u00e0"
                    },
                    {
                        "authorId": "144483761",
                        "name": "Carlos Escolano"
                    },
                    {
                        "authorId": "73312674",
                        "name": "Christine Basta"
                    },
                    {
                        "authorId": "1751450782",
                        "name": "Javier Ferrando"
                    },
                    {
                        "authorId": "2097889057",
                        "name": "Roser Batlle Roca"
                    },
                    {
                        "authorId": "71533359",
                        "name": "Ksenia Kharitonova"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Lastly, our approach provides some quantitative argument for the validity of attention-based studies (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020) and expands on earlier works looking beyond attention weights (Kobayashi et al."
            ],
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "728fff6344f9ce65fafcbf3b9aea4f0eb908d44d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-03529",
                    "ArXiv": "2206.03529",
                    "DOI": "10.1162/tacl_a_00501",
                    "CorpusId": 249461519
                },
                "corpusId": 249461519,
                "publicationVenue": {
                    "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
                    "name": "Transactions of the Association for Computational Linguistics",
                    "type": "journal",
                    "alternate_names": [
                        "Trans Assoc Comput Linguistics",
                        "TACL"
                    ],
                    "issn": "2307-387X",
                    "url": "https://www.mitpressjournals.org/loi/tacl",
                    "alternate_urls": [
                        "http://www.transacl.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/728fff6344f9ce65fafcbf3b9aea4f0eb908d44d",
                "title": "How to Dissect a Muppet: The Structure of Transformer Embedding Spaces",
                "abstract": "Abstract Pretrained embeddings based on the Transformer architecture have taken the NLP community by storm. We show that they can mathematically be reframed as a sum of vector factors and showcase how to use this reframing to study the impact of each component. We provide evidence that multi-head attentions and feed-forwards are not equally useful in all downstream applications, as well as a quantitative overview of the effects of finetuning on the overall embedding space. This approach allows us to draw connections to a wide range of previous studies, from vector space anisotropy to attention weights.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "122281127",
                        "name": "Timothee Mickus"
                    },
                    {
                        "authorId": "2129425",
                        "name": "Denis Paperno"
                    },
                    {
                        "authorId": "46180911",
                        "name": "Mathieu Constant"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Attention-based DNNs are growing in popularity [6, 37] with attention weights being used as explanations; however, recent works [29, 47, 45] show the limitations of such explanations."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "68e64acad9ff92952f4fe1fb9edd1b70d6bf842b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-02107",
                    "DOI": "10.48550/arXiv.2206.02107",
                    "CorpusId": 249394928
                },
                "corpusId": 249394928,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/68e64acad9ff92952f4fe1fb9edd1b70d6bf842b",
                "title": "Interpretable Mixture of Experts for Structured Data",
                "abstract": "With the growth of machine learning for structured data, the need for reliable model explanations is essential, especially in high-stakes applications. We introduce a novel framework, Interpretable Mixture of Experts (IME)2, that provides interpretability for structured data while preserving accuracy. IME consists of an assignment module and a mixture of interpretable experts such as linear models where each sample is assigned to a single interpretable expert. This results in an inherently-interpretable architecture where the explanations produced by IME are the exact descriptions of how the prediction is computed. In addition to constituting a standalone inherently-interpretable architecture, an additional IME capability is that it can be integrated with existing Deep Neural Networks (DNNs) to offer interpretability to a subset of samples while maintaining the accuracy of the DNNs. Experiments on various structured datasets demonstrate that IME is more accurate than a single interpretable model and performs comparably to existing state-of-the-art deep learning models in terms of accuracy while providing faithful explanations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30151156",
                        "name": "A. Ismail"
                    },
                    {
                        "authorId": "2676352",
                        "name": "Sercan \u00d6. Arik"
                    },
                    {
                        "authorId": "2144029",
                        "name": "Jinsung Yoon"
                    },
                    {
                        "authorId": "40511120",
                        "name": "Ankur Taly"
                    },
                    {
                        "authorId": "34389431",
                        "name": "S. Feizi"
                    },
                    {
                        "authorId": "1945962",
                        "name": "Tomas Pfister"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[32] apply a masked strategy for \u201cdeceiving\u201d, which improves the attention\u2019s reliability."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9813c677e3463769467d228eac80f44b2c722a06",
                "externalIds": {
                    "DBLP": "journals/entropy/LiZWZZ22",
                    "PubMedCentral": "9222507",
                    "DOI": "10.3390/e24060764",
                    "CorpusId": 249186287,
                    "PubMed": "35741485"
                },
                "corpusId": 249186287,
                "publicationVenue": {
                    "id": "8270cfe1-3713-4325-a7bd-c6a87eed889e",
                    "name": "Entropy",
                    "type": "journal",
                    "issn": "1099-4300",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606",
                    "alternate_urls": [
                        "http://www.mdpi.com/journal/entropy/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155606",
                        "https://www.mdpi.com/journal/entropy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9813c677e3463769467d228eac80f44b2c722a06",
                "title": "Inter- and Intra-Modal Contrastive Hybrid Learning Framework for Multimodal Abstractive Summarization",
                "abstract": "Internet users are benefiting from technologies of abstractive summarization enabling them to view articles on the internet by reading article summaries only instead of an entire article. However, there are disadvantages to technologies for analyzing articles with texts and images due to the semantic gap between vision and language. These technologies focus more on aggregating features and neglect the heterogeneity of each modality. At the same time, the lack of consideration of intrinsic data properties within each modality and semantic information from cross-modal correlations result in the poor quality of learned representations. Therefore, we propose a novel Inter- and Intra-modal Contrastive Hybrid learning framework which learns to automatically align the multimodal information and maintains the semantic consistency of input/output flows. Moreover, ITCH can be taken as a component to make the model suitable for both supervised and unsupervised learning approaches. Experiments on two public datasets, MMS and MSMO, show that the ITCH performances are better than the current baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118505714",
                        "name": "Jiangfeng Li"
                    },
                    {
                        "authorId": "2116459561",
                        "name": "Zijian Zhang"
                    },
                    {
                        "authorId": "2153213890",
                        "name": "Bowen Wang"
                    },
                    {
                        "authorId": "1729695",
                        "name": "Qinpei Zhao"
                    },
                    {
                        "authorId": "47424058",
                        "name": "Chenxi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Explanations can be misleading Deriving conclusions from existing findings is further complicated by evidence that explanations can mislead people\u2019s beliefs [6, 25, 33], even in cases where there is no intention to manipulate [12].",
                "[33] manipulate attention-based explanations such that people can be deceived into thinking that a model does not rely on sensitive information (e."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b54830e5166f3e5a4b4a867143ba8b4c7d453fe2",
                "externalIds": {
                    "ArXiv": "2204.13156",
                    "DBLP": "journals/corr/abs-2204-13156",
                    "DOI": "10.48550/arXiv.2204.13156",
                    "CorpusId": 248426948
                },
                "corpusId": 248426948,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b54830e5166f3e5a4b4a867143ba8b4c7d453fe2",
                "title": "On the Relationship Between Explanations, Fairness Perceptions, and Decisions",
                "abstract": "It is known that recommendations of AI-based systems can be incorrect or unfair. Hence, it is often proposed that a human be the final decision-maker. Prior work has argued that explanations are an essential pathway to help human decision-makers enhance decision quality and mitigate bias, i.e., facilitate human-AI complementarity. For these benefits to materialize, explanations should enable humans to appropriately rely on AI recommendations and override the algorithmic recommendation when necessary to increase distributive fairness of decisions. The literature, however, does not provide conclusive empirical evidence as to whether explanations enable such complementarity in practice. In this work, we (a) provide a conceptual framework to articulate the relationships between explanations, fairness perceptions, reliance, and distributive fairness, (b) apply it to understand (seemingly) contradictory research findings at the intersection of explanations and fairness, and (c) derive cohesive implications for the formulation of research questions and the design of experiments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2096373936",
                        "name": "Jakob Schoeffer"
                    },
                    {
                        "authorId": "1406443102",
                        "name": "Maria De-Arteaga"
                    },
                    {
                        "authorId": "8359709",
                        "name": "N. Kuehl"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While there have been recent advances in explaining neural network predictions [98], researchers have also demonstrated the ability to fool attention-based interpretation techniques [144]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "61d9162eea5aceb5b78c2d1230f18d8dfe10a208",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-09852",
                    "ArXiv": "2204.09852",
                    "DOI": "10.48550/arXiv.2204.09852",
                    "CorpusId": 248299822
                },
                "corpusId": 248299822,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/61d9162eea5aceb5b78c2d1230f18d8dfe10a208",
                "title": "The Risks of Machine Learning Systems",
                "abstract": "The speed and scale at which machine learning (ML) systems are deployed are accelerating even as an increasing number of studies highlight their potential for negative impact. There is a clear need for companies and regulators to manage the risk from proposed ML systems before they harm people. To achieve this, private and public sector actors first need to identify the risks posed by a proposed ML system. A system's overall risk is influenced by its direct and indirect effects. However, existing frameworks for ML risk/impact assessment often address an abstract notion of risk or do not concretize this dependence. We propose to address this gap with a context-sensitive framework for identifying ML system risks comprising two components: a taxonomy of the first- and second-order risks posed by ML systems, and their contributing factors. First-order risks stem from aspects of the ML system, while second-order risks stem from the consequences of first-order risks. These consequences are system failures that result from design and development choices. We explore how different risks may manifest in various types of ML systems, the factors that affect each risk, and how first-order risks may lead to second-order effects when the system interacts with the real world. Throughout the paper, we show how real events and prior research fit into our Machine Learning System Risk framework (MLSR). MLSR operates on ML systems rather than technologies or domains, recognizing that a system's design, implementation, and use case all contribute to its risk. In doing so, it unifies the risks that are commonly discussed in the ethical AI community (e.g., ethical/human rights risks) with system-level risks (e.g., application, design, control risks), paving the way for holistic risk assessments of ML systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110333511",
                        "name": "Samson Tan"
                    },
                    {
                        "authorId": "3299973",
                        "name": "Araz Taeihagh"
                    },
                    {
                        "authorId": "48162805",
                        "name": "K. Baxter"
                    }
                ]
            }
        },
        {
            "contexts": [
                "erefore, we set penalties to suppress the corresponding attention weight [29]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e6ff21292a3e73d5f6d404fdaaa33d6740ac25c5",
                "externalIds": {
                    "PubMedCentral": "9020900",
                    "DOI": "10.1155/2022/2419987",
                    "CorpusId": 248174168,
                    "PubMed": "35463264"
                },
                "corpusId": 248174168,
                "publicationVenue": {
                    "id": "f32b7322-b69c-4e63-801d-8f50784ef778",
                    "name": "Computational Intelligence and Neuroscience",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Intell Neurosci"
                    ],
                    "issn": "1687-5265",
                    "url": "https://www.hindawi.com/journals/cin/"
                },
                "url": "https://www.semanticscholar.org/paper/e6ff21292a3e73d5f6d404fdaaa33d6740ac25c5",
                "title": "Personality Privacy Protection Method of Social Users Based on Generative Adversarial Networks",
                "abstract": "Obscuring or otherwise minimizing the release of personality information from potential victims of social engineering attacks effectively interferes with an attacker's personality analysis and reduces the success rate of social engineering attacks. We propose a text transformation method named PerTransGAN using generative adversarial networks (GANs) to protect the personality privacy hidden in text data. Making use of reinforcement learning, we use the output of the discriminator as a reward signal to guide the training of the generator. Moreover, the model extracts text features from the discriminator network as additional semantic guidance signals. And the loss function of the generator adds a penalty item to reduce the weight of words that contribute more to personality information in the real text so as to hide the user's personality privacy. In addition, the semantic and personality modules are designed to calculate the semantic similarity and personality distribution distance between the real text and the generated text as a part of the objective function. Experiments show that the self-attention module and semantic module in the generator improved the content retention of the text by 0.11 compared with the baseline model and obtained the highest BLEU score. In addition, with the addition of penalty item and personality module, compared with the classification accuracy of the original data, the accuracy of the generated text in the personality classifier decreased by 20%. PerTransGAN model preserves users' personality privacy as found in user data by transforming the text and preserving semantic similarity while blocking privacy theft by attackers.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2054338157",
                        "name": "Yi Sui"
                    },
                    {
                        "authorId": "1638299077",
                        "name": "Xiujuan Wang"
                    },
                    {
                        "authorId": "3156914",
                        "name": "K. Zheng"
                    },
                    {
                        "authorId": "2152862055",
                        "name": "Yutong Shi"
                    },
                    {
                        "authorId": "2072594311",
                        "name": "Siwei Cao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "254440d043cea61a2c6106c80aed7ddf5918c086",
                "externalIds": {
                    "ACL": "2022.acl-long.213",
                    "DBLP": "journals/corr/abs-2204-05426",
                    "ArXiv": "2204.05426",
                    "DOI": "10.48550/arXiv.2204.05426",
                    "CorpusId": 248118589
                },
                "corpusId": 248118589,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/254440d043cea61a2c6106c80aed7ddf5918c086",
                "title": "ProtoTEx: Explaining Model Decisions with Prototype Tensors",
                "abstract": "We present ProtoTEx, a novel white-box NLP classification architecture based on prototype networks (Li et al., 2018). ProtoTEx faithfully explains model decisions based on prototype tensors that encode latent clusters of training examples. At inference time, classification decisions are based on the distances between the input text and the prototype tensors, explained via the training examples most similar to the most influential prototypes. We also describe a novel interleaved training algorithm that effectively handles classes characterized by ProtoTEx indicative features. On a propaganda detection task, ProtoTEx accuracy matches BART-large and exceeds BERTlarge with the added benefit of providing faithful explanations. A user study also shows that prototype-based explanations help non-experts to better recognize propaganda in online news.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47295297",
                        "name": "Anubrata Das"
                    },
                    {
                        "authorId": "2124123754",
                        "name": "Chitrank Gupta"
                    },
                    {
                        "authorId": "3455255",
                        "name": "Venelin Kovatchev"
                    },
                    {
                        "authorId": "1747771",
                        "name": "Matthew Lease"
                    },
                    {
                        "authorId": "2108933878",
                        "name": "J. Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f2e8a04e516e67a8550bf74620966f6c70805426",
                "externalIds": {
                    "DBLP": "journals/aeog/ChenZWHCYLLBC22",
                    "DOI": "10.1016/j.jag.2022.102762",
                    "CorpusId": 247930517
                },
                "corpusId": 247930517,
                "publicationVenue": {
                    "id": "4502a19c-ac5e-45a7-9302-2f7b3bcdc0b7",
                    "name": "International Journal of Applied Earth Observation and Geoinformation",
                    "type": "journal",
                    "alternate_names": [
                        "Itc Journal",
                        "Itc J",
                        "Int J Appl Earth Obs Geoinformation"
                    ],
                    "issn": "0303-2434",
                    "alternate_issns": [
                        "1569-8432"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/03032434",
                    "alternate_urls": [
                        "http://www.elsevier.com/locate/jag"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f2e8a04e516e67a8550bf74620966f6c70805426",
                "title": "A joint learning Im-BiLSTM model for incomplete time-series Sentinel-2A data imputation and crop classification",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108425572",
                        "name": "Baili Chen"
                    },
                    {
                        "authorId": "2115720628",
                        "name": "Hongwei Zheng"
                    },
                    {
                        "authorId": "2117932137",
                        "name": "Lili Wang"
                    },
                    {
                        "authorId": "69010848",
                        "name": "O. Hellwich"
                    },
                    {
                        "authorId": "3844876",
                        "name": "Chunbo Chen"
                    },
                    {
                        "authorId": "2111839498",
                        "name": "Liao Yang"
                    },
                    {
                        "authorId": "2110264376",
                        "name": "Tie Liu"
                    },
                    {
                        "authorId": "13908137",
                        "name": "G. Luo"
                    },
                    {
                        "authorId": "2423763",
                        "name": "A. Bao"
                    },
                    {
                        "authorId": "1683647",
                        "name": "X. Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "But there is an ongoing debate \u201cIs attention interpretable\" (Pruthi et al., 2020; Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Vashishth et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d2c1c5b87da32266dc42c6bdfd36d1be875242c5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-17081",
                    "ArXiv": "2203.17081",
                    "DOI": "10.48550/arXiv.2203.17081",
                    "CorpusId": 247839313
                },
                "corpusId": 247839313,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d2c1c5b87da32266dc42c6bdfd36d1be875242c5",
                "title": "Interpretation of Black Box NLP Models: A Survey",
                "abstract": "An increasing number of machine learning models have been deployed in domains with high stakes such as finance and healthcare. Despite their superior performances, many models are black boxes in nature which are hard to explain. There are growing efforts for researchers to develop methods to interpret these black-box models. Post hoc explanations based on perturbations, such as LIME, are widely used approaches to interpret a machine learning model after it has been built. This class of methods has been shown to exhibit large instability, posing serious challenges to the effectiveness of the method itself and harming user trust. In this paper, we propose S-LIME, which utilizes a hypothesis testing framework based on central limit theorem for determining the number of perturbation points needed to guarantee stability of the resulting explanation. Experiments on both simulated and real world data sets are provided to demonstrate the effectiveness of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111928031",
                        "name": "Shivani Choudhary"
                    },
                    {
                        "authorId": "3216579",
                        "name": "N. Chatterjee"
                    },
                    {
                        "authorId": "2614693",
                        "name": "S. Saha"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A prominent line of research has investigated the faithfulness of attention weights (Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2020; Wiegreffe and Pinter, 2019; Madsen et al., 2021b) with contradictory conclusions."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bb1c9cb431e771660cffdda1d80a7f15ff40c764",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-04212",
                    "ArXiv": "2203.04212",
                    "ACL": "2022.emnlp-main.595",
                    "DOI": "10.48550/arXiv.2203.04212",
                    "CorpusId": 247315171
                },
                "corpusId": 247315171,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/bb1c9cb431e771660cffdda1d80a7f15ff40c764",
                "title": "Measuring the Mixing of Contextual Information in the Transformer",
                "abstract": "The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block \u2013multi-head attention, residual connection, and layer normalization\u2013 and define a metric to measure token-to-token interactions within each layer. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides more faithful explanations and increased robustness than gradient-based methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1751450782",
                        "name": "Javier Ferrando"
                    },
                    {
                        "authorId": "2003752849",
                        "name": "Gerard I. G\u00e1llego"
                    },
                    {
                        "authorId": "1398996347",
                        "name": "M. Costa-juss\u00e0"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13e5f30c9c5cec8e3309e653aa563658d4644377",
                "externalIds": {
                    "DOI": "10.1007/s11229-022-03485-5",
                    "CorpusId": 255066731
                },
                "corpusId": 255066731,
                "publicationVenue": {
                    "id": "cfb7bc3b-4dad-4d1f-aea6-f5d1f2499ce8",
                    "name": "Synthese",
                    "type": "journal",
                    "issn": "0039-7857",
                    "url": "http://www.springer.com/11229",
                    "alternate_urls": [
                        "http://www.jstor.org/journals/00397857.html",
                        "https://www.jstor.org/journal/synthese"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/13e5f30c9c5cec8e3309e653aa563658d4644377",
                "title": "Conceptual challenges for interpretable machine learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2140403023",
                        "name": "David S. Watson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While the aforementioned work modified attention weights in a post-hoc manner after a model was trained, Pruthi et al. (2020) proposed to modify attention weights during model learning and produced models whose actual weights could lead to deceived interpretations. Wiegreffe and Pinter (2019) argued the validity of the claim in prior work (Jain and Wallace 2019) and proposed alternative experimental design to test when/whether attention can be used as explanation.",
                "While the aforementioned work modified attention weights in a post-hoc manner after a model was trained, Pruthi et al. (2020) proposed to modify attention weights during model learning and produced models whose actual weights could lead to deceived interpretations."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10ed05aa4a0648649c0454193becca59cf5cc181",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-09792",
                    "ArXiv": "2202.09792",
                    "DOI": "10.1162/coli_a_00459",
                    "CorpusId": 247011113
                },
                "corpusId": 247011113,
                "publicationVenue": {
                    "id": "ee37a78c-f3d8-407a-bd24-bb97fe6dbab9",
                    "name": "Computational Linguistics",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Linguistics"
                    ],
                    "issn": "0891-2017",
                    "alternate_issns": [
                        "1530-9312",
                        "0362-613x",
                        "0362-613X"
                    ],
                    "url": "http://aclanthology.info/venues/cl",
                    "alternate_urls": [
                        "http://mitpress.mit.edu/catalog/item/default.asp?ttype=4&tid=10",
                        "https://www.mitpressjournals.org/loi/coli"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/10ed05aa4a0648649c0454193becca59cf5cc181",
                "title": "Hierarchical Interpretation of Neural Text Classification",
                "abstract": "Abstract Recent years have witnessed increasing interest in developing interpretable models in Natural Language Processing (NLP). Most existing models aim at identifying input features such as words or phrases important for model predictions. Neural models developed in NLP, however, often compose word semantics in a hierarchical manner. As such, interpretation by words or phrases only cannot faithfully explain model decisions in text classification. This article proposes a novel Hierarchical Interpretable Neural Text classifier, called HINT, which can automatically generate explanations of model predictions in the form of label-associated topics in a hierarchical manner. Model interpretation is no longer at the word level, but built on topics as the basic semantic unit. Experimental results on both review datasets and news datasets show that our proposed approach achieves text classification results on par with existing state-of-the-art text classifiers, and generates interpretations more faithful to model predictions and better understood by humans than other interpretable neural text classifiers.1",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1830443015",
                        "name": "Hanqi Yan"
                    },
                    {
                        "authorId": "145096580",
                        "name": "Lin Gui"
                    },
                    {
                        "authorId": "1390509967",
                        "name": "Yulan He"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "65e0122bb57794761577621e9ce1679dfa4ae776",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-13230",
                    "ArXiv": "2201.13230",
                    "DOI": "10.1145/3511808.3557196",
                    "CorpusId": 246430420
                },
                "corpusId": 246430420,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/65e0122bb57794761577621e9ce1679dfa4ae776",
                "title": "POTATO: exPlainable infOrmation exTrAcTion framewOrk",
                "abstract": "We present POTATO, a task- and language-independent framework for human-in-the-loop (HITL) learning of rule-based text classifiers using graph-based features. POTATO handles any type of directed graph and supports parsing text into Abstract Meaning Representations (AMR), Universal Dependencies (UD), and 4lang semantic graphs. A web-based user interface allows users to build rule systems from graph patterns, provides real-time evaluation based on ground truth data, and suggests rules by ranking graph features using interpretable machine learning models. Users can also provide patterns over graphs using regular expressions, and POTATO can recommend refinements of such rules. POTATO is applied in projects across domains and languages, including classification tasks on German legal text and English social media data. All components of our system are written in Python, can be installed via pip, and are released under an MIT License on GitHub.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40061879",
                        "name": "Adam Kovacs"
                    },
                    {
                        "authorId": "2151790510",
                        "name": "Kinga G'emes"
                    },
                    {
                        "authorId": "2151790254",
                        "name": "Eszter Ikl'odi"
                    },
                    {
                        "authorId": "3094252",
                        "name": "G\u00e1bor Recski"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7caaafd5a3ee033c98e792c7ea5b699d005753d5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08164",
                    "ArXiv": "2201.08164",
                    "DOI": "10.1145/3583558",
                    "CorpusId": 246063780
                },
                "corpusId": 246063780,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7caaafd5a3ee033c98e792c7ea5b699d005753d5",
                "title": "From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI",
                "abstract": "The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the past 7 years at major AI and ML conferences that introduce an XAI method. We find that one in three papers evaluate exclusively with anecdotal evidence, and one in five papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark, and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training to optimize for accuracy and interpretability simultaneously.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "17698891",
                        "name": "Meike Nauta"
                    },
                    {
                        "authorId": "52019849",
                        "name": "Jan Trienes"
                    },
                    {
                        "authorId": "66163851",
                        "name": "Shreyasi Pathak"
                    },
                    {
                        "authorId": "13407092",
                        "name": "Elisa Nguyen"
                    },
                    {
                        "authorId": "2066935841",
                        "name": "Michelle Peters"
                    },
                    {
                        "authorId": "2150574981",
                        "name": "Yasmin Schmitt"
                    },
                    {
                        "authorId": "3044872",
                        "name": "J\u00f6rg Schl\u00f6tterer"
                    },
                    {
                        "authorId": "1711719",
                        "name": "M. V. Keulen"
                    },
                    {
                        "authorId": "145566115",
                        "name": "C. Seifert"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ce58bc73d777f871fdfbd0c97df7ae6158c73ea6",
                "externalIds": {
                    "DBLP": "conf/wacv/BlackSPS22",
                    "DOI": "10.1109/WACV51458.2022.00160",
                    "CorpusId": 246868774
                },
                "corpusId": 246868774,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/ce58bc73d777f871fdfbd0c97df7ae6158c73ea6",
                "title": "Visualizing Paired Image Similarity in Transformer Networks",
                "abstract": "Transformer architectures have shown promise for a wide range of computer vision tasks, including image embedding. As was the case with convolutional neural networks and other models, explainability of the predictions is a key concern, but visualization approaches tend to be architecture-specific. In this paper, we introduce a new method for producing interpretable visualizations that, given a pair of images encoded with a Transformer, show which regions contributed to their similarity. Additionally, for the task of image retrieval, we compare the performance of Transformer and ResNet models of similar capacity and show that while they have similar performance in aggregate, the retrieved results and the visual explanations for those results are quite different. Code is available at https://github.com/vidarlab/xformer-paired-viz.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1452228516",
                        "name": "Samuel Black"
                    },
                    {
                        "authorId": "15017879",
                        "name": "Abby Stylianou"
                    },
                    {
                        "authorId": "143857761",
                        "name": "Robert Pless"
                    },
                    {
                        "authorId": "1690110",
                        "name": "Richard Souvenir"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Attention-based methods try to understand the network logic behind the Transformer-based models mainly from analyzing their self-attention maps [11], [43]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03016a572d87b825d8abc453fc727ca6ff98f441",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/LiuLCA21",
                    "DOI": "10.1109/BigData52589.2021.9671639",
                    "CorpusId": 245943966
                },
                "corpusId": 245943966,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/03016a572d87b825d8abc453fc727ca6ff98f441",
                "title": "On Exploring Attention-based Explanation for Transformer Models in Text Classification",
                "abstract": "The Transformer models have achieved unprecedented breakthroughs in text classification, and have become the foundation of most state-of-the-art NLP systems. The core function that drives the success is the attention mechanism, which provides the ability to dynamically focus on different parts of the input sequence when producing the predictions. Several previous works have investigated the usage of attention weights to explain the model predictions, because intuitively, attention weights reflect the importance of the input positions in the output. Specifically, the objective for explanation is to compute a relevance score for each input token, such that the key input words that are most important to the prediction can be identified. However, previous efforts produced mixed results. We find that the key reason why attention weights cannot be directly used as effective relevance indications is because they do not contain the directional information for relevance (i.e., whether the input tokens contribute towards or against the prediction). We then propose two novel explanation techniques, namely AGrad and RePAGrad, that produce directional relevance scores based on attention weights. To evaluate the explanation performance, we propose three properties that an effective explanation method should satisfy (i.e., faithfulness, resilience, and consistency), and design the corresponding test to quantify each property. Through extensive evaluations with Transformer models and pre-trained BERT models on multiple public text classification datasets, we show that AGrad and RePAGrad significantly outperform existing state-of-the-art explanation methods in faithfulness and consistency, at the cost of nominal degradation on resilience compared to attention weights. In addition, we reveal that elements of a model architecture can play an important role towards explainability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1390628682",
                        "name": "Shengzhong Liu"
                    },
                    {
                        "authorId": "2149948861",
                        "name": "Franck Le"
                    },
                    {
                        "authorId": "144387904",
                        "name": "Supriyo Chakraborty"
                    },
                    {
                        "authorId": "1730531",
                        "name": "T. Abdelzaher"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspecting the attention scores is a common method of explaining a model\u2019s prediction that has been called into question in recent years (Pruthi et al., 2020; Serrano and Smith,\n2019).",
                "Inspecting the attention scores is a common method of explaining a model\u2019s prediction that has been called into question in recent years (Pruthi et al., 2020; Serrano and Smith, 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9424d98d7805574661da1adf4f8bc682be5714da",
                "externalIds": {
                    "ArXiv": "2112.05125",
                    "ACL": "2022.emnlp-main.380",
                    "DBLP": "conf/emnlp/BradMBBIP22",
                    "DOI": "10.18653/v1/2022.emnlp-main.380",
                    "CorpusId": 253244645
                },
                "corpusId": 253244645,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/9424d98d7805574661da1adf4f8bc682be5714da",
                "title": "Rethinking the Authorship Verification Experimental Setups",
                "abstract": "One of the main drivers of the recent advances in authorship verification is the PAN large-scale authorship dataset. Despite generating significant progress in the field, inconsistent performance differences between the closed and open test sets have been reported. To this end, we improve the experimental setup by proposing five new public splits over the PAN dataset, specifically designed to isolate and identify biases related to the text topic and to the author\u2019s writing style. We evaluate several BERT-like baselines on these splits, showing that such models are competitive with authorship verification state-of-the-art methods. Furthermore, using explainable AI, we find that these baselines are biased towards named entities. We show that models trained without the named entities obtain better results and generalize better when tested on DarkReddit, our new dataset for authorship verification.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "22225752",
                        "name": "Florin Brad"
                    },
                    {
                        "authorId": "2143755178",
                        "name": "Andrei Manolache"
                    },
                    {
                        "authorId": "3094164",
                        "name": "Elena Burceanu"
                    },
                    {
                        "authorId": "1739398670",
                        "name": "Antonio B\u0103rb\u0103l\u0103u"
                    },
                    {
                        "authorId": "1817759",
                        "name": "Radu Tudor Ionescu"
                    },
                    {
                        "authorId": "49006356",
                        "name": "M. Popescu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Explanation robustness and sensitivity are two desired properties and have been mostly studied on images [1], [11], [55], [48], [34] and texts [34], but none on graphs."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "50cfdcfc5b2cdf21d4e7ca9cdd9b74a426fa4671",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-14651",
                    "ArXiv": "2111.14651",
                    "DOI": "10.1109/ICDM51629.2021.00052",
                    "CorpusId": 244714677
                },
                "corpusId": 244714677,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/50cfdcfc5b2cdf21d4e7ca9cdd9b74a426fa4671",
                "title": "Multi-objective Explanations of GNN Predictions",
                "abstract": "Graph Neural Network (GNN) has achieved state-of-the-art performance in various high-stake prediction tasks, but multiple layers of aggregations on graphs with irregular structures make GNN a less interpretable model. Prior methods use simpler subgraphs to simulate the full model, or counterfactuals to identify the causes of a prediction. The two families of approaches aim at two distinct objectives, \u201csimulatability\u201d and \u201ccounterfactual relevance\u201d, but it is not clear how the objectives can jointly influence the human understanding of an explanation. We design a user-study to investigate such joint effects, and use the findings to design a multi-objective optimization (MOO) algorithm to find Pareto optimal explanations that are well-balanced in simulatability and counterfactual. Since the target model can be of any GNN variants and may not be accessible due to privacy concerns, we design a search algorithm using zero-th order information without accessing the architecture and parameters of the target model. Quantitative experiments on nine graphs from four applications demonstrate that the Pareto efficient explanations dominate single-objective baselines that use first-order continuous optimization or discrete combinatorial search. The explanations are further evaluated in robustness and sensitivity to show their capability of revealing convincing causes, while being cautious about the possible confounders. The diverse dominating counterfactuals can certify the feasibility of algorithmic recourse, that can potentially promote algorithmic fairness where humans are participating in the decision-making using GNN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108101960",
                        "name": "Yifei Liu"
                    },
                    {
                        "authorId": "2145762275",
                        "name": "Chao Chen"
                    },
                    {
                        "authorId": "2144399347",
                        "name": "Yazheng Liu"
                    },
                    {
                        "authorId": "47957054",
                        "name": "Xi Zhang"
                    },
                    {
                        "authorId": "3131378",
                        "name": "Sihong Xie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, other work has shown that it is possible to systematically manipulate attention while still retaining the same prediction [30]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7362bf55c346ce033ae806e99b43a4cb646e367d",
                "externalIds": {
                    "DBLP": "conf/icse/CitoDMC22",
                    "ArXiv": "2111.05711",
                    "DOI": "10.1145/3510457.3513081",
                    "CorpusId": 243938592
                },
                "corpusId": 243938592,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7362bf55c346ce033ae806e99b43a4cb646e367d",
                "title": "Counterfactual Explanations for Models of Code",
                "abstract": "Machine learning (ML) models play an increasingly prevalent role in many software engineering tasks. However, because most models are now powered by opaque deep neural networks, it can be difficult for developers to understand why the model came to a certain conclusion and how to act upon the model's prediction. Motivated by this problem, this paper explores counterfactual explanations for models of source code. Such counterfactual explanations constitute minimal changes to the source code under which the model \u201cchanges its mind\u201d. We integrate counterfactual explanation generation to models of source code in a real-world setting. We describe considerations that impact both the ability to find realistic and plausible counterfactual explanations, as well as the usefulness of such explanation to the developers that use the model. In a series of experiments we investigate the efficacy of our approach on three different models, each based on a BERT-like architecture operating over source code.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2897763",
                        "name": "J\u00fcrgen Cito"
                    },
                    {
                        "authorId": "1714075",
                        "name": "I\u015f\u0131l Dillig"
                    },
                    {
                        "authorId": "32269195",
                        "name": "V. Murali"
                    },
                    {
                        "authorId": "145486355",
                        "name": "S. Chandra"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "907c6845109dc1b282cc0e1b05a4ebb400879674",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-04683",
                    "ArXiv": "2111.04683",
                    "CorpusId": 243847393
                },
                "corpusId": 243847393,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/907c6845109dc1b282cc0e1b05a4ebb400879674",
                "title": "Revisiting Methods for Finding Influential Examples",
                "abstract": "Several instance-based explainability methods for finding influential training examples for test-time decisions have been proposed recently, including Influence Functions, TraceIn, Representer Point Selection, Grad-Dot, and Grad-Cos. Typically these methods are evaluated using LOO influence (Cook's distance) as a gold standard, or using various heuristics. In this paper, we show that all of the above methods are unstable, i.e., extremely sensitive to initialization, ordering of the training data, and batch size. We suggest that this is a natural consequence of how in the literature, the influence of examples is assumed to be independent of model state and other examples -- and argue it is not. We show that LOO influence and heuristics are, as a result, poor metrics to measure the quality of instance-based explanations, and instead propose to evaluate such explanations by their ability to detect poisoning attacks. Further, we provide a simple, yet effective baseline to improve all of the above methods and show how it leads to very significant improvements on downstream tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46451258",
                        "name": "Karthikeyan K"
                    },
                    {
                        "authorId": "1700187",
                        "name": "Anders S\u00f8gaard"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, recent works [7, 20, 35, 38, 44] also find that the same prediction on an input could be generated by totally different attentions, which limits its applicability to explaining neural predictions."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9dc28baf794cbd1d5f24eb91b55f94871aa52d1d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-13880",
                    "ArXiv": "2110.13880",
                    "CorpusId": 239885910
                },
                "corpusId": 239885910,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9dc28baf794cbd1d5f24eb91b55f94871aa52d1d",
                "title": "Understanding Interlocking Dynamics of Cooperative Rationalization",
                "abstract": "Selective rationalization explains the prediction of complex neural networks by finding a small subset of the input that is sufficient to predict the neural model output. The selection mechanism is commonly integrated into the model itself by specifying a two-component cascaded system consisting of a rationale generator, which makes a binary selection of the input features (which is the rationale), and a predictor, which predicts the output based only on the selected features. The components are trained jointly to optimize prediction performance. In this paper, we reveal a major problem with such cooperative rationalization paradigm -- model interlocking. Interlocking arises when the predictor overfits to the features selected by the generator thus reinforcing the generator's selection even if the selected rationales are sub-optimal. The fundamental cause of the interlocking problem is that the rationalization objective to be minimized is concave with respect to the generator's selection policy. We propose a new rationalization framework, called A2R, which introduces a third component into the architecture, a predictor driven by soft attention as opposed to selection. The generator now realizes both soft and hard attention over the features and these are fed into the two different predictors. While the generator still seeks to support the original predictor performance, it also minimizes a gap between the two predictors. As we will show theoretically, since the attention-based predictor exhibits a better convexity property, A2R can overcome the concavity barrier. Our experiments on two synthetic benchmarks and two real datasets demonstrate that A2R can significantly alleviate the interlock problem and find explanations that better align with human judgments. We release our code at https://github.com/Gorov/Understanding_Interlocking.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115621395",
                        "name": "Mo Yu"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "35132120",
                        "name": "T. Jaakkola"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent methods for explaining predictions made by deep learning models considered explanations computed through convolutional layers [3, 26, 27, 33] (in vision models), and Transformer based architectures [11, 16, 17, 30, 37, 37, 40, 43] (in NLP models).",
                "While the authors in [18] argue that attention scores sometimes does not interpret model predictions faithfully, other works show that attention scores do offer plausible and meaningful interpretations that are often sufficient and correct [30, 39, 43]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "90f7b61762e4454b9cba3afbb10c5f07465cff85",
                "externalIds": {
                    "ArXiv": "2204.11073",
                    "DBLP": "conf/cikm/BarkanHCKMAK21",
                    "DOI": "10.1145/3459637.3482126",
                    "CorpusId": 240230832
                },
                "corpusId": 240230832,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/90f7b61762e4454b9cba3afbb10c5f07465cff85",
                "title": "Grad-SAM: Explaining Transformers via Gradient Self-Attention Maps",
                "abstract": "Transformer-based language models significantly advanced the state-of-the-art in many linguistic tasks. As this revolution continues, the ability to explain model predictions has become a major area of interest for the NLP community. In this work, we present Gradient Self-Attention Maps (Grad-SAM) - a novel gradient-based method that analyzes self-attention units and identifies the input elements that explain the model's prediction the best. Extensive evaluations on various benchmarks show that Grad-SAM obtains significant improvements over state-of-the-art alternatives.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48797862",
                        "name": "Oren Barkan"
                    },
                    {
                        "authorId": "2126515058",
                        "name": "Edan Hauon"
                    },
                    {
                        "authorId": "27743758",
                        "name": "Avi Caciularu"
                    },
                    {
                        "authorId": "2066151945",
                        "name": "Ori Katz"
                    },
                    {
                        "authorId": "46252132",
                        "name": "Itzik Malkiel"
                    },
                    {
                        "authorId": "2053812997",
                        "name": "Omri Armstrong"
                    },
                    {
                        "authorId": "1683070",
                        "name": "Noam Koenigstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We notice that the use of attention weights for interpretability is a contentious topic in the literature [17, 32, 47]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5b28d8cb4056bc629e181812d34e938f827b9219",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-12148",
                    "ArXiv": "2110.12148",
                    "CorpusId": 239768359
                },
                "corpusId": 239768359,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5b28d8cb4056bc629e181812d34e938f827b9219",
                "title": "Event Detection on Dynamic Graphs",
                "abstract": "Event detection is a critical task for timely decision-making in graph analytics applications. Despite the recent progress towards deep learning on graphs, event detection on dynamic graphs presents particular challenges to existing architectures. Real-life events are often associated with sudden deviations of the normal behavior of the graph. However, existing approaches for dynamic node embedding are unable to capture the graph-level dynamics related to events. In this paper, we propose DyGED, a simple yet novel deep learning model for event detection on dynamic graphs. DyGED learns correlations between the graph macro dynamics -- i.e. a sequence of graph-level representations -- and labeled events. Moreover, our approach combines structural and temporal self-attention mechanisms to account for application-specific node and time importances effectively. Our experimental evaluation, using a representative set of datasets, demonstrates that DyGED outperforms competing solutions in terms of event detection accuracy by up to 8.5% while being more scalable than the top alternatives. We also present case studies illustrating key features of our model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2134769449",
                        "name": "Mert Kosan"
                    },
                    {
                        "authorId": "2110040391",
                        "name": "A. Silva"
                    },
                    {
                        "authorId": "3390598",
                        "name": "Sourav Medya"
                    },
                    {
                        "authorId": "1838478",
                        "name": "Brian Uzzi"
                    },
                    {
                        "authorId": "1399890865",
                        "name": "Ambuj K. Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There has been a growing debate on the question of whether attention is interpretable [164, 143, 86, 203, 193, 30].",
                "[164, 143, 86, 30] hold negative attitude towards attention\u2019s interpretability."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d5784fd3ac7e06ec030abb8f7787faa9279c1a50",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-10470",
                    "ArXiv": "2110.10470",
                    "CorpusId": 239050251
                },
                "corpusId": 239050251,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d5784fd3ac7e06ec030abb8f7787faa9279c1a50",
                "title": "Interpreting Deep Learning Models in Natural Language Processing: A Review",
                "abstract": "Neural network models have achieved state-of-the-art performances in a wide range of natural language processing (NLP) tasks. However, a long-standing criticism against neural network models is the lack of interpretability, which not only reduces the reliability of neural NLP systems but also limits the scope of their applications in areas where interpretability is essential (e.g., health care applications). In response, the increasing interest in interpreting neural NLP models has spurred a diverse array of interpretation methods over recent years. In this survey, we provide a comprehensive review of various interpretation methods for neural models in NLP. We first stretch out a high-level taxonomy for interpretation methods in NLP, i.e., training-based approaches, test-based approaches, and hybrid approaches. Next, we describe sub-categories in each category in detail, e.g., influence-function based methods, KNN-based methods, attention-based models, saliency-based methods, perturbation-based methods, etc. We point out deficiencies of current methods and suggest some avenues for future research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109329406",
                        "name": "Xiaofei Sun"
                    },
                    {
                        "authorId": "2143919864",
                        "name": "Diyi Yang"
                    },
                    {
                        "authorId": "48570150",
                        "name": "Xiaoya Li"
                    },
                    {
                        "authorId": "2146331573",
                        "name": "Tianwei Zhang"
                    },
                    {
                        "authorId": "65844131",
                        "name": "Yuxian Meng"
                    },
                    {
                        "authorId": "2052215402",
                        "name": "Han Qiu"
                    },
                    {
                        "authorId": "2107926840",
                        "name": "Guoyin Wang"
                    },
                    {
                        "authorId": "144547315",
                        "name": "E. Hovy"
                    },
                    {
                        "authorId": "49298465",
                        "name": "Jiwei Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the NLP literature, the faithfulness of attention is regularly debated (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Vashishth et al., 2019; Serrano and Smith, 2019; Pruthi et al., 2020), often with contradicting conclusions.",
                "Much recent work in NLP has been devoted to investigating the faithfulness of attention as an interpretability method (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Vashishth et al., 2019; Serrano and Smith, 2019; Pruthi et al., 2020; Meister et al., 2021).",
                "Pruthi et al. (2020) perform a similar analysis but report a contradictory finding."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e8243b2aafab80aac0be162f17a720c40fba180d",
                "externalIds": {
                    "DBLP": "conf/emnlp/MadsenMAR22",
                    "ArXiv": "2110.08412",
                    "DOI": "10.18653/v1/2022.findings-emnlp.125",
                    "CorpusId": 239016807
                },
                "corpusId": 239016807,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/e8243b2aafab80aac0be162f17a720c40fba180d",
                "title": "Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining",
                "abstract": "To explain NLP models a popular approach is to use importance measures, such as attention, which inform input tokens are important for making a prediction. However, an open question is how well these explanations accurately reflect a model's logic, a property called faithfulness. To answer this question, we propose Recursive ROAR, a new faithfulness metric. This works by recursively masking allegedly important tokens and then retraining the model. The principle is that this should result in worse model performance compared to masking random tokens. The result is a performance curve given a masking-ratio. Furthermore, we propose a summarizing metric using relative area-between-curves (RACU), which allows for easy comparison across papers, models, and tasks. We evaluate 4 different importance measures on 8 different datasets, using both LSTM-attention models and RoBERTa models. We find that the faithfulness of importance measures is both model-dependent and task-dependent. This conclusion contradicts previous evaluations in both computer vision and faithfulness of attention literature.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152446182",
                        "name": "Andreas Madsen"
                    },
                    {
                        "authorId": "150247363",
                        "name": "Nicholas Meade"
                    },
                    {
                        "authorId": "1666183192",
                        "name": "Vaibhav Adlakha"
                    },
                    {
                        "authorId": "145732771",
                        "name": "Siva Reddy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In Pruthi et al. (2020), the authors found that attention scores can be manipulated to deceive human decision makers.",
                "Following Pruthi et al. (2020), we use the biographies (DeArteaga et al., 2019) to predict whether the occupation is a surgeon or physician (non-surgeon).",
                "Pruthi et al. (2020) also provides a pre-specified list of impermissible tokens 6 that a robust model should assign low attention scores to.",
                "Pruthi et al. (2020) derived an occupation dataset to study the gender bias in NLP classification tasks."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3ca3ff98405b43fab32dcd7cbd6bd34261386e35",
                "externalIds": {
                    "DBLP": "conf/naacl/WangSY022",
                    "ArXiv": "2110.07736",
                    "DOI": "10.18653/v1/2022.findings-naacl.130",
                    "CorpusId": 239009631
                },
                "corpusId": 239009631,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3ca3ff98405b43fab32dcd7cbd6bd34261386e35",
                "title": "Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models",
                "abstract": "Recently, NLP models have achieved remarkable progress across a variety of tasks; however, they have also been criticized for being not robust. Many robustness problems can be attributed to models exploiting spurious correlations, or shortcuts between the training data and the task labels. Most existing work identifies a limited set of task-specific shortcuts via human priors or error analyses, which requires extensive expertise and efforts. In this paper, we aim to automatically identify such spurious correlations in NLP models at scale. We first leverage existing interpretability methods to extract tokens that significantly affect model's decision process from the input text. We then distinguish\"genuine\"tokens and\"spurious\"tokens by analyzing model predictions across multiple corpora and further verify them through knowledge-aware perturbations. We show that our proposed method can effectively and efficiently identify a scalable set of\"shortcuts\", and mitigating these leads to more robust models in multiple applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1785372925",
                        "name": "Tianlu Wang"
                    },
                    {
                        "authorId": "2143919864",
                        "name": "Diyi Yang"
                    },
                    {
                        "authorId": "1524732527",
                        "name": "Xuezhi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "even though multiple heads can be pruned without reducing the accuracy [21].",
                "[21] showed how attention heads can be pruned; we follow up with an even bigger reduction in attention but show that key elements can still be preserved.",
                "This could also be further underlined by the findings and hypotheses of [4], [8], [11], [21]."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d136a7563168c6303ca87ee7c104b9d3a005b948",
                "externalIds": {
                    "DBLP": "conf/dsaa/SchwenkeA21",
                    "DOI": "10.1109/DSAA53316.2021.9564126",
                    "CorpusId": 238864459
                },
                "corpusId": 238864459,
                "publicationVenue": {
                    "id": "770f1d88-52c2-43dc-a897-6881e70b2f32",
                    "name": "International Conference on Data Science and Advanced Analytics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Data Sci Adv Anal",
                        "IEEE International Conference on Data Science and Advanced Analytics",
                        "IEEE Int Conf Data Sci Adv Anal",
                        "DSAA"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d136a7563168c6303ca87ee7c104b9d3a005b948",
                "title": "Constructing Global Coherence Representations: Identifying Interpretability and Coherences of Transformer Attention in Time Series Data",
                "abstract": "Transformer models have shown significant advances recently based on the general concept of Attention \u2014 to focus on specifically important and relevant parts of the input data. However, methods for enhancing their interpretability and explainability are still lacking. This is the problem which we tackle in this paper, to make Multi-Headed Attention more interpretable and explainable for time series classification. We present a method for constructing global coherence representations from Multi-Headed Attention of Transformer architectures. Accordingly, we present abstraction and interpretation methods, leading to intuitive visualizations of the respective attention patterns. We evaluate our proposed approach and the presented methods on several datasets demonstrating their efficacy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2097670994",
                        "name": "Leonid Schwenke"
                    },
                    {
                        "authorId": "2121850331",
                        "name": "Martin Atzmueller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "instead of relying on the attention mechanism [27].",
                "As attention is not guaranteed to reflect how important a modality is [27], we guide the modality attention to be similar to human perceived modality informativeness and also evaluate how similar the predicted modality attention is to the perceived modality informativeness."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2fd5d1357a101e839e52b8cb28d8abbd4faee1e3",
                "externalIds": {
                    "DBLP": "conf/icmi/WortweinSACM21",
                    "DOI": "10.1145/3462244.3481004",
                    "CorpusId": 238992639,
                    "PubMed": "35128550"
                },
                "corpusId": 238992639,
                "publicationVenue": {
                    "id": "d11025b6-9660-45df-b13a-555e3ff4ceca",
                    "name": "International Conference on Multimodal Interaction",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Multimodal Interact",
                        "International Conference on Multimodal Interfaces",
                        "Int Conf Multimodal Interface",
                        "ICMI"
                    ],
                    "url": "https://en.wikipedia.org/wiki/ACM/IEEE_Virtual_Reality_International_Conference"
                },
                "url": "https://www.semanticscholar.org/paper/2fd5d1357a101e839e52b8cb28d8abbd4faee1e3",
                "title": "Human-Guided Modality Informativeness for Affective States",
                "abstract": "This paper studies the hypothesis that not all modalities are always needed to predict affective states. We explore this hypothesis in the context of recognizing three affective states that have shown a relation to a future onset of depression: positive, aggressive, and dysphoric. In particular, we investigate three important modalities for face-to-face conversations: vision, language, and acoustic modality. We first perform a human study to better understand which subset of modalities people find informative, when recognizing three affective states. As a second contribution, we explore how these human annotations can guide automatic affect recognition systems to be more interpretable while not degrading their predictive performance. Our studies show that humans can reliably annotate modality informativeness. Further, we observe that guided models significantly improve interpretability, i.e., they attend to modalities similarly to how humans rate the modality informativeness, while at the same time showing a slight increase in predictive performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2159324",
                        "name": "T. W\u00f6rtwein"
                    },
                    {
                        "authorId": "2545131",
                        "name": "L. Sheeber"
                    },
                    {
                        "authorId": "2060747421",
                        "name": "Nicholas Allen"
                    },
                    {
                        "authorId": "1737918",
                        "name": "J. Cohn"
                    },
                    {
                        "authorId": "49933077",
                        "name": "Louis-Philippe Morency"
                    }
                ]
            }
        },
        {
            "contexts": [
                "of attention visualization may be limited in explaining particular predictions, depending on the task, attention can be quite useful in explaining the model\u2019s overall predictions [39, 40, 41]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f202b7528b484b9a318102c49b05580226c19d76",
                "externalIds": {
                    "ArXiv": "2109.09202",
                    "DBLP": "conf/icann/MemarianiGNMH21",
                    "CorpusId": 237571354
                },
                "corpusId": 237571354,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f202b7528b484b9a318102c49b05580226c19d76",
                "title": "Automated and Explainable Ontology Extension based on Deep Learning: A Case Study in the Chemical Domain",
                "abstract": "Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction enables them to maintain a high quality, allowing them to be widely accepted across their community. However, the manual development process does not scale for large domains. We present a new methodology for automatic ontology extension and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We trained a Transformer-based deep learning model on the leaf node structures from the ChEBI ontology and the classes to which they belong. The model is then capable of automatically classifying previously unseen chemical structures. The proposed model achieved an overall F1 score of 0.80, an improvement of 6 percentage points over our previous results on the same dataset. Additionally, we demonstrate how visualizing the model's attention weights can help to explain the results by providing insight into how the model made its decisions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1994523246",
                        "name": "A. Memariani"
                    },
                    {
                        "authorId": "2453522",
                        "name": "Martin Glauer"
                    },
                    {
                        "authorId": "47498173",
                        "name": "F. Neuhaus"
                    },
                    {
                        "authorId": "1764365",
                        "name": "T. Mossakowski"
                    },
                    {
                        "authorId": "1715137",
                        "name": "Janna Hastings"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some works have criticized the use of attention weights as model explanations (Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2020), demonstrating that attention weights distributions can be modified without affecting the final prediction."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4fbd8505973851271d6e4f72a2068a855e21b2df",
                "externalIds": {
                    "ArXiv": "2109.05853",
                    "DBLP": "journals/corr/abs-2109-05853",
                    "DOI": "10.18653/v1/2021.findings-emnlp.39",
                    "CorpusId": 237491949
                },
                "corpusId": 237491949,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/4fbd8505973851271d6e4f72a2068a855e21b2df",
                "title": "Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions",
                "abstract": "This work proposes an extensive analysis of the Transformer architecture in the Neural Machine Translation (NMT) setting. Focusing on the encoder-decoder attention mechanism, we prove that attention weights systematically make alignment errors by relying mainly on uninformative tokens from the source sequence. However, we observe that NMT models assign attention to these tokens to regulate the contribution in the prediction of the two contexts, the source and the prefix of the target sequence. We provide evidence about the influence of wrong alignments on the model behavior, demonstrating that the encoder-decoder attention mechanism is well suited as an interpretability method for NMT. Finally, based on our analysis, we propose methods that largely reduce the word alignment error rate compared to standard induced alignments from attention weights.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1751450782",
                        "name": "Javier Ferrando"
                    },
                    {
                        "authorId": "1398996347",
                        "name": "M. Costa-juss\u00e0"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5969598152974c6bc35671b6b631aefe9c89dcbc",
                "externalIds": {
                    "ACL": "2022.acl-long.407",
                    "DBLP": "conf/acl/JuZYJ0022",
                    "ArXiv": "2109.05463",
                    "DOI": "10.18653/v1/2022.acl-long.407",
                    "CorpusId": 248780318
                },
                "corpusId": 248780318,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/5969598152974c6bc35671b6b631aefe9c89dcbc",
                "title": "Logic Traps in Evaluating Attribution Scores",
                "abstract": "Modern deep learning models are notoriously opaque, which has motivated the development of methods for interpreting how deep models predict.This goal is usually approached with attribution method, which assesses the influence of features on model predictions. As an explanation method, the evaluation criteria of attribution methods is how accurately it reflects the actual reasoning process of the model (faithfulness). Meanwhile, since the reasoning process of deep models is inaccessible, researchers design various evaluation methods to demonstrate their arguments.However, some crucial logic traps in these evaluation methods are ignored in most works, causing inaccurate evaluation and unfair comparison.This paper systematically reviews existing methods for evaluating attribution scores and summarizes the logic traps in these methods.We further conduct experiments to demonstrate the existence of each logic trap.Through both theoretical and experimental analysis, we hope to increase attention on the inaccurate evaluation of attribution scores. Moreover, with this paper, we suggest stopping focusing on improving performance under unreliable evaluation systems and starting efforts on reducing the impact of proposed logic traps.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143876071",
                        "name": "Yiming Ju"
                    },
                    {
                        "authorId": "2145784135",
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "authorId": "2155029396",
                        "name": "Zhao Yang"
                    },
                    {
                        "authorId": "97234013",
                        "name": "Zhongtao Jiang"
                    },
                    {
                        "authorId": "2200096",
                        "name": "Kang Liu"
                    },
                    {
                        "authorId": "11447228",
                        "name": "Jun Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d592d56418c4df24b443c038a88ec5606a615cde",
                "externalIds": {
                    "DBLP": "conf/aaai/AtanasovaSLA22",
                    "ArXiv": "2109.03756",
                    "DOI": "10.1609/aaai.v36i10.21287",
                    "CorpusId": 237442161
                },
                "corpusId": 237442161,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d592d56418c4df24b443c038a88ec5606a615cde",
                "title": "Diagnostics-Guided Explanation Generation",
                "abstract": "Explanations shed light on a machine learning model's rationales and can aid in identifying deficiencies in its reasoning process. Explanation generation models are typically trained in a supervised way given human explanations. When such annotations are not available, explanations are often selected as those portions of the input that maximise a downstream task's performance, which corresponds to optimising an explanation's Faithfulness to a given model. Faithfulness is one of several so-called diagnostic properties, which prior work has identified as useful for gauging the quality of an explanation without requiring annotations. Other diagnostic properties are Data Consistency, which measures how similar explanations are for similar input instances, and Confidence Indication, which shows whether the explanation reflects the confidence of the model. In this work, we show how to directly optimise for these diagnostic properties when training a model to generate sentence-level explanations, which markedly improves explanation quality, agreement with human rationales, and downstream task performance on three complex reasoning tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145676297",
                        "name": "Pepa Atanasova"
                    },
                    {
                        "authorId": "1707651",
                        "name": "J. Simonsen"
                    },
                    {
                        "authorId": "1784800",
                        "name": "C. Lioma"
                    },
                    {
                        "authorId": "1736067",
                        "name": "Isabelle Augenstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026have typically been used as a tool for evaluating explanation faithfulness generated by attention (Kennedy et al., 2020; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Ghorbani et al., 2019) while others used auxiliary objectives to improve the faithfulness of explanations generated by\u2026",
                "typically been used as a tool for evaluating explanation faithfulness generated by attention (Kennedy et al., 2020; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Ghorbani et al., 2019) while others used auxiliary objectives to improve the faithful-",
                "Pruthi et al. (2020) show similar outcomes by manipulating attention to attend to uninformative tokens."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "311e48e1c4a0dcd65a6699376ffc85a24a333a56",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-13759",
                    "ACL": "2021.emnlp-main.645",
                    "ArXiv": "2108.13759",
                    "DOI": "10.18653/v1/2021.emnlp-main.645",
                    "CorpusId": 237364609
                },
                "corpusId": 237364609,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/311e48e1c4a0dcd65a6699376ffc85a24a333a56",
                "title": "Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience",
                "abstract": "Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this paper, we hypothesize that salient information extracted a priori from the training data can complement the task-specific information learned by the model during fine-tuning on a downstream task. In this way, we aim to help BERT not to forget assigning importance to informative input tokens when making predictions by proposing SaLoss; an auxiliary loss function for guiding the multi-head attention mechanism during training to be close to salient information extracted a priori using TextRank. Experiments for explanation faithfulness across five datasets, show that models trained with SaLoss consistently provide more faithful explanations across four different feature attribution methods compared to vanilla BERT. Using the rationales extracted from vanilla BERT and SaLoss models to train inherently faithful classifiers, we further show that the latter result in higher predictive performance in downstream tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2125182391",
                        "name": "G. Chrysostomou"
                    },
                    {
                        "authorId": "3238627",
                        "name": "Nikolaos Aletras"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0fee82cb24c7659b28e9ac05de48e4d0edf986fd",
                "externalIds": {
                    "ArXiv": "2108.13140",
                    "CorpusId": 237428337
                },
                "corpusId": 237428337,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0fee82cb24c7659b28e9ac05de48e4d0edf986fd",
                "title": "DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation",
                "abstract": "While deep learning models have greatly improved the performance of most artificial intelligence tasks, they are often criticized to be untrustworthy due to the black-box problem. Consequently, many works have been proposed to study the trustworthiness of deep learning. However, as most open datasets are designed for evaluating the accuracy of model outputs, there is still a lack of appropriate datasets for evaluating the inner workings of neural networks. The lack of datasets obviously hinders the development of trustworthiness research. Therefore, in order to systematically evaluate the factors for building trustworthy systems, we propose a novel and well-annotated sentiment analysis dataset to evaluate robustness and interpretability. To evaluate these factors, our dataset contains diverse annotations about the challenging distribution of instances, manual adversarial instances and sentiment explanations. Several evaluation metrics are further proposed for interpretability and robustness. Based on the dataset and metrics, we conduct comprehensive comparisons for the trustworthiness of three typical models, and also study the relations between accuracy, robustness and interpretability. We release this trustworthiness evaluation dataset at https://github/xyz and hope our work can facilitate the progress on building more trustworthy systems for real-world applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118354420",
                        "name": "Lijie Wang"
                    },
                    {
                        "authorId": "2143855615",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2072712948",
                        "name": "Shu-ping Peng"
                    },
                    {
                        "authorId": "1576488202",
                        "name": "Hongxuan Tang"
                    },
                    {
                        "authorId": "2107521158",
                        "name": "Xinyan Xiao"
                    },
                    {
                        "authorId": "2118428180",
                        "name": "Ying Chen"
                    },
                    {
                        "authorId": "40354707",
                        "name": "Hua Wu"
                    },
                    {
                        "authorId": "144270731",
                        "name": "Haifeng Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "02e46711fc86877bdd279c736abe5415a2415e48",
                "externalIds": {
                    "ArXiv": "2108.11896",
                    "DBLP": "journals/tacl/GuoSV22",
                    "ACL": "2022.tacl-1.11",
                    "DOI": "10.1162/tacl_a_00454",
                    "CorpusId": 237304047
                },
                "corpusId": 237304047,
                "publicationVenue": {
                    "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
                    "name": "Transactions of the Association for Computational Linguistics",
                    "type": "journal",
                    "alternate_names": [
                        "Trans Assoc Comput Linguistics",
                        "TACL"
                    ],
                    "issn": "2307-387X",
                    "url": "https://www.mitpressjournals.org/loi/tacl",
                    "alternate_urls": [
                        "http://www.transacl.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/02e46711fc86877bdd279c736abe5415a2415e48",
                "title": "A Survey on Automated Fact-Checking",
                "abstract": "Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2681038",
                        "name": "Zhijiang Guo"
                    },
                    {
                        "authorId": "8804828",
                        "name": "M. Schlichtkrull"
                    },
                    {
                        "authorId": "2064056928",
                        "name": "Andreas Vlachos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The functionally-groundedness of input feature explanations have recived a lot of attention and discussion, however there is still little consensus on what is functionally-grounded or how to even measure it [3, 11, 54, 60, 73, 76, 89, 106, 122, 130]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "898b14509593d235414df054527b7702e35c3099",
                "externalIds": {
                    "ArXiv": "2108.04840",
                    "DBLP": "journals/corr/abs-2108-04840",
                    "DOI": "10.1145/3546577",
                    "CorpusId": 236976388
                },
                "corpusId": 236976388,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/898b14509593d235414df054527b7702e35c3099",
                "title": "Post-hoc Interpretability for Neural NLP: A Survey",
                "abstract": "Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152446182",
                        "name": "Andreas Madsen"
                    },
                    {
                        "authorId": "145732771",
                        "name": "Siva Reddy"
                    },
                    {
                        "authorId": "144631588",
                        "name": "A. Chandar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, there are also dissenting papers showing that attention weights are not (or only weakly) correlated with feature importance measures such as gradientbased methods [26, 55], and that there exist alternative \u201ccounterfactual\u201d attention distributions that yield equivalent model predictions [26, 44, 50, 22]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "915fe4c29270cf3b656cc2a39ab5d366bf6554f6",
                "externalIds": {
                    "DBLP": "journals/kbs/NguyenWO21",
                    "DOI": "10.1016/j.knosys.2021.107162",
                    "CorpusId": 235690393
                },
                "corpusId": 235690393,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/915fe4c29270cf3b656cc2a39ab5d366bf6554f6",
                "title": "Attention uncovers task-relevant semantics in emotional narrative understanding",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40647710",
                        "name": "Thanh-Son Nguyen"
                    },
                    {
                        "authorId": "2146268508",
                        "name": "Zhengxuan Wu"
                    },
                    {
                        "authorId": "144799222",
                        "name": "Desmond C. Ong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-12708",
                    "ArXiv": "2107.12708",
                    "DOI": "10.1145/3560260",
                    "CorpusId": 236447339
                },
                "corpusId": 236447339,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4",
                "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension",
                "abstract": "Alongside huge volumes of research on deep learning models in NLP in the recent years, there has been much work on benchmark datasets needed to track modeling progress. Question answering and reading comprehension have been particularly prolific in this regard, with more than 80 new datasets appearing in the past 2 years. This study is the largest survey of the field to date. We provide an overview of the various formats and domains of the current resources, highlighting the current lacunae for future work. We further discuss the current classifications of \u201cskills\u201d that question answering/reading comprehension systems are supposed to acquire and propose a new taxonomy. The supplementary materials survey the current multilingual resources and monolingual resources for languages other than English, and we discuss the implications of overfocusing on English. The study is aimed at both practitioners looking for pointers to the wealth of existing data and at researchers working on new resources.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145046059",
                        "name": "Anna Rogers"
                    },
                    {
                        "authorId": "40642935",
                        "name": "Matt Gardner"
                    },
                    {
                        "authorId": "1736067",
                        "name": "Isabelle Augenstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "MFN is highly relied on attention mechanism, whose effect improvement is limited until the mask method is proposed [16]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "36519fb1bc6fa2bfaca98b7ac8f65761fe538986",
                "externalIds": {
                    "DBLP": "conf/ijcnn/ZhangZLLZ21",
                    "DOI": "10.1109/IJCNN52387.2021.9534082",
                    "CorpusId": 237599575
                },
                "corpusId": 237599575,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/36519fb1bc6fa2bfaca98b7ac8f65761fe538986",
                "title": "CtnR: Compress-then-Reconstruct Approach for Multimodal Abstractive Summarization",
                "abstract": "With the rapid growth of multimodal data in social medias and the huge requirement of short but abundant information. Multimodal summarization has drawn much attention in both industry and academia. It usually obtains textual summary from multiple sources by computer vision or nature language processing technologies. However, there are also two challenges in modeling such task: 1) The feature representation is limited by the non-alignment among multimodal data; 2) Massive parallel data is required during training, which is time-consuming and laborious. In this paper, we introduce an unsupervised architecture (Compress-then-Reconstruct, CtnR) to generate the summary in an end-to-end manner and a Cross-Modal Transformer module (CMTrans) to fuse the multimodal non-alignment information. Comprehensive experiments show that the proposed CtnR framework with CMTrans outperforms mainstream unsupervised approaches in terms of BLEU, ROUGE and relevance scores on MSMO and Youtube News dataset, which increase 8.82% and 11.01% on average respectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47424058",
                        "name": "Chenxi Zhang"
                    },
                    {
                        "authorId": "2116459561",
                        "name": "Zijian Zhang"
                    },
                    {
                        "authorId": "2118505714",
                        "name": "Jiangfeng Li"
                    },
                    {
                        "authorId": "47362246",
                        "name": "Qin Liu"
                    },
                    {
                        "authorId": "2608808",
                        "name": "Hongming Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Model interpretability for NLP has been intensively studied in the past few years (Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2018; Jacovi et al., 2018; Chen et al., 2020a; Jacovi and Goldberg, 2020; DeYoung et al., 2020; Pruthi et al., 2020; Ye et al., 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "143310c074eb09d5e60adea4c42250dbe03bf9f2",
                "externalIds": {
                    "ArXiv": "2106.01518",
                    "DBLP": "conf/acl/XuD20",
                    "ACL": "2021.acl-long.539",
                    "DOI": "10.18653/v1/2021.acl-long.539",
                    "CorpusId": 235313847
                },
                "corpusId": 235313847,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/143310c074eb09d5e60adea4c42250dbe03bf9f2",
                "title": "Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution",
                "abstract": "Despite the prominence of neural abstractive summarization models, we know little about how they actually form summaries and how to understand where their decisions come from. We propose a two-step method to interpret summarization model decisions. We first analyze the model\u2019s behavior by ablating the full model to categorize each decoder decision into one of several generation modes: roughly, is the model behaving like a language model, is it relying heavily on the input, or is it somewhere in between? After isolating decisions that do depend on the input, we explore interpreting these decisions using several different attribution methods. We compare these techniques based on their ability to select content and reconstruct the model\u2019s predicted token from perturbations of the input, thus revealing whether highlighted attributions are truly important for the generation of the next token. While this machinery can be broadly useful even beyond summarization, we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened, as well as study complex generation phenomena like sentence fusion on a per-instance basis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2121525724",
                        "name": "Jiacheng Xu"
                    },
                    {
                        "authorId": "1814094",
                        "name": "Greg Durrett"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026Grimsley et al. (2020) found evidence that causal explanations are not attainable from attention layers over text data; Jacovi and Goldberg (2020) explored the faithfulness of attention heatmaps; Pruthi et al. (2020) showed that attention masks can be trained to give deceptive explanations."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bc73d53ba859c56ab08c41c475d45a9ae6d021cb",
                "externalIds": {
                    "DBLP": "conf/acl/MeisterLAC20",
                    "ArXiv": "2106.01087",
                    "ACL": "2021.acl-short.17",
                    "DOI": "10.18653/v1/2021.acl-short.17",
                    "CorpusId": 235293798
                },
                "corpusId": 235293798,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/bc73d53ba859c56ab08c41c475d45a9ae6d021cb",
                "title": "Is Sparse Attention more Interpretable?",
                "abstract": "Sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs. Yet the attention distribution is typically over representations internal to the model rather than the inputs themselves, suggesting this assumption may not have merit. We build on the recent work exploring the interpretability of attention; we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability tool. On three text classification tasks, we verify that only a weak relationship between inputs and co-indexed intermediate representations exists\u2014under sparse attention and otherwise. Further, we do not find any plausible mappings from sparse attention distributions to a sparse set of influential inputs through other avenues. Rather, we observe in this setting that inducing sparsity may make it less plausible that attention can be used as a tool for understanding model behavior.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150953620",
                        "name": "Clara Meister"
                    },
                    {
                        "authorId": "2106626313",
                        "name": "Stefan Lazov"
                    },
                    {
                        "authorId": "1736067",
                        "name": "Isabelle Augenstein"
                    },
                    {
                        "authorId": "2070989574",
                        "name": "Ryan Cotterell"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026al. (2020) have described the potential for human attention supervision to address the validity of attention as a faithful, human-like explanation for model decisions while Pruthi et al. (2019) have discussed the potential for deception by manipulating attention to make models appear less biased."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8c0d8027037a71e63cc12e060066ed417b82b429",
                "externalIds": {
                    "ACL": "2021.cmcl-1.26",
                    "DBLP": "conf/acl-cmcl/McGuireT21",
                    "MAG": "3166803840",
                    "DOI": "10.18653/v1/2021.cmcl-1.26",
                    "CorpusId": 235097315
                },
                "corpusId": 235097315,
                "publicationVenue": {
                    "id": "b406ee9d-e805-486f-889a-30c90f7ec447",
                    "name": "Workshop on Cognitive Modeling and Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "CMCL",
                        "Workshop Cogn Model Comput Linguistics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8c0d8027037a71e63cc12e060066ed417b82b429",
                "title": "Relation Classification with Cognitive Attention Supervision",
                "abstract": "Many current language models such as BERT utilize attention mechanisms to transform sequence representations. We ask whether we can influence BERT\u2019s attention with human reading patterns by using eye-tracking and brain imaging data. We fine-tune BERT for relation extraction with auxiliary attention supervision in which BERT\u2019s attention weights are supervised by cognitive data. Through a variety of metrics we find that this attention supervision can be used to increase similarity between model attention distributions over sequences and the cognitive data without significantly affecting classification performance while making unique errors from the baseline. In particular, models with cognitive attention supervision more often correctly classified samples misclassified by the baseline.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2101321114",
                        "name": "Erik S. McGuire"
                    },
                    {
                        "authorId": "1801234",
                        "name": "Noriko Tomuro"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[1, 30, 61, 70, 84]) and determining when attention can be used as an explanation [84]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "28fc865105bf91c70d13e7e19effc53da9d247b1",
                "externalIds": {
                    "DBLP": "conf/iclr/GhandehariounKL22",
                    "ArXiv": "2105.15164",
                    "CorpusId": 235254386
                },
                "corpusId": 235254386,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/28fc865105bf91c70d13e7e19effc53da9d247b1",
                "title": "DISSECT: Disentangled Simultaneous Explanations via Concept Traversals",
                "abstract": "Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore\"what-if\"scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent\"notion\"of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well and better than existing methods. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2214185",
                        "name": "Asma Ghandeharioun"
                    },
                    {
                        "authorId": "2115287506",
                        "name": "Been Kim"
                    },
                    {
                        "authorId": "2116729195",
                        "name": "Chun-Liang Li"
                    },
                    {
                        "authorId": "2447185",
                        "name": "Brendan Jou"
                    },
                    {
                        "authorId": "2130169",
                        "name": "B. Eoff"
                    },
                    {
                        "authorId": "1719389",
                        "name": "Rosalind W. Picard"
                    }
                ]
            }
        },
        {
            "contexts": [
                "inherent uncertainty [1], [19], [27], [49].",
                "However, humans find it hard to make sense of soft masks and instead prefer sparse binary masks or hard masks [1], [19], [27], [49]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7c85ad5f11ef9afb4f568c33304d86105da956ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-08621",
                    "ArXiv": "2105.08621",
                    "DOI": "10.1109/TKDE.2022.3201170",
                    "CorpusId": 234762791
                },
                "corpusId": 234762791,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7c85ad5f11ef9afb4f568c33304d86105da956ce",
                "title": "Zorro: Valid, Sparse, and Stable Explanations in Graph Neural Networks",
                "abstract": "With the ever-increasing popularity and applications of graph neural networks, several proposals have been made to explain and understand the decisions of a graph neural network. Explanations for graph neural networks differ in principle from other input settings. It is important to attribute the decision to input features and other related instances connected by the graph structure. We find that the previous explanation generation approaches that maximize the mutual information between the label distribution produced by the model and the explanation to be restrictive. Specifically, existing approaches do not enforce explanations to be valid, sparse, or robust to input perturbations. In this paper, we lay down some of the fundamental principles that an explanation method for graph neural networks should follow and introduce a metric RDT-Fidelity as a measure of the explanation's effectiveness. We propose a novel approach Zorro based on the principles from rate-distortion theory that uses a simple combinatorial procedure to optimize for RDT-Fidelity. Extensive experiments on real and synthetic datasets reveal that Zorro produces sparser, stable, and more faithful explanations than existing graph neural network explanation approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143923185",
                        "name": "Thorben Funke"
                    },
                    {
                        "authorId": "35070805",
                        "name": "Megha Khosla"
                    },
                    {
                        "authorId": "39775488",
                        "name": "Avishek Anand"
                    }
                ]
            }
        },
        {
            "contexts": [
                "And we follow the (Pruthi et al. 2020) to carve out a binary classification task of distinguishing between surgeons and (non-surgeon) physicians, where a majority of surgeons (> 80%) in the dataset are male.",
                "To enhance the gender bias, (Pruthi et al. 2020) further downsample minority classes \u2014 female surgeons, and male physicians by a factor of ten.",
                "Model Architecture We train a simple embedding attention model (Pruthi et al. 2020), where the attention is directly over word embeddings (128 dimensions).",
                "And we follow the (Pruthi et al. 2020) to carve out a binary classification task of distinguishing between surgeons and (non-surgeon) physicians, where a majority of surgeons (> 80"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2f88acd32214ffd870d461d62b204903a50ec70b",
                "externalIds": {
                    "DBLP": "conf/aaai/HanTL21",
                    "DOI": "10.1609/aaai.v35i9.16934",
                    "CorpusId": 232290131
                },
                "corpusId": 232290131,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/2f88acd32214ffd870d461d62b204903a50ec70b",
                "title": "Explanation Consistency Training: Facilitating Consistency-Based Semi-Supervised Learning with Interpretability",
                "abstract": "Unlabeled data exploitation and interpretability are usually both required in reality. They, however, are conducted independently, and very few works try to connect the two. For unlabeled data exploitation, state-of-the-art semi-supervised learning (SSL) results have been achieved via encouraging the consistency of model output on data perturbation, that is, consistency assumption. However, it remains hard for users to understand how particular decisions are made by state-of-the-art SSL models. To this end, in this paper we first disclose that the consistency assumption is closely related to causality invariance, where causality invariance lies in the main reason why the consistency assumption is valid. We then propose ECT (Explanation Consistency Training) which encourages a consistent reason of model decision under data perturbation. ECT employs model explanation as a surrogate of the causality of model output, which is able to bridge state-of-the-art interpretability to SSL models and alleviate the high complexity of causality. We realize ECT-SM for vision and ECT-ATT for NLP tasks. Experimental results on real-world data sets validate the highly competitive performance and better explanation of the proposed algorithms.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145421604",
                        "name": "Tao Han"
                    },
                    {
                        "authorId": "47178228",
                        "name": "Wei-Wei Tu"
                    },
                    {
                        "authorId": "2110463675",
                        "name": "Yu-Feng Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f56522d86fc0e5f6021718a68a2b92df4e776916",
                "externalIds": {
                    "ArXiv": "2105.07542",
                    "DBLP": "conf/ijcai/LuRCKN21",
                    "DOI": "10.24963/ijcai.2021/486",
                    "CorpusId": 234742197
                },
                "corpusId": 234742197,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f56522d86fc0e5f6021718a68a2b92df4e776916",
                "title": "Collaborative Graph Learning with Auxiliary Text for Temporal Event Prediction in Healthcare",
                "abstract": "Accurate and explainable health event predictions are becoming crucial for healthcare providers to develop care plans for patients. The availability of electronic health records (EHR) has enabled machine learning advances in providing these predictions. However, many deep-learning-based methods are not satisfactory in solving several key challenges: 1) effectively utilizing disease domain knowledge; 2) collaboratively learning representations of patients and diseases; and 3) incorporating unstructured features. To address these issues, we propose a collaborative graph learning model to explore patient-disease interactions and medical domain knowledge. Our solution is able to capture structural features of both patients and diseases. The proposed model also utilizes unstructured text data by employing an attention manipulating strategy and then integrates attentive text features into a sequential learning process. We conduct extensive experiments on two important healthcare problems to show the competitive prediction performance of the proposed method compared with various state-of-the-art models. We also confirm the effectiveness of learned representations and model interpretability by a set of ablation and case studies.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110141584",
                        "name": "Chang Lu"
                    },
                    {
                        "authorId": "144417522",
                        "name": "C. Reddy"
                    },
                    {
                        "authorId": "1423705934",
                        "name": "P. Chakraborty"
                    },
                    {
                        "authorId": "50779209",
                        "name": "Samantha Kleinberg"
                    },
                    {
                        "authorId": "26426534",
                        "name": "Yue Ning"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Attention-based interpretation can also be unreliable and manipulable to the point of deceiving practitioners, as Pruthi et al. (2020) and Jain and Wallace (2019) show."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7c799b7bd8c069c6feb7235345c97aa1f5330b84",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-05641",
                    "ACL": "2021.findings-acl.155",
                    "ArXiv": "2105.05641",
                    "DOI": "10.18653/v1/2021.findings-acl.155",
                    "CorpusId": 234469686
                },
                "corpusId": 234469686,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7c799b7bd8c069c6feb7235345c97aa1f5330b84",
                "title": "How Reliable are Model Diagnostics?",
                "abstract": "In the pursuit of a deeper understanding of a model's behaviour, there is recent impetus for developing suites of probes aimed at diagnosing models beyond simple metrics like accuracy or BLEU. This paper takes a step back and asks an important and timely question: how reliable are these diagnostics in providing insight into models and training setups? We critically examine three recent diagnostic tests for pre-trained language models, and find that likelihood-based and representation-based model diagnostics are not yet as reliable as previously assumed. Based on our empirical findings, we also formulate recommendations for practitioners and researchers.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1481714624",
                        "name": "V. Aribandi"
                    },
                    {
                        "authorId": "144447820",
                        "name": "Yi Tay"
                    },
                    {
                        "authorId": "1680617",
                        "name": "Donald Metzler"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2015) model the selection using a conditional importance distribution over the inputs, but the resulting explanations are noisy (Jain and Wallace, 2019; Pruthi et al., 2020).",
                "Attention mechanisms (Bahdanau et al., 2015) model the selection using a conditional importance distribution over the inputs, but the resulting explanations are noisy (Jain and Wallace, 2019; Pruthi et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3097f7d9a2696d4ec06999dee4a50052c01453a4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-04837",
                    "ArXiv": "2105.04837",
                    "ACL": "2021.findings-acl.68",
                    "DOI": "10.18653/v1/2021.findings-acl.68",
                    "CorpusId": 234357794
                },
                "corpusId": 234357794,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3097f7d9a2696d4ec06999dee4a50052c01453a4",
                "title": "Rationalization through Concepts",
                "abstract": "Automated predictions require explanations to be interpretable by humans. One type of explanation is a rationale, i.e., a selection of input features such as relevant text snippets from which the model computes the outcome. However, a single overall selection does not provide a complete explanation, e.g., weighing several aspects for decisions. To this end, we present a novel self-interpretable model called ConRAT. Inspired by how human explanations for high-level decisions are often based on key concepts, ConRAT extracts a set of text snippets as concepts and infers which ones are described in the document. Then, it explains the outcome with a linear aggregation of concepts. Two regularizers drive ConRAT to build interpretable concepts. In addition, we propose two techniques to boost the rationale and predictive performance further. Experiments on both single- and multi-aspect sentiment classification tasks show that ConRAT is the first to generate concepts that align with human rationalization while using only the overall label. Further, it outperforms state-of-the-art methods trained on each aspect label independently.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "26399699",
                        "name": "Diego Antognini"
                    },
                    {
                        "authorId": "1735128",
                        "name": "B. Faltings"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While removing the attention score normalization via softmax is not in line with an intuitive interpretation of attention as distributing \u201cfractions\u201d of an overall attention budget among inputs, a growing body of work shows that the attention weights distribution does not directly correlate with predictions (Jain & Wallace, 2019; Pruthi et al., 2020; Brunner et al., 2020).",
                "\u2026with an intuitive interpretation of attention as distributing \u201cfractions\u201d of an overall attention budget among inputs, a growing body of work shows that the attention weights distribution does not directly correlate with predictions (Jain & Wallace, 2019; Pruthi et al., 2020; Brunner et al., 2020)."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d13a0c8d49cb268d8d245925baee0316c1fe1875",
                "externalIds": {
                    "ArXiv": "2105.03928",
                    "DBLP": "journals/corr/abs-2105-03928",
                    "CorpusId": 234338042
                },
                "corpusId": 234338042,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d13a0c8d49cb268d8d245925baee0316c1fe1875",
                "title": "Which transformer architecture fits my data? A vocabulary bottleneck in self-attention",
                "abstract": "After their successful debut in natural language processing, Transformer architectures are now becoming the de-facto standard in many domains. An obstacle for their deployment over new modalities is the architectural configuration: the optimal depth-to-width ratio has been shown to dramatically vary across data types (e.g., $10$x larger over images than over language). We theoretically predict the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity. We thus directly tie the input vocabulary size and rank to the optimal depth-to-width ratio, since a small vocabulary size or rank dictates an added advantage of depth over width. We empirically demonstrate the existence of this bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains. As an additional benefit, our rank bottlenecking framework allows us to identify size redundancies of $25\\%-50\\%$ in leading NLP models such as ALBERT and T5.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "69978543",
                        "name": "Noam Wies"
                    },
                    {
                        "authorId": "152754428",
                        "name": "Yoav Levine"
                    },
                    {
                        "authorId": "2090357207",
                        "name": "Daniel Jannai"
                    },
                    {
                        "authorId": "3140335",
                        "name": "A. Shashua"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020) and is faithful to the underlying computation, providing benefits even when omitted during inference (Pruthi et al., 2020).",
                "\u2026since the attention mechanism (Bahdanau et al., 2015) provides plausible insight into what tokens the model considers relevant for a prediction (Galassi et al., 2020) and is faithful to the underlying computation, providing benefits even when omitted during inference (Pruthi et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2c68e38d9ff1c61672d3c85e4eb72ff9542d0b89",
                "externalIds": {
                    "ArXiv": "2105.03287",
                    "DBLP": "journals/corr/abs-2105-03287",
                    "CorpusId": 234096057
                },
                "corpusId": 234096057,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c68e38d9ff1c61672d3c85e4eb72ff9542d0b89",
                "title": "Order in the Court: Explainable AI Methods Prone to Disagreement",
                "abstract": "By computing the rank correlation between attention weights and feature-additive explanation methods, previous analyses either invalidate or support the role of attention-based explanations as a faithful and plausible measure of salience. To investigate whether this approach is appropriate, we compare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and attention-based explanations, applied to two neural architectures trained on single- and pair-sequence language tasks. In most cases, we find that none of our chosen methods agree. Based on our empirical observations and theoretical objections, we conclude that rank correlation does not measure the quality of feature-additive methods. Practitioners should instead use the numerous and rigorous diagnostic methods proposed by the community.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113260008",
                        "name": "Michael Neely"
                    },
                    {
                        "authorId": "89933535",
                        "name": "Stefan F. Schouten"
                    },
                    {
                        "authorId": "1452678770",
                        "name": "Maurits J. R. Bleeker"
                    },
                    {
                        "authorId": "38702106",
                        "name": "Ana Lucic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pruthi et al. (2020) also investigate the ability of attention weights to provide plausible explanations."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "32a6daa76efd00f657e65842771971decf104efc",
                "externalIds": {
                    "ACL": "2021.acl-long.40",
                    "DBLP": "journals/corr/abs-2105-02657",
                    "ArXiv": "2105.02657",
                    "DOI": "10.18653/v1/2021.acl-long.40",
                    "CorpusId": 233864728
                },
                "corpusId": 233864728,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/32a6daa76efd00f657e65842771971decf104efc",
                "title": "Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification",
                "abstract": "Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks. In this paper, we seek to improve the faithfulness of attention-based explanations for text classification. We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attention-based explanations across two attention mechanisms, five encoders and five text classification datasets without sacrificing predictive performance. Finally, we demonstrate that TaSc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51015453",
                        "name": "G. Chrysostomou"
                    },
                    {
                        "authorId": "3238627",
                        "name": "Nikolaos Aletras"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Past studies proposed various, and sometimes conflicting, criteria for such interpretation [17, 25, 35], but their correctness is unclear."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "426734685283b4a0c08b34cd9e996e2e30e7f7ee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-14403",
                    "ArXiv": "2104.14403",
                    "MAG": "3157950068",
                    "DOI": "10.1609/aaai.v36i9.21196",
                    "CorpusId": 233443847
                },
                "corpusId": 233443847,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/426734685283b4a0c08b34cd9e996e2e30e7f7ee",
                "title": "Do Feature Attribution Methods Correctly Attribute Features?",
                "abstract": "Feature attribution methods are popular in interpretable machine learning. These methods compute the attribution of each input feature to represent its importance, but there is no consensus on the definition of \"attribution\", leading to many competing methods with little systematic evaluation, complicated in particular by the lack of ground truth attribution. To address this, we propose a dataset modification procedure to induce such ground truth. Using this procedure, we evaluate three common methods: saliency maps, rationales, and attentions. We identify several deficiencies and add new perspectives to the growing body of evidence questioning the correctness and reliability of these methods applied on datasets in the wild. We further discuss possible avenues for remedy and recommend new attribution methods to be tested against ground truth before deployment. The code and appendix are available at https://yilunzhou.github.io/feature-attribution-evaluation/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110339246",
                        "name": "Yilun Zhou"
                    },
                    {
                        "authorId": "7594244",
                        "name": "S. Booth"
                    },
                    {
                        "authorId": "78846919",
                        "name": "Marco Tulio Ribeiro"
                    },
                    {
                        "authorId": "143873972",
                        "name": "J. Shah"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Due to its powerful ability, the attentionmechanism has been widely used in various neural network based applications such as language understanding tasks [11],[28], computer vision problems [32],[17]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e19bd5977927764caa8647a35d03d6977aaef3bf",
                "externalIds": {
                    "DBLP": "journals/kbs/AnZYTJW21",
                    "ArXiv": "2104.11026",
                    "DOI": "10.1016/j.knosys.2021.107534",
                    "CorpusId": 233346843
                },
                "corpusId": 233346843,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e19bd5977927764caa8647a35d03d6977aaef3bf",
                "title": "MeSIN: Multilevel Selective and Interactive Network for Medication Recommendation",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2052144878",
                        "name": "Yang An"
                    },
                    {
                        "authorId": "2146642142",
                        "name": "Liang Zhang"
                    },
                    {
                        "authorId": "2080124017",
                        "name": "Mao You"
                    },
                    {
                        "authorId": "2087102404",
                        "name": "Xueqing Tian"
                    },
                    {
                        "authorId": "1504367017",
                        "name": "Bo Jin"
                    },
                    {
                        "authorId": "2115493235",
                        "name": "Xiaopeng Wei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, learned attention weights are often uncorrelated with word importance, which is calculated using the gradient-based method [14], and perturbations to the attention mechanisms may interfere with the interpretation [10, 28]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "28713169c3b993de291f546a63ef71b7310e33f5",
                "externalIds": {
                    "DBLP": "journals/apin/KitadaI23",
                    "ArXiv": "2104.08763",
                    "DOI": "10.1007/s10489-022-04301-w",
                    "CorpusId": 233296351
                },
                "corpusId": 233296351,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/28713169c3b993de291f546a63ef71b7310e33f5",
                "title": "Making attention mechanisms more robust and interpretable with virtual adversarial training",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51267434",
                        "name": "Shunsuke Kitada"
                    },
                    {
                        "authorId": "2801969",
                        "name": "H. Iyatomi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Also, Baan et al. showed that MHA is at least partly interpretable even though multiple heads can be pruned without reducing the accuracy (Pruthi et al. 2019).",
                "showed that MHA is at least partly interpretable even though multiple heads can be pruned without reducing the accuracy (Pruthi et al. 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dabe2fae0d92936b7d57be0b5608aa4c32269847",
                "externalIds": {
                    "MAG": "3161805847",
                    "DBLP": "conf/flairs/SchwenkeA21",
                    "DOI": "10.32473/FLAIRS.V34I1.128399",
                    "CorpusId": 235350567
                },
                "corpusId": 235350567,
                "publicationVenue": {
                    "id": "546d164a-fc31-4aee-99a5-879e03ff7d36",
                    "name": "The Florida AI Research Society",
                    "type": "conference",
                    "alternate_names": [
                        "Fla AI Res Soc",
                        "FLAIRS"
                    ],
                    "url": "http://www.flairs.com/"
                },
                "url": "https://www.semanticscholar.org/paper/dabe2fae0d92936b7d57be0b5608aa4c32269847",
                "title": "Show Me What You're Looking For Visualizing Abstracted Transformer Attention for Enhancing Their Local Interpretability on Time Series Data",
                "abstract": "While Transformers have shown their advantages consideringtheir learning performance, their lack of explainabilityand interpretability is still a major problem.This specifically relates to the processing of time series,as a specific form of complex data. In this paper,we propose an approach for visualizing abstracted informationin order to enable computational sensemakingand local interpretability on the respective Transformermodel. Our results demonstrate the efficacy ofthe proposed abstraction method and visualization, utilizingboth synthetic and real world data for evaluation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2097670994",
                        "name": "Leonid Schwenke"
                    },
                    {
                        "authorId": "2029677926",
                        "name": "M. Atzmueller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2018), for inducing trustworthiness (Heuer and Breiter 2020) and have also explored the possibility of false privacy guarantees (Pruthi et al. 2019).",
                "Researchers have previously investigated the utility of explanations for various stakeholders (Preece et al. 2018), for inducing trustworthiness (Heuer and Breiter 2020) and have also explored the possibility of false privacy guarantees (Pruthi et al. 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fc266833ac5ea3befe5274f76dbad198d54d8b39",
                "externalIds": {
                    "ArXiv": "2104.08792",
                    "CorpusId": 238259336
                },
                "corpusId": 238259336,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fc266833ac5ea3befe5274f76dbad198d54d8b39",
                "title": "Human-Imitating Metrics for Training and Evaluating Privacy Preserving Emotion Recognition Models Using Sociolinguistic Knowledge",
                "abstract": "Privacy preservation is a crucial component of any real-world application. But, in applications relying on machine learning backends, privacy is challenging because models often capture more than what the model was initially trained for, resulting in the potential leakage of sensitive information. In this paper, we propose an automatic and quantifiable metric that allows us to evaluate humans' perception of a model's ability to preserve privacy with respect to sensitive variables. In this paper, we focus on saliency-based explanations, explanations that highlight regions of the input text, to infer internal workings of a black box model. We use the degree with which differences in interpretation of general vs privacy preserving models correlate with sociolinguistic biases to inform metric design. We show how certain commonly-used methods that seek to preserve privacy do not align with human perception of privacy preservation leading to distrust about model's claims. We demonstrate the versatility of our proposed metric by validating its utility for measuring cross corpus generalization for both privacy and emotion. Finally, we conduct crowdsourcing experiments to evaluate the inclination of the evaluators to choose a particular model for a given purpose when model explanations are provided, and show a positive relationship with the proposed metric. To the best of our knowledge, we take the first step in proposing automatic and quantifiable metrics that best align with human perception of model's ability for privacy preservation, allowing for cost-effective model development.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3036536",
                        "name": "Mimansa Jaiswal"
                    },
                    {
                        "authorId": "2523983",
                        "name": "E. Provost"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This discussion has continued in the interpretable ML literature, with methods demonstrating how attention mechanisms can be useful or deceptive (Zhong et al., 2019; Grimsley et al., 2020; Jain et al., 2020; Pruthi et al., 2020)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "29b13c78d323ba225cfe3038b786a38bcbee4c2f",
                "externalIds": {
                    "ArXiv": "2104.07894",
                    "DBLP": "journals/corr/abs-2104-07894",
                    "CorpusId": 233289496
                },
                "corpusId": 233289496,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/29b13c78d323ba225cfe3038b786a38bcbee4c2f",
                "title": "Faithful and Plausible Explanations of Medical Code Predictions",
                "abstract": "Machine learning models that offer excellent predictive performance often lack the interpretability necessary to support integrated human machine decision-making. In clinical medicine and other high-risk settings, domain experts may be unwilling to trust model predictions without explanations. Work in explainable AI must balance competing objectives along two different axes: 1) Explanations must balance faithfulness to the model's decision-making with their plausibility to a domain expert. 2) Domain experts desire local explanations of individual predictions and global explanations of behavior in aggregate. We propose to train a proxy model that mimics the behavior of the trained model and provides fine-grained control over these trade-offs. We evaluate our approach on the task of assigning ICD codes to clinical notes to demonstrate that explanations from the proxy model are faithful and replicate the trained model behavior.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1411379613",
                        "name": "Zach Wood-Doughty"
                    },
                    {
                        "authorId": "51199773",
                        "name": "Isabel Cachola"
                    },
                    {
                        "authorId": "1478928280",
                        "name": "Mark Dredze"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026research, and although there exist important concerns about the interpretability of attention distributions in transformers (Brunner et al., 2019; Pruthi et al., 2020), methods based on gradient attribution (Pascual et al., 2020) or on attention flow (Abnar and Zuidema, 2020) can provide\u2026",
                "cerns about the interpretability of attention distributions in transformers (Brunner et al., 2019; Pruthi et al., 2020), methods based on gradient attribution (Pascual et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d4d37ad448e845552583658cc2c3928c82b7045b",
                "externalIds": {
                    "ArXiv": "2104.06709",
                    "DBLP": "conf/bionlp/PascualLW21",
                    "ACL": "2021.bionlp-1.6",
                    "DOI": "10.18653/v1/2021.bionlp-1.6",
                    "CorpusId": 233231507
                },
                "corpusId": 233231507,
                "publicationVenue": {
                    "id": "3afb600a-49ad-40aa-858c-081def027584",
                    "name": "Workshop on Biomedical Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "BioNLP",
                        "Workshop Biomed Nat Lang Process"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d4d37ad448e845552583658cc2c3928c82b7045b",
                "title": "Towards BERT-based Automatic ICD Coding: Limitations and Opportunities",
                "abstract": "Automatic ICD coding is the task of assigning codes from the International Classification of Diseases (ICD) to medical notes. These codes describe the state of the patient and have multiple applications, e.g., computer-assisted diagnosis or epidemiological studies. ICD coding is a challenging task due to the complexity and length of medical notes. Unlike the general trend in language processing, no transformer model has been reported to reach high performance on this task. Here, we investigate in detail ICD coding using PubMedBERT, a state-of-the-art transformer model for biomedical language understanding. We find that the difficulty of fine-tuning the model on long pieces of text is the main limitation for BERT-based models on ICD coding. We run extensive experiments and show that despite the gap with current state-of-the-art, pretrained transformers can reach competitive performance using relatively small portions of text. We point at better methods to aggregate information from long texts as the main need for improving BERT-based ICD coding.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150973452",
                        "name": "Damian Pascual"
                    },
                    {
                        "authorId": "66473523",
                        "name": "Sandro Luck"
                    },
                    {
                        "authorId": "2075356250",
                        "name": "Roger Wattenhofer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, it remains uncertain whether attention weights can provide reliable insights into the decision process of the model (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020b)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "715e1b1974d961c9130a20cd09ca974dfeae4472",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-14919",
                    "ArXiv": "2103.14919",
                    "CorpusId": 232404597
                },
                "corpusId": 232404597,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/715e1b1974d961c9130a20cd09ca974dfeae4472",
                "title": "You Can Do Better! If You Elaborate the Reason When Making Prediction",
                "abstract": "Neural predictive models have achieved remarkable performance improvements in various natural language processing tasks. However, most neural predictive models suffer from the lack of explainability of predictions, limiting their practical utility. This paper proposes a neural predictive approach to make a prediction and generate its corresponding explanation simultaneously. It leverages the knowledge entailed in explanations as an additional distillation signal for more efficient learning. We conduct a preliminary study on Chinese medical multiple-choice question answering, English natural language inference, and commonsense question answering tasks. The experimental results show that the proposed approach can generate reasonable explanations for its predictions even with a small-scale training corpus. The proposed method also achieves improved prediction accuracy on three datasets, which indicates that making predictions can benefit from generating the explanation in the decision process.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1664667501",
                        "name": "Dongfang Li"
                    },
                    {
                        "authorId": "10735667",
                        "name": "Jingcong Tao"
                    },
                    {
                        "authorId": "144159781",
                        "name": "Qingcai Chen"
                    },
                    {
                        "authorId": "33968873",
                        "name": "Baotian Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, such interpretations were shown to be unreliable (Serrano and Smith, 2019; Pruthi et al., 2020) and unfaithful (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3aba582b62d1abfcd95264e6c7b32aab4c9db4b8",
                "externalIds": {
                    "ArXiv": "2103.12279",
                    "ACL": "2021.emnlp-main.64",
                    "DBLP": "journals/corr/abs-2103-12279",
                    "DOI": "10.18653/v1/2021.emnlp-main.64",
                    "CorpusId": 232320343
                },
                "corpusId": 232320343,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/3aba582b62d1abfcd95264e6c7b32aab4c9db4b8",
                "title": "SELFEXPLAIN: A Self-Explaining Architecture for Neural Text Classifiers",
                "abstract": "We introduce SelfExplain, a novel self-explaining model that explains a text classifier\u2019s predictions using phrase-based concepts. SelfExplain augments existing neural classifiers by adding (1) a globally interpretable layer that identifies the most influential concepts in the training set for a given sample and (2) a locally interpretable layer that quantifies the contribution of each local input concept by computing a relevance score relative to the predicted label. Experiments across five text-classification datasets show that SelfExplain facilitates interpretability without sacrificing performance. Most importantly, explanations from SelfExplain show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1801149",
                        "name": "Dheeraj Rajagopal"
                    },
                    {
                        "authorId": "143820870",
                        "name": "Vidhisha Balachandran"
                    },
                    {
                        "authorId": "144547315",
                        "name": "E. Hovy"
                    },
                    {
                        "authorId": "145317727",
                        "name": "Yulia Tsvetkov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, in text classification, while standard models typically attend to salient (high gradient influence) words (Serrano and Smith, 2019a), recent work constructs accurate models that attend to irrelevant words instead (Wiegreffe and Pinter, 2019a; Pruthi et al., 2020).",
                "For example, Jain and Wallace (2019); Serrano and Smith (2019b); Wiegreffe and Pinter (2019b); Pruthi et al. (2020) all show that information flows across hidden states."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4a5c7c2afeeadbc4f7c0a9101fe6ed5d8f624506",
                "externalIds": {
                    "ArXiv": "2103.07601",
                    "DBLP": "journals/corr/abs-2103-07601",
                    "CorpusId": 232232786
                },
                "corpusId": 232232786,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4a5c7c2afeeadbc4f7c0a9101fe6ed5d8f624506",
                "title": "Approximating How Single Head Attention Learns",
                "abstract": "Why do models often attend to salient words, and how does this evolve throughout training? We approximate model training as a two stage process: early on in training when the attention weights are uniform, the model learns to translate individual input word `i` to `o` if they co-occur frequently. Later, the model learns to attend to `i` while the correct output is $o$ because it knows `i` translates to `o`. To formalize, we define a model property, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that `i` translates to `o`), and claim that it drives the learning of the attention. This claim is supported by the fact that before the attention mechanism is learned, KTIW can be learned from word co-occurrence statistics, but not the other way around. Particularly, we can construct a training distribution that makes KTIW hard to learn, the learning of the attention fails, and the model cannot even learn the simple task of copying the input words to the output. Our approximation explains why models sometimes attend to salient words, and inspires a toy example where a multi-head attention model can overcome the above hard training distribution by improving learning dynamics rather than expressiveness. We end by discussing the limitation of our approximation framework and suggest future directions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "117111135",
                        "name": "Charles Burton Snell"
                    },
                    {
                        "authorId": "51011000",
                        "name": "Ruiqi Zhong"
                    },
                    {
                        "authorId": "38666915",
                        "name": "D. Klein"
                    },
                    {
                        "authorId": "5164568",
                        "name": "J. Steinhardt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, recent work raises concerns about the interpretability of attention distributions [3], [12], [21], [24]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "36aff757c91d67ece4f1d0b90550f2bc70e58c16",
                "externalIds": {
                    "ArXiv": "2101.04547",
                    "DBLP": "conf/ijcnn/ZhaoPBW21",
                    "DOI": "10.1109/IJCNN52387.2021.9533563",
                    "CorpusId": 231583152
                },
                "corpusId": 231583152,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/36aff757c91d67ece4f1d0b90550f2bc70e58c16",
                "title": "Of Non-Linearity and Commutativity in BERT",
                "abstract": "In this work we provide new insights into the transformer architecture, and in particular, its best-known variant, BERT. First, we propose a method to measure the degree of non-linearity of different elements of transformers. Next, we focus our investigation on the feed-forward networks (FFN) inside transformers, which contain two thirds of the model parameters and have so far not received much attention. We find that FFNs are an inefficient yet important architectural element and that they cannot simply be replaced by attention blocks without a degradation in performance. Moreover, we study the interactions between layers in BERT and show that, while the layers exhibit some hierarchical structure, they extract features in a fuzzy manner. Our results suggest that BERT has an inductive bias towards layer commutativity, which we find is mainly due to the skip connections. This provides a justification for the strong performance of recurrent and weight-shared transformer models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10802981",
                        "name": "Sumu Zhao"
                    },
                    {
                        "authorId": "150973452",
                        "name": "Damian Pascual"
                    },
                    {
                        "authorId": "38094934",
                        "name": "Gino Brunner"
                    },
                    {
                        "authorId": "1716440",
                        "name": "Roger Wattenhofer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Though ubiquitous, token scores may not always reflect their real importance (Pruthi et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "15e71e497a67423bfedd0d63efe423c4660e5053",
                "externalIds": {
                    "ArXiv": "2101.00288",
                    "DBLP": "conf/acl/WuRHW20",
                    "ACL": "2021.acl-long.523",
                    "DOI": "10.18653/v1/2021.acl-long.523",
                    "CorpusId": 235266322
                },
                "corpusId": 235266322,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/15e71e497a67423bfedd0d63efe423c4660e5053",
                "title": "Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models",
                "abstract": "While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35232494",
                        "name": "Tongshuang Sherry Wu"
                    },
                    {
                        "authorId": "78846919",
                        "name": "Marco Tulio Ribeiro"
                    },
                    {
                        "authorId": "1803140",
                        "name": "Jeffrey Heer"
                    },
                    {
                        "authorId": "1780531",
                        "name": "Daniel S. Weld"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026to/from Human As for human consuming attention as explanation, there has been criticism that unsupervised attention weights are too poorly correlated with the contribution of each word for machine decision (or, unfaithful) (Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2019).",
                "tion of each word for machine decision (or, unfaithful) (Jain and Wallace, 2019; Serrano and Smith, 2019; Pruthi et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "43420e2df152a58db57de8368c7a7ac2911019be",
                "externalIds": {
                    "ACL": "2020.emnlp-main.543",
                    "MAG": "3104564752",
                    "DBLP": "conf/emnlp/ChoiPYH20",
                    "DOI": "10.18653/v1/2020.emnlp-main.543",
                    "CorpusId": 226262267
                },
                "corpusId": 226262267,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/43420e2df152a58db57de8368c7a7ac2911019be",
                "title": "Less Is More: Attention Supervision with Counterfactuals for Text Classification",
                "abstract": "We aim to leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision. Specifically, for this goal, we explore the advantage of counterfactual reasoning, over associative reasoning typically used in attention supervision. Our empirical results show that this machine-augmented human attention supervision is more effective than existing methods requiring a higher annotation cost, in text classification tasks, including sentiment analysis and news categorization.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "5841595",
                        "name": "Seungtaek Choi"
                    },
                    {
                        "authorId": "74371835",
                        "name": "Haeju Park"
                    },
                    {
                        "authorId": "1898428",
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "authorId": "1716415",
                        "name": "Seung-won Hwang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Conversational Question Answering (ConvQA) is a new question answering task that requires a comprehension of the context, which has recently received more and more attention (Zhu et al., 2018; Qu et al., 2019a; Qu et al., 2019b; Meng et al., 2019; Pruthi et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "627e925371e9c0e6c72600f349dd284d31718e7b",
                "externalIds": {
                    "ACL": "2020.ccl-1.80",
                    "MAG": "3103359078",
                    "DBLP": "conf/cncl/JingHZXT20",
                    "DOI": "10.1007/978-3-030-63031-7_5",
                    "CorpusId": 225062758
                },
                "corpusId": 225062758,
                "publicationVenue": {
                    "id": "0242a0a8-eac8-4d42-a284-4789a579aa9b",
                    "name": "China National Conference on Chinese Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "CCL",
                        "Constraint Comput Log",
                        "China National Conf Chin Comput Linguistics",
                        "Constraints in Computational Logics"
                    ],
                    "issn": "0319-0080",
                    "url": "http://ccl.uwinnipeg.ca/",
                    "alternate_urls": [
                        "http://ps-www.dfki.uni-sb.de/ccl/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/627e925371e9c0e6c72600f349dd284d31718e7b",
                "title": "Combining Impression Feature Representation for Multi-turn Conversational Question Answering",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "32466879",
                        "name": "Shaoling Jing"
                    },
                    {
                        "authorId": "2213156086",
                        "name": "Shibo Hong"
                    },
                    {
                        "authorId": "144060462",
                        "name": "Dongyan Zhao"
                    },
                    {
                        "authorId": "2107669",
                        "name": "Haihua Xie"
                    },
                    {
                        "authorId": "143830636",
                        "name": "Zhi Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The robustness of explanations is studied in [1, 10, 30, 44, 48], but none of them is for Shapley values on MRF.",
                "The above answer to the T-Contrast questions can be used to study the robustness of Shapley values when the input MRF is (adversarially) perturbed, a situation that has been studied for other explanations in [1, 10, 30, 44, 48]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1b2276a9b052cba6ba8dcaebb7e75cafda4d8414",
                "externalIds": {
                    "DBLP": "conf/cikm/LiuCLZX20",
                    "MAG": "3094506236",
                    "DOI": "10.1145/3340531.3411881",
                    "CorpusId": 224270860
                },
                "corpusId": 224270860,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1b2276a9b052cba6ba8dcaebb7e75cafda4d8414",
                "title": "Shapley Values and Meta-Explanations for Probabilistic Graphical Model Inference",
                "abstract": "Probabilistic graphical models, such as Markov random fields (MRF), exploit dependencies among random variables to model a rich family of joint probability distributions. Inference algorithms, such as belief propagation (BP), can effectively compute the marginal posteriors for decision making. Nonetheless, inferences involve sophisticated probability calculations and are difficult for humans to interpret. Among all existing explanation methods for MRFs, no method is designed for fair attributions of an inference outcome to elements on the MRF where the inference takes place. Shapley values provide rigorous attributions but so far have not been studied on MRFs. We thus define Shapley values for MRFs to capture both probabilistic and topological contributions of the variables on MRFs. We theoretically characterize the new definition regarding independence, equal contribution, additivity, and submodularity. As brute-force computation of the Shapley values is challenging, we propose GraphShapley, an approximation algorithm that exploits the decomposability of Shapley values, the structure of MRFs, and the iterative nature of BP inference to speed up the computation. In practice, we propose meta-explanations to explain the Shapley values and make them more accessible and trustworthy to human users. On four synthetic and nine real-world MRFs, we demonstrate that GraphShapley generates sensible and practical explanations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2108101960",
                        "name": "Yifei Liu"
                    },
                    {
                        "authorId": "2145762275",
                        "name": "Chao Chen"
                    },
                    {
                        "authorId": "2144399347",
                        "name": "Yazheng Liu"
                    },
                    {
                        "authorId": "2108286283",
                        "name": "Xi Zhang"
                    },
                    {
                        "authorId": "3131378",
                        "name": "Sihong Xie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2017) models in an attempt to make it more interpretable (Pruthi et al. 2020; Brunner et al. 2020).",
                "\u2026for NLP models have focused on explaining individual predictions by using gradient-based saliency maps over the input text (Lei, Barzilay, and Jaakkola 2016; Ribeiro, Singh, and Guestrin 2018; Bastings, Aziz, and Titov 2019) or interpreting attention (Brunner et al. 2020; Pruthi et al. 2020).",
                "Prior interpretability techniques for NLP models have focused on explaining individual predictions by using gradient-based saliency maps over the input text (Lei, Barzilay, and Jaakkola 2016; Ribeiro, Singh, and Guestrin 2018; Bastings, Aziz, and Titov 2019) or interpreting attention (Brunner et al. 2020; Pruthi et al. 2020).",
                "Recent work has manipulated attention in Transformer (Vaswani et al. 2017) models in an attempt to make it more interpretable (Pruthi et al. 2020; Brunner et al. 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "92e0e66919569070196c951a71968414e5e01381",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-09030",
                    "ArXiv": "2010.09030",
                    "MAG": "3093205559",
                    "CorpusId": 224705350
                },
                "corpusId": 224705350,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/92e0e66919569070196c951a71968414e5e01381",
                "title": "Explaining and Improving Model Behavior with k Nearest Neighbor Representations",
                "abstract": "Interpretability techniques in NLP have mainly focused on understanding individual predictions using attention visualization or gradient-based saliency maps over tokens. We propose using k nearest neighbor (kNN) representations to identify training examples responsible for a model's predictions and obtain a corpus-level understanding of the model's behavior. Apart from interpretability, we show that kNN representations are effective at uncovering learned spurious associations, identifying mislabeled examples, and improving the fine-tuned model's performance. We focus on Natural Language Inference (NLI) as a case study and experiment with multiple datasets. Our method deploys backoff to kNN for BERT and RoBERTa on examples with low model confidence without any update to the model parameters. Our results indicate that the kNN approach makes the finetuned model more robust to adversarial inputs.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "8937909",
                        "name": "Nazneen Rajani"
                    },
                    {
                        "authorId": "9340968",
                        "name": "Ben Krause"
                    },
                    {
                        "authorId": "1999221145",
                        "name": "Wengpeng Yin"
                    },
                    {
                        "authorId": "144412704",
                        "name": "Tong Niu"
                    },
                    {
                        "authorId": "2166511",
                        "name": "R. Socher"
                    },
                    {
                        "authorId": "2228109",
                        "name": "Caiming Xiong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This complete reliance on attention mechanism is problematic as the MFN assumes synchronous inputs which is hard to achieve in real-world scenarios, and more importantly, the reliability of attentionmemory to discover inter-modal interactions is questionable as shown with deceptive attention masks in [7].",
                "Their complete reliance on attention scheme is problematic due to two reasons: 1) they have deceptive attention masks (in MFN), and hence it is obscure whether the gain in prediction is attributable to inter-modal interactions [7] and, 2) the role of training dynamics (in MARN) instead of multiple-heads [8]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "faddf17f65376da3002579fd5fe526041d8ad219",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-08218",
                    "MAG": "3093127707",
                    "ArXiv": "2010.08218",
                    "DOI": "10.1109/ICDM50108.2020.00065",
                    "CorpusId": 223956749
                },
                "corpusId": 223956749,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/faddf17f65376da3002579fd5fe526041d8ad219",
                "title": "Deep-HOSeq: Deep Higher Order Sequence Fusion for Multimodal Sentiment Analysis",
                "abstract": "Multimodal sentiment analysis utilizes multiple heterogeneous modalities for sentiment classification. The recent multimodal fusion schemes customize LSTMs to discover intra-modal dynamics and design sophisticated attention mechanisms to discover the inter-modal dynamics from multimodal sequences. Although powerful, these schemes completely rely on attention mechanisms which is problematic due to two major drawbacks 1) deceptive attention masks, and 2) training dynamics. Nevertheless, strenuous efforts are required to optimize hyperparameters of these consolidate architectures, in particular their custom-designed LSTMs constrained by attention schemes. In this research, we first propose a common network to discover both intra-modal and inter-modal dynamics by utilizing basic LSTMs and tensor based convolution networks. We then propose unique networks to encapsulate temporal-granularity among the modalities which is essential while extracting information within asynchronous sequences. We then integrate these two kinds of information via a fusion layer and call our novel multimodal fusion scheme as Deep-HOSeq (Deep network with higher order Common and Unique Sequence information). The proposed Deep-HOSeq efficiently discovers all-important information from multimodal sequences and the effectiveness of utilizing both types of information is empirically demonstrated on CMU-MOSEI and CMU-MOSI benchmark datasets. The source code of proposed Deep-HOSeq is available at https://github.com/sverma88/Deep-HOSeq-ICDM-2020.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3455244",
                        "name": "Sunny Verma"
                    },
                    {
                        "authorId": "2110191570",
                        "name": "Jiwei Wang"
                    },
                    {
                        "authorId": "1998943485",
                        "name": "Zhefeng Ge"
                    },
                    {
                        "authorId": "2042694003",
                        "name": "Rujia Shen"
                    },
                    {
                        "authorId": "2068092120",
                        "name": "Fan Jin"
                    },
                    {
                        "authorId": "2153676016",
                        "name": "Yang Wang"
                    },
                    {
                        "authorId": "2149502355",
                        "name": "Fang Chen"
                    },
                    {
                        "authorId": "2157221259",
                        "name": "Wei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pruthi et al. (2020) manipulate attention distributions in\nan end-to-end fashion; we focus on manipulating gradients.",
                "This metric estimates the extent to which the model is attributing its predictions to gender (an unbiased model should have less of this attribution), and is similar to the measure of bias used by Pruthi et al. (2020).",
                "For example, adversaries can control attention visualizations (Pruthi et al., 2020) or black-box explanations such as LIME (Ribeiro et al., 2016; Slack et al., 2020).",
                "A.4 Biosbias Details\nWe follow the setup of Pruthi et al. (2020) and only use examples with the labels of \u201cphysician\u201d and \u201csurgeon\u201d.",
                "For example, adversaries can control attention visualizations (Pruthi et al., 2020) or black-box explanations such as LIME (Ribeiro et al."
            ],
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f7b3e4b5a8e15585d09398a056bdf0b43bed68ca",
                "externalIds": {
                    "MAG": "3093901252",
                    "DBLP": "conf/emnlp/WangTW020",
                    "ArXiv": "2010.05419",
                    "ACL": "2020.findings-emnlp.24",
                    "DOI": "10.18653/v1/2020.findings-emnlp.24",
                    "CorpusId": 222290633
                },
                "corpusId": 222290633,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f7b3e4b5a8e15585d09398a056bdf0b43bed68ca",
                "title": "Gradient-based Analysis of NLP Models is Manipulable",
                "abstract": "Gradient-based analysis methods, such as saliency map visualizations and adversarial input perturbations, have found widespread use in interpreting neural NLP models due to their simplicity, flexibility, and most importantly, the fact that they directly reflect the model internals. In this paper, however, we demonstrate that the gradients of a model are easily manipulable, and thus bring into question the reliability of gradient-based analyses. In particular, we merge the layers of a target model with a Facade Model that overwhelms the gradients without affecting the predictions. This Facade Model can be trained to have gradients that are misleading and irrelevant to the task, such as focusing only on the stop words in the input. On a variety of NLP tasks (sentiment analysis, NLI, and QA), we show that the merged model effectively fools different analysis tools: saliency maps differ significantly from the original model\u2019s, input reduction keeps more irrelevant input tokens, and adversarial perturbations identify unimportant tokens as being highly important.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49606614",
                        "name": "Junlin Wang"
                    },
                    {
                        "authorId": "1388109456",
                        "name": "Jens Tuyls"
                    },
                    {
                        "authorId": "145217343",
                        "name": "Eric Wallace"
                    },
                    {
                        "authorId": "34650964",
                        "name": "Sameer Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, Pruthi et al. (2020) propose a method to produce deceptive attention weights."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "508884a136a461869be128027950d2aa1778518c",
                "externalIds": {
                    "ArXiv": "2010.05607",
                    "DBLP": "journals/corr/abs-2010-05607",
                    "MAG": "3092292656",
                    "ACL": "2020.blackboxnlp-1.14",
                    "DOI": "10.18653/V1/2020.BLACKBOXNLP-1.14",
                    "CorpusId": 222291117
                },
                "corpusId": 222291117,
                "publicationVenue": {
                    "id": "738626d7-5b8c-497d-9fd6-64bdb6dbf440",
                    "name": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
                    "type": "conference",
                    "alternate_names": [
                        "BlackboxNLP",
                        "Blackboxnlp Workshop Anal Interpr\u00e8t Neural Netw NLP"
                    ],
                    "url": "https://aclanthology.org/venues/blackboxnlp/"
                },
                "url": "https://www.semanticscholar.org/paper/508884a136a461869be128027950d2aa1778518c",
                "title": "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?",
                "abstract": "There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1994065972",
                        "name": "Jasmijn Bastings"
                    },
                    {
                        "authorId": "3017324",
                        "name": "Katja Filippova"
                    }
                ]
            }
        },
        {
            "contexts": [
                "But these weights do not reliably correlate with model predictions, making them unsuitable for explainability (Pruthi et al., 2020; Serrano and Smith, 2019; Jain and Wallace, 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c4dba2263bb636c3a0a55e87a445094217c024b4",
                "externalIds": {
                    "MAG": "3167841296",
                    "ACL": "2021.naacl-main.297",
                    "DBLP": "conf/naacl/PrabhumoyeBSB21",
                    "ArXiv": "2010.04658",
                    "DOI": "10.18653/V1/2021.NAACL-MAIN.297",
                    "CorpusId": 222272049
                },
                "corpusId": 222272049,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/c4dba2263bb636c3a0a55e87a445094217c024b4",
                "title": "Case Study: Deontological Ethics in NLP",
                "abstract": "Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "9358910",
                        "name": "Shrimai Prabhumoye"
                    },
                    {
                        "authorId": "2906717",
                        "name": "Brendon Boldt"
                    },
                    {
                        "authorId": "145124475",
                        "name": "R. Salakhutdinov"
                    },
                    {
                        "authorId": "1690706",
                        "name": "A. Black"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "14cf0b593c52dabddc3519865ad0eedc4ee9896d",
                "externalIds": {
                    "MAG": "3194691365",
                    "DOI": "10.2139/ssrn.3668444",
                    "CorpusId": 238893627
                },
                "corpusId": 238893627,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/14cf0b593c52dabddc3519865ad0eedc4ee9896d",
                "title": "Conceptual challenges for interpretable machine learning",
                "abstract": "As machine learning has gradually entered into ever more sectors of public and private life, there has been a growing demand for algorithmic explainability. How can we make the predictions of complex statistical models more intelligible to end users? A subdiscipline of computer science known as interpretable machine learning (IML) has emerged to address this urgent question. Numerous influential methods have been proposed, from local linear approximations to rule lists and counterfactuals. In this article, I highlight three conceptual challenges that are largely overlooked by authors in this area. I argue that the vast majority of IML algorithms are plagued by (1) ambiguity with respect to their true target; (2) a disregard for error rates and severe testing; and (3) an emphasis on product over process. Each point is developed at length, drawing on relevant debates in epistemology and philosophy of science. Examples and counterexamples from IML are considered, demonstrating how failure to acknowledge these problems can result in counterintuitive and potentially misleading explanations. Without greater care for the conceptual foundations of IML, future work in this area is doomed to repeat the same mistakes.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2065079791",
                        "name": "David Watson"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5eb027afa4eb8a5a4fafa626231603a7d3b8adeb",
                "externalIds": {
                    "MAG": "3109438089",
                    "DOI": "10.1109/ICEI49372.2020.00032",
                    "CorpusId": 227278999
                },
                "corpusId": 227278999,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5eb027afa4eb8a5a4fafa626231603a7d3b8adeb",
                "title": "Multi-Granular BERT: An Interpretable Model Applicable to Internet-of-Thing devices",
                "abstract": "With the development of the Energy Internet (EI), its applications have gradually spread from industrial uses to smart homes. Specifically, home Internet of Things(IoT) devices have become popular in the field of smart homes. In this paper, we propose an interpretable model that can be applied on the IoT devices. When Chinese characters are grouped into words, the meaning may vary. Inspired by the observation, we convert character-level Bi-directional Transformer (BERT) to word-level, which we call it multi-granular BERT (MLGB). It constructs the n-gram representation of different lengths within a model. It also learns the self-attention between n-grams during both pre-training and task-specific fine-tuning to learn both the word representation and word-word self-attention at the same time. As a diagnostic task, we evaluate our model on two Chinese text pair classification tasks and observe the model\u2019s behavior. The MLGB retains the BERT\u2019s accuracy on the tasks while demonstrates more interpretable word-level self-attention. Multi-granularity may also have served as a regularization of attention that alleviates the non-identifiability issue of self-attention.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2031505448",
                        "name": "Sihao Xu"
                    },
                    {
                        "authorId": "31765881",
                        "name": "Wei Zhang"
                    },
                    {
                        "authorId": "2153305486",
                        "name": "Fan Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Depending on the task and model architecture, attention may have more or less explanatory power for model predictions [35, 51, 57, 71, 79]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2b364917b0c51e91fcf2ab9c1d66a14ed4b44c03",
                "externalIds": {
                    "MAG": "3037888463",
                    "DBLP": "journals/corr/abs-2006-15222",
                    "ArXiv": "2006.15222",
                    "DOI": "10.1101/2020.06.26.174417",
                    "CorpusId": 220249726
                },
                "corpusId": 220249726,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/2b364917b0c51e91fcf2ab9c1d66a14ed4b44c03",
                "title": "BERTology Meets Biology: Interpreting Attention in Protein Language Models",
                "abstract": "Transformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. Through the lens of attention, we analyze the inner workings of the Transformer and explore how the model discerns structural and functional properties of proteins. We show that attention (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We also present a three-dimensional visualization of the interaction between attention and protein structure. Our findings align with known biological processes and provide a tool to aid discovery in protein engineering and synthetic biology. The code for visualization and analysis is available at https://github.com/salesforce/provis.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2056908",
                        "name": "Jesse Vig"
                    },
                    {
                        "authorId": "145822841",
                        "name": "Ali Madani"
                    },
                    {
                        "authorId": "1697944",
                        "name": "L. Varshney"
                    },
                    {
                        "authorId": "2228109",
                        "name": "Caiming Xiong"
                    },
                    {
                        "authorId": "2166511",
                        "name": "R. Socher"
                    },
                    {
                        "authorId": "8937909",
                        "name": "Nazneen Rajani"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c014f8bc3b521453a93a13bb2c90700fcf462738",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-12467",
                    "MAG": "3036369012",
                    "ArXiv": "2006.12467",
                    "CorpusId": 219965648
                },
                "corpusId": 219965648,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c014f8bc3b521453a93a13bb2c90700fcf462738",
                "title": "Limits to Depth Efficiencies of Self-Attention",
                "abstract": "Self-attention architectures, which are rapidly pushing the frontier in natural language processing, demonstrate a surprising depth-inefficient behavior: Empirical signals indicate that increasing the internal representation (network width) is just as useful as increasing the number of self-attention layers (network depth). In this paper, we theoretically study the interplay between depth and width in self-attention, and shed light on the root of the above phenomenon. We invalidate the seemingly plausible hypothesis by which widening is as effective as deepening for self-attention, and show that in fact stacking self-attention layers is so effective that it quickly saturates a capacity of the network width. Specifically, we pinpoint a \"depth threshold\" that is logarithmic in $d_x$, the network width: $L_{\\textrm{th}}=\\log_{3}(d_x)$. For networks of depth that is below the threshold, we establish a double-exponential depth-efficiency of the self-attention operation, while for depths over the threshold we show that depth-inefficiency kicks in. Our predictions strongly accord with extensive empirical ablations in Kaplan et al. (2020), accounting for the different behaviors in the two depth-(in)efficiency regimes. By identifying network width as a limiting factor, our analysis indicates that solutions for dramatically increasing the width can facilitate the next leap in self-attention expressivity.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "152754428",
                        "name": "Yoav Levine"
                    },
                    {
                        "authorId": "69978543",
                        "name": "Noam Wies"
                    },
                    {
                        "authorId": "3074811",
                        "name": "Or Sharir"
                    },
                    {
                        "authorId": "1753618003",
                        "name": "Hofit Bata"
                    },
                    {
                        "authorId": "3140335",
                        "name": "A. Shashua"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent work on the faithfulness of attention heat-maps (Baan et al., 2019; Pruthi et al., 2019; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) or saliency distributions (AlvarezMelis and Jaakkola, 2018; Kindermans et al., 2019) cast doubt on their faithfulness as\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "86471bf927401bf88af83626797228c2bf10a282",
                "externalIds": {
                    "ArXiv": "2006.01067",
                    "DBLP": "journals/tacl/JacoviG21",
                    "MAG": "3032150215",
                    "DOI": "10.1162/tacl_a_00367",
                    "CorpusId": 219177071
                },
                "corpusId": 219177071,
                "publicationVenue": {
                    "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
                    "name": "Transactions of the Association for Computational Linguistics",
                    "type": "journal",
                    "alternate_names": [
                        "Trans Assoc Comput Linguistics",
                        "TACL"
                    ],
                    "issn": "2307-387X",
                    "url": "https://www.mitpressjournals.org/loi/tacl",
                    "alternate_urls": [
                        "http://www.transacl.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/86471bf927401bf88af83626797228c2bf10a282",
                "title": "Aligning Faithful Interpretations with their Social Attribution",
                "abstract": "Abstract We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We reformulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution together complete the process of explaining behavior. With this formalization, we characterize various failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we implement highlight explanations of the proposed causal format using contrastive explanations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "41016275",
                        "name": "Alon Jacovi"
                    },
                    {
                        "authorId": "79775260",
                        "name": "Yoav Goldberg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The work of Pruthi et al. (2019) emphasizes the threat of interpreting models through attention weights, as they show a regularization term can be introduced to guide the attention weights away from focusing on subsets of words while retaining model accuracy, implying that models which exploit bias\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10f38aff84cf736706bd018696ddd0d3baed081d",
                "externalIds": {
                    "MAG": "3027886415",
                    "DBLP": "journals/corr/abs-2005-09379",
                    "ACL": "2020.repl4nlp-1.17",
                    "ArXiv": "2005.09379",
                    "DOI": "10.18653/v1/2020.repl4nlp-1.17",
                    "CorpusId": 218684573
                },
                "corpusId": 218684573,
                "publicationVenue": {
                    "id": "8b169440-4c13-4cf4-b3f9-1dc7c39dc888",
                    "name": "Workshop on Representation Learning for NLP",
                    "type": "conference",
                    "alternate_names": [
                        "RepL4NLP",
                        "Workshop Represent Learn NLP"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/10f38aff84cf736706bd018696ddd0d3baed081d",
                "title": "Staying True to Your Word: (How) Can Attention Become Explanation?",
                "abstract": "The attention mechanism has quickly become ubiquitous in NLP. In addition to improving performance of models, attention has been widely used as a glimpse into the inner workings of NLP models. The latter aspect has in the recent years become a common topic of discussion, most notably in recent work of Jain and Wallace; Wiegreffe and Pinter. With the shortcomings of using attention weights as a tool of transparency revealed, the attention mechanism has been stuck in a limbo without concrete proof when and whether it can be used as an explanation. In this paper, we provide an explanation as to why attention has seen rightful critique when used with recurrent networks in sequence classification tasks. We propose a remedy to these issues in the form of a word level objective and our findings give credibility for attention to provide faithful interpretations of recurrent models.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3466460",
                        "name": "Martin Tutek"
                    },
                    {
                        "authorId": "143809437",
                        "name": "J. \u0160najder"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, the above formulation (referred in literature as \u201cLuong attention\u201d) is most widely used in text classification tasks (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020).",
                "Recently, Pruthi et al. (2020) conjecture that attention offers benefits during training; our work explains, and provides empirical evidence to support the speculation."
            ],
            "intents": [
                "result",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "90ddd0cdef3144c0c25bfdf144d58fe07dffee15",
                "externalIds": {
                    "ArXiv": "2005.00159",
                    "DBLP": "conf/emnlp/MainiKPM20",
                    "MAG": "3022252430",
                    "ACL": "2020.findings-emnlp.410",
                    "DOI": "10.18653/v1/2020.findings-emnlp.410",
                    "CorpusId": 218470367
                },
                "corpusId": 218470367,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/90ddd0cdef3144c0c25bfdf144d58fe07dffee15",
                "title": "Why and when should you pool? Analyzing Pooling in Recurrent Architectures",
                "abstract": "Pooling-based recurrent neural architectures consistently outperform their counterparts without pooling on sequence classification tasks. However, the reasons for their enhanced performance are largely unexamined. In this work, we examine three commonly used pooling techniques (mean-pooling, max-pooling, and attention, and propose *max-attention*, a novel variant that captures interactions among predictive tokens in a sentence. Using novel experiments, we demonstrate that pooling architectures substantially differ from their non-pooling equivalents in their learning ability and positional biases: (i) pooling facilitates better gradient flow than BiLSTMs in initial training epochs, and (ii) BiLSTMs are biased towards tokens at the beginning and end of the input, whereas pooling alleviates this bias. Consequently, we find that pooling yields large gains in low resource scenarios, and instances when salient words lie towards the middle of the input. Across several text classification tasks, we find max-attention to frequently outperform other pooling techniques.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "153742303",
                        "name": "Pratyush Maini"
                    },
                    {
                        "authorId": "1500421125",
                        "name": "Keshav Kolluru"
                    },
                    {
                        "authorId": "7880098",
                        "name": "Danish Pruthi"
                    },
                    {
                        "authorId": "2674444",
                        "name": "Mausam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although it is wrong to equate attention with explanation (Pruthi et al., 2019; Jain and Wallace, 2019), it can offer plausible and meaningful interpretations (Wiegreffe and Pinter, 2019; Vashishth et al., 2019; Vig, 2019).",
                "Although it is wrong to equate attention with explanation (Pruthi et al., 2019; Jain and Wallace, 2019), it can offer plausible and meaningful interpretations (Wiegreffe and Pinter, 2019; Vashishth et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "76a9f336481b39515d6cea2920696f11fb686451",
                "externalIds": {
                    "MAG": "3022265721",
                    "ACL": "2020.acl-main.385",
                    "ArXiv": "2005.00928",
                    "DBLP": "conf/acl/AbnarZ20",
                    "DOI": "10.18653/v1/2020.acl-main.385",
                    "CorpusId": 218487351
                },
                "corpusId": 218487351,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/76a9f336481b39515d6cea2920696f11fb686451",
                "title": "Quantifying Attention Flow in Transformers",
                "abstract": "In the Transformer model, \u201cself-attention\u201d combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2786352",
                        "name": "Samira Abnar"
                    },
                    {
                        "authorId": "83390207",
                        "name": "W. Zuidema"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b2a839e3ee68e81b863b73ee08c6626c94477fef",
                "externalIds": {
                    "ArXiv": "2004.14546",
                    "MAG": "3023690688",
                    "DBLP": "journals/corr/abs-2004-14546",
                    "CorpusId": 216867225
                },
                "corpusId": 216867225,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b2a839e3ee68e81b863b73ee08c6626c94477fef",
                "title": "WT5?! Training Text-to-Text Models to Explain their Predictions",
                "abstract": "Neural networks have recently achieved human-level performance on various challenging natural language processing (NLP) tasks, but it is notoriously difficult to understand why a neural network produced a particular prediction. In this paper, we leverage the text-to-text framework proposed by Raffel et al.(2019) to train language models to output a natural text explanation alongside their prediction. Crucially, this requires no modifications to the loss function or training and decoding procedures -- we simply train the model to output the explanation after generating the (natural text) prediction. We show that this approach not only obtains state-of-the-art results on explainability benchmarks, but also permits learning from a limited set of labeled explanations and transferring rationalization abilities across datasets. To facilitate reproducibility and future work, we release our code use to train the models.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46617804",
                        "name": "Sharan Narang"
                    },
                    {
                        "authorId": "2402716",
                        "name": "Colin Raffel"
                    },
                    {
                        "authorId": "3844009",
                        "name": "Katherine Lee"
                    },
                    {
                        "authorId": "145625142",
                        "name": "Adam Roberts"
                    },
                    {
                        "authorId": "22640071",
                        "name": "Noah Fiedel"
                    },
                    {
                        "authorId": "1666667717",
                        "name": "Karishma Malkan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, without faithful explanations, we cannot know whether a model is exploiting sensitive features such as gender (Pruthi et al., 2020).",
                "In pa ticular, existing feature attributio methods m y n t provide robust, faithful explanations (Feng et al., 2018; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Brunner et al., 2020; Zhong et al., 2019; Pruthi et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "922e6e3bafe38a712597c05d3a907bd10763b427",
                "externalIds": {
                    "DBLP": "conf/acl/JainWPW20",
                    "ACL": "2020.acl-main.409",
                    "ArXiv": "2005.00115",
                    "MAG": "3023638010",
                    "DOI": "10.18653/v1/2020.acl-main.409",
                    "CorpusId": 218470359
                },
                "corpusId": 218470359,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/922e6e3bafe38a712597c05d3a907bd10763b427",
                "title": "Learning to Faithfully Rationalize by Construction",
                "abstract": "In many settings it is important for one to be able to understand why a model made a particular prediction. In NLP this often entails extracting snippets of an input text \u2018responsible for\u2019 corresponding model output; when such a snippet comprises tokens that indeed informed the model\u2019s prediction, it is a faithful explanation. In some settings, faithfulness may be critical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules. However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning. We propose a simpler variant of this approach that provides faithful explanations by construction. In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict. An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to \u2018end-to-end\u2019 approaches, while being more general and easier to train. Code is available at https://github.com/successar/FRESH.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49837811",
                        "name": "Sarthak Jain"
                    },
                    {
                        "authorId": "35823986",
                        "name": "Sarah Wiegreffe"
                    },
                    {
                        "authorId": "1826312",
                        "name": "Yuval Pinter"
                    },
                    {
                        "authorId": "1912476",
                        "name": "Byron C. Wallace"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c30438e316043c6f7288851fea7b663dcbf8638",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-12524",
                    "ArXiv": "2004.12524",
                    "MAG": "3018289717",
                    "CorpusId": 216553635
                },
                "corpusId": 216553635,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0c30438e316043c6f7288851fea7b663dcbf8638",
                "title": "Sequential Interpretability: Methods, Applications, and Future Direction for Understanding Deep Learning Models in the Context of Sequential Data",
                "abstract": "Deep learning continues to revolutionize an ever-growing number of critical application areas including healthcare, transportation, finance, and basic sciences. Despite their increased predictive power, model transparency and human explainability remain a significant challenge due to the \"black box\" nature of modern deep learning models. In many cases the desired balance between interpretability and performance is predominately task specific. Human-centric domains such as healthcare necessitate a renewed focus on understanding how and why these frameworks are arriving at critical and potentially life-or-death decisions. Given the quantity of research and empirical successes of deep learning for computer vision, most of the existing interpretability research has focused on image processing techniques. Comparatively, less attention has been paid to interpreting deep learning frameworks using sequential data. Given recent deep learning advancements in highly sequential domains such as natural language processing and physiological signal processing, the need for deep sequential explanations is at an all-time high. In this paper, we review current techniques for interpreting deep learning techniques involving sequential data, identify similarities to non-sequential methods, and discuss current limitations and future avenues of sequential interpretability research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3383528",
                        "name": "B. Shickel"
                    },
                    {
                        "authorId": "1715006",
                        "name": "Parisa Rashidi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "the models (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Vashishth et al., 2019)."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2a8e42995caaedadc9dc739d85bed2c57fc78568",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-10102",
                    "ArXiv": "2004.10102",
                    "MAG": "3018642446",
                    "CorpusId": 216036002
                },
                "corpusId": 216036002,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2a8e42995caaedadc9dc739d85bed2c57fc78568",
                "title": "Attention Module is Not Only a Weight: Analyzing Transformers with Vector Norms",
                "abstract": "Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Goro Kobayashi"
                    },
                    {
                        "authorId": "83446147",
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "authorId": "32286159",
                        "name": "Sho Yokoi"
                    },
                    {
                        "authorId": "3040648",
                        "name": "Kentaro Inui"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thus, it is more similar to attention as an explanation (Serrano and Smith, 2019; Jain and Wallace, 2019; Pruthi et al., 2019), where the attention structure does not necessarily reveal what tokens are used for prediction."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1e391e60081dcb148cc2732e1f8aa26655fef31e",
                "externalIds": {
                    "MAG": "3015770160",
                    "ArXiv": "2004.05569",
                    "DBLP": "journals/corr/abs-2004-05569",
                    "CorpusId": 215745173
                },
                "corpusId": 215745173,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1e391e60081dcb148cc2732e1f8aa26655fef31e",
                "title": "Explaining Question Answering Models through Text Generation",
                "abstract": "Large pre-trained language models (LMs) have been shown to perform surprisingly well when fine-tuned on tasks that require commonsense and world knowledge. However, in end-to-end architectures, it is difficult to explain what is the knowledge in the LM that allows it to make a correct prediction. In this work, we propose a model for multi-choice question answering, where a LM-based generator generates a textual hypothesis that is later used by a classifier to answer the question. The hypothesis provides a window into the information used by the fine-tuned LM that can be inspected by humans. A key challenge in this setup is how to constrain the model to generate hypotheses that are meaningful to humans. We tackle this by (a) joint training with a simple similarity classifier that encourages meaningful hypotheses, and (b) by adding loss functions that encourage natural text without repetitions. We show on several tasks that our model reaches performance that is comparable to end-to-end architectures, while producing hypotheses that elucidate the knowledge used by the LM for answering the question.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "29995084",
                        "name": "Veronica Latcinnik"
                    },
                    {
                        "authorId": "1750652",
                        "name": "Jonathan Berant"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent work (Brunner et al., 2020; Pruthi et al., 2019) has raised concerns about the interpretability of attention maps as representative of global context aggregation.",
                "The ability of attention distributions to provide explanations has been the target of a number of studies (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Pruthi et al., 2019).",
                "6 HTA: Local Validation The ability of attention distributions to provide explanations has been the target of a number of studies (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Pruthi et al., 2019).",
                "Pruthi et al. (2019) show that, just as in other attention models, it is possible to manipulate self-attention in transformers in order to generate different attention masks that cause only a small drop in performance.",
                "However, recent studies (Brunner et al., 2020; Pruthi et al., 2019) question the ability of attention maps to provide a faithful explanation of the inner workings of transformer models."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3d9605a8b6364d24b58be561ea1ce4233c088cee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-05916",
                    "MAG": "3015283273",
                    "ArXiv": "2004.05916",
                    "ACL": "2021.eacl-main.9",
                    "DOI": "10.18653/v1/2021.eacl-main.9",
                    "CorpusId": 215745034
                },
                "corpusId": 215745034,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/3d9605a8b6364d24b58be561ea1ce4233c088cee",
                "title": "Telling BERT\u2019s Full Story: from Local Attention to Global Aggregation",
                "abstract": "We take a deep look into the behaviour of self-attention heads in the transformer architecture. In light of recent work discouraging the use of attention distributions for explaining a model\u2019s behaviour, we show that attention distributions can nevertheless provide insights into the local behaviour of attention heads. This way, we propose a distinction between local patterns revealed by attention and global patterns that refer back to the input, and analyze BERT from both angles. We use gradient attribution to analyze how the output of an attention head depends on the input tokens, effectively extending the local attention-based analysis to account for the mixing of information throughout the transformer layers. We find that there is a significant mismatch between attention and attribution distributions, caused by the mixing of context inside the model. We quantify this discrepancy and observe that interestingly, there are some patterns that persist across all layers despite the mixing.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "150973452",
                        "name": "Damian Pascual"
                    },
                    {
                        "authorId": "38094934",
                        "name": "Gino Brunner"
                    },
                    {
                        "authorId": "1716440",
                        "name": "Roger Wattenhofer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Whether for attention (Baan et al., 2019; Pruthi et al., 2019; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019), saliency methods (Alvarez-Melis and Jaakkola, 2018; Kindermans et al.",
                "\u2026the following challenge to the community: We must develop formal definition and evaluation for faith-\n9Whether for attention (Baan et al., 2019; Pruthi et al., 2019; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019), saliency methods (Alvarez-Melis and Jaakkola,\u2026"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "579476d19566efc842929ea6bdd18ab760c8cfa2",
                "externalIds": {
                    "MAG": "3015575765",
                    "DBLP": "journals/corr/abs-2004-03685",
                    "ACL": "2020.acl-main.386",
                    "ArXiv": "2004.03685",
                    "DOI": "10.18653/v1/2020.acl-main.386",
                    "CorpusId": 215416110
                },
                "corpusId": 215416110,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/579476d19566efc842929ea6bdd18ab760c8cfa2",
                "title": "Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?",
                "abstract": "With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is \u201cdefined\u201d by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "41016275",
                        "name": "Alon Jacovi"
                    },
                    {
                        "authorId": "79775260",
                        "name": "Yoav Goldberg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "the models (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Vashishth et al., 2019).",
                "It has been actively discussed so far whether the attention weights can be interpreted to explain\nthe models (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Vashishth et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9a21740d87976bf76f4a9668a9da631035302fb2",
                "externalIds": {
                    "MAG": "3099143320",
                    "DBLP": "conf/emnlp/KobayashiKYI20",
                    "ACL": "2020.emnlp-main.574",
                    "DOI": "10.18653/v1/2020.emnlp-main.574",
                    "CorpusId": 222176890
                },
                "corpusId": 222176890,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/9a21740d87976bf76f4a9668a9da631035302fb2",
                "title": "Attention Is Not Only a Weight: Analyzing Transformers with Vector Norms",
                "abstract": "Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2060291042",
                        "name": "Goro Kobayashi"
                    },
                    {
                        "authorId": "83446147",
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "authorId": "32286159",
                        "name": "Sho Yokoi"
                    },
                    {
                        "authorId": "3040648",
                        "name": "Kentaro Inui"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[22] examined how attention-based methods could be fooled."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "95d4cdaec450c0e26e45f13e231c075c1809ac7c",
                "externalIds": {
                    "DBLP": "conf/ecai/DimanovBJW20",
                    "MAG": "3013484318",
                    "DOI": "10.17863/CAM.48825",
                    "CorpusId": 211838210
                },
                "corpusId": 211838210,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/95d4cdaec450c0e26e45f13e231c075c1809ac7c",
                "title": "You Shouldn't Trust Me: Learning Models Which Conceal Unfairness From Multiple Explanation Methods",
                "abstract": "Transparency of algorithmic systems is an important area of research, which has been discussed as a way for end-users and regulators to develop appropriate trust in machine learning models. One popular approach, LIME [23], even suggests that model expla- nations can answer the question \u201cWhy should I trust you?\u201d. Here we show a straightforward method for modifying a pre-trained model to manipulate the output of many popular feature importance explana- tion methods with little change in accuracy, thus demonstrating the danger of trusting such explanation methods. We show how this ex- planation attack can mask a model\u2019s discriminatory use of a sensitive feature, raising strong concerns about using such explanation meth- ods to check fairness of a model.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "108437064",
                        "name": "B. Dimanov"
                    },
                    {
                        "authorId": "32326200",
                        "name": "Umang Bhatt"
                    },
                    {
                        "authorId": "1708741",
                        "name": "M. Jamnik"
                    },
                    {
                        "authorId": "145689461",
                        "name": "Adrian Weller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In other related work, Pruthi et al. (2019) show that one can easily learn to deceive using attention weights.",
                "However, attention weights do not in general provide faithful explanations for predictions (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2019; Brunner et al., 2019; Moradi et al., 2019; Vashishth et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "087dd95e13efd47aef2a6582e6801b39fc0f83d8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1911-03429",
                    "ArXiv": "1911.03429",
                    "MAG": "3035503910",
                    "ACL": "2020.acl-main.408",
                    "DOI": "10.18653/v1/2020.acl-main.408",
                    "CorpusId": 207847663
                },
                "corpusId": 207847663,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/087dd95e13efd47aef2a6582e6801b39fc0f83d8",
                "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models",
                "abstract": "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the \u2018reasoning\u2019 behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of \u201crationales\u201d (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "48727916",
                        "name": "Jay DeYoung"
                    },
                    {
                        "authorId": "49837811",
                        "name": "Sarthak Jain"
                    },
                    {
                        "authorId": "8937909",
                        "name": "Nazneen Rajani"
                    },
                    {
                        "authorId": "51172373",
                        "name": "Eric P. Lehman"
                    },
                    {
                        "authorId": "2228109",
                        "name": "Caiming Xiong"
                    },
                    {
                        "authorId": "2166511",
                        "name": "R. Socher"
                    },
                    {
                        "authorId": "1912476",
                        "name": "Byron C. Wallace"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, their reliability has been questioned (Jain and Wallace 2019; Pruthi et al. 2020).",
                "According to various studies (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020), standard attention modules noisily predict input importance; the weights cannot provide safe and meaningful explanations.",
                "\u2026state activation (Karpathy, Johnson, and Li 2015; Li et al. 2016; Montavon, Samek, and Mu\u0308ller 2018), attention weights (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020), and learned sparse and interpretable word vectors (Faruqui et al. 2015b,a; Herbelot and Vecchi 2015).",
                "Moreover, (Pruthi et al. 2020) showed that standard attention modules can fool people into thinking that predictions from a model biased against gender minorities do not rely on the gender.",
                "2016; Montavon, Samek, and M\u00fcller 2018), attention weights (Jain and Wallace 2019; Serrano and Smith 2019; Pruthi et al. 2020), and learned sparse and interpretable word vectors (Faruqui et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2d300be18d703f1583716f81c91cf90e9f830c9a",
                "externalIds": {
                    "DBLP": "conf/aaai/AntogniniMF21",
                    "MAG": "3090249509",
                    "DOI": "10.1609/aaai.v35i14.17483",
                    "CorpusId": 222008355
                },
                "corpusId": 222008355,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/2d300be18d703f1583716f81c91cf90e9f830c9a",
                "title": "Multi-Dimensional Explanation of Target Variables from Documents",
                "abstract": "Automated predictions require explanations to be interpretable by humans. Past work used attention and rationale mechanisms to find words that predict the target variable of a document. Often though, they result in a tradeoff between noisy explanations or a drop in accuracy. Furthermore, rationale methods cannot capture the multi-faceted nature of justifications for multiple targets, because of the non-probabilistic nature of the mask. In this paper, we propose the Multi-Target Masker (MTM) to address these shortcomings. The novelty lies in the soft multi-dimensional mask that models a relevance probability distribution over the set of target variables to handle ambiguities. Additionally, two regularizers guide MTM to induce long, meaningful explanations. We evaluate MTM on two datasets and show, using standard metrics and human annotations, that the resulting masks are more accurate and coherent than those generated by the state-of-the-art methods. Moreover, MTM is the first to also achieve the highest F1 scores for all the target variables simultaneously.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "26399699",
                        "name": "Diego Antognini"
                    },
                    {
                        "authorId": "144620464",
                        "name": "C. Musat"
                    },
                    {
                        "authorId": "1735128",
                        "name": "B. Faltings"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1fe62a928bf5cfac0f373728f3a4de3cefe0951d",
                "externalIds": {
                    "DBLP": "conf/iclr/BrunnerLPRCW20",
                    "ArXiv": "1908.04211",
                    "MAG": "2977944219",
                    "CorpusId": 208100714
                },
                "corpusId": 208100714,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1fe62a928bf5cfac0f373728f3a4de3cefe0951d",
                "title": "On Identifiability in Transformers",
                "abstract": "In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "38094934",
                        "name": "Gino Brunner"
                    },
                    {
                        "authorId": "40457423",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "150973452",
                        "name": "Damian Pascual"
                    },
                    {
                        "authorId": "143944934",
                        "name": "Oliver Richter"
                    },
                    {
                        "authorId": "2754495",
                        "name": "Massimiliano Ciaramita"
                    },
                    {
                        "authorId": "1716440",
                        "name": "Roger Wattenhofer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As it is proven possible to incorporate constraints on attention while maintaining satisfactory performance [25,12,31], we propose three approaches for enforcing plausibility constraints on attention maps, namely, sparsity regularization, semi-supervised learning, and supervised learning."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "12c9d497a9538fcda9202937a4c3427d2a938b9e",
                "externalIds": {
                    "DBLP": "conf/nldb/NguyenMGS23",
                    "DOI": "10.1007/978-3-031-35320-8_20",
                    "CorpusId": 259195507
                },
                "corpusId": 259195507,
                "publicationVenue": {
                    "id": "7c0b75bf-65e3-4094-9d72-e8f59ebb154d",
                    "name": "International Conference on Applications of Natural Language to Data Bases",
                    "type": "conference",
                    "alternate_names": [
                        "Appl Nat Lang Data Base",
                        "Int Conf Appl Nat Lang Data Base",
                        "Applications of Natural Language to Data Bases",
                        "NLDB"
                    ],
                    "url": "http://www.nldb.org/"
                },
                "url": "https://www.semanticscholar.org/paper/12c9d497a9538fcda9202937a4c3427d2a938b9e",
                "title": "Regularization, Semi-supervision, and Supervision for a Plausible Attention-Based Explanation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112294501",
                        "name": "Duc Hau Nguyen"
                    },
                    {
                        "authorId": "2008743506",
                        "name": "Cyrielle Mallart"
                    },
                    {
                        "authorId": "1708671",
                        "name": "G. Gravier"
                    },
                    {
                        "authorId": "1792119",
                        "name": "P. S\u00e9billot"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Again, is TracIn intrinsic or posthoc?",
                "resentations [38], clustering [39, 40, 41], probing classifiers [16], concept activation [18], representer point selection [42], TracIn [43], and uptraining [44].",
                "For a real example, consider TracIn [43], in which influence functions are estimated across various training check points.",
                "2Examples of local methods include gradients [21, 22, 23], LRP [24], deep Taylor decomposition [25], integrated gradients [26, 27], DeepLift [28], direct interpretation of gate/attention weights [29], attention roll-out and flow [30], word association norms and analogies [31], time step dynamics [32], challenge datasets [33, 34, 35, 36], local uptraining [19], and influence sketching and influence functions [37]; examples of global methods include unstructured pruning, lottery tickets, dynamic sparse training, binary networks, sparse coding, gate and attention head pruning, correlation of representations [38], clustering [39, 40, 41], probing classifiers [16], concept activation [18], representer point selection [42], TracIn [43], and uptraining [44].\nmaintain.3 Moreover, for a method to be posthoc means different things to local and global methods.",
                "consider TracIn [43], in which influence functions are estimated across various training check points."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "271d3735bfe74af6529b207bce82a08cfa8a10d0",
                "externalIds": {
                    "DBLP": "conf/cikm/Sogaard22",
                    "CorpusId": 255547750
                },
                "corpusId": 255547750,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/271d3735bfe74af6529b207bce82a08cfa8a10d0",
                "title": "Shortcomings of Interpretability Taxonomies for Deep Neural Networks",
                "abstract": "Taxonomies are vehicles for thinking about what\u2019s possible, for identifying unconsidered options, as well as for establishing formal relations between entities. We identify several shortcomings in 10 existing taxonomies for interpretability methods for explainable artificial intelligence (XAI), focusing on methods for deep neural networks. The shortcomings include redundancies, incompleteness, and inconsistencies. We design a new taxonomy based on two orthogonal dimensions and show how it can be used to derive results about entire classes of interpretability methods for deep neural networks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1700187",
                        "name": "Anders S\u00f8gaard"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "013252b96f30eb651a78abe674545576999e718c",
                "externalIds": {
                    "ACL": "2022.bionlp-1.41",
                    "DBLP": "conf/bionlp/Wood-DoughtyCD22",
                    "DOI": "10.18653/v1/2022.bionlp-1.41",
                    "CorpusId": 248779904
                },
                "corpusId": 248779904,
                "publicationVenue": {
                    "id": "3afb600a-49ad-40aa-858c-081def027584",
                    "name": "Workshop on Biomedical Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "BioNLP",
                        "Workshop Biomed Nat Lang Process"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/013252b96f30eb651a78abe674545576999e718c",
                "title": "Model Distillation for Faithful Explanations of Medical Code Predictions",
                "abstract": "Machine learning models that offer excellent predictive performance often lack the interpretability necessary to support integrated human machine decision-making. In clinical medicine and other high-risk settings, domain experts may be unwilling to trust model predictions without explanations. Work in explainable AI must balance competing objectives along two different axes: 1) Models should ideally be both accurate and simple. 2) Explanations must balance faithfulness to the model\u2019s decision-making with their plausibility to a domain expert. We propose to use knowledge distillation, or training a student model that mimics the behavior of a trained teacher model, as a technique to generate faithful and plausible explanations. We evaluate our approach on the task of assigning ICD codes to clinical notes to demonstrate that the student model is faithful to the teacher model\u2019s behavior and produces quality natural language explanations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1411379613",
                        "name": "Zach Wood-Doughty"
                    },
                    {
                        "authorId": "51199773",
                        "name": "Isabel Cachola"
                    },
                    {
                        "authorId": "1478928280",
                        "name": "Mark Dredze"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In 538 fact, Pruthi et al. (2020) cast serious doubts on us- 539 ing attention maps as a way for users to audit expla- 540 nations in the context of fairness."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e969778bced13a339f3d0465cea4e10c489ee1cc",
                "externalIds": {
                    "DBLP": "conf/acl/BibalCAWWFW22",
                    "ACL": "2022.acl-long.269",
                    "DOI": "10.18653/v1/2022.acl-long.269",
                    "CorpusId": 248779953
                },
                "corpusId": 248779953,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/e969778bced13a339f3d0465cea4e10c489ee1cc",
                "title": "Is Attention Explanation? An Introduction to the Debate",
                "abstract": "The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of attention in neural networks. Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible. In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas. This holistic vision can be of great interest for future works in all the communities concerned by this debate. We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8533198",
                        "name": "Adrien Bibal"
                    },
                    {
                        "authorId": "2065830278",
                        "name": "R\u00e9mi Cardon"
                    },
                    {
                        "authorId": "1971580",
                        "name": "David Alfter"
                    },
                    {
                        "authorId": "144566112",
                        "name": "Rodrigo Wilkens"
                    },
                    {
                        "authorId": "2130542021",
                        "name": "Xiaoou Wang"
                    },
                    {
                        "authorId": "2320605",
                        "name": "Thomas Fran\u00e7ois"
                    },
                    {
                        "authorId": "2389742",
                        "name": "Patrick Watrin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While we believe these tools will eventually help stakeholders gain a deeper understanding of multimodal models as a step towards reliable real-world deployment, we believe that special care must be taken in the following regard to ensure that these interpretation tools are reliably deployed: Reliability of visualizations: There has been recent work examining the reliability of model interpretability methods for real-world practitioners [82, 94].",
                "[82] Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C Lipton.",
                "Reliability of visualizations: There has been recent work examining the reliability of model interpretability methods for real-world practitioners [15, 53, 65, 82, 94]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "27533b6175a1ba53b042bd982437a486764ecc21",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-00056",
                    "DOI": "10.48550/arXiv.2207.00056",
                    "CorpusId": 250243822
                },
                "corpusId": 250243822,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/27533b6175a1ba53b042bd982437a486764ecc21",
                "title": "MultiViz: An Analysis Benchmark for Visualizing and Understanding Multimodal Models",
                "abstract": "The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing MULTIVIZ, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. MULTIVIZ is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in MULTIVIZ together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MULTIVIZ is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "28130078",
                        "name": "P. Liang"
                    },
                    {
                        "authorId": "2066413750",
                        "name": "Yiwei Lyu"
                    },
                    {
                        "authorId": "1509809381",
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "authorId": "2146677401",
                        "name": "Nihal Jain"
                    },
                    {
                        "authorId": "4692365",
                        "name": "Zihao Deng"
                    },
                    {
                        "authorId": "50141732",
                        "name": "Xingbo Wang"
                    },
                    {
                        "authorId": "49933077",
                        "name": "Louis-Philippe Morency"
                    },
                    {
                        "authorId": "145124475",
                        "name": "R. Salakhutdinov"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2014) are widely used as an interpretability tool, but these attention maps are often unfaithful and unreliable (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020).",
                "Explainable NLP Heat maps generated from attention values from the models (Bahdanau et al., 2014) are widely used as an interpretability tool, but these attention maps are often unfaithful and unreliable (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5e5b0979e5c532cf494d79ab07bb1eb75d21c3bf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-07469",
                    "DOI": "10.48550/arXiv.2210.07469",
                    "CorpusId": 252907255
                },
                "corpusId": 252907255,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5e5b0979e5c532cf494d79ab07bb1eb75d21c3bf",
                "title": "StyLEx: Explaining Styles with Lexicon-Based Human Perception",
                "abstract": "Style plays a significant role in how humans express themselves and communicate with others. Large pre-trained language models produce impressive results on various style classification tasks. However, they often learn spurious domain-specific words to make predictions. This incorrect word importance learned by the model often leads to ambiguous tokenlevel explanations which do not align with human perception of linguistic styles. To tackle this challenge, we introduce StyLEx, a model that learns annotated human perceptions of stylistic lexica and uses these stylistic words as additional information for predicting the style of a sentence. Our experiments show that StyLEx can provide human-like stylistic lexical explanations without sacrificing the performance of sentence-level style prediction on both original and out-of-domain datasets. Explanations from StyLEx show higher sufficiency, and plausibility when compared to human annotations, and are also more understandable by human judges compared to the existing widely-used saliency baseline.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31998283",
                        "name": "Shirley Anugrah Hayati"
                    },
                    {
                        "authorId": "2152042873",
                        "name": "Kyumin Park"
                    },
                    {
                        "authorId": "1801149",
                        "name": "Dheeraj Rajagopal"
                    },
                    {
                        "authorId": "1717822",
                        "name": "Lyle Ungar"
                    },
                    {
                        "authorId": "48493368",
                        "name": "Dongyeop Kang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following the work, there have been a number of efforts to obtain both faithful and plausible explanations of Transformers using attention [61, 62]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c3ceffdcf2d29af8124321ac3f63f788fb0e3ec2",
                "externalIds": {
                    "CorpusId": 254190176
                },
                "corpusId": 254190176,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c3ceffdcf2d29af8124321ac3f63f788fb0e3ec2",
                "title": "Finding the Needle in a Haystack: Zero-shot Rationale Extraction for Long Text Classifiers",
                "abstract": "We investigate various soft attention architectures to extract plausible token-level rationale from long document Transformers, such as Longformer. We find that a direct application of Weighted Soft Attention, a method used to extract rationale from sentence classifiers, does not select meaningful rationale from long text classifiers. We suspect it is due to the insufficient token-level supervision signal. We propose Mean Soft Attention and Top-k Rest-0 Soft Attention as modifications to the original system that significantly improve the quality of the extracted rationale. We report slow runtimes of the soft attention architectures for long documents. We propose a novel Compositional Soft Attention system that uses a soft attention layer to compose contextual token embeddings obtained for individual sentences. When combined with RoBERTa, we find the Compositional system to be 30 \u2212 65% faster than long document Transformers. We learn that the Compositional Soft Attention ranks individual tokens substantially better than other soft attention systems, but note that it underperforms on the task of sequence labelling and document classification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2169553",
                        "name": "H. Yannakoudakis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, recent studies argued that attention cannot be used as a reliable tool to explain the behavior of models (Jain and Wallace, 2019; Pruthi et al., 2020; Bastings and Filippova, 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1fbb406a7387451bb1b6b67a44975c65120ad03a",
                "externalIds": {
                    "ACL": "2022.emnlp-main.651",
                    "DBLP": "conf/emnlp/GaciBCB22",
                    "DOI": "10.18653/v1/2022.emnlp-main.651",
                    "CorpusId": 247613246
                },
                "corpusId": 247613246,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/1fbb406a7387451bb1b6b67a44975c65120ad03a",
                "title": "Debiasing Pretrained Text Encoders by Paying Attention to Paying Attention",
                "abstract": "Natural Language Processing (NLP) models are found to exhibit discriminatory stereotypes across many social constructs, e.g. gender and race. In comparison to the progress made in reducing bias from static word embeddings, fairness in sentence-level text encoders received little consideration despite their wider applicability in contemporary NLP tasks. In this paper, we propose a debiasing method for pre-trained text encoders that both reduces social stereotypes, and inflicts next to no semantic damage. Unlike previous studies that directly manipulate the embeddings, we suggest to dive deeper into the operation of these encoders, and pay more attention to the way they pay attention to different social groups. We find that stereotypes are also encoded in the attention layer. Then, we work on model debiasing by redistributing the attention scores of a text encoder such that it forgets any preference to historically advantaged groups, and attends to all social classes with the same intensity. Our experiments confirm that reducing bias from attention effectively mitigates it from the model\u2019s text representations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055402228",
                        "name": "Yacine Gaci"
                    },
                    {
                        "authorId": "1734279",
                        "name": "B. Benatallah"
                    },
                    {
                        "authorId": "145866446",
                        "name": "F. Casati"
                    },
                    {
                        "authorId": "1776295",
                        "name": "K. Benabdeslem"
                    }
                ]
            }
        },
        {
            "contexts": [
                "34 The paper Learning to Deceive with Attention-Based Explanations [8] aims to prove that attention weights do not 35 convey true interpretability.",
                "These papers often provide results that either prove a32 correlation between the attention weights and predictions [11][10], or the ambiguity between attention weights and the33 performance of the rest of the model [6][7], or somewhere in between [9].34\nThe paper Learning to Deceive with Attention-Based Explanations [8] aims to prove that attention weights do not35 convey true interpretability.",
                "[8] introduce a method for manipulating attention that penalizes the attention weights of impermissible 52 tokens.",
                "In: arXiv preprint232 arXiv:2004.14243 (2020).233 [8] Danish Pruthi et al. \u201cLearning to Deceive with Attention-Based Explanations\u201d.",
                "[8] add a penalty R to the loss function that is used for the specific task, resulting in a total cost of 58 L\u2032 = L+R."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "049f17c072a09a9a90131052bf82cc430ece182b",
                "externalIds": {
                    "CorpusId": 237273283
                },
                "corpusId": 237273283,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/049f17c072a09a9a90131052bf82cc430ece182b",
                "title": "Manipulating Attention Does Not Deceive Us",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2124154084",
                        "name": "Howard Caulfield"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our results reproduce Pruthi et al. (2020)\u2019s finding that models can learn to deceive.",
                "Our results reproduce Pruthi et al. (2020)\u2019s finding that models can learn to deceive. Jain and Wallace (2019) note that for attention to be an explanation, a different configuration of attention weights for the same piece of text should lead to different predictions. The research which we have reproduced implies that the same accuracy (hence prediction) can be maintained while explicitly changing the configuration of attention weights. The implications are clear; either it is providing further evidence for why attention should not be thought of as an explanation, supporting Serrano and Smith (2019)\u2019s findings that attention weights can be largely zeroed out without affecting accuracy.",
                "Pruthi et al. (2020) challenge the usage of attention-based explanation through a series of experiments using classification and sequence-to-sequence (seq2seq) models.",
                "Acknowledging the debate, Pruthi et al. (2020) whose work we seek to reproduce, examine whether models can learn to deceive, by adding a penalty to the loss function that punishes the model when attention is paid to impermissible tokens.",
                "Table 3: Classification results from Table 3 in Pruthi et al. (2020) with cell scheme author | reproduced for all models except BERT(HgFc) which follows cell scheme author | replicated.",
                "Pruthi et al. (2020) argue that the accuracy of the Embedding and BiLSTM models could have been greatly impacted by the lambda parameter because those models might be under-parameterised for the SST-Wiki dataset.",
                "Especially the reported mean accuracy by Pruthi et al. (2020) shows no significant difference to our reported values for all seq2seq tasks except for the baseline experiments with uniform and no attention for the tasks sequence copy and sequence reverse.",
                "Our results reproduce Pruthi et al. (2020)\u2019s finding that models can learn to deceive. Jain and Wallace (2019) note that for attention to be an explanation, a different configuration of attention weights for the same piece of text should lead to different predictions.",
                "Pruthi et al. (2020) did not report accuracies for the translation task in their original paper, but they provided us with\nadditional raw data which also contained the accuracy scores from their experiments.",
                "Seq2seq Pruthi et al. (2020) provide a bidirectional and unidirectional Gated Recurrent Unit (GRU) with dot-product attention respectively for their encoder-decoder model tackling seq2seq tasks."
            ],
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "0c639d6155423ed343736d321d91aeecc428777e",
                "externalIds": {
                    "CorpusId": 237263050
                },
                "corpusId": 237263050,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0c639d6155423ed343736d321d91aeecc428777e",
                "title": "ML Reproducibility Challenge 2020 Learning to Deceive With Attention-Based Explanations",
                "abstract": "Based on the intuition that attention in neural networks is what the model focuses on, attention is now being used as an explanation for a models\u2019 prediction (see Galassi et al. (2020) for a survey). Pruthi et al. (2020) challenge the usage of attention-based explanation through a series of experiments using classification and sequence-to-sequence (seq2seq) models. They examine the model\u2019s use of impermissible tokens, which are user-defined tokens that can introduce bias e.g. gendered pronouns. Across multiple datasets, the authors show that with the impermissible tokens removed the model accuracy drops, implying their usage in prediction. And then by penalising attention paid to the impermissible tokens but keeping them in, they train models that retain full accuracy hence must be using the impermissible tokens, but that does not show attention being paid to the impermissible tokens. As the paper\u2019s claims have such significant implications for the use of attention-based explanations, we seek to reproduce their results.",
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [
                "In this reproducibility report we run four classification tasks and four sequence-to-sequence tasks to test the claims made by Pruthi et al.. For the classification experiments, three attention-based models are trained and evaluated on four classification tasks. The four classification experiments consist out of three binary classification task and one multiclass classification task. For the sequence-to-sequence experiments, an encoder-decoder model with varying attention mechanisms is trained and evaluated on four tasks. Three of these tasks are toy datasets created by Pruthi et al., the fourth task is an English to German machine translation task (More information in section 3). Further on in the report, we display the results obtained by conducting these experiments and compare them to the results reported by Pruthi et al. (Section 4 & 5). We cannot reproduce one of the binary classification tasks from the paper of Pruthi et al., because they do not have permission to share this private dataset. Therefore, we substitute this dataset for a multiclass classification dataset. As Wiegreffe and Pinter (2019) state, complex networks can produce outputs which can easily be aggregated to form the same binary prediction.",
                "However, Pruthi et al. (2019) show that the attention weights can easily be manipulated during training without a significant change in performance.",
                "Sentiment Analysis + Wikipedia sentences Pruthi et al. used the binary version of the Stanford Sentiment Treebank Socher et al. (2013). Pruthi et al."
            ],
            "intents": [
                "result",
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "538ef96876908f95d4e68940c7231df62c9ff0d4",
                "externalIds": {
                    "CorpusId": 237380168
                },
                "corpusId": 237380168,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/538ef96876908f95d4e68940c7231df62c9ff0d4",
                "title": "[Rp] Learning to Deceive with Attention-Based Explanations",
                "abstract": "Learning to Deceive with Attention-Based Explanations by Pruthi et al. makes two claims which we reproduce in this work. Their first claim entails that attention weights can be manipulated to shift mass away from a predefined set of impermissible tokens without significant chance in performance. Their second claim entails that these manipulated models still rely on information from the set of impermissible tokens. In this reproducibility report we argue that by running their experiments, we can reproduce the results which support the claims by Pruthi et al..",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2125354634",
                        "name": "Jelle van den Broek"
                    },
                    {
                        "authorId": "2125349866",
                        "name": "Koen Gommers"
                    }
                ]
            }
        },
        {
            "contexts": [
                "First, different evaluation methods may give rise to contradictory conclusions, which has caused many debates, such as the argument in using the magnitudes of attention weights as interpretations for transformer-based models (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019; Pruthi et al., 2019; Bastings and Filippova, 2020) and the rebuttal of gradient-based methods (Alvarez-Melis and Jaakkola, 2018; Ghorbani et al.",
                "\u2026in using the magnitudes of attention weights as interpretations for transformer-based models (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019; Pruthi et al., 2019; Bastings and Filippova, 2020) and the rebuttal of gradient-based methods (Alvarez-Melis and Jaakkola, 2018; Ghorbani et al.,\u2026",
                "At the same time, these interpretation methods have received much scrutiny, arguing that the interpretations are fragile or unreliable (Alvarez-Melis and Jaakkola, 2018; Pruthi et al., 2019; Wang et al., 2020; Slack et al., 2020).",
                "At the same time, these interpretation methods have received much scrutiny, arguing that the interpretations are fragile or unreliable (Alvarez-Melis and\nJaakkola, 2018; Pruthi et al., 2019; Wang et al., 2020; Slack et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "36ba0311f30077cd265d8cab4c2f8b68c8aeaa83",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-05463",
                    "CorpusId": 237494685
                },
                "corpusId": 237494685,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/36ba0311f30077cd265d8cab4c2f8b68c8aeaa83",
                "title": "The Logic Traps in Evaluating Post-hoc Interpretations",
                "abstract": "Post-hoc interpretation aims to explain a trained model and reveal how the model arrives at a decision. Though research on posthoc interpretations has developed rapidly, one growing pain in this field is the difficulty in evaluating interpretations. There are some crucial logic traps behind existing evaluation methods, which are ignored by most works. In this opinion piece, we summarize four kinds evaluation methods and point out the corresponding logic traps behind them. We argue that we should be clear about these traps rather than ignore them and draw conclusions assertively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143876071",
                        "name": "Yiming Ju"
                    },
                    {
                        "authorId": "2145784135",
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "authorId": "2155029396",
                        "name": "Zhao Yang"
                    },
                    {
                        "authorId": "97234013",
                        "name": "Zhongtao Jiang"
                    },
                    {
                        "authorId": "77397868",
                        "name": "Kang Liu"
                    },
                    {
                        "authorId": "11447228",
                        "name": "Jun Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However their contributions like training with an auxilary atomic-task supervi-\nsion for improved faithfulness are specific to the context of NMNs. Pruthi et al. (2020) demonstrate that it is possible to train a model that produces a deceptive attention mask, questioning the use of attention\u2026",
                "However their contributions like training with an auxilary atomic-task supervi-\nsion for improved faithfulness are specific to the context of NMNs. Pruthi et al. (2020) demonstrate that it is possible to train a model that produces a deceptive attention mask, questioning the use of attention weights as explanation from the fairness and accountability perspective."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e09f1fc72cbd53db1a06831dda5e22d996c2f3c1",
                "externalIds": {
                    "DBLP": "conf/eacl/MoradiKS21",
                    "ACL": "2021.eacl-main.243",
                    "DOI": "10.18653/v1/2021.eacl-main.243",
                    "CorpusId": 233189641
                },
                "corpusId": 233189641,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/e09f1fc72cbd53db1a06831dda5e22d996c2f3c1",
                "title": "Measuring and Improving Faithfulness of Attention in Neural Machine Translation",
                "abstract": "While the attention heatmaps produced by neural machine translation (NMT) models seem insightful, there is little evidence that they reflect a model\u2019s true internal reasoning. We provide a measure of faithfulness for NMT based on a variety of stress tests where attention weights which are crucial for prediction are perturbed and the model should alter its predictions if the learned weights are a faithful explanation of the predictions. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and has a useful regularization effect on the NMT model and can even improve translation quality in some cases.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2427084",
                        "name": "Pooya Moradi"
                    },
                    {
                        "authorId": "144673690",
                        "name": "Nishant Kambhatla"
                    },
                    {
                        "authorId": "3028658",
                        "name": "Anoop Sarkar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use unweighted average recall (UAR) as our evaluation metric for the emotion classification to account for class imbalance (Rosenberg, 2012)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dce445359304881696c8ad95b55e06689bc1f05f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-08792",
                    "CorpusId": 233296323
                },
                "corpusId": 233296323,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dce445359304881696c8ad95b55e06689bc1f05f",
                "title": "Why Should I Trust a Model is Private? Using Shifts in Model Explanation for Evaluating Privacy-Preserving Emotion Recognition Model",
                "abstract": "Privacy preservation is a crucial component of any real-world application. Yet, in applications relying on machine learning backends, this is challenging because models often capture more than a designer may have envisioned, resulting in the potential leakage of sensitive information. For example, emotion recognition models are susceptible to learning patterns between the target variable and other sensitive variables, patterns that can be maliciously re-purposed to obtain protected information. In this paper, we concentrate on using interpretable methods to evaluate a model\u2019s efficacy to preserve privacy with respect to sensitive variables. We focus on saliency-based explanations, explanations that highlight regions of the input text, which allows us to understand how model explanations shift when models are trained to preserve privacy. We show how certain commonly-used methods that seek to preserve privacy might not align with human perception of privacy preservation. We also show how some of these induce spurious correlations in the model between the input and the primary as well as secondary task, even if the improvement in evaluation metric is significant. Such correlations can hence lead to false assurances about the perceived privacy of the model because especially when used in cross corpus conditions. We conduct crowdsourcing experiments to evaluate the inclination of the evaluators to choose a particular model for a given task when model explanations are provided, and find that correlation of interpretation differences with sociolinguistic biases can be used as a proxy for user trust.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3036536",
                        "name": "Mimansa Jaiswal"
                    },
                    {
                        "authorId": "2523983",
                        "name": "E. Provost"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As shown in [10], the MHA is at least partly interpretable even though multiple heads can be pruned without reducing the accuracy [15]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24471f31f4ed2b85ad0fb6cda780d243bd69f786",
                "externalIds": {
                    "DBLP": "conf/lwa/SchwenkeA21",
                    "CorpusId": 240290338
                },
                "corpusId": 240290338,
                "publicationVenue": {
                    "id": "a0a14d7c-582f-44e2-baab-9d0b8314dca3",
                    "name": "Lernen, Wissen, Daten, Analysen",
                    "type": "conference",
                    "alternate_names": [
                        "LWDA",
                        "Lern Wissen Daten Anal"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/24471f31f4ed2b85ad0fb6cda780d243bd69f786",
                "title": "Abstracting Local Transformer Attention for Enhancing Interpretability on Time Series Data",
                "abstract": "ing Local Transformer Attention for Enhancing Interpretability on Time Series Data Leonid Schwenke1, Martin Atzmueller1 Osnabr\u00fcck University, Semantic Information Systems (SIS) Group, Osnabr\u00fcck, Germany Abstract Transformers have demonstrated considerable performance on sequential data, recently also towards time series data. However, enhancing their interpretability and explainability is still a major research problem, similar to other prominent deep learning approaches. In this paper, we tackle this issue specifically for time series data, where we build on our previous research regarding attention abstraction, aggregation and visualization. In particular, we combine two of our initial attention aggregation techniques and perform a detailed evaluation of this extended scope with our previously used local attention abstraction technique, demonstrating its efficacy on one synthetic as well as three real-world datasets.Transformers have demonstrated considerable performance on sequential data, recently also towards time series data. However, enhancing their interpretability and explainability is still a major research problem, similar to other prominent deep learning approaches. In this paper, we tackle this issue specifically for time series data, where we build on our previous research regarding attention abstraction, aggregation and visualization. In particular, we combine two of our initial attention aggregation techniques and perform a detailed evaluation of this extended scope with our previously used local attention abstraction technique, demonstrating its efficacy on one synthetic as well as three real-world datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2097670994",
                        "name": "Leonid Schwenke"
                    },
                    {
                        "authorId": "1739555",
                        "name": "M. Atzm\u00fcller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspecting the attention scores is a common method of explaining a model\u2019s prediction that has been called into question in recent years [19,26]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a3d24d9d8ca41a0b87957819a7a9095d1cdb8c15",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-05125",
                    "CorpusId": 245006010
                },
                "corpusId": 245006010,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a3d24d9d8ca41a0b87957819a7a9095d1cdb8c15",
                "title": "Transferring BERT-like Transformers' Knowledge for Authorship Verification",
                "abstract": ". The task of identifying the author of a text spans several decades and was tackled using linguistics, statistics, and, more recently, machine learning. Inspired by the impressive performance gains across a broad range of natural language processing tasks and by the recent availability of the PAN large-scale authorship dataset, we \ufb01rst study the e\ufb00ectiveness of several BERT-like transformers for the task of authorship veri\ufb01cation. Such models prove to achieve very high scores consistently. Next, we empirically show that they focus on topical clues rather than on author writing style characteristics, taking advantage of existing biases in the dataset. To address this problem, we provide new splits for PAN-2020, where training and test data are sampled from disjoint topics or authors. Finally, we introduce DarkReddit, a dataset with a di\ufb00erent input data distribution. We further use it to analyze the domain generalization performance of models in a low-data regime and how performance varies when using the proposed PAN-2020 splits for \ufb01ne-tuning. We show that those splits can enhance the models\u2019 capability to transfer knowledge over a new, signi\ufb01cantly di\ufb00erent dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143755178",
                        "name": "Andrei Manolache"
                    },
                    {
                        "authorId": "22225752",
                        "name": "Florin Brad"
                    },
                    {
                        "authorId": "3094164",
                        "name": "Elena Burceanu"
                    },
                    {
                        "authorId": "1739398670",
                        "name": "Antonio B\u0103rb\u0103l\u0103u"
                    },
                    {
                        "authorId": "50035559",
                        "name": "R. Ionescu"
                    },
                    {
                        "authorId": "49006356",
                        "name": "M. Popescu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[13] Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, and Finale Doshi-Velez."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "84f8e899f4a06d8ef609d2b5d63b4c849965bfb4",
                "externalIds": {
                    "CorpusId": 247246635
                },
                "corpusId": 247246635,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/84f8e899f4a06d8ef609d2b5d63b4c849965bfb4",
                "title": "Overcoming Opacity in Machine Learning",
                "abstract": "In order for a machine learning model to be useful, it must be used. Opaque models that predict or classify without explaining are often ignored. Thus measuring the satisfaction of those who receive an explanation is one natural way to measure the value of that explanation. A satisfied user will be more likely to trust and therefore to use the system. Nevertheless, I argue that user satisfaction with an explanation alone is not a good metric for the value of that explanation, proposing instead benchmark tests based on accuracy and suitability of explanations for purposes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "52216932",
                        "name": "Hannes Boelsen"
                    },
                    {
                        "authorId": "1383066965",
                        "name": "Kathleen A. Creel"
                    },
                    {
                        "authorId": "103529842",
                        "name": "P. Gr\u00fcnke"
                    },
                    {
                        "authorId": "2572856",
                        "name": "R. Hillerbrand"
                    },
                    {
                        "authorId": "1944880380",
                        "name": "Harrison Taylor"
                    },
                    {
                        "authorId": "151473559",
                        "name": "Liam Hiley"
                    },
                    {
                        "authorId": "50998197",
                        "name": "Richard J. Tomsett"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thus to restrict the model to only consider some specific alignments, we intuitively mask co-attention matrices AP and AH following Serrano and Smith (2019); Pruthi et al. (2020)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "648a5496a4be5690adaa888b04469de4799fff4f",
                "externalIds": {
                    "ACL": "2021.acl-long.417",
                    "DBLP": "conf/acl/JiangZY0020",
                    "DOI": "10.18653/v1/2021.acl-long.417",
                    "CorpusId": 236460314
                },
                "corpusId": 236460314,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/648a5496a4be5690adaa888b04469de4799fff4f",
                "title": "Alignment Rationale for Natural Language Inference",
                "abstract": "Deep learning models have achieved great success on the task of Natural Language Inference (NLI), though only a few attempts try to explain their behaviors. Existing explanation methods usually pick prominent features such as words or phrases from the input text. However, for NLI, alignments among words or phrases are more enlightening clues to explain the model. To this end, this paper presents AREC, a post-hoc approach to generate alignment rationale explanations for co-attention based models in NLI. The explanation is based on feature selection, which keeps few but sufficient alignments while maintaining the same prediction of the target model. Experimental results show that our method is more faithful and human-readable compared with many existing approaches. We further study and re-evaluate three typical models through our explanation beyond accuracy, and propose a simple method that greatly improves the model robustness.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "97234013",
                        "name": "Zhongtao Jiang"
                    },
                    {
                        "authorId": "2145784135",
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "authorId": "2155029396",
                        "name": "Zhao Yang"
                    },
                    {
                        "authorId": "11447228",
                        "name": "Jun Zhao"
                    },
                    {
                        "authorId": "77397868",
                        "name": "Kang Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Authors have exploited these vulnerabilities to make discriminatory models pass algorithmic audits [18] and appear fair in user studies [13]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "abe36227faa7ed52a7de2eb95627b22592098d39",
                "externalIds": {
                    "CorpusId": 248216084
                },
                "corpusId": 248216084,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/abe36227faa7ed52a7de2eb95627b22592098d39",
                "title": "No Explanation without Inference",
                "abstract": ". Complex algorithms are increasingly used to automate high-stakes decisions in sensitive areas like healthcare and \ufb01nance. However, the opacity of such models raises problems of intelligibil-ity and trust. Researchers in interpretable machine learning (iML) have proposed a number of solutions, including local linear approxi-mations, rule lists, and counterfactuals. I argue that all three methods share the same fundamental \ufb02aw \u2013 namely, a disregard for severe testing . Techniques for quantifying uncertainty and error are central to scienti\ufb01c explanation, yet iML has largely ignored this method-ological imperative. I consider examples that illustrate the dangers of such negligence, with an emphasis on issues of scoping and con-founding. Drawing on recent work in philosophy of science, I conclude that there can be no explanation \u2013 algorithmic or otherwise \u2013 without inference. I propose several ways to severely test existing iML methods and evaluate the resulting trade-offs.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2065079791",
                        "name": "David Watson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[7], Jain and Wallace [8] illustrated that there are limitations in using attentions as explanations, in terms of being incapable to provide fully faithful explanations concerning the model decisions."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "34d124291050ec7da53f4c1fec2b0d18554de58a",
                "externalIds": {
                    "CorpusId": 249246070
                },
                "corpusId": 249246070,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/34d124291050ec7da53f4c1fec2b0d18554de58a",
                "title": "Explaining Information Flow Inside Vision Transformers Using Markov Chain",
                "abstract": "Transformer-based models are receiving increasingly popularity in the field of computer vision, however, the corresponding interpretability study is less. As the simplest explainability method, visualization of attention weights exerts poor performance because of lacking association between the input and model decisions. In this study, we propose a method, named Transition Attention Maps , to generate the saliency map concerning a specific target category. The proposed approach con-nects the idea of the Markov chain, to investigate the information flow across layers of the Transformer and combine the integrated gradients to compute the relevance of input tokens for the model decisions. We compare with other explainability methods using Vision Transformer as a benchmark and demonstrate that our method achieves better performance in various aspects. We open source the implementation of our approach at https://github.com/PaddlePaddle/InterpretDL .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2057820135",
                        "name": "Ting Yuan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ed80a8888bcd3a8adf2ca784a5699969eaa597da",
                "externalIds": {
                    "CorpusId": 250921520
                },
                "corpusId": 250921520,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ed80a8888bcd3a8adf2ca784a5699969eaa597da",
                "title": "[Re] Reproducing Learning to Deceive With Attention-Based Explanations",
                "abstract": "of using classification and sequence-to-sequence (seq2seq) They examine the model\u02bcs use of impermissible tokens, which are tokens that can introduce bias e.g. gendered pronouns. Across multiple datasets, the show that with the impermissible tokens removed the model accuracy drops, implying their usage in prediction. And then by penalising attention to the impermissible tokens but keeping them in, they train models that hence be using the impermissible tokens, but does not being to the impermissible tokens. As the paper\u02bcs have significant implications for the use of attention-based explanations, we seek to reproduce their results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2169583577",
                        "name": "Rahel Habacker"
                    },
                    {
                        "authorId": "2070455842",
                        "name": "A. Harrison"
                    },
                    {
                        "authorId": "2086067275",
                        "name": "Mathias Parisot"
                    },
                    {
                        "authorId": "2178345602",
                        "name": "Ard Snijders"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, Pruthi et al. (2019) demonstrates how easy it is to manipulate the Attention weights."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9064d5cd9d237a824c1f0aefd0081529d927b9eb",
                "externalIds": {
                    "DBLP": "conf/paclic/SongL21",
                    "CorpusId": 249183158
                },
                "corpusId": 249183158,
                "publicationVenue": {
                    "id": "63eb9f8a-e9a8-43fb-a0f1-bdf78525349d",
                    "name": "Pacific Asia Conference on Language, Information and Computation",
                    "type": "conference",
                    "alternate_names": [
                        "Pacific Asia Conference on Language, Information, and Computation",
                        "Pac Asia Conf Lang Inf Comput",
                        "PACLIC"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9064d5cd9d237a824c1f0aefd0081529d927b9eb",
                "title": "HPSG/MRS-Based Natural Language Generation Using Transformer",
                "abstract": "With the massive success of stochastic language models, some claim that the symbolic approach is not necessary for natural language processing. However, recent studies made use of linguistically motivated meaning representation of Abstract Meaning Representation and Minimal Recursion Semantics in a natural language generation task and achieved impressive results while hinting that neural networks do make extensive use of linguistic information. This study partially replicates previous studies with Transformer. The model performed worse than the previous researches. We present some evidence that the model failed to make the correct lexical choices due to Attention weight assignment. Overall, it appears that models built solely on Attention mechanism are suboptimal for processing language representation with rich grammatical information, while hinting the possible use of engineered grammar for improving the controllability of the neural models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "36224728",
                        "name": "Sanghoun Song"
                    },
                    {
                        "authorId": "2140051040",
                        "name": "Gyu-Min Lee"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cf50b349b9661cebca377ce40d225f993cdddf7b",
                "externalIds": {
                    "CorpusId": 260426916
                },
                "corpusId": 260426916,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cf50b349b9661cebca377ce40d225f993cdddf7b",
                "title": "A template for the arxiv style",
                "abstract": "While deep learning models have greatly improved the performance of most artificial intelligence tasks, they are often criticized to be untrustworthy due to the black-box problem. Consequently, many works have been proposed to study the trustworthiness of deep learning. However, as most open datasets are designed for evaluating the accuracy of model outputs, there is still a lack of appropriate datasets for evaluating the inner workings of neural networks. The lack of datasets obviously hinders the development of trustworthiness research. Therefore, in order to systematically evaluate the factors for building trustworthy systems, we propose a novel and well-annotated sentiment analysis dataset to evaluate robustness and interpretability. To evaluate these factors, our dataset contains diverse annotations about the challenging distribution of instances, manual adversarial instances and sentiment explanations. Several evaluation metrics are further proposed for interpretability and robustness. Based on the dataset and metrics, we conduct comprehensive comparisons for the trustworthiness of three typical models, and also study the relations between accuracy, robustness and interpretability. We release this trustworthiness evaluation dataset at https://github/xyz and hope our work can facilitate the progress on building more trustworthy systems for real-world applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2241495161",
                        "name": "A. Preprint"
                    },
                    {
                        "authorId": "2118354420",
                        "name": "Lijie Wang"
                    },
                    {
                        "authorId": "46935802",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2072712948",
                        "name": "Shu-ping Peng"
                    },
                    {
                        "authorId": "1576488202",
                        "name": "Hongxuan Tang"
                    },
                    {
                        "authorId": "2107521158",
                        "name": "Xinyan Xiao"
                    },
                    {
                        "authorId": "2118428180",
                        "name": "Ying Chen"
                    },
                    {
                        "authorId": "2149181702",
                        "name": "Hua Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026it has been reported that learned attention weights are often uncorrelated with the word importance calculated through the gradientbased method (Simonyan et al., 2013), and perturbations to the attention mechanisms may interfere with interpretation (Jain and Wallace, 2019; Pruthi et al., 2019).",
                ", 2013), and perturbations to the attention mechanisms may interfere with interpretation (Jain and Wallace, 2019; Pruthi et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "88654b417e7f43e3b62b12c17d252313ec4d4cc8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-08763",
                    "CorpusId": 260437238
                },
                "corpusId": 260437238,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/88654b417e7f43e3b62b12c17d252313ec4d4cc8",
                "title": "Making Attention Mechanisms More Robust and Interpretable with Virtual Adversarial Training for Semi-Supervised Text Classification",
                "abstract": "We propose a new general training technique for attention mechanisms based on virtual adversarial training (VAT). VAT can compute adversarial perturbations from unlabeled data in a semi-supervised setting for the attention mechanisms that have been reported in previous studies to be vulnerable to perturbations. Empirical experiments reveal that our technique (1) provides significantly better prediction performance compared to not only conventional adversarial training-based techniques but also VAT-based techniques in a semi-supervised setting, (2) demonstrates a stronger correlation with the word importance and better agreement with evidence provided by humans, and (3) gains in performance with increasing amounts of unlabeled data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51267434",
                        "name": "Shunsuke Kitada"
                    },
                    {
                        "authorId": "2801969",
                        "name": "H. Iyatomi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "36720aae765e7705c4cd7d86e3d1873d9a81f5a6",
                "externalIds": {
                    "MAG": "3046415660",
                    "DOI": "10.1007/978-3-030-49100-0_25",
                    "CorpusId": 224989197
                },
                "corpusId": 224989197,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/36720aae765e7705c4cd7d86e3d1873d9a81f5a6",
                "title": "Explainable AI for the Operating Theater",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2479037",
                        "name": "Frank Rudzicz"
                    },
                    {
                        "authorId": "34287745",
                        "name": "Shalmali Joshi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, different layers vary in their behaviors [23, 24], and it has been shown that attention alone can be deceiving when used for interpretability and explanation [25]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5d2e62f6c8f231fb0146bb53ce164fbaf6fa772e",
                "externalIds": {
                    "DBLP": "conf/nips/GuKJ0MZS20",
                    "MAG": "3106301701",
                    "CorpusId": 227276155
                },
                "corpusId": 227276155,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5d2e62f6c8f231fb0146bb53ce164fbaf6fa772e",
                "title": "Self-Supervised Relationship Probing",
                "abstract": "Structured representations of images that model visual relationships are beneficial for many vision and vision-language applications. However, current humanannotated visual relationship datasets suffer from the long-tailed predicate distribution problem which limits the potential of visual relationship models. In this work, we introduce a self-supervised method that implicitly learns the visual relationships without relying on any ground-truth visual relationship annotations. Our method relies on 1) intraand inter-modality encodings to respectively model relationships within each modality separately and jointly, and 2) relationship probing, which seeks to discover the graph structure within each modality. By leveraging masked language modeling, contrastive learning, and dependency tree distances for self-supervision, our method learns better object features as well as implicit visual relationships. We verify the effectiveness of our proposed method on various vision-language tasks that benefit from improved visual relationship understanding.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2174964",
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "authorId": "1859486",
                        "name": "Jason Kuen"
                    },
                    {
                        "authorId": "2708940",
                        "name": "Shafiq R. Joty"
                    },
                    {
                        "authorId": "1688642",
                        "name": "Jianfei Cai"
                    },
                    {
                        "authorId": "2852035",
                        "name": "Vlad I. Morariu"
                    },
                    {
                        "authorId": "7574699",
                        "name": "Handong Zhao"
                    },
                    {
                        "authorId": "1500530510",
                        "name": "Tong Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[50] demonstrate that it is possible to train a model that produces a deceptive attention mask, questioning the use of attention weights as explanation from the fairness and accountability perspective."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "66de5060320c67225035a8a0ccdaef731fefa944",
                "externalIds": {
                    "ACL": "2020.aacl-srw.14",
                    "DBLP": "conf/ijcnlp/MoradiKS20",
                    "CorpusId": 227905404
                },
                "corpusId": 227905404,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/66de5060320c67225035a8a0ccdaef731fefa944",
                "title": "Training with Adversaries to Improve Faithfulness of Attention in Neural Machine Translation",
                "abstract": "Can we trust that the attention heatmaps produced by a neural machine translation (NMT) model reflect its true internal reasoning? We isolate and examine in detail the notion of faithfulness in NMT models. We provide a measure of faithfulness for NMT based on a variety of stress tests where model parameters are perturbed and measuring faithfulness based on how often the model output changes. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and it also seems to have a useful regularization effect on the NMT model and can even improve translation quality in some cases.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2427084",
                        "name": "Pooya Moradi"
                    },
                    {
                        "authorId": "144673690",
                        "name": "Nishant Kambhatla"
                    },
                    {
                        "authorId": "3028658",
                        "name": "Anoop Sarkar"
                    }
                ]
            }
        }
    ]
}