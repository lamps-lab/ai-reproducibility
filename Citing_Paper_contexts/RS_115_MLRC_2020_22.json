{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "056d5f39c3c7a2ae1612e311c743d54a33d905d0",
                "externalIds": {
                    "DBLP": "journals/eswa/BrunelloMS23",
                    "DOI": "10.1016/j.eswa.2023.120679",
                    "CorpusId": 259094249
                },
                "corpusId": 259094249,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/056d5f39c3c7a2ae1612e311c743d54a33d905d0",
                "title": "Towards interpretability in fingerprint based indoor positioning: May attention be with us",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "37833525",
                        "name": "Andrea Brunello"
                    },
                    {
                        "authorId": "1723502",
                        "name": "A. Montanari"
                    },
                    {
                        "authorId": "1885147205",
                        "name": "Nicola Saccomanno"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They argue that raw attention fails to capture syntactic structures in text and may not contribute to predictions as commonly assumed (Mohankumar et al., 2020).",
                "To make attention explanation, technical solutions have also been explored by optimizing input representation (Mohankumar et al., 2020), regularizing learning objectives (Moradi et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "26089bdfdbca1e6eaaceca71e3116b715bec6d47",
                "externalIds": {
                    "ArXiv": "2309.01029",
                    "DBLP": "journals/corr/abs-2309-01029",
                    "DOI": "10.48550/arXiv.2309.01029",
                    "CorpusId": 261530292
                },
                "corpusId": 261530292,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/26089bdfdbca1e6eaaceca71e3116b715bec6d47",
                "title": "Explainability for Large Language Models: A Survey",
                "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237987232",
                        "name": "Haiyan Zhao"
                    },
                    {
                        "authorId": "2237948828",
                        "name": "Hanjie Chen"
                    },
                    {
                        "authorId": "145338224",
                        "name": "F. Yang"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    },
                    {
                        "authorId": "13689700",
                        "name": "Huiqi Deng"
                    },
                    {
                        "authorId": "22561596",
                        "name": "Hengyi Cai"
                    },
                    {
                        "authorId": "2237948548",
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "authorId": "2136400100",
                        "name": "Dawei Yin"
                    },
                    {
                        "authorId": "2237804196",
                        "name": "Mengnan Du"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[0,10) [10,20) [20,30) [30-40) \u226540 SASRec AC-SASRec 0.",
                "[0,10) [10,20) [20,30) [30-40) \u226540 BERT4Rec"
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a44d904432cdda9d188bfff8e31619e02f2a4d89",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-09419",
                    "ArXiv": "2308.09419",
                    "DOI": "10.48550/arXiv.2308.09419",
                    "CorpusId": 261031741
                },
                "corpusId": 261031741,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a44d904432cdda9d188bfff8e31619e02f2a4d89",
                "title": "Attention Calibration for Transformer-based Sequential Recommendation",
                "abstract": "Transformer-based sequential recommendation (SR) has been booming in recent years, with the self-attention mechanism as its key component. Self-attention has been widely believed to be able to effectively select those informative and relevant items from a sequence of interacted items for next-item prediction via learning larger attention weights for these items. However, this may not always be true in reality. Our empirical analysis of some representative Transformer-based SR models reveals that it is not uncommon for large attention weights to be assigned to less relevant items, which can result in inaccurate recommendations. Through further in-depth analysis, we find two factors that may contribute to such inaccurate assignment of attention weights: sub-optimal position encoding and noisy input. To this end, in this paper, we aim to address this significant yet challenging gap in existing works. To be specific, we propose a simple yet effective framework called Attention Calibration for Transformer-based Sequential Recommendation (AC-TSR). In AC-TSR, a novel spatial calibrator and adversarial calibrator are designed respectively to directly calibrates those incorrectly assigned attention weights. The former is devised to explicitly capture the spatial relationships (i.e., order and distance) among items for more precise calculation of attention weights. The latter aims to redistribute the attention weights based on each item's contribution to the next-item prediction. AC-TSR is readily adaptable and can be seamlessly integrated into various existing transformer-based SR models. Extensive experimental results on four benchmark real-world datasets demonstrate the superiority of our proposed ACTSR via significant recommendation performance enhancements. The source code is available at https://github.com/AIM-SE/AC-TSR.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1800462890",
                        "name": "Peilin Zhou"
                    },
                    {
                        "authorId": "2190432576",
                        "name": "Qichen Ye"
                    },
                    {
                        "authorId": "2154871075",
                        "name": "Yueqi Xie"
                    },
                    {
                        "authorId": "2118389668",
                        "name": "Jingqi Gao"
                    },
                    {
                        "authorId": "2116951322",
                        "name": "Shoujin Wang"
                    },
                    {
                        "authorId": "2156009696",
                        "name": "Jae Boum Kim"
                    },
                    {
                        "authorId": "2061592207",
                        "name": "Chenyu You"
                    },
                    {
                        "authorId": "2118021616",
                        "name": "Sunghun Kim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e7b8ab8ff59dd54af7c1113aada524d77d26dae3",
                "externalIds": {
                    "DOI": "10.3390/ai4030033",
                    "CorpusId": 260411225
                },
                "corpusId": 260411225,
                "publicationVenue": {
                    "id": "b76366f5-0af9-45f3-8fe3-78fdb0114f67",
                    "name": "Applied Informatics",
                    "type": "conference",
                    "alternate_names": [
                        "Appl Informatics",
                        "Advances Argumentation Artificial Intelligence",
                        "AI",
                        "Can Conf Artif Intell",
                        "Adv Argum Artif Intell",
                        "Canadian Conference on Artificial Intelligence"
                    ],
                    "issn": "2196-0089",
                    "alternate_issns": [
                        "2673-2688"
                    ],
                    "url": "http://cscsi.org/",
                    "alternate_urls": [
                        "https://link.springer.com/journal/40535",
                        "http://www.applied-informatics-j.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e7b8ab8ff59dd54af7c1113aada524d77d26dae3",
                "title": "Explainable Image Classification: The Journey So Far and the Road Ahead",
                "abstract": "Explainable Artificial Intelligence (XAI) has emerged as a crucial research area to address the interpretability challenges posed by complex machine learning models. In this survey paper, we provide a comprehensive analysis of existing approaches in the field of XAI, focusing on the tradeoff between model accuracy and interpretability. Motivated by the need to address this tradeoff, we conduct an extensive review of the literature, presenting a multi-view taxonomy that offers a new perspective on XAI methodologies. We analyze various sub-categories of XAI methods, considering their strengths, weaknesses, and practical challenges. Moreover, we explore causal relationships in model explanations and discuss approaches dedicated to explaining cross-domain classifiers. The latter is particularly important in scenarios where training and test data are sampled from different distributions. Drawing insights from our analysis, we propose future research directions, including exploring explainable allied learning paradigms, developing evaluation metrics for both traditionally trained and allied learning-based classifiers, and applying neural architectural search techniques to minimize the accuracy\u2013interpretability tradeoff. This survey paper provides a comprehensive overview of the state-of-the-art in XAI, serving as a valuable resource for researchers and practitioners interested in understanding and advancing the field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "88991725",
                        "name": "V. Kamakshi"
                    },
                    {
                        "authorId": "2503137",
                        "name": "N. C. Krishnan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although much recent work has been done on explainability in the computer vision and natural language processing [Masoomi et al., 2021; Mohankumar et al., 2020; Tsang et al., 2020], this problem has been overlooked in the case of time series forecasting [Tonekaboni et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "493b10a98463686e265976523672ce41f9c1933e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-13797",
                    "ArXiv": "2308.13797",
                    "DOI": "10.24963/ijcai.2023/478",
                    "CorpusId": 260850560
                },
                "corpusId": 260850560,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/493b10a98463686e265976523672ce41f9c1933e",
                "title": "DeLELSTM: Decomposition-based Linear Explainable LSTM to Capture Instantaneous and Long-term Effects in Time Series",
                "abstract": "Time series forecasting is prevalent in various real-world applications. Despite the promising results of deep learning models in time series forecasting, especially the Recurrent Neural Networks (RNNs), the explanations of time series models, which are critical in high-stakes applications, have received little attention. In this paper, we propose a Decomposition-based Linear Explainable LSTM (DeLELSTM) to improve the interpretability of LSTM. Conventionally, the interpretability of RNNs only concentrates on the variable importance and time importance. We additionally distinguish between the instantaneous influence of new coming data and the long-term effects of historical data. Specifically, DeLELSTM consists of two components, i.e., standard LSTM and tensorized LSTM. The tensorized LSTM assigns each variable with a unique hidden state making up a matrix h(t), and the standard LSTM models all the variables with a shared hidden state H(t). By decomposing the H(t) into the linear combination of past information h(t-1) and the fresh information h(t)-h(t-1), we can get the instantaneous influence and the long-term effect of each feature. In addition, the advantage of linear regression also makes the explanation transparent and clear. We demonstrate the effectiveness and interpretability of DeLELSTM on three empirical datasets. Extensive experiments show that the proposed method achieves competitive performance against the baseline methods and provides a reliable explanation relative to domain knowledge.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144555593",
                        "name": "Chaoqun Wang"
                    },
                    {
                        "authorId": "2195022734",
                        "name": "Yijun Li"
                    },
                    {
                        "authorId": "41193578",
                        "name": "Xiangqian Sun"
                    },
                    {
                        "authorId": "1715610",
                        "name": "Qi Wu"
                    },
                    {
                        "authorId": "2111219987",
                        "name": "Dongdong Wang"
                    },
                    {
                        "authorId": "2158555203",
                        "name": "Zhixiang Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b7ebcf7224808efa16f2ff8b8e699f424bd92bdb",
                "externalIds": {
                    "PubMedCentral": "10375693",
                    "DOI": "10.1186/s12967-023-04175-7",
                    "CorpusId": 260170642,
                    "PubMed": "37501197"
                },
                "corpusId": 260170642,
                "publicationVenue": {
                    "id": "2fd4c673-ebec-4ccf-b719-594b48795306",
                    "name": "Journal of Translational Medicine",
                    "type": "journal",
                    "alternate_names": [
                        "J Transl Med"
                    ],
                    "issn": "1479-5876",
                    "url": "http://www.translational-medicine.com/",
                    "alternate_urls": [
                        "http://www.translational-medicine.com/home/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b7ebcf7224808efa16f2ff8b8e699f424bd92bdb",
                "title": "Radiomics using computed tomography to predict CD73 expression and prognosis of colorectal cancer liver metastases",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1471849048",
                        "name": "Ralph Saber"
                    },
                    {
                        "authorId": "47648479",
                        "name": "D. Henault"
                    },
                    {
                        "authorId": "39926170",
                        "name": "N. Messaoudi"
                    },
                    {
                        "authorId": "2028076929",
                        "name": "R. Rebolledo"
                    },
                    {
                        "authorId": "2757608",
                        "name": "E. Montagnon"
                    },
                    {
                        "authorId": "145174510",
                        "name": "G. Soucy"
                    },
                    {
                        "authorId": "118273836",
                        "name": "J. Stagg"
                    },
                    {
                        "authorId": "2080173111",
                        "name": "A. Tang"
                    },
                    {
                        "authorId": "3622148",
                        "name": "S. Turcotte"
                    },
                    {
                        "authorId": "1781469",
                        "name": "S. Kadoury"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6cf743c7031ef8bb29ff192849f07c653262561f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-06963",
                    "ArXiv": "2307.06963",
                    "DOI": "10.48550/arXiv.2307.06963",
                    "CorpusId": 259924516
                },
                "corpusId": 259924516,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6cf743c7031ef8bb29ff192849f07c653262561f",
                "title": "Is Task-Agnostic Explainable AI a Myth?",
                "abstract": "Our work serves as a framework for unifying the challenges of contemporary explainable AI (XAI). We demonstrate that while XAI methods provide supplementary and potentially useful output for machine learning models, researchers and decision-makers should be mindful of their conceptual and technical limitations, which frequently result in these methods themselves becoming black boxes. We examine three XAI research avenues spanning image, textual, and graph data, covering saliency, attention, and graph-type explainers. Despite the varying contexts and timeframes of the mentioned cases, the same persistent roadblocks emerge, highlighting the need for a conceptual breakthrough in the field to address the challenge of compatibility between XAI methods and application tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223551448",
                        "name": "Alicja Chaszczewicz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There has been an ongoing debate in the literature regarding the interpretability of the attention mechanism (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Moradi et al., 2019; Mohankumar et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6927a5b0152433a199ab4974ad85e787454d6a30",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-13088",
                    "ArXiv": "2305.13088",
                    "DOI": "10.48550/arXiv.2305.13088",
                    "CorpusId": 258832694
                },
                "corpusId": 258832694,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6927a5b0152433a199ab4974ad85e787454d6a30",
                "title": "Should We Attend More or Less? Modulating Attention for Fairness",
                "abstract": "The abundance of annotated data in natural language processing (NLP) poses both opportunities and challenges. While it enables the development of high-performing models for a variety of tasks, it also poses the risk of models learning harmful biases from the data, such as gender stereotypes. In this work, we investigate the role of attention, a widely-used technique in current state-of-the-art NLP models, in the propagation of social biases. Specifically, we study the relationship between the entropy of the attention distribution and the model's performance and fairness. We then propose a novel method for modulating attention weights to improve model fairness after training. Since our method is only applied post-training and pre-inference, it is an intra-processing method and is, therefore, less computationally expensive than existing in-processing and pre-processing approaches. Our results show an increase in fairness and minimal performance loss on different text classification and generation tasks using language models of varying sizes. WARNING: This work uses language that is offensive.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2077390471",
                        "name": "A. Zayed"
                    },
                    {
                        "authorId": "24039720",
                        "name": "Gon\u00e7alo Mordido"
                    },
                    {
                        "authorId": "3197429",
                        "name": "S. Shabanian"
                    },
                    {
                        "authorId": "123607932",
                        "name": "Sarath Chandar"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ceecf85fcd5b598b9585dd8cf390612fec3ffb74",
                "externalIds": {
                    "DOI": "10.1587/essfr.16.4_289",
                    "CorpusId": 257895010
                },
                "corpusId": 257895010,
                "publicationVenue": {
                    "id": "b00e8307-5b5e-4567-9406-4402e7897cb5",
                    "name": "IEICE ESS FUNDAMENTALS REVIEW",
                    "type": "journal",
                    "alternate_names": [
                        "IEICE ESS Fundamentals Review",
                        "IEICE ES Fundam Rev",
                        "IEICE ES FUNDAM REV"
                    ],
                    "issn": "1882-0875",
                    "url": "https://www.jstage.jst.go.jp/browse/essfr/-char/en",
                    "alternate_urls": [
                        "https://www.jstage.jst.go.jp/browse/essfr/list/-char/ja"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ceecf85fcd5b598b9585dd8cf390612fec3ffb74",
                "title": "Frontiers in Explainable Automated Writing Evaluation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3040648",
                        "name": "Kentaro Inui"
                    },
                    {
                        "authorId": "2055627296",
                        "name": "Yutaka Ishii"
                    },
                    {
                        "authorId": "1708079",
                        "name": "Yuichiroh Matsubayashi"
                    },
                    {
                        "authorId": "32427616",
                        "name": "Naoya Inoue"
                    },
                    {
                        "authorId": "39908445",
                        "name": "Shoichi Naito"
                    },
                    {
                        "authorId": "151411783",
                        "name": "Yoriko Isobe"
                    },
                    {
                        "authorId": "32850099",
                        "name": "Hiroaki Funayama"
                    },
                    {
                        "authorId": "2212926540",
                        "name": "Seiya Kikuchi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "And the attention mechanism is able to provide faithful explanations [49]\u2013 [51]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "07cb5b0959b09d88ec4e5455a013745eaf01b35f",
                "externalIds": {
                    "DBLP": "conf/icde/GaoWZCMWL23",
                    "DOI": "10.1109/ICDE55515.2023.00094",
                    "CorpusId": 260171791
                },
                "corpusId": 260171791,
                "publicationVenue": {
                    "id": "764e3630-ddac-4c21-af4b-9d32ffef082e",
                    "name": "IEEE International Conference on Data Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "ICDE",
                        "Int Conf Data Eng",
                        "IEEE Int Conf Data Eng",
                        "International Conference on Data Engineering"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1331"
                },
                "url": "https://www.semanticscholar.org/paper/07cb5b0959b09d88ec4e5455a013745eaf01b35f",
                "title": "Towards Explainable Table Interpretation Using Multi-view Explanations",
                "abstract": "Table interpretation (TI), which aims to predict the column types and relations of tables, plays an essential role in necessary decision-making actions for data management systems. Typically, TI is followed by a manual verification, where experts manually verify the correctness of TI\u2019s predictions. Manual verification is able to ensure the quality of decision-making actions but labor-intensive. To reduce the labour costs, providing explanations for TI\u2019s predictions is necessary as these explanations can help them do faster and more accurate verification. However, existing TI approaches overlook the manual verification process and lack explainability1. To fill this gap, this paper explores the challenging explainable table interpretation problem, which aims to provide faithful explanations and meanwhile achieve high prediction performance. Furthermore, we propose ExplainTI framework. ExplainTI consists of two phases: (i) tables are converted to sequences and lightweight column graphs; and (ii) a pre-trained transformer encoder is fine-tuned to provide multi-view explanations and aggregate contextual information. Extensive experiments on both real Web tables and database tables confirm that ExplainTI outperforms competitive baselines. Moreover, systematical analysis of explainability demonstrates that our framework can provide faithful explanations to facilitate the manual verification process.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1409828392",
                        "name": "Yunjun Gao"
                    },
                    {
                        "authorId": "2108238253",
                        "name": "Pengfei Wang"
                    },
                    {
                        "authorId": "2175355952",
                        "name": "Xiaocan Zeng"
                    },
                    {
                        "authorId": "2115384591",
                        "name": "Lu Chen"
                    },
                    {
                        "authorId": "1753922779",
                        "name": "Yuren Mao"
                    },
                    {
                        "authorId": "145855509",
                        "name": "Ziheng Wei"
                    },
                    {
                        "authorId": "2224985937",
                        "name": "Miao Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To give a sense of the success of these techniques, it suffices to say that some of them are now integrated as default explainability tools in widespread cloud machine learning services, while in areas such as natural language processing, researchers are starting to use such methods as the gold standard against which they judge the quality of other explanations (Mohankumar et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0113935b640d76e6cec5e2cd8cb98193158a636c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-02080",
                    "ArXiv": "2301.02080",
                    "DOI": "10.48550/arXiv.2301.02080",
                    "CorpusId": 255440402
                },
                "corpusId": 255440402,
                "publicationVenue": {
                    "id": "67d171e0-fd12-4512-a35d-c4d7af1bd5b3",
                    "name": "ACM Conference on Health, Inference, and Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CHIL",
                        "ACM Conf Health Inference Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0113935b640d76e6cec5e2cd8cb98193158a636c",
                "title": "Semantic match: Debugging feature attribution methods in XAI for healthcare",
                "abstract": "The recent spike in certified Artificial Intelligence (AI) tools for healthcare has renewed the debate around adoption of this technology. One thread of such debate concerns Explainable AI (XAI) and its promise to render AI devices more transparent and trustworthy. A few voices active in the medical AI space have expressed concerns on the reliability of Explainable AI techniques and especially feature attribution methods, questioning their use and inclusion in guidelines and standards. Despite valid concerns, we argue that existing criticism on the viability of post-hoc local explainability methods throws away the baby with the bathwater by generalizing a problem that is specific to image data. We begin by characterizing the problem as a lack of semantic match between explanations and human understanding. To understand when feature importance can be used reliably, we introduce a distinction between feature importance of low- and high-level features. We argue that for data types where low-level features come endowed with a clear semantics, such as tabular data like Electronic Health Records (EHRs), semantic match can be obtained, and thus feature attribution methods can still be employed in a meaningful and useful way. Finally, we sketch a procedure to test whether semantic match has been achieved.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "32346780",
                        "name": "G. Cin\u00e1"
                    },
                    {
                        "authorId": "2193550099",
                        "name": "Tabea E. Rober"
                    },
                    {
                        "authorId": "51491250",
                        "name": "R. Goedhart"
                    },
                    {
                        "authorId": "51257523",
                        "name": "cS. .Ilker Birbil"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[99] proposed the Orthogonal and Diversity based LSTMmodels.",
                "We assume a generic model framework and notations [57, 99].",
                "In general attention based explanations are characterised as faithful and plausible [99]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "4790b411835089fc593b154d3c661f5ccfc05ae6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-07126",
                    "ArXiv": "2212.07126",
                    "DOI": "10.48550/arXiv.2212.07126",
                    "CorpusId": 254636492
                },
                "corpusId": 254636492,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4790b411835089fc593b154d3c661f5ccfc05ae6",
                "title": "Explainability of Text Processing and Retrieval Methods: A Critical Survey",
                "abstract": "Deep Learning and Machine Learning based models have become extremely popular in text processing and information retrieval. However, the non-linear structures present inside the networks make these models largely inscrutable. A significant body of research has focused on increasing the transparency of these models. This article provides a broad overview of research on the explainability and interpretability of natural language processing and information retrieval methods. More specifically, we survey approaches that have been applied to explain word embeddings, sequence modeling, attention modules, transformers, BERT, and document ranking. The concluding section suggests some possible directions for future research on this topic.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152623586",
                        "name": "Sourav Saha"
                    },
                    {
                        "authorId": "1956144",
                        "name": "Debapriyo Majumdar"
                    },
                    {
                        "authorId": "1798723",
                        "name": "Mandar Mitra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Mohankumar et al. 2020) explores to modify the LSTM cell with diversity driven training to enhance explainability and transparency of attention modules."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e2f4c22090073ddd63708bea61de1e5a5bc905f4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-13290",
                    "ArXiv": "2211.13290",
                    "DOI": "10.48550/arXiv.2211.13290",
                    "CorpusId": 254017687
                },
                "corpusId": 254017687,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e2f4c22090073ddd63708bea61de1e5a5bc905f4",
                "title": "SEAT: Stable and Explainable Attention",
                "abstract": "Attention mechanism has become a standard fixture in many state-of-the-art natural language processing (NLP) models, not only due to its outstanding performance, but also because it provides plausible innate explanations for neural architectures. However, recent studies show that attention is unstable against randomness and perturbations during training or testing, such as random seeds and slight perturbation of embeddings, which impedes it from being a faithful explanation tool. Thus, a natural question is whether we can find an alternative to vanilla attention, which is more stable and could keep the key characteristics of the explanation. In this paper, we provide a rigorous definition of such an attention method named SEAT (Stable and Explainable ATtention). Specifically, SEAT has the following three properties: (1) Its prediction distribution is close to the prediction of the vanilla attention; (2) Its top-k indices largely overlap with those of the vanilla attention; (3) It is robust w.r.t perturbations, i.e., any slight perturbation on SEAT will not change the attention and prediction distribution too much, which implicitly indicates that it is stable to randomness and perturbations. Furthermore, we propose an optimization method for obtaining SEAT, which could be considered as revising the vanilla attention. Finally, through intensive experiments on various datasets, we compare our SEAT with other baseline methods using RNN, BiLSTM and BERT architectures, with different evaluation metrics on model interpretation, stability and accuracy. Results show that, besides preserving the original explainability and model performance, SEAT is more stable against input perturbations and training randomness, which indicates it is a more faithful explanation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153121378",
                        "name": "Lijie Hu"
                    },
                    {
                        "authorId": null,
                        "name": "Yixin Liu"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    },
                    {
                        "authorId": "2925985",
                        "name": "Mengdi Huai"
                    },
                    {
                        "authorId": "46732871",
                        "name": "Lichao Sun"
                    },
                    {
                        "authorId": "2119265639",
                        "name": "Di Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For regularization methods, we conducted a grid search with parameter grid [0.1, 0.3, 0.5, 1, 5, 10] for CONICITY and [0.1, 0.3, 0.5, 1, 5, 10, 20] for TYING.",
                "Taking a step further, we applied two regularization techniques, TYING and CONICITY, originally aimed at increasing faithfulness of attention explanations, with the hypothesis that the issue underpinning disagreements and unfaithfulness is the same \u2013 representation entanglement in the hidden space.",
                "C L\n] 1\n1 M\nay 2\n02 3\ntanglement significantly improve the faithfulness of attention-based explanations (Mohankumar et al., 2020; Tutek and \u0160najder, 2020).",
                "CONICITY aims to increase the angle between each hidden representation and the mean of the hidden representations of a single instance.",
                "Although both works introduced other methods of enforcing differences between hidden states, namely orthogonal-LSTM and masked language modeling as an auxiliary task, we opt for CONICITY and TYING as they were both shown to be more efficient and more stable in practice.",
                "In Table 2 we report correlation scores on the test splits of all datasets for regularized models (CONICITY, TYING) and their unregularized variants (BASE).",
                "tanglement significantly improve the faithfulness of attention-based explanations (Mohankumar et al., 2020; Tutek and \u0160najder, 2020).",
                "We include the results for all datasets across training epochs for regularized models (CONICITY, TYING) when compared to their unregularized, BASE variants.",
                "We hypothesize that the cause of saliency method disagreements is rooted in representation entanglement and experimentally show that agreement can be significantly improved by regularization techniques such as tying (Tutek and \u0160najder, 2020) and conicity (Mohankumar et al., 2020).",
                "To counteract this issue, we employ two regularization schemes that have been shown to improve the faithfulness of the attention mechanism as a method of interpretability: CONICITY (Mohankumar et al., 2020) and TYING (Tutek and \u0160najder, 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f54f5c7bf51cd0a4084b4cf5cb7a34f7d98c6bac",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-08369",
                    "ArXiv": "2211.08369",
                    "DOI": "10.48550/arXiv.2211.08369",
                    "CorpusId": 253523215
                },
                "corpusId": 253523215,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/f54f5c7bf51cd0a4084b4cf5cb7a34f7d98c6bac",
                "title": "Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods",
                "abstract": "A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful has been to use evaluation-by-agreement -- if multiple methods agree on an explanation, its credibility increases. However, recent work has found that saliency methods exhibit weak rank correlations even when applied to the same model instance and advocated for the use of alternative diagnostic methods. In our work, we demonstrate that rank correlation is not a good fit for evaluating agreement and argue that Pearson-$r$ is a better-suited alternative. We further show that regularization techniques that increase faithfulness of attention explanations also increase agreement between saliency methods. By connecting our findings to instance categories based on training dynamics, we show that the agreement of saliency method explanations is very low for easy-to-learn instances. Finally, we connect the improvement in agreement across instance categories to local representation space statistics of instances, paving the way for work on analyzing which intrinsic model properties improve their predisposition to interpretability methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2190692367",
                        "name": "Josip Jukic"
                    },
                    {
                        "authorId": "3466460",
                        "name": "Martin Tutek"
                    },
                    {
                        "authorId": "143809437",
                        "name": "J. \u0160najder"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d79aee8d08fdfcc93da7e21eee73536594c45fc5",
                "externalIds": {
                    "DBLP": "journals/cacm/Balasubramanian22",
                    "DOI": "10.1145/3550491",
                    "CorpusId": 253022328
                },
                "corpusId": 253022328,
                "publicationVenue": {
                    "id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
                    "name": "Communications of the ACM",
                    "type": "journal",
                    "alternate_names": [
                        "Commun ACM",
                        "Communications of The ACM"
                    ],
                    "issn": "0001-0782",
                    "url": "http://www.acm.org/pubs/cacm/",
                    "alternate_urls": [
                        "http://portal.acm.org/cacm",
                        "http://www.acm.org/pubs/contents/journals/cacm/",
                        "https://cacm.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d79aee8d08fdfcc93da7e21eee73536594c45fc5",
                "title": "Toward explainable deep learning",
                "abstract": "68 COMMUNICATIONS OF THE ACM | NOVEMBER 2022 | VOL. 65 | NO. 11 I M A G E C O U R T E S Y O F N I T I A A Y O G the model; a regulator may want the explanation to support the fairness of decisionmaking, while a customer support agent may want to respond accordingly to a customer query. This subjectivity necessitates a multipronged technical approach, so a suitable approach can be chosen for a specific application and user context. Researchers across academic and industry organizations in India have explored the explainability of DL models in recent years. A specific catalyst of these efforts was the development of explainable COVID-19 risk prediction models to support decisionmaking during the pandemic over the last two years.10,12,17 Noteworthy efforts from research groups in India have focused on the transparency of DL models, especially in computer vision and natural language processing. tions using logic and neurosymbolic reasoning.1,21,22 Industry researchers in India have also led and contributed to developing practical, useful software toolkits for explainability and its use in AIOps.3,4 Our extensive efforts at IIT-Hyderabad have mirrored the need to address the explainability of DL models from multiple perspectives, to benefit different groups of users in different settings. From a post-hoc explainability perspective (methods to explain a previously trained model), one of our earliest efforts, GradCAM++,19 aimed to develop a generalized gradient-based visual explanation method for convolutional neural networks (CNNs) by considering pixel-level contributions from a given CNN layer toward the predicted output. This method has been widely used around the world for applications including healthcare, bioinformatics, agriculture, and energy informatics. We have since extended such a gradient-based post-hoc perspective to obtain 3D model-normalized saliency maps for face image understanding in John et al.7 Complementarily, ante-hoc interpretability methods seek to bake the capability to provide explanations, along with a prediction, into a model\u2019s training process itself. Such methods help provide accountability to a model\u2019s decision, whereas post-hoc methods may have two different modules to predict and explain, respectively, Answering the question: \u201cWhich part of the input image or document did the model look at while making its prediction?\u201d is essential to validate DL model predictions with human understanding, and thereby increase the trust of human users in model predictions. To this end, efforts have been developed on providing saliency maps (regions of an image a DL model looks at while making a prediction) through gradient-based19 and gradient-free methods6 in computer vision. Similar methods to provide transparency in attention-based language models13 also have been proposed. Looking forward, moving toward nextgeneration AI systems that can reason and strategize, Indian researchers have also addressed the integration of commonsense reasoning in language models,2 as well as obtaining model explanaD E E P LE A R NING ( D L) models have enjoyed tremendous success across application domains within the broader umbrella of artificial intelligence (AI) technologies. However, their \u201cblack-box\u201d nature, coupled with their extensive use across application sectors\u2014including safety-critical and risk-sensitive ones such as healthcare, finance, aerospace, law enforcement, and governance\u2014 has elicited an increasing need for explainability, interpretability, and transparency of decision-making in these models.11,14,18,24 With the recent progression of legal and policy frameworks that mandate explaining decisions made by AI-driven systems (for example, the European Union\u2019s GDPR Article 15(1)(h) and the Algorithmic Accountability Act of 2019 in the U.S.), explainability has become a cornerstone of responsible AI use and deployment. In the Indian context, NITI Aayog recently released a two-part strategy document on envisioning and operationalizing Responsible AI in India,15,16 which puts significant emphasis on the explainability and transparency of AI models. Explainability of DL models lies at the human-machine interface, and different users may expect different explanations in different contexts. A data scientist may want an explanation to help improve Artificial Intelligence | DOI:10.1145/3550491",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1699429",
                        "name": "V. Balasubramanian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "221eaf42440c7ae3cca610ac57dd9ba65c6b129c",
                "externalIds": {
                    "ArXiv": "2210.06394",
                    "DBLP": "journals/corr/abs-2210-06394",
                    "DOI": "10.48550/arXiv.2210.06394",
                    "CorpusId": 252846382
                },
                "corpusId": 252846382,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/221eaf42440c7ae3cca610ac57dd9ba65c6b129c",
                "title": "On Text Style Transfer via Style Masked Language Models",
                "abstract": "Text Style Transfer (TST) is performable through approaches such as latent space disentanglement, cycle-consistency losses, prototype editing etc. The prototype editing approach, which is known to be quite successful in TST, involves two key phases a) Masking of source style-associated tokens and b) Reconstruction of this source-style masked sentence conditioned with the target style. We follow a similar transduction method, in which we transpose the more difficult direct source to target TST task to a simpler Style-Masked Language Model (SMLM) Task, wherein, similar to BERT \\cite{bert}, the goal of our model is now to reconstruct the source sentence from its style-masked version. We arrive at the SMLM mechanism naturally by formulating prototype editing/ transduction methods in a probabilistic framework, where TST resolves into estimating a hypothetical parallel dataset from a partially observed parallel dataset, wherein each domain is assumed to have a common latent style-masked prior. To generate this style-masked prior, we use\"Explainable Attention\"as our choice of attribution for a more precise style-masking step and also introduce a cost-effective and accurate\"Attribution-Surplus\"method of determining the position of masks from any arbitrary attribution model in O(1) time. We empirically show that this non-generational approach well suites the\"content preserving\"criteria for a task like TST, even for a complex style like Discourse Manipulation. Our model, the Style MLM, outperforms strong TST baselines and is on par with state-of-the-art TST models, which use complex architectures and orders of more parameters.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1516227958",
                        "name": "Sharan Narasimhan"
                    },
                    {
                        "authorId": "2061798273",
                        "name": "P. Shekar"
                    },
                    {
                        "authorId": "29832722",
                        "name": "Suvodip Dey"
                    },
                    {
                        "authorId": "2481485",
                        "name": "M. Desarkar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Attention mechanism depends on accurately distributing attention weights for each prediction, and its vulnerability to perturbation leads to incorrect translation or over translation in neural machine translation [2], [3]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f6e2ad0fe7fe6153a70a3e1cb1a612c27727bbc2",
                "externalIds": {
                    "DBLP": "conf/ictai/FanA22",
                    "DOI": "10.1109/ICTAI56018.2022.00022",
                    "CorpusId": 258220206
                },
                "corpusId": 258220206,
                "publicationVenue": {
                    "id": "ba1e9488-a629-4abe-a0c2-ec2c79c91616",
                    "name": "IEEE International Conference on Tools with Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Tool Artif Intell",
                        "ICTAI",
                        "IEEE Int Conf Tool Artif Intell",
                        "International Conference on Tools with Artificial Intelligence"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1488"
                },
                "url": "https://www.semanticscholar.org/paper/f6e2ad0fe7fe6153a70a3e1cb1a612c27727bbc2",
                "title": "Attention Constraint Mechanism through Auxiliary Attention",
                "abstract": "In current translation tasks, we discover that the weight distribution of attention has a fuzziness problem, specif-ically, its imperfection in detecting definitive information. In-tuitively, the most straightforward idea to alleviate fuzziness is to increase the number of attention heads at the expense of model complexities. However, our experiments prove this approach to be inefficient, and the effect is barely measurable. Instead, to tackle this problem, we explore an attention con-straint mechanism by introducing auxiliary attention. Instead of simply accumulating or averaging the base attention with auxiliary attention, we build a mathematical constraint function between them. The constraint function calibrates the original attention and further improves the model performance. Extensive experiments on neural machine translation tasks and abstract summarization tasks demonstrate that our attention constraint method can substantially outperform the strong baselines. The exact mathematical formulas and experimental analysis reveal the influence of constraint function on attention: attention tends to assign more weights to the decisive inputs, becoming sparse and sharp. Our code is available on Github11https://github.com/auxiliaryattention/AttentionConstraintMechanism.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2214834055",
                        "name": "Yingda Fan"
                    },
                    {
                        "authorId": "3210443",
                        "name": "Amrinder Arora"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[69] argue that when the attention distribution is computed on input representations that are very similar to each other, they cannot provide very meaningful explanations.",
                "For this purpose, the authors of [69] diversify the hidden representations over which the distribution are computed for more faithful explanations."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8af69a903a14657ec96b93c4b6e139771beec106",
                "externalIds": {
                    "ArXiv": "2210.06929",
                    "DBLP": "journals/corr/abs-2210-06929",
                    "DOI": "10.1145/3529755",
                    "CorpusId": 250624746
                },
                "corpusId": 250624746,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8af69a903a14657ec96b93c4b6e139771beec106",
                "title": "On the Explainability of Natural Language Processing Deep Models",
                "abstract": "Despite their success, deep networks are used as black-box models with outputs that are not easily explainable during the learning and the prediction phases. This lack of interpretability is significantly limiting the adoption of such models in domains where decisions are critical such as the medical and legal fields. Recently, researchers have been interested in developing methods that help explain individual decisions and decipher the hidden representations of machine learning models in general and deep networks specifically. While there has been a recent explosion of work on Explainable Artificial Intelligence (ExAI) on deep models that operate on imagery and tabular data, textual datasets present new challenges to the ExAI community. Such challenges can be attributed to the lack of input structure in textual data, the use of word embeddings that add to the opacity of the models and the difficulty of the visualization of the inner workings of deep models when they are trained on textual data. Lately, methods have been developed to address the aforementioned challenges and present satisfactory explanations on Natural Language Processing (NLP) models. However, such methods are yet to be studied in a comprehensive framework where common challenges are properly stated and rigorous evaluation practices and metrics are proposed. Motivated to democratize ExAI methods in the NLP field, we present in this work a survey that studies model-agnostic as well as model-specific explainability methods on NLP models. Such methods can either develop inherently interpretable NLP models or operate on pre-trained models in a post hoc manner. We make this distinction and we further decompose the methods into three categories according to what they explain: (1) word embeddings (input level), (2) inner workings of NLP models (processing level), and (3) models\u2019 decisions (output level). We also detail the different evaluation approaches interpretability methods in the NLP field. Finally, we present a case-study on the well-known neural machine translation in an appendix, and we propose promising future research directions for ExAI in the NLP field.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8160667",
                        "name": "Julia El Zini"
                    },
                    {
                        "authorId": "144707373",
                        "name": "M. Awad"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some works also aim to employ masks as the analytical tools to indicate the importance (Kitada and Iyatomi 2020; Mohankumar et al. 2020), attention head (Fong and Vedaldi 2017), or the contributions of the pixels in the image to the model outputs (Voita et al. 2019).",
                "Some works also aim to employ masks as the analytical tools to indicate the importance (Kitada and Iyatomi 2020; Mohankumar et al. 2020), attention head (Fong and Vedaldi 2017), or the contributions of the pixels in the image to the model outputs (Voita et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "977f49f96ec7b532c29576e468adbd140c502810",
                "externalIds": {
                    "DBLP": "conf/aaai/Fei22",
                    "DOI": "10.1609/aaai.v36i1.19940",
                    "CorpusId": 250288254
                },
                "corpusId": 250288254,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/977f49f96ec7b532c29576e468adbd140c502810",
                "title": "Attention-Aligned Transformer for Image Captioning",
                "abstract": "Recently, attention-based image captioning models, which are expected to ground correct image regions for proper word generations, have achieved remarkable performance. However, some researchers have argued \u201cdeviated focus\u201d problem of existing attention mechanisms in determining the effective and influential image features. In this paper, we present A2 - an attention-aligned Transformer for image captioning, which guides attention learning in a perturbation-based self-supervised manner, without any annotation overhead. Specifically, we add mask operation on image regions through a learnable network to estimate the true function in ultimate description generation. We hypothesize that the necessary image region features, where small disturbance causes an obvious performance degradation, deserve more attention weight. Then, we propose four aligned strategies to use this information to refine attention weight distribution. Under such a pattern, image regions are attended correctly with the output words. Extensive experiments conducted on the MS COCO dataset demonstrate that the proposed A2 Transformer consistently outperforms baselines in both automatic metrics and human evaluation. Trained models and code for reproducing the experiments are publicly available.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2066415714",
                        "name": "Zhengcong Fei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The attention mechanism [45, 46] is one of the most popular methods, which uses the computer vision system to mimic the human visual system so as to focus on the regions of interest."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "465f3ea576b8f9b05d86d3ae22b74e112f4e1e9a",
                "externalIds": {
                    "PubMedCentral": "9243744",
                    "DBLP": "journals/mms/TengLSHL22",
                    "DOI": "10.1007/s00530-022-00960-4",
                    "CorpusId": 250057074,
                    "PubMed": "35789785"
                },
                "corpusId": 250057074,
                "publicationVenue": {
                    "id": "d1997ea9-9d41-4458-9280-94feb013bd15",
                    "name": "Multimedia Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Syst"
                    ],
                    "issn": "0942-4962",
                    "url": "http://www.springer.com/computer/information+systems+and+applications/journal/530?changeHeader",
                    "alternate_urls": [
                        "https://link.springer.com/journal/530",
                        "http://www.springer.com/computer/information+systems+and+applications/journal/530?changeHeader="
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/465f3ea576b8f9b05d86d3ae22b74e112f4e1e9a",
                "title": "A survey on the interpretability of deep learning in medical diagnosis",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2173581665",
                        "name": "Qiaoying Teng"
                    },
                    {
                        "authorId": "47781621",
                        "name": "Zhe Liu"
                    },
                    {
                        "authorId": "40280182",
                        "name": "Yuqing Song"
                    },
                    {
                        "authorId": "145916712",
                        "name": "K. Han"
                    },
                    {
                        "authorId": "2146560089",
                        "name": "Yang Lu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Attention mechanism [30], one of the early approaches that incorporated explainability during the training phase, fail to consider network layers succeeding the attention layer [31], [32]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2ea13fb6bf442ae4c37a7f443420853ae0330a49",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-09943",
                    "ArXiv": "2205.09943",
                    "DOI": "10.1109/IJCNN55064.2022.9892273",
                    "CorpusId": 248964947
                },
                "corpusId": 248964947,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/2ea13fb6bf442ae4c37a7f443420853ae0330a49",
                "title": "Explainable Supervised Domain Adaptation",
                "abstract": "Domain adaptation techniques have contributed to the success of deep learning. Leveraging knowledge from an auxiliary source domain for learning in labeled data-scarce target domain is fundamental to domain adaptation. While these techniques result in increasing accuracy, the adaptation process, particularly the knowledge leveraged from the source domain, remains unclear. This paper proposes an explainable by design supervised domain adaptation framework - XSDA-Net. We integrate a case-based reasoning mechanism into the XSDA-Net to explain the prediction of a test instance in terms of similar-looking regions in the source and target train images. We empirically demonstrate the utility of the proposed framework by curating the domain adaptation settings on datasets popularly known to exhibit part-based explainability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "88991725",
                        "name": "V. Kamakshi"
                    },
                    {
                        "authorId": "2503137",
                        "name": "N. C. Krishnan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[12] show that minimizing hidden state conicity in a BiLSTM improves the Pearson correlation of attention weights with Integrated Gradients [13] attributions."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b5dca588dc5006feef49781ac82763b40788ce04",
                "externalIds": {
                    "ArXiv": "2205.04559",
                    "DBLP": "conf/hhai/NeelySBL22",
                    "DOI": "10.48550/arXiv.2205.04559",
                    "CorpusId": 248665931
                },
                "corpusId": 248665931,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b5dca588dc5006feef49781ac82763b40788ce04",
                "title": "A Song of (Dis)agreement: Evaluating the Evaluation of Explainable Artificial Intelligence in Natural Language Processing",
                "abstract": "There has been significant debate in the NLP community about whether or not attention weights can be used as an explanation - a mechanism for interpreting how important each input token is for a particular prediction. The validity of\"attention as explanation\"has so far been evaluated by computing the rank correlation between attention-based explanations and existing feature attribution explanations using LSTM-based models. In our work, we (i) compare the rank correlation between five more recent feature attribution methods and two attention-based methods, on two types of NLP tasks, and (ii) extend this analysis to also include transformer-based models. We find that attention-based explanations do not correlate strongly with any recent feature attribution methods, regardless of the model or task. Furthermore, we find that none of the tested explanations correlate strongly with one another for the transformer-based model, leading us to question the underlying assumption that we should measure the validity of attention-based explanations based on how well they correlate with existing feature attribution explanation methods. After conducting experiments on five datasets using two different models, we argue that the community should stop using rank correlation as an evaluation metric for attention-based explanations. We suggest that researchers and practitioners should instead test various explanation methods and employ a human-in-the-loop process to determine if the explanations align with human intuition for the particular use case at hand.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113260008",
                        "name": "Michael Neely"
                    },
                    {
                        "authorId": "89933535",
                        "name": "Stefan F. Schouten"
                    },
                    {
                        "authorId": "1452678770",
                        "name": "Maurits J. R. Bleeker"
                    },
                    {
                        "authorId": "38702106",
                        "name": "Ana Lucic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several researchers proposed kernels designed around constituent parse trees to capture sentence grammatical structure (Miller et al. 2000; Zelenko, Aone, and Richardella 2003; Moschitti 2006)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b91a9accfad959a1b3a0cfd7e9f6e6d5a967463e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-11424",
                    "ArXiv": "2204.11424",
                    "ACL": "2023.cl-1.3",
                    "DOI": "10.1162/coli_a_00463",
                    "CorpusId": 248377455
                },
                "corpusId": 248377455,
                "publicationVenue": {
                    "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
                    "name": "International Conference on Computational Logic",
                    "type": "conference",
                    "alternate_names": [
                        "CL",
                        "Int Conf Comput Log"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b91a9accfad959a1b3a0cfd7e9f6e6d5a967463e",
                "title": "It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers",
                "abstract": "We propose an explainable approach for relation extraction that mitigates the tension between generalization and explainability by jointly training for the two goals. Our approach uses a multi-task learning architecture, which jointly trains a classifier for relation extraction, and a sequence model that labels words in the context of the relations that explain the decisions of the relation classifier. We also convert the model outputs to rules to bring global explanations to this approach. This sequence model is trained using a hybrid strategy: supervised, when supervision from pre-existing patterns is available, and semi-supervised otherwise. In the latter situation, we treat the sequence model\u2019s labels as latent variables, and learn the best assignment that maximizes the performance of the relation classifier. We evaluate the proposed approach on the two datasets and show that the sequence model provides labels that serve as accurate explanations for the relation classifier\u2019s decisions, and, importantly, that the joint training generally improves the performance of the relation classifier. We also evaluate the performance of the generated rules and show that the new rules are a great add-on to the manual rules and bring the rule-based system much closer to the neural models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112472583",
                        "name": "Zheng Tang"
                    },
                    {
                        "authorId": "1760868",
                        "name": "M. Surdeanu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Mohankumar et al. (2020) follows up prior work to note that the distribution of attention fails to fall on important words and strays to unimportant tokens."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cd471b5ef162906ef3d9a84398b3f98e9ee4bf56",
                "externalIds": {
                    "ArXiv": "2204.06031",
                    "DBLP": "journals/corr/abs-2204-06031",
                    "DOI": "10.48550/arXiv.2204.06031",
                    "CorpusId": 248157206
                },
                "corpusId": 248157206,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cd471b5ef162906ef3d9a84398b3f98e9ee4bf56",
                "title": "A Review on Language Models as Knowledge Bases",
                "abstract": "Recently, there has been a surge of interest in the NLP community on the use of pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have shown that LMs trained on a sufficiently large (web) corpus will encode a significant amount of knowledge implicitly in its parameters. The resulting LM can be probed for different kinds of knowledge and thus acting as a KB. This has a major advantage over traditional KBs in that this method requires no human supervision. In this paper, we present a set of aspects that we deem a LM should have to fully act as a KB, and review the recent literature with respect to those aspects.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2006905770",
                        "name": "Badr AlKhamissi"
                    },
                    {
                        "authorId": "2162508414",
                        "name": "Millicent Li"
                    },
                    {
                        "authorId": "1709797",
                        "name": "Asli Celikyilmaz"
                    },
                    {
                        "authorId": "1700007",
                        "name": "Mona T. Diab"
                    },
                    {
                        "authorId": "2320509",
                        "name": "Marjan Ghazvininejad"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5a71bf38cf409b55b14b2d5159c0b06bef9ad603",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-14263",
                    "ArXiv": "2203.14263",
                    "DOI": "10.1109/TKDE.2021.3126456",
                    "CorpusId": 243973878
                },
                "corpusId": 243973878,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5a71bf38cf409b55b14b2d5159c0b06bef9ad603",
                "title": "A General Survey on Attention Mechanisms in Deep Learning",
                "abstract": "Attention is an important mechanism that can be employed for a variety of deep learning models across many different domains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The various attention mechanisms are explained by means of a framework consisting of a general attention model, uniform notation, and a comprehensive taxonomy of attention mechanisms. Furthermore, the various measures for evaluating attention models are reviewed, and methods to characterize the structure of attention models based on the proposed framework are discussed. Last, future work in the field of attention models is considered.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2089014461",
                        "name": "Gianni Brauwers"
                    },
                    {
                        "authorId": "1729599",
                        "name": "F. Frasincar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, recent extensive evaluations in NLP tasks (Serrano & Smith, 2019; Jain & Wallace, 2019; Mohankumar et al., 2020) have shown that the attention may not weigh the features that dominate the model output more than other features."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "68c25a2dcb4df9632996fdcb078ff3bed8300a9c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-12987",
                    "ArXiv": "2201.12987",
                    "CorpusId": 246430773
                },
                "corpusId": 246430773,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/68c25a2dcb4df9632996fdcb078ff3bed8300a9c",
                "title": "Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism",
                "abstract": "Interpretable graph learning is in need as many scientific applications depend on learning models to collect insights from graph-structured data. Previous works mostly focused on using post-hoc approaches to interpret pre-trained models (graph neural networks in particular). They argue against inherently interpretable models because the good interpretability of these models is often at the cost of their prediction accuracy. However, those post-hoc methods often fail to provide stable interpretation and may extract features that are spuriously correlated with the task. In this work, we address these issues by proposing Graph Stochastic Attention (GSAT). Derived from the information bottleneck principle, GSAT injects stochasticity to the attention weights to block the information from task-irrelevant graph components while learning stochasticity-reduced attention to select task-relevant subgraphs for interpretation. The selected subgraphs provably do not contain patterns that are spuriously correlated with the task under some assumptions. Extensive experiments on eight datasets show that GSAT outperforms the state-of-the-art methods by up to 20%$\\uparrow$ in interpretation AUC and 5%$\\uparrow$ in prediction accuracy. Our code is available at https://github.com/Graph-COM/GSAT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151793768",
                        "name": "Siqi Miao"
                    },
                    {
                        "authorId": "2156102035",
                        "name": "Miaoyuan Liu"
                    },
                    {
                        "authorId": "1561672016",
                        "name": "Pan Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following prior studies (Jain & Wallace, 2019; Mohankumar et al., 2020), we conduct extensive experiments on six exemplar tasks, for which attention models are widely applied.",
                "\u2026puzzles in inherent attention explanations, such as whether attention is directly explainable (Jain & Wallace, 2019; Serrano & Smith, 2019; Wiegreffe & Pinter, 2019; Brunner et al., 2020), or how attention behaves (Clark et al., 2019; Bai et al., 2021; Michel et al., 2019; Mohankumar et al., 2020).",
                ", 2020), or how attention behaves (Clark et al., 2019; Bai et al., 2021; Michel et al., 2019; Mohankumar et al., 2020).",
                "For example, Mohankumar et al. (2020) explored why attention fails to explain LSTM models and pointed out that the similar hidden states in the encoder restrict the significance of attention weights."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5fdb05e17fd503b8dbdbadc338e0a00829929dcc",
                "externalIds": {
                    "DBLP": "conf/icml/LiuLGKL022",
                    "ArXiv": "2201.12114",
                    "CorpusId": 246411233
                },
                "corpusId": 246411233,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5fdb05e17fd503b8dbdbadc338e0a00829929dcc",
                "title": "Rethinking Attention-Model Explainability through Faithfulness Violation Test",
                "abstract": "Attention mechanisms are dominating the explainability of deep models. They produce probability distributions over the input, which are widely deemed as feature-importance indicators. However, in this paper, we find one critical limitation in attention explanations: weakness in identifying the polarity of feature impact. This would be somehow misleading -- features with higher attention weights may not faithfully contribute to model predictions; instead, they can impose suppression effects. With this finding, we reflect on the explainability of current attention-based techniques, such as Attentio$\\odot$Gradient and LRP-based attention explanations. We first propose an actionable diagnostic methodology (henceforth faithfulness violation test) to measure the consistency between explanation weights and the impact polarity. Through the extensive experiments, we then show that most tested explanation methods are unexpectedly hindered by the faithfulness violation issue, especially the raw attention. Empirical analyses on the factors affecting violation issues further provide useful observations for adopting explanation methods in attention models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35435925",
                        "name": "Y. Liu"
                    },
                    {
                        "authorId": "144878149",
                        "name": "Haoliang Li"
                    },
                    {
                        "authorId": "30921555",
                        "name": "Yangyang Guo"
                    },
                    {
                        "authorId": "144332826",
                        "name": "Chen Kong"
                    },
                    {
                        "authorId": "2152914623",
                        "name": "Jing Li"
                    },
                    {
                        "authorId": "2144190725",
                        "name": "Shiqi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7caaafd5a3ee033c98e792c7ea5b699d005753d5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08164",
                    "ArXiv": "2201.08164",
                    "DOI": "10.1145/3583558",
                    "CorpusId": 246063780
                },
                "corpusId": 246063780,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7caaafd5a3ee033c98e792c7ea5b699d005753d5",
                "title": "From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI",
                "abstract": "The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the past 7 years at major AI and ML conferences that introduce an XAI method. We find that one in three papers evaluate exclusively with anecdotal evidence, and one in five papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark, and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training to optimize for accuracy and interpretability simultaneously.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "17698891",
                        "name": "Meike Nauta"
                    },
                    {
                        "authorId": "52019849",
                        "name": "Jan Trienes"
                    },
                    {
                        "authorId": "66163851",
                        "name": "Shreyasi Pathak"
                    },
                    {
                        "authorId": "13407092",
                        "name": "Elisa Nguyen"
                    },
                    {
                        "authorId": "2066935841",
                        "name": "Michelle Peters"
                    },
                    {
                        "authorId": "2150574981",
                        "name": "Yasmin Schmitt"
                    },
                    {
                        "authorId": "3044872",
                        "name": "J\u00f6rg Schl\u00f6tterer"
                    },
                    {
                        "authorId": "1711719",
                        "name": "M. V. Keulen"
                    },
                    {
                        "authorId": "145566115",
                        "name": "C. Seifert"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although, the usefulness of the interpretation of attention weights is controversial [97, 138, 214]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "592f8009798be4b2a373ae2089d70705fcac289e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-14266",
                    "ArXiv": "2203.14266",
                    "DOI": "10.1145/3503044",
                    "CorpusId": 245387017
                },
                "corpusId": 245387017,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/592f8009798be4b2a373ae2089d70705fcac289e",
                "title": "A Survey on Aspect-Based Sentiment Classification",
                "abstract": "With the constantly growing number of reviews and other sentiment-bearing texts on the Web, the demand for automatic sentiment analysis algorithms continues to expand. Aspect-based sentiment classification (ABSC) allows for the automatic extraction of highly fine-grained sentiment information from text documents or sentences. In this survey, the rapidly evolving state of the research on ABSC is reviewed. A novel taxonomy is proposed that categorizes the ABSC models into three major categories: knowledge-based, machine learning, and hybrid models. This taxonomy is accompanied with summarizing overviews of the reported model performances, and both technical and intuitive explanations of the various ABSC models. State-of-the-art ABSC models are discussed, such as models based on the transformer model, and hybrid deep learning models that incorporate knowledge bases. Additionally, various techniques for representing the model inputs and evaluating the model outputs are reviewed. Furthermore, trends in the research on ABSC are identified and a discussion is provided on the ways in which the field of ABSC can be advanced in the future.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2089014461",
                        "name": "Gianni Brauwers"
                    },
                    {
                        "authorId": "1729599",
                        "name": "F. Frasincar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, the attention can still scatter in irrelevant channels as long as it attends to the most important one [11]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "202c665dd8daae49d9cafb5694b2fafe22eb9406",
                "externalIds": {
                    "DBLP": "conf/globecom/ChanS21",
                    "DOI": "10.1109/GLOBECOM46510.2021.9685864",
                    "CorpusId": 246477937
                },
                "corpusId": 246477937,
                "publicationVenue": {
                    "id": "b189dec0-41d0-4cea-a906-7c5186895904",
                    "name": "Global Communications Conference",
                    "type": "conference",
                    "alternate_names": [
                        "GLOBECOM",
                        "Glob Commun Conf"
                    ],
                    "url": "http://www.ieee-globecom.org/"
                },
                "url": "https://www.semanticscholar.org/paper/202c665dd8daae49d9cafb5694b2fafe22eb9406",
                "title": "Explainable Health State Prediction for Social IoTs through Multi-Channel Attention",
                "abstract": "The core technology of Industry 4.0 is to enable the intelligence of manufacturing. One of the important tasks is anomaly detection. Although existing anomaly detection methods have achieved high accuracy, the basis of judgments cannot provide explainability, which greatly reduces the possibility for improving the model or facilitating human-machine cooperation. Therefore, in this paper, the goal is to provide the explainability for machine fault detection for social IoTs and realize the health monitoring and prognosis of the bearings simultaneously. Specifically, vibration signals from multiple sensors are transformed into spectrograms by short-time Fourier transform. Afterward, the features of frequency-domain data are extracted by the Squeeze-and-Excitation block and self-attention mechanism to assess the degradation of whole system. As such, when the process enters the early degradation, the source of components that causes the abnormality can be identified through the attention weight distribution. Experimental results show that the proposed approach achieves high accuracy in run-to-failure tests. Moreover, the proposed approach shows a better ability to explain the predicted results than the state-of-the-art bearing detection methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152085306",
                        "name": "Yu-Li Chan"
                    },
                    {
                        "authorId": "2426757",
                        "name": "Hong-Han Shuai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0a61f31a48f21dc0d3feeb959c504a2de9851fae",
                "externalIds": {
                    "ArXiv": "2111.04927",
                    "DBLP": "journals/corr/abs-2111-04927",
                    "CorpusId": 243860744
                },
                "corpusId": 243860744,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0a61f31a48f21dc0d3feeb959c504a2de9851fae",
                "title": "Self-Interpretable Model with TransformationEquivariant Interpretation",
                "abstract": "In this paper, we propose a self-interpretable model SITE with transformation-equivariant interpretations. We focus on the robustness and self-consistency of the interpretations of geometric transformations. Apart from the transformation equivariance, as a self-interpretable model, SITE has comparable expressive power as the benchmark black-box classifiers, while being able to present faithful and robust interpretations with high quality. It is worth noticing that although applied in most of the CNN visualization methods, the bilinear upsampling approximation is a rough approximation, which can only provide interpretations in the form of heatmaps (instead of pixel-wise). It remains an open question whether such interpretations can be direct to the input space (as shown in the MNIST experiments). Besides, we consider the translation and rotation transformations in our model. In future work, we will explore the robust interpretations under more complex transformations such as scaling and distortion. Moreover, we clarify that SITE is not limited to geometric transformation (that we used in the computer vision domain), and will explore SITEin other domains in future work.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2125060481",
                        "name": "Yipei Wang"
                    },
                    {
                        "authorId": "2108158629",
                        "name": "Xiaoqian Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To improve the faithfulness of attentions, [33, 43] regularize the hidden representations on which the attention is computed over; [17] applies attention weights on losses of pre-defined individual rationale candidates\u2019 predictions."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9dc28baf794cbd1d5f24eb91b55f94871aa52d1d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-13880",
                    "ArXiv": "2110.13880",
                    "CorpusId": 239885910
                },
                "corpusId": 239885910,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9dc28baf794cbd1d5f24eb91b55f94871aa52d1d",
                "title": "Understanding Interlocking Dynamics of Cooperative Rationalization",
                "abstract": "Selective rationalization explains the prediction of complex neural networks by finding a small subset of the input that is sufficient to predict the neural model output. The selection mechanism is commonly integrated into the model itself by specifying a two-component cascaded system consisting of a rationale generator, which makes a binary selection of the input features (which is the rationale), and a predictor, which predicts the output based only on the selected features. The components are trained jointly to optimize prediction performance. In this paper, we reveal a major problem with such cooperative rationalization paradigm -- model interlocking. Interlocking arises when the predictor overfits to the features selected by the generator thus reinforcing the generator's selection even if the selected rationales are sub-optimal. The fundamental cause of the interlocking problem is that the rationalization objective to be minimized is concave with respect to the generator's selection policy. We propose a new rationalization framework, called A2R, which introduces a third component into the architecture, a predictor driven by soft attention as opposed to selection. The generator now realizes both soft and hard attention over the features and these are fed into the two different predictors. While the generator still seeks to support the original predictor performance, it also minimizes a gap between the two predictors. As we will show theoretically, since the attention-based predictor exhibits a better convexity property, A2R can overcome the concavity barrier. Our experiments on two synthetic benchmarks and two real datasets demonstrate that A2R can significantly alleviate the interlock problem and find explanations that better align with human judgments. We release our code at https://github.com/Gorov/Understanding_Interlocking.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115621395",
                        "name": "Mo Yu"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "3307026",
                        "name": "Shiyu Chang"
                    },
                    {
                        "authorId": "35132120",
                        "name": "T. Jaakkola"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1752e279c031fe8e0145211ffecbdc104cb8445e",
                "externalIds": {
                    "DBLP": "series/synthesis/2021Sogaard",
                    "MAG": "3200247363",
                    "DOI": "10.2200/s01118ed1v01y202107hlt051",
                    "CorpusId": 240551515
                },
                "corpusId": 240551515,
                "publicationVenue": {
                    "id": "400d73aa-0d51-4582-9144-2069958881cd",
                    "name": "Synthesis Lectures on Human Language Technologies",
                    "type": "journal",
                    "alternate_names": [
                        "Synth Lect Hum Lang Technol"
                    ],
                    "issn": "1947-4040",
                    "url": "https://www.morganclaypool.com/toc/hlt/1/1"
                },
                "url": "https://www.semanticscholar.org/paper/1752e279c031fe8e0145211ffecbdc104cb8445e",
                "title": "Explainable Natural Language Processing",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1700187",
                        "name": "Anders S\u00f8gaard"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Practitioners have shown that attention-based explanations are generally not faithful (Jain and Wallace, 2019; Serrano and Smith, 2019), but that they may\nbe plausible (Wiegreffe and Pinter, 2019; Mohankumar et al., 2020; Vashishth et al., 2019)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1c709eef701d933af1383c790c13209f06806b60",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-06387",
                    "ACL": "2021.emnlp-main.807",
                    "ArXiv": "2109.06387",
                    "DOI": "10.18653/v1/2021.emnlp-main.807",
                    "CorpusId": 237503612
                },
                "corpusId": 237503612,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/1c709eef701d933af1383c790c13209f06806b60",
                "title": "Rationales for Sequential Predictions",
                "abstract": "Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization: the best rationale is the smallest subset of input tokens that would predict the same output as the full sequence. Enumerating all subsets is intractable, so we propose an efficient greedy algorithm to approximate this objective. The algorithm, which is called greedy rationalization, applies to any model. For this approach to be effective, the model should form compatible conditional distributions when making predictions on incomplete subsets of the context. This condition can be enforced with a short fine-tuning step. We study greedy rationalization on language modeling and machine translation. Compared to existing baselines, greedy rationalization is best at optimizing the sequential objective and provides the most faithful rationales. On a new dataset of annotated sequential rationales, greedy rationales are most similar to human rationales.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "70025184",
                        "name": "Keyon Vafa"
                    },
                    {
                        "authorId": "2505751",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "1796335",
                        "name": "D. Blei"
                    },
                    {
                        "authorId": "2531268",
                        "name": "Alexander M. Rush"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c50c23cf9276156adece3d58c0c40cb8022e92d5",
                "externalIds": {
                    "MAG": "3196504426",
                    "DBLP": "journals/kbs/LopezMLH21",
                    "DOI": "10.1016/j.knosys.2021.107455",
                    "CorpusId": 239696543
                },
                "corpusId": 239696543,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c50c23cf9276156adece3d58c0c40cb8022e92d5",
                "title": "ADOPS: Aspect Discovery OPinion Summarisation Methodology based on deep learning and subgroup discovery for generating explainable opinion summaries",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Miguel L\u00f3pez"
                    },
                    {
                        "authorId": "1397974254",
                        "name": "Eugenio Mart\u00ednez-C\u00e1mara"
                    },
                    {
                        "authorId": "143624194",
                        "name": "M. V. Luz\u00f3n"
                    },
                    {
                        "authorId": "2056646744",
                        "name": "Francisco Herrera"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ness of explanations generated by non-transformer based models (Ross et al., 2017b; Liu and Avci, 2019; Moradi et al., 2021; Mohankumar et al., 2020; Tutek and Snajder, 2020).",
                "\u2026and Pinter, 2019; Pruthi et al., 2020; Ghorbani et al., 2019) while others used auxiliary objectives to improve the faithfulness of explanations generated by non-transformer based models (Ross et al., 2017b; Liu and Avci, 2019; Moradi et al., 2021; Mohankumar et al., 2020; Tutek and Snajder, 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "311e48e1c4a0dcd65a6699376ffc85a24a333a56",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-13759",
                    "ACL": "2021.emnlp-main.645",
                    "ArXiv": "2108.13759",
                    "DOI": "10.18653/v1/2021.emnlp-main.645",
                    "CorpusId": 237364609
                },
                "corpusId": 237364609,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/311e48e1c4a0dcd65a6699376ffc85a24a333a56",
                "title": "Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience",
                "abstract": "Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this paper, we hypothesize that salient information extracted a priori from the training data can complement the task-specific information learned by the model during fine-tuning on a downstream task. In this way, we aim to help BERT not to forget assigning importance to informative input tokens when making predictions by proposing SaLoss; an auxiliary loss function for guiding the multi-head attention mechanism during training to be close to salient information extracted a priori using TextRank. Experiments for explanation faithfulness across five datasets, show that models trained with SaLoss consistently provide more faithful explanations across four different feature attribution methods compared to vanilla BERT. Using the rationales extracted from vanilla BERT and SaLoss models to train inherently faithful classifiers, we further show that the latter result in higher predictive performance in downstream tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2125182391",
                        "name": "G. Chrysostomou"
                    },
                    {
                        "authorId": "3238627",
                        "name": "Nikolaos Aletras"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Yet to which extend those attention models help in interpreting how the model is operating has recently become a novel debate [3, 4]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "68f882d03b19186a70fb2485df721ec4d9583c8e",
                "externalIds": {
                    "ArXiv": "2106.02566",
                    "DBLP": "journals/pr/GomezLFM22",
                    "DOI": "10.1016/j.patcog.2022.108927",
                    "CorpusId": 246431270
                },
                "corpusId": 246431270,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/68f882d03b19186a70fb2485df721ec4d9583c8e",
                "title": "BR-NPA: A non-parametric high-resolution attention model to improve the interpretability of attention",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2070904522",
                        "name": "T. Gomez"
                    },
                    {
                        "authorId": "23993939",
                        "name": "Suiyi Ling"
                    },
                    {
                        "authorId": "2107033619",
                        "name": "Thomas Fr'eour"
                    },
                    {
                        "authorId": "1790706",
                        "name": "H. Mouch\u00e8re"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026Marshall, and Wallace 2016) and through different ways of human rationale integration, e.g., by learning a mapping between human rationales and machine attention (Bao et al. 2018) or ensuring the diversity among the hidden representations learned at different time steps (Mohankumar et al. 2020).",
                "Existing work (Bahdanau, Cho, and Bengio 2014; Mohankumar et al. 2020), however, takes human rationales as gold information that is entirely trustworthy, which is typically not the case in practice; indeed, studies from human computation have found the reliability of human-contributed rationales to\u2026",
                "Prior research (Zaidan, Eisner, and Piatko 2007; Zhang, Marshall, and Wallace 2016) has shown that human rationales represent valuable input for improving model performance and for identifying explainable input features in model prediction (Bahdanau, Cho, and Bengio 2014; Mohankumar et al. 2020).",
                "In addition, we compare against rational-aware models: 1) LSTM-ortho and LSTM-diversity, both proposed in (Mohankumar et al. 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "0e6338c992b6b72da05cb783f4d422ebf0462451",
                "externalIds": {
                    "DBLP": "conf/aaai/ArousDYBCC21",
                    "DOI": "10.1609/aaai.v35i7.16734",
                    "CorpusId": 231395350
                },
                "corpusId": 231395350,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0e6338c992b6b72da05cb783f4d422ebf0462451",
                "title": "MARTA: Leveraging Human Rationales for Explainable Text Classification",
                "abstract": "Explainability is a key requirement for text classification in many application domains ranging from sentiment analysis to medical diagnosis or legal reviews. Existing methods often rely on \"attention\" mechanisms for explaining classification results by estimating the relative importance of input units. However, recent studies have shown that such mechanisms tend to mis-identify irrelevant input units in their explanation. In this work, we propose a hybrid human-AI approach that incorporates human rationales into attention-based text classification models to improve the explainability of classification results. Specifically, we ask workers to provide rationales for their annotation by selecting relevant pieces of text. We introduce MARTA, a Bayesian framework that jointly learns an attention-based model and the reliability of workers while injecting human rationales into model training. We derive a principled optimization algorithm based on variational inference with efficient updating rules for learning MARTA parameters. Extensive validation on real-world datasets shows that our framework significantly improves the state of the art both in terms of classification explainability and accuracy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "115651840",
                        "name": "Ines Arous"
                    },
                    {
                        "authorId": "1680665",
                        "name": "L. Dolamic"
                    },
                    {
                        "authorId": "2118579490",
                        "name": "Jie Yang"
                    },
                    {
                        "authorId": "28972263",
                        "name": "Akansha Bhardwaj"
                    },
                    {
                        "authorId": "2004477",
                        "name": "Giuseppe Cuccu"
                    },
                    {
                        "authorId": "1393644275",
                        "name": "P. Cudr\u00e9-Mauroux"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite that, the validity of this analysis method is a subject undergoing intense discussion and study in NLP (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Moradi et al., 2019; Mohankumar et al., 2020; Tutek and Snajder, 2020, i.a.)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3fa01ebe92d8bab53b2756beeecfe6faa9e573bb",
                "externalIds": {
                    "ACL": "2021.findings-acl.361",
                    "DBLP": "journals/corr/abs-2105-08855",
                    "ArXiv": "2105.08855",
                    "DOI": "10.18653/v1/2021.findings-acl.361",
                    "CorpusId": 234777911
                },
                "corpusId": 234777911,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3fa01ebe92d8bab53b2756beeecfe6faa9e573bb",
                "title": "Effective Attention Sheds Light On Interpretability",
                "abstract": "An attention matrix of a transformer self-attention sublayer can provably be decomposed into two components and only one of them (effective attention) contributes to the model output. This leads us to ask whether visualizing effective attention gives different conclusions than interpretation of standard attention. Using a subset of the GLUE tasks and BERT, we carry out an analysis to compare the two attention matrices, and show that their interpretations differ. Effective attention is less associated with the features related to the language modeling pretraining such as the separator token, and it has more potential to illustrate linguistic features captured by the model for solving the end-task. Given the found differences, we recommend using effective attention for studying a transformer's behavior since it is more pertinent to the model output by design.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2087314342",
                        "name": "Kaiser Sun"
                    },
                    {
                        "authorId": "3451494",
                        "name": "Ana Marasovi\u0107"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2c68e38d9ff1c61672d3c85e4eb72ff9542d0b89",
                "externalIds": {
                    "ArXiv": "2105.03287",
                    "DBLP": "journals/corr/abs-2105-03287",
                    "CorpusId": 234096057
                },
                "corpusId": 234096057,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c68e38d9ff1c61672d3c85e4eb72ff9542d0b89",
                "title": "Order in the Court: Explainable AI Methods Prone to Disagreement",
                "abstract": "By computing the rank correlation between attention weights and feature-additive explanation methods, previous analyses either invalidate or support the role of attention-based explanations as a faithful and plausible measure of salience. To investigate whether this approach is appropriate, we compare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and attention-based explanations, applied to two neural architectures trained on single- and pair-sequence language tasks. In most cases, we find that none of our chosen methods agree. Based on our empirical observations and theoretical objections, we conclude that rank correlation does not measure the quality of feature-additive methods. Practitioners should instead use the numerous and rigorous diagnostic methods proposed by the community.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113260008",
                        "name": "Michael Neely"
                    },
                    {
                        "authorId": "89933535",
                        "name": "Stefan F. Schouten"
                    },
                    {
                        "authorId": "1452678770",
                        "name": "Maurits J. R. Bleeker"
                    },
                    {
                        "authorId": "38702106",
                        "name": "Ana Lucic"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Focusing also on recurrent-encoders, Mohankumar et al. (2020) introduce a modification to recurrent encoders to reduce repetitive information across different words in the input to improve faithfulness of explanations."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "32a6daa76efd00f657e65842771971decf104efc",
                "externalIds": {
                    "ACL": "2021.acl-long.40",
                    "DBLP": "journals/corr/abs-2105-02657",
                    "ArXiv": "2105.02657",
                    "DOI": "10.18653/v1/2021.acl-long.40",
                    "CorpusId": 233864728
                },
                "corpusId": 233864728,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/32a6daa76efd00f657e65842771971decf104efc",
                "title": "Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification",
                "abstract": "Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks. In this paper, we seek to improve the faithfulness of attention-based explanations for text classification. We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attention-based explanations across two attention mechanisms, five encoders and five text classification datasets without sacrificing predictive performance. Finally, we demonstrate that TaSc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51015453",
                        "name": "G. Chrysostomou"
                    },
                    {
                        "authorId": "3238627",
                        "name": "Nikolaos Aletras"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "28713169c3b993de291f546a63ef71b7310e33f5",
                "externalIds": {
                    "DBLP": "journals/apin/KitadaI23",
                    "ArXiv": "2104.08763",
                    "DOI": "10.1007/s10489-022-04301-w",
                    "CorpusId": 233296351
                },
                "corpusId": 233296351,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/28713169c3b993de291f546a63ef71b7310e33f5",
                "title": "Making attention mechanisms more robust and interpretable with virtual adversarial training",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51267434",
                        "name": "Shunsuke Kitada"
                    },
                    {
                        "authorId": "2801969",
                        "name": "H. Iyatomi"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2014), attention-based (Mohankumar et al., 2020; Tutek and \u0160najder, 2020; Ghaeini et al., 2018; Lee et al., 2017), and occlusion-based (DeYoung et al.",
                "Such extractive explanations typically use either gradient-based (Sundararajan et al., 2017; Li et al., 2015; Denil et al., 2014), attention-based (Mohankumar et al., 2020; Tutek and \u0160najder, 2020; Ghaeini et al., 2018; Lee et al., 2017), and occlusion-based (DeYoung et al., 2019; Poerner et al.,\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a9ec8eae66d6e7a8517eef0a4060ba9d45706f41",
                "externalIds": {
                    "ArXiv": "2104.08793",
                    "DBLP": "journals/corr/abs-2104-08793",
                    "CorpusId": 233296329
                },
                "corpusId": 233296329,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a9ec8eae66d6e7a8517eef0a4060ba9d45706f41",
                "title": "SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning",
                "abstract": "Augmenting pre-trained language models with knowledge graphs (KGs) has achieved success on various commonsense reasoning tasks. However, for a given task instance, the KG, or certain parts of the KG, may not be useful. Although KG-augmented models often use attention to focus on specific KG components, the KG is still always used, and the attention mechanism is never explicitly taught which KG components should be used. Meanwhile, saliency methods can measure how much a KG feature (e.g., graph, node, path) influences the model to make the correct prediction, thus explaining which KG features are useful. This paper explores how saliency explanations can be used to improve KG-augmented models' performance. First, we propose to create coarse (Is the KG useful?) and fine (Which nodes/paths in the KG are useful?) saliency explanations. Second, to motivate saliency-based supervision, we analyze oracle KG-augmented models which directly use saliency explanations as extra inputs for guiding their attention. Third, we propose SalKG, a framework for KG-augmented models to learn from coarse and/or fine saliency explanations. Given saliency explanations created from a task's training set, SalKG jointly trains the model to predict the explanations, then solve the task by attending to KG features highlighted by the predicted explanations. On three commonsense QA benchmarks (CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can yield considerable performance gains -- up to 2.76% absolute improvement on CSQA.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2114015857",
                        "name": "Aaron Chan"
                    },
                    {
                        "authorId": "3313909",
                        "name": "Soumya Sanyal"
                    },
                    {
                        "authorId": "2052143728",
                        "name": "Bo Long"
                    },
                    {
                        "authorId": "2110519123",
                        "name": "Jiashu Xu"
                    },
                    {
                        "authorId": "2067241099",
                        "name": "Tanishq Gupta"
                    },
                    {
                        "authorId": "145201124",
                        "name": "Xiang Ren"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For courtesy reasons, we anonymize the papers surveyed, except Paper 3 (Mohankumar et al., 2020) which was the only paper that did not exhibit the Great Misalignment Problem."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2157ae8828bf8c60e954931e68e1a2f97f9d30cc",
                "externalIds": {
                    "ArXiv": "2104.05361",
                    "DBLP": "journals/corr/abs-2104-05361",
                    "ACL": "2021.humeval-1.8",
                    "CorpusId": 233210508
                },
                "corpusId": 233210508,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2157ae8828bf8c60e954931e68e1a2f97f9d30cc",
                "title": "The Great Misalignment Problem in Human Evaluation of NLP Methods",
                "abstract": "We outline the Great Misalignment Problem in natural language processing research, this means simply that the problem definition is not in line with the method proposed and the human evaluation is not in line with the definition nor the method. We study this misalignment problem by surveying 10 randomly sampled papers published in ACL 2020 that report results with human evaluation. Our results show that only one paper was fully in line in terms of problem definition, method and evaluation. Only two papers presented a human evaluation that was in line with what was modeled in the method. These results highlight that the Great Misalignment Problem is a major one and it affects the validity and reproducibility of results obtained by a human evaluation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150213555",
                        "name": "Mika H\u00e4m\u00e4l\u00e4inen"
                    },
                    {
                        "authorId": "150169636",
                        "name": "Khalid Alnajjar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A recent work on interpretability of attention distributions shows that the attention distributions could be utilized in specifically accustomed DL models to provide faithful and plausible explanations for models\u2019 predictions [19].",
                "If higher attention weights imply a more significant impact on the model\u2019s predictions, then we consider attention distributions as faithful explanations, while we can also consider them as plausible explanations if they provide human-understandable justification for the model\u2019s predictions [19]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f78def65f0ca8d2e51bf85dfa76aa860474315db",
                "externalIds": {
                    "PubMedCentral": "7912396",
                    "DBLP": "journals/entropy/BaricFHL21",
                    "DOI": "10.3390/e23020143",
                    "CorpusId": 231769875,
                    "PubMed": "33503822"
                },
                "corpusId": 231769875,
                "publicationVenue": {
                    "id": "8270cfe1-3713-4325-a7bd-c6a87eed889e",
                    "name": "Entropy",
                    "type": "journal",
                    "issn": "1099-4300",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606",
                    "alternate_urls": [
                        "http://www.mdpi.com/journal/entropy/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155606",
                        "https://www.mdpi.com/journal/entropy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f78def65f0ca8d2e51bf85dfa76aa860474315db",
                "title": "Benchmarking Attention-Based Interpretability of Deep Learning in Multivariate Time Series Predictions",
                "abstract": "The adaptation of deep learning models within safety-critical systems cannot rely only on good prediction performance but needs to provide interpretable and robust explanations for their decisions. When modeling complex sequences, attention mechanisms are regarded as the established approach to support deep neural networks with intrinsic interpretability. This paper focuses on the emerging trend of specifically designing diagnostic datasets for understanding the inner workings of attention mechanism based deep learning models for multivariate forecasting tasks. We design a novel benchmark of synthetically designed datasets with the transparent underlying generating process of multiple time series interactions with increasing complexity. The benchmark enables empirical evaluation of the performance of attention based deep neural networks in three different aspects: (i) prediction performance score, (ii) interpretability correctness, (iii) sensitivity analysis. Our analysis shows that although most models have satisfying and stable prediction performance results, they often fail to give correct interpretability. The only model with both a satisfying performance score and correct interpretability is IMV-LSTM, capturing both autocorrelations and crosscorrelations between multiple time series. Interestingly, while evaluating IMV-LSTM on simulated data from statistical and mechanistic models, the correctness of interpretability increases with more complex datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "118005087",
                        "name": "Domjan Baric"
                    },
                    {
                        "authorId": "2047677650",
                        "name": "P. Fumi\u0107"
                    },
                    {
                        "authorId": "3353260",
                        "name": "D. Horvatic"
                    },
                    {
                        "authorId": "31213251",
                        "name": "T. Lipi\u0107"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a9514bcb64bf3f3ed776e6e282a1f500c312aace",
                "externalIds": {
                    "DBLP": "journals/tai/KumarSGKK21",
                    "ArXiv": "2011.01472",
                    "MAG": "3096394582",
                    "DOI": "10.1109/tai.2021.3111138",
                    "CorpusId": 226237434
                },
                "corpusId": 226237434,
                "publicationVenue": {
                    "id": "3c27e831-750f-45bc-9914-2148a5259eba",
                    "name": "IEEE Transactions on Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Artif Intell"
                    ],
                    "issn": "2691-4581",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688"
                },
                "url": "https://www.semanticscholar.org/paper/a9514bcb64bf3f3ed776e6e282a1f500c312aace",
                "title": "MACE: Model Agnostic Concept Extractor for Explaining Image Classification Networks",
                "abstract": "Deep convolutional networks have been quite successful at various image classification tasks. The current methods to explain the predictions of a pretrained model rely on gradient information, often resulting in saliency maps that focus on the foreground object as a whole. However, humans typically reason by dissecting an image and pointing out the presence of smaller concepts. The final output is often an aggregation of the presence or absence of these smaller concepts. In this work, we propose MACE: a model agnostic concept extractor, which can explain the working of a convolutional network through smaller concepts. The MACE framework dissects the feature maps generated by a convolution network for an image to extract concept-based prototypical explanations. Furthermore, it estimates the relevance of the extracted concepts to the pretrained model\u2019s predictions, a critical aspect for explaining the individual class predictions, missing in existing approaches. We validate our framework using VGG16 and ResNet50 CNN architectures and datasets like Animals With Attributes 2 (AWA2) and Places365. Our experiments demonstrate that the concepts extracted by the MACE framework increase the human interpretability of the explanations and are faithful to the underlying pretrained black-box model.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2118069675",
                        "name": "Ashish Kumar"
                    },
                    {
                        "authorId": "2066173731",
                        "name": "Karan Sehgal"
                    },
                    {
                        "authorId": "2054104476",
                        "name": "Prerna Garg"
                    },
                    {
                        "authorId": "88991725",
                        "name": "V. Kamakshi"
                    },
                    {
                        "authorId": "2503137",
                        "name": "N. C. Krishnan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For better alignment, (Tutek and \u0160najder, 2020) utilizes masked language model (MLM) loss and (Mohankumar et al., 2020) invents orthogonal LSTM representations.",
                "For better alignment, (Tutek and S\u030cnajder, 2020) utilizes masked language model (MLM) loss and (Mohankumar et al., 2020) invents orthogonal LSTM representations."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "43420e2df152a58db57de8368c7a7ac2911019be",
                "externalIds": {
                    "ACL": "2020.emnlp-main.543",
                    "MAG": "3104564752",
                    "DBLP": "conf/emnlp/ChoiPYH20",
                    "DOI": "10.18653/v1/2020.emnlp-main.543",
                    "CorpusId": 226262267
                },
                "corpusId": 226262267,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/43420e2df152a58db57de8368c7a7ac2911019be",
                "title": "Less Is More: Attention Supervision with Counterfactuals for Text Classification",
                "abstract": "We aim to leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision. Specifically, for this goal, we explore the advantage of counterfactual reasoning, over associative reasoning typically used in attention supervision. Our empirical results show that this machine-augmented human attention supervision is more effective than existing methods requiring a higher annotation cost, in text classification tasks, including sentiment analysis and news categorization.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "5841595",
                        "name": "Seungtaek Choi"
                    },
                    {
                        "authorId": "74371835",
                        "name": "Haeju Park"
                    },
                    {
                        "authorId": "1898428",
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "authorId": "1716415",
                        "name": "Seung-won Hwang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The above works have inspired some to find ways to make attention more faithful and/or plausible, by changing the nature of the hidden representations attention is computed over using special training objectives (e.g., Mohankumar et al., 2020; Tutek and Snajder, 2020).",
                "Mohankumar et al. (2020) observe high similarity between the hidden representations of LSTM states and propose a diversity-driven training objective that makes the hidden representations more diverse across time steps."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "508884a136a461869be128027950d2aa1778518c",
                "externalIds": {
                    "ArXiv": "2010.05607",
                    "DBLP": "journals/corr/abs-2010-05607",
                    "MAG": "3092292656",
                    "ACL": "2020.blackboxnlp-1.14",
                    "DOI": "10.18653/V1/2020.BLACKBOXNLP-1.14",
                    "CorpusId": 222291117
                },
                "corpusId": 222291117,
                "publicationVenue": {
                    "id": "738626d7-5b8c-497d-9fd6-64bdb6dbf440",
                    "name": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
                    "type": "conference",
                    "alternate_names": [
                        "BlackboxNLP",
                        "Blackboxnlp Workshop Anal Interpr\u00e8t Neural Netw NLP"
                    ],
                    "url": "https://aclanthology.org/venues/blackboxnlp/"
                },
                "url": "https://www.semanticscholar.org/paper/508884a136a461869be128027950d2aa1778518c",
                "title": "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?",
                "abstract": "There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1994065972",
                        "name": "Jasmijn Bastings"
                    },
                    {
                        "authorId": "3017324",
                        "name": "Katja Filippova"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, however, it has been pointed out that rank correlations often misrepresent the relationship between the two due to the noise in the order of the low rankings [46]; we concurred with this, so we used Pearson\u2019s correlations."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1b3703be95e26c2c0603bcc4d14b92968ec93c8a",
                "externalIds": {
                    "MAG": "3088181395",
                    "DBLP": "journals/corr/abs-2009-12064",
                    "ArXiv": "2009.12064",
                    "DOI": "10.1109/ACCESS.2021.3093456",
                    "CorpusId": 221949068
                },
                "corpusId": 221949068,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1b3703be95e26c2c0603bcc4d14b92968ec93c8a",
                "title": "Attention Meets Perturbations: Robust and Interpretable Attention With Adversarial Training",
                "abstract": "Although attention mechanisms have been applied to a variety of deep learning models and have been shown to improve the prediction performance, it has been reported to be vulnerable to perturbations to the mechanism. To overcome the vulnerability to perturbations in the mechanism, we are inspired by adversarial training (AT), which is a powerful regularization technique for enhancing the robustness of the models. In this paper, we propose a general training technique for natural language processing tasks, including AT for attention (Attention AT) and more interpretable AT for attention (Attention iAT). The proposed techniques improved the prediction performance and the model interpretability by exploiting the mechanisms with AT. In particular, Attention iAT boosts those advantages by introducing adversarial perturbation, which enhances the difference in the attention of the sentences. Evaluation experiments with ten open datasets revealed that AT for attention mechanisms, especially Attention iAT, demonstrated (1) the best performance in nine out of ten tasks and (2) more interpretable attention (i.e., the resulting attention correlated more strongly with gradient-based word importance) for all tasks. Additionally, the proposed techniques are (3) much less dependent on perturbation size in AT.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51267434",
                        "name": "Shunsuke Kitada"
                    },
                    {
                        "authorId": "2801969",
                        "name": "H. Iyatomi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "QED exists in between relatively unstructured explanation forms on the one hand, such as attention distributions (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019; Mohankumar et al., 2020) or sequential outputs (Camburu et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ebd8b4ebad8c3ea6362e9a1389c09f04f6f18e84",
                "externalIds": {
                    "ArXiv": "2009.06354",
                    "DBLP": "journals/corr/abs-2009-06354",
                    "MAG": "3085552234",
                    "DOI": "10.1162/tacl_a_00398",
                    "CorpusId": 221655495
                },
                "corpusId": 221655495,
                "publicationVenue": {
                    "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
                    "name": "Transactions of the Association for Computational Linguistics",
                    "type": "journal",
                    "alternate_names": [
                        "Trans Assoc Comput Linguistics",
                        "TACL"
                    ],
                    "issn": "2307-387X",
                    "url": "https://www.mitpressjournals.org/loi/tacl",
                    "alternate_urls": [
                        "http://www.transacl.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ebd8b4ebad8c3ea6362e9a1389c09f04f6f18e84",
                "title": "QED: A Framework and Dataset for Explanations in Question Answering",
                "abstract": "A question answering system that in addition to providing an answer provides an explanation of the reasoning that leads to that answer has potential advantages in terms of debuggability, extensibility, and trust. To this end, we propose QED, a linguistically informed, extensible framework for explanations in question answering. A QED explanation specifies the relationship between a question and answer according to formal semantic notions such as referential equality, sentencehood, and entailment. We describe and publicly release an expert-annotated dataset of QED explanations built upon a subset of the Google Natural Questions dataset, and report baseline models on two tasks\u2014post- hoc explanation generation given an answer, and joint question answering and explanation generation. In the joint setting, a promising result suggests that training on a relatively small amount of QED data can improve question answering. In addition to describing the formal, language-theoretic motivations for the QED approach, we describe a large user study showing that the presence of QED explanations significantly improves the ability of untrained raters to spot errors made by a strong neural QA baseline.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48024953",
                        "name": "Matthew Lamm"
                    },
                    {
                        "authorId": "52578817",
                        "name": "Jennimaria Palomaki"
                    },
                    {
                        "authorId": "114577307",
                        "name": "Chris Alberti"
                    },
                    {
                        "authorId": "3365603",
                        "name": "D. Andor"
                    },
                    {
                        "authorId": "2890423",
                        "name": "Eunsol Choi"
                    },
                    {
                        "authorId": "7353832",
                        "name": "Livio Baldini Soares"
                    },
                    {
                        "authorId": "123052390",
                        "name": "Michael Collins"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In some views, attention in deep models can also be regarded as local interpretations [60], [61], [62]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4187c6acfdf5161bfdbc209a512582de8e24f256",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2008-09775",
                    "MAG": "3080409248",
                    "ArXiv": "2008.09775",
                    "CorpusId": 221266499
                },
                "corpusId": 221266499,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4187c6acfdf5161bfdbc209a512582de8e24f256",
                "title": "DNN2LR: Interpretation-inspired Feature Crossing for Real-world Tabular Data",
                "abstract": "For sake of reliability, it is necessary for models in real-world applications to be both powerful and globally interpretable. Simple linear classifiers, e.g., Logistic Regression (LR), are globally interpretable, but not powerful enough to model complex nonlinear interactions among features in tabular data. Meanwhile, Deep Neural Networks (DNNs) have shown great effectiveness for modeling tabular data, but is not globally interpretable. Accordingly, it will be promising if we can propose a feature crossing method to find feature interactions in DNN, and use them as cross features in LR. The local piece-wise interpretations in DNN of a specific feature are usually inconsistent in different samples, which is caused by feature interactions in the hidden layers. Inspired by this, we give definition of the interpretation inconsistency in DNN, and accordingly propose a novel feature crossing method called DNN2LR. Extensive experiments have been conducted on five public datasets and two real-world datasets. The final model, a LR model empowered with cross features, generated by DNN2LR can outperform the complex DNN model, as well as several state-of-the-art feature crossing methods. The experimental results strongly verify the effectiveness and efficiency of DNN2LR, especially on real-world datasets with large numbers of feature fields.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2118393734",
                        "name": "Zhaocheng Liu"
                    },
                    {
                        "authorId": "47362268",
                        "name": "Qiang Liu"
                    },
                    {
                        "authorId": "2144613377",
                        "name": "Hao Zhang"
                    },
                    {
                        "authorId": "2144863166",
                        "name": "Yuntian Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a8427ce5aee6d62800c725588e89940ed4910e0d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1904-02874",
                    "MAG": "2941531368",
                    "ArXiv": "1904.02874",
                    "DOI": "10.1145/3465055",
                    "CorpusId": 102350916
                },
                "corpusId": 102350916,
                "publicationVenue": {
                    "id": "0d993d4a-09ba-4df8-90a4-7dfe25f0cb9e",
                    "name": "ACM Transactions on Intelligent Systems and Technology",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Intell Syst Technol"
                    ],
                    "issn": "2157-6904",
                    "url": "http://portal.acm.org/tist",
                    "alternate_urls": [
                        "https://tist.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a8427ce5aee6d62800c725588e89940ed4910e0d",
                "title": "An Attentive Survey of Attention Models",
                "abstract": "Attention Model has now become an important concept in neural networks that has been researched within diverse application domains. This survey provides a structured and comprehensive overview of the developments in modeling attention. In particular, we propose a taxonomy that groups existing techniques into coherent categories. We review salient neural architectures in which attention has been incorporated and discuss applications in which modeling attention has shown a significant impact. We also describe how attention has been used to improve the interpretability of neural networks. Finally, we discuss some future research directions in attention. We hope this survey will provide a succinct introduction to attention models and guide practitioners while developing approaches for their applications.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "144959773",
                        "name": "S. Chaudhari"
                    },
                    {
                        "authorId": "2767134",
                        "name": "Gungor Polatkan"
                    },
                    {
                        "authorId": "2051517",
                        "name": "R. Ramanath"
                    },
                    {
                        "authorId": "1750278",
                        "name": "Varun Mithal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[19] suggested an additional constraint in the learning objective to force this representation to be sparse.",
                "The sparsity constraint can be expressed in many different ways, which have different but marginal effects on convergence speed, or on the resulting explanation [19,13].",
                "Faithfulness, a widely discussed problem [19,3], focuses on whether the weight associated with a token reflects its influence on the prediction.",
                "Plausibility refers to the extent to which the attention map can resemble human reasoning [19,31]."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "12c9d497a9538fcda9202937a4c3427d2a938b9e",
                "externalIds": {
                    "DBLP": "conf/nldb/NguyenMGS23",
                    "DOI": "10.1007/978-3-031-35320-8_20",
                    "CorpusId": 259195507
                },
                "corpusId": 259195507,
                "publicationVenue": {
                    "id": "7c0b75bf-65e3-4094-9d72-e8f59ebb154d",
                    "name": "International Conference on Applications of Natural Language to Data Bases",
                    "type": "conference",
                    "alternate_names": [
                        "Appl Nat Lang Data Base",
                        "Int Conf Appl Nat Lang Data Base",
                        "Applications of Natural Language to Data Bases",
                        "NLDB"
                    ],
                    "url": "http://www.nldb.org/"
                },
                "url": "https://www.semanticscholar.org/paper/12c9d497a9538fcda9202937a4c3427d2a938b9e",
                "title": "Regularization, Semi-supervision, and Supervision for a Plausible Attention-Based Explanation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112294501",
                        "name": "Duc Hau Nguyen"
                    },
                    {
                        "authorId": "2008743506",
                        "name": "Cyrielle Mallart"
                    },
                    {
                        "authorId": "1708671",
                        "name": "G. Gravier"
                    },
                    {
                        "authorId": "1792119",
                        "name": "P. S\u00e9billot"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", \u201cplausibility\u201d) on proxy tasks without real human participating [5, 50, 86]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3b13a46a7877a5c9785b816516e7b661483a4791",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-09770",
                    "DOI": "10.48550/arXiv.2305.09770",
                    "CorpusId": 258741213
                },
                "corpusId": 258741213,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3b13a46a7877a5c9785b816516e7b661483a4791",
                "title": "ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing",
                "abstract": "While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145028030",
                        "name": "Hua Shen"
                    },
                    {
                        "authorId": "7901504",
                        "name": "Chieh-Yang Huang"
                    },
                    {
                        "authorId": "35232494",
                        "name": "Tongshuang Sherry Wu"
                    },
                    {
                        "authorId": "144188081",
                        "name": "Ting-Hao 'Kenneth' Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", \u201cplausibility\u201d) on proxy tasks without real human participations [5, 51, 84]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6388ff0125413960aac9b1a225bd9d3e4a31b429",
                "externalIds": {
                    "CorpusId": 259212757
                },
                "corpusId": 259212757,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6388ff0125413960aac9b1a225bd9d3e4a31b429",
                "title": "ConvXAI : Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing",
                "abstract": "that are",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145028030",
                        "name": "Hua Shen"
                    },
                    {
                        "authorId": "7901504",
                        "name": "Chieh-Yang Huang"
                    },
                    {
                        "authorId": "35232494",
                        "name": "Tongshuang Sherry Wu"
                    },
                    {
                        "authorId": "144188081",
                        "name": "Ting-Hao 'Kenneth' Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "be455a7d80109b9e1d63598ec81b969218542cd8",
                "externalIds": {
                    "ACL": "2023.inlg-main.25",
                    "CorpusId": 263609407
                },
                "corpusId": 263609407,
                "publicationVenue": {
                    "id": "8648a277-d0ec-4691-9eed-399b31ff9860",
                    "name": "International Conference on Natural Language Generation",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Nat Lang Gener",
                        "INLG"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1613"
                },
                "url": "https://www.semanticscholar.org/paper/be455a7d80109b9e1d63598ec81b969218542cd8",
                "title": "On Text Style Transfer via Style-Aware Masked Language Models",
                "abstract": "Text Style Transfer (TST) is performable through approaches such as latent space disentanglement, cycle-consistency losses, prototype editing etc. The prototype editing approach, which is known to be quite successful in TST, involves two key phases a) Masking of source style-associated tokens and b) Reconstruction of this source-style masked sentence conditioned with the target style. We follow a similar transduction method, in which we transpose the more difficult direct source to target TST task to a simpler Style-Masked Language Model (SMLM) Task, wherein, similar to BERT (CITATION), the goal of our model is now to reconstruct the source sentence from its style-masked version. We arrive at the SMLM mechanism naturally by formulating prototype editing/ transduction methods in a probabilistic framework, where TST resolves into estimating a hypothetical parallel dataset from a partially observed parallel dataset, wherein each domain is assumed to have a common latent style-masked prior. To generate this style-masked prior, we use \u201cExplainable Attention\u201d as our choice of attribution for a more precise style-masking step and also introduce a cost-effective and accurate \u201cAttribution-Surplus\u201d method of determining the position of masks from any arbitrary attribution model in O(1) time. We empirically show that this non-generational approach well suites the \u201ccontent preserving\u201d criteria for a task like TST, even for a complex style like Discourse Manipulation. Our model, the Style MLM, outperforms strong TST baselines and is on par with state-of-the-art TST models, which use complex architectures and orders of more parameters.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1516227958",
                        "name": "Sharan Narasimhan"
                    },
                    {
                        "authorId": "2253598604",
                        "name": "Pooja H"
                    },
                    {
                        "authorId": "29832722",
                        "name": "Suvodip Dey"
                    },
                    {
                        "authorId": "2481485",
                        "name": "M. Desarkar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(2019) Natural Language Inference LSTM-CRF Section 4 Mohankumar et al. (2020) Sentiment Analysis, Text Classification, Natural Language Inference, Paraphrase Detection and Question Answering LSTM Sections 4, 8 and 9.",
                "\u2026581\nsolutions to three baseline explanations methods.582 Their results show that their solutions are an im-583 provement over the baselines.584\nMohankumar et al. (2020) propose the introduc-585 tion of more diversity in the hidden states learned586 by LSTMs, allowing to observe elements\u2026",
                "266\nMohankumar et al. (2020) investigate attention 267 on top of LSTMs (attention-LSTMs)."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e969778bced13a339f3d0465cea4e10c489ee1cc",
                "externalIds": {
                    "DBLP": "conf/acl/BibalCAWWFW22",
                    "ACL": "2022.acl-long.269",
                    "DOI": "10.18653/v1/2022.acl-long.269",
                    "CorpusId": 248779953
                },
                "corpusId": 248779953,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/e969778bced13a339f3d0465cea4e10c489ee1cc",
                "title": "Is Attention Explanation? An Introduction to the Debate",
                "abstract": "The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of attention in neural networks. Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible. In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas. This holistic vision can be of great interest for future works in all the communities concerned by this debate. We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8533198",
                        "name": "Adrien Bibal"
                    },
                    {
                        "authorId": "2065830278",
                        "name": "R\u00e9mi Cardon"
                    },
                    {
                        "authorId": "1971580",
                        "name": "David Alfter"
                    },
                    {
                        "authorId": "144566112",
                        "name": "Rodrigo Wilkens"
                    },
                    {
                        "authorId": "2130542021",
                        "name": "Xiaoou Wang"
                    },
                    {
                        "authorId": "2320605",
                        "name": "Thomas Fran\u00e7ois"
                    },
                    {
                        "authorId": "2389742",
                        "name": "Patrick Watrin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar concepts have been investigated for neural network models [1] and various methods of human reason integration, such as learning a mapping between human rationales and machine attention [31] or assuring variety among hidden representations learned at different time steps [32]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "06137f6020ec2625cef177fca04f0fb2a5483f60",
                "externalIds": {
                    "DBLP": "journals/algorithms/YadavN22",
                    "DOI": "10.3390/a15050143",
                    "CorpusId": 248447954
                },
                "corpusId": 248447954,
                "publicationVenue": {
                    "id": "e95c8d18-09be-464f-a3cf-5b2637f0eff6",
                    "name": "Algorithms",
                    "type": "journal",
                    "issn": "1999-4893",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-150910",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-150910",
                        "http://www.mdpi.com/journal/algorithms",
                        "http://www.mdpi.com/journal/algorithms/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/06137f6020ec2625cef177fca04f0fb2a5483f60",
                "title": "Enhancing Attention's Explanation Using Interpretable Tsetlin Machine",
                "abstract": ": Explainability is one of the key factors in Natural Language Processing (NLP) specially for legal documents, medical diagnosis, and clinical text. Attention mechanism has been a popular choice for such explainability recently by estimating the relative importance of input units. Recent research has revealed, however, that such processes tend to misidentify irrelevant input units when explaining them. This is due to the fact that language representation layers are initialized by pretrained word embedding that is not context-dependent. Such a lack of context-dependent knowledge in the initial layer makes it dif\ufb01cult for the model to concentrate on the important aspects of input. Usually, this does not impact the performance of the model, but the explainability differs from human understanding. Hence, in this paper, we propose an ensemble method to use logic-based information from the Tsetlin Machine to embed it into the initial representation layer in the neural network to enhance the model in terms of explainability. We obtain the global clause score for each word in the vocabulary and feed it into the neural network layer as context-dependent information. Our experiments show that the ensemble method enhances the explainability of the attention layer without sacri\ufb01cing any performance of the model and even outperforming in some datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144570483",
                        "name": "Rohan Kumar Yadav"
                    },
                    {
                        "authorId": "2082096580",
                        "name": "D. C. Nicolae"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026equipped with attention mechanism (Bahdanau et al., 2014) for NLP tasks, researchers often regard the weights assigned by the attention layer to different parts of input text as indicators of their importance to the model prediction (Mohankumar et al., 2020; Yang et al., 2016; Wang et al., 2016)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "511b6de584d74c2e4a6d2b9583c476d70eb9f6dc",
                "externalIds": {
                    "DBLP": "conf/naacl/ZengLGC22",
                    "ACL": "2022.naacl-main.14",
                    "DOI": "10.18653/v1/2022.naacl-main.14",
                    "CorpusId": 250390700
                },
                "corpusId": 250390700,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/511b6de584d74c2e4a6d2b9583c476d70eb9f6dc",
                "title": "Do Deep Neural Nets Display Human-like Attention in Short Answer Scoring?",
                "abstract": "Deep Learning (DL) techniques have been increasingly adopted for Automatic Text Scoring in education. However, these techniques often suffer from their inabilities to explain and justify how a prediction is made, which, unavoidably, decreases their trustworthiness and hinders educators from embracing them in practice. This study aimed to investigate whether (and to what extent) DL-based graders align with human graders regarding the important words they identify when marking short answer questions. To this end, we first conducted a user study to ask human graders to manually annotate important words in assessing answer quality and then measured the overlap between these human-annotated words and those identified by DL-based graders (i.e., those receiving large attention weights). Furthermore, we ran a randomized controlled experiment to explore the impact of highlighting important words detected by DL-based graders on human grading. The results showed that: (i) DL-based graders, to a certain degree, displayed alignment with human graders no matter whether DL-based graders and human graders agreed on the quality of an answer; and (ii) it is possible to facilitate human grading by highlighting those DL-detected important words, though further investigations are necessary to understand how human graders exploit such highlighted words.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8647899",
                        "name": "Zijie Zeng"
                    },
                    {
                        "authorId": "2108482523",
                        "name": "Xinyu Li"
                    },
                    {
                        "authorId": "65953975",
                        "name": "D. Ga\u0161evi\u0107"
                    },
                    {
                        "authorId": "1566337182",
                        "name": "Guanliang Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The conicity metric [46], C, is nothing but the mean value of ATM \u2200vi \u2208 V",
                "Attention distributions have been found to offer plausible explanations of predictions of deep learning models [46]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b71feb08c9fc4c156c920f53d44d6cef026227ee",
                "externalIds": {
                    "CorpusId": 251733060
                },
                "corpusId": 251733060,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b71feb08c9fc4c156c920f53d44d6cef026227ee",
                "title": "Computer-Aided diagnosis of closely related diseases",
                "abstract": "It is often observed that certain human diseases exhibit similarities in some form while having different prognoses and requiring treatment strategies. These similarities may be in the form of risk factors towards the diseases, symptoms observed, visual similarity in imaging studies, or in some cases, similarity in molecular associations. Computer-Aided Diagnosis (CAD) of closely related diseases is challenging and requires tailored approaches to discriminate between such closely related diseases accurately. This thesis looks at two sets of closely related diseases of two different organs, identified from two different modalities. It develops novel approaches to achieve explainable and accurate CAD for these two close diseases. These two problems are discrimination of healthy, mild cognitive impairment (MCI) and Alzheimer\u2019s Disease (AD) from brain MRI-derived surface mesh and classifying healthy, non-COVID pneumonia and COVID from chest X-ray images. In the first part of this thesis, we present a novel 2D image representation for the brain mesh surface, called a height map. Further, we explore the use of height maps towards the hierarchical classification of healthy, MCI, and AD cases. We also compare different strategies of extracting features and regions of interest from height maps and their performance towards healthy vs. MCI vs. AD classification. We demonstrate that the proposed method achieves fast classification of AD and MCI with minor loss of accuracy compared to state of the art. In the second half of this thesis, we present a novel deep learning architecture called Multi-scale Attention Learning Residual Learning (MARL) and a new conicity loss for training the MARL architecture. We utilize MARL and the conicity loss for achieving hierarchical classification of normal, non-COVID pneumonia and COVID pneumonia from Chest X-ray images. We present classification results on three public datasets and demonstrate that the proposed method achieves comparable or marginally better performance than state-of-the-art in all cases. Further, we demonstrate that the proposed framework achieves clinically consistent explanations with extensive experimentation. Qualitatively, this is shown by comparing GradCAM heatmaps for the proposed method to those for the state-of-the-art method. It is observed that the heatmaps overlap better with the bounding boxes for pneumonia marked by experts compared to the overlap achieved by the state-of-the-art method. Next, we show quantitatively that the GradCAM heatmaps for the proposed method generally lie within inner regions of the lung for nonCOVID pneumonia. However, the same heatmaps lie in outer regions in the case of COVID pneumonia. Thus, we establish the clinical consistency of explanations provided by the proposed framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9270776",
                        "name": "Abhinav Dhere"
                    },
                    {
                        "authorId": "1790095",
                        "name": "J. Sivaswamy"
                    },
                    {
                        "authorId": "2118062577",
                        "name": "A. Sharma"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "482b017a58a5b754e130e86a0bdd8b8ae3e689a9",
                "externalIds": {
                    "CorpusId": 253082894
                },
                "corpusId": 253082894,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/482b017a58a5b754e130e86a0bdd8b8ae3e689a9",
                "title": "Agent-based Splitting of Patient-Therapist Interviews for Depression Estimation",
                "abstract": "There has been considerable research in the field of automated mental health analysis. Studies based on patient-therapist interviews usually treat the dyadic discourse as a sequence of sentences, thus ignoring individual sentence types (question or answer). To avoid this situation, we design a multi-view architecture that retains the symmetric discourse structure by dividing the transcripts into patient and therapist views. Experiments on the DAIC-WOZ dataset for depression level rating show performance improvements over baselines and state-of-the-art models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2904063",
                        "name": "N. Agarwal"
                    },
                    {
                        "authorId": "153563898",
                        "name": "Gael Dias"
                    },
                    {
                        "authorId": "4179221",
                        "name": "S. Dollfus"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To address this challenge, prior studies in the software engineering domain have employed various Explainable AI approaches on transformer-based code models (Kenny and Keane, 2021; Mohankumar et al., 2020; Kobayashi et al., 2020; Liu et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "55ecf01eac0468f59432f1eba53556ef91e07b26",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-12821",
                    "DOI": "10.48550/arXiv.2211.12821",
                    "CorpusId": 253801759
                },
                "corpusId": 253801759,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/55ecf01eac0468f59432f1eba53556ef91e07b26",
                "title": "Explainable AI for Pre-Trained Code Models: What Do They Learn? When They Do Not Work?",
                "abstract": "In recent years, there has been a wide interest in designing deep neural network-based models that automate downstream software engineering tasks, such as program document generation, code search, and program repair. Although the main objective of these studies is to improve the e\ufb00ectiveness of the downstream task, many studies only attempt to employ the next best neural network model, without a proper in-depth analysis of why a particular solution works or does not, on particular tasks or scenarios. In this paper, using an eXplainable AI (XAI) method (attention mechanism), we study state-of-the-art Transformer-based models (CodeBERT and GraphCodeBERT) on a set of software engineering downstream tasks: code document generation (CDG), code re\ufb01nement (CR), and code translation (CT). We \ufb01rst evaluate the validity of the attention mechanism on each particular task. Then, through quantitative and qualitative studies, we identify what CodeBERT and Graph-CodeBERT learn (put the highest attention on, in terms of source code token types), on these tasks. Finally, we show some of the common patterns when the model does not work as expected (perform poorly while the problem in hand is easy) and suggest recommendations that may alleviate the observed challenges.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2191899265",
                        "name": "Ahmad Haji Mohammadkhani"
                    },
                    {
                        "authorId": "1957122",
                        "name": "C. Tantithamthavorn"
                    },
                    {
                        "authorId": "1857838",
                        "name": "H. Hemmati"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", punctuations) frequently attract high attention [23], resulting in wrongtranslation or over-translation in NMT [13].",
                "Another work line aims to make attention better indicative of the inputs\u2019 importance [23], [57]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5e5c7850b3486f9dd81e5fe250a199a7b3761d3a",
                "externalIds": {
                    "DBLP": "journals/taslp/LuZZWZ22",
                    "DOI": "10.1109/taslp.2022.3180678",
                    "CorpusId": 249563587
                },
                "corpusId": 249563587,
                "publicationVenue": {
                    "id": "309e00f7-4bbd-461f-ab37-a90cd14ef21d",
                    "name": "IEEE/ACM Transactions on Audio Speech and Language Processing",
                    "alternate_names": [
                        "IEEE/ACM Trans Audio Speech Lang Process"
                    ],
                    "issn": "2329-9290",
                    "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=6570655",
                    "alternate_urls": [
                        "https://signalprocessingsociety.org/publications-resources/ieeeacm-transactions-audio-speech-and-language-processing/ieeeacm"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5e5c7850b3486f9dd81e5fe250a199a7b3761d3a",
                "title": "Attention Analysis and Calibration for Transformer in Natural Language Generation",
                "abstract": "Attention mechanism has been ubiquitous in neural machine translation by dynamically selecting relevant contexts for different translations. Apart from performance gains, attention weights assigned to input tokens are often utilized to explain that high-attention tokens contribute more to the prediction. However, many works question whether this assumption holds in text classification by manually manipulating attention weights and observing decision flips. This article extends this question to Transformer-based neural machine translation, which heavily relies on cross-lingual attention to produce accurate translations but is relatively understudied in this context. We first design a mask perturbation model which automatically assesses each input\u2019s contribution to model outputs. We then test whether the token contributing most to the current translation receives the highest attention weight. We find that it sometimes does not, which closely depends on the entropy of attention weights, the syntactic role of the current generation, and language pairs. We also rethink the discrepancy between attention weights and word alignments from the view of unreliable attention weights. Our observations further motivate us to calibrate the cross-lingual multi-head attention by attaching more attention to indispensable tokens, whose removal leads to a dramatic performance drop. Empirical experiments on different-scale translation tasks and text summarization tasks demonstrate that our calibration methods significantly outperform strong baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2140045136",
                        "name": "Yu Lu"
                    },
                    {
                        "authorId": "38358352",
                        "name": "Jiajun Zhang"
                    },
                    {
                        "authorId": "50409899",
                        "name": "Jiali Zeng"
                    },
                    {
                        "authorId": "2362902",
                        "name": "Shuangzhi Wu"
                    },
                    {
                        "authorId": "2064100826",
                        "name": "Chengqing Zong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another work line aims to make attention better indicative of the inputs\u2019 importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results to enhance the\u2026",
                "\u2026caused many deaths and traffic interruption Ours:\ndays of heavy snow in countryside left many deaths and transportation disrupted Ref:\nSmith, 2019), which can be attributed to that unimportant words (e.g., punctuations) are frequently assigned with high attention weights (Mohankumar et al., 2020).",
                "Another work line aims to make attention better indicative of the inputs\u2019 importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results to enhance the NMT performance.",
                ", punctuations) are frequently assigned with high attention weights (Mohankumar et al., 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "6088df145eacfc6c018cc356c3cb9e22583fcbed",
                "externalIds": {
                    "DBLP": "conf/acl/LuZZW020",
                    "ACL": "2021.acl-long.103",
                    "DOI": "10.18653/v1/2021.acl-long.103",
                    "CorpusId": 236459916
                },
                "corpusId": 236459916,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/6088df145eacfc6c018cc356c3cb9e22583fcbed",
                "title": "Attention Calibration for Transformer in Neural Machine Translation",
                "abstract": "Attention mechanisms have achieved substantial improvements in neural machine translation by dynamically selecting relevant inputs for different predictions. However, recent studies have questioned the attention mechanisms\u2019 capability for discovering decisive inputs. In this paper, we propose to calibrate the attention weights by introducing a mask perturbation model that automatically evaluates each input\u2019s contribution to the model outputs. We increase the attention weights assigned to the indispensable tokens, whose removal leads to a dramatic performance decrease. The extensive experiments on the Transformer-based translation have demonstrated the effectiveness of our model. We further find that the calibrated attention weights are more uniform at lower layers to collect multiple information while more concentrated on the specific inputs at higher layers. Detailed analyses also show a great need for calibration in the attention weights with high entropy where the model is unconfident about its decision.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2140045136",
                        "name": "Yu Lu"
                    },
                    {
                        "authorId": "50409899",
                        "name": "Jiali Zeng"
                    },
                    {
                        "authorId": "2124819243",
                        "name": "Jiajun Zhang"
                    },
                    {
                        "authorId": "2362902",
                        "name": "Shuangzhi Wu"
                    },
                    {
                        "authorId": "2135228973",
                        "name": "Mu Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Ongoing research in xAI community examines many different approaches, with the two of them being in the focus of current work: modelspecific attention-based (Mohankumar et al., 2020) and model-agnostic (Lundberg and Lee, 2017) explanations.",
                "The implementation used in this work has been adapted from the source code1 provided by the authors of the original paper (Mohankumar et al., 2020).",
                "An interesting approach to LSTM with attention has been proposed in Mohankumar et al. (2020)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "71b19f0317a1f0a152a5b68fa5cc8f5b80c0ec93",
                "externalIds": {
                    "DBLP": "conf/semeval/PlucinskiK21",
                    "ACL": "2021.semeval-1.114",
                    "DOI": "10.18653/v1/2021.semeval-1.114",
                    "CorpusId": 236460004
                },
                "corpusId": 236460004,
                "publicationVenue": {
                    "id": "70713d09-6e4b-4554-9d3f-94d08aba320c",
                    "name": "International Workshop on Semantic Evaluation",
                    "type": "conference",
                    "alternate_names": [
                        "SemEval ",
                        "Int Workshop Semantic Evaluation"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/71b19f0317a1f0a152a5b68fa5cc8f5b80c0ec93",
                "title": "GHOST at SemEval-2021 Task 5: Is explanation all you need?",
                "abstract": "This paper discusses different approaches to the Toxic Spans Detection task. The problem posed by the task was to determine which words contribute mostly to recognising a document as toxic. As opposed to binary classification of entire texts, word-level assessment could be of great use during comment moderation, also allowing for a more in-depth comprehension of the model\u2019s predictions. As the main goal was to ensure transparency and understanding, this paper focuses on the current state-of-the-art approaches based on the explainable AI concepts and compares them to a supervised learning solution with word-level labels. The work consists of two xAI approaches that automatically provide the explanation for models trained for binary classification of toxic documents: an LSTM model with attention as a model-specific approach and the Shapley values for interpreting BERT predictions as a model-agnostic method. The competing approach considers this problem as supervised token classification, where models like BERT and its modifications were tested. The paper aims to explore, compare and assess the quality of predictions for different methods on the task. The advantages of each approach and further research direction are also discussed.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46370815",
                        "name": "Kamil Plucinski"
                    },
                    {
                        "authorId": "2051744127",
                        "name": "H. Klimczak"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Yet to which extend those attention models help in interpreting how the model is operating has recently become a novel debate [4, 24]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "64f5a1100c065ec57be72f1d6f997f1593806bc0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-02566",
                    "CorpusId": 235353032
                },
                "corpusId": 235353032,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/64f5a1100c065ec57be72f1d6f997f1593806bc0",
                "title": "Improve the Interpretability of Attention: A Fast, Accurate, and Interpretable High-Resolution Attention Model",
                "abstract": "The prevalence of employing attention mechanisms has brought along concerns on the interpretability of attention distributions. Although it provides insights about how a model is operating, utilizing attention as the explanation of model predictions is still highly dubious. The community is still seeking more interpretable strategies for better identifying local active regions that contribute the most to the final decision. To improve the interpretability of existing attention models, we propose a novel Bilinear Representative Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant human-interpretable information. The target model is first distilled to have higher-resolution intermediate feature maps. From which, representative features are then grouped based on local pairwise feature similarity, to produce finer-grained, more precise attention maps highlighting task-relevant parts of the input. The obtained attention maps are ranked according to the \u2018active level\u2019 of the compound feature, which provides information regarding the important level of the highlighted regions. The proposed model can be easily adapted in a wide variety of modern deep models, where classification is involved. It is also more accurate, faster, and with a smaller memory footprint than usual neural attention modules. Extensive experiments showcase more comprehensive visual explanations compared to the state-of-the-art visualization model across multiple tasks including few-shot classification, person re-identification, fine-grained image classification. The proposed visualization model sheds imperative light on how neural networks \u2018pay their attention\u2019 differently in different tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2070904522",
                        "name": "T. Gomez"
                    },
                    {
                        "authorId": "23993939",
                        "name": "Suiyi Ling"
                    },
                    {
                        "authorId": "2107033619",
                        "name": "Thomas Fr'eour"
                    },
                    {
                        "authorId": "1790706",
                        "name": "H. Mouch\u00e8re"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pluci\u0144ski and Klimczak (2021) used the same approach, but also employed an orthogonalisation technique (Mohankumar et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "76ecdc5441e5e6c07e250c8f45b6d8367acec56f",
                "externalIds": {
                    "ACL": "2021.semeval-1.6",
                    "DBLP": "conf/semeval/PavlopoulosSLA21",
                    "DOI": "10.18653/v1/2021.semeval-1.6",
                    "CorpusId": 236460230
                },
                "corpusId": 236460230,
                "publicationVenue": {
                    "id": "70713d09-6e4b-4554-9d3f-94d08aba320c",
                    "name": "International Workshop on Semantic Evaluation",
                    "type": "conference",
                    "alternate_names": [
                        "SemEval ",
                        "Int Workshop Semantic Evaluation"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/76ecdc5441e5e6c07e250c8f45b6d8367acec56f",
                "title": "SemEval-2021 Task 5: Toxic Spans Detection",
                "abstract": "The Toxic Spans Detection task of SemEval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts. The task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers. It could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. For the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2332587",
                        "name": "John Pavlopoulos"
                    },
                    {
                        "authorId": "144431938",
                        "name": "Jeffrey Scott Sorensen"
                    },
                    {
                        "authorId": "51921740",
                        "name": "Leo Laugier"
                    },
                    {
                        "authorId": "2065227924",
                        "name": "I. Androutsopoulos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite the differences we found between our observations and the observations reported by [2], we still see the potential value of the methods they propose."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "61d4041d8ba9ce24074ddeba55fe5381c69cb7a5",
                "externalIds": {
                    "CorpusId": 237240524
                },
                "corpusId": 237240524,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/61d4041d8ba9ce24074ddeba55fe5381c69cb7a5",
                "title": "[Re] Reproducibility study - Does enforcing diversity in hidden states of LSTM-Attention models improve transparency?",
                "abstract": "Methodology \u2014 The paper includes a link to a repository with the code used to generate its results. We follow four investigative routes: (i) Replication: we rerun experiments on datasets from the paper in order to replicate the results, and add the results that are missing in the paper; (ii) Code review: we scrutinize the code to validate its correctness; (iii) Evaluation methodology: we extend the set of evaluation metrics used in the paper with the LIME method, in an attempt to resolve inconclusive results; (iv) Generalization to other architectures: we test whether the authors\u02bc claims apply to variations of the base model (more complex forms of attention and a BiLSTM encoder).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2183979026",
                        "name": "Pieter Bouwman"
                    },
                    {
                        "authorId": "2117014567",
                        "name": "Yun Li"
                    },
                    {
                        "authorId": "70287262",
                        "name": "R. V. Weerd"
                    },
                    {
                        "authorId": "47314010",
                        "name": "Frank Verhoef"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These papers often provide results that either prove a 32 correlation between the attention weights and predictions [11][10], or the ambiguity between attention weights and the 33 performance of the rest of the model [6][7], or somewhere in between [9]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "049f17c072a09a9a90131052bf82cc430ece182b",
                "externalIds": {
                    "CorpusId": 237273283
                },
                "corpusId": 237273283,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/049f17c072a09a9a90131052bf82cc430ece182b",
                "title": "Manipulating Attention Does Not Deceive Us",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2124154084",
                        "name": "Howard Caulfield"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d8cf05a06cececb220275dc4346727f2cdc31821",
                "externalIds": {
                    "CorpusId": 237263330
                },
                "corpusId": 237263330,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d8cf05a06cececb220275dc4346727f2cdc31821",
                "title": "Reproduction study: Towards Transparent and Explainable Attention Models",
                "abstract": "Mohankumar et al. (2020) claim that current attention mechanisms in LSTM based encoders can neither provide a 3 faithful nor a plausible explanation of the model\u2019s predictions in Natural Language Processing tasks. To make attention 4 mechanisms more faithful and plausible, the authors propose two modified LSTM models with a diversity-driven 5 training objective that ensures that the hidden representations learned at different time steps are diverse: the Orthogonal 6 LSTM and the Diversity LSTM. The authors claim that the resulting attention distributions from these diversity-driven 7 LSTMs offer more explainability and transparency in contrast to a Vanilla LSTM. 8",
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [
                "Unlike Jain and Wallace (2019), and for the same reasons as Mohankumar et al. (2020) and Kitada and Iyatomi (2020), we conducted an experiment with Pearson\u2019s correlation coefficient."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "88654b417e7f43e3b62b12c17d252313ec4d4cc8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-08763",
                    "CorpusId": 260437238
                },
                "corpusId": 260437238,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/88654b417e7f43e3b62b12c17d252313ec4d4cc8",
                "title": "Making Attention Mechanisms More Robust and Interpretable with Virtual Adversarial Training for Semi-Supervised Text Classification",
                "abstract": "We propose a new general training technique for attention mechanisms based on virtual adversarial training (VAT). VAT can compute adversarial perturbations from unlabeled data in a semi-supervised setting for the attention mechanisms that have been reported in previous studies to be vulnerable to perturbations. Empirical experiments reveal that our technique (1) provides significantly better prediction performance compared to not only conventional adversarial training-based techniques but also VAT-based techniques in a semi-supervised setting, (2) demonstrates a stronger correlation with the word importance and better agreement with evidence provided by humans, and (3) gains in performance with increasing amounts of unlabeled data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51267434",
                        "name": "Shunsuke Kitada"
                    },
                    {
                        "authorId": "2801969",
                        "name": "H. Iyatomi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is worth to notice that attention weights can have a role in starting to fostering explainability [27]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "98cead15f4b9d1397efe971de636724e8ae1e120",
                "externalIds": {
                    "DBLP": "conf/aiia/MarconiAZMME20",
                    "CorpusId": 227258032
                },
                "corpusId": 227258032,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/98cead15f4b9d1397efe971de636724e8ae1e120",
                "title": "Approaching Explainable Recommendations for Personalized Social Learning: the Current Stage in the Educational Platform \"WhoTeach\"",
                "abstract": "Learning and training processes are starting to be affected by the diffusion of Artificial Intelligence (AI) techniques and methods. AI can be variously exploited for supporting education, though especially deep learning (DL) models are normally suffering from some degree of opacity and lack of interpretability. Explainable AI (XAI) is aimed at creating a set of new AI techniques able to improve their output or decisions with more transparency and interpretability. Deep attentional mechanisms proved to be particularly effective for identifying relevant communities and relationships in any given input network that can be exploited with the aim of improving useful information to interpret the suggested decision process. In this paper we provide the first stages of our ongoing research project, aimed at significantly empowering the recommender system of the educational platform \u201dWhoTeach\u201d by means of explainability, to help teachers or experts to create and manage highquality courses for personalized learning. The presented model is actually our first tentative to start to include explainability in the system. As shown, the model has strong potentialities to provide relevant recommendations. Moreover, it allows the possibility to implement effective techniques to completely reach explainability .",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144471674",
                        "name": "Luca Marconi"
                    },
                    {
                        "authorId": "1740656487",
                        "name": "Ricardo Anibal Matamoros Aragon"
                    },
                    {
                        "authorId": "1947730",
                        "name": "I. Zoppis"
                    },
                    {
                        "authorId": "1718628",
                        "name": "S. Manzoni"
                    },
                    {
                        "authorId": "1747770",
                        "name": "G. Mauri"
                    },
                    {
                        "authorId": "2449175",
                        "name": "F. Epifania"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0b08ac4be103502ea5b2d2bf70ec762aebacb3af",
                "externalIds": {
                    "CorpusId": 237236060
                },
                "corpusId": 237236060,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0b08ac4be103502ea5b2d2bf70ec762aebacb3af",
                "title": "Towards Transparent and Explainable Attention Models, ML Reproducibility Challenge 2020",
                "abstract": "The original paper proposes new LSTM variants that are supposed to improve the transparency and interpretability of the attention mechanism. They claim that modi ed LSTMs do not hurt the performance (Claim 1) and the attention distributions in the modi ed model variants provide a faithful explanation of the model\u2019s predictions (Claim 2). Additionally, we provide our extensions. We verify whether the mean conicity of the attention values in the Vanilla Transformer is similar to the conicity of the Vanilla LSTM (Claim 4), whether the performance of a modi ed Transformer does not signi cantly hurt performance across different datasets (Claim 5) and if the orthogonalization procedure has homogeneous effects across 4 languages on the sentiment classi cation task (Claim 6).",
                "year": null,
                "authors": []
            }
        }
    ]
}