{
    "data": [
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "FinTabNet [47] is a large dataset containing financial tables.",
                "As shown in Figure 8, we list the visualization results of our methods on tables scanned from documents, including PubTabNet dataset, SciTSR dataset and FinTabNet dataset.",
                "On FinTabNet, our method achieves a TEDS-Struct score of 98.6%, improving the score by 1.8% compared to the competitive method TableFormer.",
                "While tables with simple structures and clean backgrounds can be recognized well [6, 12, 14, 15, 23, 24, 30, 35, 38, 41, 47], recognizing complicated table structures remains a challenging problem, which is primarily due to two main difficulties: 1) Firstly, tables in images vary widely in terms of structure and shape.",
                "We conducted a comprehensive validation of our model\u2019s performance on various datasets, including well-known regular table benchmarks such as SciTSR, PubTabNet, and FinTabNet, which are derived from PDF documents.",
                "Due to the absence of released annotations for the test set, we follow previous approaches [20, 35, 46, 47] and evaluate our model on the validation set using TEDS and TEDS-Struct [48] metrics.",
                "GTE [47] uses an object detection-based method to detect cells directly and uses heuristic rules in post-processing to recover the table structure.",
                "Following [29, 47], we use the split set of training for training and validating samples for testing.",
                "We evaluated our proposed method on regular tables that were scanned from PDF documents, and compared it with several state-of-the-art methods on PubTab-Net, FinTabNet, and SciTSR datasets, and the results are reported in 3.",
                "[16, 25, 35, 36, 47] represent tables by a group of cells.",
                "With a simple pipeline, our method has achieved comparable or state-of-the-art performance on the public benchmarks, including PubTabNet [48], FinTabNet [47], SciTSR [2], WTW [25], and TAL [3].",
                "In works such as [16, 25, 32, 35, 47], a table is represented by a group of cells."
            ],
            "citingPaper": {
                "paperId": "3e797270bd42586b0e46ce597a5ef6d754465e43",
                "externalIds": {
                    "ArXiv": "2309.14962",
                    "CorpusId": 262825477
                },
                "corpusId": 262825477,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3e797270bd42586b0e46ce597a5ef6d754465e43",
                "title": "GridFormer: Towards Accurate Table Structure Recognition via Grid Prediction",
                "abstract": "All tables can be represented as grids. Based on this observation, we propose GridFormer, a novel approach for interpreting unconstrained table structures by predicting the vertex and edge of a grid. First, we propose a flexible table representation in the form of an MXN grid. In this representation, the vertexes and edges of the grid store the localization and adjacency information of the table. Then, we introduce a DETR-style table structure recognizer to efficiently predict this multi-objective information of the grid in a single shot. Specifically, given a set of learned row and column queries, the recognizer directly outputs the vertexes and edges information of the corresponding rows and columns. Extensive experiments on five challenging benchmarks which include wired, wireless, multi-merge-cell, oriented, and distorted tables demonstrate the competitive performance of our model over other methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "10344582",
                        "name": "Pengyuan Lyu"
                    },
                    {
                        "authorId": "47121217",
                        "name": "Weihong Ma"
                    },
                    {
                        "authorId": "2109798334",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "1631405123",
                        "name": "Yu Yu"
                    },
                    {
                        "authorId": "2248958848",
                        "name": "Chengquan Zhang"
                    },
                    {
                        "authorId": "2140371004",
                        "name": "Kun Yao"
                    },
                    {
                        "authorId": "2114922667",
                        "name": "Yang Xue"
                    },
                    {
                        "authorId": "1688516",
                        "name": "Jingdong Wang"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "FinTabNet [47] is a large dataset containing financial tables.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "As shown in Figure 8, we list the visualization results of our methods on tables scanned from documents, including PubTabNet dataset, SciTSR dataset and FinTabNet dataset.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "On FinTabNet, our method achieves a TEDS-Struct score of 98.6%, improving the score by 1.8% compared to the competitive method TableFormer.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "While tables with simple structures and clean backgrounds can be recognized well [6, 12, 14, 15, 23, 24, 30, 35, 38, 41, 47], recognizing complicated table structures remains a challenging problem, which is primarily due to two main difficulties: 1) Firstly, tables in images vary widely in terms of structure and shape.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We conducted a comprehensive validation of our model\u2019s performance on various datasets, including well-known regular table benchmarks such as SciTSR, PubTabNet, and FinTabNet, which are derived from PDF documents.",
                    "label_score": 0.5,
                    "label": "Weak"
                },
                {
                    "context": "Due to the absence of released annotations for the test set, we follow previous approaches [20, 35, 46, 47] and evaluate our model on the validation set using TEDS and TEDS-Struct [48] metrics.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "GTE [47] uses an object detection-based method to detect cells directly and uses heuristic rules in post-processing to recover the table structure.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Following [29, 47], we use the split set of training for training and validating samples for testing.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We evaluated our proposed method on regular tables that were scanned from PDF documents, and compared it with several state-of-the-art methods on PubTab-Net, FinTabNet, and SciTSR datasets, and the results are reported in 3.",
                    "label_score": 0.5,
                    "label": "Weak"
                },
                {
                    "context": "[16, 25, 35, 36, 47] represent tables by a group of cells.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "With a simple pipeline, our method has achieved comparable or state-of-the-art performance on the public benchmarks, including PubTabNet [48], FinTabNet [47], SciTSR [2], WTW [25], and TAL [3].",
                    "label_score": 0.5,
                    "label": "Weak"
                },
                {
                    "context": "In works such as [16, 25, 32, 35, 47], a table is represented by a group of cells.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a29363d6f15b9d9774d1ca5e6e56f81d230e2dc1",
                "externalIds": {
                    "ArXiv": "2308.12896",
                    "DBLP": "journals/corr/abs-2308-12896",
                    "DOI": "10.48550/arXiv.2308.12896",
                    "CorpusId": 261101131
                },
                "corpusId": 261101131,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a29363d6f15b9d9774d1ca5e6e56f81d230e2dc1",
                "title": "Beyond Document Page Classification: Design, Datasets, and Challenges",
                "abstract": "This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distribution shifts (e.g., born-digital vs. scanning noise, shifting page order). Our study ends on a hopeful note by recommending concrete avenues for future improvements.}",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1394500825",
                        "name": "Jordy Van Landeghem"
                    },
                    {
                        "authorId": "2150473007",
                        "name": "Sanket Biswas"
                    },
                    {
                        "authorId": "1758219",
                        "name": "Matthew B. Blaschko"
                    },
                    {
                        "authorId": "1802161",
                        "name": "M. Moens"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", [7, 13, 17, 18, 21, 29, 37, 44, 55, 56]) utilize a deep learning (DL)-based approach, focusing on either one, two, or all three steps."
            ],
            "citingPaper": {
                "paperId": "43f671656d293964bb5d0c5bdcff7223876ca09f",
                "externalIds": {
                    "DBLP": "conf/doceng/DashCA23",
                    "DOI": "10.1145/3573128.3604901",
                    "CorpusId": 260441780
                },
                "corpusId": 260441780,
                "publicationVenue": {
                    "id": "32c90593-78aa-40de-8700-dd31a625b15a",
                    "name": "ACM Symposium on Document Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "Doc Eng",
                        "Document Engineering",
                        "DocEng",
                        "ACM Symp Doc Eng"
                    ],
                    "url": "http://www.documentengineering.org/"
                },
                "url": "https://www.semanticscholar.org/paper/43f671656d293964bb5d0c5bdcff7223876ca09f",
                "title": "WEATHERGOV+: A Table Recognition and Summarization Dataset to Bridge the Gap Between Document Image Analysis and Natural Language Generation",
                "abstract": "Tables, ubiquitous in data-oriented documents like scientific papers and financial statements, organize and convey relational information. Automatic table recognition from document images, which involves detection within the page, structural segmentation into rows, columns, and cells, and information extraction from cells, has been a popular research topic in document image analysis (DIA). With recent advances in natural language generation (NLG) based on deep neural networks, data-to-text generation, in particular for table summarization, offers interesting solutions to time-intensive data analysis. In this paper, we aim to bridge the gap between efforts in DIA and NLG regarding tabular data: we propose WEATHERGOV+, a dataset building upon the WEATHERGOV dataset, the standard for tabular data summarization techniques, that allows for the training and testing of end-to-end methods working from input document images to generate text summaries as output. WEATHERGOV+ contains images of tables created from the tabular data of WEATHERGOV using visual variations that cover various levels of difficulty, along with the corresponding human-generated table summaries of WEATHERGOV. We also propose an end-to-end pipeline that compares state-of-the-art table recognition methods for summarization purposes. We analyse the results of the proposed pipeline by evaluating WEATHERGOV+ at each stage of the pipeline to identify the effects of error propagation and the weaknesses of the current methods, such as OCR errors. With this research (dataset and code available here1), we hope to encourage new research for the processing and management of inter- and intra-document collections.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051833",
                        "name": "Amanda Dash"
                    },
                    {
                        "authorId": "2587465",
                        "name": "Melissa Cote"
                    },
                    {
                        "authorId": "2639980",
                        "name": "A. Albu"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": ", [7, 13, 17, 18, 21, 29, 37, 44, 55, 56]) utilize a deep learning (DL)-based approach, focusing on either one, two, or all three steps.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "219e33388cacbcbf4b063ebf60c0bee48936fe37",
                "externalIds": {
                    "ArXiv": "2307.13617",
                    "DBLP": "journals/corr/abs-2307-13617",
                    "DOI": "10.48550/arXiv.2307.13617",
                    "CorpusId": 260154895
                },
                "corpusId": 260154895,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/219e33388cacbcbf4b063ebf60c0bee48936fe37",
                "title": "GPT-3 Models are Few-Shot Financial Reasoners",
                "abstract": "Financial analysis is an important tool for evaluating company performance. Practitioners work to answer financial questions to make profitable investment decisions, and use advanced quantitative analyses to do so. As a result, Financial Question Answering (QA) is a question answering task that requires deep reasoning about numbers. Furthermore, it is unknown how well pre-trained language models can reason in the financial domain. The current state-of-the-art requires a retriever to collect relevant facts about the financial question from the text and a generator to produce a valid financial program and a final answer. However, recently large language models like GPT-3 [1] have achieved state-of-the-art performance on wide variety of tasks with just a few shot examples. We run several experiments with GPT-3 and find that a separate retrieval model and logic engine continue to be essential components to achieving SOTA performance in this task, particularly due to the precise nature of financial questions and the complex information stored in financial documents. With this understanding, our refined promptengineering approach on GPT-3 achieves near SOTA accuracy without any fine-tuning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2121319811",
                        "name": "Raul Salles de Padua"
                    },
                    {
                        "authorId": "2224809971",
                        "name": "Imran Qureshi"
                    },
                    {
                        "authorId": "5297086",
                        "name": "M. Karakaplan"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Therefore, the existing approaches [28, 44, 26] to TDR are vision-only approaches.",
                ", table cells) and leverages this prior knowledge in the model design and post-processing [28, 44, 26]."
            ],
            "citingPaper": {
                "paperId": "85e6c2d34304a01e3f7071cdc7200405c4af93d0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-07929",
                    "ArXiv": "2307.07929",
                    "DOI": "10.48550/arXiv.2307.07929",
                    "CorpusId": 259937511
                },
                "corpusId": 259937511,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/85e6c2d34304a01e3f7071cdc7200405c4af93d0",
                "title": "DocTr: Document Transformer for Structured Information Extraction in Documents",
                "abstract": "We present a new formulation for structured information extraction (SIE) from visually rich documents. It aims to address the limitations of existing IOB tagging or graph-based formulations, which are either overly reliant on the correct ordering of input text or struggle with decoding a complex graph. Instead, motivated by anchor-based object detectors in vision, we represent an entity as an anchor word and a bounding box, and represent entity linking as the association between anchor words. This is more robust to text ordering, and maintains a compact graph for entity linking. The formulation motivates us to introduce 1) a DOCument TRansformer (DocTr) that aims at detecting and associating entity bounding boxes in visually rich documents, and 2) a simple pre-training strategy that helps learn entity detection in the context of language. Evaluations on three SIE benchmarks show the effectiveness of the proposed formulation, and the overall approach outperforms existing solutions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145585312",
                        "name": "Haofu Liao"
                    },
                    {
                        "authorId": "2895705",
                        "name": "Aruni RoyChowdhury"
                    },
                    {
                        "authorId": "48624767",
                        "name": "Weijian Li"
                    },
                    {
                        "authorId": "2068427",
                        "name": "Ankan Bansal"
                    },
                    {
                        "authorId": "2108148342",
                        "name": "Yuting Zhang"
                    },
                    {
                        "authorId": "144035504",
                        "name": "Z. Tu"
                    },
                    {
                        "authorId": "1710219",
                        "name": "R. Satzoda"
                    },
                    {
                        "authorId": "1758550",
                        "name": "R. Manmatha"
                    },
                    {
                        "authorId": "48493294",
                        "name": "V. Mahadevan"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Therefore, the existing approaches [28, 44, 26] to TDR are vision-only approaches.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": ", table cells) and leverages this prior knowledge in the model design and post-processing [28, 44, 26].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The study also considers the use of TABBIE for data discovery and clustering, with table embeddings derived from the CLS token in the (0,0) position of the table being used to cluster the FinTabNet dataset which is composed of tables from S&P\u2019s corporate filings [41]."
            ],
            "citingPaper": {
                "paperId": "680dac1238686141204a580768fd429bcda830ff",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-04217",
                    "ArXiv": "2307.04217",
                    "DOI": "10.48550/arXiv.2307.04217",
                    "CorpusId": 259501937
                },
                "corpusId": 259501937,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/680dac1238686141204a580768fd429bcda830ff",
                "title": "LakeBench: Benchmarks for Data Discovery over Data Lakes",
                "abstract": "Within enterprises, there is a growing need to intelligently navigate data lakes, specifically focusing on data discovery. Of particular importance to enterprises is the ability to find related tables in data repositories. These tables can be unionable, joinable, or subsets of each other. There is a dearth of benchmarks for these tasks in the public domain, with related work targeting private datasets. In LakeBench, we develop multiple benchmarks for these tasks by using the tables that are drawn from a diverse set of data sources such as government data from CKAN, Socrata, and the European Central Bank. We compare the performance of 4 publicly available tabular foundational models on these tasks. None of the existing models had been trained on the data discovery tasks that we developed for this benchmark; not surprisingly, their performance shows significant room for improvement. The results suggest that the establishment of such benchmarks may be useful to the community to build tabular models usable for data discovery in data lakes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145993352",
                        "name": "Kavitha Srinivas"
                    },
                    {
                        "authorId": "2008196211",
                        "name": "Julian Dolby"
                    },
                    {
                        "authorId": "145749443",
                        "name": "I. Abdelaziz"
                    },
                    {
                        "authorId": "1728091",
                        "name": "Oktie Hassanzadeh"
                    },
                    {
                        "authorId": "2221858230",
                        "name": "Harsha Kokel"
                    },
                    {
                        "authorId": "1738848458",
                        "name": "Aamod Khatiwada"
                    },
                    {
                        "authorId": "22218771",
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "authorId": "34597365",
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "authorId": "1756353",
                        "name": "H. Samulowitz"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "The study also considers the use of TABBIE for data discovery and clustering, with table embeddings derived from the CLS token in the (0,0) position of the table being used to cluster the FinTabNet dataset which is composed of tables from S&P\u2019s corporate filings [41].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bfdff6cb6fee8d04b41bd99c61556252908b94e4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-13526",
                    "ArXiv": "2306.13526",
                    "DOI": "10.48550/arXiv.2306.13526",
                    "CorpusId": 259243687
                },
                "corpusId": 259243687,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bfdff6cb6fee8d04b41bd99c61556252908b94e4",
                "title": "Bridging the Performance Gap between DETR and R-CNN for Graphical Object Detection in Document Images",
                "abstract": "This paper takes an important step in bridging the performance gap between DETR and R-CNN for graphical object detection. Existing graphical object detection approaches have enjoyed recent enhancements in CNN-based object detection methods, achieving remarkable progress. Recently, Transformer-based detectors have considerably boosted the generic object detection performance, eliminating the need for hand-crafted features or post-processing steps such as Non-Maximum Suppression (NMS) using object queries. However, the effectiveness of such enhanced transformer-based detection algorithms has yet to be verified for the problem of graphical object detection. Essentially, inspired by the latest advancements in the DETR, we employ the existing detection transformer with few modifications for graphical object detection. We modify object queries in different ways, using points, anchor boxes and adding positive and negative noise to the anchors to boost performance. These modifications allow for better handling of objects with varying sizes and aspect ratios, more robustness to small variations in object positions and sizes, and improved image discrimination between objects and non-objects. We evaluate our approach on the four graphical datasets: PubTables, TableBank, NTable and PubLaynet. Upon integrating query modifications in the DETR, we outperform prior works and achieve new state-of-the-art results with the mAP of 96.9\\%, 95.7\\% and 99.3\\% on TableBank, PubLaynet, PubTables, respectively. The results from extensive ablations show that transformer-based methods are more effective for document analysis analogous to other applications. We hope this study draws more attention to the research of using detection transformers in document image analysis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2029681143",
                        "name": "Tahira Shehzadi"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [],
            "isInfluential": true,
            "contexts": [
                "(ICDAR), vol. 1, Nov. 2017, pp. 1162\u2013 1167\n[13] X. Zheng, D. Burdick, L. Popa, X. Zhong, and N. X. R. Wang, \u2018\u2018Global table extractor (GTE): A framework for joint table identification and cell structure recognition using visual context,\u2019\u2019 in\nProc.",
                "Global table extractor (GTE) [13] Proposed a generic abject detection approach A comprehensive approach that is compatible with all object detection",
                "Global table extractor (GTE) [13]\nProposed a generic abject detection approach\nA comprehensive approach that is compatible with all object detection frameworks."
            ],
            "citingPaper": {
                "paperId": "4bcaef5baa082bf4cca420bbfca91c9508111267",
                "externalIds": {
                    "DOI": "10.1109/ICCES57224.2023.10192739",
                    "CorpusId": 260386295
                },
                "corpusId": 260386295,
                "publicationVenue": {
                    "id": "4a0a181c-3807-4883-8ac2-b0ddf5f46497",
                    "name": "International Conference on Communication and Electronics Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Commun Electron Syst",
                        "ICCES",
                        "Int Conf Comput Eng Syst",
                        "International Conference on Computer Engineering and Systems"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4bcaef5baa082bf4cca420bbfca91c9508111267",
                "title": "Table Detection & Data Extraction from Documents using Object Detection",
                "abstract": "Table detection and data extraction from documents is a crucial task in the field of computer vision and document analysis. The goal is to automatically detect tables in documents and extract relevant information from them. This can be achieved through object detection techniques, which use machine learning algorithms to identify objects in images and extract information from them. The process involves training a model on a dataset of table images, and then using the trained model to detect tables in new images. Once tables are detected, data extraction algorithms can be used to extract text and numerical information from the cells of the table. This information can then be used for a variety of applications, such as database creation, business intelligence, and data analysis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2227045096",
                        "name": "Mohammad Talha Khan"
                    },
                    {
                        "authorId": "9409896",
                        "name": "T. Jeyaprakash"
                    },
                    {
                        "authorId": "40964034",
                        "name": "D. chandar"
                    },
                    {
                        "authorId": "83337181",
                        "name": "V. Durga"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "(ICDAR), vol. 1, Nov. 2017, pp. 1162\u2013 1167\n[13] X. Zheng, D. Burdick, L. Popa, X. Zhong, and N. X. R. Wang, \u2018\u2018Global table extractor (GTE): A framework for joint table identification and cell structure recognition using visual context,\u2019\u2019 in\nProc.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Global table extractor (GTE) [13] Proposed a generic abject detection approach A comprehensive approach that is compatible with all object detection",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Global table extractor (GTE) [13]\nProposed a generic abject detection approach\nA comprehensive approach that is compatible with all object detection frameworks.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Document Understanding encompasses datasets related to various subtasks like document layout analysis [109, 49], classification [30], key information extraction [85, 35], table extraction [83, 108, 107], and visual question answering [57, 59, 91]."
            ],
            "citingPaper": {
                "paperId": "3d07f41628a25e2e9b895b8a3659ca3ebfd1a73f",
                "externalIds": {
                    "ArXiv": "2305.08455",
                    "DBLP": "journals/corr/abs-2305-08455",
                    "DOI": "10.48550/arXiv.2305.08455",
                    "CorpusId": 258685443
                },
                "corpusId": 258685443,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3d07f41628a25e2e9b895b8a3659ca3ebfd1a73f",
                "title": "Document Understanding Dataset and Evaluation (DUDE)",
                "abstract": "We call on the Document AI (DocAI) community to reevaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where powerful generalization and adaptation under low-resource settings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the community, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance of finding more efficient ways to model language, images, and layout in DocAI.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1394500825",
                        "name": "Jordy Van Landeghem"
                    },
                    {
                        "authorId": "134682605",
                        "name": "Rub\u00e8n P\u00e9rez Tito"
                    },
                    {
                        "authorId": "3448729",
                        "name": "\u0141ukasz Borchmann"
                    },
                    {
                        "authorId": "1749652564",
                        "name": "Michal Pietruszka"
                    },
                    {
                        "authorId": "2217255001",
                        "name": "Pawel J'oziak"
                    },
                    {
                        "authorId": "1498640514",
                        "name": "Rafal Powalski"
                    },
                    {
                        "authorId": "1400414734",
                        "name": "Dawid Jurkiewicz"
                    },
                    {
                        "authorId": "1732746",
                        "name": "Micka\u00ebl Coustaty"
                    },
                    {
                        "authorId": "2217264465",
                        "name": "Bertrand Ackaert"
                    },
                    {
                        "authorId": "2864362",
                        "name": "Ernest Valveny"
                    },
                    {
                        "authorId": "1758219",
                        "name": "Matthew B. Blaschko"
                    },
                    {
                        "authorId": "2163584501",
                        "name": "Sien Moens"
                    },
                    {
                        "authorId": "2106450032",
                        "name": "Tomasz Stanislawek"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Document Understanding encompasses datasets related to various subtasks like document layout analysis [109, 49], classification [30], key information extraction [85, 35], table extraction [83, 108, 107], and visual question answering [57, 59, 91].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Created by eleven finance professionals, FinQA is based on earnings reports from S&P 500 companies (Zheng et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "45d325884a5169df06e41288717b7a78c07bedb7",
                "externalIds": {
                    "ArXiv": "2305.05862",
                    "DBLP": "journals/corr/abs-2305-05862",
                    "DOI": "10.48550/arXiv.2305.05862",
                    "CorpusId": 258588320
                },
                "corpusId": 258588320,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/45d325884a5169df06e41288717b7a78c07bedb7",
                "title": "Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks",
                "abstract": "The most recent large language models such as ChatGPT and GPT-4 have garnered significant attention, as they are capable of generating high-quality responses to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic text corpora, showcasing their impressive capabilities, a study focusing on financial corpora has not been conducted. In this study, we aim to bridge this gap by examining the potential of ChatGPT and GPT-4 as a solver for typical financial text analytic problems in the zero-shot or few-shot setting. Specifically, we assess their capabilities on four representative tasks over five distinct financial textual datasets. The preliminary study shows that ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition (NER) and sentiment analysis, where domain-specific knowledge is required, while they excel in numerical reasoning tasks. We report both the strengths and limitations of the current versions of ChatGPT and GPT-4, comparing them to the state-of-the-art finetuned models as well as pretrained domain-specific generative models. Our experiments provide qualitative studies, through which we hope to help understand the capability of the existing models and facilitate further improvements.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108363412",
                        "name": "Xianzhi Li"
                    },
                    {
                        "authorId": "1854999",
                        "name": "Xiao-Dan Zhu"
                    },
                    {
                        "authorId": "2116415715",
                        "name": "Zhiqiang Ma"
                    },
                    {
                        "authorId": "2108959746",
                        "name": "Xiaomo Liu"
                    },
                    {
                        "authorId": "36532736",
                        "name": "Sameena Shah"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Created by eleven finance professionals, FinQA is based on earnings reports from S&P 500 companies (Zheng et al., 2020).",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "On the other hand, the problem of table structure recognition (TSR) is a lot more challenging and remains a very active area of research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22].",
                "In section 3 we review the current HTML table encoding (popularised by PubTabNet and FinTabNet) and discuss its flaws.",
                "While the majority of research in TSR is currently focused on the development and application of novel neural model architectures, the table structure representation language (e.g. HTML in PubTabNet and FinTabNet) is usually adopted as is for the sequence tokenization in Im2Seq models.",
                "Object-detection based methods [11,12,13,14,21] rely on tablestructure annotation using (overlapping) bounding boxes for training, and produce bounding-box predictions to define table cells, rows, and columns on a table image.",
                "In reality though, one needs at least 28 HTML tokens to describe the most common complex tables observed in real-world documents [21,22], due to a variety of spanning cells definitions in the HTML token vocabulary.",
                "It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.",
                "TSR and cell detection results compared between OTSL and HTML on the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using TableFormer [9] (with enc=6, dec=6, heads=8).",
                "Public table-structure data sets such as PubTabNet [22], and FinTabNet [21], which were created in a semi-automated way from paired PDF and HTML sources (e.g. PubMed Central), popularized primarily the use of HTML as ground-truth representation format for TSR.",
                "Public table-structure data sets such as PubTabNet [22], and FinTabNet [21], which were created in a semi-automated way from paired PDF and HTML sources (e.",
                "Secondly we pick the best hyper-parameters found in the first step and evaluate how OTSL impacts the performance of TableFormer after training on other publicly available data sets (FinTabNet, PubTables-1M [14]).",
                "We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples)."
            ],
            "citingPaper": {
                "paperId": "88b556a6108b98cd0e12ae2945cc6597fff9101c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-03393",
                    "ArXiv": "2305.03393",
                    "DOI": "10.48550/arXiv.2305.03393",
                    "CorpusId": 258546918
                },
                "corpusId": 258546918,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/88b556a6108b98cd0e12ae2945cc6597fff9101c",
                "title": "Optimized Table Tokenization for Table Structure Recognition",
                "abstract": "Extracting tables from documents is a crucial task in any document conversion pipeline. Recently, transformer-based models have demonstrated that table-structure can be recognized with impressive accuracy using Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since the token representation of the table structure has a significant impact on the accuracy and run-time performance of any Im2Seq model, we investigate in this paper how table-structure representation can be optimised. We propose a new, optimised table-structure language (OTSL) with a minimized vocabulary and specific rules. The benefits of OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and shortens the sequence length to half of HTML on average. Consequently, model accuracy improves significantly, inference time is halved compared to HTML-based models, and the predicted table structures are always syntactically correct. This in turn eliminates most post-processing needs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1578242957",
                        "name": "Maksym Lysak"
                    },
                    {
                        "authorId": "73241238",
                        "name": "Ahmed Nassar"
                    },
                    {
                        "authorId": "2051177400",
                        "name": "Nikolaos Livathinos"
                    },
                    {
                        "authorId": "2065064783",
                        "name": "Christoph Auer"
                    },
                    {
                        "authorId": "1831851",
                        "name": "P. Staar"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "On the other hand, the problem of table structure recognition (TSR) is a lot more challenging and remains a very active area of research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "In section 3 we review the current HTML table encoding (popularised by PubTabNet and FinTabNet) and discuss its flaws.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "While the majority of research in TSR is currently focused on the development and application of novel neural model architectures, the table structure representation language (e.g. HTML in PubTabNet and FinTabNet) is usually adopted as is for the sequence tokenization in Im2Seq models.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Object-detection based methods [11,12,13,14,21] rely on tablestructure annotation using (overlapping) bounding boxes for training, and produce bounding-box predictions to define table cells, rows, and columns on a table image.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "In reality though, one needs at least 28 HTML tokens to describe the most common complex tables observed in real-world documents [21,22], due to a variety of spanning cells definitions in the HTML token vocabulary.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on difficult financial tables (FinTabNet) that contain sparse and large tables.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "TSR and cell detection results compared between OTSL and HTML on the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using TableFormer [9] (with enc=6, dec=6, heads=8).",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Public table-structure data sets such as PubTabNet [22], and FinTabNet [21], which were created in a semi-automated way from paired PDF and HTML sources (e.g. PubMed Central), popularized primarily the use of HTML as ground-truth representation format for TSR.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Public table-structure data sets such as PubTabNet [22], and FinTabNet [21], which were created in a semi-automated way from paired PDF and HTML sources (e.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Secondly we pick the best hyper-parameters found in the first step and evaluate how OTSL impacts the performance of TableFormer after training on other publicly available data sets (FinTabNet, PubTables-1M [14]).",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We picked the model parameter configuration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples).",
                    "label_score": 0.5,
                    "label": "Weak"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8f87ad8807a4b149c69bac1f169d9f28d13f8928",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-02769",
                    "ArXiv": "2305.02769",
                    "DOI": "10.48550/arXiv.2305.02769",
                    "CorpusId": 258480298
                },
                "corpusId": 258480298,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/8f87ad8807a4b149c69bac1f169d9f28d13f8928",
                "title": "Towards End-to-End Semi-Supervised Table Detection with Deformable Transformer",
                "abstract": "Table detection is the task of classifying and localizing table objects within document images. With the recent development in deep learning methods, we observe remarkable success in table detection. However, a significant amount of labeled data is required to train these models effectively. Many semi-supervised approaches are introduced to mitigate the need for a substantial amount of label data. These approaches use CNN-based detectors that rely on anchor proposals and post-processing stages such as NMS. To tackle these limitations, this paper presents a novel end-to-end semi-supervised table detection method that employs the deformable transformer for detecting table objects. We evaluate our semi-supervised method on PubLayNet, DocBank, ICADR-19 and TableBank datasets, and it achieves superior performance compared to previous methods. It outperforms the fully supervised method (Deformable transformer) by +3.4 points on 10\\% labels of TableBank-both dataset and the previous CNN-based semi-supervised approach (Soft Teacher) by +1.8 points on 10\\% labels of PubLayNet dataset. We hope this work opens new possibilities towards semi-supervised and unsupervised table detection methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2029681143",
                        "name": "Tahira Shehzadi"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Several studies have proposed detectors for detecting cells or their contents [28,31,30,49].",
                "TRACE\u2019s F1-measure shows competitive performance with GTE [49], and it achieved a higher score in terms of Purity and Completeness.",
                "proposed Global Table Extractor (GTE)[49] that used two separate table and cell detectors based on Faster-RCNN.",
                "RobusTabNet[25] used CornerNet[16] for TD, and proposed line prediction model for TSR. Recently, Zheng et al. proposed Global Table Extractor (GTE)[49] that used two separate table and cell detectors based on Faster-RCNN.",
                "For example, when comparing the result with and without table detection in GTE, the performance dropped by 2.7%."
            ],
            "citingPaper": {
                "paperId": "eca4dfc1703529bd87e1b4fea1fad1baa06e0ac9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00630",
                    "ArXiv": "2305.00630",
                    "DOI": "10.48550/arXiv.2305.00630",
                    "CorpusId": 258427145
                },
                "corpusId": 258427145,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/eca4dfc1703529bd87e1b4fea1fad1baa06e0ac9",
                "title": "TRACE: Table Reconstruction Aligned to Corner and Edges",
                "abstract": "A table is an object that captures structured and informative content within a document, and recognizing a table in an image is challenging due to the complexity and variety of table layouts. Many previous works typically adopt a two-stage approach; (1) Table detection(TD) localizes the table region in an image and (2) Table Structure Recognition(TSR) identifies row- and column-wise adjacency relations between the cells. The use of a two-stage approach often entails the consequences of error propagation between the modules and raises training and inference inefficiency. In this work, we analyze the natural characteristics of a table, where a table is composed of cells and each cell is made up of borders consisting of edges. We propose a novel method to reconstruct the table in a bottom-up manner. Through a simple process, the proposed method separates cell boundaries from low-level features, such as corners and edges, and localizes table positions by combining the cells. A simple design makes the model easier to train and requires less computation than previous two-stage methods. We achieve state-of-the-art performance on the ICDAR2013 table competition benchmark and Wired Table in the Wild(WTW) dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057310228",
                        "name": "Youngmin Baek"
                    },
                    {
                        "authorId": "2064808754",
                        "name": "Daehyun Nam"
                    },
                    {
                        "authorId": "10787779",
                        "name": "Jaeheung Surh"
                    },
                    {
                        "authorId": "2111068603",
                        "name": "Seung Shin"
                    },
                    {
                        "authorId": "2109603647",
                        "name": "Seonghyeon Kim"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Several studies have proposed detectors for detecting cells or their contents [28,31,30,49].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "TRACE\u2019s F1-measure shows competitive performance with GTE [49], and it achieved a higher score in terms of Purity and Completeness.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "proposed Global Table Extractor (GTE)[49] that used two separate table and cell detectors based on Faster-RCNN.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "RobusTabNet[25] used CornerNet[16] for TD, and proposed line prediction model for TSR. Recently, Zheng et al. proposed Global Table Extractor (GTE)[49] that used two separate table and cell detectors based on Faster-RCNN.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "For example, when comparing the result with and without table detection in GTE, the performance dropped by 2.7%.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7c6e3aecb63ba5046d667ffe76ed250e4888b044",
                "externalIds": {
                    "DBLP": "conf/icdar/QiaoWXHL23",
                    "ArXiv": "2304.13240",
                    "DOI": "10.48550/arXiv.2304.13240",
                    "CorpusId": 258331895
                },
                "corpusId": 258331895,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/7c6e3aecb63ba5046d667ffe76ed250e4888b044",
                "title": "Structure Diagram Recognition in Financial Announcements",
                "abstract": "Accurately extracting structured data from structure diagrams in financial announcements is of great practical importance for building financial knowledge graphs and further improving the efficiency of various financial applications. First, we proposed a new method for recognizing structure diagrams in financial announcements, which can better detect and extract different types of connecting lines, including straight lines, curves, and polylines of different orientations and angles. Second, we developed a two-stage method to efficiently generate the industry's first benchmark of structure diagrams from Chinese financial announcements, where a large number of diagrams were synthesized and annotated using an automated tool to train a preliminary recognition model with fairly good performance, and then a high-quality benchmark can be obtained by automatically annotating the real-world structure diagrams using the preliminary model and then making few manual corrections. Finally, we experimentally verified the significant performance advantage of our structure diagram recognition method over previous methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2215383277",
                        "name": "Meixuan Qiao"
                    },
                    {
                        "authorId": "2152813098",
                        "name": "Jun Wang"
                    },
                    {
                        "authorId": "3287439",
                        "name": "Junfu Xiang"
                    },
                    {
                        "authorId": "2215390241",
                        "name": "Qiyu Hou"
                    },
                    {
                        "authorId": "2206099851",
                        "name": "Ruixuan Li"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Zheng et al.[37] 2022 GTE WACV PubTabNet ICDAR 2013 + ICDAR 2019 - - NR Jain et al."
            ],
            "citingPaper": {
                "paperId": "50903f5f5e714e5746152b3e369c9329655e0018",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-10439",
                    "ArXiv": "2304.10439",
                    "DOI": "10.48550/arXiv.2304.10439",
                    "CorpusId": 258236507
                },
                "corpusId": 258236507,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/50903f5f5e714e5746152b3e369c9329655e0018",
                "title": "A Study on Reproducibility and Replicability of Table Structure Recognition Methods",
                "abstract": "Concerns about reproducibility in artificial intelligence (AI) have emerged, as researchers have reported unsuccessful attempts to directly reproduce published findings in the field. Replicability, the ability to affirm a finding using the same procedures on new data, has not been well studied. In this paper, we examine both reproducibility and replicability of a corpus of 16 papers on table structure recognition (TSR), an AI task aimed at identifying cell locations of tables in digital documents. We attempt to reproduce published results using codes and datasets provided by the original authors. We then examine replicability using a dataset similar to the original as well as a new dataset, GenTSR, consisting of 386 annotated tables extracted from scientific papers. Out of 16 papers studied, we reproduce results consistent with the original in only four. Two of the four papers are identified as replicable using the similar dataset under certain IoU values. No paper is identified as replicable using the new dataset. We offer observations on the causes of irreproducibility and irreplicability. All code and data are available on Codeocean at https://codeocean.com/capsule/6680116/tree.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "11359490",
                        "name": "Kehinde E. Ajayi"
                    },
                    {
                        "authorId": "2214808132",
                        "name": "Muntabhir Hasan Choudhury"
                    },
                    {
                        "authorId": "2528276",
                        "name": "S. Rajtmajer"
                    },
                    {
                        "authorId": "97569165",
                        "name": "Jian Wu"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Zheng et al.[37] 2022 GTE WACV PubTabNet ICDAR 2013 + ICDAR 2019 - - NR Jain et al.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The following are examples of deep learning implementations for image classification, cell structure recognition, tissue segmentation, and computer-assisted disease diagnosis [11]."
            ],
            "citingPaper": {
                "paperId": "1f1405006519f7b9c59987b6b524e6b610a26202",
                "externalIds": {
                    "DOI": "10.5614/itbj.ict.res.appl.2023.17.1.7",
                    "CorpusId": 259018664
                },
                "corpusId": 259018664,
                "publicationVenue": {
                    "id": "0cb11a1c-aa39-4fa0-ae1b-c3c68b0d20a0",
                    "name": "Journal of ICT Research and Applications",
                    "type": "journal",
                    "alternate_names": [
                        "J ICT Res Appl"
                    ],
                    "issn": "2337-5787",
                    "url": "http://journals.itb.ac.id/index.php/jictra/about/index",
                    "alternate_urls": [
                        "http://journals.itb.ac.id/index.php/jictra/index"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1f1405006519f7b9c59987b6b524e6b610a26202",
                "title": "CNN Based Covid-19 Detection from Image Processing",
                "abstract": "Covid-19 is a respirational condition that looks much like pneumonia. It is highly contagious and has many variants with different symptoms. Covid-19 poses the challenge of discovering new testing and detection methods in biomedical science. X-ray images and CT scans provide high-quality and information-rich images. These images can be processed with a convolutional neural network (CNN) to detect diseases such as Covid-19 in the pulmonary system with high accuracy. Deep learning applied to X-ray images can help to develop methods to identify Covid-19 infection. Based on the research problem, this study defined the outcome as reducing the energy costs and expenses of detecting Covid-19 in X-ray images. Analysis of the results was done by comparing a CNN model with a DenseNet model, where the first achieved more accurate performance than the second.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110138677",
                        "name": "M. Rahman"
                    },
                    {
                        "authorId": "2110023850",
                        "name": "Mohammad Rabiul Islam"
                    },
                    {
                        "authorId": "2165437818",
                        "name": "Md. Anzir Hossain Rafath"
                    },
                    {
                        "authorId": "2219439517",
                        "name": "Simron Mhejabin"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "The following are examples of deep learning implementations for image classification, cell structure recognition, tissue segmentation, and computer-assisted disease diagnosis [11].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Those researches have mainly focused on Visually Rich Document(VRD)-based tasks, such as academic papers[18, 33], receipts[10, 17], and forms [11, 28], and many benchmark problems has been solved, including layout analysing [14, 33], table structure recognition [3, 31, 32], document question answering [16, 22]."
            ],
            "citingPaper": {
                "paperId": "e48e4cf40ae9cc1c7f68e36952f62440a6a8f7b9",
                "externalIds": {
                    "ArXiv": "2304.01577",
                    "DBLP": "conf/sigir/DingL0RLCH23",
                    "DOI": "10.1145/3539618.3591886",
                    "CorpusId": 257921755
                },
                "corpusId": 257921755,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/e48e4cf40ae9cc1c7f68e36952f62440a6a8f7b9",
                "title": "Form-NLU: Dataset for the Form Natural Language Understanding",
                "abstract": "Compared to general document analysis tasks, form document structure understanding and retrieval are challenging. Form documents are typically made by two types of authors; A form designer, who develops the form structure and keys, and a form user, who fills out form values based on the provided keys. Hence, the form values may not be aligned with the form designer's intention (structure and keys) if a form user gets confused. In this paper, we introduce Form-NLU, the first novel dataset for form structure understanding and its key and value information extraction, interpreting the form designer's intent and the alignment of user-written value on it. It consists of 857 form images, 6k form keys and values, and 4k table keys and values. Our dataset also includes three form types: digital, printed, and handwritten, which cover diverse form appearances and layouts. We propose a robust positional and logical relation-based form key-value information extraction framework. Using this dataset, Form-NLU, we first examine strong object detection models for the form layout understanding, then evaluate the key information extraction task on the dataset, providing fine-grained results for different types of forms and keys. Furthermore, we examine it with the off-the-shelf pdf layout extraction tool and prove its feasibility in real-world cases.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2166956722",
                        "name": "Yihao Ding"
                    },
                    {
                        "authorId": "32545338",
                        "name": "Siqu Long"
                    },
                    {
                        "authorId": "2213332565",
                        "name": "Jiabin Huang"
                    },
                    {
                        "authorId": "2212942973",
                        "name": "Kaixuan Ren"
                    },
                    {
                        "authorId": "2115606197",
                        "name": "Xingxian Luo"
                    },
                    {
                        "authorId": "3312958",
                        "name": "Hyunsuk Chung"
                    },
                    {
                        "authorId": "2046142",
                        "name": "S. Han"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Those researches have mainly focused on Visually Rich Document(VRD)-based tasks, such as academic papers[18, 33], receipts[10, 17], and forms [11, 28], and many benchmark problems has been solved, including layout analysing [14, 33], table structure recognition [3, 31, 32], document question answering [16, 22].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "This dataset contains document images from the FinTabNet dataset [11] and relevant questions about these document images."
            ],
            "citingPaper": {
                "paperId": "8f3299a306011582e21b07918e5f21ea7199b981",
                "externalIds": {
                    "ArXiv": "2303.14935",
                    "DBLP": "journals/corr/abs-2303-14935",
                    "DOI": "10.48550/arXiv.2303.14935",
                    "CorpusId": 257766584
                },
                "corpusId": 257766584,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8f3299a306011582e21b07918e5f21ea7199b981",
                "title": "TabIQA: Table Questions Answering on Business Document Images",
                "abstract": "Table answering questions from business documents has many challenges that require understanding tabular structures, cross-document referencing, and additional numeric computations beyond simple search queries. This paper introduces a novel pipeline, named TabIQA, to answer questions about business document images. TabIQA combines state-of-the-art deep learning techniques 1) to extract table content and structural information from images and 2) to answer various questions related to numerical data, text-based information, and complex queries from structured tables. The evaluation results on VQAonBD 2023 dataset demonstrate the effectiveness of TabIQA in achieving promising performance in answering table-related questions. The TabIQA repository is available at https://github.com/phucty/itabqa.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2100790",
                        "name": "Phuc Nguyen"
                    },
                    {
                        "authorId": "40325274",
                        "name": "N. Ly"
                    },
                    {
                        "authorId": "46966356",
                        "name": "Hideaki Takeda"
                    },
                    {
                        "authorId": "144752372",
                        "name": "A. Takasu"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "This dataset contains document images from the FinTabNet dataset [11] and relevant questions about these document images.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "3e0c97baa8e7b629a3d689e00aa21b94e2193766",
                "externalIds": {
                    "ArXiv": "2303.11615",
                    "DBLP": "journals/corr/abs-2303-11615",
                    "DOI": "10.48550/arXiv.2303.11615",
                    "CorpusId": 257636821
                },
                "corpusId": 257636821,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3e0c97baa8e7b629a3d689e00aa21b94e2193766",
                "title": "Robust Table Structure Recognition with Dynamic Queries Enhanced Detection Transformer",
                "abstract": "We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage dynamic queries enhanced DETR based separation line regression approach, named DQ-DETR, to predict separation lines from table images directly. Compared to Vallina DETR, we propose three improvements in DQ-DETR to make the two-stage DETR framework work efficiently and effectively for the separation line prediction task: 1) A new query design, named Dynamic Query, to decouple single line query into separable point queries which could intuitively improve the localization accuracy for regression tasks; 2) A dynamic queries based progressive line regression approach to progressively regressing points on the line which further enhances localization accuracy for distorted tables; 3) A prior-enhanced matching strategy to solve the slow convergence issue of DETR. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet, WTW and FinTabNet. Furthermore, we have validated the robustness and high localization accuracy of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2187553093",
                        "name": "Jiawei Wang"
                    },
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2181111465",
                        "name": "Mingze Li"
                    },
                    {
                        "authorId": "2145130513",
                        "name": "Zhengmao Sun"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "With the fast-paced development of digital transformation, Table Structure Recognition (TSR) task, aiming at parsing table structure from a table image into machineinterpretable formats, which are often presented by both table cell physical coordinates [38, 31, 15, 41, 4, 34, 36, 45, 35, 23, 25] and their logical relationships [18, 47].",
                "To date, several pioneers works [18, 47, 38, 15, 41, 4, 34, 36, 45, 35, 23, 25] have achieved significant progress in",
                "To attack it, a hierarchical GTE [45] employs clustering algorithm while Cycle-CenterNet [25] designs a cycle-pairing module that simultaneously detect table cells and group them into structured tables."
            ],
            "citingPaper": {
                "paperId": "2c267f43aca9bfb2f01e5400fca14545c4aa14b3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-09174",
                    "ArXiv": "2303.09174",
                    "DOI": "10.48550/arXiv.2303.09174",
                    "CorpusId": 257557431
                },
                "corpusId": 257557431,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c267f43aca9bfb2f01e5400fca14545c4aa14b3",
                "title": "Grab What You Need: Rethinking Complex Table Structure Recognition with Flexible Components Deliberation",
                "abstract": "Recently, Table Structure Recognition (TSR) task, aiming at identifying table structure into machine readable formats, has received increasing interest in the community. While impressive success, most single table component-based methods can not perform well on unregularized table cases distracted by not only complicated inner structure but also exterior capture distortion. In this paper, we raise it as Complex TSR problem, where the performance degeneration of existing methods is attributable to their inefficient component usage and redundant post-processing. To mitigate it, we shift our perspective from table component extraction towards the efficient multiple components leverage, which awaits further exploration in the field. Specifically, we propose a seminal method, termed GrabTab, equipped with newly proposed Component Deliberator. Thanks to its progressive deliberation mechanism, our GrabTab can flexibly accommodate to most complex tables with reasonable components selected but without complicated post-processing involved. Quantitative experimental results on public benchmarks demonstrate that our method significantly outperforms the state-of-the-arts, especially under more challenging scenes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216764712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2150155841",
                        "name": "Ming Gong"
                    },
                    {
                        "authorId": "47655556",
                        "name": "Bin Liu"
                    },
                    {
                        "authorId": "2164224877",
                        "name": "Yunfei Wu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "143900241",
                        "name": "Xing Sun"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "With the fast-paced development of digital transformation, Table Structure Recognition (TSR) task, aiming at parsing table structure from a table image into machineinterpretable formats, which are often presented by both table cell physical coordinates [38, 31, 15, 41, 4, 34, 36, 45, 35, 23, 25] and their logical relationships [18, 47].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "To date, several pioneers works [18, 47, 38, 15, 41, 4, 34, 36, 45, 35, 23, 25] have achieved significant progress in",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "To attack it, a hierarchical GTE [45] employs clustering algorithm while Cycle-CenterNet [25] designs a cycle-pairing module that simultaneously detect table cells and group them into structured tables.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bc8fc30c2546979cfd7e1658b868fdbb02f9a22a",
                "externalIds": {
                    "ArXiv": "2303.07641",
                    "DBLP": "journals/corr/abs-2303-07641",
                    "DOI": "10.5220/0011682600003411",
                    "CorpusId": 257356700
                },
                "corpusId": 257356700,
                "publicationVenue": {
                    "id": "8ef5945c-5b25-4774-b55a-15cd5450f6e4",
                    "name": "International Conference on Pattern Recognition Applications and Methods",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Pattern Recognit Appl Method",
                        "ICPRAM"
                    ],
                    "url": "http://icpram.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bc8fc30c2546979cfd7e1658b868fdbb02f9a22a",
                "title": "Rethinking Image-based Table Recognition Using Weakly Supervised Methods",
                "abstract": "Most of the previous methods for table recognition rely on training datasets containing many richly annotated table images. Detailed table image annotation, e.g., cell or text bounding box annotation, however, is costly and often subjective. In this paper, we propose a weakly supervised model named WSTabNet for table recognition that relies only on HTML (or LaTeX) code-level annotations of table images. The proposed model consists of three main parts: an encoder for feature extraction, a structure decoder for generating table structure, and a cell decoder for predicting the content of each cell in the table. Our system is trained end-to-end by stochastic gradient descent algorithms, requiring only table images and their ground-truth HTML (or LaTeX) representations. To facilitate table recognition with deep learning, we create and release WikiTableSet, the largest publicly available image-based table recognition dataset built from Wikipedia. WikiTableSet contains nearly 4 million English table images, 590K Japanese table images, and 640k French table images with corresponding HTML representation and cell bounding boxes. The extensive experiments on WikiTableSet and two large-scale datasets: FinTabNet and PubTabNet demonstrate that the proposed weakly supervised model achieves better, or similar accuracies compared to the state-of-the-art models on all benchmark datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40325274",
                        "name": "N. Ly"
                    },
                    {
                        "authorId": "144752372",
                        "name": "A. Takasu"
                    },
                    {
                        "authorId": "2100790",
                        "name": "Phuc Nguyen"
                    },
                    {
                        "authorId": "2052440572",
                        "name": "H. Takeda"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5966cd3ac4460a97b27b4177a89c0d1041726881",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-06949",
                    "ArXiv": "2303.06949",
                    "DOI": "10.1109/CVPR52729.2023.01071",
                    "CorpusId": 257495840
                },
                "corpusId": 257495840,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5966cd3ac4460a97b27b4177a89c0d1041726881",
                "title": "Improving Table Structure Recognition with Visual-Alignment Sequential Coordinate Modeling",
                "abstract": "Table structure recognition aims to extract the logical and physical structure of unstructured table images into a machine-readable format. The latest end-to-end image-to-text approaches simultaneously predict the two structures by two decoders, where the prediction of the physical structure (the bounding boxes of the cells) is based on the representation of the logical structure. However, the previous methods struggle with imprecise bounding boxes as the logical representation lacks local visual information. To address this issue, we propose an end-to-end sequential modeling framework for table structure recognition called VAST. It contains a novel coordinate sequence decoder triggered by the representation of the non-empty cell from the logical structure decoder. In the coordinate sequence decoder, we model the bounding box coordinates as a language sequence, where the left, top, right and bottom coordinates are decoded sequentially to leverage the inter-coordinate dependency. Furthermore, we propose an auxiliary visual-alignment loss to enforce the logical representation of the non-empty cells to contain more local visual details, which helps produce better cell bounding boxes. Extensive experiments demonstrate that our proposed method can achieve state-of-the-art results in both logical and physical structure recognition. The ablation study also validates that the proposed coordinate sequence decoder and the visual-alignment loss are the keys to the success of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145769099",
                        "name": "Yongshuai Huang"
                    },
                    {
                        "authorId": "2054645836",
                        "name": "Ning Lu"
                    },
                    {
                        "authorId": "1679279",
                        "name": "Dapeng Chen"
                    },
                    {
                        "authorId": "2110470261",
                        "name": "Yibo Li"
                    },
                    {
                        "authorId": "2002678",
                        "name": "Zecheng Xie"
                    },
                    {
                        "authorId": "2901597",
                        "name": "Shenggao Zhu"
                    },
                    {
                        "authorId": "1777642",
                        "name": "Liangcai Gao"
                    },
                    {
                        "authorId": "2187312795",
                        "name": "Wei Peng"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Limited by the training datasets [7, 2, 37, 36] used for TSR, most previous works [28, 24, 35, 26] focus on document images that are obtained from digital documents (e.",
                "Recently, FinTabNet [36] and SciTSR [2] add the cell coordinates and row-column information to become relatively comprehensive datasets for TSR."
            ],
            "citingPaper": {
                "paperId": "c78daabab3666d08d945098bc462f882b78803fd",
                "externalIds": {
                    "ArXiv": "2303.04384",
                    "DBLP": "journals/corr/abs-2303-04384",
                    "DOI": "10.48550/arXiv.2303.04384",
                    "CorpusId": 257405340
                },
                "corpusId": 257405340,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c78daabab3666d08d945098bc462f882b78803fd",
                "title": "SEMv2: Table Separation Line Detection Based on Conditional Convolution",
                "abstract": "Table structure recognition is an indispensable element for enabling machines to comprehend tables. Its primary purpose is to identify the internal structure of a table. Nevertheless, due to the complexity and diversity of their structure and style, it is highly challenging to parse the tabular data into a structured format that machines can comprehend. In this work, we adhere to the principle of the split-and-merge based methods and propose an accurate table structure recognizer, termed SEMv2 (SEM: Split, Embed and Merge). Unlike the previous works in the ``split'' stage, we aim to address the table separation line instance-level discrimination problem and introduce a table separation line detection strategy based on conditional convolution. Specifically, we design the ``split'' in a top-down manner that detects the table separation line instance first and then dynamically predicts the table separation line mask for each instance. The final table separation line shape can be accurately obtained by processing the table separation line mask in a row-wise/column-wise manner. To comprehensively evaluate the SEMv2, we also present a more challenging dataset for table structure recognition, dubbed iFLYTAB, which encompasses multiple style tables in various scenarios such as photos, scanned documents, etc. Extensive experiments on publicly available datasets (e.g. SciTSR, PubTabNet and iFLYTAB) demonstrate the efficacy of our proposed approach. The code and iFLYTAB dataset will be made publicly available upon acceptance of this paper.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2128660562",
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "authorId": "2067770685",
                        "name": "Pengfei Hu"
                    },
                    {
                        "authorId": "2143520841",
                        "name": "Jie Ma"
                    },
                    {
                        "authorId": "2149030934",
                        "name": "Jun Du"
                    },
                    {
                        "authorId": "39557762",
                        "name": "Jianshu Zhang"
                    },
                    {
                        "authorId": "2211065581",
                        "name": "Huihui Zhu"
                    },
                    {
                        "authorId": "2055464704",
                        "name": "Baocai Yin"
                    },
                    {
                        "authorId": "2185098372",
                        "name": "Bing Yin"
                    },
                    {
                        "authorId": "2108152462",
                        "name": "Cong Liu"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Limited by the training datasets [7, 2, 37, 36] used for TSR, most previous works [28, 24, 35, 26] focus on document images that are obtained from digital documents (e.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Recently, FinTabNet [36] and SciTSR [2] add the cell coordinates and row-column information to become relatively comprehensive datasets for TSR.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "When evaluating on TEDS, we use the non-styling text extracted from PDF files following Zheng et al. (2021)."
            ],
            "citingPaper": {
                "paperId": "a90a77893708ae5070e2ed4572ca9e20837e69c6",
                "externalIds": {
                    "DBLP": "conf/aaai/XingGLBZLYY23",
                    "ArXiv": "2303.03730",
                    "DOI": "10.48550/arXiv.2303.03730",
                    "CorpusId": 257378294
                },
                "corpusId": 257378294,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a90a77893708ae5070e2ed4572ca9e20837e69c6",
                "title": "LORE: Logical Location Regression Network for Table Structure Recognition",
                "abstract": "Table structure recognition (TSR) aims at extracting tables in images into machine-understandable formats. Recent methods solve this problem by predicting the adjacency relations of detected cell boxes, or learning to generate the corresponding markup sequences from the table images. However, they either count on additional heuristic rules to recover the table structures, or require a huge amount of training data and time-consuming sequential decoders. In this paper, we propose an alternative paradigm. We model TSR as a logical location regression problem and propose a new TSR framework called LORE, standing for LOgical location REgression network, which for the first time combines logical location regression together with spatial location regression of table cells. Our proposed LORE is conceptually simpler, easier to train and more accurate than previous TSR models of other paradigms. Experiments on standard benchmarks demonstrate that LORE consistently outperforms prior arts. Code is available at https:// github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LORE-TSR.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2176894025",
                        "name": "Hangdi Xing"
                    },
                    {
                        "authorId": "2112256",
                        "name": "Feiyu Gao"
                    },
                    {
                        "authorId": "143799988",
                        "name": "Rujiao Long"
                    },
                    {
                        "authorId": "2064698184",
                        "name": "Jiajun Bu"
                    },
                    {
                        "authorId": "2114172383",
                        "name": "Qi Zheng"
                    },
                    {
                        "authorId": "2145730944",
                        "name": "Liangcheng Li"
                    },
                    {
                        "authorId": "2173739231",
                        "name": "Cong Yao"
                    },
                    {
                        "authorId": "2139424603",
                        "name": "Zhi Yu"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "When evaluating on TEDS, we use the non-styling text extracted from PDF files following Zheng et al. (2021).",
                    "label_score": 0.5,
                    "label": "Weak"
                }
            ]
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "As our baseline training datasets we adopt PubTables-1M [21] and FinTabNet [25].",
                "These developments have enabled significant advances in deep learning (DL) modeling for TE [18,16,25,21,13,12].",
                "This includes the development of task-specific metrics for evaluating table structure recognition (TSR) models [5,26,20] as well as the increasing variety of datasets and benchmarks [3,25,26,21].",
                "To address the need for training data, several large-scale crowd-sourced datasets [3,26,11,25,21] have been released for training table structure recognition models."
            ],
            "citingPaper": {
                "paperId": "d943616a2b732c8b7cee18b44118db8d9a1af5e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-00716",
                    "ArXiv": "2303.00716",
                    "DOI": "10.48550/arXiv.2303.00716",
                    "CorpusId": 257255197
                },
                "corpusId": 257255197,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/d943616a2b732c8b7cee18b44118db8d9a1af5e8",
                "title": "Aligning benchmark datasets for table structure recognition",
                "abstract": "Benchmark datasets for table structure recognition (TSR) must be carefully processed to ensure they are annotated consistently. However, even if a dataset's annotations are self-consistent, there may be significant inconsistency across datasets, which can harm the performance of models trained and evaluated on them. In this work, we show that aligning these benchmarks$\\unicode{x2014}$removing both errors and inconsistency between them$\\unicode{x2014}$improves model performance significantly. We demonstrate this through a data-centric approach where we adopt one model architecture, the Table Transformer (TATR), that we hold fixed throughout. Baseline exact match accuracy for TATR evaluated on the ICDAR-2013 benchmark is 65% when trained on PubTables-1M, 42% when trained on FinTabNet, and 69% combined. After reducing annotation mistakes and inter-dataset inconsistency, performance of TATR evaluated on ICDAR-2013 increases substantially to 75% when trained on PubTables-1M, 65% when trained on FinTabNet, and 81% combined. We show through ablations over the modification steps that canonicalization of the table annotations has a significantly positive effect on performance, while other choices balance necessary trade-offs that arise when deciding a benchmark dataset's final composition. Overall we believe our work has significant implications for benchmark design for TSR and potentially other tasks as well. Dataset processing and training code will be released at https://github.com/microsoft/table-transformer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2128094499",
                        "name": "Robin Abraham"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "As our baseline training datasets we adopt PubTables-1M [21] and FinTabNet [25].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "These developments have enabled significant advances in deep learning (DL) modeling for TE [18,16,25,21,13,12].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "This includes the development of task-specific metrics for evaluating table structure recognition (TSR) models [5,26,20] as well as the increasing variety of datasets and benchmarks [3,25,26,21].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "To address the need for training data, several large-scale crowd-sourced datasets [3,26,11,25,21] have been released for training table structure recognition models.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4750b008727349147f4913a644f976168fa3e90b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-05658",
                    "ArXiv": "2302.05658",
                    "DOI": "10.48550/arXiv.2302.05658",
                    "CorpusId": 256827641
                },
                "corpusId": 256827641,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/4750b008727349147f4913a644f976168fa3e90b",
                "title": "DocILE Benchmark for Document Information Localization and Extraction",
                "abstract": "This paper introduces the DocILE benchmark with the largest dataset of business documents for the tasks of Key Information Localization and Extraction and Line Item Recognition. It contains 6.7k annotated business documents, 100k synthetically generated documents, and nearly~1M unlabeled documents for unsupervised pre-training. The dataset has been built with knowledge of domain- and task-specific aspects, resulting in the following key features: (i) annotations in 55 classes, which surpasses the granularity of previously published key information extraction datasets by a large margin; (ii) Line Item Recognition represents a highly practical information extraction task, where key information has to be assigned to items in a table; (iii) documents come from numerous layouts and the test set includes zero- and few-shot cases as well as layouts commonly seen in the training set. The benchmark comes with several baselines, including RoBERTa, LayoutLMv3 and DETR-based Table Transformer; applied to both tasks of the DocILE benchmark, with results shared in this paper, offering a quick starting point for future work. The dataset, baselines and supplementary material are available at https://github.com/rossumai/docile.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2203366733",
                        "name": "vStvep'an vSimsa"
                    },
                    {
                        "authorId": "2203364206",
                        "name": "Milan vSulc"
                    },
                    {
                        "authorId": "2205547051",
                        "name": "Michal Uvrivc'avr"
                    },
                    {
                        "authorId": "1699597533",
                        "name": "Yash J. Patel"
                    },
                    {
                        "authorId": "2924500",
                        "name": "Ahmed Hamdi"
                    },
                    {
                        "authorId": "2208709174",
                        "name": "Matvej Koci'an"
                    },
                    {
                        "authorId": "2203364204",
                        "name": "Maty'avs Skalick'y"
                    },
                    {
                        "authorId": "2000570582",
                        "name": "Jivr'i Matas"
                    },
                    {
                        "authorId": "2174737970",
                        "name": "Antoine Doucet"
                    },
                    {
                        "authorId": "1732746",
                        "name": "Micka\u00ebl Coustaty"
                    },
                    {
                        "authorId": "1694974",
                        "name": "Dimosthenis Karatzas"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "This track is derived from the task of Line Item Recognition (LIR) [26] and is related to Table Understanding [13] and Table Extraction [8, 35] \u2014 problems where the tabular structure is also crucial for IE."
            ],
            "citingPaper": {
                "paperId": "8b52355ddaa17e7d501f2326d524edaa1da41c60",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-12394",
                    "ArXiv": "2301.12394",
                    "DOI": "10.48550/arXiv.2301.12394",
                    "CorpusId": 256390134
                },
                "corpusId": 256390134,
                "publicationVenue": {
                    "id": "8ee71e17-421e-43db-ad2d-cc8af6217a0d",
                    "name": "European Conference on Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "ECIR",
                        "Eur Conf Inf Retr"
                    ],
                    "url": "https://en.wikipedia.org/wiki/European_Conference_on_Information_Retrieval"
                },
                "url": "https://www.semanticscholar.org/paper/8b52355ddaa17e7d501f2326d524edaa1da41c60",
                "title": "DocILE 2023 Teaser: Document Information Localization and Extraction",
                "abstract": "The lack of data for information extraction (IE) from semi-structured business documents is a real problem for the IE community. Publications relying on large-scale datasets use only proprietary, unpublished data due to the sensitive nature of such documents. Publicly available datasets are mostly small and domain-specific. The absence of a large-scale public dataset or benchmark hinders the reproducibility and cross-evaluation of published methods. The DocILE 2023 competition, hosted as a lab at the CLEF 2023 conference and as an ICDAR 2023 competition, will run the first major benchmark for the tasks of Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR) from business documents. With thousands of annotated real documents from open sources, a hundred thousand of generated synthetic documents, and nearly a million unlabeled documents, the DocILE lab comes with the largest publicly available dataset for KILE and LIR. We are looking forward to contributions from the Computer Vision, Natural Language Processing, Information Retrieval, and other communities. The data, baselines, code and up-to-date information about the lab and competition are available at https://docile.rossum.ai/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2203366733",
                        "name": "vStvep'an vSimsa"
                    },
                    {
                        "authorId": "2203364206",
                        "name": "Milan vSulc"
                    },
                    {
                        "authorId": "2203364204",
                        "name": "Maty'avs Skalick'y"
                    },
                    {
                        "authorId": "1699597533",
                        "name": "Yash J. Patel"
                    },
                    {
                        "authorId": "2924500",
                        "name": "Ahmed Hamdi"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "This track is derived from the task of Line Item Recognition (LIR) [26] and is related to Table Understanding [13] and Table Extraction [8, 35] \u2014 problems where the tabular structure is also crucial for IE.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "FinQA FinQA is a dataset of numerical reasoning over long-form financial data, containing 8, 281 financial reports, along with their QA pairs and annotated numerical reasoning processes by eleven finance professionals based on the earnings reports of S&p 500 companies (Zheng et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "efa92d27065501981f2ade15c1cd884fdf644f44",
                "externalIds": {
                    "ArXiv": "2212.07249",
                    "DBLP": "journals/corr/abs-2212-07249",
                    "DOI": "10.48550/arXiv.2212.07249",
                    "CorpusId": 254636560
                },
                "corpusId": 254636560,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/efa92d27065501981f2ade15c1cd884fdf644f44",
                "title": "APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning",
                "abstract": "Long-form numerical reasoning in financial analysis aims to generate a reasoning program to calculate the correct answer for a given question. Previous work followed a retriever-generator framework, where the retriever selects key facts from a long-form document, and the generator generates a reasoning program based on retrieved facts. However, they treated all facts equally without considering the different contributions of facts with and without numbers. Meanwhile, the program consistency were ignored under supervised training, resulting in lower training accuracy and diversity. To solve these problems, we proposed APOLLO to improve the long-form numerical reasoning framework. For the retriever, we adopt a number-aware negative sampling strategy to enable the retriever to be more discriminative on key numerical facts. For the generator, we design consistency-based reinforcement learning and target program augmentation strategy based on the consistency of program execution results. Experimental results on the FinQA and ConvFinQA leaderboard verify the effectiveness of our proposed method, achieving the new state-of-the-art.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2125069561",
                        "name": "Jia Sun"
                    },
                    {
                        "authorId": "2119077859",
                        "name": "Hang Zhang"
                    },
                    {
                        "authorId": "2186278677",
                        "name": "Chen Lin"
                    },
                    {
                        "authorId": "2171182",
                        "name": "Yeyun Gong"
                    },
                    {
                        "authorId": "2188226506",
                        "name": "Jian Guo"
                    },
                    {
                        "authorId": "46429989",
                        "name": "Nan Duan"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "FinQA FinQA is a dataset of numerical reasoning over long-form financial data, containing 8, 281 financial reports, along with their QA pairs and annotated numerical reasoning processes by eleven finance professionals based on the earnings reports of S&p 500 companies (Zheng et al., 2021).",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ae6e34d182fbc42bc99ba609369a3b7645d205cb",
                "externalIds": {
                    "DBLP": "journals/prl/WangXZJ23",
                    "DOI": "10.1016/j.patrec.2022.12.014",
                    "CorpusId": 255089449
                },
                "corpusId": 255089449,
                "publicationVenue": {
                    "id": "f35e3e87-9df4-497b-aa0d-bb8584197290",
                    "name": "Pattern Recognition Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit Lett"
                    ],
                    "issn": "0167-8655",
                    "url": "https://www.journals.elsevier.com/pattern-recognition-letters/",
                    "alternate_urls": [
                        "http://www.journals.elsevier.com/pattern-recognition-letters/",
                        "http://www.sciencedirect.com/science/journal/01678655"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ae6e34d182fbc42bc99ba609369a3b7645d205cb",
                "title": "Scene table structure recognition with segmentation collaboration and alignment",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109799388",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "46364544",
                        "name": "Yang Xue"
                    },
                    {
                        "authorId": "2118131879",
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "authorId": "144838978",
                        "name": "Lianwen Jin"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d08d5be4d81ba1be311e5711c4aa875e1ded2e6c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-15504",
                    "ArXiv": "2211.15504",
                    "DOI": "10.48550/arXiv.2211.15504",
                    "CorpusId": 254043986
                },
                "corpusId": 254043986,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d08d5be4d81ba1be311e5711c4aa875e1ded2e6c",
                "title": "Semantic Table Detection with LayoutLMv3",
                "abstract": "This paper presents an application of the LayoutLMv3 model for semantic table detection on financial documents from the IIIT-AR-13K dataset. The motivation behind this paper's experiment was that LayoutLMv3's official paper had no results for table detection using semantic information. We concluded that our approach did not improve the model's table detection capabilities, for which we can give several possible reasons. Either the model's weights were unsuitable for our purpose, or we needed to invest more time in optimising the model's hyperparameters. It is also possible that semantic information does not improve a model's table detection accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192604930",
                        "name": "Ivan Silajev"
                    },
                    {
                        "authorId": "2192604784",
                        "name": "Niels Victor"
                    },
                    {
                        "authorId": "2192604317",
                        "name": "Phillip Mortimer"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "In DTE, there are three main cases: TD predicts bounding boxes of table candidates on document images (Shafait and Smith, 2010; Schreiber et al., 2017; Siddiqui et al., 2018; Paliwal et al., 2019; Prasad et al., 2020; Hashmi et al., 2021b; Zheng et al., 2021); TD searches for text lines signaling table candidates in plain-text (Hu et al.",
                "\u2026images (Shafait and Smith, 2010; Schreiber et al., 2017; Siddiqui et al., 2018; Paliwal et al., 2019; Prasad et al., 2020; Hashmi et al., 2021b; Zheng et al., 2021); TD searches for text lines signaling table candidates in plain-text (Hu et al., 1999; Ng et al., 1999; Pinto et al., 2003); TD\u2026",
                "In this scenario, TSR usually deals with tables which are subareas of document images (Zuyev, 1997; Schreiber et al., 2017; Siddiqui et al., 2019a, 2019b; Zheng et al., 2021; Hashmi et al., 2021c), lines of plain-text (Kieninger, 1998; Ng et al.",
                "\u2026scenario, TSR usually deals with tables which are subareas of document images (Zuyev, 1997; Schreiber et al., 2017; Siddiqui et al., 2019a, 2019b; Zheng et al., 2021; Hashmi et al., 2021c), lines of plain-text (Kieninger, 1998; Ng et al., 1999; Hu et al., 2000), or text chunks and rulings\u2026"
            ],
            "citingPaper": {
                "paperId": "cf5f42b60003f9fc11d14a6eb126130411f861a4",
                "externalIds": {
                    "DBLP": "journals/widm/Shigarov23",
                    "DOI": "10.1002/widm.1482",
                    "CorpusId": 253823145
                },
                "corpusId": 253823145,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cf5f42b60003f9fc11d14a6eb126130411f861a4",
                "title": "Table understanding: Problem overview",
                "abstract": "Tables are probably the most natural way to represent relational data in various media and formats. They store a large number of valuable facts that could be utilized for question answering, knowledge base population, natural language generation, and other applications. However, many tables are not accompanied by semantics for the automatic interpretation of the information they present. Table Understanding (TU) aims at recovering the missing semantics that enables the extraction of facts from tables. This problem covers a range of issues from table detection in document images to semantic table interpretation with the help of external knowledge bases. To date, the TU research has been ongoing on for 30\u2009years. Nevertheless, there is no common point of view on the scope of TU; the terminology still needs agreement and unification. In recent years, science and technology have shown a rapidly increasing interest in TU. Nowadays, it is especially important to check the meaning of this research problem once again. This article gives a comprehensive characterization of the TU problem, including a description of its subproblems, tasks, subtasks, and applications. It also discusses the common limitations used in the existing problem statements and proposes some directions for further research that would help overcome the corresponding limitations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3355714",
                        "name": "A. Shigarov"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "In DTE, there are three main cases: TD predicts bounding boxes of table candidates on document images (Shafait and Smith, 2010; Schreiber et al., 2017; Siddiqui et al., 2018; Paliwal et al., 2019; Prasad et al., 2020; Hashmi et al., 2021b; Zheng et al., 2021); TD searches for text lines signaling table candidates in plain-text (Hu et al.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "\u2026images (Shafait and Smith, 2010; Schreiber et al., 2017; Siddiqui et al., 2018; Paliwal et al., 2019; Prasad et al., 2020; Hashmi et al., 2021b; Zheng et al., 2021); TD searches for text lines signaling table candidates in plain-text (Hu et al., 1999; Ng et al., 1999; Pinto et al., 2003); TD\u2026",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "In this scenario, TSR usually deals with tables which are subareas of document images (Zuyev, 1997; Schreiber et al., 2017; Siddiqui et al., 2019a, 2019b; Zheng et al., 2021; Hashmi et al., 2021c), lines of plain-text (Kieninger, 1998; Ng et al.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "\u2026scenario, TSR usually deals with tables which are subareas of document images (Zuyev, 1997; Schreiber et al., 2017; Siddiqui et al., 2019a, 2019b; Zheng et al., 2021; Hashmi et al., 2021c), lines of plain-text (Kieninger, 1998; Ng et al., 1999; Hu et al., 2000), or text chunks and rulings\u2026",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": true,
            "contexts": [
                "The most common words from PubTabNet and FinTabNet as well as randomly produced text make up the corpora used to create the table content.",
                "FinTabNet [65] introduces GTE, a vision-guided systematic framework for combined table detection and cell structured identification that can be constructed on top of any object detection model.",
                "FinTabNet FinTabNet [65] introduces GTE, a vision-guided systematic framework for combined table detection and cell structured identification that can be constructed on top of any object detection model.",
                "FinTabNet is a collection of real-world and complicated scientific and financial datasets with thorough table structure annotations to aid in structure identification training and testing."
            ],
            "citingPaper": {
                "paperId": "a81f504a1a101b9c6fe4a88957dd9ec450cf389d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-08469",
                    "ArXiv": "2211.08469",
                    "DOI": "10.48550/arXiv.2211.08469",
                    "CorpusId": 253553399
                },
                "corpusId": 253553399,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a81f504a1a101b9c6fe4a88957dd9ec450cf389d",
                "title": "Deep learning for table detection and structure recognition: A survey",
                "abstract": "Tables are everywhere, from scientific journals, papers, websites, and newspapers all the way to items we buy at the supermarket. Detecting them is thus of utmost importance to automatically understanding the content of a document. The performance of table detection has substantially increased thanks to the rapid development of deep learning networks. The goals of this survey are to provide a profound comprehension of the major developments in the field of Table Detection, offer insight into the different methodologies, and provide a systematic taxonomy of the different approaches. Furthermore, we provide an analysis of both classic and new applications in the field. Lastly, the datasets and source code of the existing models are organized to provide the reader with a compass on this vast literature. Finally, we go over the architecture of utilizing various object detection and table structure recognition methods to create an effective and efficient system, as well as a set of development trends to keep up with state-of-the-art algorithms and future research. We have also set up a public GitHub repository where we will be updating the most recent publications, open data, and source code. The GitHub repository is available at https://github.com/abdoelsayed2016/ table-detection-structure-recognition. Preprint submitted to Elsevier November 17, 2022 ar X iv :2 21 1. 08 46 9v 1 [ cs .C V ] 1 5 N ov 2 02 2",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "87034581",
                        "name": "M. Kasem"
                    },
                    {
                        "authorId": "115665360",
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "authorId": "2115948777",
                        "name": "Alexander Berendeyev"
                    },
                    {
                        "authorId": "2190955162",
                        "name": "Ebrahem Elkady"
                    },
                    {
                        "authorId": "2190955581",
                        "name": "Mahmoud Abdalla"
                    },
                    {
                        "authorId": "2163883464",
                        "name": "Mohamed Mahmoud"
                    },
                    {
                        "authorId": "2057870328",
                        "name": "Mohamed Hamada"
                    },
                    {
                        "authorId": "3253700",
                        "name": "D. Nurseitov"
                    },
                    {
                        "authorId": "1405340824",
                        "name": "I. Taj-Eddin"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "The most common words from PubTabNet and FinTabNet as well as randomly produced text make up the corpora used to create the table content.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "FinTabNet [65] introduces GTE, a vision-guided systematic framework for combined table detection and cell structured identification that can be constructed on top of any object detection model.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "FinTabNet FinTabNet [65] introduces GTE, a vision-guided systematic framework for combined table detection and cell structured identification that can be constructed on top of any object detection model.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "FinTabNet is a collection of real-world and complicated scientific and financial datasets with thorough table structure annotations to aid in structure identification training and testing.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e35ef6f4cfdc2319a641735608562d3886fa9fdd",
                "externalIds": {
                    "DOI": "10.1109/ICCCIS56430.2022.10037664",
                    "CorpusId": 256743427
                },
                "corpusId": 256743427,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e35ef6f4cfdc2319a641735608562d3886fa9fdd",
                "title": "Using CoordConv for Tabular Data Detection and Structure Recognition",
                "abstract": "This paper explores the usage of CoordConv, the novel upgrade to general convolutional layers in the problem of Tabular Data Detection and Cell-Based Structure Recognition. CoordConv has been shown to provide considerably better results in the domain of Object Detection than its counterpart. The authors integrate it within the established Anchor optimization approach which leverages guided anchors to accomplish the task of recognizing rows and columns present in tabular data. In contrast to the majority of techniques implemented for Table Structure Recognition, the authors attempt to recognize the cells present in the tabular images instead of the rows and columns. They evaluate this method on the coveted ICDAR-19 dataset (International Conference on Document Analysis and Recognition - 2019) which comprises of scanned document images containing tabular regions and achieve results surpassing those of many popular techniques. They also apply this approach for the task of Table Detection and achieve results comparable to other established techniques.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2205330992",
                        "name": "Apoorva Ambulgekar"
                    },
                    {
                        "authorId": "2205330976",
                        "name": "Naman Lad"
                    },
                    {
                        "authorId": "2183415732",
                        "name": "Krunal Doshi"
                    },
                    {
                        "authorId": "2128946657",
                        "name": "Pranit Bari"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "2021), (Zheng et al. 2021)] have proposed a modified TEDS metric named TEDS-Struct to evaluate table structure recognition accuracy only by ignoring OCR errors.",
                "Some recent works [(Raja, Mondal, and Jawahar 2020), (Qiao et al. 2021), (Zheng et al. 2021)] have proposed a modified TEDS metric named TEDS-Struct to\nevaluate table structure recognition accuracy only by ignoring OCR errors."
            ],
            "citingPaper": {
                "paperId": "0841e71068af40b77892a69378b45e0e1adf6aee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-05391",
                    "ArXiv": "2210.05391",
                    "DOI": "10.48550/arXiv.2210.05391",
                    "CorpusId": 252816075
                },
                "corpusId": 252816075,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0841e71068af40b77892a69378b45e0e1adf6aee",
                "title": "PP-StructureV2: A Stronger Document Analysis System",
                "abstract": "A large amount of document data exists in unstructured form such as raw images without any text information. Designing a practical document image analysis system is a meaningful but challenging task. In previous work, we proposed an intelligent document analysis system PP-Structure. In order to further upgrade the function and performance of PP-Structure, we propose PP-StructureV2 in this work, which contains two subsystems: Layout Information Extraction and Key Information Extraction. Firstly, we integrate Image Direction Correction module and Layout Restoration module to enhance the functionality of the system. Secondly, 8 practical strategies are utilized in PP-StructureV2 for better performance. For Layout Analysis model, we introduce ultra light-weight detector PP-PicoDet and knowledge distillation algorithm FGD for model lightweighting, which increased the inference speed by 11 times with comparable mAP. For Table Recognition model, we utilize PP-LCNet, CSP-PAN and SLAHead to optimize the backbone module, feature fusion module and decoding module, respectively, which improved the table structure accuracy by 6\\% with comparable inference speed. For Key Information Extraction model, we introduce VI-LayoutXLM which is a visual-feature independent LayoutXLM architecture, TB-YX sorting algorithm and U-DML knowledge distillation algorithm, which brought 2.8\\% and 9.1\\% improvement respectively on the Hmean of Semantic Entity Recognition and Relation Extraction tasks. All the above mentioned models and code are open-sourced in the GitHub repository PaddleOCR.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109266729",
                        "name": "Chenxia Li"
                    },
                    {
                        "authorId": "108703727",
                        "name": "Ruoyu Guo"
                    },
                    {
                        "authorId": "2151548966",
                        "name": "Jun Zhou"
                    },
                    {
                        "authorId": "2187431479",
                        "name": "Mengtao An"
                    },
                    {
                        "authorId": "2867809",
                        "name": "Yuning Du"
                    },
                    {
                        "authorId": "2118940194",
                        "name": "Lingfeng Zhu"
                    },
                    {
                        "authorId": "2153629743",
                        "name": "Yi Liu"
                    },
                    {
                        "authorId": "2109752359",
                        "name": "Xiaoguang Hu"
                    },
                    {
                        "authorId": "3046102",
                        "name": "Dianhai Yu"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "2021), (Zheng et al. 2021)] have proposed a modified TEDS metric named TEDS-Struct to evaluate table structure recognition accuracy only by ignoring OCR errors.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Some recent works [(Raja, Mondal, and Jawahar 2020), (Qiao et al. 2021), (Zheng et al. 2021)] have proposed a modified TEDS metric named TEDS-Struct to\nevaluate table structure recognition accuracy only by ignoring OCR errors.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4ebf44ebb9634cb5c60f5dc7fab0c0f84aed0939",
                "externalIds": {
                    "DBLP": "conf/mm/LiL0LCNPL22",
                    "DOI": "10.1145/3503161.3547885",
                    "CorpusId": 252782335
                },
                "corpusId": 252782335,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4ebf44ebb9634cb5c60f5dc7fab0c0f84aed0939",
                "title": "End-to-End Compound Table Understanding with Multi-Modal Modeling",
                "abstract": "Table is a widely used data form in webpages, spreadsheets, or PDFs to organize and present structural data. Although studies on table structure recognition have been successfully used to convert image-based tables into digital structural formats, solving many real problems still relies on further understanding of the table, such as cell relationship extraction. The current datasets related to table understanding are all based on the digit format. To boost research development, we release a new benchmark named ComFinTab with rich annotations that support both table recognition and understanding tasks. Unlike previous datasets containing the basic tables, ComFinTab contains a large ratio of compound tables, which is much more challenging and requires methods using multiple information sources. Based on the dataset, we also propose a uniform, concise task form with the evaluation metric to better evaluate the model's performance on the table understanding task in compound tables. Finally, a framework named CTUNet is proposed to integrate the compromised visual, semantic, and position features with a graph attention network, which can solve the table recognition task and the challenging table understanding task as a whole. Experimental results compared with some previous advanced table understanding methods demonstrate the effectiveness of our proposed model. Code and dataset are available at \\urlhttps://github.com/hikopensource/DAVAR-Lab-OCR.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109614687",
                        "name": "Zaisheng Li"
                    },
                    {
                        "authorId": "2187429408",
                        "name": "Yi Li"
                    },
                    {
                        "authorId": "2187308758",
                        "name": "Qiao Liang"
                    },
                    {
                        "authorId": "2144902872",
                        "name": "Pengfei Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    },
                    {
                        "authorId": "1490934795",
                        "name": "Yi Niu"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    },
                    {
                        "authorId": "2116227961",
                        "name": "Xi Li"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2022 FinTabNet [67] We employ FinTabNet to increase samples\u2019 diversity."
            ],
            "citingPaper": {
                "paperId": "c18b3479a389ae4bc955338a862e6bc4f68d6ac1",
                "externalIds": {
                    "DOI": "10.3390/app12188969",
                    "CorpusId": 252171274
                },
                "corpusId": 252171274,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c18b3479a389ae4bc955338a862e6bc4f68d6ac1",
                "title": "Continual Learning for Table Detection in Document Images",
                "abstract": "The growing amount of data demands methods that can gradually learn from new samples. However, it is not trivial to continually train a network. Retraining a network with new data usually results in a phenomenon called \u201ccatastrophic forgetting\u201d. In a nutshell, the performance of the model on the previous data drops by learning from the new instances. This paper explores this issue in the table detection problem. While there are multiple datasets and sophisticated methods for table detection, the utilization of continual learning techniques in this domain has not been studied. We employed an effective technique called experience replay and performed extensive experiments on several datasets to investigate the effects of catastrophic forgetting. The results show that our proposed approach mitigates the performance drop by 15 percent. To the best of our knowledge, this is the first time that continual learning techniques have been adopted for table detection, and we hope this stands as a baseline for future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1752929623",
                        "name": "Mohammad Minouei"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "2757942",
                        "name": "M. Soheili"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "\u2022 FinTabNet [67] We employ FinTabNet to increase samples\u2019 diversity.",
                    "label_score": 0.5,
                    "label": "Weak"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Additionally, it is benchmarked with a number of state-of-the-art techniques such as SPLERGE [10], TabStruct-Net [18], EDD [3], GTE [28], LGPMA [9], FLAG-Net [29], etc. Unlike many state-of-the-art methods that perform evaluations only at TEDs, our method also test the Structure TEDs on PubTabNet.",
                "Additionally, it is benchmarked with a number of state-of-the-art techniques such as SPLERGE [10], TabStruct-Net [18], EDD [3], GTE [28], LGPMA [9], FLAG-Net [29], etc."
            ],
            "citingPaper": {
                "paperId": "92132fb36fb3e470464551210926f256a1f37280",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-14687",
                    "ArXiv": "2208.14687",
                    "DOI": "10.48550/arXiv.2208.14687",
                    "CorpusId": 251953555
                },
                "corpusId": 251953555,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/92132fb36fb3e470464551210926f256a1f37280",
                "title": "TRUST: An Accurate and End-to-End Table structure Recognizer Using Splitting-based Transformers",
                "abstract": "Table structure recognition is a crucial part of document image analysis domain. Its difficulty lies in the need to parse the physical coordinates and logical indices of each cell at the same time. However, the existing methods are difficult to achieve both these goals, especially when the table splitting lines are blurred or tilted. In this paper, we propose an accurate and end-to-end transformer-based table structure recognition method, referred to as TRUST. Transformers are suitable for table structure recognition because of their global computations, perfect memory, and parallel computation. By introducing novel Transformer-based Query-based Splitting Module and Vertex-based Merging Module, the table structure recognition problem is decoupled into two joint optimization sub-tasks: multi-oriented table row/column splitting and table grid merging. The Query-based Splitting Module learns strong context information from long dependencies via Transformer networks, accurately predicts the multi-oriented table row/column separators, and obtains the basic grids of the table accordingly. The Vertex-based Merging Module is capable of aggregating local contextual information between adjacent basic grids, providing the ability to merge basic girds that belong to the same spanning cell accurately. We conduct experiments on several popular benchmarks including PubTabNet and SynthTable, our method achieves new state-of-the-art results. In particular, TRUST runs at 10 FPS on PubTabNet, surpassing the previous methods by a large margin.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109309324",
                        "name": "Zengyuan Guo"
                    },
                    {
                        "authorId": "2117164666",
                        "name": "Yuecheng Yu"
                    },
                    {
                        "authorId": "25604699",
                        "name": "Pengyuan Lv"
                    },
                    {
                        "authorId": "1979323",
                        "name": "Chengquan Zhang"
                    },
                    {
                        "authorId": "2579920",
                        "name": "Haojie Li"
                    },
                    {
                        "authorId": "47196393",
                        "name": "Zhihui Wang"
                    },
                    {
                        "authorId": "2140371004",
                        "name": "Kun Yao"
                    },
                    {
                        "authorId": "2272123",
                        "name": "Jingtuo Liu"
                    },
                    {
                        "authorId": "2109534192",
                        "name": "Jingdong Wang"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Additionally, it is benchmarked with a number of state-of-the-art techniques such as SPLERGE [10], TabStruct-Net [18], EDD [3], GTE [28], LGPMA [9], FLAG-Net [29], etc. Unlike many state-of-the-art methods that perform evaluations only at TEDs, our method also test the Structure TEDs on PubTabNet.",
                    "label_score": 0.5,
                    "label": "Weak"
                },
                {
                    "context": "Additionally, it is benchmarked with a number of state-of-the-art techniques such as SPLERGE [10], TabStruct-Net [18], EDD [3], GTE [28], LGPMA [9], FLAG-Net [29], etc.",
                    "label_score": 0.5,
                    "label": "Weak"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fb6fcca5762b26f225313f86b1f33a1cf198bfd7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-04921",
                    "ArXiv": "2208.04921",
                    "DOI": "10.1145/3503161.3548038",
                    "CorpusId": 251442367
                },
                "corpusId": 251442367,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fb6fcca5762b26f225313f86b1f33a1cf198bfd7",
                "title": "TSRFormer: Table Structure Recognition with Transformers",
                "abstract": "We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage DETR based separator prediction approach, dubbed Sep arator RE gression TR ansformer (SepRETR), to predict separation lines from table images directly. To make the two-stage DETR framework work efficiently and effectively for the separation line prediction task, we propose two improvements: 1) A prior-enhanced matching strategy to solve the slow convergence issue of DETR; 2) A new cross attention module to sample features from a high-resolution convolutional feature map directly so that high localization accuracy is achieved with low computational cost. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet and WTW. Furthermore, we have validated the robustness of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2145130513",
                        "name": "Zhengmao Sun"
                    },
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2181111465",
                        "name": "Mingze Li"
                    },
                    {
                        "authorId": "2110132415",
                        "name": "Jiawei Wang"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "(2021) [89] Object Detector PDF Table Boundary Detection Fails to detect cell structures +NN Fails to return text",
                "[89] introduced a Global table extractor (GTE) model based on object detector techniques, which themselves are based on neural networks",
                "We unfortunately could not directly evaluate the approaches presented in [5, 25, 30, 41, 47, 89] using our cybersecurity corpus documents because their respective implementations were not available online.",
                "There are three approaches in the literature to handle table detection in documents: conventional rule-based [30, 87], metadata extraction [6, 31, 57] and machine learning and deep learning approaches [5, 25, 41, 47, 89].",
                "Research models trained on reference datasets such as [5, 25, 30, 41, 47, 89] often have di culties coping with the complexity of real world document layouts [15].",
                "The analysis of the layout of documents has been used by numerous researchers to develop techniques for detecting tables, layouts, and sections [5, 6, 18, 22, 23, 25, 27, 30, 34, 77, 89]."
            ],
            "citingPaper": {
                "paperId": "87cb212eeb40171b32a4411a6919e22c36822140",
                "externalIds": {
                    "DBLP": "journals/tmis/AmeriHSLP23",
                    "DOI": "10.1145/3546580",
                    "CorpusId": 250496865
                },
                "corpusId": 250496865,
                "publicationVenue": {
                    "id": "a0b9dfee-fbf4-4bea-835f-20d6db1ce53a",
                    "name": "ACM Transactions on Management Information Systems",
                    "alternate_names": [
                        "ACM Trans Manag Inf Syst"
                    ],
                    "issn": "2158-656X",
                    "url": "http://dl.acm.org/citation.cfm?CFID=72370079&CFTOKEN=39904203&id=J1320",
                    "alternate_urls": [
                        "http://portal.acm.org/tmis",
                        "http://portal.acm.org/citation.cfm?id=J1320&picked=prox",
                        "https://tmis.acm.org/",
                        "http://tmis.acm.org/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/87cb212eeb40171b32a4411a6919e22c36822140",
                "title": "Design of a Novel Information System for Semi-automated Management of Cybersecurity in Industrial Control Systems",
                "abstract": "There is an urgent need in many critical infrastructure sectors, including the energy sector, for attaining detailed insights into cybersecurity features and compliance with cybersecurity requirements related to their Operational Technology (OT) deployments. Frequent feature changes of OT devices interfere with this need, posing a great risk to customers. One effective way to address this challenge is via a semi-automated cyber-physical security assurance approach, which enables verification and validation of the OT device cybersecurity claims against actual capabilities, both pre- and post-deployment. To realize this approach, this article presents new methodology and algorithms to automatically identify cybersecurity-related claims expressed in natural language form in ICS device documents. We developed an identification process that employs natural language processing (NLP) techniques with the goal of semi-automated vetting of detected claims against their device implementation. We also present our novel NLP components for verifying feature claims against relevant cybersecurity requirements. The verification pipeline includes components such as automated vendor identification, device document curation, feature claim identification utilizing sentiment analysis for conflict resolution, and reporting of features that are claimed to be supported or indicated as unsupported. Our novel matching engine represents the first automated information system available in the cybersecurity domain that directly aids the generation of ICS compliance reports.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "134238540",
                        "name": "Kimia Ameri"
                    },
                    {
                        "authorId": "35380390",
                        "name": "M. Hempel"
                    },
                    {
                        "authorId": "145505074",
                        "name": "H. Sharif"
                    },
                    {
                        "authorId": "2150338498",
                        "name": "Juan Lopez"
                    },
                    {
                        "authorId": "1750770",
                        "name": "K. Perumalla"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "(2021) [89] Object Detector PDF Table Boundary Detection Fails to detect cell structures +NN Fails to return text",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "[89] introduced a Global table extractor (GTE) model based on object detector techniques, which themselves are based on neural networks",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We unfortunately could not directly evaluate the approaches presented in [5, 25, 30, 41, 47, 89] using our cybersecurity corpus documents because their respective implementations were not available online.",
                    "label_score": -1.0,
                    "label": "P-NR"
                },
                {
                    "context": "There are three approaches in the literature to handle table detection in documents: conventional rule-based [30, 87], metadata extraction [6, 31, 57] and machine learning and deep learning approaches [5, 25, 41, 47, 89].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Research models trained on reference datasets such as [5, 25, 30, 41, 47, 89] often have di culties coping with the complexity of real world document layouts [15].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "The analysis of the layout of documents has been used by numerous researchers to develop techniques for detecting tables, layouts, and sections [5, 6, 18, 22, 23, 25, 27, 30, 34, 77, 89].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "644b991959bf5fb036d28d67e9918d419b212e75",
                "externalIds": {
                    "DBLP": "journals/pr/LiYDL22",
                    "DOI": "10.1016/j.patcog.2022.108946",
                    "CorpusId": 251140530
                },
                "corpusId": 251140530,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/644b991959bf5fb036d28d67e9918d419b212e75",
                "title": "Table Structure Recognition and Form Parsing by End-to-End Object Detection and Relation Parsing",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118890077",
                        "name": "Xiaohui Li"
                    },
                    {
                        "authorId": "145820427",
                        "name": "Fei Yin"
                    },
                    {
                        "authorId": "2179477671",
                        "name": "He-Sen Dai"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "1aa8da3bbd2bca7071fae029d6fa8ea851f805b7",
                "externalIds": {
                    "DBLP": "conf/clef/SkalickySUS22",
                    "ArXiv": "2206.11229",
                    "DOI": "10.48550/arXiv.2206.11229",
                    "CorpusId": 249926391
                },
                "corpusId": 249926391,
                "publicationVenue": {
                    "id": "ab453bce-d4ec-48ec-ad78-ef19dc9333ab",
                    "name": "Conference and Labs of the Evaluation Forum",
                    "type": "conference",
                    "alternate_names": [
                        "CLEF",
                        "Conf Lab Evaluation Forum",
                        "Cross-language Evaluation Forum",
                        "Cross-Language Evaluation Forum"
                    ],
                    "url": "http://www.clef-initiative.eu/"
                },
                "url": "https://www.semanticscholar.org/paper/1aa8da3bbd2bca7071fae029d6fa8ea851f805b7",
                "title": "Business Document Information Extraction: Towards Practical Benchmarks",
                "abstract": "Information extraction from semi-structured documents is crucial for frictionless business-to-business (B2B) communication. While machine learning problems related to Document Information Extraction (IE) have been studied for decades, many common problem definitions and benchmarks do not reflect domain-specific aspects and practical needs for automating B2B document communication. We review the landscape of Document IE problems, datasets and benchmarks. We highlight the practical aspects missing in the common definitions and define the Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR) problems. There is a lack of relevant datasets and benchmarks for Document IE on semi-structured business documents as their content is typically legally protected or sensitive. We discuss potential sources of available documents including synthetic data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071515342",
                        "name": "Maty\u00e1s Skalick\u00fd"
                    },
                    {
                        "authorId": "46369981",
                        "name": "Step\u00e1n Simsa"
                    },
                    {
                        "authorId": "3406363",
                        "name": "Michal U\u0159i\u010d\u00e1\u0159"
                    },
                    {
                        "authorId": "2052167",
                        "name": "Milan \u0160ulc"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "55976ee561064f301fcfce26356a6da9e062494c",
                "externalIds": {
                    "DOI": "10.3390/jcp2020022",
                    "CorpusId": 249812259
                },
                "corpusId": 249812259,
                "publicationVenue": {
                    "id": "e16082f9-390d-423d-a448-883b5a4fa88f",
                    "name": "Journal of Cybersecurity and Privacy",
                    "type": "journal",
                    "alternate_names": [
                        "J Cybersecur Priv"
                    ],
                    "issn": "2624-800X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1444919",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-1444919"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/55976ee561064f301fcfce26356a6da9e062494c",
                "title": "An Accuracy-Maximization Approach for Claims Classifiers in Document Content Analytics for Cybersecurity",
                "abstract": "This paper presents our research approach and findings towards maximizing the accuracy of our classifier of feature claims for cybersecurity literature analytics, and introduces the resulting model ClaimsBERT. Its architecture, after extensive evaluations of different approaches, introduces a feature map concatenated with a Bidirectional Encoder Representation from Transformers (BERT) model. We discuss deployment of this new concept and the research insights that resulted in the selection of Convolution Neural Networks for its feature mapping aspects. We also present our results showing ClaimsBERT to outperform all other evaluated approaches. This new claims classifier represents an essential processing stage within our vetting framework aiming to improve the cybersecurity of industrial control systems (ICS). Furthermore, in order to maximize the accuracy of our new ClaimsBERT classifier, we propose an approach for optimal architecture selection and determination of optimized hyperparameters, in particular the best learning rate, number of convolutions, filter sizes, activation function, the number of dense layers, as well as the number of neurons and the drop-out rate for each layer. Fine-tuning these hyperparameters within our model led to an increase in classification accuracy from 76% obtained with BertForSequenceClassification\u2019s original model to a 97% accuracy obtained with ClaimsBERT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "134238540",
                        "name": "Kimia Ameri"
                    },
                    {
                        "authorId": "35380390",
                        "name": "M. Hempel"
                    },
                    {
                        "authorId": "145505074",
                        "name": "H. Sharif"
                    },
                    {
                        "authorId": "1558053440",
                        "name": "Juan Lopez Jr."
                    },
                    {
                        "authorId": "1750770",
                        "name": "K. Perumalla"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Data in MULTIHIERTT is collected from the FinQA dataset (Chen et al., 2021) and FinTabNet dataset (Zheng et al., 2021).",
                "MULTIHIERTT are deployed based on the FinTabNet dataset (Zheng et al., 2021), which contains 89,646 pages with table annotations extracted from the annual reports of S&P 500 companies."
            ],
            "citingPaper": {
                "paperId": "0b7e9b6b588baa3f24fdde06feec26a067aa74bd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-01347",
                    "ACL": "2022.acl-long.454",
                    "ArXiv": "2206.01347",
                    "DOI": "10.18653/v1/2022.acl-long.454",
                    "CorpusId": 248780469
                },
                "corpusId": 248780469,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/0b7e9b6b588baa3f24fdde06feec26a067aa74bd",
                "title": "MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data",
                "abstract": "Numerical reasoning over hybrid data containing both textual and tabular content (e.g., financial reports) has recently attracted much attention in the NLP community. However, existing question answering (QA) benchmarks over hybrid data only include a single flat table in each document and thus lack examples of multi-step numerical reasoning across multiple hierarchical tables. To facilitate data analytical progress, we construct a new large-scale benchmark, MultiHiertt, with QA pairs over Multi Hierarchical Tabular and Textual data. MultiHiertt is built from a wealth of financial reports and has the following unique characteristics: 1) each document contain multiple tables and longer unstructured texts; 2) most of tables contained are hierarchical; 3) the reasoning process required for each question is more complex and challenging than existing benchmarks; and 4) fine-grained annotations of reasoning processes and supporting facts are provided to reveal complex numerical reasoning. We further introduce a novel QA model termed MT2Net, which first applies facts retrieving to extract relevant supporting facts from both tables and text and then uses a reasoning module to perform symbolic reasoning over retrieved facts. We conduct comprehensive experiments on various baselines. The experimental results show that MultiHiertt presents a strong challenge for existing baselines whose results lag far behind the performance of human experts. The dataset and code are publicly available at https://github.com/psunlpgroup/MultiHiertt.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46316984",
                        "name": "Yilun Zhao"
                    },
                    {
                        "authorId": "2155851529",
                        "name": "Yunxiang Li"
                    },
                    {
                        "authorId": "46651313",
                        "name": "Chenying Li"
                    },
                    {
                        "authorId": "15176410",
                        "name": "Rui Zhang"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Data in MULTIHIERTT is collected from the FinQA dataset (Chen et al., 2021) and FinTabNet dataset (Zheng et al., 2021).",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "MULTIHIERTT are deployed based on the FinTabNet dataset (Zheng et al., 2021), which contains 89,646 pages with table annotations extracted from the annual reports of S&P 500 companies.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Examples for such models include OCR [7], document layout analysis [8]\u2013[10], table structure recovery [11], [12], figure understanding [13], reference and citation resolution [14], etc."
            ],
            "citingPaper": {
                "paperId": "2ad782750773cb1acc7600a6ef4e684febcdcee2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-00785",
                    "ArXiv": "2206.00785",
                    "DOI": "10.1109/CLOUD55607.2022.00060",
                    "CorpusId": 249282677
                },
                "corpusId": 249282677,
                "publicationVenue": {
                    "id": "406d9f60-417a-4dc5-a6b7-1fe4689a4ff7",
                    "name": "IEEE International Conference on Cloud Computing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Cloud Comput [services Soc",
                        "CLOUD",
                        "International Conference on Cloud Computing [Services Society]",
                        "IEEE Int Conf Cloud Comput"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2ad782750773cb1acc7600a6ef4e684febcdcee2",
                "title": "Delivering Document Conversion as a Cloud Service with High Throughput and Responsiveness",
                "abstract": "Document understanding is a key business process in the data-driven economy since documents are central to knowledge discovery and business insights. Converting documents into a machine-processable format is a particular challenge here due to their huge variability in formats and complex structure. Accordingly, many algorithms and machine-learning methods emerged to solve particular tasks such as Optical Character Recognition (OCR), layout analysis, table-structure recovery, figure understanding, etc. We observe the adoption of such methods in document understanding solutions offered by all major cloud providers. Yet, publications outlining how such services are designed and optimized to scale in the cloud are scarce. In this paper, we focus on the case of document conversion to illustrate the particular challenges of scaling a complex data processing pipeline with a strong reliance on machine-learning methods on cloud infrastructure. Our key objective is to achieve high scalability and responsiveness for different workload profiles in a well-defined resource budget. We outline the requirements, design, and implementation choices of our document conversion service and reflect on the challenges we faced. Evidence for the scaling behavior and resource efficiency is provided for two alternative workload distribution strategies and deployment configurations. Our best-performing method achieves sustained throughput of over one million PDF pages per hour on 3072 CPU cores across 192 nodes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065064783",
                        "name": "Christoph Auer"
                    },
                    {
                        "authorId": "3175187",
                        "name": "Michele Dolfi"
                    },
                    {
                        "authorId": "1840587",
                        "name": "A. Carvalho"
                    },
                    {
                        "authorId": "2167580060",
                        "name": "Cesar Berrospi Ramis"
                    },
                    {
                        "authorId": "2167580817",
                        "name": "P. W. J. S. I. Research"
                    },
                    {
                        "authorId": "2167582469",
                        "name": "SoftINSA Lda."
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Examples for such models include OCR [7], document layout analysis [8]\u2013[10], table structure recovery [11], [12], figure understanding [13], reference and citation resolution [14], etc.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "GTE [46] is a Global Table Extractor that consists of table detection and cell structured recognition.",
                "Although [10,46] adopt post-processing operations to improve the accuracy, the result is not as expectedwhen the"
            ],
            "citingPaper": {
                "paperId": "0b3b6e22fba1c9dc5e5b386128f1194618692158",
                "externalIds": {
                    "DBLP": "journals/ijdar/ZhangMGJZ23",
                    "DOI": "10.1007/s10032-022-00400-z",
                    "CorpusId": 248581054
                },
                "corpusId": 248581054,
                "publicationVenue": {
                    "id": "46722266-4327-4a22-871d-5a9fe3c5ad0c",
                    "name": "International Journal on Document Analysis and Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Doc Anal Recognit"
                    ],
                    "issn": "1433-2825",
                    "url": "https://link.springer.com/journal/10032"
                },
                "url": "https://www.semanticscholar.org/paper/0b3b6e22fba1c9dc5e5b386128f1194618692158",
                "title": "YOLO-table: disclosure document table detection with involution",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2164852187",
                        "name": "Daqian Zhang"
                    },
                    {
                        "authorId": "1380055201",
                        "name": "Ruibin Mao"
                    },
                    {
                        "authorId": "1387681701",
                        "name": "R. Guo"
                    },
                    {
                        "authorId": "2164709014",
                        "name": "Yang Jiang"
                    },
                    {
                        "authorId": "2164850015",
                        "name": "Jing Zhu"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "GTE [46] is a Global Table Extractor that consists of table detection and cell structured recognition.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Although [10,46] adopt post-processing operations to improve the accuracy, the result is not as expectedwhen the",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Some recent works [8, 22, 23] have proposed a modified TEDS metric, denoted as TEDS-Struct, to evaluate table structure recognition accuracy only by ignoring OCR errors.",
                "[8] Figure 1: An outline of our table extraction approach.",
                "To eliminate this assumption, another group of methods [8, 22, 23] proposed to detect the bounding boxes of"
            ],
            "citingPaper": {
                "paperId": "29942b5588d9978af5da046ebfc642e4779b8597",
                "externalIds": {
                    "ArXiv": "2203.09056",
                    "DBLP": "journals/pr/MaLSH23",
                    "DOI": "10.1016/j.patcog.2022.109006",
                    "CorpusId": 247518554
                },
                "corpusId": 247518554,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/29942b5588d9978af5da046ebfc642e4779b8597",
                "title": "Robust Table Detection and Structure Recognition from Heterogeneous Document Images",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Some recent works [8, 22, 23] have proposed a modified TEDS metric, denoted as TEDS-Struct, to evaluate table structure recognition accuracy only by ignoring OCR errors.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "[8] Figure 1: An outline of our table extraction approach.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "To eliminate this assumption, another group of methods [8, 22, 23] proposed to detect the bounding boxes of",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "There have been a few studies that achieved promising results, such as\nTableCellNet [10] reporting an F1 score of 0.937, GTE [35] reporting an F1 score of 96.24, but still this step may introduce extra to the table structure recognition result.",
                "937, GTE [35] reporting an F1 score of 96."
            ],
            "citingPaper": {
                "paperId": "007f2dc393e363abcfd023c1a1b47f5b84d4b2ae",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-03819",
                    "ArXiv": "2203.03819",
                    "DOI": "10.48550/arXiv.2203.03819",
                    "CorpusId": 247315039
                },
                "corpusId": 247315039,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/007f2dc393e363abcfd023c1a1b47f5b84d4b2ae",
                "title": "Table Structure Recognition with Conditional Attention",
                "abstract": "Tabular data in digital documents is widely used to express compact and important information for readers. However, it is challenging to parse tables from unstructured digital documents, such as PDFs and images, into machine-readable format because of the complexity of table structures and the missing of meta-information. Table Structure Recognition (TSR) problem aims to recognize the structure of a table and transform the unstructured tables into a structured and machine-readable format so that the tabular data can be further analysed by the down-stream tasks, such as semantic modeling and information retrieval. In this study, we hypothesize that a complicated table structure can be represented by a graph whose vertices and edges represent the cells and association between cells, respectively. Then we define the table structure recognition problem as a cell association classification problem and propose a conditional attention network (CATT-Net). The experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods on various datasets. Besides, we investigate whether the alignment of a cell bounding box or a text-focused approach has more impact on the model performance. Due to the lack of public dataset annotations based on these two approaches, we further annotate the ICDAR2013 dataset providing both types of bounding boxes, which can be a new benchmark dataset for evaluating the methods in this field. Experimental results show that the alignment of a cell bounding box can help improve the Micro-averaged F1 score from 0.915 to 0.963, and the Macro-average F1 score from 0.787 to 0.923.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144025674",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "There have been a few studies that achieved promising results, such as\nTableCellNet [10] reporting an F1 score of 0.937, GTE [35] reporting an F1 score of 96.24, but still this step may introduce extra to the table structure recognition result.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "937, GTE [35] reporting an F1 score of 96.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Lately, a set of new model-architectures has been proposed by the community to address table-structure decomposition [37, 36, 18, 20].",
                "However, this has definitely changed in recent years with the deliverance of PubTabNet [37], FinTabNet [36], TableBank [17] etc.",
                "We rely on large-scale datasets such as PubTabNet [37], FinTabNet [36], and TableBank [17] datasets to train and evaluate our models.",
                "\u2022 An augmented dataset based on PubTabNet [37], FinTabNet [36], and TableBank [17] with generated ground-truth for reproducibility."
            ],
            "citingPaper": {
                "paperId": "081312b993ef1df7e64e1a5f8715852a2e7ed527",
                "externalIds": {
                    "ArXiv": "2203.01017",
                    "DBLP": "journals/corr/abs-2203-01017",
                    "DOI": "10.1109/CVPR52688.2022.00457",
                    "CorpusId": 247218660
                },
                "corpusId": 247218660,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/081312b993ef1df7e64e1a5f8715852a2e7ed527",
                "title": "TableFormer: Table Structure Understanding with Transformers",
                "abstract": "Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortu-nately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct iden-tification of the table-structure from an image is a nontrivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from program-matic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "37525891",
                        "name": "A. Nassar"
                    },
                    {
                        "authorId": "2051177400",
                        "name": "Nikolaos Livathinos"
                    },
                    {
                        "authorId": "1578242957",
                        "name": "Maksym Lysak"
                    },
                    {
                        "authorId": "1831851",
                        "name": "P. Staar"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Lately, a set of new model-architectures has been proposed by the community to address table-structure decomposition [37, 36, 18, 20].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "However, this has definitely changed in recent years with the deliverance of PubTabNet [37], FinTabNet [36], TableBank [17] etc.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We rely on large-scale datasets such as PubTabNet [37], FinTabNet [36], and TableBank [17] datasets to train and evaluate our models.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "\u2022 An augmented dataset based on PubTabNet [37], FinTabNet [36], and TableBank [17] with generated ground-truth for reproducibility.",
                    "label_score": 0.5,
                    "label": "Weak"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a9e00a1c3de05e3d06a4287b8ef07096f0cfe609",
                "externalIds": {
                    "DBLP": "journals/pr/RibaGTRFL22",
                    "DOI": "10.1016/j.patcog.2022.108641",
                    "CorpusId": 247427729
                },
                "corpusId": 247427729,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a9e00a1c3de05e3d06a4287b8ef07096f0cfe609",
                "title": "Table detection in business document images by message passing networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40420775",
                        "name": "Pau Riba"
                    },
                    {
                        "authorId": "145029669",
                        "name": "Lutz Goldmann"
                    },
                    {
                        "authorId": "3045937",
                        "name": "O. R. Terrades"
                    },
                    {
                        "authorId": "1491424368",
                        "name": "Diede Rusticus"
                    },
                    {
                        "authorId": "1686569",
                        "name": "A. Forn\u00e9s"
                    },
                    {
                        "authorId": "143826881",
                        "name": "J. Llad\u00f3s"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent public datasets include ICDAR 2019 [41], TableBank [65], PubTabNet [114], SciTSR [26], and FinTabNet [112]."
            ],
            "citingPaper": {
                "paperId": "49f4b4ca86e574c7ec688cfd45d2e17ff079c313",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-09745",
                    "ArXiv": "2201.09745",
                    "DOI": "10.24963/ijcai.2022/761",
                    "CorpusId": 246241054
                },
                "corpusId": 246241054,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/49f4b4ca86e574c7ec688cfd45d2e17ff079c313",
                "title": "Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks",
                "abstract": "Following the success of pre-training techniques in the natural language domain, a flurry of table pre-training frameworks have been proposed and have achieved new state-of-the-arts on various downstream tasks such as table question answering, table type recognition, column relation classification, table search, and formula prediction. Various model architectures have been explored to best capture the characteristics of (semi-)structured tables, especially specially-designed attention mechanisms. Moreover, to fully leverage the supervision signals in unlabeled tables, diverse pre-training objectives have been designed and evaluated, for example, denoising cell values, predicting numerical relationships, and learning a neural SQL executor. This survey aims to provide a comprehensive review of model designs, pre-training objectives, and downstream tasks for table pre-training, and we further share our thoughts on existing challenges and future opportunities.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113413583",
                        "name": "Haoyu Dong"
                    },
                    {
                        "authorId": "1471878967",
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "authorId": "2116554804",
                        "name": "Xinyi He"
                    },
                    {
                        "authorId": "2109138287",
                        "name": "Mengyuan Zhou"
                    },
                    {
                        "authorId": "1571660987",
                        "name": "Anda Zhou"
                    },
                    {
                        "authorId": "2153433679",
                        "name": "Fan Zhou"
                    },
                    {
                        "authorId": "1491241424",
                        "name": "Ao Liu"
                    },
                    {
                        "authorId": "2109750123",
                        "name": "Shi Han"
                    },
                    {
                        "authorId": "2140415600",
                        "name": "Dongmei Zhang"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Recent public datasets include ICDAR 2019 [41], TableBank [65], PubTabNet [114], SciTSR [26], and FinTabNet [112].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The success of DL has marked the revisiting of table structure parsing by [7], which inspired follow-up research [27, 1, 6, 2, 28, 29, 30, 19, 31, 18, 32, 17].",
                "In some best performing frameworks [17, 18, 19], they all jointly optimize the structure detection and entity relations in the structure, as in DocParser.",
                "We see from the previous works, the most effective methods [17, 18, 19] always jointly optimize the cell locations and cell relationships."
            ],
            "citingPaper": {
                "paperId": "10c3efc40e72674b615864c94a231e1f11913619",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-01654",
                    "ArXiv": "2201.01654",
                    "CorpusId": 245704311
                },
                "corpusId": 245704311,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/10c3efc40e72674b615864c94a231e1f11913619",
                "title": "TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets",
                "abstract": "Tables have been an ever-existing structure to store data. There exist now different approaches to store tabular data physically. PDFs, images, spreadsheets, and CSVs are leading examples. Being able to parse table structures and extract content bounded by these structures is of high importance in many applications. In this paper, we devise TableParser, a system capable of parsing tables in both native PDFs and scanned images with high precision. We have conducted extensive experiments to show the efficacy of domain adaptation in developing such a tool. Moreover, we create TableAnnotator and ExcelAnnotator, which constitute a spreadsheet-based weak supervision mechanism and a pipeline to enable table parsing. We share these resources with the research community to facilitate further research in this interesting direction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1576005199",
                        "name": "Susie Xi Rao"
                    },
                    {
                        "authorId": "5185738",
                        "name": "Johannes Rausch"
                    },
                    {
                        "authorId": "2074406990",
                        "name": "P. Egger"
                    },
                    {
                        "authorId": "1776014",
                        "name": "Ce Zhang"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "The success of DL has marked the revisiting of table structure parsing by [7], which inspired follow-up research [27, 1, 6, 2, 28, 29, 30, 19, 31, 18, 32, 17].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "In some best performing frameworks [17, 18, 19], they all jointly optimize the structure detection and entity relations in the structure, as in DocParser.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We see from the previous works, the most effective methods [17, 18, 19] always jointly optimize the cell locations and cell relationships.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d9c676eccdff80a7d1154660a09c4d2ea5164610",
                "externalIds": {
                    "DOI": "10.1007/s11042-021-11819-7",
                    "CorpusId": 254860167
                },
                "corpusId": 254860167,
                "publicationVenue": {
                    "id": "477368e9-7a8e-475a-8c93-6d623797fd06",
                    "name": "Multimedia tools and applications",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Tools and Applications",
                        "Multimedia Tool Appl",
                        "Multimedia tool appl"
                    ],
                    "issn": "1380-7501",
                    "url": "https://www.springer.com/computer/information+systems+and+applications/journal/11042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d9c676eccdff80a7d1154660a09c4d2ea5164610",
                "title": "Deep-learning and graph-based approach to table structure recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115242015",
                        "name": "Eunji Lee"
                    },
                    {
                        "authorId": "2370571",
                        "name": "Jaewoo Park"
                    },
                    {
                        "authorId": "2463454",
                        "name": "H. Koo"
                    },
                    {
                        "authorId": "1707645",
                        "name": "N. Cho"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Although many previous algorithms [2,13,18,20,22,30,31,34,35,40,45,46] have achieved impressive progress in the community, TSR is still a challenging task due to two factors of complicated tables.",
                "To tackle this issue, the hierarchical GTE [45] leverages clustering algorithm for cell structure recognition.",
                "Table structure recognition (TSR) aims to recognize the table internal structure to the machine readable data mainly presented in two formats: logical structure [18, 46] and physical structure [2, 13, 20, 22, 27, 30, 31, 34, 35, 40, 45]."
            ],
            "citingPaper": {
                "paperId": "9fb2744ef2b91033de39c121be25d3f86f759458",
                "externalIds": {
                    "DBLP": "conf/cvpr/0003LLJL022",
                    "ArXiv": "2111.13359",
                    "DOI": "10.1109/CVPR52688.2022.00449",
                    "CorpusId": 244709555
                },
                "corpusId": 244709555,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9fb2744ef2b91033de39c121be25d3f86f759458",
                "title": "Neural Collaborative Graph Machines for Table Structure Recognition",
                "abstract": "Recently, table structure recognition has achieved impressive progress with the help of deep graph models. Most of them exploit single visual cues of tabular elements or simply combine visual cues with other modalities via early fusion to reason their graph relationships. However, neither early fusion nor individually reasoning in terms of multiple modalities can be appropriate for all varieties of table structures with great diversity. Instead, different modalities are expected to collaborate with each other in different patterns for different table cases. In the community, the importance of intrainter modality interactions for table structure reasoning is still unexplored. In this paper, we define it as heterogeneous table structure recognition (HeteroTSR) problem. With the aim offilling this gap, we present a novel Neural Collaborative Graph Machines (NCGM) equipped with stacked collaborative blocks, which alternatively extracts intramodality context and models inter-modality interactions in a hierarchical way. It can represent the intrainter modality relationships of tabular elements more robustly, which significantly improves the recognition performance. We also show that the proposed NCGM can modulate collaborative pattern of different modalities conditioned on the context of intramodality cues, which is vital for diversified table cases. Experimental results on benchmarks demonstrate our proposed NCGM achieves state-of-the-art performance and beats other contemporary methods by a large margin especially under challenging scenarios.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48446712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2149124539",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Although many previous algorithms [2,13,18,20,22,30,31,34,35,40,45,46] have achieved impressive progress in the community, TSR is still a challenging task due to two factors of complicated tables.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "To tackle this issue, the hierarchical GTE [45] leverages clustering algorithm for cell structure recognition.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Table structure recognition (TSR) aims to recognize the table internal structure to the machine readable data mainly presented in two formats: logical structure [18, 46] and physical structure [2, 13, 20, 22, 27, 30, 31, 34, 35, 40, 45].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "19a2a48c63504b5ccb652b1976ba638a92947660",
                "externalIds": {
                    "DBLP": "journals/inffus/0001SZCS23",
                    "ArXiv": "2111.07533",
                    "DOI": "10.1016/j.inffus.2023.101830",
                    "CorpusId": 257901250
                },
                "corpusId": 257901250,
                "publicationVenue": {
                    "id": "06afdd0b-0d85-413f-af8a-c3045c12c561",
                    "name": "Information Fusion",
                    "type": "journal",
                    "alternate_names": [
                        "Inf Fusion"
                    ],
                    "issn": "1566-2535",
                    "url": "https://www.journals.elsevier.com/information-fusion",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/15662535"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/19a2a48c63504b5ccb652b1976ba638a92947660",
                "title": "Automated scholarly paper review: Concepts, technologies, and challenges",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1651633605",
                        "name": "Jialiang Lin"
                    },
                    {
                        "authorId": "2108456439",
                        "name": "Jiaxin Song"
                    },
                    {
                        "authorId": "2142138142",
                        "name": "Zhangping Zhou"
                    },
                    {
                        "authorId": "47558200",
                        "name": "Yidong Chen"
                    },
                    {
                        "authorId": "1755321",
                        "name": "X. Shi"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Since FinTabNet has bounding boxes wrapped around the cell\u2019s content, we pre-process the ground truth to obtain cell level coordinates (refer supplementary paper)2.",
                "Further, most cell detection methods [7, 41] evaluate using an Intersection over Union (IoU) threshold of 0.",
                "Another recent work, GTE-Cell [41], follows a nested approach by first classifying whether a table includes ruling lines or not, and then uses specifically tailored heuristics to identify the table structure.",
                "We use FinTabNet [41] dataset for training.",
                "We use FinTabNet [41] dataset for training and evaluation.",
                "Other datasets [4, 42, 41, 8] have annotations such that a cell\u2019s bounding box is the smallest rectangle that encapsulates its content.",
                "However, there are two concerning factors for cell detection: (i) How are the ground truth cell boundingboxes annotated? (ii) What is the IoU threshold value used to compute evaluation metrics? For table cells, most datasets [22, 4, 6, 17, 42, 41] have cell box annotation that spans the smallest rectangle encapsulating its content.",
                "For evaluation also, we pre-process ICDAR-2013 [8], cTDaR [7], SciTSR [4], PubTabNet [42] and FinTabNet [41] datasets before computing IoU with the corresponding predictions.",
                "We use FinTabNet [41] dataset to train TOD-Net for cell, row, and column detection.",
                "Most datasets [22, 4, 6, 17, 42, 41] use words or cell content as low-level entities to build inter-tabular relationships.",
                "[22, 4, 17, 42] introduced many large-scale automatically generated datasets, but they do not accurately represent real-world complex tables as seen in the business documents [41, 27, 8]."
            ],
            "citingPaper": {
                "paperId": "d499cd8a77e7fc4e12d1f5bd7875ce2e3f5465a4",
                "externalIds": {
                    "DBLP": "conf/wacv/RajaMV22",
                    "ArXiv": "2111.07129",
                    "DOI": "10.1109/WACV51458.2022.00260",
                    "CorpusId": 240285297
                },
                "corpusId": 240285297,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/d499cd8a77e7fc4e12d1f5bd7875ce2e3f5465a4",
                "title": "Visual Understanding of Complex Table Structures from Document Images",
                "abstract": "Table structure recognition is necessary for a comprehensive understanding of documents. Tables in unstructured business documents are tough to parse due to the high diversity of layouts, varying alignments of contents, and the presence of empty cells. The problem is particularly difficult because of challenges in identifying individual cells using visual or linguistic contexts or both. Accurate detection of table cells (including empty cells) simplifies structure extraction and hence, it becomes the prime focus of our work. We propose a novel object-detection-based deep model that captures the inherent alignments of cells within tables and is fine-tuned for fast optimization. Despite accurate detection of cells, recognizing structures for dense tables may still be challenging because of difficulties in capturing long-range row/column dependencies in presence of multi-row/column spanning cells. Therefore, we also aim to improve structure recognition by deducing a novel rectilinear graph-based formulation. From a semantics perspective, we highlight the significance of empty cells in a table. To take these cells into account, we suggest an enhancement to a popular evaluation criterion. Finally, we introduce a modestly sized evaluation dataset with an annotation style inspired by human cognition to encourage new approaches to the problem. Our framework improves the previous state-of-the-art performance by a 2.7% average F1-score on benchmark datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2053910624",
                        "name": "S. Raja"
                    },
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "2226828175",
                        "name": "Jawahar C V"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Since FinTabNet has bounding boxes wrapped around the cell\u2019s content, we pre-process the ground truth to obtain cell level coordinates (refer supplementary paper)2.",
                    "label_score": 0.5,
                    "label": "Weak"
                },
                {
                    "context": "Further, most cell detection methods [7, 41] evaluate using an Intersection over Union (IoU) threshold of 0.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Another recent work, GTE-Cell [41], follows a nested approach by first classifying whether a table includes ruling lines or not, and then uses specifically tailored heuristics to identify the table structure.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We use FinTabNet [41] dataset for training.",
                    "label_score": 0.5,
                    "label": "Weak"
                },
                {
                    "context": "We use FinTabNet [41] dataset for training and evaluation.",
                    "label_score": 0.5,
                    "label": "Weak"
                },
                {
                    "context": "Other datasets [4, 42, 41, 8] have annotations such that a cell\u2019s bounding box is the smallest rectangle that encapsulates its content.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "However, there are two concerning factors for cell detection: (i) How are the ground truth cell boundingboxes annotated? (ii) What is the IoU threshold value used to compute evaluation metrics? For table cells, most datasets [22, 4, 6, 17, 42, 41] have cell box annotation that spans the smallest rectangle encapsulating its content.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "For evaluation also, we pre-process ICDAR-2013 [8], cTDaR [7], SciTSR [4], PubTabNet [42] and FinTabNet [41] datasets before computing IoU with the corresponding predictions.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We use FinTabNet [41] dataset to train TOD-Net for cell, row, and column detection.",
                    "label_score": 0.5,
                    "label": "Weak"
                },
                {
                    "context": "Most datasets [22, 4, 6, 17, 42, 41] use words or cell content as low-level entities to build inter-tabular relationships.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "[22, 4, 17, 42] introduced many large-scale automatically generated datasets, but they do not accurately represent real-world complex tables as seen in the business documents [41, 27, 8].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "While it is true that there are a few datasets of documents [6, 7, 22, 23] available with ground-truths for the layout bounding boxes, they focus on specific corpora such as scientific publications and are difficult to extend to other domains or customize for new element types."
            ],
            "citingPaper": {
                "paperId": "5261737e877eb9da6277997893fee04b839721ad",
                "externalIds": {
                    "DBLP": "journals/pr/RamanSV22",
                    "ArXiv": "2111.06016",
                    "DOI": "10.1016/j.patcog.2022.108660",
                    "CorpusId": 243986021
                },
                "corpusId": 243986021,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5261737e877eb9da6277997893fee04b839721ad",
                "title": "Synthetic Document Generator for Annotation-free Layout Recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "32319713",
                        "name": "Natraj Raman"
                    },
                    {
                        "authorId": "36532736",
                        "name": "Sameena Shah"
                    },
                    {
                        "authorId": "2058284590",
                        "name": "M. Veloso"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "While it is true that there are a few datasets of documents [6, 7, 22, 23] available with ground-truths for the layout bounding boxes, they focus on specific corpora such as scientific publications and are difficult to extend to other domains or customize for new element types.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "The current iteration of SciA11y focuses on improving screen reader accessibility in terms of navigation within the HTML document.",
                "An intrinsic evaluation of HTML quality revealed that around 86% of papers in our sample had reasonable extractions (good or okay readability per criteria described in Wang et al. [17]), and a preliminary user study with 6 BLV researchers was also positive, with all users stating they would be likely to use the system in the future were it to have high coverage of papers.",
                "The system renders this content in HTML and introduces accessibility features such as navigational headings, tagged objects, table of contents, and within-document navigational links.",
                "For example, we intend to integrate features for reading graphs and charts [4\u20136], mathematical equations [2, 7, 13, 16], and further processing table images into HTML [8, 18, 19].",
                "A set of 1.5 million open access papers are available to read in HTML format at our demo site: https://scia11y.org/.",
                "SciA11y: Converting Scientific Papers to Accessible HTML.",
                "Science publishers are shifting towards dual publishing or alternate publishing schemes2 that yield accessible HTML or XML versions of papers in addition to PDF.",
                "We propose and demonstrate a pipeline for extracting the semantic content of paper PDFs and rendering this content as an accessible HTML document.",
                "In summary, we introduce the SciA11y system for rendering scientific PDFs as HTML, which can increase the accessibility of these documents for screen readers.",
                "We combine textual elements from S2ORC and figure/table elements from DeepFigures to create the HTML representation."
            ],
            "citingPaper": {
                "paperId": "582616fe83a85f095a16778c28297723e25b0ea7",
                "externalIds": {
                    "DBLP": "conf/assets/WangCBCHLKZWW21",
                    "DOI": "10.1145/3441852.3476545",
                    "CorpusId": 239011922
                },
                "corpusId": 239011922,
                "publicationVenue": {
                    "id": "6864fc0f-dd47-4798-a829-ac53dd78862f",
                    "name": "International ACM SIGACCESS Conference on Computers and Accessibility",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Computers and Accessibility",
                        "Conf Comput Access",
                        "Int ACM SIGACCESS Conf Comput Access",
                        "ASSETS"
                    ],
                    "url": "https://dl.acm.org/conference/assets"
                },
                "url": "https://www.semanticscholar.org/paper/582616fe83a85f095a16778c28297723e25b0ea7",
                "title": "SciA11y: Converting Scientific Papers to Accessible HTML",
                "abstract": "We present SciA11y, a system that renders inaccessible scientific paper PDFs into HTML. SciA11y uses machine learning models to extract and understand the content of scientific PDFs, and reorganizes the resulting paper components into a form that better supports skimming and scanning for blind and low vision (BLV) readers. SciA11y adds navigation features such as tagged headings, a table of contents, and bidirectional links between inline citations and references, which allow readers to resolve citations without losing their context. A set of 1.5 million open access papers are processed and available at https://scia11y.org/. This system is a first step in addressing scientific PDF accessibility, and may significantly improve the experience of paper reading for BLV users.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31860505",
                        "name": "Lucy Lu Wang"
                    },
                    {
                        "authorId": "51199773",
                        "name": "Isabel Cachola"
                    },
                    {
                        "authorId": "2699105",
                        "name": "Jonathan Bragg"
                    },
                    {
                        "authorId": "2087062598",
                        "name": "Evie (Yu-Yen) Cheng"
                    },
                    {
                        "authorId": "31809387",
                        "name": "Chelsea Hess Haupt"
                    },
                    {
                        "authorId": "2087047386",
                        "name": "Matt Latzke"
                    },
                    {
                        "authorId": "2003338023",
                        "name": "Bailey Kuehl"
                    },
                    {
                        "authorId": "15292561",
                        "name": "Madeleine van Zuylen"
                    },
                    {
                        "authorId": "82676859",
                        "name": "Linda M. Wagner"
                    },
                    {
                        "authorId": "1780531",
                        "name": "Daniel S. Weld"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "The current iteration of SciA11y focuses on improving screen reader accessibility in terms of navigation within the HTML document.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "An intrinsic evaluation of HTML quality revealed that around 86% of papers in our sample had reasonable extractions (good or okay readability per criteria described in Wang et al. [17]), and a preliminary user study with 6 BLV researchers was also positive, with all users stating they would be likely to use the system in the future were it to have high coverage of papers.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "The system renders this content in HTML and introduces accessibility features such as navigational headings, tagged objects, table of contents, and within-document navigational links.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "For example, we intend to integrate features for reading graphs and charts [4\u20136], mathematical equations [2, 7, 13, 16], and further processing table images into HTML [8, 18, 19].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "A set of 1.5 million open access papers are available to read in HTML format at our demo site: https://scia11y.org/.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "SciA11y: Converting Scientific Papers to Accessible HTML.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Science publishers are shifting towards dual publishing or alternate publishing schemes2 that yield accessible HTML or XML versions of papers in addition to PDF.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We propose and demonstrate a pipeline for extracting the semantic content of paper PDFs and rendering this content as an accessible HTML document.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "In summary, we introduce the SciA11y system for rendering scientific PDFs as HTML, which can increase the accessibility of these documents for screen readers.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We combine textual elements from S2ORC and figure/table elements from DeepFigures to create the HTML representation.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Although methods [29, 31, 43] achieve performance improvement by directly predicting cell boxes, they may suffer from \u201ccell boundary ambiguity\u201d problem, especially on those blank or non-gridded cell cases.",
                "Different from previousworks [29, 31, 43], we adopt word bounding boxes rather than cells as table elements to avoid cell boundary ambiguity issue.",
                "4(c) shows our method can well handle the table including non-gridded cells, which may cause the \u201ccell boundary ambiguity\u201d problem to the cell detection-based methods [29, 31, 43].",
                "A group of methods [27, 29, 41, 43] try to recover the relations of elements based on heuristic algorithms."
            ],
            "citingPaper": {
                "paperId": "7b99f9995a4b086c0e6913a8d2f109a5dbaa5014",
                "externalIds": {
                    "DBLP": "conf/mm/LiuLLJLRJ21",
                    "DOI": "10.1145/3474085.3481534",
                    "CorpusId": 239011898
                },
                "corpusId": 239011898,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7b99f9995a4b086c0e6913a8d2f109a5dbaa5014",
                "title": "Show, Read and Reason: Table Structure Recognition with Flexible Context Aggregator",
                "abstract": "We investigate the challenging problem of table structure recognition in this work. Many recent methods adopt graph-based context aggregator with strong inductive bias to reason sparse contextual relationships of table elements. However, the strong constraints may be too restrictive to represent the complicated table relationships. In order to learn more appropriate inductive bias from data, we try to introduce Transformer as context aggregator in this work. Nevertheless, Transformer taking dense context as input requires larger scale data and may suffer from unstable training procedure due to the weakening of inductive bias. To overcome the above limitations, we in this paper design a FLAG (FLexible context AGgregator), which marries Transformer with graph-based context aggregator in an adaptive way. Based on FLAG, an end-to-end framework requiring no extra meta-data or OCR information, termed FLAG-Net, is proposed to flexibly modulate the aggregation of dense context and sparse one for the relational reasoning of table elements. We investigate the modulation pattern in FLAG and show what contextual information is focused, which is vital for recognizing table structure. Extensive experimental results on benchmarks demonstrate the performance of our proposed FLAG-Net surpasses other compared methods by a large margin.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48446712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2149124539",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    },
                    {
                        "authorId": "145592290",
                        "name": "R. Ji"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Although methods [29, 31, 43] achieve performance improvement by directly predicting cell boxes, they may suffer from \u201ccell boundary ambiguity\u201d problem, especially on those blank or non-gridded cell cases.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Different from previousworks [29, 31, 43], we adopt word bounding boxes rather than cells as table elements to avoid cell boundary ambiguity issue.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "4(c) shows our method can well handle the table including non-gridded cells, which may cause the \u201ccell boundary ambiguity\u201d problem to the cell detection-based methods [29, 31, 43].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "A group of methods [27, 29, 41, 43] try to recover the relations of elements based on heuristic algorithms.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b8bdf103e70e5bf962f85007462a4d14cfb22c78",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-02933",
                    "ArXiv": "2110.02933",
                    "DOI": "10.1016/j.neucom.2022.09.094",
                    "CorpusId": 238407904
                },
                "corpusId": 238407904,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b8bdf103e70e5bf962f85007462a4d14cfb22c78",
                "title": "On Cropped versus Uncropped Training Sets in Tabular Structure Detection",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2130012695",
                        "name": "Yakup Akkaya"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "152235107",
                        "name": "Shahzad Khan"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Recently, there has been a shift in the research literature from traditional rule-based methods [3,10,17] for TE to data-driven methods based on deep learning (DL) [13,16,21].",
                "Recently, larger datasets [2, 8, 21, 22] for TSR have been created by collecting crowd-sourced annotations automatically from existing documents.",
                "To overcome this, researchers have turned recently to crowd-sourcing to construct larger datasets [8, 21, 22].",
                "Modeling approaches One of the most common modeling approaches for TSR is to frame the task as some form of object detection [13, 16, 21].",
                "Other approaches use custom pipelines that branch to consider different cases separately, such as training separate models to recognize tables with and without visible borders surrounding every cell [13, 21]."
            ],
            "citingPaper": {
                "paperId": "a94c3e400fc5426a0b8a650b924242abcc1e46f2",
                "externalIds": {
                    "ArXiv": "2110.00061",
                    "DBLP": "conf/cvpr/SmockPA22",
                    "DOI": "10.1109/CVPR52688.2022.00459",
                    "CorpusId": 244462899
                },
                "corpusId": 244462899,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a94c3e400fc5426a0b8a650b924242abcc1e46f2",
                "title": "PubTables-1M: Towards comprehensive table extraction from unstructured documents",
                "abstract": "Recently, significant progress has been made applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, one of the greatest challenges remains the creation of datasets with complete, unambiguous ground truth at scale. To address this, we develop a new, more comprehensive dataset for table extraction, called PubTables-1M. PubTables-1M contains nearly one million tables from scientific articles, supports multiple input modalities, and contains detailed header and location information for table structures, making it useful for a wide variety of modeling approaches. It also addresses a significant source of ground truth inconsistency observed in prior datasets called oversegmentation, using a novel canonicalization procedure. We demonstrate that these improvements lead to a significant increase in training performance and a more reliable estimate of model performance at evaluation for table structure recognition. Further, we show that transformer-based object detection models trained on PubTables-1M produce excellent results for all three tasks of detection, structure recognition, and functional analysis without the need for any special customization for these tasks. Data and code will be released at https://github.com/microsoft/table-transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2394344",
                        "name": "Robin Abraham"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Recently, there has been a shift in the research literature from traditional rule-based methods [3,10,17] for TE to data-driven methods based on deep learning (DL) [13,16,21].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Recently, larger datasets [2, 8, 21, 22] for TSR have been created by collecting crowd-sourced annotations automatically from existing documents.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "To overcome this, researchers have turned recently to crowd-sourcing to construct larger datasets [8, 21, 22].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Modeling approaches One of the most common modeling approaches for TSR is to frame the task as some form of object detection [13, 16, 21].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Other approaches use custom pipelines that branch to consider different cases separately, such as training separate models to recognize tables with and without visible borders surrounding every cell [13, 21].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Limited by this, existing TSP approaches can only handle table structure parsing in a relative simple scenario by grouping detected cells into tables [11, 16, 9, 23].",
                "For a more practical requirement of parsing table structures from images taken by hand-held cameras in the wild, the existing state-of-the-art approaches [13, 14, 11, 16, 9, 23] are prone to fail as the commonly-used assumption of tabular images no longer holds.",
                "Recently, FinTabNet [23] and SciTSR [2] datasets add the cell coordinates and row-column information to become the most complete and large-scale dataset for table structure parsing task.",
                "However, these methods mainly focuses on the well-conditioned document images, where the tables [16, 23, 12, 9, 23, 14] are well-aligned to the image axes."
            ],
            "citingPaper": {
                "paperId": "7a4c901e00fef01d66efb125bf0e2fdd773d8bc6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-02199",
                    "ArXiv": "2109.02199",
                    "DOI": "10.1109/ICCV48922.2021.00098",
                    "CorpusId": 237420694
                },
                "corpusId": 237420694,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/7a4c901e00fef01d66efb125bf0e2fdd773d8bc6",
                "title": "Parsing Table Structures in the Wild",
                "abstract": "This paper tackles the problem of table structure parsing (TSP) from images in the wild. In contrast to existing studies that mainly focus on parsing well-aligned tabular images with simple layouts from scanned PDF documents, we aim to establish a practical table structure parsing system for real-world scenarios where tabular input images are taken or scanned with severe deformation, bending or occlusions. For designing such a system, we propose an approach named Cycle-CenterNet on the top of CenterNet with a novel cycle-pairing module to simultaneously detect and group tabular cells into structured tables. In the cycle-pairing module, a new pairing loss function is proposed for the network training. Alongside with our Cycle-CenterNet, we also present a large-scale dataset, named Wired Table in the Wild (WTW), which includes well-annotated structure parsing of multiple style tables in several scenes like photo, scanning files, web pages, etc.. In experiments, we demonstrate that our Cycle-CenterNet consistently achieves the best accuracy of table structure parsing on the new WTW dataset by 24.6% absolute improvement evaluated by the TEDS metric. A more comprehensive experimental analysis also validates the advantages of our proposed methods for the TSP task.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143799988",
                        "name": "Rujiao Long"
                    },
                    {
                        "authorId": "2119293226",
                        "name": "Wen Wang"
                    },
                    {
                        "authorId": "2685089",
                        "name": "Nan Xue"
                    },
                    {
                        "authorId": "2112256",
                        "name": "Feiyu Gao"
                    },
                    {
                        "authorId": "2109432908",
                        "name": "Zhibo Yang"
                    },
                    {
                        "authorId": "153709848",
                        "name": "Yongpan Wang"
                    },
                    {
                        "authorId": "39943835",
                        "name": "Gui-Song Xia"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Limited by this, existing TSP approaches can only handle table structure parsing in a relative simple scenario by grouping detected cells into tables [11, 16, 9, 23].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "For a more practical requirement of parsing table structures from images taken by hand-held cameras in the wild, the existing state-of-the-art approaches [13, 14, 11, 16, 9, 23] are prone to fail as the commonly-used assumption of tabular images no longer holds.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Recently, FinTabNet [23] and SciTSR [2] datasets add the cell coordinates and row-column information to become the most complete and large-scale dataset for table structure parsing task.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "However, these methods mainly focuses on the well-conditioned document images, where the tables [16, 23, 12, 9, 23, 14] are well-aligned to the image axes.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Moreover, they either rely on external pre-/post-processing methods to further refine their predictions [11,13] or incorporate memory intensive deformable convolutions [12,20]."
            ],
            "citingPaper": {
                "paperId": "d10d8164e472864968725424d2195789fb5e67bb",
                "externalIds": {
                    "PubMedCentral": "8540682",
                    "MAG": "3197383755",
                    "DBLP": "journals/jimaging/HashmiPLSA21",
                    "DOI": "10.3390/jimaging7100214",
                    "CorpusId": 239202542,
                    "PubMed": "34677300"
                },
                "corpusId": 239202542,
                "publicationVenue": {
                    "id": "c0fc53c7-b0ed-487d-9191-1262c8322621",
                    "name": "Journal of Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "J Imaging"
                    ],
                    "issn": "2313-433X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-556372",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jimaging",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-556372"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d10d8164e472864968725424d2195789fb5e67bb",
                "title": "CasTabDetectoRS: Cascade Network for Table Detection in Document Images with Recursive Feature Pyramid and Switchable Atrous Convolution",
                "abstract": "Table detection is a preliminary step in extracting reliable information from tables in scanned document images. We present CasTabDetectoRS, a novel end-to-end trainable table detection framework that operates on Cascade Mask R-CNN, including Recursive Feature Pyramid network and Switchable Atrous Convolution in the existing backbone architecture. By utilizing a comparativelyightweight backbone of ResNet-50, this paper demonstrates that superior results are attainable without relying on pre- and post-processing methods, heavier backbone networks (ResNet-101, ResNeXt-152), and memory-intensive deformable convolutions. We evaluate the proposed approach on five different publicly available table detection datasets. Our CasTabDetectoRS outperforms the previous state-of-the-art results on four datasets (ICDAR-19, TableBank, UNLV, and Marmot) and accomplishes comparable results on ICDAR-17 POD. Upon comparing with previous state-of-the-art results, we obtain a significant relative error reduction of 56.36%, 20%, 4.5%, and 3.5% on the datasets of ICDAR-19, TableBank, UNLV, and Marmot, respectively. Furthermore, this paper sets a new benchmark by performing exhaustive cross-datasets evaluations to exhibit the generalization capabilities of the proposed method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1771057",
                        "name": "A. Pagani"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Moreover, they either rely on external pre-/post-processing methods to further refine their predictions [11,13] or incorporate memory intensive deformable convolutions [12,20].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "We develop FINQA based on the publicly available earnings reports of S&P 500 companies from 1999 to 2019, collected in the FinTabNet dataset (Zheng et al., 2021).",
                "Eleven finance professionals collectively constructed FINQA based on the earnings reports of S&P 500 companies (Zheng et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "99053e3a708fc27709c9dab33110dc98b187c158",
                "externalIds": {
                    "ArXiv": "2109.00122",
                    "DBLP": "journals/corr/abs-2109-00122",
                    "ACL": "2021.emnlp-main.300",
                    "DOI": "10.18653/v1/2021.emnlp-main.300",
                    "CorpusId": 235399966
                },
                "corpusId": 235399966,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/99053e3a708fc27709c9dab33110dc98b187c158",
                "title": "FinQA: A Dataset of Numerical Reasoning over Financial Data",
                "abstract": "The sheer volume of financial statements makes it difficult for humans to access and analyze a business\u2019s financials. Robust numerical reasoning likewise faces unique challenges in this domain. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the finance domain includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, FinQA, with Question-Answering pairs over Financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce baselines and conduct comprehensive experiments in our dataset. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset \u2013 the first of its kind \u2013 should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available at https://github.com/czyssrs/FinQA.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Zhiyu Chen"
                    },
                    {
                        "authorId": "2109664620",
                        "name": "Wenhu Chen"
                    },
                    {
                        "authorId": "37722032",
                        "name": "Charese Smiley"
                    },
                    {
                        "authorId": "36532736",
                        "name": "Sameena Shah"
                    },
                    {
                        "authorId": "2125334899",
                        "name": "Iana Borova"
                    },
                    {
                        "authorId": "2125329151",
                        "name": "Dylan Langdon"
                    },
                    {
                        "authorId": "2125329600",
                        "name": "Reema N Moussa"
                    },
                    {
                        "authorId": "67048282",
                        "name": "Matthew I. Beane"
                    },
                    {
                        "authorId": "144188081",
                        "name": "Ting-Hao 'Kenneth' Huang"
                    },
                    {
                        "authorId": "3087875",
                        "name": "Bryan R. Routledge"
                    },
                    {
                        "authorId": "152876475",
                        "name": "W. Wang"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "We develop FINQA based on the publicly available earnings reports of S&P 500 companies from 1999 to 2019, collected in the FinTabNet dataset (Zheng et al., 2021).",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Eleven finance professionals collectively constructed FINQA based on the earnings reports of S&P 500 companies (Zheng et al., 2021).",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "This diversity should help using the trained models in other domains, which could be evaluated using new data sets generated for other domains such as FinTabNet [15] for the financial domain."
            ],
            "citingPaper": {
                "paperId": "9c567778502d2b44c3bdbce6ad5de54e01a41f33",
                "externalIds": {
                    "ArXiv": "2106.14616",
                    "DBLP": "conf/icdar/Jimeno-YepesZB21",
                    "DOI": "10.1007/978-3-030-86337-1_40",
                    "CorpusId": 235658670
                },
                "corpusId": 235658670,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/9c567778502d2b44c3bdbce6ad5de54e01a41f33",
                "title": "ICDAR 2021 Competition on Scientific Literature Parsing",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1399097376",
                        "name": "Antonio Jimeno-Yepes"
                    },
                    {
                        "authorId": "2113309578",
                        "name": "Xu Zhong"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "This diversity should help using the trained models in other domains, which could be evaluated using new data sets generated for other domains such as FinTabNet [15] for the financial domain.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For table extraction, we use our Global Table Extractor (GTE) (Zheng et al. 2020), which leverages specialized object detection models and clustering techniques to extract, for each table, both its bounding box and cell structure."
            ],
            "citingPaper": {
                "paperId": "759314189ecdffdc979deebf334646e847d93b1b",
                "externalIds": {
                    "DBLP": "conf/aaai/FaucegliaCGLWBM21",
                    "MAG": "3175051584",
                    "DOI": "10.1609/aaai.v35i18.18002",
                    "CorpusId": 235363687
                },
                "corpusId": 235363687,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/759314189ecdffdc979deebf334646e847d93b1b",
                "title": "KAAPA: Knowledge Aware Answers from PDF Analysis",
                "abstract": "We present KaaPa (Knowledge Aware Answers from Pdf Analysis), an integrated solution for machine reading comprehension over both text and tables extracted from PDFs. KaaPa enables interactive question refinement using facets generated from an automatically induced Knowledge Graph. In addition it provides a concise summary of the supporting evidence for the provided answers by aggregating information across multiple sources. KaaPa can be applied consistently to any collection of documents in English with zero domain adaptation effort. We showcase the use of KaaPa for QA on scientific literature using the COVID-19 Open Research Dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107085016",
                        "name": "Nicolas R. Fauceglia"
                    },
                    {
                        "authorId": "1888104",
                        "name": "Mustafa Canim"
                    },
                    {
                        "authorId": "1711133",
                        "name": "A. Gliozzo"
                    },
                    {
                        "authorId": "50685486",
                        "name": "Jennifer J. Liang"
                    },
                    {
                        "authorId": "31856827",
                        "name": "N. Wang"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    },
                    {
                        "authorId": "2689774",
                        "name": "Nandana Mihindukulasooriya"
                    },
                    {
                        "authorId": "2879453",
                        "name": "Vittorio Castelli"
                    },
                    {
                        "authorId": "2169082",
                        "name": "Guy Feigenblat"
                    },
                    {
                        "authorId": "1775524",
                        "name": "D. Konopnicki"
                    },
                    {
                        "authorId": "2208580",
                        "name": "Yannis Katsis"
                    },
                    {
                        "authorId": "1707117",
                        "name": "Radu Florian"
                    },
                    {
                        "authorId": "1573872877",
                        "name": "Yunyao Li"
                    },
                    {
                        "authorId": "1781292",
                        "name": "S. Roukos"
                    },
                    {
                        "authorId": "2707234",
                        "name": "Avirup Sil"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "For table extraction, we use our Global Table Extractor (GTE) (Zheng et al. 2020), which leverages specialized object detection models and clustering techniques to extract, for each table, both its bounding box and cell structure.",
                    "label_score": 0.5,
                    "label": "Weak"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "After that, a group of methods [36,23,38] tries to recover the cell relations based on some heuristic rules and algorithms.",
                "Another group of methods solves the above problems in a bottom-up way to firstly detect the text blocks\u2019 positions and then recover the bounding-boxes\u2019 relations by heuristic rules [38] or GNN(Graph Neural Networks) [29,14,2,24,26]."
            ],
            "citingPaper": {
                "paperId": "ea25282d27368d3d04db91b165b5003d63e335d6",
                "externalIds": {
                    "DBLP": "conf/icdar/QiaoLCZPNRT021",
                    "ArXiv": "2105.06224",
                    "DOI": "10.1007/978-3-030-86549-8_7",
                    "CorpusId": 234482682
                },
                "corpusId": 234482682,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/ea25282d27368d3d04db91b165b5003d63e335d6",
                "title": "LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2065512194",
                        "name": "Liang Qiao"
                    },
                    {
                        "authorId": "2109614687",
                        "name": "Zaisheng Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    },
                    {
                        "authorId": "2151333065",
                        "name": "Peng Zhang"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    },
                    {
                        "authorId": "1490934795",
                        "name": "Yi Niu"
                    },
                    {
                        "authorId": "144850642",
                        "name": "Wenqi Ren"
                    },
                    {
                        "authorId": "51151126",
                        "name": "Wenming Tan"
                    },
                    {
                        "authorId": "144894837",
                        "name": "Fei Wu"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "After that, a group of methods [36,23,38] tries to recover the cell relations based on some heuristic rules and algorithms.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Another group of methods solves the above problems in a bottom-up way to firstly detect the text blocks\u2019 positions and then recover the bounding-boxes\u2019 relations by heuristic rules [38] or GNN(Graph Neural Networks) [29,14,2,24,26].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "To find out, we perform clustering experiments on the FinTabNet dataset from Zheng et al. (2021)."
            ],
            "citingPaper": {
                "paperId": "386bfd0e411dee4f512a8737c55dd84846981182",
                "externalIds": {
                    "DBLP": "conf/naacl/IidaTMI21",
                    "ACL": "2021.naacl-main.270",
                    "ArXiv": "2105.02584",
                    "MAG": "3158303960",
                    "DOI": "10.18653/V1/2021.NAACL-MAIN.270",
                    "CorpusId": 233864627
                },
                "corpusId": 233864627,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/386bfd0e411dee4f512a8737c55dd84846981182",
                "title": "TABBIE: Pretrained Representations of Tabular Data",
                "abstract": "Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model\u2019s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49093685",
                        "name": "H. Iida"
                    },
                    {
                        "authorId": "2064894364",
                        "name": "Dung Ngoc Thai"
                    },
                    {
                        "authorId": "1977256",
                        "name": "Varun Manjunatha"
                    },
                    {
                        "authorId": "2136562",
                        "name": "Mohit Iyyer"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "To find out, we perform clustering experiments on the FinTabNet dataset from Zheng et al. (2021).",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Applying the graph-based table representation for perceptional tasks, such as cell structure recognition [68] and functional block detection [29], is another meaningful direction."
            ],
            "citingPaper": {
                "paperId": "221ce6b97e4128ea3af592c885239367e48df095",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-01736",
                    "ArXiv": "2105.01736",
                    "DOI": "10.1145/3404835.3462909",
                    "CorpusId": 233739808
                },
                "corpusId": 233739808,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/221ce6b97e4128ea3af592c885239367e48df095",
                "title": "Retrieving Complex Tables with Multi-Granular Graph Representation Learning",
                "abstract": "The task of natural language table retrieval (NLTR) seeks to retrieve semantically relevant tables based on natural language queries. Existing learning systems for this task often treat tables as plain text based on the assumption that tables are structured as dataframes. However, tables can have complex layouts which indicate diverse dependencies between subtable structures, such as nested headers. As a result, queries may refer to different spans of relevant content that is distributed across these structures. Moreover, such systems fail to generalize to novel scenarios beyond those seen in the training set. Prior methods are still distant from a generalizable solution to the NLTR problem, as they fall short in handling complex table layouts or queries over multiple granularities. To address these issues, we propose Graph-based Table Retrieval (GTR), a generalizable NLTR framework with multi-granular graph representation learning. In our framework, a table is first converted into a tabular graph, with cell nodes, row nodes and column nodes to capture content at different granularities. Then the tabular graph is input to a Graph Transformer model that can capture both table cell content and the layout structures. To enhance the robustness and generalizability of the model, we further incorporate a self-supervised pre-training task based on graph-context matching. Experimental results on two benchmarks show that our method leads to significant improvements over the current state-of-the-art systems. Further experiments demonstrate promising performance of our method on cross-dataset generalization, and enhanced capability of handling complex tables and fulfilling diverse query intents.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47939052",
                        "name": "Fei Wang"
                    },
                    {
                        "authorId": "35329068",
                        "name": "Kexuan Sun"
                    },
                    {
                        "authorId": "1998918",
                        "name": "Muhao Chen"
                    },
                    {
                        "authorId": "2634786",
                        "name": "J. Pujara"
                    },
                    {
                        "authorId": "144171096",
                        "name": "Pedro A. Szekely"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Applying the graph-based table representation for perceptional tasks, such as cell structure recognition [68] and functional block detection [29], is another meaningful direction.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[52] published a framework for both the detection and structure recognition of tables in document images.",
                "Recently, the problem of table structure recognition has been evaluated on the precise prediction of cellular boundaries in a tabular image [48], [52], [94].",
                "[52] is an end-to-end framework that not only detects the tables but recognizes the structures of tables in document images."
            ],
            "citingPaper": {
                "paperId": "bc8863440e26e48866a64f63438051d24880b793",
                "externalIds": {
                    "DBLP": "journals/access/HashmiLSAAA21",
                    "ArXiv": "2104.14272",
                    "DOI": "10.1109/ACCESS.2021.3087865",
                    "CorpusId": 233444247
                },
                "corpusId": 233444247,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bc8863440e26e48866a64f63438051d24880b793",
                "title": "Current Status and Performance Analysis of Table Recognition in Document Images With Deep Neural Networks",
                "abstract": "The first phase of table recognition is to detect the tabular area in a document. Subsequently, the tabular structures are recognized in the second phase in order to extract information from the respective cells. Table detection and structural recognition are pivotal problems in the domain of table understanding. However, table analysis is a perplexing task due to the colossal amount of diversity and asymmetry in tables. Therefore, it is an active area of research in document image analysis. Recent advances in the computing capabilities of graphical processing units have enabled the deep neural networks to outperform traditional state-of-the-art machine learning methods. Table understanding has substantially benefited from the recent breakthroughs in deep neural networks. However, there has not been a consolidated description of the deep learning methods for table detection and table structure recognition. This review paper provides a thorough analysis of the modern methodologies that utilize deep neural networks. Moreover, it presents a comprehensive understanding of the current state-of-the-art and related challenges of table understanding in document images. The leading datasets and their intricacies have been elaborated along with the quantitative results. Furthermore, a brief overview is given regarding the promising directions that can further improve table analysis in document images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "2206035070",
                        "name": "Muhammad Adnan Afzal"
                    },
                    {
                        "authorId": "2206035072",
                        "name": "Muhammad Ahtsham Afzal"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "[52] published a framework for both the detection and structure recognition of tables in document images.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Recently, the problem of table structure recognition has been evaluated on the precise prediction of cellular boundaries in a tabular image [48], [52], [94].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "[52] is an end-to-end framework that not only detects the tables but recognizes the structures of tables in document images.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "There are a few existing large-scale datasets for scientific papers [9] and financial reports [8] but many documents in business are confidential.",
                "To begin, we apply our table extraction module (based on the GTE framework [8]) to the user\u2019s document collection."
            ],
            "citingPaper": {
                "paperId": "9dbb26e4f04f38aa2a11289cbc82a2d9ed533741",
                "externalIds": {
                    "ArXiv": "2102.08445",
                    "DBLP": "journals/corr/abs-2102-08445",
                    "DOI": "10.1145/3397482.3450718",
                    "CorpusId": 231942681
                },
                "corpusId": 231942681,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9dbb26e4f04f38aa2a11289cbc82a2d9ed533741",
                "title": "TableLab: An Interactive Table Extraction System with Adaptive Deep Learning",
                "abstract": "Table extraction from PDF and image documents is a ubiquitous task in the real-world. Perfect extraction quality is difficult to achieve with one single out-of-box model due to (1) the wide variety of table styles, (2) the lack of training data representing this variety and (3) the inherent ambiguity and subjectivity of table definitions between end-users. Meanwhile, building customized models from scratch can be difficult due to the expensive nature of annotating table data. We attempt to solve these challenges with TableLab by providing a system where users and models seamlessly work together to quickly customize high-quality extraction models with a few labelled examples for the user\u2019s document collection, which contains pages with tables. Given an input document collection, TableLab first detects tables with similar structures (templates) by clustering embeddings from the extraction model. Document collections often contain tables created with a limited set of templates or similar structures. It then selects a few representative table examples already extracted with a pre-trained base deep learning model. Via an easy-to-use user interface, users provide feedback to these selections without necessarily having to identify every single error. TableLab then applies such feedback to finetune the pre-trained model and returns the results of the finetuned model back to the user. The user can choose to repeat this process iteratively until obtaining a customized model with satisfactory performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31856827",
                        "name": "N. Wang"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    },
                    {
                        "authorId": "1718694",
                        "name": "Yunyao Li"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "There are a few existing large-scale datasets for scientific papers [9] and financial reports [8] but many documents in business are confidential.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "To begin, we apply our table extraction module (based on the GTE framework [8]) to the user\u2019s document collection.",
                    "label_score": 0.5,
                    "label": "Weak"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Paper metadata from these sources are harmonized, PDFs are converted into machinereadable JSON using the S2ORC pipeline described in [54] and HTML representations of tables in papers are added using IBM Watson Discovery\u2019s Global Table Extractor [115]."
            ],
            "citingPaper": {
                "paperId": "4370d0abff93011622cb3ba95373d716aa8ec7b0",
                "externalIds": {
                    "PubMedCentral": "7799291",
                    "DBLP": "journals/bib/WangL21",
                    "MAG": "3111412136",
                    "DOI": "10.1093/bib/bbaa296",
                    "CorpusId": 227524452,
                    "PubMed": "33279995"
                },
                "corpusId": 227524452,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4370d0abff93011622cb3ba95373d716aa8ec7b0",
                "title": "Text mining approaches for dealing with the rapidly expanding literature on COVID-19",
                "abstract": "Abstract More than 50 000 papers have been published about COVID-19 since the beginning of 2020 and several hundred new papers continue to be published every day. This incredible rate of scientific productivity leads to information overload, making it difficult for researchers, clinicians and public health officials to keep up with the latest findings. Automated text mining techniques for searching, reading and summarizing papers are helpful for addressing information overload. In this review, we describe the many resources that have been introduced to support text mining applications over the COVID-19 literature; specifically, we discuss the corpora, modeling resources, systems and shared tasks that have been introduced for COVID-19. We compile a list of 39 systems that provide functionality such as search, discovery, visualization and summarization over the COVID-19 literature. For each system, we provide a qualitative description and assessment of the system\u2019s performance, unique data or user interface features and modeling decisions. Many systems focus on search and discovery, though several systems provide novel features, such as the ability to summarize findings over multiple documents or linking between scientific articles and clinical trials. We also describe the public corpora, models and shared tasks that have been introduced to help reduce repeated effort among community members; some of these resources (especially shared tasks) can provide a basis for comparing the performance of different systems. Finally, we summarize promising results and open challenges for text mining the COVID-19 literature.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "31860505",
                        "name": "Lucy Lu Wang"
                    },
                    {
                        "authorId": "46258841",
                        "name": "Kyle Lo"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Paper metadata from these sources are harmonized, PDFs are converted into machinereadable JSON using the S2ORC pipeline described in [54] and HTML representations of tables in papers are added using IBM Watson Discovery\u2019s Global Table Extractor [115].",
                    "label_score": 0.5,
                    "label": "Weak"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4a10dffca6dcce9c570cb75aa4d76522c34a2fd4",
                "externalIds": {
                    "PubMedCentral": "7251955",
                    "ArXiv": "2004.10706",
                    "MAG": "3020786614",
                    "DBLP": "journals/corr/abs-2004-10706",
                    "ACL": "2020.nlpcovid19-acl.1",
                    "CorpusId": 216056360,
                    "PubMed": "32510522"
                },
                "corpusId": 216056360,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4a10dffca6dcce9c570cb75aa4d76522c34a2fd4",
                "title": "CORD-19: The Covid-19 Open Research Dataset",
                "abstract": "The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "31860505",
                        "name": "Lucy Lu Wang"
                    },
                    {
                        "authorId": "46258841",
                        "name": "Kyle Lo"
                    },
                    {
                        "authorId": "1648642525",
                        "name": "Yoganand Chandrasekhar"
                    },
                    {
                        "authorId": "65983884",
                        "name": "Russell Reas"
                    },
                    {
                        "authorId": "82148460",
                        "name": "Jiangjiang Yang"
                    },
                    {
                        "authorId": "40329918",
                        "name": "Darrin Eide"
                    },
                    {
                        "authorId": "37996742",
                        "name": "Kathryn Funk"
                    },
                    {
                        "authorId": "143967880",
                        "name": "Rodney Michael Kinney"
                    },
                    {
                        "authorId": "2145253428",
                        "name": "Ziyang Liu"
                    },
                    {
                        "authorId": "143696607",
                        "name": "William Cooper Merrill"
                    },
                    {
                        "authorId": "115392299",
                        "name": "P. Mooney"
                    },
                    {
                        "authorId": "69437054",
                        "name": "D. Murdick"
                    },
                    {
                        "authorId": "1453742562",
                        "name": "Devvret Rishi"
                    },
                    {
                        "authorId": "2055678827",
                        "name": "J. Sheehan"
                    },
                    {
                        "authorId": "3303634",
                        "name": "Zhihong Shen"
                    },
                    {
                        "authorId": "1405473759",
                        "name": "Brandon Stilson"
                    },
                    {
                        "authorId": "1860983",
                        "name": "Alex D Wade"
                    },
                    {
                        "authorId": "1748169",
                        "name": "Kuansan Wang"
                    },
                    {
                        "authorId": "46212260",
                        "name": "Christopher Wilhelm"
                    },
                    {
                        "authorId": "2064542611",
                        "name": "Boya Xie"
                    },
                    {
                        "authorId": "21811471",
                        "name": "Douglas A. Raymond"
                    },
                    {
                        "authorId": "1780531",
                        "name": "Daniel S. Weld"
                    },
                    {
                        "authorId": "1741101",
                        "name": "Oren Etzioni"
                    },
                    {
                        "authorId": "41018147",
                        "name": "Sebastian Kohlmeier"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6c411f52c151fe537ee76c310f45f0dd3da88e92",
                "externalIds": {
                    "DBLP": "series/sbcs/Bhowmik23",
                    "DOI": "10.1007/978-981-99-4277-0",
                    "CorpusId": 260337124
                },
                "corpusId": 260337124,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6c411f52c151fe537ee76c310f45f0dd3da88e92",
                "title": "Document Layout Analysis",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3272848",
                        "name": "Showmik Bhowmik"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "This tool is available online.12 For table extraction, Zheng et al (2021) proposed Global Table Extractor (GTE) to detect tables and recognize cell structure jointly based on visual context."
            ],
            "citingPaper": {
                "paperId": "c84e662e81b9ea0a532e26fea2187d59d4bf8e61",
                "externalIds": {
                    "CorpusId": 248405703
                },
                "corpusId": 248405703,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c84e662e81b9ea0a532e26fea2187d59d4bf8e61",
                "title": "Automated scholarly paper review: Technologies and challenges",
                "abstract": "Peer review is a widely accepted mechanism for research evaluation, playing a pivotal role in scholarly publishing. However, criticisms have long been leveled on this mechanism, mostly because of its ine\ufb03ciency and subjectivity. Recent years have seen the application of arti\ufb01cial intelligence (AI) in assisting the peer review process. Nonetheless, with the involvement of humans, such limitations remain inevitable. In this review paper, we propose the concept and pipeline of automated scholarly paper review (ASPR) and review the relevant literature and technologies of achieving a full-scale computerized review process. On the basis of the review and discussion, we conclude that there is already corresponding research and implementation at each stage of ASPR. We further look into the challenges in ASPR with the existing technologies. The major di\ufb03culties lie in imperfect document parsing and representation, inadequate data, defective human-computer interaction and \ufb02awed deep logical reasoning. Moreover, we discuss the possible moral & ethical issues and point out the future directions of ASPR. In the foreseeable future, ASPR and peer review will coexist in a reinforcing manner before ASPR is able to fully undertake the reviewing workload from humans.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1651633605",
                        "name": "Jialiang Lin"
                    },
                    {
                        "authorId": "2108456439",
                        "name": "Jiaxin Song"
                    },
                    {
                        "authorId": "2142138142",
                        "name": "Zhangping Zhou"
                    },
                    {
                        "authorId": "47558200",
                        "name": "Yidong Chen"
                    },
                    {
                        "authorId": "114438707",
                        "name": "Xiaodon Shi"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "This tool is available online.12 For table extraction, Zheng et al (2021) proposed Global Table Extractor (GTE) to detect tables and recognize cell structure jointly based on visual context.",
                    "label_score": 0.5,
                    "label": "Weak"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[36] introduced an end-to-end framework that not only detected but also recognized table structures in document images."
            ],
            "citingPaper": {
                "paperId": "826b48bd521751bc147db32e52f55c0207fa82a3",
                "externalIds": {
                    "DBLP": "journals/access/NguyenNVN22",
                    "DOI": "10.1109/ACCESS.2022.3211069",
                    "CorpusId": 252660247
                },
                "corpusId": 252660247,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/826b48bd521751bc147db32e52f55c0207fa82a3",
                "title": "Vietnamese Document Analysis: Dataset, Method and Benchmark Suite",
                "abstract": "Document image understanding is increasingly useful since the number of digital documents is increasing day-by-day and the need for automation is increasing. Object detection plays a significant role in detecting vital objects and layouts in document images and contributes to providing a clearer understanding of the documents. Nonetheless, previous research mainly focuses on English document images, and studies on Vietnamese document images are limited. In this study, we extensively benchmark state-of-the-art object detectors and analyze the performance of each method on Vietnamese document images. Moreover, we also investigate the effectiveness of four different loss functions on the experimental object detection methods. Extensive experiments on the UIT-DODV dataset are conducted to provide insightful discussions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1399684830",
                        "name": "Khang Nguyen"
                    },
                    {
                        "authorId": null,
                        "name": "An Nguyen"
                    },
                    {
                        "authorId": "2198372",
                        "name": "Nguyen D. Vo"
                    },
                    {
                        "authorId": "34646933",
                        "name": "Tam V. Nguyen"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "[36] introduced an end-to-end framework that not only detected but also recognized table structures in document images.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "44220fb755ed8259a6d1183e47e5e957ab5b0874",
                "externalIds": {
                    "CorpusId": 253857301
                },
                "corpusId": 253857301,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/44220fb755ed8259a6d1183e47e5e957ab5b0874",
                "title": "Relative Layout Matching for Document Data Extraction",
                "abstract": "This thesis explores the field of business document information extraction, emphasizing one-shot learning systems that improve their performance by utilizing a database of previously processed documents. A benchmark to evaluate one-shot information extraction systems was defined and used with a newly created dataset. A novel representation-learning approach to one-shot document information extraction was proposed. For a newly received document, the proposed approach uses learned document representation to first retrieve field representations from similar documents. Retrieved representations are then used to localize information on the newly received document. The proposed method was evaluated and compared against several proposed baselines showing an improvement on fields with high positional variance. The baseline method still achieves better results on fields that remain fixed within the layout.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071515342",
                        "name": "Maty\u00e1s Skalick\u00fd"
                    }
                ]
            },
            "context_scores": []
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "MULTIHIERTT (Zhao et al., 2022), which is also based on FinTabNet, combines the challenges of the above-mentioned datasets, bringing together complex tabular structures and hybrid table/text contexts.",
                "companies that were released as part of FinTabNet (Zheng et al., 2021).",
                "FinQA (Chen et al., 2021) is based on a collection of financial reports published by U.S. companies that were released as part of FinTabNet (Zheng et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "834fd676114c0b6e077c735ddffd9fa14ee856d2",
                "externalIds": {
                    "ACL": "2022.emnlp-main.125",
                    "DBLP": "conf/emnlp/NourbakhshJSR22",
                    "DOI": "10.18653/v1/2022.emnlp-main.125",
                    "CorpusId": 256461049
                },
                "corpusId": 256461049,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/834fd676114c0b6e077c735ddffd9fa14ee856d2",
                "title": "Improving compositional generalization for multi-step quantitative reasoning in question answering",
                "abstract": "Quantitative reasoning is an important aspect of question answering, especially when numeric and verbal cues interact to indicate sophisticated, multi-step programs. In this paper, we demonstrate how modeling the compositional nature of quantitative text can enhance the performance and robustness of QA models, allowing them to capture arithmetic logic that is expressed verbally. Borrowing from the literature on semantic parsing, we propose a method that encourages the QA models to adjust their attention patterns and capture input/output alignments that are meaningful to the reasoning task. We show how this strategy improves program accuracy and renders the models more robust against overfitting as the number of reasoning steps grows. Our approach is designed as a standalone module which can be prepended to many existing models and trained in an end-to-end fashion without the need for additional supervisory signal. As part of this exercise, we also create a unified dataset building on four previously released numerical QA datasets over tabular data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144369",
                        "name": "Armineh Nourbakhsh"
                    },
                    {
                        "authorId": "2064549240",
                        "name": "Cathy Jiao"
                    },
                    {
                        "authorId": "36532736",
                        "name": "Sameena Shah"
                    },
                    {
                        "authorId": "35959897",
                        "name": "C. Ros\u00e9"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "MULTIHIERTT (Zhao et al., 2022), which is also based on FinTabNet, combines the challenges of the above-mentioned datasets, bringing together complex tabular structures and hybrid table/text contexts.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "companies that were released as part of FinTabNet (Zheng et al., 2021).",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "FinQA (Chen et al., 2021) is based on a collection of financial reports published by U.S. companies that were released as part of FinTabNet (Zheng et al., 2021).",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "TableBank[4] Image 145k X SciTSR[16] Image 15k X X PubTabNet[3, 11] Image 568k X X X\u2020 FinTabNet[11] Image, PDF 113k X X X\u2020",
                "Datasets Several large datasets have been introduced recently for TE [17, 18, 4, 3, 11].",
                "Among previous datasets, PubTabNet is the largest with 568k tables, although no test set has been released for benchmarking.",
                "PubTabNet, for example, is created using an automated alignment procedure [18] to match the same table in unaligned pairs of PDF and XML versions of the same scientific articles from the PMCOA database.",
                "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] for TE to data-driven methods based on deep learning (DL) [2, 10, 11].",
                "In terms of usability, FinTabNet is the most widely applicable, as it annotates source PDF documents rather than rendered images; and both FinTabNet and the updated version of PubTabNet [11] have the most complete annotations, as they both contain location information for cells.",
                "However, both FinTabNet and PubTabNet are missing bounding boxes for rows, columns, and blank cells; and in these datasets a cell\u2019s bounding box covers only its text portion, which ignores the role of the non-text portion of the cell.",
                "We hope to apply canonicalization to data from additional domains, such as the financial documents in FinTabNet.",
                "Recent datasets for table structure recognition (TSR) [4, 3, 11], while large, have several limitations, including in some cases lack of location information for cells, compatibility with only specific model architectures, and lack of guarantees for data quality and consistency."
            ],
            "citingPaper": {
                "paperId": "17119b49d68800bbc57bbd786ca339917c6f00fa",
                "externalIds": {
                    "CorpusId": 238253171
                },
                "corpusId": 238253171,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/17119b49d68800bbc57bbd786ca339917c6f00fa",
                "title": "Scientific evidence extraction",
                "abstract": "Recently, interest has grown in applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, progress in this area has been challenging both to make and to measure, due to several issues that arise in training and evaluating models from labeled data. This includes challenges as fundamental as the lack of a single definitive ground truth output for each input sample and the lack of an ideal metric for measuring partial correctness for this task. To address these we propose a new dataset, PubMed Tables One Million (PubTables-1M), and a new class of metric, grid table similarity (GriTS). PubTables-1M is nearly twice as large as the previous largest comparable dataset, can be used for models across multiple architectures and modalities, and addresses issues such as ambiguity and lack of consistency in the annotations. We apply DETR [1] to table extraction for the first time and show that object detection models trained on PubTables-1M produce excellent results out-of-the-box for all three tasks of detection, structure recognition, and functional analysis. We describe the dataset in detail to enable others to build on our work and combine this data with other datasets for these and related tasks. It is our hope that PubTables-1M and the proposed metrics can further progress in this area by creating a benchmark suitable for training and evaluating a wide variety of models for table extraction. Data and code will be released at https: //github.com/microsoft/table-transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2394344",
                        "name": "Robin Abraham"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "TableBank[4] Image 145k X SciTSR[16] Image 15k X X PubTabNet[3, 11] Image 568k X X X\u2020 FinTabNet[11] Image, PDF 113k X X X\u2020",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Datasets Several large datasets have been introduced recently for TE [17, 18, 4, 3, 11].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Among previous datasets, PubTabNet is the largest with 568k tables, although no test set has been released for benchmarking.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "PubTabNet, for example, is created using an automated alignment procedure [18] to match the same table in unaligned pairs of PDF and XML versions of the same scientific articles from the PMCOA database.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] for TE to data-driven methods based on deep learning (DL) [2, 10, 11].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "In terms of usability, FinTabNet is the most widely applicable, as it annotates source PDF documents rather than rendered images; and both FinTabNet and the updated version of PubTabNet [11] have the most complete annotations, as they both contain location information for cells.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "However, both FinTabNet and PubTabNet are missing bounding boxes for rows, columns, and blank cells; and in these datasets a cell\u2019s bounding box covers only its text portion, which ignores the role of the non-text portion of the cell.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We hope to apply canonicalization to data from additional domains, such as the financial documents in FinTabNet.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Recent datasets for table structure recognition (TSR) [4, 3, 11], while large, have several limitations, including in some cases lack of location information for cells, compatibility with only specific model architectures, and lack of guarantees for data quality and consistency.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "TableBank[4] Image 145k X SciTSR[16] Image 15k X X PubTabNet[3, 11] Image 568k X X X\u2020 FinTabNet[11] Image, PDF 113k X X X\u2020",
                "Datasets Several large datasets have been introduced recently for TE [17, 18, 4, 3, 11].",
                "Among previous datasets, PubTabNet is the largest with 568k tables, although no test set has been released for benchmarking.",
                "PubTabNet, for example, is created using an automated alignment procedure [18] to match the same table in unaligned pairs of PDF and XML versions of the same scientific articles from the PMCOA database.",
                "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] for TE to data-driven methods based on deep learning (DL) [2, 10, 11].",
                "In terms of usability, FinTabNet is the most widely applicable, as it annotates source PDF documents rather than rendered images; and both FinTabNet and the updated version of PubTabNet [11] have the most complete annotations, as they both contain location information for cells.",
                "However, both FinTabNet and PubTabNet are missing bounding boxes for rows, columns, and blank cells; and in these datasets a cell\u2019s bounding box covers only its text portion, which ignores the role of the non-text portion of the cell.",
                "We hope to apply canonicalization to data from additional domains, such as the financial documents in FinTabNet.",
                "Recent datasets for table structure recognition (TSR) [4, 3, 11], while large, have several limitations, including in some cases lack of location information for cells, compatibility with only specific model architectures, and lack of guarantees for data quality and consistency."
            ],
            "citingPaper": {
                "paperId": "2773c7621402cf014b9871796f0e3ba788a95f60",
                "externalIds": {
                    "CorpusId": 238634624
                },
                "corpusId": 238634624,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2773c7621402cf014b9871796f0e3ba788a95f60",
                "title": "PubTables-1M: Towards a universal dataset and metrics for training and evaluating table extraction models",
                "abstract": "Recently, interest has grown in applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, progress in this area has been challenging both to make and to measure, due to several issues that arise in training and evaluating models from labeled data. This includes challenges as fundamental as the lack of a single definitive ground truth output for each input sample and the lack of an ideal metric for measuring partial correctness for this task. To address these issues we propose a new dataset, PubMed Tables One Million (PubTables-1M), and a new class of metric, grid table similarity (GriTS). PubTables-1M is nearly twice as large as the previous largest comparable dataset, contains highly-detailed structure annotations, and can be used for models across multiple architectures and modalities. Further, it addresses issues such as ambiguity and lack of consistency in the annotations via a novel canonicalization and quality control procedure. We apply DETR [1] to table extraction for the first time and show that object detection models trained on PubTables-1M produce excellent results out-of-the-box for all three tasks of detection, structure recognition, and functional analysis. It is our hope that PubTables-1M and GriTS can further progress in this area by creating data and metrics suitable for training and evaluating a wide variety of models for table extraction. Data and code will be released at https://github.com/microsoft/table-transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2394344",
                        "name": "Robin Abraham"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "TableBank[4] Image 145k X SciTSR[16] Image 15k X X PubTabNet[3, 11] Image 568k X X X\u2020 FinTabNet[11] Image, PDF 113k X X X\u2020",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Datasets Several large datasets have been introduced recently for TE [17, 18, 4, 3, 11].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Among previous datasets, PubTabNet is the largest with 568k tables, although no test set has been released for benchmarking.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "PubTabNet, for example, is created using an automated alignment procedure [18] to match the same table in unaligned pairs of PDF and XML versions of the same scientific articles from the PMCOA database.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] for TE to data-driven methods based on deep learning (DL) [2, 10, 11].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "In terms of usability, FinTabNet is the most widely applicable, as it annotates source PDF documents rather than rendered images; and both FinTabNet and the updated version of PubTabNet [11] have the most complete annotations, as they both contain location information for cells.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "However, both FinTabNet and PubTabNet are missing bounding boxes for rows, columns, and blank cells; and in these datasets a cell\u2019s bounding box covers only its text portion, which ignores the role of the non-text portion of the cell.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "We hope to apply canonicalization to data from additional domains, such as the financial documents in FinTabNet.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Recent datasets for table structure recognition (TSR) [4, 3, 11], while large, have several limitations, including in some cases lack of location information for cells, compatibility with only specific model architectures, and lack of guarantees for data quality and consistency.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Name Format # Tables Cell Topology Cell Content Cell Location Canonical Ground Truth TableBank[4] Image 145k X SciTSR[16] Image 15k X X PubTabNet[3] Image 568k X X FinTabNet[11] Image, PDF 113k X X X",
                "Recent datasets for table structure recognition (TSR) [4, 3, 11], while large, have several limitations, 40 including in some cases missing cell-level location information, compatibility with only specific 41 model architectures, and lack of guarantees for data quality and consistency.",
                "Datasets Several large datasets have been introduced recently for table extraction [17, 18, 4, 3, 11].",
                "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] 34 for table extraction to data-driven methods based on deep learning (DL) [2, 10, 11]."
            ],
            "citingPaper": {
                "paperId": "4f723c06164b3eacfb6787e9cd732e04a31e5a93",
                "externalIds": {
                    "CorpusId": 259311838
                },
                "corpusId": 259311838,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4f723c06164b3eacfb6787e9cd732e04a31e5a93",
                "title": "Towards a universal dataset and metrics for training and evaluating table extraction models",
                "abstract": "Recently, interest has grown in applying machine learning approaches to the 1 problem of table structure inference and extraction from unstructured documents. 2 However, progress in this area has been challenging not only to make but to 3 measure, due to several issues that arise in both training and evaluating such 4 systems from labeled data. This includes challenges as fundamental as the lack of 5 a single definitive ground truth output for a given input sample and the lack of an 6 ideal metric for measuring partial correctness for this task. To address these we 7 propose a new dataset, PubMed Tables One Million (PubTables1M), and a new 8 class of metric, grid table similarity (GriTS). PubTables1M is nearly twice as large 9 as the current largest comparable dataset, can be used for models across multiple 10 architectures and modalities, and addresses issues such as ambiguity and lack of 11 consistency in the annotations. We apply DETR [1] to table extraction for the first 12 time and show that object detection models trained on images and bounding boxes 13 derived from this data produce excellent results out-of-the-box for all three tasks of 14 detection, structure recognition, and functional analysis. In addition to releasing 15 the data, we describe the dataset creation process in detail to enable others to build 16 on our work and to ensure forward and backward compatibility of this data for 17 combining it with other datasets created for these tasks. It is our hope that this data 18 and the proposed metrics can further progress in this area by serving as a single 19 source of data for training and evaluation of a wide variety of models for table 20 extraction. 21",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51981031",
                        "name": "Microsoft"
                    },
                    {
                        "authorId": "2063070799",
                        "name": "Redmond"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "Name Format # Tables Cell Topology Cell Content Cell Location Canonical Ground Truth TableBank[4] Image 145k X SciTSR[16] Image 15k X X PubTabNet[3] Image 568k X X FinTabNet[11] Image, PDF 113k X X X",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Recent datasets for table structure recognition (TSR) [4, 3, 11], while large, have several limitations, 40 including in some cases missing cell-level location information, compatibility with only specific 41 model architectures, and lack of guarantees for data quality and consistency.",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Datasets Several large datasets have been introduced recently for table extraction [17, 18, 4, 3, 11].",
                    "label_score": 0.0,
                    "label": "Neutral"
                },
                {
                    "context": "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] 34 for table extraction to data-driven methods based on deep learning (DL) [2, 10, 11].",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "This diversity should help using the trained models in other domains, which could be evaluated using new data sets generated for other domains such as FinTabNet [15] for the financial domain."
            ],
            "citingPaper": {
                "paperId": "5e2c5df2be48c08f0cf4fc8d72c013c0b98f53b2",
                "externalIds": {
                    "CorpusId": 235692898
                },
                "corpusId": 235692898,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5e2c5df2be48c08f0cf4fc8d72c013c0b98f53b2",
                "title": "J un 2 02 1 ICDAR 2021 Competition on Scientific Literature Parsing",
                "abstract": "Scientific literature contain important information related to cutting-edge innovations in diverse domains. Advances in natural language processing have been driving the fast development in automated information extraction from scientific literature. However, scientific literature is often available in unstructured PDF format. While PDF is great for preserving basic visual elements, such as characters, lines, shapes, etc., on a canvas for presentation to humans, automatic processing of the PDF format by machines presents many challenges. With over 2.5 trillion PDF documents in existence, these issues are prevalent in many other important application domains as well. A critical challenge for automated information extraction from scientific literature is that documents often contain content that is not in natural language, such as figures and tables. Nevertheless, such content usually illustrates key results, messages, or summarizations of the research. To obtain a comprehensive understanding of scientific literature, the automated system must be able to recognize the layout of the documents and parse the non-natural-language content into a machine readable format. Our ICDAR 2021 Scientific Literature Parsing Competition (ICDAR2021SLP) aims to drive the advances specifically in document understanding. ICDAR2021-SLP leverages the PubLayNet and PubTabNet datasets, which provide hundreds of thousands of training and evaluation examples. In Task A, Document Layout Recognition, submissions with the highest performance combine object detection and specialised solutions for the different categories. In Task B, Table Recognition, top submissions rely on methods to identify table components and post-processing methods to generate the table structure and content. Results from both tasks show an impressive performance and opens the possibility for high performance practical applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35207071",
                        "name": "A. Yepes"
                    },
                    {
                        "authorId": "2062708279",
                        "name": "Peter Zhong"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "This diversity should help using the trained models in other domains, which could be evaluated using new data sets generated for other domains such as FinTabNet [15] for the financial domain.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2022 Auto-generated annotations: These statements are auto-generated using a random paraphraser and table understanding service (Zheng et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "736bebb9c6d4451486453e1dc4fe6726c99bb127",
                "externalIds": {
                    "ACL": "2021.semeval-1.182",
                    "DBLP": "conf/semeval/VarmaJRR21",
                    "DOI": "10.18653/v1/2021.semeval-1.182",
                    "CorpusId": 236459991
                },
                "corpusId": 236459991,
                "publicationVenue": {
                    "id": "70713d09-6e4b-4554-9d3f-94d08aba320c",
                    "name": "International Workshop on Semantic Evaluation",
                    "type": "conference",
                    "alternate_names": [
                        "SemEval ",
                        "Int Workshop Semantic Evaluation"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/736bebb9c6d4451486453e1dc4fe6726c99bb127",
                "title": "AttesTable at SemEval-2021 Task 9: Extending Statement Verification with Tables for Unknown Class, and Semantic Evidence Finding",
                "abstract": "This paper describes our approach for Task 9 of SemEval 2021: Statement Verification and Evidence Finding with Tables. We participated in both subtasks, namely statement verification and evidence finding. For the subtask of statement verification, we extend the TAPAS model to adapt to the \u2018unknown\u2019 class of statements by finetuning it on an augmented version of the task data. For the subtask of evidence finding, we finetune the DistilBERT model in a Siamese setting.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2121301789",
                        "name": "Harshit Varma"
                    },
                    {
                        "authorId": "51032401",
                        "name": "Aadish Jain"
                    },
                    {
                        "authorId": "1379948027",
                        "name": "Pratik Ratadiya"
                    },
                    {
                        "authorId": "2064168638",
                        "name": "Abhishek Rathi"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "\u2022 Auto-generated annotations: These statements are auto-generated using a random paraphraser and table understanding service (Zheng et al., 2020).",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026papers Author-written and expertderived TLDRs FacetSum (Meng et al, 2021) 60k articles from Emerald journals Paper and structured abstract\nFor table extraction, Zheng et al (2021) proposed Global Table Extractor (GTE) to detect tables and recognize cell structure jointly based on visual context."
            ],
            "citingPaper": {
                "paperId": "398a5dae26fdf80f76710eab6cfea10308210fe0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-07533",
                    "CorpusId": 244116975
                },
                "corpusId": 244116975,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/398a5dae26fdf80f76710eab6cfea10308210fe0",
                "title": "Automated scholarly paper review: Possibility and challenges",
                "abstract": "Peer review is a widely accepted mechanism for research evaluation, playing a pivotal role in scholarly publishing. However, criticisms have long been leveled on this mechanism, mostly because of its ine\ufb03ciency and subjectivity. Recent years have seen the application of arti\ufb01cial intelligence (AI) in assisting the peer review process. Nonetheless, with the involvement of humans, such limitations remain inevitable. In this review paper, we propose the concept of automated scholarly paper review (ASPR) and review the relevant literature and technologies to discuss the possibility of achieving a full-scale computerized review process. We further look into the challenges in ASPR with the existing technologies. On the basis of the review and discussion, we conclude that there are already corresponding research and technologies at each stage of ASPR. This veri\ufb01es that ASPR can be realized in the long term as the relevant technologies continue to develop. The major di\ufb03culties in its realization lie in imperfect document parsing and representation, inadequate data, defected human-computer interaction and \ufb02awed deep logical reasoning. In the foreseeable future, ASPR and peer review will coexist in a reinforcing manner before ASPR is able to fully undertake the reviewing workload from humans.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1651633605",
                        "name": "Jialiang Lin"
                    },
                    {
                        "authorId": "2108456439",
                        "name": "Jiaxin Song"
                    },
                    {
                        "authorId": "2142138142",
                        "name": "Zhangping Zhou"
                    },
                    {
                        "authorId": "1755321",
                        "name": "X. Shi"
                    }
                ]
            },
            "context_scores": [
                {
                    "context": "\u2026papers Author-written and expertderived TLDRs FacetSum (Meng et al, 2021) 60k articles from Emerald journals Paper and structured abstract\nFor table extraction, Zheng et al (2021) proposed Global Table Extractor (GTE) to detect tables and recognize cell structure jointly based on visual context.",
                    "label_score": 0.0,
                    "label": "Neutral"
                }
            ]
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bd58fc27b05f4d9a1e9b83d23afcd02bbc9ec491",
                "externalIds": {
                    "DBLP": "conf/nips/BorchmannPSJTSG21",
                    "CorpusId": 244906279
                },
                "corpusId": 244906279,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bd58fc27b05f4d9a1e9b83d23afcd02bbc9ec491",
                "title": "DUE: End-to-End Document Understanding Benchmark",
                "abstract": "Understanding documents with rich layouts plays a vital role in digitization and hyper-automation but remains a challenging topic in the NLP research community. Additionally, the lack of a commonly accepted benchmark made it dif\ufb01cult to quantify progress in the domain. To empower research in this \ufb01eld, we intro-duce the Document Understanding Evaluation (DUE) benchmark consisting of both available and reformulated datasets to measure the end-to-end capabilities of systems in real-world scenarios. The benchmark includes Visual Question Answering, Key Information Extraction, and Machine Reading Comprehension tasks over various document domains and layouts featuring tables, graphs, lists, and infographics. In addition, the current study reports systematic baselines and analyzes challenges in currently available datasets using recent advances in layout-aware language modeling. We open both the benchmarks and reference imple-mentations and make them available at https://duebenchmark.com and https://github.com/due-benchmark .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "72722800",
                        "name": "\u0141ukasz Borchmann"
                    },
                    {
                        "authorId": "1749652564",
                        "name": "Michal Pietruszka"
                    },
                    {
                        "authorId": "2106450032",
                        "name": "Tomasz Stanislawek"
                    },
                    {
                        "authorId": "1400414734",
                        "name": "Dawid Jurkiewicz"
                    },
                    {
                        "authorId": "153451419",
                        "name": "M. Turski"
                    },
                    {
                        "authorId": "2143182984",
                        "name": "Karolina Szyndler"
                    },
                    {
                        "authorId": "2195272",
                        "name": "F. Gralinski"
                    }
                ]
            },
            "context_scores": []
        }
    ]
}