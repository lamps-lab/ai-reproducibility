{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "748194e591fa7689940d7d6a8766f1a4afa2dac0",
                "externalIds": {
                    "DOI": "10.1016/j.commatsci.2023.112441",
                    "CorpusId": 261451820
                },
                "corpusId": 261451820,
                "publicationVenue": {
                    "id": "3ec21075-eb50-4e2c-bdcb-ac69565537af",
                    "name": "Computational materials science",
                    "type": "journal",
                    "alternate_names": [
                        "Comput mater sci",
                        "Computational Materials Science",
                        "Comput Mater Sci"
                    ],
                    "issn": "0927-0256",
                    "url": "https://www.journals.elsevier.com/computational-materials-science/",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09270256"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/748194e591fa7689940d7d6a8766f1a4afa2dac0",
                "title": "A literature-mining method of integrating text and table extraction for materials science publications",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237356572",
                        "name": "Rui Zhang"
                    },
                    {
                        "authorId": "2237108818",
                        "name": "Jiawang Zhang"
                    },
                    {
                        "authorId": "30718739",
                        "name": "Qiaochuan Chen"
                    },
                    {
                        "authorId": "2237602275",
                        "name": "Bing Wang"
                    },
                    {
                        "authorId": "2237415468",
                        "name": "Yi Liu"
                    },
                    {
                        "authorId": "3090200",
                        "name": "Q. Qian"
                    },
                    {
                        "authorId": "2236989180",
                        "name": "Deng Pan"
                    },
                    {
                        "authorId": "2237017211",
                        "name": "Jinhua Xia"
                    },
                    {
                        "authorId": "2237106578",
                        "name": "Yinggang Wang"
                    },
                    {
                        "authorId": "40161348",
                        "name": "Yuexing Han"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Furthermore, to assess the effectiveness of our method in oriented and distorted scenarios, we compare it with two mainstream open-source methods, SPLERGE [39] and Table-Master [44], on the TAL, TAL_rotated, and TAL_curved datasets.",
                "Furthermore, to assess the effectiveness of our method in oriented and distorted scenarios, we compare it with two mainstream open-source methods, SPLERGE [39] and TableMaster [44], on the TAL, TAL_rotated, and TAL_curved datasets.",
                "In [8, 20, 27, 39, 46], a table is represented by a grid of regions."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e797270bd42586b0e46ce597a5ef6d754465e43",
                "externalIds": {
                    "ArXiv": "2309.14962",
                    "CorpusId": 262825477
                },
                "corpusId": 262825477,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3e797270bd42586b0e46ce597a5ef6d754465e43",
                "title": "GridFormer: Towards Accurate Table Structure Recognition via Grid Prediction",
                "abstract": "All tables can be represented as grids. Based on this observation, we propose GridFormer, a novel approach for interpreting unconstrained table structures by predicting the vertex and edge of a grid. First, we propose a flexible table representation in the form of an MXN grid. In this representation, the vertexes and edges of the grid store the localization and adjacency information of the table. Then, we introduce a DETR-style table structure recognizer to efficiently predict this multi-objective information of the grid in a single shot. Specifically, given a set of learned row and column queries, the recognizer directly outputs the vertexes and edges information of the corresponding rows and columns. Extensive experiments on five challenging benchmarks which include wired, wireless, multi-merge-cell, oriented, and distorted tables demonstrate the competitive performance of our model over other methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "10344582",
                        "name": "Pengyuan Lyu"
                    },
                    {
                        "authorId": "47121217",
                        "name": "Weihong Ma"
                    },
                    {
                        "authorId": "2109798334",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "1631405123",
                        "name": "Yu Yu"
                    },
                    {
                        "authorId": "2248958848",
                        "name": "Chengquan Zhang"
                    },
                    {
                        "authorId": "2140371004",
                        "name": "Kun Yao"
                    },
                    {
                        "authorId": "2114922667",
                        "name": "Yang Xue"
                    },
                    {
                        "authorId": "1688516",
                        "name": "Jingdong Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13de1c39dfe5733363ff1a8d4c6d577914f79618",
                "externalIds": {
                    "DBLP": "conf/ijcai/ShenGWQZLC23",
                    "DOI": "10.24963/ijcai.2023/152",
                    "CorpusId": 260853966
                },
                "corpusId": 260853966,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/13de1c39dfe5733363ff1a8d4c6d577914f79618",
                "title": "Divide Rows and Conquer Cells: Towards Structure Recognition for Large Tables",
                "abstract": "Recent advanced Table Structure Recognition (TSR) models adopt image-to-text solutions to parse table structure. These methods can be formulated as image caption problem, i.e., input a single-table image and output table structure description in a specific text format, e.g., HTML. With the impressive success of Transformer in text generation tasks, these methods use Transformer architecture to predict HTML table text in an autoregressive manner. However, tables always emerge with a large variety of shapes and sizes. Autoregressive models usually suffer from the error accumulation problem as the length of predicted text increases, which results in unsatisfactory performance for large tables. In this paper, we propose a novel image-to-text based TSR method that relieves error accumulation problems and improves performance noticeably. At the core of our method is a cascaded two-step decoder architecture with the former decoder predicting HTML table row tags non-autoregressively and the latter predicting HTML table cell tags of each row in a semi-autoregressive manner. Compared with existing methods that predict HTML text autoregressively, the superiority of our row-to-cell progressive table parsing is twofold: (1) it generates an HTML tag sequence with a vertical-and-horizontal two-step `scanning', which better fits the inherent 2D structure of image data, (2) it performs substantially better for large tables (long sequence prediction) since it alleviates error accumulation problem specific to autoregressive models. Extensive experiments demonstrate that our method achieves competitive performance on three public benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2230137426",
                        "name": "Huawen Shen"
                    },
                    {
                        "authorId": "2149397987",
                        "name": "Xiang Gao"
                    },
                    {
                        "authorId": "2111524263",
                        "name": "Jin Wei"
                    },
                    {
                        "authorId": "2065512194",
                        "name": "Liang Qiao"
                    },
                    {
                        "authorId": "49454803",
                        "name": "Yu Zhou"
                    },
                    {
                        "authorId": "2229647956",
                        "name": "Qiang Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "This system works well on both PDF and scanned document images [91]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1af79802f3e9864cf1dce509a137982972a8c9de",
                "externalIds": {
                    "DOI": "10.3390/smartcities6030067",
                    "CorpusId": 258819254
                },
                "corpusId": 258819254,
                "publicationVenue": {
                    "id": "d0bfb97a-a20e-4896-9afe-1e63e459db20",
                    "name": "Smart Cities",
                    "alternate_names": [
                        "Smart City"
                    ],
                    "issn": "2624-6511",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1343723",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-1343723",
                        "https://www.mdpi.com/journal/smartcities"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1af79802f3e9864cf1dce509a137982972a8c9de",
                "title": "From Traffic Congestion to Sustainable Mobility: A Case Study of Public Transport in Odesa, Ukraine",
                "abstract": "Consistent and reliable information on passenger traffic is considered crucial for the efficient operation of the public transport (PT) network. The PT network is used to improve public services and thus attract more passengers. This study evaluated the passenger traffic in Odesa, Ukraine, due to the inefficient urban transport system. The main aim of this study was to make PT better by examining passenger distribution on traffic routes and specifying characteristics of PT travel influencing individual satisfaction. The metric-tabular method was used to collect data and examine the number of incoming and outgoing passengers at each bus stop. The results of the passenger and PT analysis provide valuable recommendations for optimizing future routes. It is beneficial for transport companies to implement such recommendations so that inefficient transport on the route can be reduced by either reforming the route network or choosing the optimal number of buses. According to the findings of this study, understanding PT services is the most important determinant of PT adoption. The main implications of the findings are of particular interest to policymakers who develop policies in the field of passenger transport and also to transport scientists and students.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145142157",
                        "name": "S. Myronenko"
                    },
                    {
                        "authorId": "103419590",
                        "name": "H. Oborskyi"
                    },
                    {
                        "authorId": "2213539666",
                        "name": "Dmytro Dmytryshyn"
                    },
                    {
                        "authorId": "2218280687",
                        "name": "Vyacheslav Shobik"
                    },
                    {
                        "authorId": "144702836",
                        "name": "D. Lauwers"
                    },
                    {
                        "authorId": "1702946",
                        "name": "F. Witlox"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "TRACE employs a split-merge strategy, inspired by SPLERGE [45], to reconstruct tables.",
                "In recent years, a split-merge approach has emerged as a popular technique for TSR, in which the separators between cells are initially detected and then subsequently merged [45,14,48,19,25]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eca4dfc1703529bd87e1b4fea1fad1baa06e0ac9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00630",
                    "ArXiv": "2305.00630",
                    "DOI": "10.48550/arXiv.2305.00630",
                    "CorpusId": 258427145
                },
                "corpusId": 258427145,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/eca4dfc1703529bd87e1b4fea1fad1baa06e0ac9",
                "title": "TRACE: Table Reconstruction Aligned to Corner and Edges",
                "abstract": "A table is an object that captures structured and informative content within a document, and recognizing a table in an image is challenging due to the complexity and variety of table layouts. Many previous works typically adopt a two-stage approach; (1) Table detection(TD) localizes the table region in an image and (2) Table Structure Recognition(TSR) identifies row- and column-wise adjacency relations between the cells. The use of a two-stage approach often entails the consequences of error propagation between the modules and raises training and inference inefficiency. In this work, we analyze the natural characteristics of a table, where a table is composed of cells and each cell is made up of borders consisting of edges. We propose a novel method to reconstruct the table in a bottom-up manner. Through a simple process, the proposed method separates cell boundaries from low-level features, such as corners and edges, and localizes table positions by combining the cells. A simple design makes the model easier to train and requires less computation than previous two-stage methods. We achieve state-of-the-art performance on the ICDAR2013 table competition benchmark and Wired Table in the Wild(WTW) dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057310228",
                        "name": "Youngmin Baek"
                    },
                    {
                        "authorId": "2064808754",
                        "name": "Daehyun Nam"
                    },
                    {
                        "authorId": "10787779",
                        "name": "Jaeheung Surh"
                    },
                    {
                        "authorId": "2111068603",
                        "name": "Seung Shin"
                    },
                    {
                        "authorId": "2109603647",
                        "name": "Seonghyeon Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "contexts": [
                "Tensmeyer et al.[31]4 2019 SPLERGE ICDAR Web-screaped PDFs + ICDAR 2013 ICDAR 2013 - X NR",
                "The results shown in Figure 3 and Figure 4 indicate that none of the 4 methods that allow inference on custom data [13, 17, 16, 31] was replicable with respect to the GenTSR dataset, under a threshold of 10% absolute F1-score."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "50903f5f5e714e5746152b3e369c9329655e0018",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-10439",
                    "ArXiv": "2304.10439",
                    "DOI": "10.48550/arXiv.2304.10439",
                    "CorpusId": 258236507
                },
                "corpusId": 258236507,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/50903f5f5e714e5746152b3e369c9329655e0018",
                "title": "A Study on Reproducibility and Replicability of Table Structure Recognition Methods",
                "abstract": "Concerns about reproducibility in artificial intelligence (AI) have emerged, as researchers have reported unsuccessful attempts to directly reproduce published findings in the field. Replicability, the ability to affirm a finding using the same procedures on new data, has not been well studied. In this paper, we examine both reproducibility and replicability of a corpus of 16 papers on table structure recognition (TSR), an AI task aimed at identifying cell locations of tables in digital documents. We attempt to reproduce published results using codes and datasets provided by the original authors. We then examine replicability using a dataset similar to the original as well as a new dataset, GenTSR, consisting of 386 annotated tables extracted from scientific papers. Out of 16 papers studied, we reproduce results consistent with the original in only four. Two of the four papers are identified as replicable using the similar dataset under certain IoU values. No paper is identified as replicable using the new dataset. We offer observations on the causes of irreproducibility and irreplicability. All code and data are available on Codeocean at https://codeocean.com/capsule/6680116/tree.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "11359490",
                        "name": "Kehinde E. Ajayi"
                    },
                    {
                        "authorId": "2214808132",
                        "name": "Muntabhir Hasan Choudhury"
                    },
                    {
                        "authorId": "2528276",
                        "name": "S. Rajtmajer"
                    },
                    {
                        "authorId": "97569165",
                        "name": "Jian Wu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To alleviate this problem, methods like [4, 6, 29] tried different context enhancement techniques, e.",
                "Under the new evaluation setting, the result of SPLERGE significantly drops by 10.2% absolutely in F1-score.",
                "Among these methods, SPLERGE [6] was the first to deal with spanning cells, which proposed to add a simple cell merging module after a row/column extraction module to recover spanning cells by merging adjacent cells."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3e0c97baa8e7b629a3d689e00aa21b94e2193766",
                "externalIds": {
                    "ArXiv": "2303.11615",
                    "DBLP": "journals/corr/abs-2303-11615",
                    "DOI": "10.48550/arXiv.2303.11615",
                    "CorpusId": 257636821
                },
                "corpusId": 257636821,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3e0c97baa8e7b629a3d689e00aa21b94e2193766",
                "title": "Robust Table Structure Recognition with Dynamic Queries Enhanced Detection Transformer",
                "abstract": "We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage dynamic queries enhanced DETR based separation line regression approach, named DQ-DETR, to predict separation lines from table images directly. Compared to Vallina DETR, we propose three improvements in DQ-DETR to make the two-stage DETR framework work efficiently and effectively for the separation line prediction task: 1) A new query design, named Dynamic Query, to decouple single line query into separable point queries which could intuitively improve the localization accuracy for regression tasks; 2) A dynamic queries based progressive line regression approach to progressively regressing points on the line which further enhances localization accuracy for distorted tables; 3) A prior-enhanced matching strategy to solve the slow convergence issue of DETR. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet, WTW and FinTabNet. Furthermore, we have validated the robustness and high localization accuracy of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2187553093",
                        "name": "Jiawei Wang"
                    },
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2181111465",
                        "name": "Mingze Li"
                    },
                    {
                        "authorId": "2145130513",
                        "name": "Zhengmao Sun"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "The SPLERGE [41] divides table into grid elements and merges adjacent ones to restore spanning cells, where the boundary ambiguity issue still remains unsolved.",
                "With the fast-paced development of digital transformation, Table Structure Recognition (TSR) task, aiming at parsing table structure from a table image into machineinterpretable formats, which are often presented by both table cell physical coordinates [38, 31, 15, 41, 4, 34, 36, 45, 35, 23, 25] and their logical relationships [18, 47].",
                "TRUST [9] has the similar spirit with SPLERGE [41], where the features of row/column separators are extracted for a further vertex-based merging module to predict the linking relations between adjacent cells, which thus still suffers from boundary ambiguity problem.",
                "Method #Param FLOPs GPU CPU SPLERGE [41] 0.37 115.49 0.95 24.25 TabStruct-Net [36] 68.63 3719.06 22.63 76.52 FLAG-Net [23] 17.00 71.69 0.13 2.37 GrabTab 57.88 513.65 0.82 11.54\nTable H4.",
                "Method #Param FLOPs GPU CPU SPLERGE [41] 0.",
                "the filed, which can be mainly categorized into boundary extraction-based methods [38, 31, 15, 41, 25] and element relationship-based methods [23, 4, 34, 36] according to the component type leveraged.",
                "The first group are the single component-based methods, which rely on either boundary extraction [15, 25, 31, 38, 41] or element relationship [4, 23, 34, 36].",
                "To date, several pioneers works [18, 47, 38, 15, 41, 4, 34, 36, 45, 35, 23, 25] have achieved significant progress in"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2c267f43aca9bfb2f01e5400fca14545c4aa14b3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-09174",
                    "ArXiv": "2303.09174",
                    "DOI": "10.48550/arXiv.2303.09174",
                    "CorpusId": 257557431
                },
                "corpusId": 257557431,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c267f43aca9bfb2f01e5400fca14545c4aa14b3",
                "title": "Grab What You Need: Rethinking Complex Table Structure Recognition with Flexible Components Deliberation",
                "abstract": "Recently, Table Structure Recognition (TSR) task, aiming at identifying table structure into machine readable formats, has received increasing interest in the community. While impressive success, most single table component-based methods can not perform well on unregularized table cases distracted by not only complicated inner structure but also exterior capture distortion. In this paper, we raise it as Complex TSR problem, where the performance degeneration of existing methods is attributable to their inefficient component usage and redundant post-processing. To mitigate it, we shift our perspective from table component extraction towards the efficient multiple components leverage, which awaits further exploration in the field. Specifically, we propose a seminal method, termed GrabTab, equipped with newly proposed Component Deliberator. Thanks to its progressive deliberation mechanism, our GrabTab can flexibly accommodate to most complex tables with reasonable components selected but without complicated post-processing involved. Quantitative experimental results on public benchmarks demonstrate that our method significantly outperforms the state-of-the-arts, especially under more challenging scenes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216764712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2150155841",
                        "name": "Ming Gong"
                    },
                    {
                        "authorId": "47655556",
                        "name": "Bin Liu"
                    },
                    {
                        "authorId": "2164224877",
                        "name": "Yunfei Wu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "143900241",
                        "name": "Xing Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The SPLURGE method [47] proposed the idea of table splitting and merging."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5966cd3ac4460a97b27b4177a89c0d1041726881",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-06949",
                    "ArXiv": "2303.06949",
                    "DOI": "10.1109/CVPR52729.2023.01071",
                    "CorpusId": 257495840
                },
                "corpusId": 257495840,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5966cd3ac4460a97b27b4177a89c0d1041726881",
                "title": "Improving Table Structure Recognition with Visual-Alignment Sequential Coordinate Modeling",
                "abstract": "Table structure recognition aims to extract the logical and physical structure of unstructured table images into a machine-readable format. The latest end-to-end image-to-text approaches simultaneously predict the two structures by two decoders, where the prediction of the physical structure (the bounding boxes of the cells) is based on the representation of the logical structure. However, the previous methods struggle with imprecise bounding boxes as the logical representation lacks local visual information. To address this issue, we propose an end-to-end sequential modeling framework for table structure recognition called VAST. It contains a novel coordinate sequence decoder triggered by the representation of the non-empty cell from the logical structure decoder. In the coordinate sequence decoder, we model the bounding box coordinates as a language sequence, where the left, top, right and bottom coordinates are decoded sequentially to leverage the inter-coordinate dependency. Furthermore, we propose an auxiliary visual-alignment loss to enforce the logical representation of the non-empty cells to contain more local visual details, which helps produce better cell bounding boxes. Extensive experiments demonstrate that our proposed method can achieve state-of-the-art results in both logical and physical structure recognition. The ablation study also validates that the proposed coordinate sequence decoder and the visual-alignment loss are the keys to the success of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145769099",
                        "name": "Yongshuai Huang"
                    },
                    {
                        "authorId": "2054645836",
                        "name": "Ning Lu"
                    },
                    {
                        "authorId": "1679279",
                        "name": "Dapeng Chen"
                    },
                    {
                        "authorId": "2110470261",
                        "name": "Yibo Li"
                    },
                    {
                        "authorId": "2002678",
                        "name": "Zecheng Xie"
                    },
                    {
                        "authorId": "2901597",
                        "name": "Shenggao Zhu"
                    },
                    {
                        "authorId": "1777642",
                        "name": "Liangcai Gao"
                    },
                    {
                        "authorId": "2187312795",
                        "name": "Wei Peng"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c78daabab3666d08d945098bc462f882b78803fd",
                "externalIds": {
                    "ArXiv": "2303.04384",
                    "DBLP": "journals/corr/abs-2303-04384",
                    "DOI": "10.48550/arXiv.2303.04384",
                    "CorpusId": 257405340
                },
                "corpusId": 257405340,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c78daabab3666d08d945098bc462f882b78803fd",
                "title": "SEMv2: Table Separation Line Detection Based on Conditional Convolution",
                "abstract": "Table structure recognition is an indispensable element for enabling machines to comprehend tables. Its primary purpose is to identify the internal structure of a table. Nevertheless, due to the complexity and diversity of their structure and style, it is highly challenging to parse the tabular data into a structured format that machines can comprehend. In this work, we adhere to the principle of the split-and-merge based methods and propose an accurate table structure recognizer, termed SEMv2 (SEM: Split, Embed and Merge). Unlike the previous works in the ``split'' stage, we aim to address the table separation line instance-level discrimination problem and introduce a table separation line detection strategy based on conditional convolution. Specifically, we design the ``split'' in a top-down manner that detects the table separation line instance first and then dynamically predicts the table separation line mask for each instance. The final table separation line shape can be accurately obtained by processing the table separation line mask in a row-wise/column-wise manner. To comprehensively evaluate the SEMv2, we also present a more challenging dataset for table structure recognition, dubbed iFLYTAB, which encompasses multiple style tables in various scenarios such as photos, scanned documents, etc. Extensive experiments on publicly available datasets (e.g. SciTSR, PubTabNet and iFLYTAB) demonstrate the efficacy of our proposed approach. The code and iFLYTAB dataset will be made publicly available upon acceptance of this paper.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2128660562",
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "authorId": "2067770685",
                        "name": "Pengfei Hu"
                    },
                    {
                        "authorId": "2143520841",
                        "name": "Jie Ma"
                    },
                    {
                        "authorId": "2149030934",
                        "name": "Jun Du"
                    },
                    {
                        "authorId": "39557762",
                        "name": "Jianshu Zhang"
                    },
                    {
                        "authorId": "2211065581",
                        "name": "Huihui Zhu"
                    },
                    {
                        "authorId": "2055464704",
                        "name": "Baocai Yin"
                    },
                    {
                        "authorId": "2185098372",
                        "name": "Bing Yin"
                    },
                    {
                        "authorId": "2108152462",
                        "name": "Cong Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d943616a2b732c8b7cee18b44118db8d9a1af5e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-00716",
                    "ArXiv": "2303.00716",
                    "DOI": "10.48550/arXiv.2303.00716",
                    "CorpusId": 257255197
                },
                "corpusId": 257255197,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/d943616a2b732c8b7cee18b44118db8d9a1af5e8",
                "title": "Aligning benchmark datasets for table structure recognition",
                "abstract": "Benchmark datasets for table structure recognition (TSR) must be carefully processed to ensure they are annotated consistently. However, even if a dataset's annotations are self-consistent, there may be significant inconsistency across datasets, which can harm the performance of models trained and evaluated on them. In this work, we show that aligning these benchmarks$\\unicode{x2014}$removing both errors and inconsistency between them$\\unicode{x2014}$improves model performance significantly. We demonstrate this through a data-centric approach where we adopt one model architecture, the Table Transformer (TATR), that we hold fixed throughout. Baseline exact match accuracy for TATR evaluated on the ICDAR-2013 benchmark is 65% when trained on PubTables-1M, 42% when trained on FinTabNet, and 69% combined. After reducing annotation mistakes and inter-dataset inconsistency, performance of TATR evaluated on ICDAR-2013 increases substantially to 75% when trained on PubTables-1M, 65% when trained on FinTabNet, and 81% combined. We show through ablations over the modification steps that canonicalization of the table annotations has a significantly positive effect on performance, while other choices balance necessary trade-offs that arise when deciding a benchmark dataset's final composition. Overall we believe our work has significant implications for benchmark design for TSR and potentially other tasks as well. Dataset processing and training code will be released at https://github.com/microsoft/table-transformer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2128094499",
                        "name": "Robin Abraham"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7ed32e4a8e7ee87b2c441acd2b88ad012159eccf",
                "externalIds": {
                    "DBLP": "conf/iclr/YuLZZGQYHD023",
                    "ArXiv": "2303.00289",
                    "DOI": "10.48550/arXiv.2303.00289",
                    "CorpusId": 257255242
                },
                "corpusId": 257255242,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7ed32e4a8e7ee87b2c441acd2b88ad012159eccf",
                "title": "StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training",
                "abstract": "In this paper, we present StrucTexTv2, an effective document image pre-training framework, by performing masked visual-textual prediction. It consists of two self-supervised pre-training tasks: masked image modeling and masked language modeling, based on text region-level image masking. The proposed method randomly masks some image regions according to the bounding box coordinates of text words. The objectives of our pre-training tasks are reconstructing the pixels of masked image regions and the corresponding masked tokens simultaneously. Hence the pre-trained encoder can capture more textual semantics in comparison to the masked image modeling that usually predicts the masked image patches. Compared to the masked multi-modal modeling methods for document image understanding that rely on both the image and text modalities, StrucTexTv2 models image-only input and potentially deals with more application scenarios free from OCR pre-processing. Extensive experiments on mainstream benchmarks of document image understanding demonstrate the effectiveness of StrucTexTv2. It achieves competitive or even new state-of-the-art performance in various downstream tasks such as image classification, layout analysis, table structure recognition, document OCR, and information extraction under the end-to-end scenario.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1631405123",
                        "name": "Yu Yu"
                    },
                    {
                        "authorId": "2144444694",
                        "name": "Yulin Li"
                    },
                    {
                        "authorId": "1979323",
                        "name": "Chengquan Zhang"
                    },
                    {
                        "authorId": "2108113547",
                        "name": "Xiaoqiang Zhang"
                    },
                    {
                        "authorId": "2109309324",
                        "name": "Zengyuan Guo"
                    },
                    {
                        "authorId": "3343260",
                        "name": "Xiameng Qin"
                    },
                    {
                        "authorId": "2140371004",
                        "name": "Kun Yao"
                    },
                    {
                        "authorId": "1912505",
                        "name": "Junyu Han"
                    },
                    {
                        "authorId": "12081764",
                        "name": "Errui Ding"
                    },
                    {
                        "authorId": "2109534192",
                        "name": "Jingdong Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4750b008727349147f4913a644f976168fa3e90b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-05658",
                    "ArXiv": "2302.05658",
                    "DOI": "10.48550/arXiv.2302.05658",
                    "CorpusId": 256827641
                },
                "corpusId": 256827641,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/4750b008727349147f4913a644f976168fa3e90b",
                "title": "DocILE Benchmark for Document Information Localization and Extraction",
                "abstract": "This paper introduces the DocILE benchmark with the largest dataset of business documents for the tasks of Key Information Localization and Extraction and Line Item Recognition. It contains 6.7k annotated business documents, 100k synthetically generated documents, and nearly~1M unlabeled documents for unsupervised pre-training. The dataset has been built with knowledge of domain- and task-specific aspects, resulting in the following key features: (i) annotations in 55 classes, which surpasses the granularity of previously published key information extraction datasets by a large margin; (ii) Line Item Recognition represents a highly practical information extraction task, where key information has to be assigned to items in a table; (iii) documents come from numerous layouts and the test set includes zero- and few-shot cases as well as layouts commonly seen in the training set. The benchmark comes with several baselines, including RoBERTa, LayoutLMv3 and DETR-based Table Transformer; applied to both tasks of the DocILE benchmark, with results shared in this paper, offering a quick starting point for future work. The dataset, baselines and supplementary material are available at https://github.com/rossumai/docile.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2203366733",
                        "name": "vStvep'an vSimsa"
                    },
                    {
                        "authorId": "2203364206",
                        "name": "Milan vSulc"
                    },
                    {
                        "authorId": "2205547051",
                        "name": "Michal Uvrivc'avr"
                    },
                    {
                        "authorId": "1699597533",
                        "name": "Yash J. Patel"
                    },
                    {
                        "authorId": "2924500",
                        "name": "Ahmed Hamdi"
                    },
                    {
                        "authorId": "2208709174",
                        "name": "Matvej Koci'an"
                    },
                    {
                        "authorId": "2203364204",
                        "name": "Maty'avs Skalick'y"
                    },
                    {
                        "authorId": "2000570582",
                        "name": "Jivr'i Matas"
                    },
                    {
                        "authorId": "2174737970",
                        "name": "Antoine Doucet"
                    },
                    {
                        "authorId": "1732746",
                        "name": "Micka\u00ebl Coustaty"
                    },
                    {
                        "authorId": "1694974",
                        "name": "Dimosthenis Karatzas"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ca2fabed8604b296d713262794a427e5c4b51ffa",
                "externalIds": {
                    "DBLP": "journals/apin/WanZLZS23",
                    "DOI": "10.1007/s10489-022-04420-4",
                    "CorpusId": 255362168
                },
                "corpusId": 255362168,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ca2fabed8604b296d713262794a427e5c4b51ffa",
                "title": "Contextual transformer sequence-based recognition network for medical examination reports",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152912519",
                        "name": "Honglin Wan"
                    },
                    {
                        "authorId": "2199057485",
                        "name": "Zongfeng Zhong"
                    },
                    {
                        "authorId": "2263987",
                        "name": "Tianping Li"
                    },
                    {
                        "authorId": "2856513",
                        "name": "Huaxiang Zhang"
                    },
                    {
                        "authorId": "51299154",
                        "name": "Jiande Sun"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ae6e34d182fbc42bc99ba609369a3b7645d205cb",
                "externalIds": {
                    "DBLP": "journals/prl/WangXZJ23",
                    "DOI": "10.1016/j.patrec.2022.12.014",
                    "CorpusId": 255089449
                },
                "corpusId": 255089449,
                "publicationVenue": {
                    "id": "f35e3e87-9df4-497b-aa0d-bb8584197290",
                    "name": "Pattern Recognition Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit Lett"
                    ],
                    "issn": "0167-8655",
                    "url": "https://www.journals.elsevier.com/pattern-recognition-letters/",
                    "alternate_urls": [
                        "http://www.journals.elsevier.com/pattern-recognition-letters/",
                        "http://www.sciencedirect.com/science/journal/01678655"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ae6e34d182fbc42bc99ba609369a3b7645d205cb",
                "title": "Scene table structure recognition with segmentation collaboration and alignment",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109799388",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "46364544",
                        "name": "Yang Xue"
                    },
                    {
                        "authorId": "2118131879",
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "authorId": "144838978",
                        "name": "Lianwen Jin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "C Tensmeyer [120] Dilated Convolutions + Fully CNN The technique is effective with both scanned and PDF document images.",
                "C Tensmeyer [120] has presented SPLERGE (Split and Merge), another method using dilated convolutions.",
                "C Tensmeyer[120] ICDAR2013 Dilated Convolutions + Fully CNN Precision 95."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a81f504a1a101b9c6fe4a88957dd9ec450cf389d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-08469",
                    "ArXiv": "2211.08469",
                    "DOI": "10.48550/arXiv.2211.08469",
                    "CorpusId": 253553399
                },
                "corpusId": 253553399,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a81f504a1a101b9c6fe4a88957dd9ec450cf389d",
                "title": "Deep learning for table detection and structure recognition: A survey",
                "abstract": "Tables are everywhere, from scientific journals, papers, websites, and newspapers all the way to items we buy at the supermarket. Detecting them is thus of utmost importance to automatically understanding the content of a document. The performance of table detection has substantially increased thanks to the rapid development of deep learning networks. The goals of this survey are to provide a profound comprehension of the major developments in the field of Table Detection, offer insight into the different methodologies, and provide a systematic taxonomy of the different approaches. Furthermore, we provide an analysis of both classic and new applications in the field. Lastly, the datasets and source code of the existing models are organized to provide the reader with a compass on this vast literature. Finally, we go over the architecture of utilizing various object detection and table structure recognition methods to create an effective and efficient system, as well as a set of development trends to keep up with state-of-the-art algorithms and future research. We have also set up a public GitHub repository where we will be updating the most recent publications, open data, and source code. The GitHub repository is available at https://github.com/abdoelsayed2016/ table-detection-structure-recognition. Preprint submitted to Elsevier November 17, 2022 ar X iv :2 21 1. 08 46 9v 1 [ cs .C V ] 1 5 N ov 2 02 2",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "87034581",
                        "name": "M. Kasem"
                    },
                    {
                        "authorId": "115665360",
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "authorId": "2115948777",
                        "name": "Alexander Berendeyev"
                    },
                    {
                        "authorId": "2190955162",
                        "name": "Ebrahem Elkady"
                    },
                    {
                        "authorId": "2190955581",
                        "name": "Mahmoud Abdalla"
                    },
                    {
                        "authorId": "2163883464",
                        "name": "Mohamed Mahmoud"
                    },
                    {
                        "authorId": "2057870328",
                        "name": "Mohamed Hamada"
                    },
                    {
                        "authorId": "3253700",
                        "name": "D. Nurseitov"
                    },
                    {
                        "authorId": "1405340824",
                        "name": "I. Taj-Eddin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "299a74663a5230545ba9549b226d7116d1a31b43",
                "externalIds": {
                    "DOI": "10.23919/APSIPAASC55919.2022.9980172",
                    "CorpusId": 254930638
                },
                "corpusId": 254930638,
                "publicationVenue": {
                    "id": "5b924e1a-30f3-4275-bdb8-5a15517c0fde",
                    "name": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Asia-pacific Signal Inf Process Assoc Annu Summit Conf",
                        "APSIPA"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/299a74663a5230545ba9549b226d7116d1a31b43",
                "title": "Table Structure Recognition Based on Grid Shape Graph",
                "abstract": "Since tables in documents provide important information in compact form, table understanding has been an essential topic in document image processing. Researchers represented table structures in various formats for table understanding, such as simple grid structure, a graph with text/cell boxes as nodes, or a sequence of HTML tokens. However, these approaches have difficulties in handling regularities, e.g., global row and column information, and spanning cells simultaneously. In this paper, we propose a new table recognition method based on a grid shape graph and present grid localization and grid elements grouping networks. This approach is designed to exploit the grid structure and deal with spanning cells. To convert grid structure into cell structure, we only have to test adjacent pairs of grid elements, enabling efficient inference. In addition, we have discovered that predicting row/column-based relationships between grid elements improve cell-based connectivity estimation performance. We demonstrate the effectiveness of the proposed method through experiments on three benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115242015",
                        "name": "Eunji Lee"
                    },
                    {
                        "authorId": "151053767",
                        "name": "Junhyeong Kwon"
                    },
                    {
                        "authorId": "2109472075",
                        "name": "Haeyoon Yang"
                    },
                    {
                        "authorId": "2370571",
                        "name": "Jaewoo Park"
                    },
                    {
                        "authorId": "2144550201",
                        "name": "Soonyoung Lee"
                    },
                    {
                        "authorId": "2463454",
                        "name": "H. Koo"
                    },
                    {
                        "authorId": "1707645",
                        "name": "N. Cho"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[31] used dilated convolutions but depend upon heuristics approaches during post-processing [12]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bc8abf90ebc6d0e463bd0ad9952a437d5fea159b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-17246",
                    "ArXiv": "2210.17246",
                    "DOI": "10.1007/s10032-022-00420-9",
                    "CorpusId": 253193064
                },
                "corpusId": 253193064,
                "publicationVenue": {
                    "id": "46722266-4327-4a22-871d-5a9fe3c5ad0c",
                    "name": "International Journal on Document Analysis and Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Doc Anal Recognit"
                    ],
                    "issn": "1433-2825",
                    "url": "https://link.springer.com/journal/10032"
                },
                "url": "https://www.semanticscholar.org/paper/bc8abf90ebc6d0e463bd0ad9952a437d5fea159b",
                "title": "Tables to LaTeX: structure and content extraction from scientific tables",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "117769145",
                        "name": "Pratik Kayal"
                    },
                    {
                        "authorId": "2106415501",
                        "name": "Mrinal Anand"
                    },
                    {
                        "authorId": "73654757",
                        "name": "Harsh Desai"
                    },
                    {
                        "authorId": "145431050",
                        "name": "Mayank Singh"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "To handle the cells crossing multiple rows/columns, [59, 71] predicts another indicator to merge the separated cells."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4ebf44ebb9634cb5c60f5dc7fab0c0f84aed0939",
                "externalIds": {
                    "DBLP": "conf/mm/LiL0LCNPL22",
                    "DOI": "10.1145/3503161.3547885",
                    "CorpusId": 252782335
                },
                "corpusId": 252782335,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4ebf44ebb9634cb5c60f5dc7fab0c0f84aed0939",
                "title": "End-to-End Compound Table Understanding with Multi-Modal Modeling",
                "abstract": "Table is a widely used data form in webpages, spreadsheets, or PDFs to organize and present structural data. Although studies on table structure recognition have been successfully used to convert image-based tables into digital structural formats, solving many real problems still relies on further understanding of the table, such as cell relationship extraction. The current datasets related to table understanding are all based on the digit format. To boost research development, we release a new benchmark named ComFinTab with rich annotations that support both table recognition and understanding tasks. Unlike previous datasets containing the basic tables, ComFinTab contains a large ratio of compound tables, which is much more challenging and requires methods using multiple information sources. Based on the dataset, we also propose a uniform, concise task form with the evaluation metric to better evaluate the model's performance on the table understanding task in compound tables. Finally, a framework named CTUNet is proposed to integrate the compromised visual, semantic, and position features with a graph attention network, which can solve the table recognition task and the challenging table understanding task as a whole. Experimental results compared with some previous advanced table understanding methods demonstrate the effectiveness of our proposed model. Code and dataset are available at \\urlhttps://github.com/hikopensource/DAVAR-Lab-OCR.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109614687",
                        "name": "Zaisheng Li"
                    },
                    {
                        "authorId": "2187429408",
                        "name": "Yi Li"
                    },
                    {
                        "authorId": "2187308758",
                        "name": "Qiao Liang"
                    },
                    {
                        "authorId": "2144902872",
                        "name": "Pengfei Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    },
                    {
                        "authorId": "1490934795",
                        "name": "Yi Niu"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    },
                    {
                        "authorId": "2116227961",
                        "name": "Xi Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For example, SPLERGE [10] first predicts the basic table grid pattern using Row Projection Networks and Column Projection Networks with novel projection pooling and then combines them to get table structure.",
                "However, recent Splitting-based approaches such as SPLERGE [10] and SEM [11] may suffer from following disadvantages: (1) The pipeline of SEM is inefficient, which may involve time-consuming Region of Interest (RoI) [12] operations and context features extraction via BERT [13].",
                "Split: split model proposed in SPLERGE [10], QBS: Query-Based Splitting Module, Heuristic: Heuristic Post-processing, Merge: merge model proposed in SPLERGE [10], VBM: Vertex-Based Merging Module",
                "We expanded the definition of splitter in SPLERGE [10] to support inclined separators.",
                "Splitting Model Merging Model Performance(Pubtabnet/SynthTable(C4)) # Split [10] QBS Heuristic [10] Merge [10] VBM Str-TEDs TEDs",
                "Compared with the training of two independent modules in SPLERGE [10], the whole framework of TRUST can be trained in an end-to-end manner and achieve better performance.",
                "To evaluate this module, we replace the Query-Based Splitting Module with the Split Model proposed in SPLERGE [10].",
                "Additionally, it is benchmarked with a number of state-of-the-art techniques such as SPLERGE [10], TabStruct-Net [18], EDD [3], GTE [28], LGPMA [9], FLAG-Net [29], etc."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "92132fb36fb3e470464551210926f256a1f37280",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-14687",
                    "ArXiv": "2208.14687",
                    "DOI": "10.48550/arXiv.2208.14687",
                    "CorpusId": 251953555
                },
                "corpusId": 251953555,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/92132fb36fb3e470464551210926f256a1f37280",
                "title": "TRUST: An Accurate and End-to-End Table structure Recognizer Using Splitting-based Transformers",
                "abstract": "Table structure recognition is a crucial part of document image analysis domain. Its difficulty lies in the need to parse the physical coordinates and logical indices of each cell at the same time. However, the existing methods are difficult to achieve both these goals, especially when the table splitting lines are blurred or tilted. In this paper, we propose an accurate and end-to-end transformer-based table structure recognition method, referred to as TRUST. Transformers are suitable for table structure recognition because of their global computations, perfect memory, and parallel computation. By introducing novel Transformer-based Query-based Splitting Module and Vertex-based Merging Module, the table structure recognition problem is decoupled into two joint optimization sub-tasks: multi-oriented table row/column splitting and table grid merging. The Query-based Splitting Module learns strong context information from long dependencies via Transformer networks, accurately predicts the multi-oriented table row/column separators, and obtains the basic grids of the table accordingly. The Vertex-based Merging Module is capable of aggregating local contextual information between adjacent basic grids, providing the ability to merge basic girds that belong to the same spanning cell accurately. We conduct experiments on several popular benchmarks including PubTabNet and SynthTable, our method achieves new state-of-the-art results. In particular, TRUST runs at 10 FPS on PubTabNet, surpassing the previous methods by a large margin.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109309324",
                        "name": "Zengyuan Guo"
                    },
                    {
                        "authorId": "2117164666",
                        "name": "Yuecheng Yu"
                    },
                    {
                        "authorId": "25604699",
                        "name": "Pengyuan Lv"
                    },
                    {
                        "authorId": "1979323",
                        "name": "Chengquan Zhang"
                    },
                    {
                        "authorId": "2579920",
                        "name": "Haojie Li"
                    },
                    {
                        "authorId": "47196393",
                        "name": "Zhihui Wang"
                    },
                    {
                        "authorId": "2140371004",
                        "name": "Kun Yao"
                    },
                    {
                        "authorId": "2272123",
                        "name": "Jingtuo Liu"
                    },
                    {
                        "authorId": "2109534192",
                        "name": "Jingdong Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In contrast, some work, such as DeepDeSRT [8], CascadeTabNet [9] and TableDet [10], using top-down approaches would define the structural recognition as object detection or segmentation problem, often together with table detection problem."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "551c36552b2d5af8493963ac38ee288f3cdb34ed",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-06031",
                    "ArXiv": "2208.06031",
                    "DOI": "10.1109/GLOBECOM48099.2022.10001253",
                    "CorpusId": 251554763
                },
                "corpusId": 251554763,
                "publicationVenue": {
                    "id": "b189dec0-41d0-4cea-a906-7c5186895904",
                    "name": "Global Communications Conference",
                    "type": "conference",
                    "alternate_names": [
                        "GLOBECOM",
                        "Glob Commun Conf"
                    ],
                    "url": "http://www.ieee-globecom.org/"
                },
                "url": "https://www.semanticscholar.org/paper/551c36552b2d5af8493963ac38ee288f3cdb34ed",
                "title": "Handling big tabular data of ICT supply chains: a multi-task, machine-interpretable approach",
                "abstract": "Due to the characteristics of Information and Communications Technology (ICT) products, the critical information of ICT devices is often summarized in big tabular data shared across supply chains. Therefore, it is critical to automatically interpret tabular structures with the surging amount of electronic assets. To transform the tabular data in electronic documents into a machine-interpretable format and provide layout and semantic information for information extraction and interpretation, we define a Table Structure Recognition (TSR) task and a Table Cell Type Classification (CTC) task. We use a graph to represent complex table structures for the TSR task. Meanwhile, table cells are categorized into three groups based on their functional roles for the CTC task, namely Header, Attribute, and Data. Subsequently, we propose a multi-task model to solve the defined two tasks simultaneously by using the text modal and image modal features. Our experimental results show that our proposed method can outperform state-of-the-art methods on ICDAR2013 and UNLV datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146517209",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "All six axes add up to the unique, complete generic identity for the examination, as per the LONIC nomenclature guideline (31)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "79dbb5e1b3ae4e6565bc6a5829604452daf1be0e",
                "externalIds": {
                    "PubMedCentral": "9399779",
                    "DOI": "10.3389/fpubh.2022.926229",
                    "CorpusId": 251448668,
                    "PubMed": "36033768"
                },
                "corpusId": 251448668,
                "publicationVenue": {
                    "id": "d3dd5449-daa1-4d57-9412-9add6037584f",
                    "name": "Frontiers in Public Health",
                    "type": "journal",
                    "alternate_names": [
                        "Front Public Health"
                    ],
                    "issn": "2296-2565",
                    "url": "https://www.frontiersin.org/journals/public-health",
                    "alternate_urls": [
                        "http://journal.frontiersin.org/journal/public-health",
                        "http://www.frontiersin.org/Public_Health",
                        "http://www.frontiersin.org/Public_Health/about"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/79dbb5e1b3ae4e6565bc6a5829604452daf1be0e",
                "title": "An efficient modular framework for automatic LIONC classification of MedIMG using unified medical language",
                "abstract": "Handwritten prescriptions and radiological reports: doctors use handwritten prescriptions and radiological reports to give drugs to patients who have illnesses, injuries, or other problems. Clinical text data, like physician prescription visuals and radiology reports, should be labelled with specific information such as disease type, features, and anatomical location for more effective use. The semantic annotation of vast collections of biological and biomedical texts, like scientific papers, medical reports, and general practitioner observations, has lately been examined by doctors and scientists. By identifying and disambiguating references to biomedical concepts in texts, medical semantics annotators could generate such annotations automatically. For Medical Images (MedIMG), we provide a methodology for learning an effective holistic representation (handwritten word pictures as well as radiology reports). Deep Learning (DL) methods have recently gained much interest for their capacity to achieve expert-level accuracy in automated MedIMG analysis. We discovered that tasks requiring significant responsive fields are ideal for downscaled input images that are qualitatively verified by examining functional, responsive areas and class activating maps for training models. This article focuses on the following contributions: (a) Information Extraction from Narrative MedImages, (b) Automatic categorisation on image resolution with an impact on MedIMG, and (c) Hybrid Model to Predictions of Named Entity Recognition utilising RNN + LSTM + GRM that perform admirably in every trainee for every input purpose. At the same time, supplying understandable scale weight implies that such multi-scale structures are also crucial for extracting information from high-resolution MedIMG. A portion of the reports (30%) are manually evaluated by trained physicians, while the rest were automatically categorised using deep supervised training models based on attention mechanisms and supplied with test reports. MetaMapLite proved recall and precision, but also an F1-score equivalent for primary biomedicine text search techniques and medical text examination on many databases of MedIMG. In addition to implementing as well as getting the requirements for MedIMG, the article explores the quality of medical data by using DL techniques for reaching large-scale labelled clinical data and also the significance of their real-time efforts in the biomedical study that have played an instrumental role in its extramural diffusion and global appeal.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2147349647",
                        "name": "Surbhi Bhatia"
                    },
                    {
                        "authorId": "9361345",
                        "name": "M. Alojail"
                    },
                    {
                        "authorId": "3342300",
                        "name": "Sudhakar Sengan"
                    },
                    {
                        "authorId": "9194134",
                        "name": "P. Dadheech"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Among these methods, only SPLERGE [45] can deal with spanning cells, which proposed to add a simple cell merging module after a row/column extraction module to recover spanning cells by merging adjacent cells.",
                "To alleviate this problem, methods like [13, 43, 45] tried different context enhancement techniques, e.",
                "In order to verify the effectiveness of TSRFormer for more challenging borderless tables, we re-implement another split-and-merge based method SPLERGE [45] and compare our approach with it on serveral datasets.",
                ", [45]), we formulate separation line prediction as a line regression problem instead of an image segmentation problem and propose a new separator prediction approach, dubbed Separator REgression TRansformer (SepRETR), to predict separation lines from table images directly.",
                "As shown in Table 4, the re-implemented SPLERGE can achieve competitve results on SciTSR and PubTabNet datasets while it is still 11.4% worse than TSRFormer in F1-score on our challenging in-house dataset."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fb6fcca5762b26f225313f86b1f33a1cf198bfd7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-04921",
                    "ArXiv": "2208.04921",
                    "DOI": "10.1145/3503161.3548038",
                    "CorpusId": 251442367
                },
                "corpusId": 251442367,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fb6fcca5762b26f225313f86b1f33a1cf198bfd7",
                "title": "TSRFormer: Table Structure Recognition with Transformers",
                "abstract": "We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage DETR based separator prediction approach, dubbed Sep arator RE gression TR ansformer (SepRETR), to predict separation lines from table images directly. To make the two-stage DETR framework work efficiently and effectively for the separation line prediction task, we propose two improvements: 1) A prior-enhanced matching strategy to solve the slow convergence issue of DETR; 2) A new cross attention module to sample features from a high-resolution convolutional feature map directly so that high localization accuracy is achieved with low computational cost. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet and WTW. Furthermore, we have validated the robustness of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2145130513",
                        "name": "Zhengmao Sun"
                    },
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2181111465",
                        "name": "Mingze Li"
                    },
                    {
                        "authorId": "2110132415",
                        "name": "Jiawei Wang"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "644b991959bf5fb036d28d67e9918d419b212e75",
                "externalIds": {
                    "DBLP": "journals/pr/LiYDL22",
                    "DOI": "10.1016/j.patcog.2022.108946",
                    "CorpusId": 251140530
                },
                "corpusId": 251140530,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/644b991959bf5fb036d28d67e9918d419b212e75",
                "title": "Table Structure Recognition and Form Parsing by End-to-End Object Detection and Relation Parsing",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118890077",
                        "name": "Xiaohui Li"
                    },
                    {
                        "authorId": "145820427",
                        "name": "Fei Yin"
                    },
                    {
                        "authorId": "2179477671",
                        "name": "He-Sen Dai"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The table can therefore be represented as a grid [68,78]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1aa8da3bbd2bca7071fae029d6fa8ea851f805b7",
                "externalIds": {
                    "DBLP": "conf/clef/SkalickySUS22",
                    "ArXiv": "2206.11229",
                    "DOI": "10.48550/arXiv.2206.11229",
                    "CorpusId": 249926391
                },
                "corpusId": 249926391,
                "publicationVenue": {
                    "id": "ab453bce-d4ec-48ec-ad78-ef19dc9333ab",
                    "name": "Conference and Labs of the Evaluation Forum",
                    "type": "conference",
                    "alternate_names": [
                        "CLEF",
                        "Conf Lab Evaluation Forum",
                        "Cross-language Evaluation Forum",
                        "Cross-Language Evaluation Forum"
                    ],
                    "url": "http://www.clef-initiative.eu/"
                },
                "url": "https://www.semanticscholar.org/paper/1aa8da3bbd2bca7071fae029d6fa8ea851f805b7",
                "title": "Business Document Information Extraction: Towards Practical Benchmarks",
                "abstract": "Information extraction from semi-structured documents is crucial for frictionless business-to-business (B2B) communication. While machine learning problems related to Document Information Extraction (IE) have been studied for decades, many common problem definitions and benchmarks do not reflect domain-specific aspects and practical needs for automating B2B document communication. We review the landscape of Document IE problems, datasets and benchmarks. We highlight the practical aspects missing in the common definitions and define the Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR) problems. There is a lack of relevant datasets and benchmarks for Document IE on semi-structured business documents as their content is typically legally protected or sensitive. We discuss potential sources of available documents including synthetic data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071515342",
                        "name": "Maty\u00e1s Skalick\u00fd"
                    },
                    {
                        "authorId": "46369981",
                        "name": "Step\u00e1n Simsa"
                    },
                    {
                        "authorId": "3406363",
                        "name": "Michal U\u0159i\u010d\u00e1\u0159"
                    },
                    {
                        "authorId": "2052167",
                        "name": "Milan \u0160ulc"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[17], didn\u2019t take spanning cells into consideration and can only recover the basic grid structures of tables.",
                "1st row: original images; 2nd row: results from SPLERGE [17]; 3rd row: results from our table structure recognizer.",
                "Following SPLERGE [17], we calculate the GT separator masks by maximizing the size of the separation regions without intersecting any non-spanning cell contents, as shown in Fig.",
                "As shown in Table 8, our approach outperforms SPLERGE significantly by improving the WAvg.",
                "As shown in Table 7, our approach outperforms the closest method, SPLERGE [17], substantially by improving the WAvg.",
                "[17] presented the SPLERGE method, which used a Split model to produce the grid structure of an input table first, and then used a Merge model",
                "To further validate the robustness of our approach to distorted or even curved table images, we conducted\nexperiments on the in-house dataset and compared our table structure recognizer with SPLERGE.",
                "Following SPLERGE [17], we calculate the GT separator masks by maximizing the size of the separation regions",
                "SPLERGE [17] 75.9 55.2 63.9 75.8 55.1 63.8 75.7 55.0 63.7 75.7 55.0 63.7 63.8 Ours 94.9 94.5 94.7 94.8 94.4 94.6 94.8 94.4 94.6 94.7 94.3 94.5 94.6\nCornerNet based table proposal generation algorithm for achieving higher localization accuracy and better end-to-end table detection results.",
                "To deal with spanning cells, Tensmeyer et al. [17] presented the SPLERGE method, which used a Split model to produce the grid structure of an input table first, and then used a Merge model to predict which grid elements should be merged to recover spanning cells.",
                "vanilla FCN based row/column segmentation method cannot robustly predict complete segmentation masks for rows and columns when tables contain large blank spaces [16, 17].",
                "SPLERGE [17], substantially by improving the WAvg."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "29942b5588d9978af5da046ebfc642e4779b8597",
                "externalIds": {
                    "ArXiv": "2203.09056",
                    "DBLP": "journals/pr/MaLSH23",
                    "DOI": "10.1016/j.patcog.2022.109006",
                    "CorpusId": 247518554
                },
                "corpusId": 247518554,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/29942b5588d9978af5da046ebfc642e4779b8597",
                "title": "Robust Table Detection and Structure Recognition from Heterogeneous Document Images",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "007f2dc393e363abcfd023c1a1b47f5b84d4b2ae",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-03819",
                    "ArXiv": "2203.03819",
                    "DOI": "10.48550/arXiv.2203.03819",
                    "CorpusId": 247315039
                },
                "corpusId": 247315039,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/007f2dc393e363abcfd023c1a1b47f5b84d4b2ae",
                "title": "Table Structure Recognition with Conditional Attention",
                "abstract": "Tabular data in digital documents is widely used to express compact and important information for readers. However, it is challenging to parse tables from unstructured digital documents, such as PDFs and images, into machine-readable format because of the complexity of table structures and the missing of meta-information. Table Structure Recognition (TSR) problem aims to recognize the structure of a table and transform the unstructured tables into a structured and machine-readable format so that the tabular data can be further analysed by the down-stream tasks, such as semantic modeling and information retrieval. In this study, we hypothesize that a complicated table structure can be represented by a graph whose vertices and edges represent the cells and association between cells, respectively. Then we define the table structure recognition problem as a cell association classification problem and propose a conditional attention network (CATT-Net). The experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods on various datasets. Besides, we investigate whether the alignment of a cell bounding box or a text-focused approach has more impact on the model performance. Due to the lack of public dataset annotations based on these two approaches, we further annotate the ICDAR2013 dataset providing both types of bounding boxes, which can be a new benchmark dataset for evaluating the methods in this field. Experimental results show that the alignment of a cell bounding box can help improve the Micro-averaged F1 score from 0.915 to 0.963, and the Macro-average F1 score from 0.787 to 0.923.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144025674",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a9e00a1c3de05e3d06a4287b8ef07096f0cfe609",
                "externalIds": {
                    "DBLP": "journals/pr/RibaGTRFL22",
                    "DOI": "10.1016/j.patcog.2022.108641",
                    "CorpusId": 247427729
                },
                "corpusId": 247427729,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a9e00a1c3de05e3d06a4287b8ef07096f0cfe609",
                "title": "Table detection in business document images by message passing networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40420775",
                        "name": "Pau Riba"
                    },
                    {
                        "authorId": "145029669",
                        "name": "Lutz Goldmann"
                    },
                    {
                        "authorId": "3045937",
                        "name": "O. R. Terrades"
                    },
                    {
                        "authorId": "1491424368",
                        "name": "Diede Rusticus"
                    },
                    {
                        "authorId": "1686569",
                        "name": "A. Forn\u00e9s"
                    },
                    {
                        "authorId": "143826881",
                        "name": "J. Llad\u00f3s"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[32] proposed a method called SPLERGE, which first split tables into rows and columns (split module) and recovered the spanning cells using FCN (merge module).",
                "Although we tried to train split network with the same training data we used, we failed to reproduce their results and used the model trained by the authors [32].",
                "SPLERGE [32] predicts cell-regions from table images using split and merge modules.",
                "For example, the authors of [32] used their private collection of 93,000 tables.",
                "For instance, in [26, 29, 30, 32], the authors considered columns and rows as object types and reconstructed table structures from the detected rows and columns.",
                "of parameters CascadeTabNet [23] 82,852,033 TabStructNet [25] 68,636,098 SPLERGE [32] 255,862 Ours 1,120,692",
                "8 Examples of qualitative results on complex tables, the colored regions are the recognized cell regions: Table structure recognition results of (a) CascadeTabNet  [23], (b) TabStructNet  [25], (c) SPLERGE [32], and (d) the proposed method",
                "We compare the performance of our method with three coventional methods: CascadeTabNet [23], TabStructNet [25], and SPLERGE [32].",
                "Results of CascadeTabNet [23], TabStructNet [25], SPLERGE [32], and the proposed method, from top to bottom 5844 Multimedia Tools and Applications (2022) 81:5827\u20135848",
                "(a) Result from CascadeTabNet [23], (b) TabStructNet [25] with ground truth table locations, (c) SPLERGE [32] with ground truth table locations, and (d) result from the proposed method 5845 Multimedia Tools and Applications (2022) 81:5827\u20135848"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d9c676eccdff80a7d1154660a09c4d2ea5164610",
                "externalIds": {
                    "DOI": "10.1007/s11042-021-11819-7",
                    "CorpusId": 254860167
                },
                "corpusId": 254860167,
                "publicationVenue": {
                    "id": "477368e9-7a8e-475a-8c93-6d623797fd06",
                    "name": "Multimedia tools and applications",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Tools and Applications",
                        "Multimedia Tool Appl",
                        "Multimedia tool appl"
                    ],
                    "issn": "1380-7501",
                    "url": "https://www.springer.com/computer/information+systems+and+applications/journal/11042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d9c676eccdff80a7d1154660a09c4d2ea5164610",
                "title": "Deep-learning and graph-based approach to table structure recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115242015",
                        "name": "Eunji Lee"
                    },
                    {
                        "authorId": "2370571",
                        "name": "Jaewoo Park"
                    },
                    {
                        "authorId": "2463454",
                        "name": "H. Koo"
                    },
                    {
                        "authorId": "1707645",
                        "name": "N. Cho"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9fb2744ef2b91033de39c121be25d3f86f759458",
                "externalIds": {
                    "DBLP": "conf/cvpr/0003LLJL022",
                    "ArXiv": "2111.13359",
                    "DOI": "10.1109/CVPR52688.2022.00449",
                    "CorpusId": 244709555
                },
                "corpusId": 244709555,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9fb2744ef2b91033de39c121be25d3f86f759458",
                "title": "Neural Collaborative Graph Machines for Table Structure Recognition",
                "abstract": "Recently, table structure recognition has achieved impressive progress with the help of deep graph models. Most of them exploit single visual cues of tabular elements or simply combine visual cues with other modalities via early fusion to reason their graph relationships. However, neither early fusion nor individually reasoning in terms of multiple modalities can be appropriate for all varieties of table structures with great diversity. Instead, different modalities are expected to collaborate with each other in different patterns for different table cases. In the community, the importance of intrainter modality interactions for table structure reasoning is still unexplored. In this paper, we define it as heterogeneous table structure recognition (HeteroTSR) problem. With the aim offilling this gap, we present a novel Neural Collaborative Graph Machines (NCGM) equipped with stacked collaborative blocks, which alternatively extracts intramodality context and models inter-modality interactions in a hierarchical way. It can represent the intrainter modality relationships of tabular elements more robustly, which significantly improves the recognition performance. We also show that the proposed NCGM can modulate collaborative pattern of different modalities conditioned on the context of intramodality cues, which is vital for diversified table cases. Experimental results on benchmarks demonstrate our proposed NCGM achieves state-of-the-art performance and beats other contemporary methods by a large margin especially under challenging scenarios.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48446712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2149124539",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "91 - - - - - - - - - - - SPLERGE(H) [30] Private 0.",
                "While these methods contribute to significant progress, they make certain assumptions like the availability of accurate word bounding boxes, machine readable PDF documents, and others, as additional inputs [18, 22, 4, 30].",
                "95 - - - - - - - - - - - SPLIT [30] Private SEC 0.",
                "Table structure recognition generates a machine-interpretable output for a given table image, which encodes its layout according to a pre-defined standard [30, 17, 20, 42, 4, 39, 24].",
                "In order to compare our method against others on TUCD dataset, we develop our implementation of DeepDeSRT [26], and use open source implementations of DGCNN (TIES) [22], SPLERGE [30], and TabStruct-Net [24].",
                "A combination of heuristics and deep learning methods was also proposed [30] based on splitting the table into sub-cells, and then merging semantically connected"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d499cd8a77e7fc4e12d1f5bd7875ce2e3f5465a4",
                "externalIds": {
                    "DBLP": "conf/wacv/RajaMV22",
                    "ArXiv": "2111.07129",
                    "DOI": "10.1109/WACV51458.2022.00260",
                    "CorpusId": 240285297
                },
                "corpusId": 240285297,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/d499cd8a77e7fc4e12d1f5bd7875ce2e3f5465a4",
                "title": "Visual Understanding of Complex Table Structures from Document Images",
                "abstract": "Table structure recognition is necessary for a comprehensive understanding of documents. Tables in unstructured business documents are tough to parse due to the high diversity of layouts, varying alignments of contents, and the presence of empty cells. The problem is particularly difficult because of challenges in identifying individual cells using visual or linguistic contexts or both. Accurate detection of table cells (including empty cells) simplifies structure extraction and hence, it becomes the prime focus of our work. We propose a novel object-detection-based deep model that captures the inherent alignments of cells within tables and is fine-tuned for fast optimization. Despite accurate detection of cells, recognizing structures for dense tables may still be challenging because of difficulties in capturing long-range row/column dependencies in presence of multi-row/column spanning cells. Therefore, we also aim to improve structure recognition by deducing a novel rectilinear graph-based formulation. From a semantics perspective, we highlight the significance of empty cells in a table. To take these cells into account, we suggest an enhancement to a popular evaluation criterion. Finally, we introduce a modestly sized evaluation dataset with an annotation style inspired by human cognition to encourage new approaches to the problem. Our framework improves the previous state-of-the-art performance by a 2.7% average F1-score on benchmark datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2053910624",
                        "name": "S. Raja"
                    },
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "2226828175",
                        "name": "Jawahar C V"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[35] train the SPLERGE to split the grid structure of table and merge adjacent spanning cells, which is still unable to achieve decent results for the skew table images.",
                "DeepDeSRT [32] - - SPLERGE [35] SciTSR + ICDAR-2019 0.",
                "They can be categorized into two groups: non-table-element-based approaches [13, 17, 25, 32, 35, 44] and tableelement-based approaches [2, 19, 27\u201329, 31, 41, 43].",
                "Method #Param FLOPs GPU CPU SPLERGE [35] 0.",
                "To solve this problem, Tensmeyer et al. [35] train the SPLERGE to split the grid structure of table and merge adjacent spanning cells, which is still unable to achieve decent results for the skew table images.",
                "Although the model size of SPLERGE [35] is the smallest among the compared methods, it spends round 7 times GPU time and 12 times CPU time than our Session 10: Industrial Track MM \u201921, October 20\u201324, 2021, Virtual Event, China",
                "Although the model size of SPLERGE [35] is the smallest among the compared methods, it spends round 7 times GPU time and 12 times CPU time than our"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7b99f9995a4b086c0e6913a8d2f109a5dbaa5014",
                "externalIds": {
                    "DBLP": "conf/mm/LiuLLJLRJ21",
                    "DOI": "10.1145/3474085.3481534",
                    "CorpusId": 239011898
                },
                "corpusId": 239011898,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7b99f9995a4b086c0e6913a8d2f109a5dbaa5014",
                "title": "Show, Read and Reason: Table Structure Recognition with Flexible Context Aggregator",
                "abstract": "We investigate the challenging problem of table structure recognition in this work. Many recent methods adopt graph-based context aggregator with strong inductive bias to reason sparse contextual relationships of table elements. However, the strong constraints may be too restrictive to represent the complicated table relationships. In order to learn more appropriate inductive bias from data, we try to introduce Transformer as context aggregator in this work. Nevertheless, Transformer taking dense context as input requires larger scale data and may suffer from unstable training procedure due to the weakening of inductive bias. To overcome the above limitations, we in this paper design a FLAG (FLexible context AGgregator), which marries Transformer with graph-based context aggregator in an adaptive way. Based on FLAG, an end-to-end framework requiring no extra meta-data or OCR information, termed FLAG-Net, is proposed to flexibly modulate the aggregation of dense context and sparse one for the relational reasoning of table elements. We investigate the modulation pattern in FLAG and show what contextual information is focused, which is vital for recognizing table structure. Extensive experimental results on benchmarks demonstrate the performance of our proposed FLAG-Net surpasses other compared methods by a large margin.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48446712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2149124539",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    },
                    {
                        "authorId": "145592290",
                        "name": "R. Ji"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7a4c901e00fef01d66efb125bf0e2fdd773d8bc6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-02199",
                    "ArXiv": "2109.02199",
                    "DOI": "10.1109/ICCV48922.2021.00098",
                    "CorpusId": 237420694
                },
                "corpusId": 237420694,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/7a4c901e00fef01d66efb125bf0e2fdd773d8bc6",
                "title": "Parsing Table Structures in the Wild",
                "abstract": "This paper tackles the problem of table structure parsing (TSP) from images in the wild. In contrast to existing studies that mainly focus on parsing well-aligned tabular images with simple layouts from scanned PDF documents, we aim to establish a practical table structure parsing system for real-world scenarios where tabular input images are taken or scanned with severe deformation, bending or occlusions. For designing such a system, we propose an approach named Cycle-CenterNet on the top of CenterNet with a novel cycle-pairing module to simultaneously detect and group tabular cells into structured tables. In the cycle-pairing module, a new pairing loss function is proposed for the network training. Alongside with our Cycle-CenterNet, we also present a large-scale dataset, named Wired Table in the Wild (WTW), which includes well-annotated structure parsing of multiple style tables in several scenes like photo, scanning files, web pages, etc.. In experiments, we demonstrate that our Cycle-CenterNet consistently achieves the best accuracy of table structure parsing on the new WTW dataset by 24.6% absolute improvement evaluated by the TEDS metric. A more comprehensive experimental analysis also validates the advantages of our proposed methods for the TSP task.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143799988",
                        "name": "Rujiao Long"
                    },
                    {
                        "authorId": "2119293226",
                        "name": "Wen Wang"
                    },
                    {
                        "authorId": "2685089",
                        "name": "Nan Xue"
                    },
                    {
                        "authorId": "2112256",
                        "name": "Feiyu Gao"
                    },
                    {
                        "authorId": "2109432908",
                        "name": "Zhibo Yang"
                    },
                    {
                        "authorId": "153709848",
                        "name": "Yongpan Wang"
                    },
                    {
                        "authorId": "39943835",
                        "name": "Gui-Song Xia"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Different from the previous table structure recognition methods [6, 7, 8] which mostly recover the table structure based on the visual modality, we fuse the output features for each basic table grid from both vision and language modalities.",
                "Analyzing tabular data in unstructured documents focuses mainly on three problems: i) table detection: localizing the bounding boxes of tables in documents [18, 19], ii) table structure recognition: parsing only the structural (row and column layout) information of tables [4, 7, 8], and iii) table recognition: parsing both the structural information and content of table cells [6].",
                "Although significant efforts have been made in the past to recognize the internal structure of tables through an automated process [4, 6, 7, 8, 9, 10], most of these methods [4, 11] only focus on simple tables and are hard to accurately recognize the structure of complex tables."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a0a97c129f1c8339b9fb4fd768cf9f52e64b2b0a",
                "externalIds": {
                    "ArXiv": "2107.05214",
                    "DBLP": "journals/corr/abs-2107-05214",
                    "DOI": "10.1016/j.patcog.2022.108565",
                    "CorpusId": 235795015
                },
                "corpusId": 235795015,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a0a97c129f1c8339b9fb4fd768cf9f52e64b2b0a",
                "title": "Split, embed and merge: An accurate table structure recognizer",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2128660562",
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "authorId": "39557762",
                        "name": "Jianshu Zhang"
                    },
                    {
                        "authorId": "2149030934",
                        "name": "Jun Du"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "For cell spatial location detection, we use the same evaluation metrics with recent methods [25, 27, 29, 28, 20].",
                "In [8, 28, 29], they classify an entire row or column into the cell or non-cell categories instead of the pixel-wise classification.",
                "[29] trained the SPLERGE model on a private dataset and evaluated it on ICDAR13-Table by randomly choosing a subset as well.",
                "2, with the success of deep learning, recent deep learning-based table structure recognition approaches can be divided into three categories: 1) identify cell bounding boxes through visual detection and segmentation methods [25, 27, 29, 28, 8]; 2) transform a table image into the markup sequence, such as LaTeX and HTML [10, 2]; and 3) explore the adjacency relation between different table cells [30, 22, 11]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6e26602986e56d3524c325601674decb05cd8f2b",
                "externalIds": {
                    "DBLP": "conf/iccv/XueYWTL21",
                    "ArXiv": "2106.10598",
                    "DOI": "10.1109/ICCV48922.2021.00133",
                    "CorpusId": 235490364
                },
                "corpusId": 235490364,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/6e26602986e56d3524c325601674decb05cd8f2b",
                "title": "TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition",
                "abstract": "A table arranging data in rows and columns is a very effective data structure, which has been widely used in business and scientific research. Considering large-scale tabular data in online and offline documents, automatic table recognition has attracted increasing attention from the document analysis community. Though human can easily understand the structure of tables, it remains a challenge for machines to understand that, especially due to a variety of different table layouts and styles. Existing methods usually model a table as either the markup sequence or the adjacency matrix between different table cells, failing to address the importance of the logical location of table cells, e.g., a cell is located in the first row and the second column of the table. In this paper, we reformulate the problem of table structure recognition as the table graph reconstruction, and propose an end-to-end trainable table graph reconstruction network (TGRNet) for table structure recognition. Specifically, the proposed method has two main branches, a cell detection branch and a cell logical location branch, to jointly predict the spatial location and the logical location of different cells. Experimental results on three popular table recognition datasets and a new dataset with table graph annotations (TableGraph-350K) demonstrate the effectiveness of the proposed TGRNet for table structure recognition. Code and annotations will be made publicly available at https://github.com/xuewenyuan/TGRNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51469630",
                        "name": "Wenyuan Xue"
                    },
                    {
                        "authorId": "2110431812",
                        "name": "Baosheng Yu"
                    },
                    {
                        "authorId": "2119293226",
                        "name": "Wen Wang"
                    },
                    {
                        "authorId": "2075330732",
                        "name": "Dacheng Tao"
                    },
                    {
                        "authorId": "1612980447",
                        "name": "Qingyong Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Table structure recognizer is our own implementation of the model proposed in [13]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9c567778502d2b44c3bdbce6ad5de54e01a41f33",
                "externalIds": {
                    "ArXiv": "2106.14616",
                    "DBLP": "conf/icdar/Jimeno-YepesZB21",
                    "DOI": "10.1007/978-3-030-86337-1_40",
                    "CorpusId": 235658670
                },
                "corpusId": 235658670,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/9c567778502d2b44c3bdbce6ad5de54e01a41f33",
                "title": "ICDAR 2021 Competition on Scientific Literature Parsing",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1399097376",
                        "name": "Antonio Jimeno-Yepes"
                    },
                    {
                        "authorId": "2113309578",
                        "name": "Xu Zhong"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[33] detects the rows and columns by learning the interval areas\u2019 segmentation between rows/columns and then predicting the indicator to merge the separated cells.",
                "With the great success of deep neural network in computer vision field, works began to focus on the image-based table with more general structures [21,30,24,9,13,36,23,14,33].",
                "Though recent works [30,22,32,31] attempt to predict row/column regions or even invisible grid lines [33], they are limited to handle tables that cross span multiple rows/columns."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ea25282d27368d3d04db91b165b5003d63e335d6",
                "externalIds": {
                    "DBLP": "conf/icdar/QiaoLCZPNRT021",
                    "ArXiv": "2105.06224",
                    "DOI": "10.1007/978-3-030-86549-8_7",
                    "CorpusId": 234482682
                },
                "corpusId": 234482682,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/ea25282d27368d3d04db91b165b5003d63e335d6",
                "title": "LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2065512194",
                        "name": "Liang Qiao"
                    },
                    {
                        "authorId": "2109614687",
                        "name": "Zaisheng Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    },
                    {
                        "authorId": "2151333065",
                        "name": "Peng Zhang"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    },
                    {
                        "authorId": "1490934795",
                        "name": "Yi Niu"
                    },
                    {
                        "authorId": "144850642",
                        "name": "Wenqi Ren"
                    },
                    {
                        "authorId": "51151126",
                        "name": "Wenming Tan"
                    },
                    {
                        "authorId": "144894837",
                        "name": "Fei Wu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "\u2026TABBIE\u2019s pretraining task of corrupt cell detection, which again is practically useful as a postprocessing step after table structure decomposition (Tensmeyer et al., 2019; Raja et al., 2020) because mistakes in predicting row/column/cell boundaries (sometimes compounded by OCR errors) can lead\u2026",
                "\u2026for tabular data, as corrupt cell detection is actually a fundamental task in table structure decomposition pipelines such as (Nishida et al., 2017; Tensmeyer et al., 2019; Raja et al., 2020), in which incorrectly predicted row/column separators or cell boundaries can lead to corrupted cell text.",
                "While this objective was originally motivated as enabling more efficient training compared to BERT\u2019s masked language modeling objective, it is especially suited for tabular data, as corrupt cell detection is actually a fundamental task in table structure decomposition pipelines such as (Nishida et al., 2017; Tensmeyer et al., 2019; Raja et al., 2020), in which incorrectly predicted row/column separators or cell boundaries can lead to corrupted cell text.",
                "We emphasize that this pretraining objective is a fundamental task in table structure decomposition pipelines (Nishida et al., 2017; Tensmeyer et al., 2019; Raja et al., 2020), in which incorrectly predicting row/column separators or cell boundaries leads to corrupted cell text.",
                "We first examine how TaBERT performs on TABBIE\u2019s pretraining task of corrupt cell detection, which again is practically useful as a postprocessing step after table structure decomposition (Tensmeyer et al., 2019; Raja et al., 2020) because mistakes in predicting row/column/cell boundaries (sometimes compounded by OCR errors) can lead to inaccurate extraction."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "386bfd0e411dee4f512a8737c55dd84846981182",
                "externalIds": {
                    "DBLP": "conf/naacl/IidaTMI21",
                    "ACL": "2021.naacl-main.270",
                    "ArXiv": "2105.02584",
                    "MAG": "3158303960",
                    "DOI": "10.18653/V1/2021.NAACL-MAIN.270",
                    "CorpusId": 233864627
                },
                "corpusId": 233864627,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/386bfd0e411dee4f512a8737c55dd84846981182",
                "title": "TABBIE: Pretrained Representations of Tabular Data",
                "abstract": "Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model\u2019s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49093685",
                        "name": "H. Iida"
                    },
                    {
                        "authorId": "2064894364",
                        "name": "Dung Ngoc Thai"
                    },
                    {
                        "authorId": "1977256",
                        "name": "Varun Manjunatha"
                    },
                    {
                        "authorId": "2136562",
                        "name": "Mohit Iyyer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We intentionally leave out the Partial Detections, Missed Segments, and False Positive Detections from our evaluation metrics, as due to the problem formulation of Split Model [23] they always evaluate to zero.",
                "[23] on table structure recognition, we see a more stable and natural approach towards the formulation of problem.",
                "Following the promising results of Split-model [23] trained on the publicly available ICDAR 2013 dataset using TabAug, we believe our work provides a strong foundation for numerous future extensions.",
                "[23], even though more data-efficient than Qasim et al.",
                "[23] formulating the problem of structure recognition as a combination of row, column splits and cell mergings for defining the structure of a table.",
                "This ground-truth format is used for the augmentation process, however, Split model [23] requires pixel-wise annotations.",
                "promising results of Split-model [23] trained on the publicly available ICDAR 2013 dataset using TabAug, we believe our work provides a strong foundation for numerous future extensions.",
                "[23] on ICDAR 2013 dataset reveals sub-optimal results than the network potential due to the lack of diverse and large training dataset.",
                "To evaluate the efficacy of our proposed augmentation methodology, we train the Split model proposed by [23] on ICDAR 2013 dataset."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5dbb224c85d767730e880c3226c9dc5d59cb765f",
                "externalIds": {
                    "ArXiv": "2104.14237",
                    "DBLP": "journals/corr/abs-2104-14237",
                    "DOI": "10.1007/978-3-030-86331-9_38",
                    "CorpusId": 233443846
                },
                "corpusId": 233443846,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/5dbb224c85d767730e880c3226c9dc5d59cb765f",
                "title": "TabAug: Data Driven Augmentation for Enhanced Table Structure Recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2054749978",
                        "name": "U. Khan"
                    },
                    {
                        "authorId": "49192264",
                        "name": "Sohaib Zahid"
                    },
                    {
                        "authorId": "2111291830",
                        "name": "Muhammad Asad Ali"
                    },
                    {
                        "authorId": "1403326950",
                        "name": "A. Ul-Hasan"
                    },
                    {
                        "authorId": "1688013",
                        "name": "F. Shafait"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[55] Dilated Convolutions in Fully Convolutional Networks (Section III-B4b)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bc8863440e26e48866a64f63438051d24880b793",
                "externalIds": {
                    "DBLP": "journals/access/HashmiLSAAA21",
                    "ArXiv": "2104.14272",
                    "DOI": "10.1109/ACCESS.2021.3087865",
                    "CorpusId": 233444247
                },
                "corpusId": 233444247,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bc8863440e26e48866a64f63438051d24880b793",
                "title": "Current Status and Performance Analysis of Table Recognition in Document Images With Deep Neural Networks",
                "abstract": "The first phase of table recognition is to detect the tabular area in a document. Subsequently, the tabular structures are recognized in the second phase in order to extract information from the respective cells. Table detection and structural recognition are pivotal problems in the domain of table understanding. However, table analysis is a perplexing task due to the colossal amount of diversity and asymmetry in tables. Therefore, it is an active area of research in document image analysis. Recent advances in the computing capabilities of graphical processing units have enabled the deep neural networks to outperform traditional state-of-the-art machine learning methods. Table understanding has substantially benefited from the recent breakthroughs in deep neural networks. However, there has not been a consolidated description of the deep learning methods for table detection and table structure recognition. This review paper provides a thorough analysis of the modern methodologies that utilize deep neural networks. Moreover, it presents a comprehensive understanding of the current state-of-the-art and related challenges of table understanding in document images. The leading datasets and their intricacies have been elaborated along with the quantitative results. Furthermore, a brief overview is given regarding the promising directions that can further improve table analysis in document images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "2206035070",
                        "name": "Muhammad Adnan Afzal"
                    },
                    {
                        "authorId": "2206035072",
                        "name": "Muhammad Ahtsham Afzal"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2a242e4a54323eee9e0f514510b008d9b2119641",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-10538",
                    "ArXiv": "2104.10538",
                    "DOI": "10.1109/ACCESS.2021.3103413",
                    "CorpusId": 233324171
                },
                "corpusId": 233324171,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2a242e4a54323eee9e0f514510b008d9b2119641",
                "title": "Guided Table Structure Recognition Through Anchor Optimization",
                "abstract": "This paper presents the novel approach towards table structure recognition by leveraging the guided anchors. The concept differs from current state-of-the-art systems for table structure recognition that naively apply object detection methods. In contrast to prior techniques, first, we estimate the viable anchors for table structure recognition. Subsequently, these anchors are exploited to locate the rows and columns in tabular images. Furthermore, the paper introduces a simple and effective method that improves the results using tabular layouts in realistic scenarios. The proposed method is exhaustively evaluated on the two publicly available datasets of table structure recognition: ICDAR-2013 and TabStructDB. Moreover, we empirically established the validity of our method by implementing it on the previous approaches. We accomplished state-of-the-art results on the ICDAR-2013 dataset with an average F1-measure of 94.19% (92.06% for rows and 96.32% for columns). Thus, a relative error reduction of more than 25% is achieved. Furthermore, our proposed post-processing improves the average F1-measure to 95.46% that results in a relative error reduction of more than 35%. Moreover, we surpassed the baseline results on the TabStructDB dataset with an average F1-measure of 94.57% (94.08% for rows and 95.06% for columns).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "2207509298",
                        "name": "Muhammad Noman Afzal"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[6] proposed a top-down and then bottomup two-stage table structure recognition method called SPLERGE, which is divided into two parts: Split and Merge."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e9c50d3d75798ef5265a2ab3253669df6246267d",
                "externalIds": {
                    "DBLP": "conf/icmlc2/KongBWCZ21",
                    "DOI": "10.1145/3457682.3457752",
                    "CorpusId": 235495067
                },
                "corpusId": 235495067,
                "publicationVenue": {
                    "id": "d79c0d2a-37de-4287-87e7-1e57576dcae7",
                    "name": "International Conference on Machine Learning and Computing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Mach Learn Cybern",
                        "International Conference on Machine Learning and Cybernetics",
                        "International Conference Machine Learning and Computing",
                        "Int Conf Mach Learn Comput",
                        "ICMLC"
                    ],
                    "url": "http://www.icmlc.com/"
                },
                "url": "https://www.semanticscholar.org/paper/e9c50d3d75798ef5265a2ab3253669df6246267d",
                "title": "A Gradient heatmap based Table Structure Recognition",
                "abstract": "Most methods to recognize the structure of a table are to use the object detection approach to directly locate each cell in the table or to segment the table line based on the fully convolutional network (FCN). The problem of the former is that it is laborious to recognize the distorted table, while the problem of the latter is that the sample imbalance makes it difficult to train the model. In this paper, a gradient heatmap based table structure recognition method is proposed, by exploring the gradient heatmaps of the vertical lines and horizontal lines in the table. Specifically, the pixels of the vertical lines of the table are obtained according to the gradient heatmap, then the pixels of the horizontal lines are obtained using the same method, and finally the table structure is restored by using the connected domain search method. Compared with the Single Shot MultiBox Detector (SSD) and Faster RCNN that directly detects cells, our Average Precision (AP) value reached up to 99.5%, which is much higher than the above models. Additionally, we demonstrate that the AP values of the proposed models are reduced almost negligibly when the IoU threshold increased from 0.5 to 0.75, while the AP value of the fast RCNN and SSD model decreased significantly.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2069275327",
                        "name": "Lingjun Kong"
                    },
                    {
                        "authorId": "2106679362",
                        "name": "Yunchao Bao"
                    },
                    {
                        "authorId": "1934355987",
                        "name": "Qianwen Wang"
                    },
                    {
                        "authorId": "47238733",
                        "name": "Lijun Cao"
                    },
                    {
                        "authorId": "2903770",
                        "name": "Shengmei Zhao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9dbb26e4f04f38aa2a11289cbc82a2d9ed533741",
                "externalIds": {
                    "ArXiv": "2102.08445",
                    "DBLP": "journals/corr/abs-2102-08445",
                    "DOI": "10.1145/3397482.3450718",
                    "CorpusId": 231942681
                },
                "corpusId": 231942681,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9dbb26e4f04f38aa2a11289cbc82a2d9ed533741",
                "title": "TableLab: An Interactive Table Extraction System with Adaptive Deep Learning",
                "abstract": "Table extraction from PDF and image documents is a ubiquitous task in the real-world. Perfect extraction quality is difficult to achieve with one single out-of-box model due to (1) the wide variety of table styles, (2) the lack of training data representing this variety and (3) the inherent ambiguity and subjectivity of table definitions between end-users. Meanwhile, building customized models from scratch can be difficult due to the expensive nature of annotating table data. We attempt to solve these challenges with TableLab by providing a system where users and models seamlessly work together to quickly customize high-quality extraction models with a few labelled examples for the user\u2019s document collection, which contains pages with tables. Given an input document collection, TableLab first detects tables with similar structures (templates) by clustering embeddings from the extraction model. Document collections often contain tables created with a limited set of templates or similar structures. It then selects a few representative table examples already extracted with a pre-trained base deep learning model. Via an easy-to-use user interface, users provide feedback to these selections without necessarily having to identify every single error. TableLab then applies such feedback to finetune the pre-trained model and returns the results of the finetuned model back to the user. The user can choose to repeat this process iteratively until obtaining a customized model with satisfactory performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31856827",
                        "name": "N. Wang"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    },
                    {
                        "authorId": "1718694",
                        "name": "Yunyao Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To capture long-range column and row features globally, we add our proposed TPM at each downsampling shortcut of ResNet backbone inspired by [17], where they utilize the row projection and column projection with maximum pooling to downsample.",
                "In [17] an approach of capturing global information with row pooling and column pooling was used.",
                "More recently, [17] extended the traditional approach of detecting table separation lines by predicting table grid with Split model and predicting the merge of grid elements with Merge model."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "be18da882d59ee35b0b4545c90d120d6e9bc3612",
                "externalIds": {
                    "DBLP": "conf/icpr/WeiLZC20",
                    "DOI": "10.1109/ICPR48806.2021.9413122",
                    "CorpusId": 233877793
                },
                "corpusId": 233877793,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/be18da882d59ee35b0b4545c90d120d6e9bc3612",
                "title": "Image-based Table Cell Detection: a Novel Table Structure Decomposition Method with New Dataset",
                "abstract": "Recently deep learning has been applied to decompose table structure with the main ideas of detecting table lines and then forming table cells. However, the existing methods face problems in dealing with tables with rotation or no internal table lines. To tackle these problems, we propose a novel table structure decomposition method, which directly detects table cells as objects and creates table structure. Extensions to the existing object detection models including effective table projection module are proposed to adapt to the table cell detection. To support the training of the enhanced models, we create a large image-based table dataset TableCell with cell level annotations. A novel and efficient semi-supervised method is proposed to annotate this new dataset. Experiments demonstrate that our proposed table structure decomposition method is simple, effective and robust to the tables without table lines or with rotation. Our dataset and code will be made available11https://github.com/weidafeng/TableCell.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2088884394",
                        "name": "Dafeng Wei"
                    },
                    {
                        "authorId": "46385957",
                        "name": "Hongtao Lu"
                    },
                    {
                        "authorId": "46433090",
                        "name": "Yi Zhou"
                    },
                    {
                        "authorId": "153819461",
                        "name": "Kai Chen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "SPLERGE [20] model is free from spanning cells."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5afded5e53199d2bee71c2067afde4b338d6d49c",
                "externalIds": {
                    "DOI": "10.1109/ICSP48669.2020.9321003",
                    "CorpusId": 231682183
                },
                "corpusId": 231682183,
                "publicationVenue": {
                    "id": "63410070-a9b9-46ac-bdec-92b016246795",
                    "name": "International Conference on the Software Process",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Signal Process",
                        "ICSP",
                        "Int Conf Softw Process",
                        "International Conference on Signal Processing"
                    ],
                    "url": "http://www.ece.utexas.edu/~perry/prof/ispa/index.html"
                },
                "url": "https://www.semanticscholar.org/paper/5afded5e53199d2bee71c2067afde4b338d6d49c",
                "title": "A Deep Semantic Segmentation Model for Image-based Table Structure Recognition",
                "abstract": "Table structure recognition is a crucial step for automatic table information extraction. It is conventional to utilize the features such as ruling lines or words for parsing the rows, columns and cells in a table. However, these conventional methods are ineffective for image-based tables when ruling lines are not visible or the words cannot be recognized through the OCR system. In order to overcome these problems, we propose a deep semantic segmentation model for image-based table structure recognition. Specifically, it is an end-to-end semantic segmentation neural network to determine a pixel-wise prediction map for an input table image where the labels are row separator, column separator, cell content and background. Moreover, by making the connected componnet analysis on the prediction map, we can obtain the bounding boxes of row separators, column separators and cell contents, more accurately. Then we number row/column separators in order by coordinate sorting. Thus, we can make full use of relative positions between row/column separators and cell contents, and further assign the row/column number to each cell. Due to the lack of training data, a large amount of synthetic data are automatically generated in our experiments. It is demonstrated by the experimental results that our proposed model is suitable for various table types, which can achieve 0.9769 and 0.9343 average F1 scores on a generative dataset when the IoU threshold is set to 0.6 and 0.8, respectively.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2047123130",
                        "name": "Y. Zou"
                    },
                    {
                        "authorId": "1685259",
                        "name": "Jinwen Ma"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Among these, the problem of table structure recognition has been of high interest in the community [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20].",
                "In the space of document images, researchers have been working on understanding equations [30,31], figures [32,33] and tables [6,7,8,9,10,11,12,13,14,15,16,17].",
                "[10] presented another interesting deep model, called splerge, which is based on the fundamental idea of first splitting the table into sub-cells, and then merging semantically connected sub-cells to preserve the complete table structure.",
                "Table structure recognition is a challenging problem due to complex structures and high variability in table layouts [4,5,6,7,8,9,10,11,12,13,14,15,16,17].",
                "It can be represented in the form of either physical [10,12,14,17] ar X iv :2 01 0.",
                "Many cognitive methods [6,7,8,9,10,11,12,14,15,16,37,38,39,40,41,42,43] have also been presented to understand table structures as they are robust to the input type (whether being scanned images or native digital).",
                "split+heuristic [10] method outperforms tabstruct-net by a small margin, however, it requires icdar-2013 dataset-specific cell merging heuristics and is trained on a considerably larger set of images.",
                ", meta-features extracted from the pdfs [10], content-level bounding boxes from ground truths [12,14] and cell\u2019s location features generated from synthetic dataset [9]).",
                "Table structure recognition refers to representation of a table in a machinereadable format, where its layout is encoded according to a pre-defined standard [10,11,12,13,14,17].",
                "We compare the performance of our tabstruct-net against seven benchmark methods \u2014 deepdesrt [7], tablenet [12], graphtsr [14], splerge [10], dgcnn [9], Bi-directional gru [15] and Image-to-Text [11]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "30860e7c569d777db8d2b6fb0f7c374965ac62dd",
                "externalIds": {
                    "MAG": "3109870706",
                    "DBLP": "conf/eccv/RajaMJ20",
                    "ArXiv": "2010.04565",
                    "DOI": "10.1007/978-3-030-58604-1_5",
                    "CorpusId": 221990801
                },
                "corpusId": 221990801,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/30860e7c569d777db8d2b6fb0f7c374965ac62dd",
                "title": "Table Structure Recognition using Top-Down and Bottom-Up Cues",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2053910624",
                        "name": "S. Raja"
                    },
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "1694502",
                        "name": "C. V. Jawahar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, a lot of works use deep learning method have been proposed [11], [12], [13], [14]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "add0be3435c750b83999802cbfcd32346c978e27",
                "externalIds": {
                    "MAG": "3095052895",
                    "DBLP": "conf/mapr/NguyenD20",
                    "DOI": "10.1109/MAPR49794.2020.9237784",
                    "CorpusId": 226264757
                },
                "corpusId": 226264757,
                "publicationVenue": {
                    "id": "1e3b2c00-e4cd-4b5a-b5f1-23b9e4d77569",
                    "name": "International Conference on Multimedia Analysis and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "MAPR",
                        "International Conference Multimedia Analysis and Pattern Recognition",
                        "Int Conf Multimedia Anal Pattern Recognit"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/add0be3435c750b83999802cbfcd32346c978e27",
                "title": "Extracting Handwritten Regions In Japanese Document Images",
                "abstract": "Extracting handwritten regions in document images is an important task that has received a lot of interest in the image processing community because of its wide application in digitizing documents for recovery, storing, and searching purposes. Typically, the handwritten regions are found using different shapes features in document images such as the vertical and horizontal lines. Those approaches are time-consuming because the processing time depends largely on the complexity of the form (i.e., the number of information boxes). Moreover, we cannot use these features to gain more knowledge about the document. This paper presents a new approach to the problem that includes company's logo and form's title. First, each document is classified into different categories based on its logo and form's name and then the handwritten regions are extracted based on the type of each document. The classification stage is performed using a local feature descriptor. For performance comparison, four local descriptors (SIFT, SURF, BRIEF and ORB) were used to evaluate their performance in detecting and classifying logos in document image. A dataset of more than 2000 Japanese document images was built from 100 unpublished images with varying orientation and scaling to evaluate the accuracy of the proposed method.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2372762",
                        "name": "Kim-Ngan Nguyen"
                    },
                    {
                        "authorId": "2269079",
                        "name": "T. Do"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In the last few years an important research topic in DIAR has been the analysis of tables in contemporary documents by using graph neural networks [49] or dealing directly with PDF documents [50]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2e0789efacf26ea57b361998e3facea4c9a080ec",
                "externalIds": {
                    "PubMedCentral": "8321201",
                    "MAG": "3092694106",
                    "DBLP": "journals/jimaging/LombardiM20",
                    "DOI": "10.3390/jimaging6100110",
                    "CorpusId": 225121284,
                    "PubMed": "34460551"
                },
                "corpusId": 225121284,
                "publicationVenue": {
                    "id": "c0fc53c7-b0ed-487d-9191-1262c8322621",
                    "name": "Journal of Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "J Imaging"
                    ],
                    "issn": "2313-433X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-556372",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jimaging",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-556372"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2e0789efacf26ea57b361998e3facea4c9a080ec",
                "title": "Deep Learning for Historical Document Analysis and Recognition\u2014A Survey",
                "abstract": "Nowadays, deep learning methods are employed in a broad range of research fields. The analysis and recognition of historical documents, as we survey in this work, is not an exception. Our study analyzes the papers published in the last few years on this topic from different perspectives: we first provide a pragmatic definition of historical documents from the point of view of the research in the area, then we look at the various sub-tasks addressed in this research. Guided by these tasks, we go through the different input-output relations that are expected from the used deep learning approaches and therefore we accordingly describe the most used models. We also discuss research datasets published in the field and their applications. This analysis shows that the latest research is a leap forward since it is not the simple use of recently proposed algorithms to previous problems, but novel tasks and novel applications of state of the art methods are now considered. Rather than just providing a conclusive picture of the current research in the topic we lastly suggest some potential future trends that can represent a stimulus for innovative research directions.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "153148164",
                        "name": "F. Lombardi"
                    },
                    {
                        "authorId": "3285734",
                        "name": "S. Marinai"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Deep learning approaches include two categories: (a) End-to-end image-to-sequence models [18, 36]; (b) Object detection based methods [26, 33, 23].",
                "Most existing work on object detection-based methods detect entire rows and column separately, and represents the intersection of detected rows and columns as cells [26, 33]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "73a906a988e54defee536a120125f957059d595e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-00589",
                    "MAG": "3022479961",
                    "ArXiv": "2005.00589",
                    "DOI": "10.1109/WACV48630.2021.00074",
                    "CorpusId": 218487305
                },
                "corpusId": 218487305,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/73a906a988e54defee536a120125f957059d595e",
                "title": "Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context",
                "abstract": "Documents are often used for knowledge sharing and preservation in business and science, within which are tables that capture most of the critical data. Unfortunately, most documents are stored and distributed as PDF or scanned images, which fail to preserve logical table structure. Recent vision-based deep learning approaches have been proposed to address this gap, but most still cannot achieve state-of-the-art results. We present Global Table Extractor (GTE), a vision-guided systematic framework for joint table detection and cell structured recognition, which could be built on top of any object detection model. With GTE-Table, we invent a new penalty based on the natural cell containment constraint of tables to train our table network aided by cell location predictions. GTE-Cell is a new hierarchical cell detection network that leverages table styles. Further, we design a method to automatically label table and cell structure in existing documents to cheaply create a large corpus of training and test data. We use this to enhance PubTabNet with cell labels and create FinTabNet, real-world and complex scientific and financial datasets with detailed table structure annotations to help train and test structure recognition. Our framework surpasses previous state-of-the-art results on the ICDAR 2013 and ICDAR 2019 table competition in both table detection and cell structure recognition. Further experiments demonstrate a greater than 45% improvement in cell structure recognition when compared to a vanilla RetinaNet object detection model in our new out-of-domain FinTabNet.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1662767314",
                        "name": "Xinyi Zheng"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    },
                    {
                        "authorId": "145378077",
                        "name": "Lucian Popa"
                    },
                    {
                        "authorId": "31856827",
                        "name": "N. Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ba86c20ea740c728b80fd6b1965eb3cbb637aaec",
                "externalIds": {
                    "MAG": "3010871581",
                    "DBLP": "journals/corr/abs-2003-07560",
                    "ArXiv": "2003.07560",
                    "DOI": "10.1007/978-3-030-68790-8_50",
                    "CorpusId": 212737040
                },
                "corpusId": 212737040,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ba86c20ea740c728b80fd6b1965eb3cbb637aaec",
                "title": "GFTE: Graph-based Financial Table Extraction",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2111150050",
                        "name": "Yiren Li"
                    },
                    {
                        "authorId": "1390799037",
                        "name": "Zheng Huang"
                    },
                    {
                        "authorId": "3063894",
                        "name": "Junchi Yan"
                    },
                    {
                        "authorId": "46433090",
                        "name": "Yi Zhou"
                    },
                    {
                        "authorId": "2104156250",
                        "name": "Fan Ye"
                    },
                    {
                        "authorId": "8016263",
                        "name": "Xianhui Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "olutional neural network to determine whether the regions contain a table [22]. Fullyconvolutional neural networks, followed by a conditional random \ufb01eld, have also been used for table detection [23]\u2013[25]. In addition, deep neural networks for object detection, such as Faster-RCNN [26], Mask-RCNN [27], and YOLO [28] have been exploited for table detection and row/column segmentation [7], [29]\u2013[31]. Fu"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "99b2a05177c17dd86627863608ac45dc4cfe0446",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1911-10683",
                    "MAG": "2990963180",
                    "ArXiv": "1911.10683",
                    "DOI": "10.1007/978-3-030-58589-1_34",
                    "CorpusId": 208267858
                },
                "corpusId": 208267858,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/99b2a05177c17dd86627863608ac45dc4cfe0446",
                "title": "Image-based table recognition: data, model, and evaluation",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2113309578",
                        "name": "Xu Zhong"
                    },
                    {
                        "authorId": "9585722",
                        "name": "Elaheh Shafieibavani"
                    },
                    {
                        "authorId": "1399097376",
                        "name": "Antonio Jimeno-Yepes"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6c411f52c151fe537ee76c310f45f0dd3da88e92",
                "externalIds": {
                    "DBLP": "series/sbcs/Bhowmik23",
                    "DOI": "10.1007/978-981-99-4277-0",
                    "CorpusId": 260337124
                },
                "corpusId": 260337124,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6c411f52c151fe537ee76c310f45f0dd3da88e92",
                "title": "Document Layout Analysis",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3272848",
                        "name": "Showmik Bhowmik"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44220fb755ed8259a6d1183e47e5e957ab5b0874",
                "externalIds": {
                    "CorpusId": 253857301
                },
                "corpusId": 253857301,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/44220fb755ed8259a6d1183e47e5e957ab5b0874",
                "title": "Relative Layout Matching for Document Data Extraction",
                "abstract": "This thesis explores the field of business document information extraction, emphasizing one-shot learning systems that improve their performance by utilizing a database of previously processed documents. A benchmark to evaluate one-shot information extraction systems was defined and used with a newly created dataset. A novel representation-learning approach to one-shot document information extraction was proposed. For a newly received document, the proposed approach uses learned document representation to first retrieve field representations from similar documents. Retrieved representations are then used to localize information on the newly received document. The proposed method was evaluated and compared against several proposed baselines showing an improvement on fields with high positional variance. The baseline method still achieves better results on fields that remain fixed within the layout.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071515342",
                        "name": "Maty\u00e1s Skalick\u00fd"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Table structure recognizer is our own implementation of the model proposed in [13]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5e2c5df2be48c08f0cf4fc8d72c013c0b98f53b2",
                "externalIds": {
                    "CorpusId": 235692898
                },
                "corpusId": 235692898,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5e2c5df2be48c08f0cf4fc8d72c013c0b98f53b2",
                "title": "J un 2 02 1 ICDAR 2021 Competition on Scientific Literature Parsing",
                "abstract": "Scientific literature contain important information related to cutting-edge innovations in diverse domains. Advances in natural language processing have been driving the fast development in automated information extraction from scientific literature. However, scientific literature is often available in unstructured PDF format. While PDF is great for preserving basic visual elements, such as characters, lines, shapes, etc., on a canvas for presentation to humans, automatic processing of the PDF format by machines presents many challenges. With over 2.5 trillion PDF documents in existence, these issues are prevalent in many other important application domains as well. A critical challenge for automated information extraction from scientific literature is that documents often contain content that is not in natural language, such as figures and tables. Nevertheless, such content usually illustrates key results, messages, or summarizations of the research. To obtain a comprehensive understanding of scientific literature, the automated system must be able to recognize the layout of the documents and parse the non-natural-language content into a machine readable format. Our ICDAR 2021 Scientific Literature Parsing Competition (ICDAR2021SLP) aims to drive the advances specifically in document understanding. ICDAR2021-SLP leverages the PubLayNet and PubTabNet datasets, which provide hundreds of thousands of training and evaluation examples. In Task A, Document Layout Recognition, submissions with the highest performance combine object detection and specialised solutions for the different categories. In Task B, Table Recognition, top submissions rely on methods to identify table components and post-processing methods to generate the table structure and content. Results from both tasks show an impressive performance and opens the possibility for high performance practical applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35207071",
                        "name": "A. Yepes"
                    },
                    {
                        "authorId": "2062708279",
                        "name": "Peter Zhong"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Top-down methods [19,33,34,37] try to split entire table images into rows and columns using detection or segmentation models, then cells can be obtained through row-column intersection."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24dd39035349c38d0bff17bbe5203cfcfd0ad79a",
                "externalIds": {
                    "DBLP": "conf/icdar/LiYZL21",
                    "DOI": "10.1007/978-3-030-86549-8_6",
                    "CorpusId": 237458405
                },
                "corpusId": 237458405,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/24dd39035349c38d0bff17bbe5203cfcfd0ad79a",
                "title": "Adaptive Scaling for Archival Table Structure Recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118890804",
                        "name": "Xiaohe Li"
                    },
                    {
                        "authorId": "145820427",
                        "name": "Fei Yin"
                    },
                    {
                        "authorId": "2870877",
                        "name": "Xu-Yao Zhang"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        }
    ]
}