{
    "offset": 0,
    "data": [
        {
            "isInfluential": true,
            "intents": [
                "background",
                "result"
            ],
            "contexts": [
                "Similar to the interpretation of image generation process [10], we consider the",
                "effective way to remove spurious correlations and help learn causal representations, and has attracted considerable attention in visual and language learning [10].",
                "Image generation process is decomposed into independent causal mechanisms in [10]."
            ],
            "citingPaper": {
                "paperId": "42bd7c26f22e888393c5fd3ed2d31d109e0dcdd6",
                "externalIds": {
                    "DBLP": "journals/tkde/XiaoXLZLZ23",
                    "DOI": "10.1109/TKDE.2023.3250523",
                    "CorpusId": 257276566
                },
                "corpusId": 257276566,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/42bd7c26f22e888393c5fd3ed2d31d109e0dcdd6",
                "title": "Counterfactual Graph Learning for Anomaly Detection on Attributed Networks",
                "abstract": "Graph anomaly detection is attracting remarkable multidisciplinary research interests ranging from finance, healthcare, and social network analysis. Recent advances on graph neural networks have substantially improved the detection performance via semi-supervised representation learning. However, prior work suggests that deep graph-based methods tend to learn spurious correlations. As a result, they fail to generalize beyond training data distribution. In this article, we aim to identify structural and contextual anomaly nodes in an attributed graph. Based on our preliminary data analyses, spurious correlations can be eliminated with causal subgraph interventions. Therefore, we propose a new graph-based anomaly detection model that can learn causal relations for anomaly detection while generalizing to new environments. To handle situations with varying environments, we steer the generative model to manufacture synthetic environment features, which are exerted on realistic subgraphs to generate counterfactual subgraphs. Further, these counterfactual subgraphs help a few-shot anomaly detection model learn transferable and causal relations across different environments. The experiments on three real-world attributed graphs show that the proposed approach achieves the best performance compared to the state-of-the-art baselines and learns robust causal representations resistant to noises and spurious correlations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119300162",
                        "name": "Chunjing Xiao"
                    },
                    {
                        "authorId": "1593903641",
                        "name": "Xovee Xu"
                    },
                    {
                        "authorId": "2113652437",
                        "name": "Yue Lei"
                    },
                    {
                        "authorId": "2737830",
                        "name": "Kunpeng Zhang"
                    },
                    {
                        "authorId": "2210327727",
                        "name": "Siyuan Liu"
                    },
                    {
                        "authorId": "144315735",
                        "name": "Fan Zhou"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Even state-ofthe-art deep learning models, with their high computational cost and carbon footprint [3], tend to learn simple correlations instead of capturing the underlying causal dynamics of the data [4], [5]."
            ],
            "citingPaper": {
                "paperId": "bb54e9062f388a3f176ce5f23555a77b9571e97d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-06315",
                    "ArXiv": "2309.06315",
                    "DOI": "10.48550/arXiv.2309.06315",
                    "CorpusId": 261696960
                },
                "corpusId": 261696960,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bb54e9062f388a3f176ce5f23555a77b9571e97d",
                "title": "Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning",
                "abstract": "A set of variables is the Markov blanket of a random variable if it contains all the information needed for predicting the variable. If the blanket cannot be reduced without losing useful information, it is called a Markov boundary. Identifying the Markov boundary of a random variable is advantageous because all variables outside the boundary are superfluous. Hence, the Markov boundary provides an optimal feature set. However, learning the Markov boundary from data is challenging for two reasons. If one or more variables are removed from the Markov boundary, variables outside the boundary may start providing information. Conversely, variables within the boundary may stop providing information. The true role of each candidate variable is only manifesting when the Markov boundary has been identified. In this paper, we propose a new Tsetlin Machine (TM) feedback scheme that supplements Type I and Type II feedback. The scheme introduces a novel Finite State Automaton - a Context-Specific Independence Automaton. The automaton learns which features are outside the Markov boundary of the target, allowing them to be pruned from the TM during learning. We investigate the new scheme empirically, showing how it is capable of exploiting context-specific independence to find Markov boundaries. Further, we provide a theoretical analysis of convergence. Our approach thus connects the field of Bayesian networks (BN) with TMs, potentially opening up for synergies when it comes to inference and learning, including TM-produced Bayesian knowledge bases and TM-based Bayesian inference.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2493161",
                        "name": "Ole-Christoffer Granmo"
                    },
                    {
                        "authorId": "33999307",
                        "name": "Per-Arne Andersen"
                    },
                    {
                        "authorId": "2143819774",
                        "name": "Lei Jiao"
                    },
                    {
                        "authorId": "2108231795",
                        "name": "Xuan Zhang"
                    },
                    {
                        "authorId": "2158168120",
                        "name": "Christian D. Blakely"
                    },
                    {
                        "authorId": "2239101224",
                        "name": "Tor Tveit"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Even state-of-the-art deep learning models, with their high computational cost and carbon footprint [13], tend to learn simple correlations instead of capturing the underlying causal dynamics of the data [14, 4]."
            ],
            "citingPaper": {
                "paperId": "f1d3d2a1b47063b89f8cc5240cb222eac91a7c95",
                "externalIds": {
                    "ArXiv": "2309.04801",
                    "DBLP": "journals/corr/abs-2309-04801",
                    "DOI": "10.48550/arXiv.2309.04801",
                    "CorpusId": 261682220
                },
                "corpusId": 261682220,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f1d3d2a1b47063b89f8cc5240cb222eac91a7c95",
                "title": "TMComposites: Plug-and-Play Collaboration Between Specialized Tsetlin Machines",
                "abstract": "Tsetlin Machines (TMs) provide a fundamental shift from arithmetic-based to logic-based machine learning. Supporting convolution, they deal successfully with image classification datasets like MNIST, Fashion-MNIST, and CIFAR-2. However, the TM struggles with getting state-of-the-art performance on CIFAR-10 and CIFAR-100, representing more complex tasks. This paper introduces plug-and-play collaboration between specialized TMs, referred to as TM Composites. The collaboration relies on a TM's ability to specialize during learning and to assess its competence during inference. When teaming up, the most confident TMs make the decisions, relieving the uncertain ones. In this manner, a TM Composite becomes more competent than its members, benefiting from their specializations. The collaboration is plug-and-play in that members can be combined in any way, at any time, without fine-tuning. We implement three TM specializations in our empirical evaluation: Histogram of Gradients, Adaptive Gaussian Thresholding, and Color Thermometers. The resulting TM Composite increases accuracy on Fashion-MNIST by two percentage points, CIFAR-10 by twelve points, and CIFAR-100 by nine points, yielding new state-of-the-art results for TMs. Overall, we envision that TM Composites will enable an ultra-low energy and transparent alternative to state-of-the-art deep learning on more tasks and datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2493161",
                        "name": "Ole-Christoffer Granmo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The inputs can be images (Sauer and Geiger 2021), tabular vectors (Wachter, Mittelstadt, and Russell 2017), sequential actions (Tsirtsis, De, and"
            ],
            "citingPaper": {
                "paperId": "6cb1a49f248413f851eb6c345a2006aca8e96192",
                "externalIds": {
                    "DBLP": "conf/aaai/LiuSL23",
                    "DOI": "10.1609/aaai.v37i2.25265",
                    "CorpusId": 259720282
                },
                "corpusId": 259720282,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6cb1a49f248413f851eb6c345a2006aca8e96192",
                "title": "Counterfactual Dynamics Forecasting - a New Setting of Quantitative Reasoning",
                "abstract": "Rethinking and introspection are important elements of human intelligence. To mimic these capabilities, counterfactual reasoning has attracted attention of AI researchers recently, which aims to forecast the alternative outcomes for hypothetical scenarios (\u201cwhat-if\u201d). However, most existing approaches focused on qualitative reasoning (e.g., casual-effect relationship). It lacks a well-defined description of the differences between counterfactuals and facts, as well as how these differences evolve over time. This paper defines a new problem formulation - counterfactual dynamics forecasting - which is described in middle-level abstraction under the structural causal models (SCM) framework and derived as ordinary differential equations (ODEs) as low-level quantitative computation. Based on it, we propose a method to infer counterfactual dynamics considering the factual dynamics as demonstration. Moreover, the evolution of differences between facts and counterfactuals are modelled by an explicit temporal component. The experimental results on two dynamical systems demonstrate the effectiveness of the proposed method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108172209",
                        "name": "Yanzhu Liu"
                    },
                    {
                        "authorId": "2000311149",
                        "name": "Ying Sun"
                    },
                    {
                        "authorId": "2109781618",
                        "name": "J. Lim"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We adopt this hypothesis from Sch\u00f6lkopf et al. [2021], positing that distribution changes typically affect only a sparse or local subset of factors, rather than all factors simultaneously.",
                "If one has access to or can recover this causal structure, it can be used to generate samples from interventional and counterfactual queries [Kocaoglu et al., 2018, Sauer and Geiger, 2021, Nemirovsky et al., 2022]."
            ],
            "citingPaper": {
                "paperId": "1aac63bb81b8bbcd063b80d33ed248094178f6f2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-11281",
                    "ArXiv": "2306.11281",
                    "DOI": "10.48550/arXiv.2306.11281",
                    "CorpusId": 259202902
                },
                "corpusId": 259202902,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1aac63bb81b8bbcd063b80d33ed248094178f6f2",
                "title": "Towards Characterizing Domain Counterfactuals For Invertible Latent Causal Models",
                "abstract": "Learning latent causal models from data has many important applications such as robustness, model extrapolation, and counterfactuals. Most prior theoretic work has focused on full causal discovery (i.e., recovering the true latent variables) but requires strong assumptions such as linearity or fails to have any analysis of the equivalence class of solutions (e.g., IRM). Instead of full causal discovery, we focus on a specific type of causal query called the domain counterfactual, which hypothesizes what a sample would have looked like if it had been generated in a different domain (or environment). Concretely, we assume domain-specific invertible latent structural causal models and a shared invertible observation function, both of which are less restrictive assumptions than prior theoretic works. Under these assumptions, we define domain counterfactually equivalent models and prove that any model can be transformed into an equivalent model via two invertible functions. This constructive property provides a tight characterization of the domain counterfactual equivalence classes. Building upon this result, we prove that every equivalence class contains a model where all intervened variables are at the end when topologically sorted by the causal DAG, i.e., all non-intervened variables have non-intervened ancestors. This surprising result suggests that an algorithm that only allows intervention in the last $k$ latent variables may improve model estimation for counterfactuals. In experiments, we enforce the sparse intervention hypothesis via this theoretic result by constraining that the latent SCMs can only differ in the last few causal mechanisms and demonstrate the feasibility of this algorithm in simulated and image-based experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2031453877",
                        "name": "Sean Kulinski"
                    },
                    {
                        "authorId": "50251628",
                        "name": "Zeyu Zhou"
                    },
                    {
                        "authorId": "2041002856",
                        "name": "Ruqi Bai"
                    },
                    {
                        "authorId": "3124214",
                        "name": "M. Kocaoglu"
                    },
                    {
                        "authorId": "2055998168",
                        "name": "David I. Inouye"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                "(1) Point counterfactual identification has been recently addressed through neural methods [16, 24, 26, 60, 73, 85, 97, 98, 101, 102, 118, 119, 124] but without identifiability results.",
                "Examples are normalizing flows [85]; diffusion models [16, 97]; variational inference [60, 85, 98, 118, 119]; adversarial learning [24, 73, 85, 101, 102, 124], and neural expectation-maximization [26].",
                "(1) The first stream [16, 24, 26, 60, 73, 85, 97, 98, 101, 102, 118, 119, 124] makes no explicit assumptions besides assuming a structural causal model (SCM) with",
                "L 3 C ou nt er fa ct ua l M Deep generative models [16, 24, 60, 85, 97, 98, 101, 102] ; Markovian BGMs [50, 56, 79, 80, 107, 132] ; transport-based counterfactuals [27] CSM (this paper)"
            ],
            "citingPaper": {
                "paperId": "e1f3d6d2b9ebf296e2d1ae359c2822f4f2a0e4d3",
                "externalIds": {
                    "ArXiv": "2306.01424",
                    "DBLP": "journals/corr/abs-2306-01424",
                    "DOI": "10.48550/arXiv.2306.01424",
                    "CorpusId": 259063780
                },
                "corpusId": 259063780,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e1f3d6d2b9ebf296e2d1ae359c2822f4f2a0e4d3",
                "title": "Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model",
                "abstract": "Counterfactual inference aims to answer retrospective ''what if'' questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual identification methods are special cases of our Curvature Sensitivity Model when the bound of the curvature is set to zero. We then propose an implementation of our Curvature Sensitivity Model in the form of a novel deep generative model, which we call Augmented Pseudo-Invertible Decoder. Our implementation employs (i) residual normalizing flows with (ii) variational augmentations. We empirically demonstrate the effectiveness of our Augmented Pseudo-Invertible Decoder. To the best of our knowledge, ours is the first partial identification model for Markovian structural causal models with continuous outcomes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2067160835",
                        "name": "Valentyn Melnychuk"
                    },
                    {
                        "authorId": "2156928565",
                        "name": "Dennis Frauen"
                    },
                    {
                        "authorId": "3207649",
                        "name": "S. Feuerriegel"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "3798f68b213d2b586199be107fce86335fe01cdc",
                "externalIds": {
                    "ArXiv": "2305.19164",
                    "DBLP": "journals/corr/abs-2305-19164",
                    "DOI": "10.48550/arXiv.2305.19164",
                    "CorpusId": 258967791
                },
                "corpusId": 258967791,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3798f68b213d2b586199be107fce86335fe01cdc",
                "title": "LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images",
                "abstract": "We propose an automated algorithm to stress-test a trained visual model by generating language-guided counterfactual test images (LANCE). Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights. We benchmark the performance of a diverse set of pretrained models on our generated data and observe significant and consistent performance drops. We further analyze model sensitivity across different types of edits, and demonstrate its applicability at surfacing previously unknown class-level model biases in ImageNet.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "39351028",
                        "name": "Viraj Prabhu"
                    },
                    {
                        "authorId": "2088846095",
                        "name": "Sriram Yenamandra"
                    },
                    {
                        "authorId": "40424000",
                        "name": "Prithvijit Chattopadhyay"
                    },
                    {
                        "authorId": "50196944",
                        "name": "Judy Hoffman"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "When confounders are present, disentanglement of features exhibiting spurious correlations through generative modeling becomes an arduous task [43, 40, 11].",
                "The application of empirical risk minimization (ERM) [49] to augmented data has proven to be effective in alleviating confounding bias and learning informative features for image data [19, 52, 13, 43].",
                "While previous studies [52, 13, 43] have demonstrated the efficacy of this approach in enhancing downstream task performance, they have predominantly focused on its practical benefits without conducting a comprehensive analysis of data augmentation.",
                "Recent endeavors have made significant strides in addressing spurious correlations stemming from confounding effects in observational data [48, 43, 13, 19, 52, 50, 2].",
                "Image data augmentations encompass a wide range of approaches, ranging from traditional image manipulation techniques such as rotation, flipping, cropping, among others [25, 46, 36, 16, 10, 58, 57, 19], to more recent generative-based augmentations [1, 43, 52, 13] that manipulate higher-level semantic aspects of an image, such as smiling or hair color.",
                ", n}, X := g(Z) (5) U \u223c pU, Zi \u223c pZi ; \u2200Zi \u2208 Zcnf , Zj := fj(pa(Zj)); \u2200Zj \u0338\u2208 Zcnf , X := g(Z) (6) U \u223c pU, Zi \u223c pZi ; \u2200Zi \u2208 Zcnf \u222a {Z0}, Zj := fj(pa(Zj)); \u2200Zj \u0338\u2208 Zcnf \u222a {Z0}, X := g(Z) (7) As explained in \u00a7 2, counterfactual generative networks (CGN) [43] generates counterfactual images by simulating causal model in Eqn 7 above, performing interventions on all of {Z0} \u222a Zcnf .",
                "Addressing confounding biases in trained machine learning models has demonstrated its usefulness in various applications such as zero or few-shot learning [3, 56], disentanglement [47, 40], domain generalization [43, 8, 19], algorithmic fairness [22, 23], healthcare [13, 59].",
                "To overcome this limitation, counterfactual data augmentation has emerged as a promising approach [43, 52, 13, 26, 38, 9].",
                "X Y Gdo({Z0}\u222aZcnf ) [43, 15] Gdo(X) [16, 57, 10, 58] Gdo(Zcnf ) [52, 13] Gdo(Z0)",
                "A recent method known as Counterfactual Generative Networks (CGN)[43] assumes that each image is a result of a composition of three fixed generative factors: shape, texture, and background."
            ],
            "citingPaper": {
                "paperId": "9c86e29c3e911498be0221cf45511acc229795ff",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-18183",
                    "ArXiv": "2305.18183",
                    "DOI": "10.48550/arXiv.2305.18183",
                    "CorpusId": 258960270
                },
                "corpusId": 258960270,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9c86e29c3e911498be0221cf45511acc229795ff",
                "title": "Rethinking Counterfactual Data Augmentation Under Confounding",
                "abstract": "Counterfactual data augmentation has recently emerged as a method to mitigate confounding biases in the training data for a machine learning model. These biases, such as spurious correlations, arise due to various observed and unobserved confounding variables in the data generation process. In this paper, we formally analyze how confounding biases impact downstream classifiers and present a causal viewpoint to the solutions based on counterfactual data augmentation. We explore how removing confounding biases serves as a means to learn invariant features, ultimately aiding in generalization beyond the observed data distribution. Additionally, we present a straightforward yet powerful algorithm for generating counterfactual images, which effectively mitigates the influence of confounding effects on downstream classifiers. Through experiments on MNIST variants and the CelebA datasets, we demonstrate the effectiveness and practicality of our approach.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110653264",
                        "name": "Abbavaram Gowtham Reddy"
                    },
                    {
                        "authorId": "2052114813",
                        "name": "Saketh Bachu"
                    },
                    {
                        "authorId": "151203958",
                        "name": "Saloni Dash"
                    },
                    {
                        "authorId": "2006289314",
                        "name": "Charchit Sharma"
                    },
                    {
                        "authorId": "2109648033",
                        "name": "Amit Sharma"
                    },
                    {
                        "authorId": "1699429",
                        "name": "V. Balasubramanian"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "For instance, in computer vision applications, the image generation process can often be modelled based on a fixed graph (Sauer and Geiger, 2021; Tangemann et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "fe4c9cc477da29f3c482a2119d1a50895797b049",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-17225",
                    "ArXiv": "2305.17225",
                    "DOI": "10.48550/arXiv.2305.17225",
                    "CorpusId": 258959447
                },
                "corpusId": 258959447,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fe4c9cc477da29f3c482a2119d1a50895797b049",
                "title": "Causal Component Analysis",
                "abstract": "Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a corollary, this interventional perspective also leads to new identifiability results for nonlinear ICA -- a special case of CauCA with an empty graph -- requiring strictly fewer datasets than previous results. We introduce a likelihood-based approach using normalizing flows to estimate both the unmixing function and the causal mechanisms, and demonstrate its effectiveness through extensive synthetic experiments in the CauCA and ICA setting.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "153667762",
                        "name": "Wendong Liang"
                    },
                    {
                        "authorId": "2078911543",
                        "name": "Armin Keki'c"
                    },
                    {
                        "authorId": "51135567",
                        "name": "Julius von K\u00fcgelgen"
                    },
                    {
                        "authorId": "28845390",
                        "name": "Simon Buchholz"
                    },
                    {
                        "authorId": "2599082",
                        "name": "M. Besserve"
                    },
                    {
                        "authorId": "31821560",
                        "name": "Luigi Gresele"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Some of these approaches focus on generating a realistic counterfactual sample (such as a \u201cwhat-if\u201d image) instead of quantifying the counterfactual distributions [42, 53, 61].",
                "Counterfactual generative model Generative models, including a variety of deep network architectures such as generative adversarial networks (GAN) and autoencoders, have been recently developed to estimate counterfactual outcomes [3, 12, 14, 19, 28, 31, 32, 42, 52, 53, 61, 65, 67]."
            ],
            "citingPaper": {
                "paperId": "9c2331530d0d3a493297a8496776c52306e98505",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-15742",
                    "ArXiv": "2305.15742",
                    "DOI": "10.48550/arXiv.2305.15742",
                    "CorpusId": 258887484
                },
                "corpusId": 258887484,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9c2331530d0d3a493297a8496776c52306e98505",
                "title": "Counterfactual Generative Models for Time-Varying Treatments",
                "abstract": "Estimating average causal effects is a common practice to test new treatments. However, the average effect ''masks'' important individual characteristics in the counterfactual distribution, which may lead to safety, fairness, and ethical concerns. This issue is exacerbated in the temporal setting, where the treatment is sequential and time-varying, leading to an intricate influence on the counterfactual distribution. In this paper, we propose a novel conditional generative modeling approach to capture the whole counterfactual distribution, allowing efficient inference on certain statistics of the counterfactual distribution. This makes the proposed approach particularly suitable for healthcare and public policy making. Our generative modeling approach carefully tackles the distribution mismatch in the observed data and the targeted counterfactual distribution via a marginal structural model. Our method outperforms state-of-the-art baselines on both synthetic and real data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1635642896",
                        "name": "Shenghao Wu"
                    },
                    {
                        "authorId": "2118883803",
                        "name": "Wen-liang Zhou"
                    },
                    {
                        "authorId": "2108809403",
                        "name": "Minshuo Chen"
                    },
                    {
                        "authorId": "2920254",
                        "name": "Shixiang Zhu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "This is a fairly new area, and relevant to this line of work may be semantic adversarial approaches [8, 12, 17] which rely on generative models in order to generate adversarial examples, and [6] where interpretable controls are discovered for generative models."
            ],
            "citingPaper": {
                "paperId": "a74450ce1701c6c43f542946ea4bd6876c252bc6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-03212",
                    "ArXiv": "2305.03212",
                    "DOI": "10.48550/arXiv.2305.03212",
                    "CorpusId": 258547232
                },
                "corpusId": 258547232,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a74450ce1701c6c43f542946ea4bd6876c252bc6",
                "title": "LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics",
                "abstract": "Trained on a vast amount of data, Large Language models (LLMs) have achieved unprecedented success and generalization in modeling fairly complex textual inputs in the abstract space, making them powerful tools for zero-shot learning. Such capability is extended to other modalities such as the visual domain using cross-modal foundation models such as CLIP, and as a result, semantically meaningful representation are extractable from visual inputs. In this work, we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases. Given a black box model, its training data, and task definition, we first calculate its task-related loss for each data point. We then extract a semantically meaningful representation for each training data point (such as CLIP embeddings from its visual encoder) and train a lightweight diagnosis model which maps this semantically meaningful representation of a data point to its task loss. We show that an ensemble of such lightweight models can be used to generate insights on the performance of the black-box model, in terms of identifying its patterns of failures and biases.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2599451",
                        "name": "Shervin Ardeshir"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026(2009); Peters et al. (2017); Chalupka et al. (2014); Zhang et al. (2022b); Mitrovic et al. (2021); B\u00fchlmann (2020); Zhang et al. (2020); Tang et al. (2020); Sauer and Geiger (2021) has been widely applied to machine learning to identify causal relations and ignore nuisance factors by intervention."
            ],
            "citingPaper": {
                "paperId": "cfc8f4fce3248023f3e3700c8836c6f03ec28042",
                "externalIds": {
                    "ArXiv": "2305.00374",
                    "DBLP": "journals/corr/abs-2305-00374",
                    "DOI": "10.48550/arXiv.2305.00374",
                    "CorpusId": 258426565
                },
                "corpusId": 258426565,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cfc8f4fce3248023f3e3700c8836c6f03ec28042",
                "title": "Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization",
                "abstract": "Adversarial contrastive learning (ACL), without requiring labels, incorporates adversarial data with standard contrastive learning (SCL) and outputs a robust representation which is generalizable and resistant to adversarial attacks and common corruptions. The style-independence property of representations has been validated to be beneficial in improving robustness transferability. Standard invariant regularization (SIR) has been proposed to make the learned representations via SCL to be independent of the style factors. However, how to equip robust representations learned via ACL with the style-independence property is still unclear so far. To this end, we leverage the technique of causal reasoning to propose an adversarial invariant regularization (AIR) that enforces robust representations learned via ACL to be style-independent. Then, we enhance ACL using invariant regularization (IR), which is a weighted sum of SIR and AIR. Theoretically, we show that AIR implicitly encourages the prediction of adversarial data and consistency between adversarial and natural data to be independent of data augmentations. We also theoretically demonstrate that the style-independence property of robust representation learned via ACL still holds in downstream tasks, providing generalization guarantees. Empirically, our comprehensive experimental results corroborate that IR can significantly improve the performance of ACL and its variants on various datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1507120550",
                        "name": "Xilie Xu"
                    },
                    {
                        "authorId": "47539929",
                        "name": "Jingfeng Zhang"
                    },
                    {
                        "authorId": "2152943340",
                        "name": "Feng Liu"
                    },
                    {
                        "authorId": "67154907",
                        "name": "Masashi Sugiyama"
                    },
                    {
                        "authorId": "145977143",
                        "name": "Mohan S. Kankanhalli"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                ", counterfactuals) by making small changes to the original samples, which can be divided into hand-crafted [5], [6] and using causal generative models [19], [20], demonstrating competitive performance."
            ],
            "citingPaper": {
                "paperId": "0338bf755af0cd6b585f179d4964da399fef3ba7",
                "externalIds": {
                    "ArXiv": "2304.13431",
                    "DBLP": "journals/corr/abs-2304-13431",
                    "DOI": "10.48550/arXiv.2304.13431",
                    "CorpusId": 258332092
                },
                "corpusId": 258332092,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0338bf755af0cd6b585f179d4964da399fef3ba7",
                "title": "Implicit Counterfactual Data Augmentation for Deep Neural Networks",
                "abstract": "Machine-learning models are prone to capturing the spurious correlations between non-causal attributes and classes, with counterfactual data augmentation being a promising direction for breaking these spurious associations. However, explicitly generating counterfactual data is challenging, with the training efficiency declining. Therefore, this study proposes an implicit counterfactual data augmentation (ICDA) method to remove spurious correlations and make stable predictions. Specifically, first, a novel sample-wise augmentation strategy is developed that generates semantically and counterfactually meaningful deep features with distinct augmentation strength for each sample. Second, we derive an easy-to-compute surrogate loss on the augmented feature set when the number of augmented samples becomes infinite. Third, two concrete schemes are proposed, including direct quantification and meta-learning, to derive the key parameters for the robust loss. In addition, ICDA is explained from a regularization aspect, with extensive experiments indicating that our method consistently improves the generalization performance of popular depth networks on multiple typical learning scenarios that require out-of-distribution generalization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48667407",
                        "name": "Xiaoling Zhou"
                    },
                    {
                        "authorId": "2061463107",
                        "name": "Ou Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Generative networks have become an emerging technique to model the inference of counterfactual features [3], [10], [19].",
                "CGN [19] disentangles the components of an image into three independent mechanisms that are decided by the class label.",
                "Recall that we use CGN [19] to generate counterfactual samples for each training and testing samples (cf. Section II-B) by intervening the texture of the object (i.e., T represents texture).",
                "For both Animal and Vehicle datasets, we use two widely used data augmentation strategies: 1) randomly cropping\n2We adopt this initialization for fair comparison since the CGN used in L2D is trained on ImageNet [22].\nthe images with random retain ratio in [0.8, 1.0]; and 2) randomly applied horizontal flipping with 50% probability.",
                "Unlike our work which calculates the consensus for every class and utilizes the result for correction, CGN [19] simply adds these samples to the training set and GCM-CF [10] provides binary information about seen/unseen of an image, but do not interfere the inference.",
                "Recall that we use CGN [19] to generate counterfactual samples for each training and testing samples (cf.",
                "(4); 5: Return \u03b8\u0302, CGN, \u03b7\u0302, and \u03c9\u0302.",
                "Among the existing models, we find that Counterfactual Generative Network (CGN) [19] can well support our requirement, which accounts for the mediator between X and Y and consists of components to model TY=y and TX=x.",
                "Considering that CGN is not a perfect generative network, which shows high cognitive uncertainty, we run four repeats for each x\u2217y\u2032 .",
                "We thus directly use CGN as the CI module in the L2D framework to generate the counterfactual samples for all candidate classes: {x\u2217y\u2032 |y\u2032 \u2208 [1, C]}.",
                "/* Testing */ 6: Infer y = f(x|\u03b8\u0302); 7: for y\u2032 = 1 \u2192 C do 8: Infer x\u2217y\u2032 = CGN(x, y\n\u2032); 9: end for\n10: Calculate zy (Eq.",
                "In particular, given a sample x, we generate a counterfactual image x\u2217y\u2032 by feeding CGN with the common textual of class y\u2032 where we enumerate all possible classes y\u2032.",
                "This pattern occurs mostly in images where the shape is not correctly given by CGN.",
                "(1)); 2: Train CGN; 3: Train CM module (optimize Eq.",
                "\u2022 CGN [19]: CGN is a data augmentation method, which generates various counterfactual samples with the imagined texture and background.",
                "Following the rules [19], we generate the same amount of counterfactual samples as the training set to train models.",
                "Apparently, this counterfactual is identifiable as long as TY=y and TX=x are identifiable, which can be easily inferred through texture extraction tools [19]."
            ],
            "citingPaper": {
                "paperId": "34f7ed9673b8e49e44c2e68dc641395947bc7b3e",
                "externalIds": {
                    "DOI": "10.1109/TNNLS.2023.3264712",
                    "CorpusId": 258110708,
                    "PubMed": "37053061"
                },
                "corpusId": 258110708,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/34f7ed9673b8e49e44c2e68dc641395947bc7b3e",
                "title": "Learning to Double-Check Model Prediction From a Causal Perspective.",
                "abstract": "The present machine learning schema typically uses a one-pass model inference (e.g., forward propagation) to make predictions in the testing phase. It is inherently different from human students who double-check the answer during examinations especially when the confidence is low. To bridge this gap, we propose a learning to double-check (L2D) framework, which formulates double check as a learnable procedure with two core operations: recognizing unreliable predictions and revising predictions. To judge the correctness of a prediction, we resort to counterfactual faithfulness in causal theory and design a contrastive faithfulness measure. In particular, L2D generates counterfactual features by imagining: \"what would the sample features be if its label was the predicted class\" and judges the prediction by the faithfulness of the counterfactual features. Furthermore, we design a simple and effective revision module to revise the original model prediction according to the faithfulness. We apply the L2D framework to three classification models and conduct experiments on two public datasets for image classification, validating the effectiveness of L2D in prediction correctness judgment and revision.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214550033",
                        "name": "Xun Deng"
                    },
                    {
                        "authorId": "2163400298",
                        "name": "Fuli Feng"
                    },
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "5462268",
                        "name": "Hanwang Zhang"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "More recently, [9, 23, 26] propose generative methods to synthesize semantically perturbed images to visualize where the target model fails."
            ],
            "citingPaper": {
                "paperId": "28ac6312b46c8341df9f8406d2417d16468346fe",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-15441",
                    "ArXiv": "2303.15441",
                    "DOI": "10.1109/CVPR52729.2023.01119",
                    "CorpusId": 257766385
                },
                "corpusId": 257766385,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/28ac6312b46c8341df9f8406d2417d16468346fe",
                "title": "Zero-Shot Model Diagnosis",
                "abstract": "When it comes to deploying deep vision models, the behavior of these systems must be explicable to ensure confidence in their reliability and fairness. A common approach to evaluate deep learning models is to build a labeled test set with attributes of interest and assess how well it performs. However, creating a balanced test set (i.e., one that is uniformly sampled over all the important traits) is often time-consuming, expensive, and prone to mistakes. The question we try to address is: can we evaluate the sensitivity of deep learning models to arbitrary visual attributes without an annotated test set? This paper argues the case that Zero-shot Model Diagnosis (ZOOM) is possible without the need for a test set nor labeling. To avoid the need for test sets, our system relies on a generative model and CLIP. The key idea is enabling the user to select a set of prompts (relevant to the problem) and our system will automatically search for semantic counterfactual images (i.e., synthesized images that flip the prediction in the case of a binary classifier) using the generative model. We evaluate several visual tasks (classification, key-point detection, and segmentation) in multiple visual domains to demonstrate the viability of our methodology. Extensive experiments demonstrate that our method is capable of producing counterfactual images and offering sensitivity analysis for model diagnosis without the need for a test set.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "32020492",
                        "name": "Jinqi Luo"
                    },
                    {
                        "authorId": "2156070663",
                        "name": "Zhaoning Wang"
                    },
                    {
                        "authorId": "114621402",
                        "name": "Chen Henry Wu"
                    },
                    {
                        "authorId": "145252513",
                        "name": "Dong Huang"
                    },
                    {
                        "authorId": "143867160",
                        "name": "F. D. L. Torre"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "110e47eb13e0e1aed94608a213e7baa709e0bc38",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-02186",
                    "ArXiv": "2303.02186",
                    "DOI": "10.48550/arXiv.2303.02186",
                    "CorpusId": 257365217
                },
                "corpusId": 257365217,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/110e47eb13e0e1aed94608a213e7baa709e0bc38",
                "title": "Causal Deep Learning",
                "abstract": "Causality has the potential to truly transform the way we solve a large number of real-world problems. Yet, so far, its potential remains largely unlocked since most work so far requires strict assumptions which do not hold true in practice. To address this challenge and make progress in solving real-world problems, we propose a new way of thinking about causality - we call this causal deep learning. The framework which we propose for causal deep learning spans three dimensions: (1) a structural dimension, which allows incomplete causal knowledge rather than assuming either full or no causal knowledge; (2) a parametric dimension, which encompasses parametric forms which are typically ignored; and finally, (3) a temporal dimension, which explicitly allows for situations which capture exposure times or temporal structure. Together, these dimensions allow us to make progress on a variety of real-world problems by leveraging (sometimes incomplete) causal knowledge and/or combining diverse causal deep learning methods. This new framework also enables researchers to compare systematically across existing works as well as identify promising research areas which can lead to real-world impact.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2122909265",
                        "name": "Jeroen Berrevoets"
                    },
                    {
                        "authorId": "2052541552",
                        "name": "Krzysztof Kacprzyk"
                    },
                    {
                        "authorId": "8797071",
                        "name": "Z. Qian"
                    },
                    {
                        "authorId": "1729969",
                        "name": "M. Schaar"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Sauer & Geiger (2021) use a deep network to disentangle object shape, object texture and background in natural images."
            ],
            "citingPaper": {
                "paperId": "582efef4fba2ded7d66b0b68dbfbc71654f414a3",
                "externalIds": {
                    "DBLP": "conf/iclr/MonteiroRPCG23",
                    "ArXiv": "2303.01274",
                    "DOI": "10.48550/arXiv.2303.01274",
                    "CorpusId": 257280401
                },
                "corpusId": 257280401,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/582efef4fba2ded7d66b0b68dbfbc71654f414a3",
                "title": "Measuring axiomatic soundness of counterfactual image models",
                "abstract": "We present a general framework for evaluating image counterfactuals. The power and flexibility of deep generative models make them valuable tools for learning mechanisms in structural causal models. However, their flexibility makes counterfactual identifiability impossible in the general case. Motivated by these issues, we revisit Pearl's axiomatic definition of counterfactuals to determine the necessary constraints of any counterfactual inference model: composition, reversibility, and effectiveness. We frame counterfactuals as functions of an input variable, its parents, and counterfactual parents and use the axiomatic constraints to restrict the set of functions that could represent the counterfactual, thus deriving distance metrics between the approximate and ideal functions. We demonstrate how these metrics can be used to compare and choose between different approximate counterfactual inference models and to provide insight into a model's shortcomings and trade-offs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1483573919",
                        "name": "M. Monteiro"
                    },
                    {
                        "authorId": "47841458",
                        "name": "Fabio De Sousa Ribeiro"
                    },
                    {
                        "authorId": "2122367673",
                        "name": "Nick Pawlowski"
                    },
                    {
                        "authorId": "39135119",
                        "name": "Daniel Coelho de Castro"
                    },
                    {
                        "authorId": "1381873274",
                        "name": "B. Glocker"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "CGN, unlike CausalGAN, aims for robust, interpretable classifiers and mitigates shortcut learning [148].",
                "For specific implementations and results in this property, readers can refer to [64, 74, 150, 72, 101, 90].",
                "Empirical evaluations have validated the effectiveness of this approach on the MNIST dataset [65], although achieving complete invariance remains elusive on more complex datasets like ImageNet-9 [67].",
                "In a vein similar to CausalGAN, the Counterfactual Generative Network (CGN) [64] incorporates SCMs into the generator\u2019s architecture to control specific factors of variation, such as shape, texture, and background.",
                "Various benchmark datasets, including MNIST, its derivatives like ColoredMNIST, Wildlife MNIST [64], and Morpho-MNIST [83], have been instrumental in assessing this integration\u2019s effectiveness.",
                "Integrating Causal Principles in DGMs CausalGAN [62] GANs image generation generalization CelebA [63] CGN [64] GANs multi-task (image generation, classification) generalization MNISTs [65, 66], ImageNet [67]",
                "For instance, some studies leverage counterfactuals to train invariant classifiers, bolster out-of-distribution robustness [64], eliminate undesired spurious features, and enhance model resilience [74, 150].",
                "The generator function in CGN is formally described as:\nxgen = C(m,f , b) = m\u2299 f + (1\u2212m)\u2299 b (5)\nwhere m represents the mask, f is the foreground, and b stands for the background."
            ],
            "citingPaper": {
                "paperId": "5f7ce7c66e0c8c60535798757a080bf402a26f18",
                "externalIds": {
                    "ArXiv": "2301.12351",
                    "CorpusId": 256390218
                },
                "corpusId": 256390218,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5f7ce7c66e0c8c60535798757a080bf402a26f18",
                "title": "Emerging Synergies in Causality and Deep Generative Models: A Survey",
                "abstract": "In the field of artificial intelligence (AI), the quest to understand and model data-generating processes (DGPs) is of paramount importance. Deep generative models (DGMs) have proven adept in capturing complex data distributions but often fall short in generalization and interpretability. On the other hand, causality offers a structured lens to comprehend the mechanisms driving data generation and highlights the causal-effect dynamics inherent in these processes. While causality excels in interpretability and the ability to extrapolate, it grapples with intricacies of high-dimensional spaces. Recognizing the synergistic potential, we delve into the confluence of causality and DGMs. We elucidate the integration of causal principles within DGMs, investigate causal identification using DGMs, and navigate an emerging research frontier of causality in large-scale generative models, particularly generative large language models (LLMs). We offer insights into methodologies, highlight open challenges, and suggest future directions, positioning our comprehensive review as an essential guide in this swiftly emerging and evolving area.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "93925973",
                        "name": "Guanglin Zhou"
                    },
                    {
                        "authorId": "25106675",
                        "name": "Shaoan Xie"
                    },
                    {
                        "authorId": "2241095115",
                        "name": "Guangyuan Hao"
                    },
                    {
                        "authorId": "2241215749",
                        "name": "Shiming Chen"
                    },
                    {
                        "authorId": "1938684",
                        "name": "Biwei Huang"
                    },
                    {
                        "authorId": "3087664",
                        "name": "Xiwei Xu"
                    },
                    {
                        "authorId": "2109117515",
                        "name": "Chen Wang"
                    },
                    {
                        "authorId": "2145199748",
                        "name": "Liming Zhu"
                    },
                    {
                        "authorId": "2106357243",
                        "name": "Lina Yao"
                    },
                    {
                        "authorId": "2119016656",
                        "name": "Kun Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Still, as the difficulty of the integration with otherwise prominent success stories of deep learning such as computer vision, with publications at CVPR and such (including (Sauer and Geiger, 2021; Lv et al., 2022; Liu et al., 2022) to mention a select few), becomes apparent, countering opinions start speaking out against causal AI/ML (Bishop, 2021).",
                "\u2026of the integration with otherwise prominent success stories of deep learning such as computer vision, with publications at CVPR and such (including (Sauer and Geiger, 2021; Lv et al., 2022; Liu et al., 2022) to mention a select few), becomes apparent, countering opinions start speaking out against\u2026"
            ],
            "citingPaper": {
                "paperId": "1936525e29e29f51aa2ea261f3abbab8e55514a2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-12570",
                    "ArXiv": "2212.12570",
                    "DOI": "10.48550/arXiv.2212.12570",
                    "CorpusId": 255125552
                },
                "corpusId": 255125552,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1936525e29e29f51aa2ea261f3abbab8e55514a2",
                "title": "Pearl Causal Hierarchy on Image Data: Intricacies & Challenges",
                "abstract": "Many researchers have voiced their support towards Pearl's counterfactual theory of causation as a stepping stone for AI/ML research's ultimate goal of intelligent systems. As in any other growing subfield, patience seems to be a virtue since significant progress on integrating notions from both fields takes time, yet, major challenges such as the lack of ground truth benchmarks or a unified perspective on classical problems such as computer vision seem to hinder the momentum of the research movement. This present work exemplifies how the Pearl Causal Hierarchy (PCH) can be understood on image data by providing insights on several intricacies but also challenges that naturally arise when applying key concepts from Pearlian causality to the study of image data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35745805",
                        "name": "M. Zecevic"
                    },
                    {
                        "authorId": "1387999893",
                        "name": "Moritz Willig"
                    },
                    {
                        "authorId": "39552485",
                        "name": "D. Dhami"
                    },
                    {
                        "authorId": "2113406566",
                        "name": "K. Kersting"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Also, there are known short-cut learning problems with regard to training on RGB images Geirhos et al. (2019; 2020); Sauer & Geiger (2021) \u2013 there is no constraint for overfitting the textures or the discriminative parts of the known classes during training."
            ],
            "citingPaper": {
                "paperId": "959c50155d0b32ef4d472cb6e5aefa9f83b03fe6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-11720",
                    "ArXiv": "2212.11720",
                    "DOI": "10.48550/arXiv.2212.11720",
                    "CorpusId": 254974436
                },
                "corpusId": 254974436,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/959c50155d0b32ef4d472cb6e5aefa9f83b03fe6",
                "title": "GOOD: Exploring Geometric Cues for Detecting Objects in an Open World",
                "abstract": "We address the task of open-world class-agnostic object detection, i.e., detecting every object in an image by learning from a limited number of base object classes. State-of-the-art RGB-based models suffer from overfitting the training classes and often fail at detecting novel-looking objects. This is because RGB-based models primarily rely on appearance similarity to detect novel objects and are also prone to overfitting short-cut cues such as textures and discriminative parts. To address these shortcomings of RGB-based object detectors, we propose incorporating geometric cues such as depth and normals, predicted by general-purpose monocular estimators. Specifically, we use the geometric cues to train an object proposal network for pseudo-labeling unannotated novel objects in the training set. Our resulting Geometry-guided Open-world Object Detector (GOOD) significantly improves detection recall for novel object categories and already performs well with only a few training classes. Using a single\"person\"class for training on the COCO dataset, GOOD surpasses SOTA methods by 5.0% AR@100, a relative improvement of 24%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146285297",
                        "name": "Haiwen Huang"
                    },
                    {
                        "authorId": "47237027",
                        "name": "Andreas Geiger"
                    },
                    {
                        "authorId": "2109979241",
                        "name": "Dan Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "[39] utilizes independent mechanisms to generate images to improve image classification."
            ],
            "citingPaper": {
                "paperId": "6d4c8499fd70b4658191976da43eb316989da904",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-13854",
                    "ArXiv": "2211.13854",
                    "DOI": "10.48550/arXiv.2211.13854",
                    "CorpusId": 254018146
                },
                "corpusId": 254018146,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6d4c8499fd70b4658191976da43eb316989da904",
                "title": "ComCLIP: Training-Free Compositional Image and Text Matching",
                "abstract": "Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-lanaguage pretrained models like CLIP to compositional image and text matching -- a more challenging image and text matching task requiring the model understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel \\textbf{\\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action sub-images and composes CLIP's vision encoder and text encoder to perform evolving matching over compositional text embedding and sub-image embeddings. In this way, ComCLIP can mitigate spurious correlations introduced by the pretrained CLIP models and dynamically evaluate the importance of each component. Experiments on four compositional image-text matching datasets: SVO, ComVG, Winoground, and VL-checklist, and two general image-text retrieval datasets: Flick30K, and MSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts the \\textbf{\\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even without further training or fine-tuning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2052746421",
                        "name": "Kenan Jiang"
                    },
                    {
                        "authorId": "2149253467",
                        "name": "Xuehai He"
                    },
                    {
                        "authorId": "2166702840",
                        "name": "Ruize Xu"
                    },
                    {
                        "authorId": "47120131",
                        "name": "X. Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "70c059a3e761a4eedb4445b104b20dd2927af625",
                "externalIds": {
                    "ArXiv": "2211.14221",
                    "CorpusId": 258958992
                },
                "corpusId": 258958992,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/70c059a3e761a4eedb4445b104b20dd2927af625",
                "title": "Learning Large Causal Structures from Inverse Covariance Matrix via Matrix Decomposition",
                "abstract": "Learning causal structures from observational data is a fundamental yet highly complex problem when the number of variables is large. In this paper, we start from linear structural equation models (SEMs) and investigate ways of learning causal structures from the inverse covariance matrix. The proposed method, called $\\mathcal{O}$-ICID (for {\\it Independence-preserving} Decomposition from Oracle Inverse Covariance matrix), is based on continuous optimization of a type of matrix decomposition that preserves the nonzero patterns of the inverse covariance matrix. We show that $\\mathcal{O}$-ICID provides an efficient way for identifying the true directed acyclic graph (DAG) under the knowledge of noise variances. With weaker prior information, the proposed method gives directed graph solutions that are useful for making more refined causal discovery. The proposed method enjoys a low complexity when the true DAG has bounded node degrees, as reflected by its time efficiency in experiments in comparison with state-of-the-art algorithms.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51931117",
                        "name": "Shuyu Dong"
                    },
                    {
                        "authorId": "2106199883",
                        "name": "Kento Uemura"
                    },
                    {
                        "authorId": "2192513619",
                        "name": "Akito Fujii"
                    },
                    {
                        "authorId": "2116683671",
                        "name": "Shuang Chang"
                    },
                    {
                        "authorId": "31846119",
                        "name": "Yusuke Koyanagi"
                    },
                    {
                        "authorId": "2197887",
                        "name": "Koji Maruhashi"
                    },
                    {
                        "authorId": "69343681",
                        "name": "M. Sebag"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[5], Sauer and Geiger [41] aim to explain a deep classifier using the counterfactual approach.",
                "Recently, Chang et al. [4], Cohen et al. [5], Sauer and Geiger [41] aim to explain a deep classifier using the counterfactual approach."
            ],
            "citingPaper": {
                "paperId": "6dd16c86a1a242426c33a7a2af79d5624e309ef6",
                "externalIds": {
                    "DBLP": "conf/icaif/ZhangBP22",
                    "DOI": "10.1145/3533271.3561722",
                    "CorpusId": 253022312
                },
                "corpusId": 253022312,
                "publicationVenue": {
                    "id": "5276bfc2-b6f9-4564-83fa-df391a9ea260",
                    "name": "International Conference on AI in Finance",
                    "type": "conference",
                    "alternate_names": [
                        "ICAIF",
                        "Int Conf AI Finance"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6dd16c86a1a242426c33a7a2af79d5624e309ef6",
                "title": "An Interpretable Deep Classifier for Counterfactual Generation",
                "abstract": "Counterfactual explanation has been the core of interpretable machine learning, which requires a trained model to be able to not only infer but also justify its inference. This problem is crucial in many fields, such as fintech and the healthcare industry, where accurate decisions and their justifications are equally important. Many studies have leveraged the power of deep generative models for counterfactual generation. However, most focus on vision data and leave the latent space unsupervised. In this paper, we propose a new and general framework that uses a supervised extension to the Variational Auto-Encoder (VAE) with Normalizing Flow (NF) for simultaneous classification and counterfactual generation. We show experiments on two tabular financial data-sets, Lending Club (LCD) and Give Me Some Credit (GMC), which show that the model can achieve a state-of-art level prediction accuracy while also producing meaningful counterfactual examples to interpret and justify the classifier\u2019s decision.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155470511",
                        "name": "Wei Zhang"
                    },
                    {
                        "authorId": "14206973",
                        "name": "Brian Barr"
                    },
                    {
                        "authorId": "143855009",
                        "name": "J. Paisley"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Generating Counterfactuals by Learning ICMs: In a more recent effort, assuming any real-world image is generated with three independent causal mechanisms for shape, texture, background, and a composition mechanism of the first three, (Sauer & Geiger, 2021) developed Counterfactual Generative Networks (CGN) that generate counterfactual images of a given image.",
                "3) variants (Arjovsky et al., 2019; Castro et al., 2019; Sauer & Geiger, 2021): (i) colored morpho MNIST (CM-MNIST), (ii) double colored morpho MNIST (DCM-MNIST), and (iii) wildlife morpho MNIST (WLMMNIST).",
                "Figure 1: (a) causal data generating process considered in this paper (CONIC = Ours); (b) causal data generating process considered in CGN [34].",
                "3) variants [1, 4, 34]: (i) colored morpho MNIST (CM-MNIST), (ii) double colored morpho MNIST (DCMMNIST), and (iii) wildlife morpho MNIST (WLM-MNIST).",
                "Recent years have seen a few efforts to handle the spurious correlations caused by confounding effects in observational data (Tra\u0308uble et al., 2021; Sauer & Geiger, 2021; Goel et al., 2021; Reddy et al., 2022).",
                "We compare CONIC with various baselines including traditional Empirical Risk Minimizer (ERM), Conditional GAN (CGAN) (Goodfellow et al., 2014a), Conditional VAE (CVAE) (Kingma & Welling, 2013), Conditional-\u03b2-VAE (C-\u03b2VAE) (Higgins et al., 2017), AugMix (Hendrycks et al., 2020), CutMix (Yun et al., 2019), Invariant Risk Minimization (IRM) (Arjovsky et al., 2019), and Counterfactual Generative Networks (CGN) (Sauer & Geiger, 2021).",
                "In this work, we begin with quantifying confounding in observational data that is generated by an underlying causal graph (more general than considered by CGN) of the form shown in Figure 1(a).",
                "We compare CONIC with various baselines including Empirical Risk Minimizer (ERM), Conditional GAN (CGAN) [12], Conditional VAE (CVAE) [20], Conditional-\u03b2-VAE (C-\u03b2VAE) [16], AugMix [15], CutMix [43], Invariant Risk Minimization (IRM) [1], and Counterfactual Generative Networks (CGN) [34].",
                "A related recent effort by (Sauer & Geiger, 2021) proposes Counterfactual Generative Networks (CGN) to address this problem using a data augmentation approach.",
                "We also note that the deterministic models such as CGN fail when they are applied to a different task where the number and type of generative factors are not fixed and are difficult to separate (e.g., CelebA).",
                "CGN trains three Generative Adversarial Networks (GANs) (Goodfellow et al., 2014b) to learn shape, texture, background mechanisms and combine these three mechanisms using a composition mechanism g as g(shape, texture, background) = shape texture+(1\u2212 shape) background where is the Hadamard product.",
                "Recent years have seen a few efforts to handle the spurious correlations caused by confounding effects in observational data [36, 34, 11, 32].",
                "\u2026domain generalization, counterfactual generation, algorithmic fairness, healthcare, etc. (Suter et al., 2019; Kilbertus et al., 2020; Atzmon et al., 2020; Zhao et al., 2020; Yue et al., 2021; Sauer & Geiger, 2021; Goel et al., 2021; Dash et al., 2022; Reddy et al., 2022; Dinga et al., 2020).",
                "CGN results in table 1 are obtained with only 1000 counterfactual images as augmented data points.",
                "When we increase the number of counterfactual instances, performance of CGN reduces further.",
                "Time Complexity Analysis: Apart from its simple methodology, CONIC brings\nDataset CONIC CGN\nCM-MNIST 2.76 \u00b1 0.19 103 \u00b1 1.50 DCM-MNIST 2.22 \u00b1 0.01 103 \u00b1 2.04 WLM-MNIST 1.22 \u00b1 0.01 111 \u00b1 2.50\nTable 2: Run time (in minutes) of CONIC compared to CGN on MNIST variants\nadditional advantages in terms of computing time required to train the model that generates counterfactual images.",
                "\u20262014a), Conditional VAE (CVAE) (Kingma & Welling, 2013), Conditional-\u03b2-VAE (C-\u03b2VAE) (Higgins et al., 2017), AugMix (Hendrycks et al., 2020), CutMix (Yun et al., 2019), Invariant Risk Minimization (IRM) (Arjovsky et al., 2019), and Counterfactual Generative Networks (CGN) (Sauer & Geiger, 2021).",
                "Once the independent mechanisms are trained, counterfactual images are generated by sampling a label and a noise vector corresponding to each mechanism and then feeding the input to CGN. Finally, a classifier is trained with both original and counterfactual images to achieve better test time accuracy, showing the usefulness of CGN.",
                "\u2026effort, assuming any real-world image is generated with three independent causal mechanisms for shape, texture, background, and a composition mechanism of the first three, (Sauer & Geiger, 2021) developed Counterfactual Generative Networks (CGN) that generate counterfactual images of a given image.",
                "Generating Counterfactuals by Learning ICMs: In a more recent effort, assuming any real-world image is generated with three independent causal mechanisms for shape, texture, background, and a composition mechanism of the first three, [34] developed Counterfactual Generative Networks (CGN) that generate counterfactual images of a given image.",
                "A related recent effort by [34] proposes Counterfactual Generative Networks (CGN) to address this problem using a data augmentation approach.",
                "As shown in Table 2, the time required to run our method to generate counterfactual images w.r.t. a generative factor Zj is significantly less than CGN that learns deterministic causal mechanisms as discussed in Section 2.",
                "L G\n] 1\n0 D\nec 2\nusing generative modeling when there are confounders is challenging (Sauer & Geiger, 2021; Reddy et al., 2022; Funke et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "54f99b2d05d6805dbdca1a66b3b0b684bcab0700",
                "externalIds": {
                    "ArXiv": "2210.12368",
                    "DBLP": "journals/corr/abs-2210-12368",
                    "DOI": "10.48550/arXiv.2210.12368",
                    "CorpusId": 253098620
                },
                "corpusId": 253098620,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/54f99b2d05d6805dbdca1a66b3b0b684bcab0700",
                "title": "Counterfactual Generation Under Confounding",
                "abstract": ",",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110653264",
                        "name": "Abbavaram Gowtham Reddy"
                    },
                    {
                        "authorId": "151203958",
                        "name": "Saloni Dash"
                    },
                    {
                        "authorId": "2109648033",
                        "name": "Amit Sharma"
                    },
                    {
                        "authorId": "1699429",
                        "name": "V. Balasubramanian"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Additionally, one could use counterfactual generation methods (Karras et al., 2019; Sauer & Geiger, 2021; Pawelczyk et al., 2020) and apply them for \u201cdistributional counterfactuals\u201d which would show what a sample from Ptgt would have looked like if it instead came from Psrc (e.g., Pawelczyk et al.\u2026",
                "Using Grad-CAM (Selvaraju et al., 2016) to explain a ResNet-50 (He et al., 2016) domain classifier (bottom-left) does not lead to actionable insights.",
                "The baseline method of unpaired samples (top-left) requires many more samples to begin to understand the differences across the hospitals domains and using Grad-CAM Selvaraju et al. [2016] to explain a ResNet-50 He et al."
            ],
            "citingPaper": {
                "paperId": "f1c4a605ebdec90036851475cb98b162acd4b096",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-10275",
                    "ArXiv": "2210.10275",
                    "DOI": "10.48550/arXiv.2210.10275",
                    "CorpusId": 252992858
                },
                "corpusId": 252992858,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f1c4a605ebdec90036851475cb98b162acd4b096",
                "title": "Towards Explaining Distribution Shifts",
                "abstract": "A distribution shift can have fundamental consequences such as signaling a change in the operating environment or significantly reducing the accuracy of downstream models. Thus, understanding distribution shifts is critical for examining and hopefully mitigating the effect of such a shift. Most prior work focuses on merely detecting if a shift has occurred and assumes any detected shift can be understood and handled appropriately by a human operator. We hope to aid in these manual mitigation tasks by explaining the distribution shift using interpretable transportation maps from the original distribution to the shifted one. We derive our interpretable mappings from a relaxation of optimal transport, where the candidate mappings are restricted to a set of interpretable mappings. We then inspect multiple quintessential use-cases of distribution shift in real-world tabular, text, and image datasets to showcase how our explanatory mappings provide a better balance between detail and interpretability than baseline explanations by both visual inspection and our PercentExplained metric.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2031453877",
                        "name": "Sean Kulinski"
                    },
                    {
                        "authorId": "3373810",
                        "name": "David I. Inouye"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[Goudet et al., 2017, Yoon et al., 2018, Kocaoglu et al., 2018, Sauer and Geiger, 2021] use generative models to capture a causal perspective on evaluating the effect of interventions on high-dimensional data such as images."
            ],
            "citingPaper": {
                "paperId": "fe141b34c03a3e5f830fee948499f55e59ea8639",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-08139",
                    "ArXiv": "2210.08139",
                    "DOI": "10.48550/arXiv.2210.08139",
                    "CorpusId": 252917666
                },
                "corpusId": 252917666,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fe141b34c03a3e5f830fee948499f55e59ea8639",
                "title": "Partial Identification of Treatment Effects with Implicit Generative Models",
                "abstract": "We consider the problem of partial identification, the estimation of bounds on the treatment effects from observational data. Although studied using discrete treatment variables or in specific causal graphs (e.g., instrumental variables), partial identification has been recently explored using tools from deep generative modeling. We propose a new method for partial identification of average treatment effects(ATEs) in general causal graphs using implicit generative models comprising continuous and discrete random variables. Since ATE with continuous treatment is generally non-regular, we leverage the partial derivatives of response functions to define a regular approximation of ATE, a quantity we call uniform average treatment derivative (UATD). We prove that our algorithm converges to tight bounds on ATE in linear structural causal models (SCMs). For nonlinear SCMs, we empirically show that using UATD leads to tighter and more stable bounds than methods that directly optimize the ATE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1491820409",
                        "name": "Vahid Balazadeh Meresht"
                    },
                    {
                        "authorId": "3043674",
                        "name": "Vasilis Syrgkanis"
                    },
                    {
                        "authorId": "145253891",
                        "name": "R. G. Krishnan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "e9828b6105cc2adc9bda09ea49aae4fbee9d7641",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-00055",
                    "ArXiv": "2210.00055",
                    "DOI": "10.48550/arXiv.2210.00055",
                    "CorpusId": 252683237
                },
                "corpusId": 252683237,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e9828b6105cc2adc9bda09ea49aae4fbee9d7641",
                "title": "MaskTune: Mitigating Spurious Correlations by Forcing to Explore",
                "abstract": "A fundamental challenge of over-parameterized deep learning models is learning meaningful data representations that yield good performance on a downstream task without over-fitting spurious input features. This work proposes MaskTune, a masking strategy that prevents over-reliance on spurious (or a limited number of) features. MaskTune forces the trained model to explore new features during a single epoch finetuning by masking previously discovered features. MaskTune, unlike earlier approaches for mitigating shortcut learning, does not require any supervision, such as annotating spurious features or labels for subgroup samples in a dataset. Our empirical results on biased MNIST, CelebA, Waterbirds, and ImagenNet-9L datasets show that MaskTune is effective on tasks that often suffer from the existence of spurious correlations. Finally, we show that MaskTune outperforms or achieves similar performance to the competing methods when applied to the selective classification (classification with rejection option) task. Code for MaskTune is available at https://github.com/aliasgharkhani/Masktune.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "17803311",
                        "name": "Saeid Asgari Taghanaki"
                    },
                    {
                        "authorId": "1642129353",
                        "name": "A. Khani"
                    },
                    {
                        "authorId": "3218675",
                        "name": "Fereshte Khani"
                    },
                    {
                        "authorId": "2064252419",
                        "name": "A. Gholami"
                    },
                    {
                        "authorId": "2131259572",
                        "name": "Linh-Tam Tran"
                    },
                    {
                        "authorId": "1399132068",
                        "name": "Ali Mahdavi-Amiri"
                    },
                    {
                        "authorId": "3049056",
                        "name": "G. Hamarneh"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                ", the methods considering representations of distribution balance[40, 42, 43], exploiting the effects of covariates confounding learning[52, 65, 66, 67], the methods based on generative adversarial networks[44, 68, 69, 70] , and so forth[56, 33, 71].",
                "com/usaito/counterfactual-cv CGN[70] MNIST,ImageNet Pytorch https://github.",
                "Moreover, CGN[70] proposed more robust and interpretable classifiers that explicitly expose the causal structure of tasks."
            ],
            "citingPaper": {
                "paperId": "ce5d0a8abe1574508ce8475b4916029bb750abf1",
                "externalIds": {
                    "ArXiv": "2209.08860",
                    "CorpusId": 253523500
                },
                "corpusId": 253523500,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ce5d0a8abe1574508ce8475b4916029bb750abf1",
                "title": "A Survey of Deep Causal Models and Their Industrial Applications",
                "abstract": "The concept of causality plays a significant role in human cognition. In the past few decades, causal effect estimation has been well developed in many fields, such as computer science, medicine, economics, and other industrial applications. With the advancement of deep learning, it has been increasingly applied in causal effect estimation against counterfactual data. Typically, deep causal models map the characteristics of covariates to a representation space and then design various objective functions to estimate counterfactual data unbiasedly. Different from the existing surveys on causal models in machine learning, this paper mainly focuses on the overview of the deep causal models, and its core contributions are as follows: 1) we summarize the popularly adopted relevant metrics under multiple treatment, continuous-dose treatment and times series treatment; 2) we cast insight on a comprehensive overview of deep causal models from both timeline of development and method classification perspectives; 3) we outline some typical applications of causal effect estimation to industry; 4) we also endeavor to present a detailed categorization and analysis on relevant datasets, source codes and experiments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118208232",
                        "name": "Zongyu Li"
                    },
                    {
                        "authorId": "46637795",
                        "name": "Zheng Hua Zhu"
                    },
                    {
                        "authorId": "2146374076",
                        "name": "Xiaoning Guo"
                    },
                    {
                        "authorId": "2111073268",
                        "name": "Shuai Zheng"
                    },
                    {
                        "authorId": "145181674",
                        "name": "Zhenyu Guo"
                    },
                    {
                        "authorId": "34729169",
                        "name": "Siwei Qiang"
                    },
                    {
                        "authorId": "2143397278",
                        "name": "Yao Zhao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "A simple way of (approximately) achieving counterfactualinvariant predictors is via counterfactual data augmentations (CDA) [Lu et al., 2020, Kaushik et al., 2019, Sauer and Geiger, 2021], where one augments the training data with inputs generated from different spurious features."
            ],
            "citingPaper": {
                "paperId": "50a5c61b5283e4dc2e0de1fcfd3ed9a525d2973a",
                "externalIds": {
                    "ArXiv": "2209.05104",
                    "DBLP": "journals/corr/abs-2209-05104",
                    "DOI": "10.48550/arXiv.2209.05104",
                    "CorpusId": 252199225
                },
                "corpusId": 252199225,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/50a5c61b5283e4dc2e0de1fcfd3ed9a525d2973a",
                "title": "Bias Challenges in Counterfactual Data Augmentation",
                "abstract": "Deep learning models tend not to be out-of-distribution robust primarily due to their reliance on spurious features to solve the task. Counterfactual data augmentations provide a general way of (approximately) achieving representations that are counterfactual-invariant to spurious features, a requirement for out-of-distribution (OOD) robustness. In this work, we show that counterfactual data augmentations may not achieve the desired counterfactual-invariance if the augmentation is performed by a context-guessing machine, an abstract machine that guesses the most-likely context of a given input. We theoretically analyze the invariance imposed by such counterfactual data augmentations and describe an exemplar NLP task where counterfactual data augmentation by a context-guessing machine does not lead to robust OOD classifiers.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145872582",
                        "name": "S Chandra Mouli"
                    },
                    {
                        "authorId": "2052302442",
                        "name": "Yangze Zhou"
                    },
                    {
                        "authorId": "2055013652",
                        "name": "Bruno Ribeiro"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "For example, do(ai+A\u2032) where A\u2032 \u2208 {15, 20} and a \u2208 [60, 70] results in xcf with the highest median VED, since ai+A\u2032 \u2208 [65, 90] is generally outside the learned range of ages.",
                "Thus far, state-of-the-art causal structure learning frameworks, utilising deep learning components, have been employed to model the data generation process of 2D images [54,41,65,76].",
                "1) can also be built using neural networks to enable this functionality [76,41,65].",
                "Although some work has been done in this field [41,65], Pawlowski et al.",
                "Sauer and Geiger [65] improved upon this by jointly optimising the losses of all high-dimensional mechanisms."
            ],
            "citingPaper": {
                "paperId": "8975f550eed321c203d3990692f82e3f7b112b8f",
                "externalIds": {
                    "ArXiv": "2208.10950",
                    "DBLP": "journals/corr/abs-2208-10950",
                    "DOI": "10.48550/arXiv.2208.10950",
                    "CorpusId": 251741450
                },
                "corpusId": 251741450,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8975f550eed321c203d3990692f82e3f7b112b8f",
                "title": "Deep Structural Causal Shape Models",
                "abstract": "Causal reasoning provides a language to ask important interventional and counterfactual questions beyond purely statistical association. In medical imaging, for example, we may want to study the causal effect of genetic, environmental, or lifestyle factors on the normal and pathological variation of anatomical phenotypes. However, while anatomical shape models of 3D surface meshes, extracted from automated image segmentation, can be reliably constructed, there is a lack of computational tooling to enable causal reasoning about morphological variations. To tackle this problem, we propose deep structural causal shape models (CSMs), which utilise high-quality mesh generation techniques, from geometric deep learning, within the expressive framework of deep structural causal models. CSMs enable subject-specific prognoses through counterfactual mesh generation (\"How would this patient's brain structure change if they were ten years older?\"), which is in contrast to most current works on purely population-level statistical shape modelling. We demonstrate the capabilities of CSMs at all levels of Pearl's causal hierarchy through a number of qualitative and quantitative experiments leveraging a large dataset of 3D brain structures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182427359",
                        "name": "Rajat Rasal"
                    },
                    {
                        "authorId": "39135119",
                        "name": "Daniel Coelho de Castro"
                    },
                    {
                        "authorId": "2122367673",
                        "name": "Nick Pawlowski"
                    },
                    {
                        "authorId": "1709824",
                        "name": "Ben Glocker"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "20b064771f485b1a96bfdf8320e2ebb2f3836cff",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-04226",
                    "ArXiv": "2208.04226",
                    "DOI": "10.48550/arXiv.2208.04226",
                    "CorpusId": 251402963
                },
                "corpusId": 251402963,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/20b064771f485b1a96bfdf8320e2ebb2f3836cff",
                "title": "SKDCGN: Source-free Knowledge Distillation of Counterfactual Generative Networks using cGANs",
                "abstract": "With the usage of appropriate inductive biases, Counterfactual Generative Networks (CGNs) can generate novel images from random combinations of shape, texture, and background manifolds. These images can be utilized to train an invariant classifier, avoiding the wide spread problem of deep architectures learning spurious correlations rather than meaningful ones. As a consequence, out-of-domain robustness is improved. However, the CGN architecture comprises multiple over parameterized networks, namely BigGAN and U2-Net. Training these networks requires appropriate background knowledge and extensive computation. Since one does not always have access to the precise training details, nor do they always possess the necessary knowledge of counterfactuals, our work addresses the following question: Can we use the knowledge embedded in pre-trained CGNs to train a lower-capacity model, assuming black-box access (i.e., only access to the pretrained CGN model) to the components of the architecture? In this direction, we propose a novel work named SKDCGN that attempts knowledge transfer using Knowledge Distillation (KD). In our proposed architecture, each independent mechanism (shape, texture, background) is represented by a student 'TinyGAN' that learns from the pretrained teacher 'BigGAN'. We demonstrate the efficacy of the proposed method using state-of-the-art datasets such as ImageNet, and MNIST by using KD and appropriate loss functions. Moreover, as an additional contribution, our paper conducts a thorough study on the composition mechanism of the CGNs, to gain a better understanding of how each mechanism influences the classification accuracy of an invariant classifier. Code available at: https://github.com/ambekarsameer96/SKDCGN",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1492120929",
                        "name": "Sameer Ambekar"
                    },
                    {
                        "authorId": "2180785491",
                        "name": "Matteo Tafuro"
                    },
                    {
                        "authorId": "51906228",
                        "name": "Ankit Ankit"
                    },
                    {
                        "authorId": "2180790735",
                        "name": "Diego van der Mast"
                    },
                    {
                        "authorId": "2180790664",
                        "name": "Mark Alence"
                    },
                    {
                        "authorId": "144561257",
                        "name": "C. Athanasiadis"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The goal of disentangled representation learning is to construct a compact and interpretable latent representation, by discovering independent factors of variation (FoVs) in the data [4,11,24]."
            ],
            "citingPaper": {
                "paperId": "f571ac7357635407e22706ee6500f689963b76e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-10002",
                    "ArXiv": "2207.10002",
                    "DOI": "10.48550/arXiv.2207.10002",
                    "CorpusId": 250699302
                },
                "corpusId": 250699302,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/f571ac7357635407e22706ee6500f689963b76e8",
                "title": "Overcoming Shortcut Learning in a Target Domain by Generalizing Basic Visual Factors from a Source Domain",
                "abstract": "Shortcut learning occurs when a deep neural network overly relies on spurious correlations in the training dataset in order to solve downstream tasks. Prior works have shown how this impairs the compositional generalization capability of deep learning models. To address this problem, we propose a novel approach to mitigate shortcut learning in uncontrolled target domains. Our approach extends the training set with an additional dataset (the source domain), which is specifically designed to facilitate learning independent representations of basic visual factors. We benchmark our idea on synthetic target domains where we explicitly control shortcut opportunities as well as real-world target domains. Furthermore, we analyze the effect of different specifications of the source domain and the network architecture on compositional generalization. Our main finding is that leveraging data from a source domain is an effective way to mitigate shortcut learning. By promoting independence across different factors of variation in the learned representations, networks can learn to consider only predictive factors and ignore potential shortcut factors during inference.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9359914",
                        "name": "Piyapat Saranrittichai"
                    },
                    {
                        "authorId": "29359383",
                        "name": "Chaithanya Kumar Mummadi"
                    },
                    {
                        "authorId": "3472572",
                        "name": "Claudia Blaiotta"
                    },
                    {
                        "authorId": "2067762722",
                        "name": "Mauricio Mu\u00f1oz"
                    },
                    {
                        "authorId": "2144816511",
                        "name": "Volker Fischer"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The CRM has been wildly adopted in computer vision community for unbiased learning [1,33,38], which is however not been well studied in 3D pose estimation tasks."
            ],
            "citingPaper": {
                "paperId": "26e579bab600515d8ed5f77c4fa43e55b1276499",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-03618",
                    "ArXiv": "2207.03618",
                    "DOI": "10.48550/arXiv.2207.03618",
                    "CorpusId": 250408033
                },
                "corpusId": 250408033,
                "publicationVenue": {
                    "id": "5fbb417b-d7a5-44e6-856d-993f0624ed9c",
                    "name": "Computer Vision and Image Understanding",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Vis Image Underst"
                    ],
                    "issn": "1077-3142",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622809/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/10773142",
                        "http://www.idealibrary.com/links/toc/cviu",
                        "https://www.journals.elsevier.com/computer-vision-and-image-understanding"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/26e579bab600515d8ed5f77c4fa43e55b1276499",
                "title": "PoseGU: 3D Human Pose Estimation with Novel Human Pose Generator and Unbiased Learning",
                "abstract": "3D pose estimation has recently gained substantial interests in computer vision domain. Existing 3D pose estimation methods have a strong reliance on large size well-annotated 3D pose datasets, and they suffer poor model generalization on unseen poses due to limited diversity of 3D poses in training sets. In this work, we propose PoseGU, a novel human pose generator that generates diverse poses with access only to a small size of seed samples, while equipping the Counterfactual Risk Minimization to pursue an unbiased evaluation objective. Extensive experiments demonstrate PoseGU outforms almost all the state-of-the-art 3D human pose methods under consideration over three popular benchmark datasets. Empirical analysis also proves PoseGU generates 3D poses with improved data diversity and better generalization ability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143325988",
                        "name": "S. Guan"
                    },
                    {
                        "authorId": "2149892001",
                        "name": "Haiyan Lu"
                    },
                    {
                        "authorId": "2948393",
                        "name": "Linchao Zhu"
                    },
                    {
                        "authorId": "1694166",
                        "name": "Gengfa Fang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "For robust out-of-domain classification, Sauer and Geiger [28] enables counterfactual generation by disentangling object shape, texture, and background without direct supervision."
            ],
            "citingPaper": {
                "paperId": "9c7ec94901efcbc22656cb0d9924d1716578bfb1",
                "externalIds": {
                    "DBLP": "conf/mm/YuZWZLCX0M22",
                    "ArXiv": "2207.02812",
                    "DOI": "10.1145/3503161.3547935",
                    "CorpusId": 250311554
                },
                "corpusId": 250311554,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9c7ec94901efcbc22656cb0d9924d1716578bfb1",
                "title": "Towards Counterfactual Image Manipulation via CLIP",
                "abstract": "Leveraging StyleGAN's expressivity and its disentangled latent codes, existing methods can achieve realistic editing of different visual attributes such as age and gender of facial images. An intriguing yet challenging problem arises: Can generative models achieve counterfactual editing against their learnt priors? Due to the lack of counterfactual samples in natural datasets, we investigate this problem in a text-driven manner with Contrastive-Language-Image-Pretraining (CLIP), which can offer rich semantic knowledge even for various counterfactual concepts. Different from in-domain manipulation, counterfactual manipulation requires more comprehensive exploitation of semantic knowledge encapsulated in CLIP as well as more delicate handling of editing directions for avoiding being stuck in local minimum or undesired editing. To this end, we design a novel contrastive loss that exploits predefined CLIP-space directions to guide the editing toward desired directions from different perspectives. In addition, we design a simple yet effective scheme that explicitly maps CLIP embeddings (of target text) to the latent space and fuses them with latent codes for effective latent code optimization and accurate editing. Extensive experiments show that our design achieves accurate and realistic editing while driving by target texts with various counterfactual concepts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "101206696",
                        "name": "Yingchen Yu"
                    },
                    {
                        "authorId": "51111483",
                        "name": "Fangneng Zhan"
                    },
                    {
                        "authorId": "153088941",
                        "name": "Rongliang Wu"
                    },
                    {
                        "authorId": "2121386031",
                        "name": "Jiahui Zhang"
                    },
                    {
                        "authorId": "1771189",
                        "name": "Shijian Lu"
                    },
                    {
                        "authorId": "2055099005",
                        "name": "Miaomiao Cui"
                    },
                    {
                        "authorId": "65863521",
                        "name": "Xuansong Xie"
                    },
                    {
                        "authorId": "143863244",
                        "name": "Xiansheng Hua"
                    },
                    {
                        "authorId": "1679209",
                        "name": "C. Miao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "Counterfactual explanation methods [18, 19, 20, 21] help in analysing a classifier by creating several carefully constructed what-if scenarios by perturbing specific features, but are also example-based."
            ],
            "citingPaper": {
                "paperId": "514eec7a2c3c9445b4a8a8408c47594709cf9be1",
                "externalIds": {
                    "ArXiv": "2207.01916",
                    "DBLP": "journals/corr/abs-2207-01916",
                    "DOI": "10.48550/arXiv.2207.01916",
                    "CorpusId": 250279756
                },
                "corpusId": 250279756,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/514eec7a2c3c9445b4a8a8408c47594709cf9be1",
                "title": "Hierarchical Symbolic Reasoning in Hyperbolic Space for Deep Discriminative Models",
                "abstract": "Explanations for \\emph{black-box} models help us understand model decisions as well as provide information on model biases and inconsistencies. Most of the current explainability techniques provide a single level of explanation, often in terms of feature importance scores or feature attention maps in input space. Our focus is on explaining deep discriminative models at \\emph{multiple levels of abstraction}, from fine-grained to fully abstract explanations. We achieve this by using the natural properties of \\emph{hyperbolic geometry} to more efficiently model a hierarchy of symbolic features and generate \\emph{hierarchical symbolic rules} as part of our explanations. Specifically, for any given deep discriminative model, we distill the underpinning knowledge by discretisation of the continuous latent space using vector quantisation to form symbols, followed by a \\emph{hyperbolic reasoning block} to induce an \\emph{abstraction tree}. We traverse the tree to extract explanations in terms of symbolic rules and its corresponding visual semantics. We demonstrate the effectiveness of our method on the MNIST and AFHQ high-resolution animal faces dataset. Our framework is available at \\url{https://github.com/koriavinash1/SymbolicInterpretability}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9696089",
                        "name": "Ainkaran Santhirasekaram"
                    },
                    {
                        "authorId": "35982249",
                        "name": "A. Kori"
                    },
                    {
                        "authorId": "2064337286",
                        "name": "A. Rockall"
                    },
                    {
                        "authorId": "2174812191",
                        "name": "Mathias Winkler"
                    },
                    {
                        "authorId": "49973505",
                        "name": "Francesca Toni"
                    },
                    {
                        "authorId": "1709824",
                        "name": "Ben Glocker"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Among the different forms of explanations, counterfactual explanations are recently gaining attention [9, 10, 16, 17].",
                "There are many different forms of explainability techniques, including feature attribution methods [5], network dissection-based interpretability [6], mechanistic approaches for understanding neural networks [7, 8], and causal/counterfactual explanations [9, 10, 11]."
            ],
            "citingPaper": {
                "paperId": "299d9193e2843c0bed4513d4a647f19206bebb2d",
                "externalIds": {
                    "ArXiv": "2207.01917",
                    "DBLP": "journals/corr/abs-2207-01917",
                    "DOI": "10.48550/arXiv.2207.01917",
                    "CorpusId": 250280171
                },
                "corpusId": 250280171,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/299d9193e2843c0bed4513d4a647f19206bebb2d",
                "title": "GLANCE: Global to Local Architecture-Neutral Concept-based Explanations",
                "abstract": "Most of the current explainability techniques focus on capturing the importance of features in input space. However, given the complexity of models and data-generating processes, the resulting explanations are far from being \u2018complete\u2019, in that they lack an indication of feature interactions and visualization of their \u2018effect\u2019. In this work, we propose a novel twin-surrogate explainability framework to explain the decisions made by any CNN-based image classi\ufb01er (irrespective of the architec-ture). For this, we \ufb01rst disentangle latent features from the classi\ufb01er, followed by aligning these features to observed/human-de\ufb01ned \u2018context\u2019 features. These aligned features form semantically meaningful concepts that are used for extracting a causal graph depicting the \u2018perceived\u2019 data-generating process, describing the inter- and intra-feature interactions between unobserved latent features and observed \u2018context\u2019 features. This causal graph serves as a global model from which local explanations of different forms can be extracted. Speci\ufb01cally, we provide a generator to visualize the \u2018effect\u2019 of interactions among features in latent space and draw feature importance therefrom as local explanations. Our framework utilizes adversarial knowledge distillation to faithfully learn a representation from the classi\ufb01ers\u2019 latent space and use it for extracting visual explanations. We use the styleGAN-v2 architecture with an additional regularization term to enforce disentanglement and alignment. We demonstrate and evaluate explanations obtained with our framework on Morpho-MNIST and on the FFHQ human faces dataset. Our framework is available at https://github.com/koriavinash1/GLANCE-Explanations",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35982249",
                        "name": "A. Kori"
                    },
                    {
                        "authorId": "1709824",
                        "name": "Ben Glocker"
                    },
                    {
                        "authorId": "49973505",
                        "name": "Francesca Toni"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[25,15] introduce causality in their generation process, to produce images from classes."
            ],
            "citingPaper": {
                "paperId": "6207a19c62b61f0451ed4d59887b518e7ebf9e2d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-01651",
                    "ArXiv": "2206.01651",
                    "DOI": "10.48550/arXiv.2206.01651",
                    "CorpusId": 249375240
                },
                "corpusId": 249375240,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6207a19c62b61f0451ed4d59887b518e7ebf9e2d",
                "title": "D'ARTAGNAN: Counterfactual Video Generation",
                "abstract": "Causally-enabled machine learning frameworks could help clinicians to identify the best course of treatments by answering counterfactual questions. We explore this path for the case of echocardiograms by looking into the variation of the Left Ventricle Ejection Fraction, the most essential clinical metric gained from these examinations. We combine deep neural networks, twin causal networks and generative adversarial methods for the first time to build D'ARTAGNAN (Deep ARtificial Twin-Architecture GeNerAtive Networks), a novel causal generative model. We demonstrate the soundness of our approach on a synthetic dataset before applying it to cardiac ultrasound videos to answer the question:\"What would this echocardiogram look like if the patient had a different ejection fraction?\". To do so, we generate new ultrasound videos, retaining the video style and anatomy of the original patient, while modifying the Ejection Fraction conditioned on a given input. We achieve an SSIM score of 0.79 and an R2 score of 0.51 on the counterfactual videos. Code and models are available at: https://github.com/HReynaud/dartagnan.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117569565",
                        "name": "Hadrien Reynaud"
                    },
                    {
                        "authorId": "3468426",
                        "name": "Athanasios Vlontzos"
                    },
                    {
                        "authorId": "2168100904",
                        "name": "Mischa Dombrowski"
                    },
                    {
                        "authorId": "2140052777",
                        "name": "Ciar\u00e1n M. Lee"
                    },
                    {
                        "authorId": "4538457",
                        "name": "A. Beqiri"
                    },
                    {
                        "authorId": "1882894",
                        "name": "P. Leeson"
                    },
                    {
                        "authorId": "2015193",
                        "name": "Bernhard Kainz"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                ", replacing with another image\u2019s texture as done in prior works [7, 17] hinders the model from learning semantically meaningful representations of images in the original dataset and thus negatively affects the model\u2019s accuracy on the in-domain samples.",
                "Motivated by the findings from the prior works [9, 17], we designed the shape-focused augmentation introduced in Sec.",
                "To reduce the model\u2019s heavy reliance on the image\u2019s texture information in classifying images, numerous works [7, 17] proposed to generate images whose texture is modified and to train the model on the generated images along (b) Shape-focused augmentation (a) Original Image",
                "Regarding the superior performance of the model trained with shape-focused augmentation over the model trained on counterfactual images across all OOD datasets, we supposed that it results from the inherent limitations that the model architecture in CGN [17] has.",
                "Sauer-Geiger [17] proposed the generative model to create counterfactual images where the texture in the foreground and background of an object are independently altered to that of other classes in ImageNet.",
                "By utilizing CGN [17]\u2019s method, we generate a set of the counterfactual images where the alteration of texture makes them semantically not resembled with the images of the identical label from the original dataset as shown in Fig.",
                "Additionally, Sauer-Geiger [17] proposed the method differentiating an image into two parts by the shape silhouette of the object and independently altering texture in each divided part to the texture of other classes.",
                "Sauer-Geiger [17] proposed counterfactual generative network (CGN) changing the texture of the foreground and background of an object in the image from ImageNet separately to the texture of other classes.",
                "We implemented the experiment to validate the effectiveness of shape-focused augmentation by comparing it with the prior method [17] in terms of the accuracy on both the original dataset (ImageNet-100) and out-of-distribution samples (OOD dataset [6]).",
                "However, our work also partly shares the same intuition with CGN [17] in that defining visual features in an image more structurally as the foreground and background of the object and giving different variations to each part."
            ],
            "citingPaper": {
                "paperId": "7fee1380b3a3e883ff59d5828232c514474c62eb",
                "externalIds": {
                    "DBLP": "conf/cvpr/LeeHKZ22",
                    "DOI": "10.1109/CVPRW56347.2022.00478",
                    "CorpusId": 251034466
                },
                "corpusId": 251034466,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7fee1380b3a3e883ff59d5828232c514474c62eb",
                "title": "Improving Robustness to Texture Bias via Shape-focused Augmentation",
                "abstract": "Despite significant progress of deep neural networks in image classification, it has been reported that CNNs trained on ImageNet have heavily focused on local texture information, rather than capturing complex visual concepts of the objects. To delve into this phenomenon, recent studies proposed to generate images with modified texture information for training the model. However, these methods largely sacrifice the classification accuracy on the in-domain dataset while achieving improved performance on the out-of-distribution dataset. Motivated by the fact that human tends to focus on shape information, we aim to resolve this issue by proposing a shape-focused augmentation where the texture in the object\u2019s foreground and background are separately changed. Key idea is that by applying different modifications to the inside and outside of an object, not only the bias toward texture is reduced but also the model is induced to focus on shape. Experiments show that the proposed method successfully reduces texture bias and also improves the classification performance on the original dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156555053",
                        "name": "Sangjun Lee"
                    },
                    {
                        "authorId": "2054813535",
                        "name": "Inwoo Hwang"
                    },
                    {
                        "authorId": "71119060",
                        "name": "Gi-Cheon Kang"
                    },
                    {
                        "authorId": "1692756",
                        "name": "Byoung-Tak Zhang"
                    },
                    {
                        "authorId": "2056978544",
                        "name": "Ai"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026In recent years, it has drawn increasing attention\nfrom the machine learning community (Scho\u0308lkopf, 2019; Scho\u0308lkopf et al., 2021), e.g., in few-shot learning (Teshima et al., 2020)(Yue et al., 2020), long-tail classification (Tang et al., 2020), and generative modeling (Sauer & Geiger, 2021)."
            ],
            "citingPaper": {
                "paperId": "27faee1141f69cb909b86635c718f7f409dd099c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-12331",
                    "ArXiv": "2205.12331",
                    "DOI": "10.48550/arXiv.2205.12331",
                    "CorpusId": 249062932
                },
                "corpusId": 249062932,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/27faee1141f69cb909b86635c718f7f409dd099c",
                "title": "Certified Robustness Against Natural Language Attacks by Causal Intervention",
                "abstract": "Deep learning models have achieved great success in many fields, yet they are vulnerable to adversarial examples. This paper follows a causal perspective to look into the adversarial vulnerability and proposes Causal Intervention by Semantic Smoothing (CISS), a novel framework towards robustness against natural language attacks. Instead of merely fitting observational data, CISS learns causal effects p ( y | do ( x )) by smoothing in the latent semantic space to make robust predictions, which scales to deep architectures and avoids tedious construction of noise customized for specific attacks. CISS is provably robust against word substitution attacks, as well as empirically robust even when perturbations are strengthened by unknown attack algorithms. For example, on YELP, CISS surpasses the runner-up by 6.8% in terms of certified robustness against word substitutions, and achieves 80.7% empirical robustness when syntactic attacks are integrated.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146233407",
                        "name": "Haiteng Zhao"
                    },
                    {
                        "authorId": "2149147043",
                        "name": "Chang Ma"
                    },
                    {
                        "authorId": "2029648342",
                        "name": "Xinshuai Dong"
                    },
                    {
                        "authorId": "1755919",
                        "name": "A. Luu"
                    },
                    {
                        "authorId": null,
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "authorId": "2119078220",
                        "name": "Hanwang Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Counterfactual Generative Network (CGN) (Sauer & Geiger, 2020) suggests to decouple the ImageNet generation into four aspects of the shape, texture, background, and composer.",
                "Counterfactual synthesis (Kocaoglu et al., 2018; Sauer & Geiger, 2020; Nemirovsky et al., 2020; Yang et al., 2021; Averitt et al., 2020; Thiagarajan et al., 2021) is one of the most promising tasks to achieve the general goal of knowledge extrapolation in GANs.",
                "CGN explicitly models the causality in the four aspects to yield counterfactual combinations of them, like triumphal arch with the elephant texture."
            ],
            "citingPaper": {
                "paperId": "b5f24d66a147ac7e0534567d55fe386831d24082",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-13444",
                    "ArXiv": "2205.13444",
                    "DOI": "10.48550/arXiv.2205.13444",
                    "CorpusId": 249097850
                },
                "corpusId": 249097850,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b5f24d66a147ac7e0534567d55fe386831d24082",
                "title": "Principled Knowledge Extrapolation with GANs",
                "abstract": "Human can extrapolate well, generalize daily knowledge into unseen scenarios, raise and answer counterfactual questions. To imitate this ability via generative models, previous works have extensively studied explicitly encoding Structural Causal Models (SCMs) into architectures of generator networks. This methodology, however, lim-its the flexibility of the generator as they must be carefully crafted to follow the causal graph, and demands a ground truth SCM with strong ignorability assumption as prior, which is a nontrivial assumption in many real scenarios. Thus, many current causal GAN methods fail to generate high fidelity counterfactual results as they cannot easily leverage state-of-the-art generative models. In this paper, we propose to study counterfactual synthesis from a new perspective of knowledge extrapolation, where a given knowledge dimension of the data distribution is extrapolated, but the remaining knowledge is kept indistinguishable from the original distribution. We show that an adversarial game with a closed-form discriminator can be used to address the knowledge extrapolation problem, and a novel principal knowledge descent method can efficiently estimate the extrapolated distribution through the adversarial game. Our method enjoys both elegant theoretical guarantees and superior performance in many scenarios.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2119237546",
                        "name": "Ruili Feng"
                    },
                    {
                        "authorId": "2166974008",
                        "name": "Jie Xiao"
                    },
                    {
                        "authorId": "84005711",
                        "name": "Kecheng Zheng"
                    },
                    {
                        "authorId": "1678783",
                        "name": "Deli Zhao"
                    },
                    {
                        "authorId": "1709595",
                        "name": "Jingren Zhou"
                    },
                    {
                        "authorId": "2133377336",
                        "name": "Qibin Sun"
                    },
                    {
                        "authorId": "143962510",
                        "name": "Zhengjun Zha"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Previous works have also used GANs and VAEs for counterfactual generation (Goyal et al., 2019; Mertes et al., 2020; Sauer & Geiger, 2021; Singla et al., 2020; Liu et al., 2019; Baumgartner et al., 2018; Chang et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "be1f40bebd38b3e66638da680aad9b3ce21ac5ec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-11642",
                    "ArXiv": "2204.11642",
                    "DOI": "10.48550/arXiv.2204.11642",
                    "CorpusId": 247741267
                },
                "corpusId": 247741267,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/be1f40bebd38b3e66638da680aad9b3ce21ac5ec",
                "title": "Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset",
                "abstract": "A variety of methods exist to explain image classification models. However, whether they provide any benefit to users over simply comparing various inputs and the model's respective predictions remains unclear. We conducted a user study (N=240) to test how such a baseline explanation technique performs against concept-based and counterfactual explanations. To this end, we contribute a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. In a study, we assess if participants can identify the relevant set of attributes compared to the ground-truth. Our results show that the baseline outperformed concept-based explanations. Counterfactual explanations from an invertible neural network performed similarly as the baseline. Still, they allowed users to identify some attributes more accurately. Our results highlight the importance of measuring how well users can reason about biases of a model, rather than solely relying on technical evaluations or proxy tasks. We open-source our study and dataset so it can serve as a blue-print for future studies. For code see, https://github.com/berleon/do_users_benefit_from_interpretable_vision",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7200137",
                        "name": "Leon Sixt"
                    },
                    {
                        "authorId": "153631073",
                        "name": "M. Schuessler"
                    },
                    {
                        "authorId": "32686828",
                        "name": "Oana-Iuliana Popescu"
                    },
                    {
                        "authorId": "2060019404",
                        "name": "Philipp Wei\u00df"
                    },
                    {
                        "authorId": "1748047",
                        "name": "Tim Landgraf"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The learning of directed acyclic graphs (DAGs) is an important problem for probabilistic and causal inference [15,17] with important applications in social sciences [11], genome research [21] and machine learning itself [16,2,18]."
            ],
            "citingPaper": {
                "paperId": "05a871d87e2375359cce267e381d9a67eb1e168a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-04644",
                    "ArXiv": "2204.04644",
                    "DOI": "10.48550/arXiv.2204.04644",
                    "CorpusId": 248084996
                },
                "corpusId": 248084996,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/05a871d87e2375359cce267e381d9a67eb1e168a",
                "title": "From graphs to DAGs: a low-complexity model and a scalable algorithm",
                "abstract": "Learning directed acyclic graphs (DAGs) is long known a critical challenge at the core of probabilistic and causal modeling. The NoTears approach of (Zheng et al., 2018), through a differentiable function involving the matrix exponential trace $\\mathrm{tr}(\\exp(\\cdot))$, opens up a way to learning DAGs via continuous optimization, though with a $O(d^3)$ complexity in the number $d$ of nodes. This paper presents a low-complexity model, called LoRAM for Low-Rank Additive Model, which combines low-rank matrix factorization with a sparsification mechanism for the continuous optimization of DAGs. The main contribution of the approach lies in an efficient gradient approximation method leveraging the low-rank property of the model, and its straightforward application to the computation of projections from graph matrices onto the DAG matrix space. The proposed method achieves a reduction from a cubic complexity to quadratic complexity while handling the same DAG characteristic function as NoTears, and scales easily up to thousands of nodes for the projection problem. The experiments show that the LoRAM achieves efficiency gains of orders of magnitude compared to the state-of-the-art at the expense of a very moderate accuracy loss in the considered range of sparse matrices, and with a low sensitivity to the rank choice of the model's low-rank component.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51931117",
                        "name": "Shuyu Dong"
                    },
                    {
                        "authorId": "69343681",
                        "name": "M. Sebag"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Multiple works on CE use generative models to create tangible changes in the image [28, 48, 51]."
            ],
            "citingPaper": {
                "paperId": "3be678c7d63e66ae5cd7d62a598cbb8f0935fe55",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-15636",
                    "ArXiv": "2203.15636",
                    "DOI": "10.48550/arXiv.2203.15636",
                    "CorpusId": 247779169
                },
                "corpusId": 247779169,
                "publicationVenue": {
                    "id": "a8f26d13-e373-4e48-b57b-ef89bf48f4db",
                    "name": "Asian Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Comput Vis",
                        "ACCV"
                    ],
                    "url": "http://www.cvl.iis.u-tokyo.ac.jp/afcv/"
                },
                "url": "https://www.semanticscholar.org/paper/3be678c7d63e66ae5cd7d62a598cbb8f0935fe55",
                "title": "Diffusion Models for Counterfactual Explanations",
                "abstract": "Counterfactual explanations have shown promising results as a post-hoc framework to make image classifiers more explainable. In this paper, we propose DiME, a method allowing the generation of counterfactual images using the recent diffusion models. By leveraging the guided generative diffusion process, our proposed methodology shows how to use the gradients of the target classifier to generate counterfactual explanations of input instances. Further, we analyze current approaches to evaluate spurious correlations and extend the evaluation measurements by proposing a new metric: Correlation Difference. Our experimental validations show that the proposed algorithm surpasses previous State-of-the-Art results on 5 out of 6 metrics on CelebA.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "94637863",
                        "name": "Guillaume Jeanneret"
                    },
                    {
                        "authorId": "145304110",
                        "name": "Lo\u00efc Simon"
                    },
                    {
                        "authorId": "82117876",
                        "name": "F. Jurie"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "More recently, [40] decomposes image generation into parallel mechanisms (shape, texture, and background) and the distributions over the individual mechanisms are learned."
            ],
            "citingPaper": {
                "paperId": "17c6c737832b9c531aed829e04daec89b5bcc698",
                "externalIds": {
                    "ArXiv": "2203.15064",
                    "DBLP": "journals/corr/abs-2203-15064",
                    "DOI": "10.1109/CVPR52688.2022.00996",
                    "CorpusId": 247779341
                },
                "corpusId": 247779341,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/17c6c737832b9c531aed829e04daec89b5bcc698",
                "title": "Cycle-Consistent Counterfactuals by Latent Transformations",
                "abstract": "CounterFactual (CF) visual explanations try to find images similar to the query image that change the decision of a vision system to a specified outcome. Existing methods either require inference-time optimization or joint training with a generative adversarial model which makes them time-consuming and difficult to use in practice. We propose a novel approach, Cycle-Consistent Counterfactuals by Latent Transformations (C3LT), which learns a latent transformation that automatically generates visual CFs by steering in the latent space of generative models. Our method uses cycle consistency between the query and CF latent representations which helps our training to find better solutions. C3LT can be easily plugged into any state-of-the-art pretrained generative network. This enables our method to generate high-quality and interpretable CF images at high resolution such as those in ImageNet. In addition to several established metrics for evaluating CF explanations, we introduce a novel metric tailored to assess the quality of the generated CF examples and validate the effectiveness of our method on an extensive set of experiments.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "33353193",
                        "name": "S. Khorram"
                    },
                    {
                        "authorId": "66262000",
                        "name": "L. Fuxin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "While scaling datasets and model architectures has worked well, more structured approaches could lead to more data-efficient generalization: for instance, generative models with explicit machinery for counterfactual reasoning (Sauer & Geiger, 2021)."
            ],
            "citingPaper": {
                "paperId": "0f4fff63f5f637e0f807532e37462e0619c86568",
                "externalIds": {
                    "ArXiv": "2204.05133",
                    "DBLP": "journals/corr/abs-2204-05133",
                    "DOI": "10.48550/arXiv.2204.05133",
                    "CorpusId": 248085052
                },
                "corpusId": 248085052,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0f4fff63f5f637e0f807532e37462e0619c86568",
                "title": "On the link between conscious function and general intelligence in humans and machines",
                "abstract": "In popular media, there is often a connection drawn between the advent of awareness in artificial agents and those same agents simultaneously achieving human or superhuman level intelligence. In this work, we explore the validity and potential application of this seemingly intuitive link between consciousness and intelligence. We do so by examining the cognitive abilities associated with three contemporary theories of conscious function: Global Workspace Theory (GWT), Information Generation Theory (IGT), and Attention Schema Theory (AST). We find that all three theories specifically relate conscious function to some aspect of domain-general intelligence in humans. With this insight, we turn to the field of Artificial Intelligence (AI) and find that, while still far from demonstrating general intelligence, many state-of-the-art deep learning methods have begun to incorporate key aspects of each of the three functional theories. Having identified this trend, we use the motivating example of mental time travel in humans to propose ways in which insights from each of the three theories may be combined into a single unified and implementable model. Given that it is made possible by cognitive abilities underlying each of the three functional theories, artificial agents capable of mental time travel would not only possess greater general intelligence than current approaches, but also be more consistent with our current understanding of the functional role of consciousness in humans, thus making it a promising near-term goal for AI research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7174267",
                        "name": "A. Juliani"
                    },
                    {
                        "authorId": "68972911",
                        "name": "Kai Arulkumaran"
                    },
                    {
                        "authorId": "1820481",
                        "name": "Shuntaro Sasai"
                    },
                    {
                        "authorId": "1800112",
                        "name": "R. Kanai"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "cd9e77dea8d054e550cf6fb20dd6a7d25f5ec528",
                "externalIds": {
                    "DBLP": "journals/pr/ZhangD22",
                    "DOI": "10.1016/j.patcog.2022.108625",
                    "CorpusId": 247446011
                },
                "corpusId": 247446011,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cd9e77dea8d054e550cf6fb20dd6a7d25f5ec528",
                "title": "2K-Fold-Net and feature enhanced 4-Fold-Net for medical image segmentation",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2136589206",
                        "name": "Yunchu Zhang"
                    },
                    {
                        "authorId": "2117143608",
                        "name": "Jianfei Dong"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                ", 2020); (vi) increasing robustness against spurious correlations (Sauer and Geiger, 2021).",
                "\u2026al., 2021); (ii) defining fairness (Kusner et al., 2017); (iii) mitigating data biases (Denton et al., 2019); (iv) improving reinforcement learning (Lu et al., 2020); (v) predicting accuracy (Kaushik et al., 2020); (vi) increasing robustness against spurious correlations (Sauer and Geiger, 2021)."
            ],
            "citingPaper": {
                "paperId": "08f078764e74ac3ec72825fd70d0017998097c16",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-10166",
                    "ArXiv": "2202.10166",
                    "CorpusId": 247011291
                },
                "corpusId": 247011291,
                "publicationVenue": {
                    "id": "3d07319c-4f2a-4f30-b619-c295ccd29367",
                    "name": "CLEaR",
                    "type": "conference",
                    "alternate_names": [
                        "Classification of Events, Activities and Relationships",
                        "CLEAR",
                        "CLeaR",
                        "Conf Causal Learn Reason",
                        "Classif Event Act Relatsh",
                        "Conference on Causal Learning and Reasoning"
                    ],
                    "issn": "2453-7128",
                    "url": "http://www.jolace.com/publications/clear/",
                    "alternate_urls": [
                        "https://www.cclear.cc/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/08f078764e74ac3ec72825fd70d0017998097c16",
                "title": "Diffusion Causal Models for Counterfactual Estimation",
                "abstract": "We consider the task of counterfactual estimation from observational imaging data given a known causal structure. In particular, quantifying the causal effect of interventions for high-dimensional data with neural networks remains an open challenge. Herein we propose Diff-SCM, a deep structural causal model that builds on recent advances of generative energy-based models. In our setting, inference is performed by iteratively sampling gradients of the marginal and conditional distributions entailed by the causal model. Counterfactual estimation is achieved by firstly inferring latent variables with deterministic forward diffusion, then intervening on a reverse diffusion process using the gradients of an anti-causal predictor w.r.t the input. Furthermore, we propose a metric for evaluating the generated counterfactuals. We find that Diff-SCM produces more realistic and minimal counterfactuals than baselines on MNIST data and can also be applied to ImageNet data. Code is available https://github.com/vios-s/Diff-SCM.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2130572313",
                        "name": "Pedro Sanchez"
                    },
                    {
                        "authorId": "1919157",
                        "name": "S. Tsaftaris"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "StyleGAN-XL inherits this ability and, to a certain extent, even generates out-of-domain combinations between different classes, akin to counterfactual images presented in [53]."
            ],
            "citingPaper": {
                "paperId": "82ba96443173da0b8b3e870c5ab8f41109a67203",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-00273",
                    "ArXiv": "2202.00273",
                    "DOI": "10.1145/3528233.3530738",
                    "CorpusId": 246441861
                },
                "corpusId": 246441861,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/82ba96443173da0b8b3e870c5ab8f41109a67203",
                "title": "StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets",
                "abstract": "Computer graphics has experienced a recent surge of data-centric approaches for photorealistic and controllable content creation. StyleGAN in particular sets new standards for generative modeling regarding image quality and controllability. However, StyleGAN\u2019s performance severely degrades on large unstructured datasets such as ImageNet. StyleGAN was designed for controllability; hence, prior works suspect its restrictive design to be unsuitable for diverse datasets. In contrast, we find the main limiting factor to be the current training strategy. Following the recently introduced Projected GAN paradigm, we leverage powerful neural network priors and a progressive growing strategy to successfully train the latest StyleGAN3 generator on ImageNet. Our final model, StyleGAN-XL, sets a new state-of-the-art on large-scale image synthesis and is the first to generate images at a resolution of 10242 at such a dataset scale. We demonstrate that this model can invert and edit images beyond the narrow domain of portraits or specific object classes. Code, models, and supplementary videos can be found at https://sites.google.com/view/stylegan-xl/ .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40562186",
                        "name": "Axel Sauer"
                    },
                    {
                        "authorId": "40502376",
                        "name": "Katja Schwarz"
                    },
                    {
                        "authorId": "47237027",
                        "name": "Andreas Geiger"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Other constructive works aimed to reduce the reliance of deep models on spurious features appeal to counterfactual data generation [1, 6, 17], often appealing to disentangled representations or explicit annotations to break correlations of texture, shapes, colors, and backgrounds."
            ],
            "citingPaper": {
                "paperId": "88b5acbec09ed39eecdca136f75ff90feb2fc3a3",
                "externalIds": {
                    "ArXiv": "2201.10766",
                    "DBLP": "journals/corr/abs-2201-10766",
                    "DOI": "10.1109/CVPR52688.2022.01850",
                    "CorpusId": 246285765
                },
                "corpusId": 246285765,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/88b5acbec09ed39eecdca136f75ff90feb2fc3a3",
                "title": "A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes",
                "abstract": "While datasets with single-label supervision have propelled rapid advances in image classification, additional annotations are necessary in order to quantitatively assess how models make predictions. To this end, for a subset of ImageNet samples, we collect segmentation masks for the entire object and 18 informative attributes. We call this dataset RIVAL10 (RIch Visual Attributes with Localization), consisting of roughly 26k instances over 10 classes. Using RIVAL10, we evaluate the sensitivity of a broad set of models to noise corruptions in foregrounds, backgrounds and attributes. In our analysis, we consider diverse state-of-the-art architectures (ResNets, Transformers) and training procedures (CLIP, SimCLR, DeiT, Adversarial Training). We find that, somewhat surprisingly, in ResNets, adversarial training makes models more sensitive to the background compared to foreground than standard training. Similarly, contrastively-trained models also have lower relative foreground sensitivity in both transformers and ResNets. Lastly, we observe intriguing adaptive abilities of transformers to increase relative foreground sensitivity as corruption level increases. Using saliency methods, we automatically discover spurious features that drive the background sensitivity of models and assess alignment of saliency maps with foregrounds. Finally, we quantitatively study the attribution problem for neural features by comparing feature saliency with ground-truth localization of semantic attributes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "104644443",
                        "name": "Mazda Moayeri"
                    },
                    {
                        "authorId": "105000868",
                        "name": "Phillip E. Pope"
                    },
                    {
                        "authorId": "3458479",
                        "name": "Y. Balaji"
                    },
                    {
                        "authorId": "34389431",
                        "name": "S. Feizi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Sauer and Geiger [26] propose to disentangle object shape, object texture and background in the image generation process and generate structured conterfacturals which help improve the robustness and interpretability of classifiers."
            ],
            "citingPaper": {
                "paperId": "85b029777a4e87e8bdd9a692be1cb520bd4f3214",
                "externalIds": {
                    "ArXiv": "2201.09689",
                    "DBLP": "journals/corr/abs-2201-09689",
                    "CorpusId": 246240028
                },
                "corpusId": 246240028,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/85b029777a4e87e8bdd9a692be1cb520bd4f3214",
                "title": "Which Style Makes Me Attractive? Interpretable Control Discovery and Counterfactual Explanation on StyleGAN",
                "abstract": "The semantically disentangled latent subspace in GAN provides rich interpretable controls in image generation. This paper includes two contributions on semantic latent subspace analysis in the scenario of face generation using StyleGAN2. First, we propose a novel approach to disentangle latent subspace semantics by exploiting existing face analysis models, e.g., face parsers and face landmark detectors. These models provide the flexibility to construct various criterions with very concrete and interpretable semantic meanings (e.g., change face shape or change skin color) to restrict latent subspace disentanglement. Rich latent space controls unknown previously can be discovered using the constructed criterions. Second, we propose a new perspective to explain the behavior of a CNN classifier by generating counterfactuals in the interpretable latent subspaces we discovered. This explanation helps reveal whether the classifier learns semantics as intended. Experiments on various disentanglement criterions demonstrate the effectiveness of our approach. We believe this approach contributes to both areas of image manipulation and counterfactual explainability of CNNs. The code is available at \\url{https://github.com/prclibo/ice}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48218911",
                        "name": "B. Li"
                    },
                    {
                        "authorId": "1684884626",
                        "name": "Qiuli Wang"
                    },
                    {
                        "authorId": "2143385094",
                        "name": "Jiquan Pei"
                    },
                    {
                        "authorId": "2118809263",
                        "name": "Yu Yang"
                    },
                    {
                        "authorId": "7807689",
                        "name": "Xiangyang Ji"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                ", the work [61] specified independent modules for image background and texture.",
                "A growing number of recent work has used causality with machine learning [62] for disentangled representation [54, 78], model explanation [6, 13], and robust prediction [61, 24, 83].",
                "We use two datasets where the target attribute has a correlation strength of over 90% with the confounding factor, following the challenging settings of the latest work on visual bias [73, 17, 61, 65].",
                "Several latest works have studied causal inference combined with deep generative models for images, to learn causal structures between attributes [78, 65, 51], synthesize novel images [32, 3], and augment unbiased classifier training [61]."
            ],
            "citingPaper": {
                "paperId": "21adf4285eb85f4a50106b84906f41c2bd68d510",
                "externalIds": {
                    "ArXiv": "2201.09119",
                    "DBLP": "conf/nips/HuL21",
                    "CorpusId": 243843906
                },
                "corpusId": 243843906,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/21adf4285eb85f4a50106b84906f41c2bd68d510",
                "title": "A Causal Lens for Controllable Text Generation",
                "abstract": "Controllable text generation concerns two fundamental tasks of wide applications, namely generating text of given attributes (i.e., attribute-conditional generation), and minimally editing existing text to possess desired attributes (i.e., text attribute transfer). Extensive prior work has largely studied the two problems separately, and developed different conditional models which, however, are prone to producing biased text (e.g., various gender stereotypes). This paper proposes to formulate controllable text generation from a principled causal perspective which models the two tasks with a unified framework. A direct advantage of the causal formulation is the use of rich causality tools to mitigate generation biases and improve control. We treat the two tasks as interventional and counterfactual causal inference based on a structural causal model, respectively. We then apply the framework to the challenging practical setting where confounding factors (that induce spurious correlations) are observable only on a small fraction of data. Experiments show significant superiority of the causal approach over previous conditional models for improved control accuracy and reduced bias.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2749311",
                        "name": "Zhiting Hu"
                    },
                    {
                        "authorId": "144180616",
                        "name": "Erran L. Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "The causal lens and generative counterfactual data augmentation have also shown recent, positive results in the computer vision domain [27]."
            ],
            "citingPaper": {
                "paperId": "fab5f884301cf5fc6e07907e3d136090d2641923",
                "externalIds": {
                    "DBLP": "conf/nips/PlylerGC21",
                    "ArXiv": "2201.05177",
                    "CorpusId": 244897664
                },
                "corpusId": 244897664,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/fab5f884301cf5fc6e07907e3d136090d2641923",
                "title": "Making a (Counterfactual) Difference One Rationale at a Time",
                "abstract": "Rationales, snippets of extracted text that explain an inference, have emerged as a popular framework for interpretable natural language processing (NLP). Rationale models typically consist of two cooperating modules: a selector and a classi\ufb01er with the goal of maximizing the mutual information (MMI) between the \"selected\" text and the document label. Despite their promises, MMI-based methods often pick up on spurious text patterns and result in models with nonsensical behaviors. In this work, we investigate whether counterfactual data augmentation (CDA) , without human assistance, can improve the performance of the selector by lowering the mutual information between spurious signals and the document label. Our counterfactuals are produced in an unsupervised fashion using class-dependent generative models. From an information theoretic lens, we derive properties of the unaugmented dataset for which our CDA approach would succeed. The effectiveness of CDA is empirically evaluated by comparing against several baselines including an improved MMI-based rationale schema [19] on two multi-aspect datasets. Our results show that CDA produces rationales that better capture the signal of interest.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "69917271",
                        "name": "Michael J. Plyler"
                    },
                    {
                        "authorId": "2116946462",
                        "name": "Michal Green"
                    },
                    {
                        "authorId": "2065992126",
                        "name": "Min Chi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Varying Textures, Backgrounds and More As shown in [14, 29], it is hard to control texture and background in standard GANs."
            ],
            "citingPaper": {
                "paperId": "98598d0c472694f3bdaf84997400c98d9ec7ffd6",
                "externalIds": {
                    "ArXiv": "2112.01573",
                    "DBLP": "journals/corr/abs-2112-01573",
                    "CorpusId": 244896415
                },
                "corpusId": 244896415,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/98598d0c472694f3bdaf84997400c98d9ec7ffd6",
                "title": "FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization",
                "abstract": "Generating images from natural language instructions is an intriguing yet highly challenging task. We approach text-to-image generation by combining the power of the retrained CLIP representation with an off-the-shelf image generator (GANs), optimizing in the latent space of GAN to find images that achieve maximum CLIP score with the given input text. Compared to traditional methods that train generative models from text to image starting from scratch, the CLIP+GAN approach is training-free, zero shot and can be easily customized with different generators. However, optimizing CLIP score in the GAN space casts a highly challenging optimization problem and off-the-shelf optimizers such as Adam fail to yield satisfying results. In this work, we propose a FuseDream pipeline, which improves the CLIP+GAN approach with three key techniques: 1) an AugCLIP score which robustifies the CLIP objective by introducing random augmentation on image. 2) a novel initialization and over-parameterization strategy for optimization which allows us to efficiently navigate the non-convex landscape in GAN space. 3) a composed generation technique which, by leveraging a novel bi-level optimization formulation, can compose multiple images to extend the GAN space and overcome the data-bias. When promoted by different input text, FuseDream can generate high-quality images with varying objects, backgrounds, artistic styles, even novel counterfactual concepts that do not appear in the training data of the GAN we use. Quantitatively, the images generated by FuseDream yield top-level Inception score and FID score on MS COCO dataset, without additional architecture design or training. Our code is publicly available at \\url{https://github.com/gnobitab/FuseDream}.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46521757",
                        "name": "Xingchao Liu"
                    },
                    {
                        "authorId": "29777869",
                        "name": "Chengyue Gong"
                    },
                    {
                        "authorId": "8687492",
                        "name": "Lemeng Wu"
                    },
                    {
                        "authorId": "2107944048",
                        "name": "Shujian Zhang"
                    },
                    {
                        "authorId": "49466406",
                        "name": "Haoran Su"
                    },
                    {
                        "authorId": "47362268",
                        "name": "Qiang Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "Pretrained models can be used as a guiding mechanism to disentangle causal generative factors [54], for text-driven image manipulation [44], matching the generator activations to inverted classifiers [19, 56], or to generate images via gradient ascent in the latent space of a generator [41]."
            ],
            "citingPaper": {
                "paperId": "ffd43946c7fe947ef3213e7a668a36e9d41c8f4b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-01007",
                    "ArXiv": "2111.01007",
                    "CorpusId": 240354401
                },
                "corpusId": 240354401,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ffd43946c7fe947ef3213e7a668a36e9d41c8f4b",
                "title": "Projected GANs Converge Faster",
                "abstract": "Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train. They need careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space. Motivated by the finding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fr\\'echet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40562186",
                        "name": "Axel Sauer"
                    },
                    {
                        "authorId": "31352445",
                        "name": "Kashyap Chitta"
                    },
                    {
                        "authorId": "2057007308",
                        "name": "Jens Muller"
                    },
                    {
                        "authorId": "47237027",
                        "name": "Andreas Geiger"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "a0daae92e2e0b72cb36c7bb0547ec8725779f359",
                "externalIds": {
                    "ArXiv": "2110.06257",
                    "DBLP": "journals/corr/abs-2110-06257",
                    "CorpusId": 238744400
                },
                "corpusId": 238744400,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a0daae92e2e0b72cb36c7bb0547ec8725779f359",
                "title": "Causal discovery from conditionally stationary time-series",
                "abstract": "Causal discovery, i.e., inferring underlying cause-effect relationships from observations of a scene or system, is an inherent mechanism in human cognition, but has been shown to be highly challenging to automate. The majority of approaches in the literature aiming for this task consider constrained scenarios with fully observed variables or data from stationary time-series. In this work we aim for causal discovery in a more general class of scenarios, scenes with non-stationary behavior over time. For our purposes we here regard a scene as a composition objects interacting with each other over time. Non-stationarity is modeled as stationarity conditioned on an underlying variable, a state, which can be of varying dimension, more or less hidden given observations of the scene, and also depend more or less directly on these observations. We propose a probabilistic deep learning approach called State-Dependent Causal Inference (SDCI) for causal discovery in such conditionally stationary time-series data. Results in two different synthetic scenarios show that this method is able to recover the underlying causal dependencies with high accuracy even in cases with hidden states.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1580285428",
                        "name": "Carles Balsells Rodas"
                    },
                    {
                        "authorId": "51132868",
                        "name": "Ruibo Tu"
                    },
                    {
                        "authorId": "1704879",
                        "name": "H. Kjellstr\u00f6m"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Sauer & Geiger (2021) propose a generative framework which allows choosing the color, texture and background of a generated image independently."
            ],
            "citingPaper": {
                "paperId": "ed45598c2e25999069ebccd7ddbd7b0b4e4bc7da",
                "externalIds": {
                    "DBLP": "conf/icml/KattakindaF22",
                    "ArXiv": "2110.03804",
                    "CorpusId": 238531449
                },
                "corpusId": 238531449,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ed45598c2e25999069ebccd7ddbd7b0b4e4bc7da",
                "title": "FOCUS: Familiar Objects in Common and Uncommon Settings",
                "abstract": "Standard training datasets for deep learning often contain objects in common settings (e.g.,\"a horse on grass\"or\"a ship in water\") since they are usually collected by randomly scraping the web. Uncommon and rare settings (e.g.,\"a plane on water\",\"a car in snowy weather\") are thus severely under-represented in the training data. This can lead to an undesirable bias in model predictions towards common settings and create a false sense of accuracy. In this paper, we introduce FOCUS (Familiar Objects in Common and Uncommon Settings), a dataset for stress-testing the generalization power of deep image classifiers. By leveraging the power of modern search engines, we deliberately gather data containing objects in common and uncommon settings in a wide range of locations, weather conditions, and time of day. We present a detailed analysis of the performance of various popular image classifiers on our dataset and demonstrate a clear drop in performance when classifying images in uncommon settings. By analyzing deep features of these models, we show that such errors can be due to the use of spurious features in model predictions. We believe that our dataset will aid researchers in understanding the inability of deep models to generalize well to uncommon settings and drive future work on improving their distributional robustness.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1962835975",
                        "name": "Priyatham Kattakinda"
                    },
                    {
                        "authorId": "34389431",
                        "name": "S. Feizi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Sauer and Geiger [51] proposed to decompose the image generation process into independent causal mechanisms, which disentangle object shape, object texture and background, for generating counterfactual images that improve out-of-distribution robustness.",
                "In previous work, [39, 51] proposed to utilize generative models to produce counterfactual images.",
                "Secondly, unlike [39, 51], our method proposes to generate counterfactual features rather than counterfactual images."
            ],
            "citingPaper": {
                "paperId": "a1b2f287dae6c28f2e2f942be3287af82be72bb9",
                "externalIds": {
                    "DBLP": "conf/iccv/ZhangWWLKLG21",
                    "DOI": "10.1109/ICCV48922.2021.01108",
                    "CorpusId": 244045083
                },
                "corpusId": 244045083,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/a1b2f287dae6c28f2e2f942be3287af82be72bb9",
                "title": "Learning Causal Representation for Training Cross-Domain Pose Estimator via Generative Interventions",
                "abstract": "3D pose estimation has attracted increasing attention with the availability of high-quality benchmark datasets. However, prior works show that deep learning models tend to learn spurious correlations, which fail to generalize beyond the specific dataset they are trained on. In this work, we take a step towards training robust models for cross-domain pose estimation task, which brings together ideas from causal representation learning and generative adversarial networks. Specifically, this paper introduces a novel framework for causal representation learning which explicitly exploits the causal structure of the task. We consider changing domain as interventions on images under the data-generation process and steer the generative model to produce counterfactual features. This help the model learn transferable and causal relations across different domains. Our framework is able to learn with various types of unlabeled datasets. We demonstrate the efficacy of our proposed method on both human and hand pose estimation task. The experiment results show the proposed approach achieves state-of-the-art performance on most datasets for both domain adaptation and domain generalization settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "113506452",
                        "name": "Xiheng Zhang"
                    },
                    {
                        "authorId": "3026404",
                        "name": "Yongkang Wong"
                    },
                    {
                        "authorId": "2154606520",
                        "name": "Xiaofei Wu"
                    },
                    {
                        "authorId": "2151166415",
                        "name": "Juwei Lu"
                    },
                    {
                        "authorId": "145977143",
                        "name": "Mohan S. Kankanhalli"
                    },
                    {
                        "authorId": "2116273649",
                        "name": "Xiangdong Li"
                    },
                    {
                        "authorId": "5504255",
                        "name": "Wei-dong Geng"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "b586094d51c17d37807a17d06e3cb8e40269eb16",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-14274",
                    "ArXiv": "2109.14274",
                    "CorpusId": 238215683
                },
                "corpusId": 238215683,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b586094d51c17d37807a17d06e3cb8e40269eb16",
                "title": "Designing Counterfactual Generators using Deep Model Inversion",
                "abstract": "Explanation techniques that synthesize small, interpretable changes to a given image while producing desired changes in the model prediction have become popular for introspecting black-box models. Commonly referred to as counterfactuals, the synthesized explanations are required to contain discernible changes (for easy interpretability) while also being realistic (consistency to the data manifold). In this paper, we focus on the case where we have access only to the trained deep classifier and not the actual training data. While the problem of inverting deep models to synthesize images from the training distribution has been explored, our goal is to develop a deep inversion approach to generate counterfactual explanations for a given query image. Despite their effectiveness in conditional image synthesis, we show that existing deep inversion methods are insufficient for producing meaningful counterfactuals. We propose DISC (Deep Inversion for Synthesizing Counterfactuals) that improves upon deep inversion by utilizing (a) stronger image priors, (b) incorporating a novel manifold consistency objective and (c) adopting a progressive optimization strategy. We find that, in addition to producing visually meaningful explanations, the counterfactuals from DISC are effective at learning classifier decision boundaries and are robust to unknown test-time corruptions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1744175",
                        "name": "Jayaraman J. Thiagarajan"
                    },
                    {
                        "authorId": "51881215",
                        "name": "V. Narayanaswamy"
                    },
                    {
                        "authorId": "145882781",
                        "name": "Deepta Rajan"
                    },
                    {
                        "authorId": "2115341173",
                        "name": "J. Liang"
                    },
                    {
                        "authorId": "2063969492",
                        "name": "Akshay Chaudhari"
                    },
                    {
                        "authorId": "144924839",
                        "name": "A. Spanias"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "ducing a counterfactual explanation exploit generative models with generative adversarial network (GAN) and its variants [33], [34], [35]."
            ],
            "citingPaper": {
                "paperId": "9de882f9b2398b9b1e47c863791a5c281516eb57",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-09451",
                    "ArXiv": "2108.09451",
                    "DOI": "10.1109/TPAMI.2022.3197845",
                    "CorpusId": 237266521,
                    "PubMed": "35947563"
                },
                "corpusId": 237266521,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9de882f9b2398b9b1e47c863791a5c281516eb57",
                "title": "Learn-Explain-Reinforce: Counterfactual Reasoning and its Guidance to Reinforce an Alzheimer's Disease Diagnosis Model",
                "abstract": "Existing studies on disease diagnostic models focus either on diagnostic model learning for performance improvement or on the visual explanation of a trained diagnostic model. We propose a novel learn-explain-reinforce (LEAR) framework that unifies diagnostic model learning, visual explanation generation (explanation unit), and trained diagnostic model reinforcement (reinforcement unit) guided by the visual explanation. For the visual explanation, we generate a counterfactual map that transforms an input sample to be identified as an intended target label. For example, a counterfactual map can localize hypothetical abnormalities within a normal brain image that may cause it to be diagnosed with Alzheimer's disease (AD). We believe that the generated counterfactual maps represent data-driven knowledge about a target task, i.e., AD diagnosis using structural MRI, which can be a vital source of information to reinforce the generalization of the trained diagnostic model. To this end, we devise an attention-based feature refinement module with the guidance of the counterfactual maps. The explanation and reinforcement units are reciprocal and can be operated iteratively. Our proposed approach was validated via qualitative and quantitative analysis on the ADNI dataset. Its comprehensibility and fidelity were demonstrated through ablation studies and comparisons with existing methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1629212330",
                        "name": "Kwanseok Oh"
                    },
                    {
                        "authorId": "38964064",
                        "name": "Jeeseok Yoon"
                    },
                    {
                        "authorId": "143802908",
                        "name": "Heung-Il Suk"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Despite their state-of-the-art performance on object classification tasks, deep neural networks (DNN) are highly prone to shortcut learning [8, 33, 11].",
                "This approach has further been extended to generative model-based augmentations [31, 34, 33]."
            ],
            "citingPaper": {
                "paperId": "c3cf4b15a19aa116247ad9420f6ce4b80e22cf70",
                "externalIds": {
                    "ArXiv": "2108.05779",
                    "DBLP": "journals/corr/abs-2108-05779",
                    "DOI": "10.1109/ICCV48922.2021.01048",
                    "CorpusId": 236987259
                },
                "corpusId": 236987259,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/c3cf4b15a19aa116247ad9420f6ce4b80e22cf70",
                "title": "DiagViB-6: A Diagnostic Benchmark Suite for Vision Models in the Presence of Shortcut and Generalization Opportunities",
                "abstract": "Common deep neural networks (DNNs) for image classification have been shown to rely on shortcut opportunities (SO) in the form of predictive and easy-to-represent visual factors. This is known as shortcut learning and leads to impaired generalization. In this work, we show that common DNNs also suffer from shortcut learning when predicting only basic visual object factors of variation (FoV) such as shape, color, or texture. We argue that besides shortcut opportunities, generalization opportunities (GO) are also an inherent part of real-world vision data and arise from partial independence between predicted classes and FoVs. We also argue that it is necessary for DNNs to exploit GO to overcome shortcut learning. Our core contribution is to introduce the Diagnostic Vision Benchmark suite DiagViB-6, which includes datasets and metrics to study a network\u2019s shortcut vulnerability and generalization capability for six independent FoV. In particular, DiagViB-6 allows controlling the type and degree of SO and GO in a dataset. We benchmark a wide range of popular vision architectures and show that they can exploit GO only to a limited extent.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "114474725",
                        "name": "Elias Eulig"
                    },
                    {
                        "authorId": "9359914",
                        "name": "Piyapat Saranrittichai"
                    },
                    {
                        "authorId": "29359383",
                        "name": "Chaithanya Kumar Mummadi"
                    },
                    {
                        "authorId": "2451538",
                        "name": "K. Rambach"
                    },
                    {
                        "authorId": "52020792",
                        "name": "William H. Beluch"
                    },
                    {
                        "authorId": "148377669",
                        "name": "Xiahan Shi"
                    },
                    {
                        "authorId": "2144816511",
                        "name": "Volker Fischer"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Recent work shows the benefits of introducing causality into machine learning from various aspects (Zhang et al., 2020a; Mitrovic et al., 2020; Teshima et al., 2020; Tang et al., 2020; Sauer & Geiger, 2020; Tang et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "3076cfc34ed4cbcea88da074a0faa841cbf82036",
                "externalIds": {
                    "ArXiv": "2106.06196",
                    "CorpusId": 249191849
                },
                "corpusId": 249191849,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3076cfc34ed4cbcea88da074a0faa841cbf82036",
                "title": "CausalAdv: Adversarial Robustness through the Lens of Causality",
                "abstract": "The adversarial vulnerability of deep neural networks has attracted significant attention in machine learning. As causal reasoning has an instinct for modelling distribution change, it is essential to incorporate causality into analyzing this specific type of distribution change induced by adversarial attacks. However, causal formulations of the intuition of adversarial attacks and the development of robust DNNs are still lacking in the literature. To bridge this gap, we construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of adversarial attacks. From the causal perspective, we study the distinction between the natural and adversarial distribution and conclude that the origin of adversarial vulnerability is the focus of models on spurious correlations. Inspired by the causal understanding, we propose the Causal inspired Adversarial distribution alignment method, CausalAdv, to eliminate the difference between natural and adversarial distributions by considering spurious correlations. Extensive experiments demonstrate the efficacy of the proposed method. Our work is the first attempt towards using causality to understand and mitigate the adversarial vulnerability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109116068",
                        "name": "Yonggang Zhang"
                    },
                    {
                        "authorId": "29393235",
                        "name": "Mingming Gong"
                    },
                    {
                        "authorId": "121698214",
                        "name": "Tongliang Liu"
                    },
                    {
                        "authorId": "47537639",
                        "name": "Gang Niu"
                    },
                    {
                        "authorId": "40434674",
                        "name": "Xinmei Tian"
                    },
                    {
                        "authorId": "2087238859",
                        "name": "Bo Han"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    },
                    {
                        "authorId": "2119017697",
                        "name": "Kun Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In addition, we will investigate generation module for language understanding with unsupervised generative techniques (Sauer and Geiger, 2021)."
            ],
            "citingPaper": {
                "paperId": "f50abcae0477351e9d2814547158d348ebf59bcb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-03046",
                    "ACL": "2021.findings-acl.196",
                    "ArXiv": "2106.03046",
                    "DOI": "10.18653/v1/2021.findings-acl.196",
                    "CorpusId": 235358557
                },
                "corpusId": 235358557,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f50abcae0477351e9d2814547158d348ebf59bcb",
                "title": "Empowering Language Understanding with Counterfactual Reasoning",
                "abstract": "Present language understanding methods have demonstrated extraordinary ability of recognizing patterns in texts via machine learning. However, existing methods indiscriminately use the recognized patterns in the testing phase that is inherently different from us humans who have counterfactual thinking, e.g., to scrutinize for the hard testing samples. Inspired by this, we propose a Counterfactual Reasoning Model, which mimics the counterfactual thinking by learning from few counterfactual samples. In particular, we devise a generation module to generate representative counterfactual samples for each factual sample, and a retrospective module to retrospect the model prediction by comparing the counterfactual and factual samples. Extensive experiments on sentiment analysis (SA) and natural language inference (NLI) validate the effectiveness of our method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2163400298",
                        "name": "Fuli Feng"
                    },
                    {
                        "authorId": "2570260",
                        "name": "Jizhi Zhang"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "2119078220",
                        "name": "Hanwang Zhang"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Subsequent works (Wang et al., 2019; Bahng et al., 2020; Shi et al., 2020; Nam et al., 2020; Li et al., 2021; Sauer & Geiger, 2021) focus on addressing the bias problem with explicit debiasing procedure."
            ],
            "citingPaper": {
                "paperId": "4a9244d0fc9b2b87eca5fa7579835f4a05eb8866",
                "externalIds": {
                    "DBLP": "conf/icml/ZhangAX0C21",
                    "MAG": "3170824577",
                    "ArXiv": "2106.02890",
                    "CorpusId": 235358638
                },
                "corpusId": 235358638,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4a9244d0fc9b2b87eca5fa7579835f4a05eb8866",
                "title": "Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?",
                "abstract": "Can models with particular structure avoid being biased towards spurious correlation in out-of-distribution (OOD) generalization? Peters et al. (2016) provides a positive answer for linear cases. In this paper, we use a functional modular probing method to analyze deep model structures under OOD setting. We demonstrate that even in biased models (which focus on spurious correlation) there still exist unbiased functional subnetworks. Furthermore, we articulate and demonstrate the functional lottery ticket hypothesis: full network contains a subnetwork that can achieve better OOD performance. We then propose Modular Risk Minimization to solve the subnetwork selection problem. Our algorithm learns the subnetwork structure from a given dataset, and can be combined with any other OOD regularization methods. Experiments on various OOD generalization tasks corroborate the effectiveness of our method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "113087412",
                        "name": "Dinghuai Zhang"
                    },
                    {
                        "authorId": "3048927",
                        "name": "Kartik Ahuja"
                    },
                    {
                        "authorId": "152252625",
                        "name": "Yilun Xu"
                    },
                    {
                        "authorId": "2115869684",
                        "name": "Yisen Wang"
                    },
                    {
                        "authorId": "1760871",
                        "name": "Aaron C. Courville"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "1e58f1e94a03ef6434ce5e3360781d546f8a2f5b",
                "externalIds": {
                    "DBLP": "journals/tmlr/PlumbRT22",
                    "ArXiv": "2106.02112",
                    "CorpusId": 235352578
                },
                "corpusId": 235352578,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1e58f1e94a03ef6434ce5e3360781d546f8a2f5b",
                "title": "Finding and Fixing Spurious Patterns with Explanations",
                "abstract": "Image classifiers often use spurious patterns, such as\"relying on the presence of a person to detect a tennis racket, which do not generalize. In this work, we present an end-to-end pipeline for identifying and mitigating spurious patterns for such models, under the assumption that we have access to pixel-wise object-annotations. We start by identifying patterns such as\"the model's prediction for tennis racket changes 63% of the time if we hide the people.\"Then, if a pattern is spurious, we mitigate it via a novel form of data augmentation. We demonstrate that our method identifies a diverse set of spurious patterns and that it mitigates them by producing a model that is both more accurate on a distribution where the spurious pattern is not helpful and more robust to distribution shift.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31929250",
                        "name": "Gregory Plumb"
                    },
                    {
                        "authorId": "78846919",
                        "name": "Marco Tulio Ribeiro"
                    },
                    {
                        "authorId": "145532827",
                        "name": "Ameet Talwalkar"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Augmentation using GANs can be applied at training time for robustness or test-time for ensembling; while we primarily focus on the latter, concurrent works [35, 50] investigate the benefits of the former approach while [56] investigates intermediate GAN representations for few-shot segmentation."
            ],
            "citingPaper": {
                "paperId": "8f9d1de9e1bd60783eb75fd80c42bf99ec67363a",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChaiZSI021",
                    "ArXiv": "2104.14551",
                    "DOI": "10.1109/CVPR46437.2021.01475",
                    "CorpusId": 233444321
                },
                "corpusId": 233444321,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8f9d1de9e1bd60783eb75fd80c42bf99ec67363a",
                "title": "Ensembling with Deep Generative Views",
                "abstract": "Recent generative models can synthesize \"views\" of artificial images that mimic real-world variations, such as changes in color or pose, simply by learning from unlabeled image collections. Here, we investigate whether such views can be applied to real images to benefit downstream analysis tasks such as image classification. Using a pre-trained generator, we first find the latent code corresponding to a given real input image. Applying perturbations to the code creates natural variations of the image, which can then be ensembled together at test-time. We use StyleGAN2 as the source of generative augmentations and investigate this setup on classification tasks involving facial attributes, cat faces, and cars. Critically, we find that several design decisions are required towards making this process work; the perturbation procedure, weighting between the augmentations and original image, and training the classifier on synthesized images can all impact the result. Currently, we find that while test-time ensembling with GAN-based augmentations can offer some small improvements, the remaining bottlenecks are the efficiency and accuracy of the GAN reconstructions, coupled with classifier sensitivities to artifacts in GAN-generated images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51322829",
                        "name": "Lucy Chai"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "2094770",
                        "name": "Phillip Isola"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c4be409ff415f45d272606995757b71ad0c9fcdc",
                "externalIds": {
                    "DBLP": "conf/ijcai/ChengMS021",
                    "ArXiv": "2104.12278",
                    "DOI": "10.24963/ijcai.2021/598",
                    "CorpusId": 233394173
                },
                "corpusId": 233394173,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c4be409ff415f45d272606995757b71ad0c9fcdc",
                "title": "Causal Learning for Socially Responsible AI",
                "abstract": "There have been increasing concerns about Artificial Intelligence (AI) due to its unfathomable potential power. To make AI address ethical challenges and shun undesirable outcomes, researchers proposed to develop socially responsible AI (SRAI). One of these approaches is causal learning (CL). We survey state-of-the-art methods of CL for SRAI. We begin by examining the seven CL tools to enhance the social responsibility of AI, then review how existing works have succeeded using these tools to tackle issues in developing SRAI such as fairness. The goal of this survey is to bring forefront the potentials and promises of CL for SRAI.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144842921",
                        "name": "Lu Cheng"
                    },
                    {
                        "authorId": "1380253791",
                        "name": "Ahmadreza Mosallanezhad"
                    },
                    {
                        "authorId": "73409823",
                        "name": "Paras Sheth"
                    },
                    {
                        "authorId": "145896397",
                        "name": "Huan Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "4088d430a6c4c8d2c4dedc1533df8dc8077cbd8d",
                "externalIds": {
                    "DBLP": "conf/iccv/RodriguezCLZLC021",
                    "ArXiv": "2103.10226",
                    "DOI": "10.1109/ICCV48922.2021.00109",
                    "CorpusId": 232270015
                },
                "corpusId": 232270015,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/4088d430a6c4c8d2c4dedc1533df8dc8077cbd8d",
                "title": "Beyond Trivial Counterfactual Explanations with Diverse Valuable Explanations",
                "abstract": "Explainability for machine learning models has gained considerable attention within the research community given the importance of deploying more reliable machine-learning systems. In computer vision applications, generative counterfactual methods indicate how to perturb a model\u2019s input to change its prediction, providing details about the model\u2019s decision-making. Current methods tend to generate trivial counterfactuals about a model\u2019s decisions, as they often suggest to exaggerate or remove the presence of the attribute being classified. For the machine learning practitioner, these types of counterfactuals offer little value, since they provide no new information about undesired model or data biases. In this work, we identify the problem of trivial counterfactual generation and we propose DiVE to alleviate it. DiVE learns a perturbation in a disentangled latent space that is constrained using a diversity-enforcing loss to uncover multiple valuable explanations about the model\u2019s prediction. Further, we introduce a mechanism to prevent the model from producing trivial explanations. Experiments on CelebA and Synbols demonstrate that our model improves the success rate of producing high-quality valuable explanations when compared to previous state-of-the-art methods. Code is available at https://github.com/ElementAI/beyond-trivial-explanations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "117849477",
                        "name": "Pau Rodr\u00edguez L\u00f3pez"
                    },
                    {
                        "authorId": "1750641",
                        "name": "Massimo Caccia"
                    },
                    {
                        "authorId": "8651990",
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "authorId": "8540541",
                        "name": "L. Zamparo"
                    },
                    {
                        "authorId": "3266173",
                        "name": "I. Laradji"
                    },
                    {
                        "authorId": "1778839",
                        "name": "Laurent Charlin"
                    },
                    {
                        "authorId": "2064388326",
                        "name": "David V\u00e1zquez"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Along\nthis direction, for example, Sauer and Geiger (2021) recently combined disentangled generative models and out-of-distribution classification, but adopted a different disentanglement framework."
            ],
            "citingPaper": {
                "paperId": "6894301180bc0660c36da2a8f602febc42671a74",
                "externalIds": {
                    "DBLP": "journals/jmlr/0002LDLC022",
                    "ArXiv": "2010.02637",
                    "CorpusId": 251765391
                },
                "corpusId": 251765391,
                "publicationVenue": {
                    "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                    "name": "Journal of machine learning research",
                    "type": "journal",
                    "alternate_names": [
                        "Journal of Machine Learning Research",
                        "J mach learn res",
                        "J Mach Learn Res"
                    ],
                    "issn": "1532-4435",
                    "alternate_issns": [
                        "1533-7928"
                    ],
                    "url": "http://www.ai.mit.edu/projects/jmlr/",
                    "alternate_urls": [
                        "http://jmlr.csail.mit.edu/",
                        "http://www.jmlr.org/",
                        "http://portal.acm.org/affiliated/jmlr"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6894301180bc0660c36da2a8f602febc42671a74",
                "title": "Weakly Supervised Disentangled Generative Causal Representation Learning",
                "abstract": "This paper proposes a Disentangled gEnerative cAusal Representation (DEAR) learning method under appropriate supervised information. Unlike existing disentanglement methods that enforce independence of the latent variables, we consider the general case where the underlying factors of interests can be causally related. We show that previous methods with independent priors fail to disentangle causally related factors even under supervision. Motivated by this finding, we propose a new disentangled learning method called DEAR that enables causal controllable generation and causal representation learning. The key ingredient of this new formulation is to use a structural causal model (SCM) as the prior distribution for a bidirectional generative model. The prior is then trained jointly with a generator and an encoder using a suitable GAN algorithm incorporated with supervised information on the ground-truth factors and their underlying causal structure. We provide theoretical justification on the identifiability and asymptotic convergence of the proposed method. We conduct extensive experiments on both synthesized and real data sets to demonstrate the effectiveness of DEAR in causal controllable generation, and the benefits of the learned representations for downstream tasks in terms of sample efficiency and distributional robustness.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2111109710",
                        "name": "Xinwei Shen"
                    },
                    {
                        "authorId": "39853706",
                        "name": "Furui Liu"
                    },
                    {
                        "authorId": "35279146",
                        "name": "Hanze Dong"
                    },
                    {
                        "authorId": "2059872750",
                        "name": "Qing Lian"
                    },
                    {
                        "authorId": "2827164",
                        "name": "Zhitang Chen"
                    },
                    {
                        "authorId": "2146325375",
                        "name": "Tong Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                ", 2020) is effective for enhancing OOD generalization, which leverages factual and counterfactual samples to push the learning of decision boundary to focus on features causally affect the label (Sauer and Geiger, 2021; Teney et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "cb91d7e7c3a579a335e65f9872403a2082cc3d51",
                "externalIds": {
                    "DBLP": "conf/acl/DengWFZ0L23",
                    "ACL": "2023.acl-long.636",
                    "DOI": "10.18653/v1/2023.acl-long.636",
                    "CorpusId": 259370891
                },
                "corpusId": 259370891,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/cb91d7e7c3a579a335e65f9872403a2082cc3d51",
                "title": "Counterfactual Active Learning for Out-of-Distribution Generalization",
                "abstract": "We study the out-of-distribution generalization of active learning that adaptively selects samples for annotation in learning the decision boundary of classification. Our empirical study finds that increasingly annotating seen samples may hardly benefit the generalization. To address the problem, we propose Counterfactual Active Learning (CounterAL) that empowers active learning with counterfactual thinking to bridge the seen samples with unseen cases. In addition to annotating factual samples, CounterAL requires annotators to answer counterfactual questions to construct counterfactual samples for training. To achieve CounterAL, we design a new acquisition strategy that selects the informative factual-counterfactual pairs for annotation; and a new training strategy that pushes the model update to focus on the discrepancy between factual and counterfactual samples. We evaluate CounterAL on multiple public datasets of sentiment analysis and natural language inference. The experiment results show that CounterAL requires fewer acquisition rounds and outperforms existing active learning methods by a large margin in OOD tests with comparable IID performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214550033",
                        "name": "Xun Deng"
                    },
                    {
                        "authorId": "2117833732",
                        "name": "Wenjie Wang"
                    },
                    {
                        "authorId": "2163400298",
                        "name": "Fuli Feng"
                    },
                    {
                        "authorId": "2119078220",
                        "name": "Hanwang Zhang"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "2114122035",
                        "name": "Yong Liao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "A set of methods condition the generative model on attributes annotated in the dataset by using a conditional Generative Adversarial\nNetwork (GAN) (Joshi et al., 2018; Liu et al., 2019; Sauer & Geiger, 2021; Van Looveren et al., 2021; Yang et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "cb210e14dff0351c35d58a97cf43703d99becc37",
                "externalIds": {
                    "CorpusId": 259372993
                },
                "corpusId": 259372993,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cb210e14dff0351c35d58a97cf43703d99becc37",
                "title": "Evaluating Visual Counterfactual Explainers",
                "abstract": "Explainability methods have been widely used to provide insight into the decisions made by statistical models, thus facilitating their adoption in various domains within the industry. Counterfactual explanation methods aim to improve our understanding of a model by perturbing samples in a way that would alter its response in an unexpected manner. This information is helpful for users and for machine learning practitioners to understand and improve their models. Given the value provided by counterfactual explanations, there is a growing interest in the research community to investigate and propose new methods. However, we identify two issues that could hinder the progress in this field. (1) Existing metrics do not accurately reflect the value of an explainability method for the users. (2) Comparisons between methods are usually performed with datasets like CelebA, where images are annotated with attributes that do not fully describe them and with subjective attributes such as \u201cAttractive\u201d. In this work, we address these problems by proposing an evaluation method with a principled metric to evaluate and compare different counterfactual explanation methods. The evaluation is based on a synthetic dataset where images are fully described by their annotated attributes. As a result, we are able to perform a fair comparison of multiple explainability methods in the recent literature, obtaining insights about their performance. We make the code 1 and data public to the research community.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1820967551",
                        "name": "D. Velazquez"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "61546120ba2399bdbbe1e5b9a3ee471db14e34f3",
                "externalIds": {
                    "DBLP": "conf/icml/HwangCCK23",
                    "CorpusId": 260927828
                },
                "corpusId": 260927828,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/61546120ba2399bdbbe1e5b9a3ee471db14e34f3",
                "title": "MAGANet: Achieving Combinatorial Generalization by Modeling a Group Action",
                "abstract": "Combinatorial generalization refers to the ability to collect and assemble various attributes from diverse data to generate novel unexperienced data. This ability is considered a necessary passing point for achieving human-level intelligence. To achieve this ability, previous unsupervised approaches mainly focused on learning the disen-tangled representation, such as the variational au-toencoder. However, recent studies discovered that the disentangled representation is insufficient for combinatorial generalization and is not even correlated. In this regard, we propose a novel framework for data generation that can robustly generalize under these distribution shift situations. Instead of representing each data, our model discovers the fundamental transformation between a pair of data by simulating a group action. To test the combinatorial generalizability, we evaluated our model in two settings: Recombination-to-Element and Recombination-to-Range. The experiments demonstrated that our method has quantitatively and qualitatively superior generalizability and generates better images than traditional models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1947131861",
                        "name": "Geonho Hwang"
                    },
                    {
                        "authorId": "2921953",
                        "name": "Jaewoong Choi"
                    },
                    {
                        "authorId": "2111236954",
                        "name": "Hyunsoo Cho"
                    },
                    {
                        "authorId": "2259103",
                        "name": "Myung-joo Kang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "e95b9b729ea11ed9ca2eedd005c35e80eba0a390",
                "externalIds": {
                    "CorpusId": 261615012
                },
                "corpusId": 261615012,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e95b9b729ea11ed9ca2eedd005c35e80eba0a390",
                "title": "D ISENTANGLING C ASUAL M ECHANIMS BY O BSTRUCTING C LASSIFIERS",
                "abstract": "Deep neural networks (DNNs) often entangle features in latent representations leads to a reliance on spurious correlations, black-box behavior causing uninterpretable predictions, and non-compositional representations. We build on the idea of isolating independent mechanisms to build disentangled representations. We propose a novel method for learning to disentangle generative factors in the latent space by pressuring an explicitly mapped partition of causal mechanisms. We separate counterfactual image generation into independent causal mechanisms trained with supervision, creating a Disentangled Autoencoder. We do this by applying a projection-based classifier obstruction algorithm called R-LACE on latent space representations of the Colored MNIST dataset during training to separate the encoding of distinct features. In experiments testing our method, we were able to achieve disentanglement by partitioning the latent space into one subspace that encodes for digit and one subspace that encodes for color. 1",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2238481588",
                        "name": "Suraj Anand"
                    },
                    {
                        "authorId": "2238486811",
                        "name": "Neil Xu"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Our experimental setup is largely based on the description provided by Sauer and Geiger [22].",
                "69 Section 5 concludes this work by discussing our experience with reproducing the research by Sauer and Geiger [22].",
                "One central 53 principle in causal inference is the assumption of independent mechanisms (IMs), which states that a causal generative 54 process is composed of autonomous modules that do not influence each other [19, 22, 24].",
                "The counterfactual generative network is a manifestation of a structural causal model (SCM) for the task of image 72 classification [22].",
                "Throughout this work, we have conducted several experiments to reproduce the main results from the research by 221 Sauer and Geiger [22].",
                "In summary, this work makes the following contributions: 44 \u2022 We reproduce the main experiments conducted by Sauer and Geiger [22] to identify which parts of the 45 experimental results supporting their claims can be reproduced, and at what cost in terms of resources (e.",
                "37 In order to enhance the robustness and interpretability of classifiers, Sauer and Geiger [22] introduce the idea of a 38 Counterfactual Generative Network (CGN)."
            ],
            "citingPaper": {
                "paperId": "2d95007dacea0e618ee3e0de4b1761473720afed",
                "externalIds": {
                    "CorpusId": 247006995
                },
                "corpusId": 247006995,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2d95007dacea0e618ee3e0de4b1761473720afed",
                "title": "Reproducibility Study of \u201cCounterfactual Generative Networks\u201d",
                "abstract": "In this work, we study the reproducibility of the paper Counterfactual Generative Networks (CGN) by Sauer and Geiger 3 to verify their main claims, which state that (i) their proposed model can reliably generate high-quality counterfactual 4 images by disentangling the shape, texture and background of the image into independent mechanisms, (ii) each 5 independent mechanism has to be considered, and jointly optimizing all of them end-to-end is needed for high-quality 6 images, and (iii) despite being synthetic, these counterfactual images can improve out-of-distribution performance of 7 classifiers by making them invariant to spurious signals. 8",
                "year": 2022,
                "authors": []
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "(2020) employ normalizing flows and variational inference for enabling tractable counterfactual inference, Sanchez & Tsaftaris (2022) use diffusion models for counterfactual estimation, Axel Sauer (2021) proposes counterfactual generative networks, Dash et al.",
                "\u2026inference for enabling tractable counterfactual inference, Sanchez & Tsaftaris (2022) use diffusion models for counterfactual estimation, Axel Sauer (2021) proposes counterfactual generative networks, Dash et al. (2022) incorporates a structural causal model (SCM) in a variant of\u2026",
                "(2020) employ normalizing flows and variational inference for enabling tractable counterfactual inference, Sanchez & Tsaftaris (2022) use diffusion models for counterfactual estimation, Axel Sauer (2021) proposes counterfactual generative networks, Dash et al. (2022) incorporates a structural causal model (SCM) in a variant of Adversarially Learned Inference for generating counterfactual images. Normalizing flow-based methods for answering counterfactual queries has received a lot of attention in no time. For example, Pawlowski et al. (2020)\u2019s work on healthy magnetic resonance images of the brain has been extended to account for the clinical and radiological phenotype of multiple sclerosis (MS) by Reinhold et al. (2021). Wang et al. (2021) perform counterfactual inference to achieve harmonization of brain imaging data with different protocols and from different sites in a clinical study. From a deep learning perspective, an exogenous variable might be considered as an inferred latent variable. To infer the state of the latent noise attached to an endogenous variable, we typically model a normalizing flow, perform amortized variational inference (in the case of very high dimensional variables) (Pawlowski et al., 2020) or use deterministic forward diffusion(Sanchez & Tsaftaris, 2022). Our ability to infer a latent variable comes at a computational cost as well as a statistical cost. To illustrate, the framework for counterfactual estimation by inferring exogenous noises via normalising flows parameterizes each structural assignment of an SCM as an invertible mechanism. Each mechanism explicitly calculates its inverse to enable efficient abduction of exogenous noises. These invertible architectures are typically computationally heavy. For a description of normalizing flows, see Appendix A and Papamakarios et al. (2019). However, given an SCM, in practice, we are interested in counterfactual queries involving a few variables (not all)! For example, Reinhold et al. (2021) studied what the brain image of the subject would look like if the subject did not have lesions, given the observation that they have a 60 mL lesion load. While the proposed SCM consists of age, lesion volume of the subject, duration of MS symptoms, slice number, brain volume, biological sex, image, ventricle volume, and the expanded disability severity score. Hence, it is quite natural to ask for noise variables that we can get rid of from abducting. While Pawlowski et al. (2020) have mentioned (on a footnote) in the case of brain imaging example that abduction of the noise attached to \u2018sex\u2019 is not necessary as \u2018sex\u2019 has no causal parents in the SCM1 (Figure 5, Pawlowski et al. (2020)), we are unaware of any dedicated effort to identify the noises that must be abducted to answer a counterfactual query.",
                "(2020) employ normalizing flows and variational inference for enabling tractable counterfactual inference, Sanchez & Tsaftaris (2022) use diffusion models for counterfactual estimation, Axel Sauer (2021) proposes counterfactual generative networks, Dash et al. (2022) incorporates a structural causal model (SCM) in a variant of Adversarially Learned Inference for generating counterfactual images. Normalizing flow-based methods for answering counterfactual queries has received a lot of attention in no time. For example, Pawlowski et al. (2020)\u2019s work on healthy magnetic resonance images of the brain has been extended to account for the clinical and radiological phenotype of multiple sclerosis (MS) by Reinhold et al. (2021). Wang et al. (2021) perform counterfactual inference to achieve harmonization of brain imaging data with different protocols and from different sites in a clinical study.",
                "For instance, Pawlowski et al. (2020) employ normalizing flows and variational inference for enabling tractable counterfactual inference, Sanchez & Tsaftaris (2022) use diffusion models for counterfactual estimation, Axel Sauer (2021) proposes counterfactual generative networks, Dash et al. (2022) incorporates a structural causal model (SCM) in a variant of Adversarially Learned Inference for generating counterfactual images.",
                "(2020) employ normalizing flows and variational inference for enabling tractable counterfactual inference, Sanchez & Tsaftaris (2022) use diffusion models for counterfactual estimation, Axel Sauer (2021) proposes counterfactual generative networks, Dash et al. (2022) incorporates a structural causal model (SCM) in a variant of Adversarially Learned Inference for generating counterfactual images. Normalizing flow-based methods for answering counterfactual queries has received a lot of attention in no time. For example, Pawlowski et al. (2020)\u2019s work on healthy magnetic resonance images of the brain has been extended to account for the clinical and radiological phenotype of multiple sclerosis (MS) by Reinhold et al. (2021). Wang et al. (2021) perform counterfactual inference to achieve harmonization of brain imaging data with different protocols and from different sites in a clinical study. From a deep learning perspective, an exogenous variable might be considered as an inferred latent variable. To infer the state of the latent noise attached to an endogenous variable, we typically model a normalizing flow, perform amortized variational inference (in the case of very high dimensional variables) (Pawlowski et al., 2020) or use deterministic forward diffusion(Sanchez & Tsaftaris, 2022). Our ability to infer a latent variable comes at a computational cost as well as a statistical cost. To illustrate, the framework for counterfactual estimation by inferring exogenous noises via normalising flows parameterizes each structural assignment of an SCM as an invertible mechanism. Each mechanism explicitly calculates its inverse to enable efficient abduction of exogenous noises. These invertible architectures are typically computationally heavy. For a description of normalizing flows, see Appendix A and Papamakarios et al. (2019). However, given an SCM, in practice, we are interested in counterfactual queries involving a few variables (not all)! For example, Reinhold et al. (2021) studied what the brain image of the subject would look like if the subject did not have lesions, given the observation that they have a 60 mL lesion load. While the proposed SCM consists of age, lesion volume of the subject, duration of MS symptoms, slice number, brain volume, biological sex, image, ventricle volume, and the expanded disability severity score. Hence, it is quite natural to ask for noise variables that we can get rid of from abducting. While Pawlowski et al. (2020) have mentioned (on a footnote) in the case of brain imaging example that abduction of the noise attached to \u2018sex\u2019 is not necessary as \u2018sex\u2019 has no causal parents in the SCM1 (Figure 5, Pawlowski et al.",
                "(2020) employ normalizing flows and variational inference for enabling tractable counterfactual inference, Sanchez & Tsaftaris (2022) use diffusion models for counterfactual estimation, Axel Sauer (2021) proposes counterfactual generative networks, Dash et al. (2022) incorporates a structural causal model (SCM) in a variant of Adversarially Learned Inference for generating counterfactual images. Normalizing flow-based methods for answering counterfactual queries has received a lot of attention in no time. For example, Pawlowski et al. (2020)\u2019s work on healthy magnetic resonance images of the brain has been extended to account for the clinical and radiological phenotype of multiple sclerosis (MS) by Reinhold et al. (2021). Wang et al.",
                "(2020) employ normalizing flows and variational inference for enabling tractable counterfactual inference, Sanchez & Tsaftaris (2022) use diffusion models for counterfactual estimation, Axel Sauer (2021) proposes counterfactual generative networks, Dash et al. (2022) incorporates a structural causal model (SCM) in a variant of Adversarially Learned Inference for generating counterfactual images.",
                "(2020) employ normalizing flows and variational inference for enabling tractable counterfactual inference, Sanchez & Tsaftaris (2022) use diffusion models for counterfactual estimation, Axel Sauer (2021) proposes counterfactual generative networks, Dash et al. (2022) incorporates a structural causal model (SCM) in a variant of Adversarially Learned Inference for generating counterfactual images. Normalizing flow-based methods for answering counterfactual queries has received a lot of attention in no time. For example, Pawlowski et al. (2020)\u2019s work on healthy magnetic resonance images of the brain has been extended to account for the clinical and radiological phenotype of multiple sclerosis (MS) by Reinhold et al.",
                "(2020) employ normalizing flows and variational inference for enabling tractable counterfactual inference, Sanchez & Tsaftaris (2022) use diffusion models for counterfactual estimation, Axel Sauer (2021) proposes counterfactual generative networks, Dash et al. (2022) incorporates a structural causal model (SCM) in a variant of Adversarially Learned Inference for generating counterfactual images. Normalizing flow-based methods for answering counterfactual queries has received a lot of attention in no time. For example, Pawlowski et al. (2020)\u2019s work on healthy magnetic resonance images of the brain has been extended to account for the clinical and radiological phenotype of multiple sclerosis (MS) by Reinhold et al. (2021). Wang et al. (2021) perform counterfactual inference to achieve harmonization of brain imaging data with different protocols and from different sites in a clinical study. From a deep learning perspective, an exogenous variable might be considered as an inferred latent variable. To infer the state of the latent noise attached to an endogenous variable, we typically model a normalizing flow, perform amortized variational inference (in the case of very high dimensional variables) (Pawlowski et al., 2020) or use deterministic forward diffusion(Sanchez & Tsaftaris, 2022). Our ability to infer a latent variable comes at a computational cost as well as a statistical cost. To illustrate, the framework for counterfactual estimation by inferring exogenous noises via normalising flows parameterizes each structural assignment of an SCM as an invertible mechanism. Each mechanism explicitly calculates its inverse to enable efficient abduction of exogenous noises. These invertible architectures are typically computationally heavy. For a description of normalizing flows, see Appendix A and Papamakarios et al. (2019). However, given an SCM, in practice, we are interested in counterfactual queries involving a few variables (not all)! For example, Reinhold et al.",
                "(2020) employ normalizing flows and variational inference for enabling tractable counterfactual inference, Sanchez & Tsaftaris (2022) use diffusion models for counterfactual estimation, Axel Sauer (2021) proposes counterfactual generative networks, Dash et al. (2022) incorporates a structural causal model (SCM) in a variant of Adversarially Learned Inference for generating counterfactual images. Normalizing flow-based methods for answering counterfactual queries has received a lot of attention in no time. For example, Pawlowski et al. (2020)\u2019s work on healthy magnetic resonance images of the brain has been extended to account for the clinical and radiological phenotype of multiple sclerosis (MS) by Reinhold et al. (2021). Wang et al. (2021) perform counterfactual inference to achieve harmonization of brain imaging data with different protocols and from different sites in a clinical study. From a deep learning perspective, an exogenous variable might be considered as an inferred latent variable. To infer the state of the latent noise attached to an endogenous variable, we typically model a normalizing flow, perform amortized variational inference (in the case of very high dimensional variables) (Pawlowski et al., 2020) or use deterministic forward diffusion(Sanchez & Tsaftaris, 2022). Our ability to infer a latent variable comes at a computational cost as well as a statistical cost. To illustrate, the framework for counterfactual estimation by inferring exogenous noises via normalising flows parameterizes each structural assignment of an SCM as an invertible mechanism. Each mechanism explicitly calculates its inverse to enable efficient abduction of exogenous noises. These invertible architectures are typically computationally heavy. For a description of normalizing flows, see Appendix A and Papamakarios et al. (2019). However, given an SCM, in practice, we are interested in counterfactual queries involving a few variables (not all)! For example, Reinhold et al. (2021) studied what the brain image of the subject would look like if the subject did not have lesions, given the observation that they have a 60 mL lesion load."
            ],
            "citingPaper": {
                "paperId": "0fb1b3d51a0556b6b4268fd9c2f2b2ce238af3c7",
                "externalIds": {
                    "DBLP": "journals/tmlr/SahaG22",
                    "CorpusId": 253162697
                },
                "corpusId": 253162697,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0fb1b3d51a0556b6b4268fd9c2f2b2ce238af3c7",
                "title": "On Noise Abduction for Answering Counterfactual Queries: A Practical Outlook",
                "abstract": "A crucial step in counterfactual inference is abduction - inference of the exogenous noise variables. Deep Learning approaches model an exogenous noise variable as a latent variable. Our ability to infer a latent variable comes at a computational cost as well as a statistical cost. In this paper, we show that it may not be necessary to abduct all the noise variables in a structural causal model (SCM) to answer a counterfactual query. In a fully specified causal model with no unobserved confounding, we also identify exogenous noises that must be abducted for a counterfactual query. We introduce a graphical condition for noise identification from an action consisting of an arbitrary combination of hard and soft interventions. We report experimental results on both synthetic and real-world German Credit Dataset, showcasing the promise and usefulness of the proposed exogenous noise identification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114848244",
                        "name": "Saptarshi Saha"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "FlexTENet[131] # ! # # SCP[132] # ! # # CGN[74] ! ! # # SyncTwin[101] # # ! #",
                "com/trends/explore?date=all&q=mnist [125, 198, 74, 199, 200, 201, 202, 203, 204] ADNI www.",
                ", the methods considering representations of distribution balance[40, 42, 43], exploiting the effects of covariates confounding learning[53, 69, 70, 71], the methods based on generative adversarial networks[44, 72, 73, 74] , and so forth[57, 33, 75].",
                "com/usaito/counterfactual-cv CGN[74] MNIST,ImageNet Pytorch https://github."
            ],
            "citingPaper": {
                "paperId": "ecd18261dd5c2022253568852f7726bff5bc557c",
                "externalIds": {
                    "CorpusId": 252411866
                },
                "corpusId": 252411866,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ecd18261dd5c2022253568852f7726bff5bc557c",
                "title": "A SURVEY OF DEEP CAUSAL MODELS",
                "abstract": "The concept of causality plays an important role in human cognition . In the past few decades, causal inference has been well developed in many fields, such as computer science, medicine, economics, and education. With the advancement of deep learning techniques, it has been increasingly used in causal inference against counterfactual data. Typically, deep causal models map the characteristics of covariates to a representation space and then design various objective optimization functions to estimate counterfactual data unbiasedly based on the different optimization methods. This paper focuses on the survey of the deep causal models, and its core contributions are as follows: 1) we provide relevant metrics under multiple treatments and continuous-dose treatment; 2) we incorporate a comprehensive overview of deep causal models from both temporal development and method classification perspectives; 3) we assist a detailed and comprehensive classification and analysis of relevant datasets and source code.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46637795",
                        "name": "Zheng Hua Zhu"
                    },
                    {
                        "authorId": "145181674",
                        "name": "Zhenyu Guo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "few studies along this line also generate counterfactual samples with neural networks (Sauer and Geiger, 2021; Yue et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "80fd13b8ad610572ff83b8aa77a264a188711e7a",
                "externalIds": {
                    "DBLP": "conf/acl/LiFZ0ZC22",
                    "ACL": "2022.acl-long.5",
                    "DOI": "10.18653/v1/2022.acl-long.5",
                    "CorpusId": 248780430
                },
                "corpusId": 248780430,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/80fd13b8ad610572ff83b8aa77a264a188711e7a",
                "title": "Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning",
                "abstract": "Neural discrete reasoning (NDR) has shown remarkable progress in combining deep models with discrete reasoning. However, we find that existing NDR solution suffers from large performance drop on hypothetical questions, e.g. \u201cwhat the annualized rate of return would be if the revenue in 2020 was doubled\u201d. The key to hypothetical question answering (HQA) is counterfactual thinking, which is a natural ability of human reasoning but difficult for deep models. In this work, we devise a Learning to Imagine (L2I) module, which can be seamlessly incorporated into NDR models to perform the imagination of unseen counterfactual. In particular, we formulate counterfactual thinking into two steps: 1) identifying the fact to intervene, and 2) deriving the counterfactual from the fact and assumption, which are designed as neural networks. Based on TAT-QA, we construct a very challenging HQA dataset with 8,283 hypothetical questions. We apply the proposed L2I to TAGOP, the state-of-the-art solution on TAT-QA, validating the rationality and effectiveness of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118769749",
                        "name": "Moxin Li"
                    },
                    {
                        "authorId": "2163400298",
                        "name": "Fuli Feng"
                    },
                    {
                        "authorId": "5462268",
                        "name": "Hanwang Zhang"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "31734386",
                        "name": "Fengbin Zhu"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Thus far, state-of-the-art deep causal structure learning frameworks have been employed to model the data generation process of 2D images (Pawlowski et al., 2020; Vlontzos et al., 2022; Sauer and Geiger, 2021)."
            ],
            "citingPaper": {
                "paperId": "ff45f878a0c0cfa870816faca11ca8ce99b10a37",
                "externalIds": {
                    "CorpusId": 253083376
                },
                "corpusId": 253083376,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ff45f878a0c0cfa870816faca11ca8ce99b10a37",
                "title": "A Framework for Generating 3D Shape Counterfactuals",
                "abstract": "Many important problems in medical imaging require analysing the causal effect of genetic, environmental, or lifestyle factors on the normal and pathological variation of anatomical phenotypes. There is, however, a lack of computational tooling to enable causal reasoning about morphological variations of 3D surface meshes. To tackle this problem, we present the framework of deep structural causal shape models (CSMs) using a database of subcortical brain meshes. CSMs enable subject-specific prognoses through counterfactual mesh generation, by utilising high-quality mesh generation techniques, from geometric deep learning, within the expressive framework of deep structural causal models (DSCM).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182427359",
                        "name": "Rajat Rasal"
                    },
                    {
                        "authorId": "39135119",
                        "name": "Daniel Coelho de Castro"
                    },
                    {
                        "authorId": "2122367673",
                        "name": "Nick Pawlowski"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026we also find that our approach offers the ability to effectively interpolate between OOD face images and more importantly, manipulate specific attributes of interest (e.g., non-smiling \u2192 smiling), thus validating its utility in semantic editing and counterfactual reasoning (Axel Sauer, 2021)."
            ],
            "citingPaper": {
                "paperId": "580032e97d0c8840bc67e7cba6d040cf6508231d",
                "externalIds": {
                    "DBLP": "conf/icml/SubramanyamNNST22",
                    "CorpusId": 250340713
                },
                "corpusId": 250340713,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/580032e97d0c8840bc67e7cba6d040cf6508231d",
                "title": "Improved StyleGAN-v2 based Inversion for Out-of-Distribution Images",
                "abstract": "Inverting an image onto the latent space of pre-trained generators, e.g., StyleGAN-v2, has emerged as a popular strategy to leverage strong image priors for ill-posed restoration. Several studies have showed that this approach is effective at inverting images similar to the data used for training. However, with out-of-distribution (OOD) data that the generator has not been ex-posed to, existing inversion techniques produce sub-optimal results. In this paper, we propose SPHInX (StyleGAN with Projection Heads for Inverting X), an approach for accurately embedding OOD images onto the StyleGAN latent space. SPHInX optimizes a style projection head using a novel training strategy that imposes a vicinal regularization in the StyleGAN latent space. To further enhance OOD inversion, SPHInX can ad-ditionally optimize a content projection head and noise variables in every layer. Our empirical studies on a suite of OOD data show that, in addition to producing higher quality reconstructions over the state-of-the-art inversion techniques, SPHInX is effective for ill-posed restoration tasks while offering semantic editing capabilities.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2175280217",
                        "name": "Rakshith Subramanyam"
                    },
                    {
                        "authorId": "51881215",
                        "name": "V. Narayanaswamy"
                    },
                    {
                        "authorId": "1658781889",
                        "name": "M. Naufel"
                    },
                    {
                        "authorId": "49413461",
                        "name": "A. Spanias"
                    },
                    {
                        "authorId": "1744175",
                        "name": "Jayaraman J. Thiagarajan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "b6a13d5b925f26d69b84fe2f9b78b14d06584ab9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-14221",
                    "DOI": "10.48550/arXiv.2211.14221",
                    "CorpusId": 254017438
                },
                "corpusId": 254017438,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b6a13d5b925f26d69b84fe2f9b78b14d06584ab9",
                "title": "High-Dimensional Causal Discovery: Learning from Inverse Covariance via Independence-based Decomposition",
                "abstract": ",",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51931117",
                        "name": "Shuyu Dong"
                    },
                    {
                        "authorId": "2106199883",
                        "name": "Kento Uemura"
                    },
                    {
                        "authorId": "2192513619",
                        "name": "Akito Fujii"
                    },
                    {
                        "authorId": "2116683671",
                        "name": "Shuang Chang"
                    },
                    {
                        "authorId": "31846119",
                        "name": "Yusuke Koyanagi"
                    },
                    {
                        "authorId": "2197887",
                        "name": "Koji Maruhashi"
                    },
                    {
                        "authorId": "69343681",
                        "name": "M. Sebag"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Such data can be manipulated, interpolated or composed [12, 13, 28, 43, 44] with dedicated operators in their latent space, and further used for counterfactual reasoning [56, 65, 72, 92]."
            ],
            "citingPaper": {
                "paperId": "70e8f98fbb0a0acdbd08af343a8504e7fd664267",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-08420",
                    "DOI": "10.48550/arXiv.2212.08420",
                    "CorpusId": 254823400
                },
                "corpusId": 254823400,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/70e8f98fbb0a0acdbd08af343a8504e7fd664267",
                "title": "Fake it till you make it: Learning(s) from a synthetic ImageNet clone",
                "abstract": "Recent large-scale image generation models such as Stable Diffusion have exhibited an impressive ability to generate fairly realistic images starting from a very simple text prompt. Could such models render real images obsolete for training image prediction models? In this paper, we an-swer part of this provocative question by questioning the need for real images when training models for ImageNet classi\ufb01cation. More precisely, provided only with the class names that have been used to build the dataset, we explore the ability of Stable Diffusion to generate synthetic clones of ImageNet and measure how useful they are for training classi\ufb01cation models from scratch. We show that with minimal and class-agnostic prompt engineering those ImageNet clones we denote as ImageNet-SD are able to close a large part of the gap between models produced by synthetic images and models trained with real images for the several standard classi\ufb01cation benchmarks that we consider in this study. More importantly, we show that models trained on synthetic images exhibit strong generalization properties and perform on par with models trained on real data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1413822807",
                        "name": "Mert Bulent Sariyildiz"
                    },
                    {
                        "authorId": "72492981",
                        "name": "Alahari Karteek"
                    },
                    {
                        "authorId": "2295553",
                        "name": "Diane Larlus"
                    },
                    {
                        "authorId": "1944225",
                        "name": "Yannis Kalantidis"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Some works investigate intermediate GAN representations to construct part segmentation datasets [74, 75], while some treat pretrained GANs as black-box models and use them to augment data for robustness [76, 35] or ensembling [77]."
            ],
            "citingPaper": {
                "paperId": "b22bc897b9f18b22fdd39a1973c31bf4f65f1a7d",
                "externalIds": {
                    "DBLP": "conf/nips/LiCMS022",
                    "CorpusId": 258509181
                },
                "corpusId": 258509181,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b22bc897b9f18b22fdd39a1973c31bf4f65f1a7d",
                "title": "Optimal Positive Generation via Latent Transformation for Contrastive Learning",
                "abstract": "Contrastive learning, which learns to contrast positive with negative pairs of samples, has been popular for self-supervised visual representation learning. Although great effort has been made to design proper positive pairs through data augmentation, few works attempt to generate optimal positives for each instance. Inspired by semantic consistency and computational advantage in latent space of pretrained generative models, this paper proposes to learn instance-specific latent transformations to generate Contrastive Optimal Positives (COP-Gen) for self-supervised contrastive learning. Specifically, we formulate COP-Gen as an instance-specific latent space navigator which minimizes the mutual information between the generated positive pair subject to the semantic consistency constraint. Theoretically, the learned latent transformation creates optimal positives for contrastive learning, which removes as much nuisance information as possible while preserving the semantics. Empirically, using generated positives by COP-Gen consistently outperforms other latent transformation methods and even real-image-based methods in self-supervised contrastive learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1527098277",
                        "name": "Yinqi Li"
                    },
                    {
                        "authorId": "2116284897",
                        "name": "Hong Chang"
                    },
                    {
                        "authorId": "1798982",
                        "name": "Bingpeng Ma"
                    },
                    {
                        "authorId": "145455919",
                        "name": "S. Shan"
                    },
                    {
                        "authorId": "46772547",
                        "name": "Xilin Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Examples include CausalGAN, CausalVAE, and counterfactual generative networks (Kocaoglu et al., 2017; Yang et al., 2021; Sauer & Geiger, 2021)."
            ],
            "citingPaper": {
                "paperId": "e2bc83185fbac80d61d2ad7c242ebe7425a40d23",
                "externalIds": {
                    "DBLP": "journals/tmlr/BoseMG22",
                    "CorpusId": 258787951
                },
                "corpusId": 258787951,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e2bc83185fbac80d61d2ad7c242ebe7425a40d23",
                "title": "Controllable Generative Modeling via Causal Reasoning",
                "abstract": "Deep latent variable generative models excel at generating complex, high-dimensional data, often exhibiting impressive generalization beyond the training distribution. However, many such models in use today are black-boxes trained on large unlabelled datasets with statistical objectives and lack an interpretable understanding of the latent space required for controlling the generative process. We propose CAGE, a framework for controllable generation in latent variable models based on causal reasoning. Given a pair of attributes, CAGE infers the implicit cause-effect relationships between these attributes as induced by a deep generative model. This is achieved by defining and estimating a novel notion of unit-level causal effects in the latent space of the generative model. Thereafter, we use the inferred cause-effect relationships to design a novel strategy for controllable generation based on counterfactual sampling. Through a series of large-scale synthetic and human evaluations, we demonstrate that generating counterfactual samples which respect the underlying causal relationships inferred via CAGE leads to subjectively more realistic images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2175276350",
                        "name": "Joey Bose"
                    },
                    {
                        "authorId": "2738769",
                        "name": "R. Monti"
                    },
                    {
                        "authorId": "1954250",
                        "name": "Aditya Grover"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "b3c851cc6ed0a98a174e3a77c49a40d40d6f60f7",
                "externalIds": {
                    "CorpusId": 261091142
                },
                "corpusId": 261091142,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b3c851cc6ed0a98a174e3a77c49a40d40d6f60f7",
                "title": "MAGA: M ODELING A G ROUP A CTION",
                "abstract": "Combinatorial generalization, an ability to collect various attributes from diverse data and assemble them to generate novel unexperienced data, is considered an essential traversal point to achieve human-level intelligence. Previous unsupervised approaches mainly focused on learning the disentangled representation, such as the variational autoencoder. However, recent studies discovered that the disentangled representation is insufficient for combinatorial generalization and is not even correlated. In this regard, we proposed a novel framework of data generation that can robustly generalize under these distribution shift situations. The model, simulating the group action, carries out combinatorial generalization by discovering the fundamental transformation between the data. We conducted experiments on the two settings: Recombination-to-Element and Recombination-to-Range. The experiments demonstrated that our method has quantitatively and qualitatively superior generalizability and generates better images over traditional models.",
                "year": 2022,
                "authors": []
            }
        }
    ]
}