{
    "offset": 0,
    "data": [
        {
            "contexts": [
                "based representation learning leads to fairness [34, 61]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f114d42d06663c3f4d1f8970714e4ede8736934c",
                "externalIds": {
                    "DBLP": "conf/IEEEares/FarayolaTBSC23",
                    "DOI": "10.1145/3600160.3605033",
                    "CorpusId": 260736697
                },
                "corpusId": 260736697,
                "publicationVenue": {
                    "id": "6be078fe-e448-4123-8096-25493e7a2502",
                    "name": "ARES",
                    "type": "conference",
                    "alternate_names": [
                        "Availability, Reliability and Security",
                        "Availab Reliab Secur",
                        "International Conference on Availability, Reliability and Security",
                        "Int Conf Availab Reliab Secur"
                    ],
                    "issn": "2554-2656",
                    "url": "https://ares.hypotheses.org/",
                    "alternate_urls": [
                        "http://www.ares-conference.eu/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f114d42d06663c3f4d1f8970714e4ede8736934c",
                "title": "Fairness of AI in Predicting the Risk of Recidivism: Review and Phase Mapping of AI Fairness Techniques",
                "abstract": "Artificial Intelligence (AI) is applied in almost every public sector because of its positive impacts. However, AI\u2019s ethical aspects and trustworthiness constitute a significant uproar and concern among different AI stakeholders due to AI\u2019s adverse effect on users when the AI system lacks cautionary measures. AI is used in the criminal justice system for predicting recidivism risk. However, AI\u2019s negative impact translates into bias and high incarceration towards a group of defendants in a population assessed for recidivism risk. This paper focuses on fairness as a requirement of a trustworthy AI framework previously proposed to ascertain the appropriate application of AI systems in predicting recidivism. This paper aims to raise awareness about the fairness of AI models and stimulate further research and deployment of efficient and effective exploitation of fair and trustworthy AI models in the criminal justice system when predicting recidivism. Fairness has been a significant concern for criminal justice system stakeholders and has received considerable attention with more theoretical and practical studies than other trustworthy AI requirements. Hence, this paper reviews state-of-the-art fairness, outlines valuable findings, and proposes future directions to achieve fair AI systems for predicting recidivism risk. In addition, this paper ensures mapping existing technical works in the literature to the fairness pipeline corresponding to the criminal justice system\u2019s AI development phases.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2006348694",
                        "name": "M. Farayola"
                    },
                    {
                        "authorId": "2094553",
                        "name": "Irina Tal"
                    },
                    {
                        "authorId": "2080356488",
                        "name": "Bendechache Malika"
                    },
                    {
                        "authorId": "2588256",
                        "name": "Takfarinas Saber"
                    },
                    {
                        "authorId": "144829604",
                        "name": "R. Connolly"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c1336bef909d15143f48fc5f46229cf7c45bd199",
                "externalIds": {
                    "DOI": "10.1145/3587035",
                    "CorpusId": 261077213
                },
                "corpusId": 261077213,
                "publicationVenue": {
                    "id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
                    "name": "Communications of the ACM",
                    "type": "journal",
                    "alternate_names": [
                        "Commun ACM",
                        "Communications of The ACM"
                    ],
                    "issn": "0001-0782",
                    "url": "http://www.acm.org/pubs/cacm/",
                    "alternate_urls": [
                        "http://portal.acm.org/cacm",
                        "http://www.acm.org/pubs/contents/journals/cacm/",
                        "https://cacm.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c1336bef909d15143f48fc5f46229cf7c45bd199",
                "title": "Humble AI",
                "abstract": "An effort to bring artificial intelligence into better alignment with our moral aims and finally realize the vision of superior decision making through AI.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34774636",
                        "name": "Bran Knowles"
                    },
                    {
                        "authorId": "1397797306",
                        "name": "J. D\u2019cruz"
                    },
                    {
                        "authorId": "2065497554",
                        "name": "John T. Richards"
                    },
                    {
                        "authorId": "1712865",
                        "name": "K. Varshney"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2021) point out that selective classification may amplify unfair decisions, for which fairselective algorithms are required (Lee et al. 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4ddaaf045afc618f3a1c849d6a088b7982e8477a",
                "externalIds": {
                    "DBLP": "conf/aaai/PugnanaR23",
                    "DOI": "10.1609/aaai.v37i8.26133",
                    "CorpusId": 259678288
                },
                "corpusId": 259678288,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4ddaaf045afc618f3a1c849d6a088b7982e8477a",
                "title": "A Model-Agnostic Heuristics for Selective Classification",
                "abstract": "Selective classification (also known as classification with reject option) conservatively extends a classifier with a selection function to determine whether or not a prediction should be accepted (i.e., trusted, used, deployed). This is a highly relevant issue in socially sensitive tasks, such as credit scoring.\nState-of-the-art approaches rely on Deep Neural Networks (DNNs) that train at the same time both the classifier and the selection function. These approaches are model-specific and computationally expensive. \nWe propose a model-agnostic approach, as it can work with any base probabilistic binary classification algorithm, and it can be scalable to large tabular datasets if the base classifier is so. The proposed algorithm, called SCROSS, exploits a cross-fitting strategy and theoretical results for quantile estimation to build the selection function. Experiments on real-world data show that SCROSS improves over existing methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2150773786",
                        "name": "Andrea Pugnana"
                    },
                    {
                        "authorId": "2325428",
                        "name": "S. Ruggieri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In our review of algorithms and recent surveys [28], methods for achieving PP does not appear to receive as much focus as other group fairness definitions such as demographic parity and equalized odds [26] as also observed in [38], [63].",
                "Existing works for obtaining PP include [54] which attempts to maintain calibration of a model alongside equalized odds, [27] which presents a individual-fairness notion of calibration called multicalibration, [29] which uses group membership in post-processing for PP but does not calibrate, [38] which regularizes for PP using an information-theoretic approach, and [63] which applies to binary 0/1 classifiers."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6d6e429120058e09ef37b8d53ac65b9b17ca9b62",
                "externalIds": {
                    "ArXiv": "2302.01574",
                    "DBLP": "journals/corr/abs-2302-01574",
                    "DOI": "10.48550/arXiv.2302.01574",
                    "CorpusId": 256598179
                },
                "corpusId": 256598179,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6d6e429120058e09ef37b8d53ac65b9b17ca9b62",
                "title": "An Operational Perspective to Fairness Interventions: Where and How to Intervene",
                "abstract": "As AI-based decision systems proliferate, their successful operationalization requires balancing multiple desiderata: predictive performance, disparity across groups, safeguarding sensitive group attributes (e.g., race), and engineering cost. We present a holistic framework for evaluating and contextualizing fairness interventions with respect to the above desiderata. The two key points of practical consideration are \\emph{where} (pre-, in-, post-processing) and \\emph{how} (in what way the sensitive group data is used) the intervention is introduced. We demonstrate our framework with a case study on predictive parity. In it, we first propose a novel method for achieving predictive parity fairness without using group data at inference time via distibutionally robust optimization. Then, we showcase the effectiveness of these methods in a benchmarking study of close to 400 variations across two major model types (XGBoost vs. Neural Net), ten datasets, and over twenty unique methodologies. Methodological insights derived from our empirical study inform the practical design of ML workflow with fairness as a central concern. We find predictive parity is difficult to achieve without using group data, and despite requiring group data during model training (but not inference), distributionally robust methods we develop provide significant Pareto improvement. Moreover, a plain XGBoost model often Pareto-dominates neural networks with fairness interventions, highlighting the importance of model inductive bias.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064231723",
                        "name": "Brian Hsu"
                    },
                    {
                        "authorId": "2144149078",
                        "name": "Xiaotong Chen"
                    },
                    {
                        "authorId": "2204528611",
                        "name": "Ying Han"
                    },
                    {
                        "authorId": "40281109",
                        "name": "Hongseok Namkoong"
                    },
                    {
                        "authorId": "38428283",
                        "name": "Kinjal Basu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[41] Joshua K Lee, Yuheng Bu, Deepta Rajan, Prasanna Sattigeri, Rameswar Panda, Subhro Das, and Gregory W Wornell.",
                "[41] recently studied the fair selective classification w.",
                "(4) FSCS [41] adopted the conditional mutual information constraint I(A, Y |f(X)) to promote the sufficiency."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ddb2101759fc004d9139df107279a2667add7ffd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-10837",
                    "ArXiv": "2210.10837",
                    "DOI": "10.48550/arXiv.2210.10837",
                    "CorpusId": 253018377
                },
                "corpusId": 253018377,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ddb2101759fc004d9139df107279a2667add7ffd",
                "title": "On Learning Fairness and Accuracy on Multiple Subgroups",
                "abstract": "We propose an analysis in fair learning that preserves the utility of the data while reducing prediction disparities under the criteria of group sufficiency. We focus on the scenario where the data contains multiple or even many subgroups, each with limited number of samples. As a result, we present a principled method for learning a fair predictor for all subgroups via formulating it as a bilevel objective. Specifically, the subgroup specific predictors are learned in the lower-level through a small amount of data and the fair predictor. In the upper-level, the fair predictor is updated to be close to all subgroup specific predictors. We further prove that such a bilevel objective can effectively control the group sufficiency and generalization error. We evaluate the proposed framework on real-world datasets. Empirical evidence suggests the consistently improved fair predictions, as well as the comparable accuracy to the baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35701651",
                        "name": "Changjian Shui"
                    },
                    {
                        "authorId": "2049043240",
                        "name": "Gezheng Xu"
                    },
                    {
                        "authorId": "2157956022",
                        "name": "Qi Chen"
                    },
                    {
                        "authorId": "2108992225",
                        "name": "Jiaqi Li"
                    },
                    {
                        "authorId": "2059988586",
                        "name": "C. Ling"
                    },
                    {
                        "authorId": "1699104",
                        "name": "T. Arbel"
                    },
                    {
                        "authorId": null,
                        "name": "Boyu Wang"
                    },
                    {
                        "authorId": "11146706",
                        "name": "Christian Gagn'e"
                    }
                ]
            }
        },
        {
            "contexts": [
                "dence (this is known as uncertainty quantification [7]) and that second, the confidence is well-calibrated so that it is not over-confident or under-confident [9].",
                "Discussions around AI equity have focused on the issues arising from uncertainty [9], such as: how AI systems arrive at determi-"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4f9ee85b6b4c8bb49ddbe4ce5a907b2b64ad187e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-01305",
                    "ArXiv": "2208.01305",
                    "DOI": "10.48550/arXiv.2208.01305",
                    "CorpusId": 251253182
                },
                "corpusId": 251253182,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4f9ee85b6b4c8bb49ddbe4ce5a907b2b64ad187e",
                "title": "Humble Machines: Attending to the Underappreciated Costs of Misplaced Distrust",
                "abstract": "It is curious that AI increasingly outperforms human decision makers, yet much of the public distrusts AI to make decisions affecting their lives. In this paper we explore a novel theory that may explain one reason for this. We propose that public distrust of AI is a moral consequence of designing systems that prioritize reduction of costs of false positives over less tangible costs of false negatives. We show that such systems, which we characterize as 'distrustful', are more likely to miscategorize trustworthy individuals, with cascading consequences to both those individuals and the overall human-AI trust relationship. Ultimately, we argue that public distrust of AI stems from well-founded concern about the potential of being miscategorized. We propose that restoring public trust in AI will require that systems are designed to embody a stance of 'humble trust', whereby the moral costs of the misplaced distrust associated with false negatives is weighted appropriately during development and use.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "34774636",
                        "name": "Bran Knowles"
                    },
                    {
                        "authorId": "1397797306",
                        "name": "J. D\u2019cruz"
                    },
                    {
                        "authorId": "2065497554",
                        "name": "John T. Richards"
                    },
                    {
                        "authorId": "1712865",
                        "name": "K. Varshney"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[326] 2021 ICML Hort and Sarro [327] 2021 ASE Perrone et al.",
                ", the classification model abstains from making some of the predictions), selective classification approaches can be used [326]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "002040f8c411fc4a077a0f9a726f80e95509a388",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-07068",
                    "ArXiv": "2207.07068",
                    "DOI": "10.48550/arXiv.2207.07068",
                    "CorpusId": 250526377
                },
                "corpusId": 250526377,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/002040f8c411fc4a077a0f9a726f80e95509a388",
                "title": "Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey",
                "abstract": "This paper provides a comprehensive survey of bias mitigation methods for achieving fairness in Machine Learning (ML) models. We collect a total of 341 publications concerning bias mitigation for ML classifiers. These methods can be distinguished based on their intervention procedure (i.e., pre-processing, in-processing, post-processing) and the technology they apply. We investigate how existing bias mitigation methods are evaluated in the literature. In particular, we consider datasets, metrics and benchmarking. Based on the gathered insights (e.g., what is the most popular fairness metric? How many datasets are used for evaluating bias mitigation methods?). We hope to support practitioners in making informed choices when developing and evaluating new bias mitigation methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2091097171",
                        "name": "Max Hort"
                    },
                    {
                        "authorId": "115023982",
                        "name": "Zhenpeng Chen"
                    },
                    {
                        "authorId": "51250527",
                        "name": "J Zhang"
                    },
                    {
                        "authorId": "2103653",
                        "name": "Federica Sarro"
                    },
                    {
                        "authorId": "145836176",
                        "name": "M. Harman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In terms of the societal impact, fairness in selection remains a concern as lowering the coverage can magnify the difference in recall between groups and increase unfairness (Jones et al., 2021; Lee et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "23fd30a95dccb8627a5935708a5e76722cde8b05",
                "externalIds": {
                    "DBLP": "conf/iclr/FengAHA23",
                    "ArXiv": "2206.09034",
                    "CorpusId": 253553811
                },
                "corpusId": 253553811,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/23fd30a95dccb8627a5935708a5e76722cde8b05",
                "title": "Towards Better Selective Classification",
                "abstract": "We tackle the problem of Selective Classification where the objective is to achieve the best performance on a predetermined ratio (coverage) of the dataset. Recent state-of-the-art selective methods come with architectural changes either via introducing a separate selection head or an extra abstention logit. In this paper, we challenge the aforementioned methods. The results suggest that the superior performance of state-of-the-art methods is owed to training a more generalizable classifier rather than their proposed selection mechanisms. We argue that the best performing selection mechanism should instead be rooted in the classifier itself. Our proposed selection strategy uses the classification scores and achieves better results by a significant margin, consistently, across all coverages and all datasets, without any added compute cost. Furthermore, inspired by semi-supervised learning, we propose an entropy-based regularizer that improves the performance of selective classification methods. Our proposed selection mechanism with the proposed entropy-based regularizer achieves new state-of-the-art results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31787052",
                        "name": "Leo Feng"
                    },
                    {
                        "authorId": "50307467",
                        "name": "M. O. Ahmed"
                    },
                    {
                        "authorId": "1858551",
                        "name": "Hossein Hajimirsadeghi"
                    },
                    {
                        "authorId": "51227724",
                        "name": "A. Abdi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The first one is ADULT [24], which is a widely used public dataset for fair ML [20, 25, 33, 51]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d504693fe20a82f97389da1362d6950dd7a741eb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-03200",
                    "ArXiv": "2206.03200",
                    "DOI": "10.48550/arXiv.2206.03200",
                    "CorpusId": 249431495
                },
                "corpusId": 249431495,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d504693fe20a82f97389da1362d6950dd7a741eb",
                "title": "FairVFL: A Fair Vertical Federated Learning Framework with Contrastive Adversarial Learning",
                "abstract": "Vertical federated learning (VFL) is a privacy-preserving machine learning paradigm that can learn models from features distributed on different platforms in a privacy-preserving way. Since in real-world applications the data may contain bias on fairness-sensitive features (e.g., gender), VFL models may inherit bias from training data and become unfair for some user groups. However, existing fair machine learning methods usually rely on the centralized storage of fairness-sensitive features to achieve model fairness, which are usually inapplicable in federated scenarios. In this paper, we propose a fair vertical federated learning framework (FairVFL), which can improve the fairness of VFL models. The core idea of FairVFL is to learn unified and fair representations of samples based on the decentralized feature fields in a privacy-preserving way. Specifically, each platform with fairness-insensitive features first learns local data representations from local features. Then, these local representations are uploaded to a server and aggregated into a unified representation for the target task. In order to learn a fair unified representation, we send it to each platform storing fairness-sensitive features and apply adversarial learning to remove bias from the unified representation inherited from the biased data. Moreover, for protecting user privacy, we further propose a contrastive adversarial learning method to remove private information from the unified representation in server before sending it to the platforms keeping fairness-sensitive features. Experiments on three real-world datasets validate that our method can effectively improve model fairness with user privacy well-protected.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50329599",
                        "name": "Tao Qi"
                    },
                    {
                        "authorId": "2397264",
                        "name": "Fangzhao Wu"
                    },
                    {
                        "authorId": "15161448",
                        "name": "Chuhan Wu"
                    },
                    {
                        "authorId": "145225199",
                        "name": "Lingjuan Lyu"
                    },
                    {
                        "authorId": "2151647543",
                        "name": "Tongye Xu"
                    },
                    {
                        "authorId": "143932857",
                        "name": "Zhongliang Yang"
                    },
                    {
                        "authorId": "2145525387",
                        "name": "Yongfeng Huang"
                    },
                    {
                        "authorId": "2149195520",
                        "name": "Xing Xie"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c6c666507387c8d71e12173b3b8194c69d6ba60d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-02237",
                    "ArXiv": "2206.02237",
                    "DOI": "10.1145/3531146.3534645",
                    "CorpusId": 249395227
                },
                "corpusId": 249395227,
                "publicationVenue": {
                    "id": "cdc83875-a82d-445c-b097-cbe91afe99a8",
                    "name": "Conference on Fairness, Accountability and Transparency",
                    "type": "conference",
                    "alternate_names": [
                        "FAccT",
                        "Conf Fairness Account Transpar"
                    ],
                    "url": "https://facctconference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c6c666507387c8d71e12173b3b8194c69d6ba60d",
                "title": "Enforcing Group Fairness in Algorithmic Decision Making: Utility Maximization Under Sufficiency",
                "abstract": "Binary decision making classifiers are not fair by default. Fairness requirements are an additional element to the decision making rationale, which is typically driven by maximizing some utility function. In that sense, algorithmic fairness can be formulated as a constrained optimization problem. This paper contributes to the discussion on how to implement fairness, focusing on the fairness concepts of positive predictive value (PPV) parity, false omission rate (FOR) parity, and sufficiency (which combines the former two). We show that group-specific threshold rules are optimal for PPV parity and FOR parity, similar to well-known results for other group fairness criteria. However, depending on the underlying population distributions and the utility function, we find that sometimes an upper-bound threshold rule for one group is optimal: utility maximization under PPV parity (or FOR parity) might thus lead to selecting the individuals with the smallest utility for one group, instead of selecting the most promising individuals. This result is counter-intuitive and in contrast to the analogous solutions for statistical parity and equality of opportunity. We also provide a solution for the optimal decision rules satisfying the fairness constraint sufficiency. We show that more complex decision rules are required and that this leads to within-group unfairness for all but one of the groups. We illustrate our findings based on simulated and real data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2058322501",
                        "name": "J. Baumann"
                    },
                    {
                        "authorId": "1688130",
                        "name": "Anik'o Hann'ak"
                    },
                    {
                        "authorId": "151473392",
                        "name": "Christoph Heitz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[29] Joshua K Lee, Yuheng Bu, Deepta Rajan, Prasanna Sattigeri, Rameswar Panda, Subhro Das, and Gregory W Wornell.",
                "Since predictive parity is commonly considered in scenarios where false positives are particularly harmful [29], we study cost-sensitive classification.",
                "For selective classification, [29] finds that sufficiency-based representation learning leads to fairness."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "efa2c10ce8a2d767314b5c15bab0f8c334f10227",
                "externalIds": {
                    "ArXiv": "2205.07182",
                    "DBLP": "journals/corr/abs-2205-07182",
                    "DOI": "10.48550/arXiv.2205.07182",
                    "CorpusId": 248811490
                },
                "corpusId": 248811490,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/efa2c10ce8a2d767314b5c15bab0f8c334f10227",
                "title": "Fair Bayes-Optimal Classifiers Under Predictive Parity",
                "abstract": "Increasing concerns about disparate effects of AI have motivated a great deal of work on fair machine learning. Existing works mainly focus on independence- and separation-based measures (e.g., demographic parity, equality of opportunity, equalized odds), while sufficiency-based measures such as predictive parity are much less studied. This paper considers predictive parity, which requires equalizing the probability of success given a positive prediction among different protected groups. We prove that, if the overall performances of different groups vary only moderately, all fair Bayes-optimal classifiers under predictive parity are group-wise thresholding rules. Perhaps surprisingly, this may not hold if group performance levels vary widely; in this case we find that predictive parity among protected groups may lead to within-group unfairness. We then propose an algorithm we call FairBayes-DPP, aiming to ensure predictive parity when our condition is satisfied. FairBayes-DPP is an adaptive thresholding algorithm that aims to achieve predictive parity, while also seeking to maximize test accuracy. We provide supporting experiments conducted on synthetic and empirical data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111550463",
                        "name": "Xianli Zeng"
                    },
                    {
                        "authorId": "2694895",
                        "name": "Edgar Dobriban"
                    },
                    {
                        "authorId": "2156114644",
                        "name": "Guang Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", AUC (D = 1), and (c) the area under the absolute difference of the subgroup MSE vs coverage curves (AUADC) (Franc & Prusa, 2019; Lee et al., 2021) respectively.",
                "coverage curve (AUC), which encapsulates performance across different coverage (Franc & Prusa, 2019; Lee et al., 2021).",
                "\u2026(a) the area under the majority MSE vs. coverage curve, i.e., AUC (D = 0), (b) the area under the minority MSE vs. coverage curve, i.e., AUC (D = 1), and (c) the area under the absolute difference of the subgroup MSE vs coverage curves (AUADC) (Franc & Prusa, 2019; Lee et al., 2021) respectively.",
                "\u2026end for for each batch do\n# update feature extractor \u03b8\u03a6 \u2190 \u03b8\u03a6 \u2212 1n\u03b7\u2207\u03b8\u03a6(LG(\u03a6, \u03b8) + \u03bbLR(\u03a6)) # update mean/variance predictor \u03b8 \u2190 \u03b8 \u2212 1n\u03b7\u2207\u03b8LG(\u03a6, \u03b8)\nend for end for\nupper bound for I(Y ;D|\u03a6(X)) from (Lee et al., 2021):\nI(Y ;D|\u03a6(X)) \u2264 E\u03a6(X),Y,D [logP(Y |\u03a6(X), D)] (3) \u2212 ED [ E\u03a6(X),Y [logP(Y |\u03a6(X), D)]\u2026",
                ", 2021) and has been used in fair selective classification (Lee et al., 2021).",
                "In Section 6, we compared the different algorithms in terms of how well they perform fair selective regression by looking at the subgroup MSE vs. coverage curves in addition to AUC, AUC (D = 0), AUC (D = 1), and AUADC.",
                "upper bound for I(Y ;D|\u03a6(X)) from (Lee et al., 2021):",
                "As discussed in (Lee et al., 2021), existing methods using mutual information for fairness are ill-equipped to handle conditioning on the feature representation \u03a6(\u00b7).",
                "To compare different algorithms in terms of how well they perform selective regression (i.e., without fairness), we look at area under MSE vs. coverage curve (AUC), which encapsulates performance across different coverage (Franc & Prusa, 2019; Lee et al., 2021).",
                "Similar to (Lee et al., 2021), we do not assume access to the identity of sensitive groups at test time.",
                "These aspects could be quantitatively captured by looking at (a) the area under the majority MSE vs. coverage curve, i.e., AUC (D = 0), (b) the area under the minority MSE vs. coverage curve, i.e., AUC (D = 1), and (c) the area under the absolute difference of the subgroup MSE vs coverage curves (AUADC) (Franc & Prusa, 2019; Lee et al., 2021) respectively.",
                "However, for a particular coverage, Algorithm 2 achieves a better MSE for the minority subgroup, a comparable MSE for the majority subgroup, and reduces the gap between the subgroup curves than Baseline 2 (see the values of AUC (D = 0), AUC (D = 1), and AUADC in Table 2/3).",
                "Additionally, Algorithm 1 achieves a comparable MSE for the majority subgroup, and reduces the gap between the subgroup curves than Baseline 1 (see the values of AUC (D = 0), AUC (D = 1), and AUADC in Table 2/3).",
                "Further, for a particular coverage, Algorithm 1 achieves a better MSE for the minority subgroup, a comparable MSE for the majority subgroup, and reduces the gap between the subgroup curves than Baseline 1 (see the values of AUC (D = 0), AUC (D = 1), and AUADC in Table 2/3).",
                "We provide these results in Table 2 and observe that our algorithms outperform the baselines across datasets in terms of AUC (D = 1) and AUADC while being comparable in terms of AUC (D = 0).",
                "Sufficiency is closely tied with learning domain-invariant feature representation (Arjovsky et al., 2019; Creager et al., 2021) and has been used in fair selective classification (Lee et al., 2021).",
                "Standard deviations\nIn Table 3 below, we provide the standard deviations associated with AUC, AUC (D = 0), AUC (D = 1), and AUADC whose means where provided in Table 2 in Section 6.",
                "Table 2 suggests that our algorithm outperforms the baseline in terms of AUC (D = 1) and AUADC while being comparable in AUC (D = 0).",
                "To mitigate such disparities, (Lee et al., 2021; Schreuder & Chzhen, 2021) proposed methods for performing fair selective classification."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d3c772f7bb4d6660d6ee5fd49e4c6f33c485a97c",
                "externalIds": {
                    "ArXiv": "2110.15403",
                    "DBLP": "journals/corr/abs-2110-15403",
                    "CorpusId": 240288866
                },
                "corpusId": 240288866,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d3c772f7bb4d6660d6ee5fd49e4c6f33c485a97c",
                "title": "Selective Regression Under Fairness Criteria",
                "abstract": "Selective regression allows abstention from prediction if the con\ufb01dence to make an accurate prediction is not suf\ufb01cient. In general, by allowing a reject option, one expects the performance of a regression model to increase at the cost of reducing coverage (i.e., by predicting on fewer samples). However, as we show, in some cases, the performance of a minority subgroup can decrease while we reduce the coverage, and thus selective regression can magnify disparities between different sensitive subgroups. Motivated by these disparities, we propose new fairness criteria for selective regression requiring the performance of every subgroup to improve with a decrease in coverage. We prove that if a feature representation satis\ufb01es the suf\ufb01ciency criterion or is calibrated for mean and variance , then the proposed fairness criteria is met. Further, we introduce two approaches to mitigate the performance disparity across subgroups: (a) by regularizing an upper bound of conditional mutual information under a Gaussian assumption and (b) by regularizing a contrastive loss for conditional mean and conditional variance prediction. The effectiveness of these approaches is demonstrated on synthetic and real-world datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35678279",
                        "name": "Abhin Shah"
                    },
                    {
                        "authorId": "2371451",
                        "name": "Yuheng Bu"
                    },
                    {
                        "authorId": "2119256327",
                        "name": "Joshua K. Lee"
                    },
                    {
                        "authorId": "3225635",
                        "name": "Subhro Das"
                    },
                    {
                        "authorId": "1819152",
                        "name": "R. Panda"
                    },
                    {
                        "authorId": "1706272",
                        "name": "P. Sattigeri"
                    },
                    {
                        "authorId": "1779692",
                        "name": "G. Wornell"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While fairness in supervised learning is studied (Correa et al., 2021; Chikahara et al., 2021; Lee et al., 2021; Mehrabi et al., 2021; Le Quy et al., 2022; Dwork et al., 2012), the fairness in unsupervised learning is still in its formative stages (Deepak et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7060df1d6b30d4586579a3e5abeba497b818d4ef",
                "externalIds": {
                    "DBLP": "journals/datamine/GuptaGKJ23",
                    "ArXiv": "2109.00708",
                    "DOI": "10.1007/s10618-023-00928-6",
                    "CorpusId": 237385198
                },
                "corpusId": 237385198,
                "publicationVenue": {
                    "id": "d263025a-9eaf-443f-9bbf-72377e8d22a6",
                    "name": "Data mining and knowledge discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Data Mining and Knowledge Discovery",
                        "Data Min Knowl Discov",
                        "Data min knowl discov"
                    ],
                    "issn": "1384-5810",
                    "url": "https://www.springer.com/computer/database+management+&+information+retrieval/journal/10618",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10618",
                        "http://www.springer.com/computer/database+management+&+information+retrieval/journal/10618"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7060df1d6b30d4586579a3e5abeba497b818d4ef",
                "title": "Efficient algorithms for fair clustering with a new notion of fairness",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118927614",
                        "name": "Shivam Gupta"
                    },
                    {
                        "authorId": "3402720",
                        "name": "Ganesh Ghalme"
                    },
                    {
                        "authorId": "2503137",
                        "name": "N. C. Krishnan"
                    },
                    {
                        "authorId": "8126815",
                        "name": "Shweta Jain"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Fourth, avoiding strongly biased predictions helps build a more fair model (Lee et al. 2021; Ruggieri et al. 2023)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "01c8c49d9aea8dc14fc0367e6ec7424e89ff71bb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-11277",
                    "ArXiv": "2107.11277",
                    "CorpusId": 236318084
                },
                "corpusId": 236318084,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/01c8c49d9aea8dc14fc0367e6ec7424e89ff71bb",
                "title": "Machine Learning with a Reject Option: A survey",
                "abstract": "Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake. This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machine learning research areas.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "90958315",
                        "name": "Kilian Hendrickx"
                    },
                    {
                        "authorId": "2073552588",
                        "name": "Lorenzo Perini"
                    },
                    {
                        "authorId": "2120815740",
                        "name": "Dries Van der Plas"
                    },
                    {
                        "authorId": "1718583",
                        "name": "Wannes Meert"
                    },
                    {
                        "authorId": "144815446",
                        "name": "Jesse Davis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Table 2 shows that on the Alzheimer\u2019s disease dataset, our method FACIMS outperforms EIIL, FSCS, FAMS, and ERM in terms of balanced accuracy, with improvements of 2.6%, 4.0%, 5.3%, and 2.1% respectively.",
                "\u2022 FSCS (Lee et al., 2021): An approach that adopts the conditional mutual information constraint to improve group sufficiency.",
                "Lee et al. (2021) proposed a bilevel objective approach to achieve fairness in predictive models across all groups.",
                "Although EIIL, FSCS, and FAMS specifically target the group sufficiency problem and achieve lower sufficiency gaps than ERM and BERM, our method still outperforms these three baseline methods by improving the sufficiency gap by 1.4%, 2.0%, and 2.8% respectively.",
                "As for balanced accuracy, our method FACIMS improves the performance by 4.4%, 8.2%, 2.6%, 7.0%, and 2.3% comapred to EIIL, FSCS, FAMS, ERM and BERM."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b00339d9036478fa048ded875c949d9656b0727f",
                "externalIds": {
                    "DBLP": "conf/uai/TarzanaghHTL023",
                    "CorpusId": 260843665
                },
                "corpusId": 260843665,
                "publicationVenue": {
                    "id": "f9af8000-42f8-410d-a622-e8811e41660a",
                    "name": "Conference on Uncertainty in Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Uncertainty in Artificial Intelligence",
                        "UAI",
                        "Conf Uncertain Artif Intell",
                        "Uncertain Artif Intell"
                    ],
                    "url": "http://www.auai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b00339d9036478fa048ded875c949d9656b0727f",
                "title": "Fairness-aware class imbalanced learning on multiple subgroups",
                "abstract": "We present a novel Bayesian-based optimization framework that addresses the challenge of generalization in overparameterized models when dealing with imbalanced subgroups and limited samples per subgroup. Our proposed tri-level optimization framework utilizes local predictors, which are trained on a small amount of data, as well as a fair and class-balanced predictor at the middle and lower levels. To effectively overcome saddle points for minority classes, our lower-level formulation incorporates sharpness-aware minimization. Meanwhile, at the upper level, the framework dynamically adjusts the loss function based on validation loss, ensuring a close alignment between the global predictor and local predictors. Theoretical analysis demonstrates the framework\u2019s ability to enhance classification and fairness generalization, potentially resulting in improvements in the generalization bound. Empirical results validate the superior performance of our tri-level framework compared to existing state-of-the-art approaches. The source code can be found at https: //github.com/PennShenLab/FACIMS .",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3281605",
                        "name": "Davoud Ataee Tarzanagh"
                    },
                    {
                        "authorId": "2136332838",
                        "name": "Bojian Hou"
                    },
                    {
                        "authorId": "2220440884",
                        "name": "Boning Tong"
                    },
                    {
                        "authorId": "2055685346",
                        "name": "Qi Long"
                    },
                    {
                        "authorId": "2172820082",
                        "name": "Li Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026(e.g., individual fairness (Dwork et al., 2012) and causal fairness (Kusner et al., 2017)), 2) handling noisy or missing group labels (Hashimoto et al., 2018; Celis et al., 2021), and 3) improving fairness in special classification scenarios (e.g., selective classification (Lee et al., 2021))."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "37aa9f75ac8b983470e89989a533e277e00c0ef8",
                "externalIds": {
                    "CorpusId": 259282681
                },
                "corpusId": 259282681,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/37aa9f75ac8b983470e89989a533e277e00c0ef8",
                "title": "Dr-Fairness: Dynamic Data Ratio Adjustment for Fair Training on Real and Generated Data",
                "abstract": "Fair visual recognition has become critical for preventing demographic disparity. A major cause of model unfairness is the imbalanced representation of different groups in training data. Recently, several works aim to alleviate this issue using generated data. However, these approaches often use generated data to obtain similar amounts of data across groups, which is not optimal for achieving high fairness due to different learning difficulties and generated data qualities across groups. To address this issue, we propose a novel adaptive sampling approach that leverages both real and generated data for fairness. We design a bilevel optimization that finds the optimal data sampling ratios among groups and between real and generated data while training a model. The ratios are dynamically adjusted considering both the model\u2019s accuracy as well as its fairness. To efficiently solve our non-convex bilevel optimization, we propose a simple approximation to the solution given by the implicit function theorem. Extensive experiments show that our framework achieves state-of-the-art fairness and accuracy on the CelebA and ImageNet People Subtree datasets. We also observe that our method adaptively relies less on the generated data when it has poor quality. Our work shows the importance of using generated data together with real data for improving model fairness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "30840932",
                        "name": "Yuji Roh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In terms of the societal impact, fairness in selection remains a concern as lowering the coverage can magnify the difference in recall between groups and increase unfairness [14, 19]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1aa97542984776a3797b60dc03af4228f4057d68",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-09034",
                    "DOI": "10.48550/arXiv.2206.09034",
                    "CorpusId": 249890128
                },
                "corpusId": 249890128,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1aa97542984776a3797b60dc03af4228f4057d68",
                "title": "Stop Overcomplicating Selective Classification: Use Max-Logit",
                "abstract": "We tackle the problem of Selective Classi\ufb01cation where the goal is to achieve the best performance on the desired coverages of the dataset. Recent state-of-the-art selective methods come with architectural changes either via introducing a separate selection head or an extra abstention logit. In this paper, we present surprising results for Selective Classi\ufb01cation by con\ufb01rming that the superior performance of state-of-the-art methods is owed to training a more generalizable classi\ufb01er; however, their selection mechanism is suboptimal. We argue that the selection mechanism should be rooted in the objective function instead of a separately calculated score. Accordingly, in this paper, we motivate an alternative selection strategy that is based on the cross entropy loss for the classi\ufb01cation settings, namely, max of the logits. Our proposed selection strategy achieves better results by a signi\ufb01cant margin, consistently, across all coverages and all datasets, without any additional computation. Finally, inspired by our superior selection mechanism, we propose to further regularize the objective function with entropy-minimization. Our proposed max-logit selection with the modi\ufb01ed loss function achieves new state-of-the-art results for Selective Classi\ufb01cation. Gamblers, We empirically show that selecting via max class logit achieves state-of-the-art results across all without additional we suggest entropy-regularized further improve upon by a considerable In contrast we assess all the on and show that struggles in and fails to scale to where de\ufb01ned by the number With the rigorously conducted experiments, we any future research on selection to as",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31787052",
                        "name": "Leo Feng"
                    },
                    {
                        "authorId": "50307467",
                        "name": "M. O. Ahmed"
                    },
                    {
                        "authorId": "1858551",
                        "name": "Hossein Hajimirsadeghi"
                    },
                    {
                        "authorId": "51227724",
                        "name": "A. Abdi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4e90482dce29d55bf5fecf4193417b2a9b303b98",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-00708",
                    "CorpusId": 260972980
                },
                "corpusId": 260972980,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4e90482dce29d55bf5fecf4193417b2a9b303b98",
                "title": "Efficient Algorithms For Fair Clustering with a New Fairness Notion",
                "abstract": "We revisit the problem of fair clustering, first introduced by Chierichetti et al. (2017) , that requires each protected attribute to have approximately equal representation in every cluster; i.e., a Balance property. Existing solutions to fair clustering are either not scalable or do not achieve an optimal trade-off between clustering objective and fairness. In this paper, we propose a new notion of fairness, which we call \u03c4 -ratio fairness, that strictly generalizes the Balance property and enables a finegrained efficiency vs. fairness trade-off. Furthermore, we show that simple greedy round-robin based algorithms achieve this trade-off efficiently. Under a more general setting of multi-valued protected attributes, we rigorously analyze the theoretical properties of the our algorithms. Our experimental results suggest that the proposed solution outperforms all the state-of-the-art algorithms and works exceptionally well even for a large number of clusters.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118927614",
                        "name": "Shivam Gupta"
                    },
                    {
                        "authorId": "3402720",
                        "name": "Ganesh Ghalme"
                    },
                    {
                        "authorId": "2503137",
                        "name": "N. C. Krishnan"
                    },
                    {
                        "authorId": "2116971410",
                        "name": "Shweta Jain"
                    }
                ]
            }
        }
    ]
}